{"claim": "To validate the pretrained model for locating objects in three-dimensional space, the researchers relied on SECOND and PV-RCNN as the two chief detection schemes.", "label": "Supported", "paragraph": "Evaluation Protocol. In evaluating our pre-trained model for 3D object detection, we utilized two prominent architectures: SECOND [70] and PV-RCNN [51]. Both are built on the VoxelNet 3D backbone [82], which processes voxels via 3D sparse convolutions and includes a 2D backbone for bird's-eye-view encoding after BEV projection.", "section_name": "4.2 Transfer on 3D Object Detection", "subsection_name": "", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:02:34.773507", "model": "gpt-5-mini", "original_claim": "For evaluation of the pre-trained 3D object detection model, the authors employed the SECOND and PV-RCNN architectures as the two primary detection frameworks.", "rephrasing_timestamp": "2026-01-11T19:13:06.260008", "rephrasing_model": "gpt-5-mini"}
{"claim": "Both SECOND and PV-RCNN use the same voxel-based three-dimensional feature extractor: it applies sparse three-dimensional convolutions to discretized volumetric cells and, after mapping those features onto a top-down plane, processes them with a two-dimensional network to produce an overhead-view encoding.", "label": "Supported", "paragraph": "Evaluation Protocol. In evaluating our pre-trained model for 3D object detection, we utilized two prominent architectures: SECOND [70] and PV-RCNN [51]. Both are built on the VoxelNet 3D backbone [82], which processes voxels via 3D sparse convolutions and includes a 2D backbone for bird's-eye-view encoding after BEV projection.", "section_name": "4.2 Transfer on 3D Object Detection", "subsection_name": "", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:02:34.773520", "model": "gpt-5-mini", "original_claim": "Both SECOND and PV-RCNN share a common VoxelNet 3D backbone that applies 3D sparse convolutional processing to voxels and adds a two-dimensional backbone for bird's-eye-view encoding after BEV projection.", "rephrasing_timestamp": "2026-01-11T19:13:12.177080", "rephrasing_model": "gpt-5-mini"}
{"claim": "The chief variation among the designs lies in how their final prediction blocks are built. That output stage is the dominant factor distinguishing the systems under evaluation.", "label": "Supported", "paragraph": "The primary distinction between the architectures is in their detection heads. SECOND uses a region proposal network (RPN) directly on the 2D\nTable 3: Evaluation of various pretraining methods initially trained on the nuScenes dataset and subsequently fine-tuned on multiple downstream point cloud datasets.", "section_name": "4.2 Transfer on 3D Object Detection", "subsection_name": "", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:02:46.811499", "model": "gpt-5-mini", "original_claim": "The architectures primarily differ in the configuration of their detection head modules, with the detection head identified as the main distinguishing element among the compared models.", "rephrasing_timestamp": "2026-01-11T19:13:04.283206", "rephrasing_model": "gpt-5-mini"}
{"claim": "In the SECOND design, the component that generates candidate object regions is applied directly to planar input representations rather than being executed on deep, multi-dimensional feature tensors.", "label": "Supported", "paragraph": "The primary distinction between the architectures is in their detection heads. SECOND uses a region proposal network (RPN) directly on the 2D\nTable 3: Evaluation of various pretraining methods initially trained on the nuScenes dataset and subsequently fine-tuned on multiple downstream point cloud datasets.", "section_name": "4.2 Transfer on 3D Object Detection", "subsection_name": "", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:02:46.811515", "model": "gpt-5-mini", "original_claim": "The SECOND architecture implements a region proposal network (RPN) that is applied directly to two-dimensional input representations instead of being run on higher-dimensional feature maps.", "rephrasing_timestamp": "2026-01-11T19:13:07.650192", "rephrasing_model": "gpt-5-mini"}
{"claim": "In our trials, this approach—after being refined using only one percent of annotated samples—attained a 44.91% mean IoU on the ScribbleKITTI benchmark.", "label": "Supported", "paragraph": "The mIoU scores are presented as percentages (%). | Method     | ScribbleKITTI   | ScribbleKITTI   | RELLIS-3D   | RELLIS-3D   | SemanticPOSS   | SemanticPOSS   | SemanticSTF   | SemanticSTF   | SynLiDAR   | SynLiDAR   | DAPS-3D   | DAPS-3D   |\n|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|\n| Method     | 1%          | 10%          | 1%          | 10%         | 50%          | 100%          | 50%          | 100%          | 1%         | 10%        | 50%       | 100%      |\n| Random     | 23.81          | 47.60          | 38.46       | 53.60       | 46.26          | 54.12          | 48.03         | 48.15         | 19.89      | 44.74      | 74.32     | 79.38     |\n| PPKT [39]  | 36.50          | 51.67          | 49.71       | 54.33       | 50.18          | 56.00          | 50.92         | 54.69         | 37.57      | 46.48      | 78.90     | 84.00     |\n| SLidR [50] | 39.60          | 50.45          | 49.75       | 54.57       | 51.56          | 55.36          | 52.01         | 54.35         | 42.05      | 47.84      | 81.00     | 85.40     |\n| Seal [38]  | 40.64          | 52.77          | 51.09       | 55.03       | 53.26          | 56.89          | 53.46         | 55.36         | 43.58      | 49.26      | 81.88     | 85.90     |\n| Ours       | 44.91          | 54.96          | 53.47       | 58.21       | 55.70          | 58.51          | 56.65         | 60.42         | 46.34      | 52.78      | 83.63     | 86.84     |\nTable 4: Comparison of our method with other pre-training techniques through fine-tuning on the KITTI dataset.", "section_name": "4.2 Transfer on 3D Object Detection", "subsection_name": "", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:02:57.634444", "model": "gpt-5-mini", "original_claim": "The proposed method reached 44.91% mIoU on ScribbleKITTI when fine-tuned with 1% labeled data during the experiments.", "rephrasing_timestamp": "2026-01-11T19:13:08.274663", "rephrasing_model": "gpt-5-mini"}
{"claim": "The first algorithm presented outlines a biphasic training routine whose opening phase performs learning at the intended resolution within the routine's opening iterative loop.", "label": "Supported", "paragraph": "We describe a two-stage training algorithm in Algorithm. 1. We first stage for training with target resolution in the first while loop, followed by the second while loop for training with various resolutions.\n<!-- image -->", "section_name": "C TRAINING ALGORITHM", "subsection_name": "", "paper_id": "Cf4FJGmHRQ", "paper_title": "PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:04:15.578248", "model": "gpt-5-mini", "original_claim": "Algorithm 1 specifies a two-stage training procedure whose first stage trains at the target resolution within the algorithm's first while loop.", "rephrasing_timestamp": "2026-01-11T19:13:14.570299", "rephrasing_model": "gpt-5-mini"}
{"claim": "In Algorithm 1, the subsequent loop iteratively updates the model across a range of resolution levels rather than operating at one constant resolution.", "label": "Supported", "paragraph": "We describe a two-stage training algorithm in Algorithm. 1. We first stage for training with target resolution in the first while loop, followed by the second while loop for training with various resolutions.\n<!-- image -->", "section_name": "C TRAINING ALGORITHM", "subsection_name": "", "paper_id": "Cf4FJGmHRQ", "paper_title": "PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:04:15.578260", "model": "gpt-5-mini", "original_claim": "Algorithm 1's second while loop performs training over multiple different resolutions instead of a single fixed resolution.", "rephrasing_timestamp": "2026-01-11T19:13:14.404041", "rephrasing_model": "gpt-5-mini"}
{"claim": "The research received partial support from the NRF (grant number 2022R1A2C4002300), accounting for one-tenth of the study's budget.", "label": "Supported", "paragraph": "This work was partly supported by the NRF grant (No.2022R1A2C4002300, 10%) and IITP grants (No.2020-0-01361 (20%, Yonsei AI), No.2021-0-01343 (10%, SNU AI), No.2022-0-00077 (10%), No.2022-0-00113 (40%), No.2021-0-02068 (10%, AI Innov. Hub)) funded by the Korea government (MSIT). Sanghyun is partially supported by the Google Faculty Research Award. Work is done when J. Choi and N. Park are with Yonsei University. J. Choi and N. Park are co-corresponding authors.", "section_name": "ACKNOWLEDGMENTS", "subsection_name": "", "paper_id": "Cf4FJGmHRQ", "paper_title": "PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:04:43.419624", "model": "gpt-5-mini", "original_claim": "The project was partially financed by the NRF grant No.2022R1A2C4002300, which provided 10% of the study's funding.", "rephrasing_timestamp": "2026-01-11T19:13:24.625468", "rephrasing_model": "gpt-5-mini"}
{"claim": "Portions of this research were financed by the South Korean government via the Ministry of Science and ICT, administered by the Institute for Information & communications Technology Planning and Evaluation (IITP), under grants No. 2020-0-01361 (20%, Yonsei University AI), No. 2021-0-01343 (10%, Seoul National University AI), No. 2022-0-00077 (10%), No. 2022-0-00113 (40%), and No. 2021-0-02068 (10%, AI Innovation Hub).", "label": "Supported", "paragraph": "This work was partly supported by the NRF grant (No.2022R1A2C4002300, 10%) and IITP grants (No.2020-0-01361 (20%, Yonsei AI), No.2021-0-01343 (10%, SNU AI), No.2022-0-00077 (10%), No.2022-0-00113 (40%), No.2021-0-02068 (10%, AI Innov. Hub)) funded by the Korea government (MSIT). Sanghyun is partially supported by the Google Faculty Research Award. Work is done when J. Choi and N. Park are with Yonsei University. J. Choi and N. Park are co-corresponding authors.", "section_name": "ACKNOWLEDGMENTS", "subsection_name": "", "paper_id": "Cf4FJGmHRQ", "paper_title": "PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:04:43.419640", "model": "gpt-5-mini", "original_claim": "The Korea government (MSIT) supported parts of the work via IITP grants No.2020-0-01361 (20%, Yonsei AI), No.2021-0-01343 (10%, SNU AI), No.2022-0-00077 (10%), No.2022-0-00113 (40%), and No.2021-0-02068 (10%, AI Innov. Hub).", "rephrasing_timestamp": "2026-01-11T19:13:25.551672", "rephrasing_model": "gpt-5-mini"}
{"claim": "Adopting PAC-FNO raises the network’s parameter count by roughly nine hundred ten thousand, one million six hundred ten thousand, or three million six hundred fifty thousand, depending on the chosen base architecture or dataset.", "label": "Supported", "paragraph": "We report the number of parameters of PAC-FNO according to backbone models and datasets. Table 11: The number of parameters according to backbone models. Table 12: The number of parameters according to datasets\n| ImageNet-1k ResNet-18 Inception-V3 Vision Transformer ConvNeXt-Tiny ConvNeXt-Tiny StanfordCars Oxford-IIIT Pets Flowers FGVC Aircraft Food-101   | ImageNet-1k ResNet-18 Inception-V3 Vision Transformer ConvNeXt-Tiny ConvNeXt-Tiny StanfordCars Oxford-IIIT Pets Flowers FGVC Aircraft Food-101   | ImageNet-1k ResNet-18 Inception-V3 Vision Transformer ConvNeXt-Tiny ConvNeXt-Tiny StanfordCars Oxford-IIIT Pets Flowers FGVC Aircraft Food-101   | ImageNet-1k ResNet-18 Inception-V3 Vision Transformer ConvNeXt-Tiny ConvNeXt-Tiny StanfordCars Oxford-IIIT Pets Flowers FGVC Aircraft Food-101   | ImageNet-1k ResNet-18 Inception-V3 Vision Transformer ConvNeXt-Tiny ConvNeXt-Tiny StanfordCars Oxford-IIIT Pets Flowers FGVC Aircraft Food-101   | ImageNet-1k ResNet-18 Inception-V3 Vision Transformer ConvNeXt-Tiny ConvNeXt-Tiny StanfordCars Oxford-IIIT Pets Flowers FGVC Aircraft Food-101   | ImageNet-1k ResNet-18 Inception-V3 Vision Transformer ConvNeXt-Tiny ConvNeXt-Tiny StanfordCars Oxford-IIIT Pets Flowers FGVC Aircraft Food-101   | ImageNet-1k ResNet-18 Inception-V3 Vision Transformer ConvNeXt-Tiny ConvNeXt-Tiny StanfordCars Oxford-IIIT Pets Flowers FGVC Aircraft Food-101   | ImageNet-1k ResNet-18 Inception-V3 Vision Transformer ConvNeXt-Tiny ConvNeXt-Tiny StanfordCars Oxford-IIIT Pets Flowers FGVC Aircraft Food-101   | ImageNet-1k ResNet-18 Inception-V3 Vision Transformer ConvNeXt-Tiny ConvNeXt-Tiny StanfordCars Oxford-IIIT Pets Flowers FGVC Aircraft Food-101   | ImageNet-1k ResNet-18 Inception-V3 Vision Transformer ConvNeXt-Tiny ConvNeXt-Tiny StanfordCars Oxford-IIIT Pets Flowers FGVC Aircraft Food-101   |\n|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|\n| # of PAC-FNO parameters          | +0.91M          | +1.61M          | +0.91M          | +3.65M          | # of PAC-FNO parameters          | +3.65M          | +3.65M          | +3.65M          | +3.65M          | +3.65M          |", "section_name": "F.2 NUMBER OF PARAMETERS OF PAC-FNO", "subsection_name": "", "paper_id": "Cf4FJGmHRQ", "paper_title": "PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:05:02.021197", "model": "gpt-5-mini", "original_claim": "PAC-FNO increases model size by specific amounts; the reported additional parameter counts include 0.91 million, 1.61 million, and 3.65 million depending on backbone or dataset choice.", "rephrasing_timestamp": "2026-01-11T19:13:23.603483", "rephrasing_model": "gpt-5-mini"}
{"claim": "Consistent with earlier studies, the researchers employed the Flickr-Faces-HQ dataset (Karras et al., 2019) as the starting dataset for experiments that transfer models to other domains.", "label": "Supported", "paragraph": "Datasets: Following previous literature, we consider Flickr-Faces-HQ (FFHQ) (Karras et al., 2019) as one of the source domains and adapt to the combination of the following individual target domains (a) FFHQbaby , (b) FFHQsunglasses , (c) sketch , and (d) FFHQsmile .", "section_name": "4.1 DATASETS AND METRICS", "subsection_name": "", "paper_id": "FE2e8664Sl", "paper_title": "Few-shot Hybrid Domain Adaptation of Image Generator", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:05:30.822216", "model": "gpt-5-mini", "original_claim": "In line with prior work, the authors treated the Flickr-Faces-HQ (FFHQ) dataset (Karras et al., 2019) as a source domain for their domain adaptation experiments.", "rephrasing_timestamp": "2026-01-11T19:13:23.313177", "rephrasing_model": "gpt-5-mini"}
{"claim": "The adaptation is applied to an aggregated group composed of four distinct datasets: FFHQbaby, FFHQsunglasses, sketch, and FFHQsmile.", "label": "Supported", "paragraph": "Datasets: Following previous literature, we consider Flickr-Faces-HQ (FFHQ) (Karras et al., 2019) as one of the source domains and adapt to the combination of the following individual target domains (a) FFHQbaby , (b) FFHQsunglasses , (c) sketch , and (d) FFHQsmile .", "section_name": "4.1 DATASETS AND METRICS", "subsection_name": "", "paper_id": "FE2e8664Sl", "paper_title": "Few-shot Hybrid Domain Adaptation of Image Generator", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:05:30.822228", "model": "gpt-5-mini", "original_claim": "The adaptation targets a combined set of four individual domains: FFHQbaby, FFHQsunglasses, sketch, and FFHQsmile.", "rephrasing_timestamp": "2026-01-11T19:13:22.955679", "rephrasing_model": "gpt-5-mini"}
{"claim": "Across all trials, the researchers selected ten examples at random for each category, so every category's evaluation was based on that same set of ten randomly chosen examples.", "label": "Supported", "paragraph": "As in previous works, all our experiments use 10 randomly sampled targets for each domain. Unless stated otherwise, we operate on 256 × 256 resolution images for both the source and target domains. Metrics: A key difference between the proposed HDA task against previous DA (Ojha et al., 2021; Mondal et al.; Nitzan et al., 2023) is that there are no real images in the hybrid target domains.", "section_name": "4.1 DATASETS AND METRICS", "subsection_name": "", "paper_id": "FE2e8664Sl", "paper_title": "Few-shot Hybrid Domain Adaptation of Image Generator", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:05:41.502019", "model": "gpt-5-mini", "original_claim": "All experiments in the study used ten targets randomly sampled for each domain, so every domain evaluation relied on a set of ten randomly selected target instances.", "rephrasing_timestamp": "2026-01-11T19:13:35.162606", "rephrasing_model": "gpt-5-mini"}
{"claim": "Unless the paper reported a different input size for some runs, every experiment used images from both datasets resized to 256 by 256 pixels.", "label": "Supported", "paragraph": "As in previous works, all our experiments use 10 randomly sampled targets for each domain. Unless stated otherwise, we operate on 256 × 256 resolution images for both the source and target domains. Metrics: A key difference between the proposed HDA task against previous DA (Ojha et al., 2021; Mondal et al.; Nitzan et al., 2023) is that there are no real images in the hybrid target domains.", "section_name": "4.1 DATASETS AND METRICS", "subsection_name": "", "paper_id": "FE2e8664Sl", "paper_title": "Few-shot Hybrid Domain Adaptation of Image Generator", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:05:41.502035", "model": "gpt-5-mini", "original_claim": "By default, the experiments processed both source and target domain images at a resolution of 256×256 pixels, unless the paper explicitly specified a different image size for particular experiments.", "rephrasing_timestamp": "2026-01-11T19:13:38.161944", "rephrasing_model": "gpt-5-mini"}
{"claim": "The experiments rely on a CLIP-derived numeric score to measure how well visual–textual pairs align, treating that alignment as the semantic closeness of the pairs to the intended domain.", "label": "Supported", "paragraph": "In terms of the quantitative evaluation, we focus on these evaluation metrics in our experiments: (1) CLIP-Score (Nitzan et al., 2023; Radford et al., 2021) measures the compatibility of image-text pairs, which can be thought of as the semantic similarity with the target domain.", "section_name": "4.1 DATASETS AND METRICS", "subsection_name": "", "paper_id": "FE2e8664Sl", "paper_title": "Few-shot Hybrid Domain Adaptation of Image Generator", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:05:48.928626", "model": "gpt-5-mini", "original_claim": "The experiments employ CLIP-Score as a quantitative evaluation metric to evaluate image–text pair compatibility, treating that compatibility as the semantic similarity between the pairs and the target domain.", "rephrasing_timestamp": "2026-01-11T19:13:33.685252", "rephrasing_model": "gpt-5-mini"}
{"claim": "Through a thorough set of experiments, the researchers show that their CQ-based compression method effectively reduces the storage footprint of key–value caches.", "label": "Supported", "paragraph": "In this section, we perform extensive experiments to validate the effectiveness of our proposed CQ approach for KV cache compression. We first introduce the experimental setups including hardware, software, datasets, metrics, and baselines used.", "section_name": "4 Experiments", "subsection_name": "", "paper_id": "pNnvzQsS4P", "paper_title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:06:19.260463", "model": "gpt-5-mini", "original_claim": "The authors conduct extensive experiments aimed at validating the effectiveness of their proposed CQ technique for compressing KV caches.", "rephrasing_timestamp": "2026-01-11T19:13:35.996418", "rephrasing_model": "gpt-5-mini"}
{"claim": "Initially, the manuscript outlines the testbed, listing the computing equipment and application stack, the data collections employed, the measures used to assess outcomes, and the comparator techniques against which results were benchmarked.", "label": "Supported", "paragraph": "In this section, we perform extensive experiments to validate the effectiveness of our proposed CQ approach for KV cache compression. We first introduce the experimental setups including hardware, software, datasets, metrics, and baselines used.", "section_name": "4 Experiments", "subsection_name": "", "paper_id": "pNnvzQsS4P", "paper_title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:06:19.260474", "model": "gpt-5-mini", "original_claim": "The paper first describes the experimental configuration, detailing the hardware and software, datasets, evaluation metrics, and baseline methods used in the experiments.", "rephrasing_timestamp": "2026-01-11T19:13:39.804612", "rephrasing_model": "gpt-5-mini"}
{"claim": "The authors ran systematic omission experiments—removing or disabling components one at a time—to show each part's impact on and value to the performance of the method they presented.", "label": "Supported", "paragraph": "Then, we present the detailed empirical results and provide discussions. Finally, we perform an ablation study to validate the effectiveness of each component of our proposal. Hardware and Software Experiments are performed on a Linux server equipped with 4 NVIDIA A100 40GB GPUs.", "section_name": "4 Experiments", "subsection_name": "", "paper_id": "pNnvzQsS4P", "paper_title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:06:30.914301", "model": "gpt-5-mini", "original_claim": "The authors conducted an ablation study to validate the contribution and effectiveness of every individual component within their proposed approach.", "rephrasing_timestamp": "2026-01-11T19:13:46.200404", "rephrasing_model": "gpt-5-mini"}
{"claim": "Every test, whether involving physical components or programs, was carried out on a Linux-based host outfitted with four NVIDIA A100 accelerator cards, each supplying 40 GB of dedicated memory.", "label": "Supported", "paragraph": "Then, we present the detailed empirical results and provide discussions. Finally, we perform an ablation study to validate the effectiveness of each component of our proposal. Hardware and Software Experiments are performed on a Linux server equipped with 4 NVIDIA A100 40GB GPUs.", "section_name": "4 Experiments", "subsection_name": "", "paper_id": "pNnvzQsS4P", "paper_title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:06:30.914316", "model": "gpt-5-mini", "original_claim": "All hardware and software experiments were executed on a Linux server outfitted with four NVIDIA A100 GPUs, each providing 40 GB of memory.", "rephrasing_timestamp": "2026-01-11T19:13:48.444869", "rephrasing_model": "gpt-5-mini"}
{"claim": "They examined several techniques for compressing the internal attention-state cache, measuring the impact on five high-capacity language models across a range of standard evaluation corpora.", "label": "Supported", "paragraph": "Our software implementation of CQ is based on PyTorch [29] and the HuggingFace Transformers library [39]. Evaluation Metrics and Datasets We compare different KV cache quantization by evaluating the quality of 5 LLMs on various benchmarks.", "section_name": "4 Experiments", "subsection_name": "", "paper_id": "pNnvzQsS4P", "paper_title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:06:40.774214", "model": "gpt-5-mini", "original_claim": "They compared various KV cache quantization approaches by evaluating the performance of five large language models across multiple benchmark datasets.", "rephrasing_timestamp": "2026-01-11T19:13:47.369681", "rephrasing_model": "gpt-5-mini"}
{"claim": "Several recently introduced training paradigms have proven effective at creating informative encodings of ligand-binding cavities that transfer well to a wide range of subsequent applications.", "label": "Supported", "paragraph": "Recently, several pretraining methods have demonstrated exceptional performance in creating effective representations of protein pockets that can be utilized across a range of downstream tasks. Some methods view the binding site as a part of protein targets and directly pretrain on protein structures(Liu et al., 2023; Wu et al., 2022) or sequences(Karimi et al., 2019).", "section_name": "2.2 POCKET PRETRAINING METHODS", "subsection_name": "", "paper_id": "uMAujpVi9m", "paper_title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:07:08.658113", "model": "gpt-5-mini", "original_claim": "Multiple recent pretraining approaches have achieved strong performance in producing useful protein pocket representations that can be applied to a variety of downstream tasks.", "rephrasing_timestamp": "2026-01-11T19:13:47.393715", "rephrasing_model": "gpt-5-mini"}
{"claim": "Some approaches treat ligand-interaction regions as inherent parts of the protein of interest and begin model training using either three-dimensional protein structure information (Liu et al., 2023; Wu et al., 2022) or the proteins' primary amino-acid sequences (Karimi et al., 2019).", "label": "Supported", "paragraph": "Recently, several pretraining methods have demonstrated exceptional performance in creating effective representations of protein pockets that can be utilized across a range of downstream tasks. Some methods view the binding site as a part of protein targets and directly pretrain on protein structures(Liu et al., 2023; Wu et al., 2022) or sequences(Karimi et al., 2019).", "section_name": "2.2 POCKET PRETRAINING METHODS", "subsection_name": "", "paper_id": "uMAujpVi9m", "paper_title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:07:08.658124", "model": "gpt-5-mini", "original_claim": "Certain techniques regard binding sites as components of protein targets and conduct direct pretraining using protein structural data (Liu et al., 2023; Wu et al., 2022) or sequence data (Karimi et al., 2019).", "rephrasing_timestamp": "2026-01-11T19:13:49.587126", "rephrasing_model": "gpt-5-mini"}
{"claim": "Uni-Mol is a general-purpose three-dimensional molecular self-supervision system that incorporates a bespoke component aimed at ligand-binding cavities to yield richer molecular encodings.", "label": "Supported", "paragraph": "Other methods operate on explicit pockets isolated from a target to make the model more focused. One such method is Uni-Mol (Zhou et al., 2023), which offers a universal 3D molecular pretraining framework and involves a pocket model that has been pretrained on 3 million pocket data.", "section_name": "2.2 POCKET PRETRAINING METHODS", "subsection_name": "", "paper_id": "uMAujpVi9m", "paper_title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:07:18.386284", "model": "gpt-5-mini", "original_claim": "Uni-Mol implements a universal 3D molecular pretraining framework and includes a specialized model focused on binding pockets to enhance representation learning.", "rephrasing_timestamp": "2026-01-11T19:13:59.388724", "rephrasing_model": "gpt-5-mini"}
{"claim": "A Uni-Mol–based model tailored for ligand-binding cavities underwent initial training on a corpus of three million binding-site examples to boost its performance on cavity-related tasks.", "label": "Supported", "paragraph": "Other methods operate on explicit pockets isolated from a target to make the model more focused. One such method is Uni-Mol (Zhou et al., 2023), which offers a universal 3D molecular pretraining framework and involves a pocket model that has been pretrained on 3 million pocket data.", "section_name": "2.2 POCKET PRETRAINING METHODS", "subsection_name": "", "paper_id": "uMAujpVi9m", "paper_title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:07:18.386300", "model": "gpt-5-mini", "original_claim": "A Uni-Mol pocket-focused model was pretrained on a dataset containing three million pocket instances to improve its pocket-specific performance.", "rephrasing_timestamp": "2026-01-11T19:13:58.970805", "rephrasing_model": "gpt-5-mini"}
{"claim": "The approach proposed by Gao et al. (2022), called CoSP, uses a paired-supervision pretraining framework to simultaneously learn embeddings for protein ligand-binding sites and their matching small-molecule binders.", "label": "Supported", "paragraph": "However, this method lacks a specific focus on the interactions between pockets and ligands. Additionally, CoSP (Gao et al., 2022) employs a co-supervised pretraining framework to learn both pocket and ligand representations simultaneously.", "section_name": "2.2 POCKET PRETRAINING METHODS", "subsection_name": "", "paper_id": "uMAujpVi9m", "paper_title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:07:27.606323", "model": "gpt-5-mini", "original_claim": "The CoSP method (Gao et al., 2022) applies a co-supervised pretraining strategy to jointly learn representations for protein binding pockets and their ligands concurrently.", "rephrasing_timestamp": "2026-01-11T19:13:58.033266", "rephrasing_model": "gpt-5-mini"}
{"claim": "The central claim is proved in two stages. In the first stage we describe a construction that employs limiting processes to create languages while placing no restrictions on the computing resources of the system carrying out the construction.", "label": "Supported", "paragraph": "We prove our main result in two parts. We first give a method for language generation in the limit that is not concerned with the computational power required by the agent performing the generation. Thus, rather than an algorithm to generate the string, we ask whether we can construct a function f C based on the given language collection that maps a finite set of strings to a new string; this function takes the strings S t seen so far and outputs a string f C ( S t ) intended to be in K -S t .", "section_name": "4 Generation in the Limit via a Function", "subsection_name": "", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:07:46.949463", "model": "gpt-5-mini", "original_claim": "The main result is established in two parts; the first part presents a limit-based procedure for generating languages without imposing any limitations on the computational power of the agent performing generation.", "rephrasing_timestamp": "2026-01-11T19:14:03.237585", "rephrasing_model": "gpt-5-mini"}
{"claim": "Rather than giving a string-generator, the authors advocate defining a rule phi_C, built from the available family of languages, that takes the finite sample of strings seen up to time t and outputs a new string meant to be in K but not in that sample.", "label": "Supported", "paragraph": "We prove our main result in two parts. We first give a method for language generation in the limit that is not concerned with the computational power required by the agent performing the generation. Thus, rather than an algorithm to generate the string, we ask whether we can construct a function f C based on the given language collection that maps a finite set of strings to a new string; this function takes the strings S t seen so far and outputs a string f C ( S t ) intended to be in K -S t .", "section_name": "4 Generation in the Limit via a Function", "subsection_name": "", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:07:46.949475", "model": "gpt-5-mini", "original_claim": "Instead of presenting a generation algorithm, they propose defining a function f_C, derived from the given language collection, that maps the finite observed string set S_t to a new string intended to lie in K minus S_t.", "rephrasing_timestamp": "2026-01-11T19:14:04.721398", "rephrasing_model": "gpt-5-mini"}
{"claim": "For any countable collection C of languages, one can define a map φ_C from the finite subsets of U to U that picks, for each finite subset of U, a single element of U.", "label": "Supported", "paragraph": "We will prove the following:\n(4.1) For every countable collection of languages C , there is a function f C from finite subsets of U to elements of U , such that for every enumeration of a language K ∈ C , there is a t ∗ such that for all t ≥ t ∗ , we have f C ( S t ) ∈ K -S t .", "section_name": "4 Generation in the Limit via a Function", "subsection_name": "", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:07:57.006920", "model": "gpt-5-mini", "original_claim": "For any countable family of languages C there exists a function f_C that assigns to each finite subset of U a particular element of U.", "rephrasing_timestamp": "2026-01-11T19:14:17.632241", "rephrasing_model": "gpt-5-mini"}
{"claim": "For any listing of a set K drawn from class C, there is a threshold stage T0 such that for all stages t ≥ T0 the element produced by f_C on S_t lies in K yet is not present in S_t.", "label": "Supported", "paragraph": "We will prove the following:\n(4.1) For every countable collection of languages C , there is a function f C from finite subsets of U to elements of U , such that for every enumeration of a language K ∈ C , there is a t ∗ such that for all t ≥ t ∗ , we have f C ( S t ) ∈ K -S t .", "section_name": "4 Generation in the Limit via a Function", "subsection_name": "", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:07:57.006941", "model": "gpt-5-mini", "original_claim": "Given any enumeration of a language K in C, there is a time t* after which for every t ≥ t* the value f_C(S_t) always belongs to K but not to S_t.", "rephrasing_timestamp": "2026-01-11T19:14:16.181343", "rephrasing_model": "gpt-5-mini"}
{"claim": "Although the favorable result does not consider the computational cost of constructing the mapping tied to C, it makes a key point: languages remain unidentifiable even if the sole requirement is to output that mapping.", "label": "Supported", "paragraph": "Note that while this positive result is not concerned with the computational power required to evaluate f C , it already contains the core contrast with language identification, which remains impossible even if we simply ask for a function f C .", "section_name": "4 Generation in the Limit via a Function", "subsection_name": "", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:08:05.879012", "model": "gpt-5-mini", "original_claim": "The positive finding, despite not addressing the computational resources needed to compute f_C, establishes a central contrast: identifying languages remains impossible even when the requirement is merely to produce a function f_C.", "rephrasing_timestamp": "2026-01-11T19:14:12.845493", "rephrasing_model": "gpt-5-mini"}
{"claim": "The goal presented in Section 3.1 is independent of the input data: it assigns equal importance to every element of W when rebuilding the matrix during factorization.", "label": "Supported", "paragraph": "The decomposition objective considered in §3.1 is data-agnostic insofar as it treats each entry of W as equally important for reconstruction during factorization. Following recent works which demon-\n6 Here we overload c to refer to both its tuple representation c ∈ C and its index representation c ∈ [ |C| ] .", "section_name": "3.3 DATA-AWARE MATRIX DECOMPOSITION VIA FISHER-WEIGHTED SVD", "subsection_name": "", "paper_id": "xw29VvOMmU", "paper_title": "LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:08:25.373757", "model": "gpt-5-mini", "original_claim": "The decomposition objective described in Section 3.1 is data-agnostic, treating every entry of the matrix W with equal importance when reconstructing it during the factorization process.", "rephrasing_timestamp": "2026-01-11T19:14:18.316325", "rephrasing_model": "gpt-5-mini"}
{"claim": "The symbol c is reused to denote both an element of C (seen as a sequence) and the integer that identifies its position among the elements, i.e., a value in {1, …, |C|}.", "label": "Supported", "paragraph": "The decomposition objective considered in §3.1 is data-agnostic insofar as it treats each entry of W as equally important for reconstruction during factorization. Following recent works which demon-\n6 Here we overload c to refer to both its tuple representation c ∈ C and its index representation c ∈ [ |C| ] .", "section_name": "3.3 DATA-AWARE MATRIX DECOMPOSITION VIA FISHER-WEIGHTED SVD", "subsection_name": "", "paper_id": "xw29VvOMmU", "paper_title": "LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:08:25.373769", "model": "gpt-5-mini", "original_claim": "The notation c is overloaded to represent both its tuple form in the set C and its index form in the index set [|C|].", "rephrasing_timestamp": "2026-01-11T19:14:15.047586", "rephrasing_model": "gpt-5-mini"}
{"claim": "Several papers (Frantar et al., 2022; Lin et al., 2023; Kim et al., 2023b) report that using a small, representative tuning set is essential for successfully converting transformer-based language models to lower-precision formats.", "label": "Supported", "paragraph": "7 https://www.gurobi.com/\n8 https://github.com/TimDettmers/bitsandbytes\nstrate the importance of using calibration data for quantizating LLMs (Frantar et al., 2022; Lin et al., 2023; Kim et al., 2023b), we next consider a data-aware version of the approach by using a diagonal approximation of the Fisher information matrix to weight the reconstruction objective.", "section_name": "3.3 DATA-AWARE MATRIX DECOMPOSITION VIA FISHER-WEIGHTED SVD", "subsection_name": "", "paper_id": "xw29VvOMmU", "paper_title": "LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:08:40.517173", "model": "gpt-5-mini", "original_claim": "Multiple cited studies (Frantar et al., 2022; Lin et al., 2023; Kim et al., 2023b) indicate that employing calibration data is important for quantizing large language models.", "rephrasing_timestamp": "2026-01-11T19:14:20.545216", "rephrasing_model": "gpt-5-mini"}
{"claim": "They propose a data-dependent method that approximates the model’s information matrix with only its diagonal and uses that approximation to scale the reconstruction loss during quantization.", "label": "Supported", "paragraph": "7 https://www.gurobi.com/\n8 https://github.com/TimDettmers/bitsandbytes\nstrate the importance of using calibration data for quantizating LLMs (Frantar et al., 2022; Lin et al., 2023; Kim et al., 2023b), we next consider a data-aware version of the approach by using a diagonal approximation of the Fisher information matrix to weight the reconstruction objective.", "section_name": "3.3 DATA-AWARE MATRIX DECOMPOSITION VIA FISHER-WEIGHTED SVD", "subsection_name": "", "paper_id": "xw29VvOMmU", "paper_title": "LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:08:40.517189", "model": "gpt-5-mini", "original_claim": "The paper introduces a data-aware variant that applies a diagonal approximation of the Fisher information matrix to weight the reconstruction objective during quantization.", "rephrasing_timestamp": "2026-01-11T19:14:25.861846", "rephrasing_model": "gpt-5-mini"}
{"claim": "Entries along the main diagonal of the sample-based Fisher information matrix serve as per-parameter measures of how strongly each weight affects the model’s predictions, and these measures have been used to improve low-rank approximation and parameter quantization methods for pretrained language models (Hsu et al., 2022; Kim et al., 2023b).", "label": "Supported", "paragraph": "The (diagonal of the) empirical Fisher information matrix measures how sensitive the model's output is to a perturbation of each parameter, and has previously been exploited to improve low-rank compression (Hsu et al., 2022) and quantization (Kim et al., 2023b) of pretrained language models.", "section_name": "3.3 DATA-AWARE MATRIX DECOMPOSITION VIA FISHER-WEIGHTED SVD", "subsection_name": "", "paper_id": "xw29VvOMmU", "paper_title": "LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:08:47.728802", "model": "gpt-5-mini", "original_claim": "The diagonal of the empirical Fisher information matrix quantifies each parameter's effect on model outputs and has been used to enhance low-rank compression and quantization of pretrained language models (Hsu et al., 2022; Kim et al., 2023b).", "rephrasing_timestamp": "2026-01-11T19:14:27.980005", "rephrasing_model": "gpt-5-mini"}
{"claim": "The paper posits that in cases where the predictive function f has no attributes in common with a given input x, its decision is effectively equivalent to making a choice at random.", "label": "Supported", "paragraph": "Another assumption we made is that when the model f does not share any feature with a data point x , the model will make a random guess. At first look, this seems like a strong assumption that requires the model to make a perfectly random guess.", "section_name": "D.2 SOURCES OF RANDOMNESS", "subsection_name": "", "paper_id": "ze7DOLi394", "paper_title": "On the Joint Interaction of Models, Data, and Features", "paper_venue": "iclr2024", "paper_decision": "Accept (oral)", "decision": "Oral", "extraction_timestamp": "2026-01-11T15:09:16.707430", "model": "gpt-5-mini", "original_claim": "The authors assume that whenever model f shares no features with a data point x, the model's prediction behaves as a random guess.", "rephrasing_timestamp": "2026-01-11T19:14:27.822304", "rephrasing_model": "gpt-5-mini"}
{"claim": "At first glance the researchers judge the assumption to be overly restrictive, because it would require the model to emit completely random outputs whenever there are no shared characteristics.", "label": "Supported", "paragraph": "Another assumption we made is that when the model f does not share any feature with a data point x , the model will make a random guess. At first look, this seems like a strong assumption that requires the model to make a perfectly random guess.", "section_name": "D.2 SOURCES OF RANDOMNESS", "subsection_name": "", "paper_id": "ze7DOLi394", "paper_title": "On the Joint Interaction of Models, Data, and Features", "paper_venue": "iclr2024", "paper_decision": "Accept (oral)", "decision": "Oral", "extraction_timestamp": "2026-01-11T15:09:16.707443", "model": "gpt-5-mini", "original_claim": "Upon initial consideration, the authors view this assumption as strong because it would compel the model to produce perfectly random outputs when no features are shared.", "rephrasing_timestamp": "2026-01-11T19:14:23.852696", "rephrasing_model": "gpt-5-mini"}
{"claim": "Rather than being evaluated on one particular sample, the quantity is averaged over the model's uncertainty and the full range of observations produced by the data-generating process.", "label": "Supported", "paragraph": "However, recall that we are computing the expectation over the model distribution and the data distribution rather than a single fixed data point. For a single model f , its prediction is effectively random if its average prediction over all the distribution of data that do not share features with f is at the chance:\n<!-- formula-not-decoded -->\n✶ This means that f can be completely deterministic as long as its accuracy over all the data that it doesn't share feature with is random chance.", "section_name": "D.2 SOURCES OF RANDOMNESS", "subsection_name": "", "paper_id": "ze7DOLi394", "paper_title": "On the Joint Interaction of Models, Data, and Features", "paper_venue": "iclr2024", "paper_decision": "Accept (oral)", "decision": "Oral", "extraction_timestamp": "2026-01-11T15:09:23.065571", "model": "gpt-5-mini", "original_claim": "The expectation is taken over both the model distribution and the data distribution, instead of being computed for a single fixed data point.", "rephrasing_timestamp": "2026-01-11T19:14:29.345471", "rephrasing_model": "gpt-5-mini"}
{"claim": "A fixed-function predictor f can still generate outputs that look random whenever, on inputs that do not contain the attributes f uses, its expected success rate equals that of random guessing.", "label": "Supported", "paragraph": "However, recall that we are computing the expectation over the model distribution and the data distribution rather than a single fixed data point. For a single model f , its prediction is effectively random if its average prediction over all the distribution of data that do not share features with f is at the chance:\n<!-- formula-not-decoded -->\n✶ This means that f can be completely deterministic as long as its accuracy over all the data that it doesn't share feature with is random chance.", "section_name": "D.2 SOURCES OF RANDOMNESS", "subsection_name": "", "paper_id": "ze7DOLi394", "paper_title": "On the Joint Interaction of Models, Data, and Features", "paper_venue": "iclr2024", "paper_decision": "Accept (oral)", "decision": "Oral", "extraction_timestamp": "2026-01-11T15:09:23.065592", "model": "gpt-5-mini", "original_claim": "A deterministic model f can nonetheless yield effectively random predictions if its average accuracy across all data that lack f's features is at chance level.", "rephrasing_timestamp": "2026-01-11T19:14:33.033626", "rephrasing_model": "gpt-5-mini"}
{"claim": "If you keep one example (x,y) constant, any randomness left over originates solely from which model f the training process produces when drawn from the algorithm's distribution F_A — even though each resulting model may itself be a deterministic function.", "label": "Supported", "paragraph": "This is in fact the only sensible outcome if we assume that features are indeed what the models use to make predictions. In this case, the source of randomness comes from the data , ( x , y ) ∼ D . We now analyze the case where we hold a single data point ( x, y ) fixed and generate the source of randomness from the training algorithm f ∼ F A (once again, the individual model can be completely deterministic).", "section_name": "D.2 SOURCES OF RANDOMNESS", "subsection_name": "", "paper_id": "ze7DOLi394", "paper_title": "On the Joint Interaction of Models, Data, and Features", "paper_venue": "iclr2024", "paper_decision": "Accept (oral)", "decision": "Oral", "extraction_timestamp": "2026-01-11T15:09:32.496363", "model": "gpt-5-mini", "original_claim": "When a single data example (x,y) is held fixed, the only source of randomness is sampling the training algorithm f from the distribution F_A, even though individual learned models may be deterministic.", "rephrasing_timestamp": "2026-01-11T19:14:36.060182", "rephrasing_model": "gpt-5-mini"}
{"claim": "The DUSA method condenses a procedure that would ordinarily run through numerous denoising iterations into a single pass, thereby substituting the usual multi-iteration schedule with one model update.", "label": "Supported", "paragraph": "Selection of timestep t . As discussed in Sec. 3.2, our DUSA significantly reduces the number of timesteps to a single timestep of diffusion models. Fig. 3 illustrates the influence of timestep selection on DUSA through adapting the ConvNeXt-L classifier to corruptions from the four main categories.", "section_name": "4.4 Ablation Study", "subsection_name": "", "paper_id": "c7m1HahBNf", "paper_title": "Exploring Structured Semantic Priors Underlying Diffusion Score for Test-time Adaptation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:09:56.834091", "model": "gpt-5-mini", "original_claim": "The DUSA approach compresses the diffusion process from multiple steps into a single timestep, effectively replacing the original multi-step diffusion schedule with one timestep for the model.", "rephrasing_timestamp": "2026-01-11T19:14:37.366679", "rephrasing_model": "gpt-5-mini"}
{"claim": "The third panel illustrates how changing the chosen timesteps alters DUSA’s behavior, by fine-tuning a ConvNeXt-L network on images corrupted across the four principal corruption groups.", "label": "Supported", "paragraph": "Selection of timestep t . As discussed in Sec. 3.2, our DUSA significantly reduces the number of timesteps to a single timestep of diffusion models. Fig. 3 illustrates the influence of timestep selection on DUSA through adapting the ConvNeXt-L classifier to corruptions from the four main categories.", "section_name": "4.4 Ablation Study", "subsection_name": "", "paper_id": "c7m1HahBNf", "paper_title": "Exploring Structured Semantic Priors Underlying Diffusion Score for Test-time Adaptation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:09:56.834103", "model": "gpt-5-mini", "original_claim": "Figure 3 demonstrates the effect of timestep selection on DUSA by adapting a ConvNeXt-L classifier to image corruptions sampled from the four primary corruption categories.", "rephrasing_timestamp": "2026-01-11T19:14:38.287102", "rephrasing_model": "gpt-5-mini"}
{"claim": "Their examination in Section 3.3 finds that the corrective cues produced by the generative process degrade substantially at the extremes of the time axis — both very close to the start and very close to the finish.", "label": "Supported", "paragraph": "Consistent with our analysis in Sec. 3.3, the guidance from diffusion models is far from perfect when the chosen timestep is either too small ( t → 0 ) or too large ( t → T ). We empirically find that t = 100 shows a good performance here, and generalizes well to other backbones and tasks as well.", "section_name": "4.4 Ablation Study", "subsection_name": "", "paper_id": "c7m1HahBNf", "paper_title": "Exploring Structured Semantic Priors Underlying Diffusion Score for Test-time Adaptation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:10:08.877000", "model": "gpt-5-mini", "original_claim": "The guidance provided by the diffusion models is notably imperfect when the selected timestep is extremely low (t→0) or extremely high (t→T), according to the authors' analysis in Section 3.3.", "rephrasing_timestamp": "2026-01-11T19:14:41.346638", "rephrasing_model": "gpt-5-mini"}
{"claim": "Experimental trials showed that using a time-step setting of 100 achieved strong results and remained effective when applied to varied model architectures and multiple task types.", "label": "Supported", "paragraph": "Consistent with our analysis in Sec. 3.3, the guidance from diffusion models is far from perfect when the chosen timestep is either too small ( t → 0 ) or too large ( t → T ). We empirically find that t = 100 shows a good performance here, and generalizes well to other backbones and tasks as well.", "section_name": "4.4 Ablation Study", "subsection_name": "", "paper_id": "c7m1HahBNf", "paper_title": "Exploring Structured Semantic Priors Underlying Diffusion Score for Test-time Adaptation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:10:08.877016", "model": "gpt-5-mini", "original_claim": "Empirically, selecting a timestep of t=100 produced good performance and generalized well across different model backbones and across multiple tasks.", "rephrasing_timestamp": "2026-01-11T19:14:41.791172", "rephrasing_model": "gpt-5-mini"}
{"claim": "To keep the setup straightforward, the research team fixed the temporal increment at 100 and used that same setting for every experiment described in the article.", "label": "Supported", "paragraph": "The other timesteps, e.g., t = 50 , however also emerge as strong contenders and outperform Diffusion-TTA by a considerable margin. For simplicity, we adopt t = 100 in all our experiments. Figure 3: Accuracy of ConvNeXt-L across different selections of timestep.", "section_name": "4.4 Ablation Study", "subsection_name": "", "paper_id": "c7m1HahBNf", "paper_title": "Exploring Structured Semantic Priors Underlying Diffusion Score for Test-time Adaptation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:10:13.155048", "model": "gpt-5-mini", "original_claim": "For simplicity, the authors selected timestep t = 100 and employed this specific timestep value consistently across all experiments reported in the paper.", "rephrasing_timestamp": "2026-01-11T19:14:44.128931", "rephrasing_model": "gpt-5-mini"}
{"claim": "The table reports numerical scores for four out of the five assessed metrics, contrasting the unaltered approaches with versions enhanced by the proposed physics-aware compatibility optimization.", "label": "Supported", "paragraph": "Table 1 shows the quantitative results for four out of five metrics evaluated for both baselines and those integrated with our physical compatibility optimization. Fig. 3 shows the curve of fracture rate.", "section_name": "4.2 Quantitative Results", "subsection_name": "", "paper_id": "k29Iv0XrBF", "paper_title": "Physically Compatible 3D Object Modeling from a Single Image", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:10:33.435514", "model": "gpt-5-mini", "original_claim": "Table 1 presents quantitative results for four of the five evaluated metrics, comparing baseline methods and versions augmented with the authors' physical compatibility optimization.", "rephrasing_timestamp": "2026-01-11T19:14:47.554854", "rephrasing_model": "gpt-5-mini"}
{"claim": "Panel three presents a line graph illustrating the frequency of fractures documented in the paper.", "label": "Supported", "paragraph": "Table 1 shows the quantitative results for four out of five metrics evaluated for both baselines and those integrated with our physical compatibility optimization. Fig. 3 shows the curve of fracture rate.", "section_name": "4.2 Quantitative Results", "subsection_name": "", "paper_id": "k29Iv0XrBF", "paper_title": "Physically Compatible 3D Object Modeling from a Single Image", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:10:33.435536", "model": "gpt-5-mini", "original_claim": "Figure 3 displays the fracture-rate curve, providing a visual plot of the fracture rate reported in the study.", "rephrasing_timestamp": "2026-01-11T19:14:45.899273", "rephrasing_model": "gpt-5-mini"}
{"claim": "A numerical evaluation showed that the method used to encode geometry strongly affects the cohesion of rebuilt models, as evidenced by changes in the observed number of disjoint parts.", "label": "Supported", "paragraph": "Our quantitative analysis yields several observations: 1) The underlying geometry representation significantly impacts the structural integrity of reconstructed shapes, as evidenced by the number of connected components ( # CC.).", "section_name": "4.2 Quantitative Results", "subsection_name": "", "paper_id": "k29Iv0XrBF", "paper_title": "Physically Compatible 3D Object Modeling from a Single Image", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:10:44.460791", "model": "gpt-5-mini", "original_claim": "The quantitative analysis found that the choice of geometry representation has a significant effect on the structural integrity of reconstructed shapes, indicated by variations in the measured number of connected components.", "rephrasing_timestamp": "2026-01-11T19:14:53.339855", "rephrasing_model": "gpt-5-mini"}
{"claim": "The researchers used the tally of separate parts as a numeric indicator to show how different methods of representing geometry influence the robustness of rebuilt objects.", "label": "Supported", "paragraph": "Our quantitative analysis yields several observations: 1) The underlying geometry representation significantly impacts the structural integrity of reconstructed shapes, as evidenced by the number of connected components ( # CC.).", "section_name": "4.2 Quantitative Results", "subsection_name": "", "paper_id": "k29Iv0XrBF", "paper_title": "Physically Compatible 3D Object Modeling from a Single Image", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:10:44.460809", "model": "gpt-5-mini", "original_claim": "The study used the number of connected components as a quantitative metric to demonstrate how different geometry representations affect the structural integrity of reconstructed shapes.", "rephrasing_timestamp": "2026-01-11T19:14:50.386924", "rephrasing_model": "gpt-5-mini"}
{"claim": "When shapes are represented as point sets, the LGM method produces the least robust results, often creating detached, hovering parts because it cannot infer volumetric occupancy — it fails to tell which regions belong to the solid versus the surrounding void.", "label": "Supported", "paragraph": "LGM, using a point cloud representation, exhibits the poorest structural integrity, often resulting in floating structures due to its inability to differentiate the interior from the exterior of a 3D object.", "section_name": "4.2 Quantitative Results", "subsection_name": "", "paper_id": "k29Iv0XrBF", "paper_title": "Physically Compatible 3D Object Modeling from a Single Image", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:10:50.221979", "model": "gpt-5-mini", "original_claim": "When using a point-cloud representation, LGM produces the weakest structural integrity, frequently generating floating components because it cannot distinguish an object's interior from its exterior.", "rephrasing_timestamp": "2026-01-11T19:14:57.254795", "rephrasing_model": "gpt-5-mini"}
{"claim": "NAEPro relies on a set of six NAEL subcomponents embedded in its network layout to capture feature representations and carry out the model's computations.", "label": "Supported", "paragraph": "Implementation Details We use 6 NAELs in NAEPro. The global attention sub-layer parameters are initialized with released ESM-2 (Lin et al., 2022) parameters (esm2\\_t6\\_8M\\_UR50D). The hyperparameter λ/ 2 and k are respectively set to 1 .", "section_name": "4.2 EXPERIMENTAL DETAILS", "subsection_name": "", "paper_id": "Dr4qD9bzZd", "paper_title": "Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:11:23.905926", "model": "gpt-5-mini", "original_claim": "The NAEPro implementation uses six NAEL layers incorporated into its architecture to perform representation learning and model computation.", "rephrasing_timestamp": "2026-01-11T19:14:55.497924", "rephrasing_model": "gpt-5-mini"}
{"claim": "NAEPro was initialized with specific parameter choices: both the quantity equal to half of λ and the parameter k were given the value 1.", "label": "Supported", "paragraph": "Implementation Details We use 6 NAELs in NAEPro. The global attention sub-layer parameters are initialized with released ESM-2 (Lin et al., 2022) parameters (esm2\\_t6\\_8M\\_UR50D). The hyperparameter λ/ 2 and k are respectively set to 1 .", "section_name": "4.2 EXPERIMENTAL DETAILS", "subsection_name": "", "paper_id": "Dr4qD9bzZd", "paper_title": "Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:11:23.905945", "model": "gpt-5-mini", "original_claim": "During setup, the hyperparameters λ/2 and k were each assigned the value 1 in the NAEPro configuration.", "rephrasing_timestamp": "2026-01-11T19:15:01.310010", "rephrasing_model": "gpt-5-mini"}
{"claim": "Across the study, parameter updates were performed on groups of eight samples, using an update magnitude fixed at 5×10^4.", "label": "Supported", "paragraph": "0 and 30 . The mini-batch size and learning rate are set to 8 and 5 e4 respectively. The model is trained for 100 epochs with 1 NVIDIA RTX A 6000 GPU card. The sequences are decoded using greedy decoding strategy.", "section_name": "4.2 EXPERIMENTAL DETAILS", "subsection_name": "", "paper_id": "Dr4qD9bzZd", "paper_title": "Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:11:50.265137", "model": "gpt-5-mini", "original_claim": "Training used a mini-batch of size eight and a learning-rate value set to 5 e4 for model optimization during the experiments.", "rephrasing_timestamp": "2026-01-11T19:14:59.519555", "rephrasing_model": "gpt-5-mini"}
{"claim": "When producing results and measuring performance, generated sequences were formed by repeatedly choosing the single most likely next element from the network's output scores.", "label": "Supported", "paragraph": "0 and 30 . The mini-batch size and learning rate are set to 8 and 5 e4 respectively. The model is trained for 100 epochs with 1 NVIDIA RTX A 6000 GPU card. The sequences are decoded using greedy decoding strategy.", "section_name": "4.2 EXPERIMENTAL DETAILS", "subsection_name": "", "paper_id": "Dr4qD9bzZd", "paper_title": "Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:11:50.265156", "model": "gpt-5-mini", "original_claim": "During inference and evaluation, output sequences were produced by applying a greedy decoding approach to the model's predictions.", "rephrasing_timestamp": "2026-01-11T19:15:03.194560", "rephrasing_model": "gpt-5-mini"}
{"claim": "For the first ten training cycles they apply a gradual reduction schedule: at cycle i they randomly designate a fraction of residues as synthetic fragments given by f_i = 0.85*(10 - i)/i. Beginning with cycle eleven they replace those synthetic fragments with actual fragments.", "label": "Supported", "paragraph": "To make training easier, we employ an annealing training strategy for the first 10 epochs, which randomly sample ( 85% * ( 10 -epoch) / epoch) residues as pseudo fragments and use real fragments after 10 epochs.", "section_name": "4.2 EXPERIMENTAL DETAILS", "subsection_name": "", "paper_id": "Dr4qD9bzZd", "paper_title": "Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:11:58.261257", "model": "gpt-5-mini", "original_claim": "During the first ten epochs they use an annealing procedure that in each epoch randomly marks a fraction of residues equal to 0.85*(10 - epoch)/epoch as pseudo-fragments, and after epoch ten they switch to real fragments.", "rephrasing_timestamp": "2026-01-11T19:15:10.377719", "rephrasing_model": "gpt-5-mini"}
{"claim": "Write p(x) as A x^2 + B x + C; substituting x = 0 and using that the polynomial equals 1 at zero implies the constant coefficient C is 1.", "label": "Supported", "paragraph": "Question: Find the quadratic polynomial p ( x ) such that p ( - 3) = 10 , p (0) = 1 , and p (2) = 5 . Solution: Let p ( x ) = ax 2 + bx + c. Then from the given information,\n<!-- formula-not-decoded -->\nThen 9 a -3 b = 9 and 4 a +2 b = 4 , which reduce to 3 a -b = 3 and 2 a + b = 2 .", "section_name": "CoT Prompt for MATH-Intermediate Algebra", "subsection_name": "", "paper_id": "UkxJd64mki", "paper_title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:12:26.234654", "model": "gpt-5-mini", "original_claim": "Representing p(x) as ax^2 + bx + c and applying the condition p(0) = 1 determines the constant term c to be 1.", "rephrasing_timestamp": "2026-01-11T19:15:06.969112", "rephrasing_model": "gpt-5-mini"}
{"claim": "Setting c = 1 and requiring that p(-3) = 10 and p(2) = 5 gives the two equations 9a − 3b = 9 and 4a + 2b = 4, which simplify to 3a − b = 3 and 2a + b = 2.", "label": "Supported", "paragraph": "Question: Find the quadratic polynomial p ( x ) such that p ( - 3) = 10 , p (0) = 1 , and p (2) = 5 . Solution: Let p ( x ) = ax 2 + bx + c. Then from the given information,\n<!-- formula-not-decoded -->\nThen 9 a -3 b = 9 and 4 a +2 b = 4 , which reduce to 3 a -b = 3 and 2 a + b = 2 .", "section_name": "CoT Prompt for MATH-Intermediate Algebra", "subsection_name": "", "paper_id": "UkxJd64mki", "paper_title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:12:26.234673", "model": "gpt-5-mini", "original_claim": "After substituting c = 1, the conditions p(-3) = 10 and p(2) = 5 yield the linear system 9a - 3b = 9 and 4a + 2b = 4, equivalently 3a - b = 3 and 2a + b = 2.", "rephrasing_timestamp": "2026-01-11T19:15:09.167020", "rephrasing_model": "gpt-5-mini"}
{"claim": "Evaluating the constraints shows that a equals 1 and b equals 0, so the resulting polynomial is p(x) = x^2 + 1.", "label": "Supported", "paragraph": "Adding, we get 5 a = 5 , so a = 1 . Then 4 + 2 b = 4 , so b = 0 . Therefore, p ( x ) = x 2 +1 . Answer:\nThe answer is x 2 +1\nQuestion: Calculate:\n5 3 × 6 10 × 15 9 × 12 20 × 25 15 × 18 30 × 35 21 × 24 40\nSolution: Each of the fractions 5 3 , 15 9 , 25 15 , 35 21 reduce to 5 3 , and each of the fractions 6 10 , 12 20 , 18 30 , 24 40 3\nreduce to 5 .", "section_name": "CoT Prompt for MATH-Intermediate Algebra", "subsection_name": "", "paper_id": "UkxJd64mki", "paper_title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:12:50.749205", "model": "gpt-5-mini", "original_claim": "Solving the given equations yields coefficients a = 1 and b = 0, producing the polynomial p(x) = x^2 + 1.", "rephrasing_timestamp": "2026-01-11T19:15:11.358961", "rephrasing_model": "gpt-5-mini"}
{"claim": "After canceling common factors, 5/3, 15/9, 25/15, and 35/21 all represent the same simplest fraction, 5/3.", "label": "Supported", "paragraph": "Adding, we get 5 a = 5 , so a = 1 . Then 4 + 2 b = 4 , so b = 0 . Therefore, p ( x ) = x 2 +1 . Answer:\nThe answer is x 2 +1\nQuestion: Calculate:\n5 3 × 6 10 × 15 9 × 12 20 × 25 15 × 18 30 × 35 21 × 24 40\nSolution: Each of the fractions 5 3 , 15 9 , 25 15 , 35 21 reduce to 5 3 , and each of the fractions 6 10 , 12 20 , 18 30 , 24 40 3\nreduce to 5 .", "section_name": "CoT Prompt for MATH-Intermediate Algebra", "subsection_name": "", "paper_id": "UkxJd64mki", "paper_title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:12:50.749229", "model": "gpt-5-mini", "original_claim": "In the calculation, the four fractions 5/3, 15/9, 25/15, and 35/21 each simplify to the identical reduced value 5/3.", "rephrasing_timestamp": "2026-01-11T19:15:19.673786", "rephrasing_model": "gpt-5-mini"}
{"claim": "For every x with -25 ≤ x ≤ 25, the expression sqrt(25 + x) + sqrt(25 − x) does not exceed 10, and 10 is its largest attainable value.", "label": "Supported", "paragraph": "Therefore, the product of all eight fractions is 1. Answer:\nThe answer is 1\nQuestion: For -25 ≤ x ≤ 25 , find the maximum value of √ 25 + x + √ 25 -x. Solution: By QM-AM,\n<!-- formula-not-decoded -->\nso √ 25 + x + √ 25 -x ≤ 10 .", "section_name": "CoT Prompt for MATH-Intermediate Algebra", "subsection_name": "", "paper_id": "UkxJd64mki", "paper_title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:12:59.508831", "model": "gpt-5-mini", "original_claim": "When x ranges from -25 to 25 inclusive, the value of sqrt(25+x) + sqrt(25-x) is never greater than 10, reaching its maximum value of 10.", "rephrasing_timestamp": "2026-01-11T19:15:22.440678", "rephrasing_model": "gpt-5-mini"}
{"claim": "By applying their improvements, the researchers claim to have achieved results on ImageNet and COCO that are comparable to those produced by the reference diffusion networks.", "label": "Supported", "paragraph": "With the proposed improvements, we are able to match the performance of teacher diffusion models on ImageNet and COCO (see Tab. 1 and Tab. 5). However, we found that larger scale models like SDXL [58] remain challenging to distill into a one-step generator because of limited model capacity and a complex optimization landscape to learn the direct mapping from noise to highly diverse and detailed images.", "section_name": "4.4 Multi-step generator", "subsection_name": "", "paper_id": "tQukGCDaNT", "paper_title": "Improved Distribution Matching Distillation for Fast Image Synthesis", "paper_venue": "neurips2024", "paper_decision": "Accept (oral)", "decision": "Oral", "extraction_timestamp": "2026-01-11T15:13:24.819724", "model": "gpt-5-mini", "original_claim": "Using the proposed improvements, the authors report matching the performance of teacher diffusion models on the ImageNet and COCO benchmarks.", "rephrasing_timestamp": "2026-01-11T19:15:32.091268", "rephrasing_model": "gpt-5-mini"}
{"claim": "Compressing very large generative networks such as SDXL into a single-pass model is challenging: smaller networks lack the representational power to reproduce the original, and training requires navigating a highly rugged objective to learn how to turn noisy inputs into richly detailed images.", "label": "Supported", "paragraph": "With the proposed improvements, we are able to match the performance of teacher diffusion models on ImageNet and COCO (see Tab. 1 and Tab. 5). However, we found that larger scale models like SDXL [58] remain challenging to distill into a one-step generator because of limited model capacity and a complex optimization landscape to learn the direct mapping from noise to highly diverse and detailed images.", "section_name": "4.4 Multi-step generator", "subsection_name": "", "paper_id": "tQukGCDaNT", "paper_title": "Improved Distribution Matching Distillation for Fast Image Synthesis", "paper_venue": "neurips2024", "paper_decision": "Accept (oral)", "decision": "Oral", "extraction_timestamp": "2026-01-11T15:13:24.819736", "model": "gpt-5-mini", "original_claim": "Large-scale models like SDXL are difficult to distill into a single-step generator because of limited model capacity and a complex optimization landscape for learning the noise-to-detailed-image mapping.", "rephrasing_timestamp": "2026-01-11T19:15:17.390954", "rephrasing_model": "gpt-5-mini"}
{"claim": "They modified DMD to sample across multiple future horizons by using a fixed, chosen-in-advance list of N time instants t1 through tN, and that identical list is used both when fitting the model and when generating predictions.", "label": "Supported", "paragraph": "This motivated us to extend DMD to support multi-step sampling. We fix a predetermined schedule with N timestep { t 1 , t 2 , . . . t N } , identical during training and inference. During inference, at each step, we alternate between denoising and noise injection steps, following the consistency model [9], to improve sample quality.", "section_name": "4.4 Multi-step generator", "subsection_name": "", "paper_id": "tQukGCDaNT", "paper_title": "Improved Distribution Matching Distillation for Fast Image Synthesis", "paper_venue": "neurips2024", "paper_decision": "Accept (oral)", "decision": "Oral", "extraction_timestamp": "2026-01-11T15:13:34.418850", "model": "gpt-5-mini", "original_claim": "They extended DMD to perform multi-step sampling using a fixed, preselected sequence of N timepoints {t1,...,tN} that is applied identically during both training and inference.", "rephrasing_timestamp": "2026-01-11T19:15:24.778861", "rephrasing_model": "gpt-5-mini"}
{"claim": "During model execution, their method alternately carries out purification steps and reintroduces randomness at each iteration, relying on a consistency-driven framework to improve the realism and fidelity of the final outputs.", "label": "Supported", "paragraph": "This motivated us to extend DMD to support multi-step sampling. We fix a predetermined schedule with N timestep { t 1 , t 2 , . . . t N } , identical during training and inference. During inference, at each step, we alternate between denoising and noise injection steps, following the consistency model [9], to improve sample quality.", "section_name": "4.4 Multi-step generator", "subsection_name": "", "paper_id": "tQukGCDaNT", "paper_title": "Improved Distribution Matching Distillation for Fast Image Synthesis", "paper_venue": "neurips2024", "paper_decision": "Accept (oral)", "decision": "Oral", "extraction_timestamp": "2026-01-11T15:13:34.418865", "model": "gpt-5-mini", "original_claim": "During inference, their procedure alternates between applying denoising operations and injecting noise at each timestep, adopting the consistency-model approach to improve generated sample quality.", "rephrasing_timestamp": "2026-01-11T19:15:29.143483", "rephrasing_model": "gpt-5-mini"}
{"claim": "Initialize with z_0 sampled from an isotropic Gaussian. Then, for each i, iteratively apply a denoising step x̂_{t_i} = G_θ(x_{t_i}, t_i) and a noisy forward update to the next time index via x_{t_{i+1}} = α_{t_{i+1}} x̂_{t_i} + σ_{t_{i+1}} η_i, where η_i is drawn from a standard normal, repeating these paired operations until x̂_{t_N} is obtained.", "label": "Supported", "paragraph": "Specifically, starting from Gaussian noise z 0 ∼ N (0 , I ) , we alternate between denoising updates ˆ x t i = G θ ( x t i , t i ) , and forward diffusion steps x t i +1 = α t i +1 ˆ x t i + σ t i +1 ϵ with ϵ ∼ N (0 , I ) , until we obtain our final image ˆ x t N .", "section_name": "4.4 Multi-step generator", "subsection_name": "", "paper_id": "tQukGCDaNT", "paper_title": "Improved Distribution Matching Distillation for Fast Image Synthesis", "paper_venue": "neurips2024", "paper_decision": "Accept (oral)", "decision": "Oral", "extraction_timestamp": "2026-01-11T15:13:44.297883", "model": "gpt-5-mini", "original_claim": "Sampling starts from Gaussian noise z_0 drawn from N(0,I) and alternates denoising updates x̂_{t_i}=G_θ(x_{t_i},t_i) with forward diffusion x_{t_{i+1}}=α_{t_{i+1}}x̂_{t_i}+σ_{t_{i+1}}ε (ε∼N(0,I)) until x̂_{t_N}.", "rephrasing_timestamp": "2026-01-11T19:15:51.492913", "rephrasing_model": "gpt-5-mini"}
{"claim": "GraphMETRO's effectiveness is partly determined by how well its routing mechanism can detect statistical changes in the data introduced by the transformations it applies.", "label": "Supported", "paragraph": "The performance of gating model . One factor that affect the performance of GraphMETRO is the effectiveness of gating model in identifing distribution shifts from transform functions. Specifically, some transform functions are inherently disentangled, e.g., adding nodes feature noise and random subgraph extraction.", "section_name": "F OPEN DISCUSSION AND FUTURE WORKS", "subsection_name": "", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:14:01.613665", "model": "gpt-5-mini", "original_claim": "GraphMETRO's overall performance is influenced in part by the gating model's capability to recognize distributional shifts caused by the applied transformation functions.", "rephrasing_timestamp": "2026-01-11T19:15:34.913977", "rephrasing_model": "gpt-5-mini"}
{"claim": "Certain augmentation strategies naturally promote separation of underlying factors — for example, corrupting vertex attributes with stochastic perturbations and randomly sampling subsets of the graph's structure.", "label": "Supported", "paragraph": "The performance of gating model . One factor that affect the performance of GraphMETRO is the effectiveness of gating model in identifing distribution shifts from transform functions. Specifically, some transform functions are inherently disentangled, e.g., adding nodes feature noise and random subgraph extraction.", "section_name": "F OPEN DISCUSSION AND FUTURE WORKS", "subsection_name": "", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:14:01.613675", "model": "gpt-5-mini", "original_claim": "Some transformation methods are inherently disentangled, with examples including adding noise to node features and performing random subgraph extraction.", "rephrasing_timestamp": "2026-01-11T19:15:35.345870", "rephrasing_model": "gpt-5-mini"}
{"claim": "Any two of the three dataset types—graphs with corrupted vertices, graphs formed by randomly sampling subgraphs, and sampled subgraphs that also include vertex corruption—show observable differences that allow them to be distinguished from one another.", "label": "Supported", "paragraph": "In this case, there will be certain distinction between any pair from these three data distributions, i.e., (graphs with node noise, random subgraph graphs, random subgraphs with node noise), which the gating model can easily tell.", "section_name": "F OPEN DISCUSSION AND FUTURE WORKS", "subsection_name": "", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:14:12.751623", "model": "gpt-5-mini", "original_claim": "Each pair among the three data distributions (graphs with node noise, random-subgraph graphs, and random subgraphs with node noise) exhibits identifiable differences that distinguish one distribution from another.", "rephrasing_timestamp": "2026-01-11T19:15:41.872036", "rephrasing_model": "gpt-5-mini"}
{"claim": "The selector module reliably separates the three underlying distributions and determines which one generated any particular graph instance.", "label": "Supported", "paragraph": "In this case, there will be certain distinction between any pair from these three data distributions, i.e., (graphs with node noise, random subgraph graphs, random subgraphs with node noise), which the gating model can easily tell.", "section_name": "F OPEN DISCUSSION AND FUTURE WORKS", "subsection_name": "", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:14:12.751637", "model": "gpt-5-mini", "original_claim": "The gating model can readily discriminate among those three distributions, easily identifying which distribution produced a given graph sample.", "rephrasing_timestamp": "2026-01-11T19:15:43.638891", "rephrasing_model": "gpt-5-mini"}
{"claim": "If the applied alterations are functionally alike — for example, deleting entire routes instead of single connections — our approach's effectiveness stays unchanged, as long as each specialist component produces the appropriate stable encoding.", "label": "Supported", "paragraph": "While some transform functions can be essentially similar, e.g., dropping path and dropping edges, this won't affect the performance of our method as long as each expert outputs the corresponding invariant representation.", "section_name": "F OPEN DISCUSSION AND FUTURE WORKS", "subsection_name": "", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:14:20.587905", "model": "gpt-5-mini", "original_claim": "The performance of our method is not impacted when transformation functions are essentially similar (e.g., path-dropping versus edge-dropping), provided each expert produces the corresponding invariant representation.", "rephrasing_timestamp": "2026-01-11T19:15:49.632717", "rephrasing_model": "gpt-5-mini"}
{"claim": "For each position i, a step is expressed by combining a_i with b_i to yield c_i; a_i, b_i, and c_i are the tokenized strings associated with that step.", "label": "Supported", "paragraph": "For each step i , we represent the step in the format a ( i ) + b ( i ) = c ( i ) , where a ( i ) , b ( i ) and c ( i ) are sequences of n tokens, each of which is in [0 , 9] , representing a number from the lowest digit to the highest digit.", "section_name": "C FORMAL DESCRIPTION OF THE DIGIT ADDITION DATA", "subsection_name": "", "paper_id": "aaYBsuGRne", "paper_title": "Understanding In-context Learning with a Pelican Soup Hypothesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:14:41.058341", "model": "gpt-5-mini", "original_claim": "Every step indexed by i is encoded as an equation a_i plus b_i equals c_i, with a_i, b_i, and c_i serving as the represented token sequences for that step.", "rephrasing_timestamp": "2026-01-11T19:15:49.984907", "rephrasing_model": "gpt-5-mini"}
{"claim": "Each sequence a_i, b_i, and c_i has length n; every element is an integer between 0 and 9, and the elements encode the number in positional order, starting from the ones place and progressing to the highest place value.", "label": "Supported", "paragraph": "For each step i , we represent the step in the format a ( i ) + b ( i ) = c ( i ) , where a ( i ) , b ( i ) and c ( i ) are sequences of n tokens, each of which is in [0 , 9] , representing a number from the lowest digit to the highest digit.", "section_name": "C FORMAL DESCRIPTION OF THE DIGIT ADDITION DATA", "subsection_name": "", "paper_id": "aaYBsuGRne", "paper_title": "Understanding In-context Learning with a Pelican Soup Hypothesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:14:41.058352", "model": "gpt-5-mini", "original_claim": "Each sequence a_i, b_i, and c_i contains n tokens; each token is a digit from 0 to 9 and the tokens represent the number with digits ordered from least significant to most significant.", "rephrasing_timestamp": "2026-01-11T19:15:57.980752", "rephrasing_model": "gpt-5-mini"}
{"claim": "When the process begins, a(0) and b(0) receive random values, while c(0) is filled entirely with zeros.", "label": "Supported", "paragraph": "a (0) and b (0) represent two randomly drawn numbers and c (0) is all zero. At each step i &gt; 0 , most of the digit in a ( i ) , b ( i ) , c ( i ) is the same as the previous step. For a ( i ) and b ( i ) , we only update the i th digit by setting a ( i ) i = 0 and b ( i ) i = 0 .", "section_name": "C FORMAL DESCRIPTION OF THE DIGIT ADDITION DATA", "subsection_name": "", "paper_id": "aaYBsuGRne", "paper_title": "Understanding In-context Learning with a Pelican Soup Hypothesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:14:53.890101", "model": "gpt-5-mini", "original_claim": "At initialization, a(0) and b(0) are two randomly drawn numbers, while c(0) is initialized to contain only zeros.", "rephrasing_timestamp": "2026-01-11T19:15:55.987248", "rephrasing_model": "gpt-5-mini"}
{"claim": "Whenever i>0, most digit positions of a(i), b(i), and c(i) keep the same values they had at index i−1; moreover, a(i) and b(i) differ from a(i−1) only in that the digit at position i is replaced by 0.", "label": "Supported", "paragraph": "a (0) and b (0) represent two randomly drawn numbers and c (0) is all zero. At each step i &gt; 0 , most of the digit in a ( i ) , b ( i ) , c ( i ) is the same as the previous step. For a ( i ) and b ( i ) , we only update the i th digit by setting a ( i ) i = 0 and b ( i ) i = 0 .", "section_name": "C FORMAL DESCRIPTION OF THE DIGIT ADDITION DATA", "subsection_name": "", "paper_id": "aaYBsuGRne", "paper_title": "Understanding In-context Learning with a Pelican Soup Hypothesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:14:53.890114", "model": "gpt-5-mini", "original_claim": "For every step i > 0, most digits of a(i), b(i), and c(i) remain identical to the previous step, and for a(i) and b(i) only the i-th digit is set to zero.", "rephrasing_timestamp": "2026-01-11T19:16:05.601759", "rephrasing_model": "gpt-5-mini"}
{"claim": "For every index i, form t_i by adding the three digits at position i; then store the ones digit of t_i in the i-th entry of c(i) and place the count of full tens in t_i into the (i+1)-th entry.", "label": "Supported", "paragraph": "As for c ( i ) , it serves as a buffer for both the answer and the carry. We update it based on s ( i ) = a ( i -1) i + b ( i -1) i + c ( i -1) i , the summation of the digits at i . We set c ( i ) i = s ( i ) mod 10 , c ( i ) i +1 = ⌊ s ( i ) / 10 ⌋ .", "section_name": "C FORMAL DESCRIPTION OF THE DIGIT ADDITION DATA", "subsection_name": "", "paper_id": "aaYBsuGRne", "paper_title": "Understanding In-context Learning with a Pelican Soup Hypothesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:15:07.965812", "model": "gpt-5-mini", "original_claim": "For each position i, compute s(i) as the sum of the three digits at that position, then set c(i)_i to the remainder of s(i) modulo 10 and c(i)_{i+1} to the integer quotient of s(i) divided by 10.", "rephrasing_timestamp": "2026-01-11T19:16:05.363609", "rephrasing_model": "gpt-5-mini"}
{"claim": "The paragraph indicates that incorporating several predictions multiplies the algorithm's computational cost by a factor equal to the logarithm of m.", "label": "Supported", "paragraph": "We now state the following theorem regarding the algorithm for multiple predictions. The overhead of using multiple predictions is a log m factor. The proof is very similar to the proof of Theorem 3.1 and has been deferred to Appendix A.\nTheorem 4.1. Given m different distributions, the expected query complexity of the algorithm is log( m ) · O ( H ( p ) + max(min k ∈ [ m ] log η k , 0)) .", "section_name": "4.2 Analysis for Multiple Predictions", "subsection_name": "", "paper_id": "JEKXTLjEIq", "paper_title": "Binary Search with Distributional Predictions", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:15:44.309445", "model": "gpt-5-mini", "original_claim": "Using multiple predictions causes the algorithm's computational overhead to increase by a multiplicative factor of log m, according to the paragraph.", "rephrasing_timestamp": "2026-01-11T19:16:04.278213", "rephrasing_model": "gpt-5-mini"}
{"claim": "For m distinct probability models, the algorithm's average number of queries scales, up to constant factors, as log m times the quantity H(p) plus the nonnegative part of the smallest log η_k across k = 1,…,m; formally, its expected queries are O( log m · ( H(p) + max( min_{k=1..m} log η_k, 0 ) ) ).", "label": "Supported", "paragraph": "We now state the following theorem regarding the algorithm for multiple predictions. The overhead of using multiple predictions is a log m factor. The proof is very similar to the proof of Theorem 3.1 and has been deferred to Appendix A.\nTheorem 4.1. Given m different distributions, the expected query complexity of the algorithm is log( m ) · O ( H ( p ) + max(min k ∈ [ m ] log η k , 0)) .", "section_name": "4.2 Analysis for Multiple Predictions", "subsection_name": "", "paper_id": "JEKXTLjEIq", "paper_title": "Binary Search with Distributional Predictions", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:15:44.309456", "model": "gpt-5-mini", "original_claim": "Given m different distributions, the algorithm's expected query complexity is log m multiplied by O(H(p) + max(min over k in [m] of log eta_k, 0)).", "rephrasing_timestamp": "2026-01-11T19:16:11.259465", "rephrasing_model": "gpt-5-mini"}
{"claim": "The passage indicates Michael Dinitz is part of the Johns Hopkins academic community and lists an e‑mail contact hosted on the cs.jhu.edu server.", "label": "Supported", "paragraph": "Michael Dinitz ∗\nJohns Hopkins University mdinitz@cs.jhu.edu\nSungjin Im † UC Merced sim3@ucmerced.edu\nBenjamin Moseley ‡\nCarnegie Mellon University moseleyb@andrew.cmu.edu", "section_name": "Binary Search with Distributional Predictions", "subsection_name": "", "paper_id": "JEKXTLjEIq", "paper_title": "Binary Search with Distributional Predictions", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:16:07.204298", "model": "gpt-5-mini", "original_claim": "The paragraph states that Michael Dinitz is affiliated with Johns Hopkins University and provides his contact via an email address on the cs.jhu.edu domain.", "rephrasing_timestamp": "2026-01-11T19:16:06.938441", "rephrasing_model": "gpt-5-mini"}
{"claim": "The passage identifies Benjamin Moseley as connected to Carnegie Mellon University and provides a way to contact him via an electronic mail address ending with @andrew.cmu.edu.", "label": "Supported", "paragraph": "Michael Dinitz ∗\nJohns Hopkins University mdinitz@cs.jhu.edu\nSungjin Im † UC Merced sim3@ucmerced.edu\nBenjamin Moseley ‡\nCarnegie Mellon University moseleyb@andrew.cmu.edu", "section_name": "Binary Search with Distributional Predictions", "subsection_name": "", "paper_id": "JEKXTLjEIq", "paper_title": "Binary Search with Distributional Predictions", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:16:07.204315", "model": "gpt-5-mini", "original_claim": "The paragraph lists Benjamin Moseley as affiliated with Carnegie Mellon University and supplies his contact using an email address on the andrew.cmu.edu domain.", "rephrasing_timestamp": "2026-01-11T19:16:25.723696", "rephrasing_model": "gpt-5-mini"}
{"claim": "The algorithm's average running cost is obtained by summing, for i = 1 to n, the product p_i · C(a_i); here C(a_i) is the cost incurred when the search target is a_i.", "label": "Supported", "paragraph": "Proof of Theorem 4.1. With probability p i , the key a that we are looking for is a i . The goal is to bound the expected cost of the algorithm, which is ∑ n i =1 p i C ( a i ) , where C ( a i ) is the cost of the algorithm when a = a i .", "section_name": "A Omitted Proofs", "subsection_name": "", "paper_id": "JEKXTLjEIq", "paper_title": "Binary Search with Distributional Predictions", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:16:15.195677", "model": "gpt-5-mini", "original_claim": "The algorithm's expected cost equals the weighted sum over i from 1 to n of p_i times C(a_i), where C(a_i) denotes the algorithm's cost when the sought key is a_i.", "rephrasing_timestamp": "2026-01-11T19:16:21.545395", "rephrasing_model": "gpt-5-mini"}
{"claim": "The eighth supplemental figure demonstrates how the SSC and SDC indices are employed in the paper’s analysis.", "label": "Supported", "paragraph": "To illustrate the SSC and SDC measures, Supp. Fig. 8 has been provided. It shows the images including/excluding the top concepts in addition to the values of SSC and SDC for one example of the tiger cat class.", "section_name": "C.2 INSIGHTS INTO THE NUMBER OF REQUIRED CONCEPTS", "subsection_name": "", "paper_id": "UqZecMwLTo", "paper_title": "A Unified Concept-Based System for Local, Global, and Misclassification Explanations", "paper_venue": "iclr2024", "paper_decision": null, "decision": "Unknown", "extraction_timestamp": "2026-01-11T15:16:35.527318", "model": "gpt-5-mini", "original_claim": "Supplementary Figure 8 was provided as an illustration demonstrating how the SSC and SDC metrics are applied in the study.", "rephrasing_timestamp": "2026-01-11T19:16:19.965034", "rephrasing_model": "gpt-5-mini"}
{"claim": "Figure S8 (supplementary) shows examples that both contain and omit the primary concepts and reports SSC and SDC numeric scores for one representative instance of the tiger cat category.", "label": "Supported", "paragraph": "To illustrate the SSC and SDC measures, Supp. Fig. 8 has been provided. It shows the images including/excluding the top concepts in addition to the values of SSC and SDC for one example of the tiger cat class.", "section_name": "C.2 INSIGHTS INTO THE NUMBER OF REQUIRED CONCEPTS", "subsection_name": "", "paper_id": "UqZecMwLTo", "paper_title": "A Unified Concept-Based System for Local, Global, and Misclassification Explanations", "paper_venue": "iclr2024", "paper_decision": null, "decision": "Unknown", "extraction_timestamp": "2026-01-11T15:16:35.527328", "model": "gpt-5-mini", "original_claim": "Supplementary Figure 8 presents images that include and exclude the top concepts and also lists SSC and SDC numerical values for a single example from the tiger cat class.", "rephrasing_timestamp": "2026-01-11T19:16:25.798087", "rephrasing_model": "gpt-5-mini"}
{"claim": "The team ran the image set through two classifiers, NetS and NetB, and logged the label each classifier assigned to every image.", "label": "Supported", "paragraph": "We passed these images through the NetS as well as NetB and reported the predictions below of each image. We did this experiment to evaluate the effectiveness of the obtained concepts in distinguishing the target images not only in a binary manner (target or non-target) but also among all the 1000 classes of the ImageNet.", "section_name": "C.2 INSIGHTS INTO THE NUMBER OF REQUIRED CONCEPTS", "subsection_name": "", "paper_id": "UqZecMwLTo", "paper_title": "A Unified Concept-Based System for Local, Global, and Misclassification Explanations", "paper_venue": "iclr2024", "paper_decision": null, "decision": "Unknown", "extraction_timestamp": "2026-01-11T15:16:50.778757", "model": "gpt-5-mini", "original_claim": "The images were processed by two models, NetS and NetB, and the researchers recorded the predicted class outputs for each individual image from those networks.", "rephrasing_timestamp": "2026-01-11T19:16:21.732846", "rephrasing_model": "gpt-5-mini"}
{"claim": "The study was set up to determine whether the derived representations can discriminate target images among the full set of one thousand object categories in the ImageNet dataset, instead of being restricted to a simple two-way target/non-target split.", "label": "Supported", "paragraph": "We passed these images through the NetS as well as NetB and reported the predictions below of each image. We did this experiment to evaluate the effectiveness of the obtained concepts in distinguishing the target images not only in a binary manner (target or non-target) but also among all the 1000 classes of the ImageNet.", "section_name": "C.2 INSIGHTS INTO THE NUMBER OF REQUIRED CONCEPTS", "subsection_name": "", "paper_id": "UqZecMwLTo", "paper_title": "A Unified Concept-Based System for Local, Global, and Misclassification Explanations", "paper_venue": "iclr2024", "paper_decision": null, "decision": "Unknown", "extraction_timestamp": "2026-01-11T15:16:50.778774", "model": "gpt-5-mini", "original_claim": "The experiment was designed to assess whether the extracted concepts can distinguish target images across all 1000 ImageNet categories rather than only performing a binary target versus non-target separation.", "rephrasing_timestamp": "2026-01-11T19:16:26.769183", "rephrasing_model": "gpt-5-mini"}
{"claim": "The classifier sometimes mistakes photos of tiger-striped cats for classes like Conch, Tabby, Egyptian cat, Persian cat, or other labels.", "label": "Supported", "paragraph": "In the latter, the tiger cat object can wrongly be recognized as a Conch, a Tabby, an Egyptian cat, a Persian cat, and so on. In this context, the minimum number of top local concepts to recognize objects of target class c is obtained as follows:\n<!-- formula-not-decoded -->\nin which D c is the training set of this target.", "section_name": "C.2 INSIGHTS INTO THE NUMBER OF REQUIRED CONCEPTS", "subsection_name": "", "paper_id": "UqZecMwLTo", "paper_title": "A Unified Concept-Based System for Local, Global, and Misclassification Explanations", "paper_venue": "iclr2024", "paper_decision": null, "decision": "Unknown", "extraction_timestamp": "2026-01-11T15:16:56.985225", "model": "gpt-5-mini", "original_claim": "Images of the tiger cat class are sometimes misclassified by the model as Conch, Tabby, Egyptian cat, Persian cat, or other categories.", "rephrasing_timestamp": "2026-01-11T19:16:35.038411", "rephrasing_model": "gpt-5-mini"}
{"claim": "For CIFAR-10, we tested five different counts of exchange cycles: 10, 20, 30, 40, and 50.", "label": "Supported", "paragraph": "Group test performed\n10 20 30 40 50 Communication rounds (b) Experiments over CIFAR-10 dataset. Throughout the paper, we have considered a scenario with n = 15 clients and the parity-check matrix of a (15 , 7) BCH code of length 15 and dimension 7 as the assignment matrix.", "section_name": "I PRIVACY-SECURITY TRADE-OFF", "subsection_name": "", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:17:18.584545", "model": "gpt-5-mini", "original_claim": "In the CIFAR-10 experiments, the number of communication rounds was set to five distinct values: 10, 20, 30, 40, and 50.", "rephrasing_timestamp": "2026-01-11T19:16:33.549098", "rephrasing_model": "gpt-5-mini"}
{"claim": "The work considered a configuration of fifteen clients and, in every experiment, used the BCH [15,7] code’s redundancy-constraint matrix (the matrix that specifies the code’s redundancy checks) to define the allocation matrix.", "label": "Supported", "paragraph": "Group test performed\n10 20 30 40 50 Communication rounds (b) Experiments over CIFAR-10 dataset. Throughout the paper, we have considered a scenario with n = 15 clients and the parity-check matrix of a (15 , 7) BCH code of length 15 and dimension 7 as the assignment matrix.", "section_name": "I PRIVACY-SECURITY TRADE-OFF", "subsection_name": "", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:17:18.584556", "model": "gpt-5-mini", "original_claim": "The study assumed a system of 15 clients and employed the parity-check matrix of a (15,7) BCH code (length 15, dimension 7) as the assignment matrix throughout.", "rephrasing_timestamp": "2026-01-11T19:16:35.565930", "rephrasing_model": "gpt-5-mini"}
{"claim": "The outlined FedGT setup involves fifteen participants distributed across eight clusters, with each cluster holding four members.", "label": "Supported", "paragraph": "This corresponds to a FedGT scheme with 15 clients and 8 groups, each with 4 clients, and the privacy achieved is the same as secure aggregation with 4 clients. However, there are other ways of grouping the clients, where the number of groups can be smaller and the number of clients per group larger, yielding a trade-off between privacy and security.", "section_name": "I PRIVACY-SECURITY TRADE-OFF", "subsection_name": "", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:17:31.757021", "model": "gpt-5-mini", "original_claim": "The described FedGT configuration has 15 clients organized into 8 groups, with each group comprising 4 clients.", "rephrasing_timestamp": "2026-01-11T19:16:34.807642", "rephrasing_model": "gpt-5-mini"}
{"claim": "In that FedGT configuration, the level of data confidentiality achieved is equivalent to what an aggregation protocol that secures four participants would provide.", "label": "Supported", "paragraph": "This corresponds to a FedGT scheme with 15 clients and 8 groups, each with 4 clients, and the privacy achieved is the same as secure aggregation with 4 clients. However, there are other ways of grouping the clients, where the number of groups can be smaller and the number of clients per group larger, yielding a trade-off between privacy and security.", "section_name": "I PRIVACY-SECURITY TRADE-OFF", "subsection_name": "", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:17:31.757035", "model": "gpt-5-mini", "original_claim": "Under that FedGT arrangement, the level of privacy achieved matches the privacy of a secure aggregation scheme that protects four clients.", "rephrasing_timestamp": "2026-01-11T19:16:35.587348", "rephrasing_model": "gpt-5-mini"}
{"claim": "They substituted the assignment matrices with the constraint (check) matrices obtained from two cyclically invariant linear codes of block length fifteen—one supporting nine information symbols and the other supporting eleven.", "label": "Supported", "paragraph": "To highlight this trade-off, here we consider two other assignment matrices obtained by taking the parity-check matrices of cyclic codes of length 15 . More specifically, we choose two other cyclic codes: A (15 , 9) code of length 15 and dimension 9 and a (15 , 11) code of length 15 and dimension 11 .", "section_name": "I PRIVACY-SECURITY TRADE-OFF", "subsection_name": "", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:17:37.386767", "model": "gpt-5-mini", "original_claim": "The authors used parity-check matrices from two cyclic codes of length 15—one with dimension 9 (a (15,9) code) and another with dimension 11 (a (15,11) code)—as alternative assignment matrices.", "rephrasing_timestamp": "2026-01-11T19:16:45.444754", "rephrasing_model": "gpt-5-mini"}
{"claim": "The article lists evaluation measurements for three-dimensional semantic labeling on each of the three data splits, with the results shown in Table 1.", "label": "Supported", "paragraph": "We show 3D semantic segmentation scores for all three subsets in Table 1. The 3D scene segmentations are obtained by querying the 3D scene representations for each one of the annotated ground truth semantic classes and assigning the class with the highest similarity score to each 3D point.", "section_name": "4.2 RESULTS ON 3D SEMANTIC SEGMENTATION", "subsection_name": "", "paper_id": "SgjAojPKb3", "paper_title": "OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:17:50.600395", "model": "gpt-5-mini", "original_claim": "The paper reports 3D semantic segmentation performance metrics for all three dataset subsets, with those results compiled in Table 1.", "rephrasing_timestamp": "2026-01-11T19:16:46.654657", "rephrasing_model": "gpt-5-mini"}
{"claim": "For each labeled category, the approach evaluates its relation to the three-dimensional scene model and tags every point in space with the category that achieves the strongest match.", "label": "Supported", "paragraph": "We show 3D semantic segmentation scores for all three subsets in Table 1. The 3D scene segmentations are obtained by querying the 3D scene representations for each one of the annotated ground truth semantic classes and assigning the class with the highest similarity score to each 3D point.", "section_name": "4.2 RESULTS ON 3D SEMANTIC SEGMENTATION", "subsection_name": "", "paper_id": "SgjAojPKb3", "paper_title": "OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:17:50.600407", "model": "gpt-5-mini", "original_claim": "For each annotated semantic class, the method queries the 3D scene representation and assigns each 3D point the class that attains the highest similarity score.", "rephrasing_timestamp": "2026-01-11T19:16:50.460033", "rephrasing_model": "gpt-5-mini"}
{"claim": "Retrieval is achieved by aligning descriptor vectors computed from three-dimensional point-set scans with textual vector representations produced by a pre-trained multimodal text encoder — specifically CLIP’s ViT-L/14 configured at 336-pixel resolution.", "label": "Supported", "paragraph": "Querying is performed via correlation of the 3D point cloud features with the embedding from large language models, specifically the CLIP-text encoder (ViT-L14@336). For OpenScene, we observe a similar trend as reported in (Peng et al., 2023), where the Ensemble model improves over the Distilled model ( +1 .", "section_name": "4.2 RESULTS ON 3D SEMANTIC SEGMENTATION", "subsection_name": "", "paper_id": "SgjAojPKb3", "paper_title": "OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:18:00.975715", "model": "gpt-5-mini", "original_claim": "Querying is done by correlating features extracted from 3D point clouds with text embeddings produced by a large language model, specifically using the CLIP text encoder ViT-L14@336.", "rephrasing_timestamp": "2026-01-11T19:16:51.797303", "rephrasing_model": "gpt-5-mini"}
{"claim": "When evaluated on the OpenScene benchmark, a model created by combining multiple learners achieves better results than a compact, compressed student model, echoing the gains reported by Peng et al. (2023).", "label": "Supported", "paragraph": "Querying is performed via correlation of the 3D point cloud features with the embedding from large language models, specifically the CLIP-text encoder (ViT-L14@336). For OpenScene, we observe a similar trend as reported in (Peng et al., 2023), where the Ensemble model improves over the Distilled model ( +1 .", "section_name": "4.2 RESULTS ON 3D SEMANTIC SEGMENTATION", "subsection_name": "", "paper_id": "SgjAojPKb3", "paper_title": "OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:18:00.975732", "model": "gpt-5-mini", "original_claim": "On the OpenScene benchmark, the Ensemble model yields higher performance than the Distilled model, replicating the improvement trend reported by Peng et al. (2023).", "rephrasing_timestamp": "2026-01-11T19:16:51.321646", "rephrasing_model": "gpt-5-mini"}
{"claim": "Relative to the OpenScene reference model, our technique delivers a mean intersection-over-union gain of 4.5 points when averaged over all tested categories.", "label": "Supported", "paragraph": "1 mIoU). Our results clearly improve over all baseline methods. We outperform LERF significantly while possessing a simpler overall design. Compared to OpenScene, we achieve +4 . 5 mIoU over all classes, +3 .", "section_name": "4.2 RESULTS ON 3D SEMANTIC SEGMENTATION", "subsection_name": "", "paper_id": "SgjAojPKb3", "paper_title": "OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:18:06.764557", "model": "gpt-5-mini", "original_claim": "When compared with the OpenScene baseline, our method attains an average increase of 4.5 mIoU across all evaluated classes.", "rephrasing_timestamp": "2026-01-11T19:16:55.146554", "rephrasing_model": "gpt-5-mini"}
{"claim": "Several OpenCLIP variants at different capacities served as supervisory models to guide the training of the tokenizers built with VQ-KD.", "label": "Supported", "paragraph": "We incorporate OpenCLIP [17] models of varying sizes as teacher models to train the VQ-KD tokenizers. As illustrated in Tab. 6, the FIDAR metric sees a reduction from 10 . 31 to 8 . 70 when the size of the OpenCLIP model escalates from ViTL/14 to ViT-G/14.", "section_name": "4.5 Large Teacher Models in VQ-KD", "subsection_name": "", "paper_id": "RMmgu49lwn", "paper_title": "Image Understanding Makes for A Good Tokenizer for Image Generation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:18:27.976370", "model": "gpt-5-mini", "original_claim": "OpenCLIP models of multiple sizes were employed as teacher networks to train the VQ-KD tokenizers.", "rephrasing_timestamp": "2026-01-11T19:16:59.283137", "rephrasing_model": "gpt-5-mini"}
{"claim": "The sixth table shows that replacing the OpenCLIP mentor from ViTL/14 with ViT-G/14 cut the FIDAR score from 10.31 to 8.70.", "label": "Supported", "paragraph": "We incorporate OpenCLIP [17] models of varying sizes as teacher models to train the VQ-KD tokenizers. As illustrated in Tab. 6, the FIDAR metric sees a reduction from 10 . 31 to 8 . 70 when the size of the OpenCLIP model escalates from ViTL/14 to ViT-G/14.", "section_name": "4.5 Large Teacher Models in VQ-KD", "subsection_name": "", "paper_id": "RMmgu49lwn", "paper_title": "Image Understanding Makes for A Good Tokenizer for Image Generation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:18:27.976381", "model": "gpt-5-mini", "original_claim": "Table 6 indicates that raising the OpenCLIP teacher model from ViTL/14 to ViT-G/14 decreased the FIDAR metric from 10.31 to 8.70.", "rephrasing_timestamp": "2026-01-11T19:17:02.734884", "rephrasing_model": "gpt-5-mini"}
{"claim": "As noted in the paragraph, higher-capacity networks in the OpenCLIP lineup consistently demonstrate better visual comprehension than their lower-capacity counterparts.", "label": "Supported", "paragraph": "Given that larger OpenCLIP models inherently possess stronger IU capabilities, these findings further corroborate the superiority of image tokenizers with more potent IU capabilities. Table 6: Effect of different teachers in VQ-KD.", "section_name": "4.5 Large Teacher Models in VQ-KD", "subsection_name": "", "paper_id": "RMmgu49lwn", "paper_title": "Image Understanding Makes for A Good Tokenizer for Image Generation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:18:38.732957", "model": "gpt-5-mini", "original_claim": "Larger OpenCLIP models inherently exhibit stronger image understanding (IU) capabilities than smaller OpenCLIP variants, as stated in the paragraph.", "rephrasing_timestamp": "2026-01-11T19:17:00.348337", "rephrasing_model": "gpt-5-mini"}
{"claim": "The evidence shown alongside the sixth table, addressing the influence of VQ-KD-based mentors, indicates that visual token generators with stronger IU abilities outperform weaker ones.", "label": "Supported", "paragraph": "Given that larger OpenCLIP models inherently possess stronger IU capabilities, these findings further corroborate the superiority of image tokenizers with more potent IU capabilities. Table 6: Effect of different teachers in VQ-KD.", "section_name": "4.5 Large Teacher Models in VQ-KD", "subsection_name": "", "paper_id": "RMmgu49lwn", "paper_title": "Image Understanding Makes for A Good Tokenizer for Image Generation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:18:38.732972", "model": "gpt-5-mini", "original_claim": "The findings, referenced alongside Table 6 about VQ-KD teacher effects, support that image tokenizers with stronger IU capabilities are superior in performance.", "rephrasing_timestamp": "2026-01-11T19:17:06.337066", "rephrasing_model": "gpt-5-mini"}
{"claim": "In the reported results, the ViT-H/14 edition of OpenCLIP achieved a reconstructed Fréchet Inception Distance of 3.6, a perceptual-path-length value of 97.32, an adjusted FID of 9.64, and an adjusted Inception Score of 161.13.", "label": "Supported", "paragraph": "| OpenCLIP   |   rFID ↓ |   PPL ↓ |   FID AR ↓ |   IS AR |\n|----------|----------|---------|----------|---------|\n| ViT-L/14   |     4.03 |   80.56 |      10.31 |  146.21 |\n| ViT-H/14   |     3.6  |   97.32 |       9.64 |  161.13 |\n| ViT-G/14   |     3.8  |   77.79 |       8.7  |  152.71 |\nTable 7: Effect of codebook size and dimension.", "section_name": "4.5 Large Teacher Models in VQ-KD", "subsection_name": "", "paper_id": "RMmgu49lwn", "paper_title": "Image Understanding Makes for A Good Tokenizer for Image Generation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:18:53.448222", "model": "gpt-5-mini", "original_claim": "The OpenCLIP ViT-H/14 variant produced an rFID of 3.6, PPL of 97.32, FID AR equal to 9.64, and IS AR equal to 161.13 in the reported results.", "rephrasing_timestamp": "2026-01-11T19:17:10.832883", "rephrasing_model": "gpt-5-mini"}
{"claim": "Under this labeling convention, a value of 0 means the two sentences express different content, while a value of 1 means they express the same content.", "label": "Supported", "paragraph": "We assign label 0 as ' not\\_quivalent ' and label 1 as ' equivalent '.\nOriginal inputs:\n- sentence1: Amrozi accused his brother , whom he called 'the witness' , of deliberately distorting his evidence .\n- sentence2: Referring to him as only 'the witness' , Amrozi accused his brother of deliberately distorting his evidence .\n- label: 1", "section_name": "F.1.3 MRPC", "subsection_name": "", "paper_id": "iynRvVVAmH", "paper_title": "Parameter-Efficient Multi-Task Model Fusion with Partial Linearization", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:19:33.648041", "model": "gpt-5-mini", "original_claim": "In this annotation scheme, the integer 0 denotes that sentence pair meanings differ, while the integer 1 indicates that the two sentences convey the same meaning.", "rephrasing_timestamp": "2026-01-11T19:17:10.282751", "rephrasing_model": "gpt-5-mini"}
{"claim": "The two sentences—one naming Amrozi's sibling as the person who testified and the other referring only to that individual—were tagged with a 1, indicating they convey the same meaning.", "label": "Supported", "paragraph": "We assign label 0 as ' not\\_quivalent ' and label 1 as ' equivalent '.\nOriginal inputs:\n- sentence1: Amrozi accused his brother , whom he called 'the witness' , of deliberately distorting his evidence .\n- sentence2: Referring to him as only 'the witness' , Amrozi accused his brother of deliberately distorting his evidence .\n- label: 1", "section_name": "F.1.3 MRPC", "subsection_name": "", "paper_id": "iynRvVVAmH", "paper_title": "Parameter-Efficient Multi-Task Model Fusion with Partial Linearization", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:19:33.648061", "model": "gpt-5-mini", "original_claim": "The provided pair of sentences—one labeling Amrozi's brother as 'the witness' and the other referring only to 'the witness'—was annotated with label 1, indicating semantic equivalence.", "rephrasing_timestamp": "2026-01-11T19:17:09.872313", "rephrasing_model": "gpt-5-mini"}
{"claim": "The researchers tested combining models for multiple objectives by creating every task grouping that included at least two tasks, leaving out the no-task case and any group containing a single task.", "label": "Supported", "paragraph": "We extensively investigate the multi-task model fusion by considering all possible subsets of the tasks, amounting to a total of 2 n -( n +1) subsets (excluding subsets with a single task and an empty set).", "section_name": "5.1 MULTI-TASK MODEL FUSION ON VISION AND LANGUAGE DOMAINS", "subsection_name": "", "paper_id": "iynRvVVAmH", "paper_title": "Parameter-Efficient Multi-Task Model Fusion with Partial Linearization", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:19:46.864927", "model": "gpt-5-mini", "original_claim": "The study evaluated multi-task model fusion by enumerating every possible subset of tasks while omitting the empty set and all single-task subsets.", "rephrasing_timestamp": "2026-01-11T19:17:12.499678", "rephrasing_model": "gpt-5-mini"}
{"claim": "Omitting the no-task scenario and all one-task groupings, the researchers evaluated 2^n − (n + 1) unique task combinations.", "label": "Supported", "paragraph": "We extensively investigate the multi-task model fusion by considering all possible subsets of the tasks, amounting to a total of 2 n -( n +1) subsets (excluding subsets with a single task and an empty set).", "section_name": "5.1 MULTI-TASK MODEL FUSION ON VISION AND LANGUAGE DOMAINS", "subsection_name": "", "paper_id": "iynRvVVAmH", "paper_title": "Parameter-Efficient Multi-Task Model Fusion with Partial Linearization", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:19:46.864943", "model": "gpt-5-mini", "original_claim": "By excluding the empty set and single-task subsets, the authors analyzed exactly 2^n - (n + 1) distinct task subsets as the total number of combinations considered.", "rephrasing_timestamp": "2026-01-11T19:17:23.612528", "rephrasing_model": "gpt-5-mini"}
{"claim": "After merging models using multiple integration methods, the researchers tested the resulting system on a variety of follow-up benchmarks covering both visual and textual tasks.", "label": "Supported", "paragraph": "We employ various model fusion algorithms to accomplish this. Subsequently, we assess the performance of the final model across the downstream tasks in both the vision and language domains. Detailed experimental settings are provided in Appendix E.", "section_name": "5.1 MULTI-TASK MODEL FUSION ON VISION AND LANGUAGE DOMAINS", "subsection_name": "", "paper_id": "iynRvVVAmH", "paper_title": "Parameter-Efficient Multi-Task Model Fusion with Partial Linearization", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:19:53.079150", "model": "gpt-5-mini", "original_claim": "After applying several model fusion algorithms, the authors evaluated the final model's performance on downstream tasks spanning both the vision and language domains.", "rephrasing_timestamp": "2026-01-11T19:17:24.236551", "rephrasing_model": "gpt-5-mini"}
{"claim": "Shitong Duan, Peng Zhang, Tun Lu, and Ning Gu are recorded in the authors' institutional details as belonging to Fudan University.", "label": "Supported", "paragraph": "Shitong Duan 1 , Xiaoyuan Yi 2 ∗ , Peng Zhang 1 ∗ , Tun Lu 1 , Xing Xie 2 , Ning Gu 1\n1 Fudan University, 2 Microsoft Research Asia stduan22@m.fudan.edu.cn , zhangpeng @fudan.edu.cn , { xiaoyuanyi, xingx } @microsoft.com", "section_name": "DENEVIL: TOWARDS DECIPHERING AND NAVIGATING THE ETHICAL VALUES OF LARGE LANGUAGE MODELS VIA INSTRUCTION LEARNING", "subsection_name": "", "paper_id": "m3RRWWFaVe", "paper_title": "DENEVIL: TOWARDS DECIPHERING AND NAVIGATING THE ETHICAL VALUES OF LARGE LANGUAGE MODELS VIA INSTRUCTION LEARNING", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:20:33.022928", "model": "gpt-5-mini", "original_claim": "Shitong Duan, Peng Zhang, Tun Lu, and Ning Gu are listed as affiliated with Fudan University in the author affiliation information.", "rephrasing_timestamp": "2026-01-11T19:17:21.487662", "rephrasing_model": "gpt-5-mini"}
{"claim": "The author-affiliation information attributes Xiaoyuan Yi and Xing Xie to Microsoft Research Asia.", "label": "Supported", "paragraph": "Shitong Duan 1 , Xiaoyuan Yi 2 ∗ , Peng Zhang 1 ∗ , Tun Lu 1 , Xing Xie 2 , Ning Gu 1\n1 Fudan University, 2 Microsoft Research Asia stduan22@m.fudan.edu.cn , zhangpeng @fudan.edu.cn , { xiaoyuanyi, xingx } @microsoft.com", "section_name": "DENEVIL: TOWARDS DECIPHERING AND NAVIGATING THE ETHICAL VALUES OF LARGE LANGUAGE MODELS VIA INSTRUCTION LEARNING", "subsection_name": "", "paper_id": "m3RRWWFaVe", "paper_title": "DENEVIL: TOWARDS DECIPHERING AND NAVIGATING THE ETHICAL VALUES OF LARGE LANGUAGE MODELS VIA INSTRUCTION LEARNING", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:20:33.022939", "model": "gpt-5-mini", "original_claim": "Xiaoyuan Yi and Xing Xie are identified as members of Microsoft Research Asia in the listed author affiliations.", "rephrasing_timestamp": "2026-01-11T19:17:24.132785", "rephrasing_model": "gpt-5-mini"}
{"claim": "Instead of claiming them as new concepts, the paper draws on the five moral dimensions set out by Haidt and Joseph (2004)—compassion, impartiality, allegiance, deference to hierarchy, and purity—as part of their earlier theoretical framework.", "label": "Supported", "paragraph": "The selction of Moral Foundations As delineated in our paper, the five foundations (care, fairness, loyalty, authority and sanctity) and their interpretations are rooted in the well-established Moral Foundations Theory (MFT) (Haidt &amp; Joseph, 2004) in social psychology, intended to explain the intercultural origins of and variation in human moral reasoning, instead of being selected or proposed by us.We utilize MFT, as we mentioned in 2, as an example for our DeNEVIL algorithm primarily for its cross-cultural universality(Graham et al., 2013; Yilmaz et al., 2016; Hu et al., 2020) and emphasis on morality, helping avoid the bias when applied to diverse scenarios.", "section_name": "A DETAILS OF DATASET CONSTRUCTION", "subsection_name": "", "paper_id": "m3RRWWFaVe", "paper_title": "DENEVIL: TOWARDS DECIPHERING AND NAVIGATING THE ETHICAL VALUES OF LARGE LANGUAGE MODELS VIA INSTRUCTION LEARNING", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:20:52.473709", "model": "gpt-5-mini", "original_claim": "The paper adopts the five Moral Foundations—care, fairness, loyalty, authority, and sanctity—as originating from Moral Foundations Theory (Haidt & Joseph, 2004), rather than being proposed by the authors.", "rephrasing_timestamp": "2026-01-11T19:17:22.335654", "rephrasing_model": "gpt-5-mini"}
{"claim": "They chose a well-known model that maps basic moral dimensions to demonstrate their DeNEVIL system, mainly because research supports its relevance across cultures and its focus on ethical factors helps lessen bias when applied in varied contexts.", "label": "Supported", "paragraph": "The selction of Moral Foundations As delineated in our paper, the five foundations (care, fairness, loyalty, authority and sanctity) and their interpretations are rooted in the well-established Moral Foundations Theory (MFT) (Haidt &amp; Joseph, 2004) in social psychology, intended to explain the intercultural origins of and variation in human moral reasoning, instead of being selected or proposed by us.We utilize MFT, as we mentioned in 2, as an example for our DeNEVIL algorithm primarily for its cross-cultural universality(Graham et al., 2013; Yilmaz et al., 2016; Hu et al., 2020) and emphasis on morality, helping avoid the bias when applied to diverse scenarios.", "section_name": "A DETAILS OF DATASET CONSTRUCTION", "subsection_name": "", "paper_id": "m3RRWWFaVe", "paper_title": "DENEVIL: TOWARDS DECIPHERING AND NAVIGATING THE ETHICAL VALUES OF LARGE LANGUAGE MODELS VIA INSTRUCTION LEARNING", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:20:52.473731", "model": "gpt-5-mini", "original_claim": "They used Moral Foundations Theory as the example framework for their DeNEVIL algorithm mainly because MFT has documented cross-cultural applicability and emphasizes morality, helping reduce bias when applied to varied scenarios.", "rephrasing_timestamp": "2026-01-11T19:17:30.155297", "rephrasing_model": "gpt-5-mini"}
{"claim": "Harper & Harris (2017), Tatalovich & Wendell (2018), and Atari et al. (2020) report that employing the moral-values model within a range of scholarly fields supports its credibility and real-world applicability.", "label": "Supported", "paragraph": "Furthermore, MFT's utilization across multiple disciplines (Harper &amp; Harris, 2017; Tatalovich &amp; Wendell, 2018; Atari et al., 2020) have demonstrated its validity and practical practicality. A comprehensive exploration of optimal value systems/principles falls into the realm of humanity and social science, and is beyond the scope of this work.", "section_name": "A DETAILS OF DATASET CONSTRUCTION", "subsection_name": "", "paper_id": "m3RRWWFaVe", "paper_title": "DENEVIL: TOWARDS DECIPHERING AND NAVIGATING THE ETHICAL VALUES OF LARGE LANGUAGE MODELS VIA INSTRUCTION LEARNING", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:21:00.110474", "model": "gpt-5-mini", "original_claim": "Applications of Moral Foundations Theory across multiple academic disciplines, documented by Harper & Harris (2017), Tatalovich & Wendell (2018), and Atari et al. (2020), have evidenced its validity and practical usefulness.", "rephrasing_timestamp": "2026-01-11T19:17:30.832794", "rephrasing_model": "gpt-5-mini"}
{"claim": "Figure 15 shows QRA’s performance on the CIFAR‑10 benchmark when adversarial interference corrupted one percent and ten percent of the dataset.", "label": "Supported", "paragraph": "In this section, we evaluate the QRA under various attack configurations on the CIFAR-10 dataset. Specifically, we include additional poisoning rates ( 1% and 10% ) in Figure 15, and two model architectures (ResNet-50 and DenseNet-161) in Figure 16.", "section_name": "C.3 Additional Results of QRA", "subsection_name": "", "paper_id": "qZFshkbWDo", "paper_title": "Uncovering, Explaining, and Mitigating the Superficial Safety of Backdoor Defense", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:21:25.801199", "model": "gpt-5-mini", "original_claim": "The QRA was assessed on the CIFAR-10 dataset under attack configurations that added data poisoning rates of 1% and 10%, with results shown in Figure 15.", "rephrasing_timestamp": "2026-01-11T19:17:35.492536", "rephrasing_model": "gpt-5-mini"}
{"claim": "Two network models—ResNet-50 and DenseNet-161—were used to evaluate QRA on the CIFAR-10 benchmark, with the findings shown in Figure 16.", "label": "Supported", "paragraph": "In this section, we evaluate the QRA under various attack configurations on the CIFAR-10 dataset. Specifically, we include additional poisoning rates ( 1% and 10% ) in Figure 15, and two model architectures (ResNet-50 and DenseNet-161) in Figure 16.", "section_name": "C.3 Additional Results of QRA", "subsection_name": "", "paper_id": "qZFshkbWDo", "paper_title": "Uncovering, Explaining, and Mitigating the Superficial Safety of Backdoor Defense", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:21:25.801212", "model": "gpt-5-mini", "original_claim": "The evaluation included two neural network architectures, ResNet-50 and DenseNet-161, used to test QRA performance on CIFAR-10 and depicted in Figure 16.", "rephrasing_timestamp": "2026-01-11T19:17:32.824590", "rephrasing_model": "gpt-5-mini"}
{"claim": "The paper reports experimental findings on the method's behavior when evaluated against two backdoor attack variants, BadNet and Blended.", "label": "Supported", "paragraph": "We report the performance against two attacks, namely the BadNet and Blended. Note that for certain defense methods, their ASR after applying RA is low, making it hard to perform QRA evaluations on them.", "section_name": "C.3 Additional Results of QRA", "subsection_name": "", "paper_id": "qZFshkbWDo", "paper_title": "Uncovering, Explaining, and Mitigating the Superficial Safety of Backdoor Defense", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:21:33.020354", "model": "gpt-5-mini", "original_claim": "The study presents performance results measured against two attacks referred to as BadNet and Blended.", "rephrasing_timestamp": "2026-01-11T19:17:32.074373", "rephrasing_model": "gpt-5-mini"}
{"claim": "Some protection techniques, when tested with randomized methods, produce very few successful attacks, which makes it hard to perform query-focused robustness evaluations on them.", "label": "Supported", "paragraph": "We report the performance against two attacks, namely the BadNet and Blended. Note that for certain defense methods, their ASR after applying RA is low, making it hard to perform QRA evaluations on them.", "section_name": "C.3 Additional Results of QRA", "subsection_name": "", "paper_id": "qZFshkbWDo", "paper_title": "Uncovering, Explaining, and Mitigating the Superficial Safety of Backdoor Defense", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:21:33.020373", "model": "gpt-5-mini", "original_claim": "For some defense approaches, applying RA yields a low ASR, which makes conducting QRA evaluations on those defenses difficult.", "rephrasing_timestamp": "2026-01-11T19:17:37.523588", "rephrasing_model": "gpt-5-mini"}
{"claim": "The LMC approach was empirically tested on the 100-class CIFAR image benchmark using an 18-layer residual network, with five percent of the training samples poisoned.", "label": "Supported", "paragraph": "Therefore, we exclude these defense methods from our evaluation and represent them as light gray in the Figure. Experimental\n<!-- image -->\nFigure 12: The experimental results of LMC on CIFAR-100. We evaluate the performance on ResNet-18 and set the poisoning rate to 5% .", "section_name": "C.3 Additional Results of QRA", "subsection_name": "", "paper_id": "qZFshkbWDo", "paper_title": "Uncovering, Explaining, and Mitigating the Superficial Safety of Backdoor Defense", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:21:38.724817", "model": "gpt-5-mini", "original_claim": "The LMC method was evaluated experimentally on the CIFAR-100 dataset using a ResNet-18 model with the poisoning rate set to 5%.", "rephrasing_timestamp": "2026-01-11T19:17:41.196906", "rephrasing_model": "gpt-5-mini"}
{"claim": "Each model was trained using just one of the two collections—RealEstate10K or ACID—and then tested on the opposite collection with no further training or parameter adjustments.", "label": "Supported", "paragraph": "To further demonstrate the generalization of Gaussian Graph Network, we conduct cross-dataset experiments. Specifically, all models are trained on RealEstate10K [64] or ACID [24] datasets, and are tested on the other dataset without any fine-tuning. Our method constructs the Gaussian Graph according to the relations of input views. As shown in Table 4, our GGN consistently outperforms pixelSplat [4] and MVSplat [6] on both benchmarks.", "section_name": "4.3 Cross Dataset Generalization.", "subsection_name": "", "paper_id": "2dfBpyqh0A", "paper_title": "Gaussian Graph Network: Learning Efficient and Generalizable Gaussian Representations from Multi-view Images", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:21:59.122644", "model": "gpt-5-mini", "original_claim": "All models were trained exclusively on either the RealEstate10K dataset or the ACID dataset and then evaluated on the alternate dataset with no fine-tuning applied.", "rephrasing_timestamp": "2026-01-11T19:17:40.019609", "rephrasing_model": "gpt-5-mini"}
{"claim": "Table 4 shows that, in experiments where models are trained and evaluated on different datasets, the Gaussian Graph Network achieves higher accuracy than both pixelSplat and MVSplat on the two benchmark datasets.", "label": "Supported", "paragraph": "To further demonstrate the generalization of Gaussian Graph Network, we conduct cross-dataset experiments. Specifically, all models are trained on RealEstate10K [64] or ACID [24] datasets, and are tested on the other dataset without any fine-tuning. Our method constructs the Gaussian Graph according to the relations of input views. As shown in Table 4, our GGN consistently outperforms pixelSplat [4] and MVSplat [6] on both benchmarks.", "section_name": "4.3 Cross Dataset Generalization.", "subsection_name": "", "paper_id": "2dfBpyqh0A", "paper_title": "Gaussian Graph Network: Learning Efficient and Generalizable Gaussian Representations from Multi-view Images", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:21:59.122656", "model": "gpt-5-mini", "original_claim": "In cross-dataset evaluations reported in Table 4, the Gaussian Graph Network achieves better performance than both pixelSplat and MVSplat on the two benchmark datasets.", "rephrasing_timestamp": "2026-01-11T19:17:44.162918", "rephrasing_model": "gpt-5-mini"}
{"claim": "Given twenty-four input views, the network built on Gaussian graphs delivers a peak signal-to-noise ratio that surpasses MVSplat's by 8.6 dB.", "label": "Supported", "paragraph": "In addition to delivering superior rendering quality, our Gaussian Graph network also enhances efficiency in 3D representations. As illustrated in Figure 4, when using 24 input images, our model outperforms MVSplat by 8.6dB on PSNR with approximately one-tenth the number of 3D Gaussians and more than three times faster rendering speed.", "section_name": "4.2 Efficiency Analysis.", "subsection_name": "", "paper_id": "2dfBpyqh0A", "paper_title": "Gaussian Graph Network: Learning Efficient and Generalizable Gaussian Representations from Multi-view Images", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:22:14.692726", "model": "gpt-5-mini", "original_claim": "With 24 input images, the Gaussian Graph network attains a PSNR that is 8.6 dB greater than the PSNR achieved by MVSplat.", "rephrasing_timestamp": "2026-01-11T19:17:42.957875", "rephrasing_model": "gpt-5-mini"}
{"claim": "With twenty-four viewpoints, the approach needs only about ten percent of the three-dimensional Gaussian kernels MVSplat uses and delivers image rendering at over three times the rate.", "label": "Supported", "paragraph": "In addition to delivering superior rendering quality, our Gaussian Graph network also enhances efficiency in 3D representations. As illustrated in Figure 4, when using 24 input images, our model outperforms MVSplat by 8.6dB on PSNR with approximately one-tenth the number of 3D Gaussians and more than three times faster rendering speed.", "section_name": "4.2 Efficiency Analysis.", "subsection_name": "", "paper_id": "2dfBpyqh0A", "paper_title": "Gaussian Graph Network: Learning Efficient and Generalizable Gaussian Representations from Multi-view Images", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:22:14.692747", "model": "gpt-5-mini", "original_claim": "Using 24 input views, the model requires roughly one-tenth the number of 3D Gaussian primitives compared to MVSplat and produces rendered images at over three times the speed.", "rephrasing_timestamp": "2026-01-11T19:17:46.658024", "rephrasing_model": "gpt-5-mini"}
{"claim": "By pruning redundant Gaussian primitives and strengthening their mutual interactions, our proposed network produces more accurate scene reconstructions than techniques that operate on individual pixels, while keeping inference time comparable to the MVSplat baseline.", "label": "Supported", "paragraph": "Additionally, we compare the average inference time of our model with pixel-wise methods in Table 3. Our GGN is able to efficiently remove duplicated Gaussians and enhance Gaussian-level interactions, which allows it to achieve superior reconstruction performance with comparable inference speed to MVSplat.", "section_name": "4.2 Efficiency Analysis.", "subsection_name": "", "paper_id": "2dfBpyqh0A", "paper_title": "Gaussian Graph Network: Learning Efficient and Generalizable Gaussian Representations from Multi-view Images", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:22:25.929782", "model": "gpt-5-mini", "original_claim": "Our GGN attains higher reconstruction quality than pixel-wise methods while maintaining inference latency similar to MVSplat by removing duplicate Gaussians and improving Gaussian-level interactions.", "rephrasing_timestamp": "2026-01-11T19:17:50.751457", "rephrasing_model": "gpt-5-mini"}
{"claim": "This section offers a thorough empirical study of the proposed video labeling technique, detailing how it operates and measuring its effectiveness across a range of experimental setups.", "label": "Supported", "paragraph": "In this part of the paper, we undertake an extensive analysis of our video semantic segmentation approach. The evaluation starts with an overview of the datasets utilized, along with a detailed breakdown of the models and settings implemented.", "section_name": "4 Experiments", "subsection_name": "", "paper_id": "paobkszgIA", "paper_title": "End-to-End Video Semantic Segmentation in Adverse Weather using Fusion Blocks and Temporal-Spatial Teacher-Student Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:22:44.732647", "model": "gpt-5-mini", "original_claim": "In this section, the authors conduct a comprehensive analysis of their video semantic segmentation method, examining its behavior and performance across multiple experimental evaluations.", "rephrasing_timestamp": "2026-01-11T19:17:51.413057", "rephrasing_model": "gpt-5-mini"}
{"claim": "The assessment opens with an overview of the data collections examined and then presents a comprehensive account of the models developed, describing the testing arrangements and the chosen parameter values.", "label": "Supported", "paragraph": "In this part of the paper, we undertake an extensive analysis of our video semantic segmentation approach. The evaluation starts with an overview of the datasets utilized, along with a detailed breakdown of the models and settings implemented.", "section_name": "4 Experiments", "subsection_name": "", "paper_id": "paobkszgIA", "paper_title": "End-to-End Video Semantic Segmentation in Adverse Weather using Fusion Blocks and Temporal-Spatial Teacher-Student Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:22:44.732658", "model": "gpt-5-mini", "original_claim": "The evaluation begins with a summary of the datasets used and provides a detailed description of the implemented models along with their experimental configurations and settings.", "rephrasing_timestamp": "2026-01-11T19:17:51.297092", "rephrasing_model": "gpt-5-mini"}
{"claim": "By putting their proposed technique through extensive trials, the researchers provided a meticulous account that demonstrates the solution’s strengths and robustness across the diverse conditions they examined.", "label": "Supported", "paragraph": "Subsequently, we delve into a detailed examination of our approach, showcasing its capabilities and robustness against a range of challenging weather conditions through both quantitative metrics and qualitative examples.", "section_name": "4 Experiments", "subsection_name": "", "paper_id": "paobkszgIA", "paper_title": "End-to-End Video Semantic Segmentation in Adverse Weather using Fusion Blocks and Temporal-Spatial Teacher-Student Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:22:54.273711", "model": "gpt-5-mini", "original_claim": "The authors conducted a comprehensive evaluation of their approach, presenting a detailed analysis that highlights the method's capabilities and resilience across the scenarios they assessed.", "rephrasing_timestamp": "2026-01-11T19:17:52.588265", "rephrasing_model": "gpt-5-mini"}
{"claim": "They confirmed the technique’s reliability across a variety of challenging weather conditions by providing quantitative evaluation figures together with representative images to convey the outcomes.", "label": "Supported", "paragraph": "Subsequently, we delve into a detailed examination of our approach, showcasing its capabilities and robustness against a range of challenging weather conditions through both quantitative metrics and qualitative examples.", "section_name": "4 Experiments", "subsection_name": "", "paper_id": "paobkszgIA", "paper_title": "End-to-End Video Semantic Segmentation in Adverse Weather using Fusion Blocks and Temporal-Spatial Teacher-Student Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:22:54.273726", "model": "gpt-5-mini", "original_claim": "They demonstrated the approach's robustness to multiple adverse weather scenarios by reporting numerical performance metrics alongside visual qualitative examples to illustrate results.", "rephrasing_timestamp": "2026-01-11T19:17:55.800990", "rephrasing_model": "gpt-5-mini"}
{"claim": "We chose the VIPER and SYNTHIA collections as training sources for our per-frame labeling experiments because both contain extensive sets of annotated, computer-generated images of city scenes.", "label": "Supported", "paragraph": "To conclude, we engage in ablation studies to discern the impact and necessity of the distinct components integral to our method. Datasets For our source datasets in the video semantic segmentation work, we select VIPER [28] and Synthia [29] for their extensive collections of labeled, synthetic urban landscape frames.", "section_name": "4 Experiments", "subsection_name": "", "paper_id": "paobkszgIA", "paper_title": "End-to-End Video Semantic Segmentation in Adverse Weather using Fusion Blocks and Temporal-Spatial Teacher-Student Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:22:58.736824", "model": "gpt-5-mini", "original_claim": "For the video semantic segmentation experiments, we used the VIPER and Synthia datasets as source datasets because each provides large collections of labeled synthetic frames depicting urban landscapes.", "rephrasing_timestamp": "2026-01-11T19:17:59.263548", "rephrasing_model": "gpt-5-mini"}
{"claim": "If additive offset terms are included, the two-layer network has a first-layer offset b1 that is a real-valued vector of length l and a second-layer offset b2 that is a real-valued vector of length e.", "label": "Supported", "paragraph": "Let us consider the case where we include biases. The resulting two-layer neural network can be written as\n<!-- formula-not-decoded -->\nwhere b ( 1 ) ∈ R l and b ( 2 ) ∈ R e . To work with this extended set of parameters, we re-define the space\n<!-- formula-not-decoded -->\nand the single hidden neuron spaces\n<!-- formula-not-decoded -->\nwhere the second bias term b ( 2 ) does not appear because it is not directly involved with the computations of the hidden neurons.", "section_name": "E Including biases", "subsection_name": "", "paper_id": "3hcn0UxP72", "paper_title": "Topological obstruction to the training of shallow ReLU neural networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:23:20.892736", "model": "gpt-5-mini", "original_claim": "When biases are included, the two-layer neural network uses a first-layer bias b(1) that is an l-dimensional real vector and a second-layer bias b(2) that is an e-dimensional real vector.", "rephrasing_timestamp": "2026-01-11T19:18:02.223711", "rephrasing_model": "gpt-5-mini"}
{"claim": "In the reformulated parameter set for a network with only one hidden unit, the output-layer bias is omitted since it does not influence the calculation of the hidden unit’s activation.", "label": "Supported", "paragraph": "Let us consider the case where we include biases. The resulting two-layer neural network can be written as\n<!-- formula-not-decoded -->\nwhere b ( 1 ) ∈ R l and b ( 2 ) ∈ R e . To work with this extended set of parameters, we re-define the space\n<!-- formula-not-decoded -->\nand the single hidden neuron spaces\n<!-- formula-not-decoded -->\nwhere the second bias term b ( 2 ) does not appear because it is not directly involved with the computations of the hidden neurons.", "section_name": "E Including biases", "subsection_name": "", "paper_id": "3hcn0UxP72", "paper_title": "Topological obstruction to the training of shallow ReLU neural networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:23:20.892755", "model": "gpt-5-mini", "original_claim": "In the redefined single-hidden-neuron parameter spaces, the second-layer bias b(2) is excluded because it does not take part in the computations that determine hidden neuron outputs.", "rephrasing_timestamp": "2026-01-11T19:17:57.130148", "rephrasing_model": "gpt-5-mini"}
{"claim": "R_e is introduced into the construction so that it provides values for the entries of the second layer's bias, thus incorporating those bias variables into the model's overall parameter description.", "label": "Supported", "paragraph": "This means that we can write\n<!-- formula-not-decoded -->\nwhere R e is included to describe the parameters in b ( 2 ) . The neuron rescaling action now acts on the biases b ( 1 ) as well as the weights:\n<!-- formula-not-decoded -->\nand can be extended to the whole space of parameters\n<!-- formula-not-decoded -->\nOnce again, we find that T α θ ∼ θ .", "section_name": "E Including biases", "subsection_name": "", "paper_id": "3hcn0UxP72", "paper_title": "Topological obstruction to the training of shallow ReLU neural networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:23:29.450672", "model": "gpt-5-mini", "original_claim": "The group R_e is incorporated into the formulation to parametrize the components of the second-layer bias vector b(2), thereby accounting for those bias parameters within the described parameter representation.", "rephrasing_timestamp": "2026-01-11T19:17:58.562565", "rephrasing_model": "gpt-5-mini"}
{"claim": "The scaling operation on units modifies both the biases in the initial layer and the model’s weight parameters, can be defined for every point in parameter space, and produces a transformed parameter vector that implements the same input–output mapping as the original θ.", "label": "Supported", "paragraph": "This means that we can write\n<!-- formula-not-decoded -->\nwhere R e is included to describe the parameters in b ( 2 ) . The neuron rescaling action now acts on the biases b ( 1 ) as well as the weights:\n<!-- formula-not-decoded -->\nand can be extended to the whole space of parameters\n<!-- formula-not-decoded -->\nOnce again, we find that T α θ ∼ θ .", "section_name": "E Including biases", "subsection_name": "", "paper_id": "3hcn0UxP72", "paper_title": "Topological obstruction to the training of shallow ReLU neural networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:23:29.450688", "model": "gpt-5-mini", "original_claim": "The neuron-rescaling transformation acts on both the first-layer bias vector b(1) and the network weights, can be extended across the entire parameter space, and yields T_α(θ) that is equivalent to θ.", "rephrasing_timestamp": "2026-01-11T19:18:04.605886", "rephrasing_model": "gpt-5-mini"}
{"claim": "Q(c_k) is the locus inside Θ_k made up of those θ whose inner product ⟪θ,θ⟫_k equals c_k. H(c_k) is the collection of elements of Θ that satisfy ⟪θ,θ⟫_k = c_k simultaneously for every index k.", "label": "Supported", "paragraph": "In this more general case, we can rewrite the bilinear form to include the biases. If θ = ( W ( 1 ) , b ( 1 ) , W ( 2 ) , b ( 2 ) ) and η = ( V ( 1 ) , p ( 1 ) , V ( 2 ) , p ( 2 ) ) , we define\n<!-- formula-not-decoded -->\nand see that, once gradient flow optimization, we have a conservation condition like the one of Equation (9)\n<!-- formula-not-decoded -->\nOnce again, we call Q( c k ) the hypersurface of Θ k which satisfies the equation ⟪ θ, θ ⟫ k = c k and H( c k ) the set in Θ defined by ⟪ θ, θ ⟫ k = c k ∀ k = 1 , .", "section_name": "E Including biases", "subsection_name": "", "paper_id": "3hcn0UxP72", "paper_title": "Topological obstruction to the training of shallow ReLU neural networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:23:39.091042", "model": "gpt-5-mini", "original_claim": "Q(c_k) denotes the hypersurface within Θ_k of parameters satisfying ⟪θ,θ⟫_k = c_k, while H(c_k) denotes the subset of Θ where ⟪θ,θ⟫_k = c_k holds for every k.", "rephrasing_timestamp": "2026-01-11T19:18:07.215712", "rephrasing_model": "gpt-5-mini"}
{"claim": "Provided assumptions (A1)–(A6) hold and the run is well-behaved, the decision rule that assigns a positive label when f(x, W(t))>0 and a negative label when f(x, W(t))<0 recovers the true label for every example in the training set.", "label": "Supported", "paragraph": "Theorem A.8. Suppose that Assumptions (A1)-(A6) hold. Under a good run, the classifier sgn( f ( x, W ( t ) )) can correctly classify all training datapoints for 1 ≤ t ≤ 1 / ( √ npα ) -2 . Proof. Without loss of generality, we only consider datapoints in the cluster C + µ 1 ∪N + µ 1 .", "section_name": "A.5.1 PROOF OF THEOREM A.8: 1-STEP OVERFITTING", "subsection_name": "", "paper_id": "BxHgpC6FNv", "paper_title": "Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:24:09.313611", "model": "gpt-5-mini", "original_claim": "Under Assumptions (A1)–(A6) and given a good run, the classifier defined by the sign of f(x, W(t)) correctly labels every training datapoint.", "rephrasing_timestamp": "2026-01-11T19:18:07.264408", "rephrasing_model": "gpt-5-mini"}
{"claim": "For each time t with 1 ≤ t ≤ 1/(α√(n p)) − 2, the learner achieves zero error on the training set.", "label": "Supported", "paragraph": "Theorem A.8. Suppose that Assumptions (A1)-(A6) hold. Under a good run, the classifier sgn( f ( x, W ( t ) )) can correctly classify all training datapoints for 1 ≤ t ≤ 1 / ( √ npα ) -2 . Proof. Without loss of generality, we only consider datapoints in the cluster C + µ 1 ∪N + µ 1 .", "section_name": "A.5.1 PROOF OF THEOREM A.8: 1-STEP OVERFITTING", "subsection_name": "", "paper_id": "BxHgpC6FNv", "paper_title": "Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:24:09.313623", "model": "gpt-5-mini", "original_claim": "The classifier's perfect training accuracy holds for every time t in the interval 1 ≤ t ≤ 1/(sqrt(n p) α) − 2.", "rephrasing_timestamp": "2026-01-11T19:18:08.481155", "rephrasing_model": "gpt-5-mini"}
{"claim": "If the execution is well-behaved, then for every i = 1,…,n the count of indices that start in the positive group and the count that start in the negative group are each no smaller than m/7.", "label": "Supported", "paragraph": "According to (D1) in Lemma 4.4, we have that under a good run, |J i, (0) P | ≥ m/ 7 , |J i, (0) N | ≥ m/ 7 for each i ∈ [ n ] . For x k ∈ C + µ 1 , by Corollary A.5, we have\n<!-- formula-not-decoded -->\nfor all j ∈ J k, (0) and 0 ≤ s ≤ 1 / ( npα ) -2 ; and\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nfor all √ √\n<!-- formula-not-decoded -->\nwhere the first inequality uses ϕ ( x ) ≥ 0 , ∀ x ; the second inequality uses the definition of J k, (0) P and (E2) in Corollary A.5; the third inequality uses (A.36) in Corollary A.5; and the last inequality is from Assumption (A2).", "section_name": "A.5.1 PROOF OF THEOREM A.8: 1-STEP OVERFITTING", "subsection_name": "", "paper_id": "BxHgpC6FNv", "paper_title": "Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:24:24.216002", "model": "gpt-5-mini", "original_claim": "Under a good run, for every i in [n], the sizes of the initial positive and initial negative index sets are each at least m/7.", "rephrasing_timestamp": "2026-01-11T19:18:11.229833", "rephrasing_model": "gpt-5-mini"}
{"claim": "Whenever an observation x_k lies in C_+^{μ1}, Corollary A.5 ensures that certain inequality bounds are satisfied for each index j from the starting index set J_{k,(0)} and for every s with 0 ≤ s ≤ 1/(n p α) − 2.", "label": "Supported", "paragraph": "According to (D1) in Lemma 4.4, we have that under a good run, |J i, (0) P | ≥ m/ 7 , |J i, (0) N | ≥ m/ 7 for each i ∈ [ n ] . For x k ∈ C + µ 1 , by Corollary A.5, we have\n<!-- formula-not-decoded -->\nfor all j ∈ J k, (0) and 0 ≤ s ≤ 1 / ( npα ) -2 ; and\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nfor all √ √\n<!-- formula-not-decoded -->\nwhere the first inequality uses ϕ ( x ) ≥ 0 , ∀ x ; the second inequality uses the definition of J k, (0) P and (E2) in Corollary A.5; the third inequality uses (A.36) in Corollary A.5; and the last inequality is from Assumption (A2).", "section_name": "A.5.1 PROOF OF THEOREM A.8: 1-STEP OVERFITTING", "subsection_name": "", "paper_id": "BxHgpC6FNv", "paper_title": "Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:24:24.216018", "model": "gpt-5-mini", "original_claim": "For any data point x_k in C_+^{μ1}, Corollary A.5 guarantees specific inequalities for every index j in the initial index set J_{k,(0)} and for all s with 0 ≤ s ≤ 1/(n p α) - 2.", "rephrasing_timestamp": "2026-01-11T19:18:14.658197", "rephrasing_model": "gpt-5-mini"}
{"claim": "Whenever t lies in the interval [1, 1/(√(n p α)) − 2], the predictor labels all training samples correctly (producing zero training error).", "label": "Supported", "paragraph": "For x k ∈ N + µ 1 , similarly we have\n<!-- formula-not-decoded -->\nThus our classifier can correctly classify all training datapoints for 1 ≤ t ≤ 1 / ( √ npα ) -2 .", "section_name": "A.5.1 PROOF OF THEOREM A.8: 1-STEP OVERFITTING", "subsection_name": "", "paper_id": "BxHgpC6FNv", "paper_title": "Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:24:32.797875", "model": "gpt-5-mini", "original_claim": "The classifier attains perfect classification of every training datapoint for all t values satisfying 1 ≤ t ≤ 1/(√(n p α)) - 2.", "rephrasing_timestamp": "2026-01-11T19:18:15.940311", "rephrasing_model": "gpt-5-mini"}
{"claim": "For every i, viewing \\hat{L}_{α_i} as a mapping in the variable \\hat{p}_i yields a convex function that attains its sole minimum at \\hat{p}_i^* = α_i.", "label": "Supported", "paragraph": "Proposition D.5. (Originally Proposition 2.3) Given α i , ˆ L α i ( ˆ p i ) is a convex function of ˆ p i with unique minimizer ˆ p ∗ i = α i . Let ∆( α ) = ˆ L α ( α ) be a function of α , ∆ is a concave function with unique maximizer α ∗ = 1 n 1 .", "section_name": "D.1.2 PROOF OF PROPOSITION 2.3", "subsection_name": "", "paper_id": "wYvuY60SdD", "paper_title": "Mixture of Weak and Strong Experts on Graphs", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:25:01.658278", "model": "gpt-5-mini", "original_claim": "For each index i, the function \\hat{L}_{α_i} viewed as a function of \\hat{p}_i is convex and possesses a unique minimizer \\hat{p}_i^* which equals α_i.", "rephrasing_timestamp": "2026-01-11T19:18:16.977050", "rephrasing_model": "gpt-5-mini"}
{"claim": "Let Δ be the map that assigns to each α the value of \\hat{L}_α evaluated at α. Δ is concave and reaches its maximum at a unique point α^*, which is the vector with every entry equal to 1/n.", "label": "Supported", "paragraph": "Proposition D.5. (Originally Proposition 2.3) Given α i , ˆ L α i ( ˆ p i ) is a convex function of ˆ p i with unique minimizer ˆ p ∗ i = α i . Let ∆( α ) = ˆ L α ( α ) be a function of α , ∆ is a concave function with unique maximizer α ∗ = 1 n 1 .", "section_name": "D.1.2 PROOF OF PROPOSITION 2.3", "subsection_name": "", "paper_id": "wYvuY60SdD", "paper_title": "Mixture of Weak and Strong Experts on Graphs", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:25:01.658290", "model": "gpt-5-mini", "original_claim": "If Δ(α) is defined as \\hat{L}_α evaluated at α, then Δ is a concave function that has a single maximizer α^* equal to the uniform vector (1/n)·1.", "rephrasing_timestamp": "2026-01-11T19:18:20.482602", "rephrasing_model": "gpt-5-mini"}
{"claim": "For each index i, the j-th entry of the optimal vector p̂*_i is α_{ij} divided by the total of that row's α-values, i.e. p̂*_{ij} = α_{ij} / (∑_{ℓ=1}^n α_{iℓ}).", "label": "Supported", "paragraph": "Proof. We can formulate the problem of finding the minimizer ˆ p ∗ i of ˆ L α i ( ˆ p i ) as\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nThe Larangian is\n<!-- formula-not-decoded -->\nApplying the KKT condition (Bertsekas, 2009), we have\n<!-- formula-not-decoded -->\nSolving the system of equations in 12, we have ˆ p ∗ ij = α ij / ( ∑ 1 ≤ j ≤ n α ij ) .", "section_name": "D.1.2 PROOF OF PROPOSITION 2.3", "subsection_name": "", "paper_id": "wYvuY60SdD", "paper_title": "Mixture of Weak and Strong Experts on Graphs", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:25:10.191520", "model": "gpt-5-mini", "original_claim": "Each component of the optimizer ˆp*_{i} is given by α_{ij} normalized by the sum of α_{ij} over j, i.e., ˆp*_{ij} = α_{ij} / (Σ_{j=1}^{n} α_{ij}).", "rephrasing_timestamp": "2026-01-11T19:18:20.598775", "rephrasing_model": "gpt-5-mini"}
{"claim": "The proof introduces an auxiliary function that combines the objective with multiplier-weighted constraints and then applies the standard necessary conditions for constrained optima (cf. Bertsekas, 2009) to solve the resulting system of equations, yielding an explicit analytical expression.", "label": "Supported", "paragraph": "Proof. We can formulate the problem of finding the minimizer ˆ p ∗ i of ˆ L α i ( ˆ p i ) as\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nThe Larangian is\n<!-- formula-not-decoded -->\nApplying the KKT condition (Bertsekas, 2009), we have\n<!-- formula-not-decoded -->\nSolving the system of equations in 12, we have ˆ p ∗ ij = α ij / ( ∑ 1 ≤ j ≤ n α ij ) .", "section_name": "D.1.2 PROOF OF PROPOSITION 2.3", "subsection_name": "", "paper_id": "wYvuY60SdD", "paper_title": "Mixture of Weak and Strong Experts on Graphs", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:25:10.191541", "model": "gpt-5-mini", "original_claim": "The proof constructs the Lagrangian for the constrained minimization and employs the Karush–Kuhn–Tucker conditions (citing Bertsekas, 2009) to solve the optimality system and obtain the closed-form solution.", "rephrasing_timestamp": "2026-01-11T19:18:23.279380", "rephrasing_model": "gpt-5-mini"}
{"claim": "Because the function L̂ associated with α_i curves upward everywhere, it attains a single lowest point at α_i; hence the value of Δ at α_i is −∑_{j=1}^n α_{ij} log α_{ij}, which is the Shannon entropy of α_i.", "label": "Supported", "paragraph": "Since α i forms a distribution, we further have ˆ p ∗ ij = α ij , i.e.,\n<!-- formula-not-decoded -->\nSince ˆ L α i is also strictly convex (Proposition D.7), its minimizer is unique. In particular, ∆( α i ) = ˆ L α i ( α i ) = -∑ 1 ≤ j ≤ n α ij · log α ij = H ( α i ) .", "section_name": "D.1.2 PROOF OF PROPOSITION 2.3", "subsection_name": "", "paper_id": "wYvuY60SdD", "paper_title": "Mixture of Weak and Strong Experts on Graphs", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:25:21.685575", "model": "gpt-5-mini", "original_claim": "Because the function ˆL_{α_i} is strictly convex, it has a unique minimizer at α_i, and Δ(α_i) equals -∑_{j=1}^n α_{ij} log α_{ij}, i.e., the entropy H(α_i).", "rephrasing_timestamp": "2026-01-11T19:18:31.341132", "rephrasing_model": "gpt-5-mini"}
{"claim": "Hao Deng is reported as being associated with a state–province collaborative engineering and research hub focused on cutting‑edge networking and intelligent information services, as well as a Shaanxi‑designated key laboratory housed in Northwest University's Faculty of Information Science and Technology.", "label": "Supported", "paragraph": "```\nHao Deng 1 , 2 Kunlei Jing 3 , 4 ∗ Shengmei Cheng 1 , 2 Cheng Liu 1 , 2 Jiawei Ru 1 , 2 Jiang Bo 1 , 2 Lin Wang 1 , 2 ∗ 1 State-Province Joint Engineering and Research Center of Advanced Networking and Intelligent Information Services, School of Information Science and Technology, Northwest University 2 Shaanxi Key Laboratory of Higher Education Institution of Generative Artificial Intelligence and Mixed Reality 3 School of Software Engineering, Xi'an Jiaotong University 4 Department of Computing, The Hong Kong Polytechnic University denghao@stumail.nwu.edu.cn, kunlei.jing@xjtu.edu.cn 1615241805@qq.com, lc@nwu.edu.cn rujiawei@stumail.nwu.edu.cn, {jiangbo, wanglin}@nwu.edu.cn\n```", "section_name": "LinNet: Linear Network for Efficient Point Cloud Representation Learning", "subsection_name": "", "paper_id": "ehfCxpDsrw", "paper_title": "LinNet: Linear Network for Efficient Point Cloud Representation Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:25:44.914157", "model": "gpt-5-mini", "original_claim": "Hao Deng is listed as affiliated with the State-Province Joint Engineering and Research Center for Advanced Networking and Intelligent Information Services and the Shaanxi Key Laboratory at Northwest University's School of Information Science and Technology.", "rephrasing_timestamp": "2026-01-11T19:18:25.679575", "rephrasing_model": "gpt-5-mini"}
{"claim": "The authorship listing indicates that Kunlei Jing is associated with the software engineering faculty at Xi'an Jiaotong University and the computing division of The Hong Kong Polytechnic University.", "label": "Supported", "paragraph": "```\nHao Deng 1 , 2 Kunlei Jing 3 , 4 ∗ Shengmei Cheng 1 , 2 Cheng Liu 1 , 2 Jiawei Ru 1 , 2 Jiang Bo 1 , 2 Lin Wang 1 , 2 ∗ 1 State-Province Joint Engineering and Research Center of Advanced Networking and Intelligent Information Services, School of Information Science and Technology, Northwest University 2 Shaanxi Key Laboratory of Higher Education Institution of Generative Artificial Intelligence and Mixed Reality 3 School of Software Engineering, Xi'an Jiaotong University 4 Department of Computing, The Hong Kong Polytechnic University denghao@stumail.nwu.edu.cn, kunlei.jing@xjtu.edu.cn 1615241805@qq.com, lc@nwu.edu.cn rujiawei@stumail.nwu.edu.cn, {jiangbo, wanglin}@nwu.edu.cn\n```", "section_name": "LinNet: Linear Network for Efficient Point Cloud Representation Learning", "subsection_name": "", "paper_id": "ehfCxpDsrw", "paper_title": "LinNet: Linear Network for Efficient Point Cloud Representation Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:25:44.914169", "model": "gpt-5-mini", "original_claim": "Kunlei Jing holds affiliations with the School of Software Engineering at Xi'an Jiaotong University and with the Department of Computing at The Hong Kong Polytechnic University, as shown in the author list.", "rephrasing_timestamp": "2026-01-11T19:18:28.664751", "rephrasing_model": "gpt-5-mini"}
{"claim": "For each element x_i of the point set in three-dimensional space, x_i is described by the pair (p_i, f_i), where p_i is its three-component position vector in Euclidean 3-space and f_i is an attribute vector with c components (a c-dimensional vector).", "label": "Supported", "paragraph": "Given a 3D point cloud V = ( P , F ) consisting of n points x . For the i -th point x i = ( p i , f i ) , p i ∈ R 3 and f i ∈ R c are the space coordinates and features, respectively. The task of point cloud semantic segmentation involves assigning a class label to each point x i , while scene classification entails predicting a class label for the entire scene C .", "section_name": "3.1 Problem Formulation", "subsection_name": "", "paper_id": "ehfCxpDsrw", "paper_title": "LinNet: Linear Network for Efficient Point Cloud Representation Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:25:55.838483", "model": "gpt-5-mini", "original_claim": "Each point x_i in the 3D point cloud is represented as the ordered pair (p_i, f_i), where p_i is a three-dimensional spatial coordinate in R^3 and f_i is a c-dimensional feature vector in R^c.", "rephrasing_timestamp": "2026-01-11T19:18:31.865590", "rephrasing_model": "gpt-5-mini"}
{"claim": "Given a collection of 3D coordinates {x_i}, the goal is to determine which semantic group each coordinate belongs to, effectively tagging every element in the set with its corresponding class.", "label": "Supported", "paragraph": "Given a 3D point cloud V = ( P , F ) consisting of n points x . For the i -th point x i = ( p i , f i ) , p i ∈ R 3 and f i ∈ R c are the space coordinates and features, respectively. The task of point cloud semantic segmentation involves assigning a class label to each point x i , while scene classification entails predicting a class label for the entire scene C .", "section_name": "3.1 Problem Formulation", "subsection_name": "", "paper_id": "ehfCxpDsrw", "paper_title": "LinNet: Linear Network for Efficient Point Cloud Representation Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:25:55.838503", "model": "gpt-5-mini", "original_claim": "Point cloud semantic segmentation assigns a class label to each individual point x_i in the point cloud, labeling every point with a category.", "rephrasing_timestamp": "2026-01-11T19:18:32.733412", "rephrasing_model": "gpt-5-mini"}
{"claim": "Techniques that categorize individual 3D samples typically operate across a sequence of phases, and at the beginning of each phase they carry out a subsampling step that picks a subset of the coordinates, making the spatial sampling sparser.", "label": "Supported", "paragraph": "The point-based methods usually employ several stages to classify the points or point clouds. In each stage, a downsample layer is first applied to sample the points, reducing the density of the point cloud.", "section_name": "3.1 Problem Formulation", "subsection_name": "", "paper_id": "ehfCxpDsrw", "paper_title": "LinNet: Linear Network for Efficient Point Cloud Representation Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:26:02.927849", "model": "gpt-5-mini", "original_claim": "Point-based classification approaches typically process data through multiple stages, and at the start of each stage they apply a downsampling layer that samples points and lowers point-cloud density.", "rephrasing_timestamp": "2026-01-11T19:18:31.966548", "rephrasing_model": "gpt-5-mini"}
{"claim": "For each state and action combination (s, a), the method obtains its count N(s, a) by looping i = 1 to n and incrementing by one whenever the observed pair (s_i, a_i) matches (s, a) (adding zero otherwise).", "label": "Supported", "paragraph": "- 1 Input: Offline rating dataset , confidence level δ (0 ↪ 1)\n- 6 return ̂ π LCB ( · ) = arg max a ∈ A ̂ r ( · ↪ a )\n```\nD ∈ 2 for all ( s↪ a ) ∈ S × A do 3 Set n ( s↪a ) = ∑ n i =1 1 ¶ ( s i ↪ a i ) = ( s↪ a ) ♦ ; 4 Set ˜ r ( s↪ a ) = 1 n ∑ n i =1 ˜ r i 1 ¶ ( s i ↪ a i ) = ( s↪ a ) ♦ ; 5 Set ̂ r ( s↪ a ) = max ¶ ˜ r ( s↪ a ) -b n ( s↪a ) ↪ 0 ♦ ; .", "section_name": "Algorithm 1: LCB for contextual bandits", "subsection_name": "", "paper_id": "XmkuQfWZAB", "paper_title": "On Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:26:22.169085", "model": "gpt-5-mini", "original_claim": "For every state-action pair (s↦a), the algorithm computes its occurrence count n(s↦a) by summing, over i=1 to n, indicator functions that (s_i↦a_i) equals (s↦a).", "rephrasing_timestamp": "2026-01-11T19:18:37.781488", "rephrasing_model": "gpt-5-mini"}
{"claim": "For each state-action pair the method assigns an estimated payoff equal to the positive part of the observed mean minus a visit-dependent correction:\nr̂(s,a) = max(r̄(s,a) − c_{n(s,a)}, 0),\nwhere r̄(s,a) is the sample average reward for that pair and c_{n(s,a)} is a penalty determined by the number of times it was seen. The algorithm outputs the policy that, in every state, picks an action with the largest such estimate (i.e., is greedy with respect to r̂).", "label": "Supported", "paragraph": "- 1 Input: Offline rating dataset , confidence level δ (0 ↪ 1)\n- 6 return ̂ π LCB ( · ) = arg max a ∈ A ̂ r ( · ↪ a )\n```\nD ∈ 2 for all ( s↪ a ) ∈ S × A do 3 Set n ( s↪a ) = ∑ n i =1 1 ¶ ( s i ↪ a i ) = ( s↪ a ) ♦ ; 4 Set ˜ r ( s↪ a ) = 1 n ∑ n i =1 ˜ r i 1 ¶ ( s i ↪ a i ) = ( s↪ a ) ♦ ; 5 Set ̂ r ( s↪ a ) = max ¶ ˜ r ( s↪ a ) -b n ( s↪a ) ↪ 0 ♦ ; .", "section_name": "Algorithm 1: LCB for contextual bandits", "subsection_name": "", "paper_id": "XmkuQfWZAB", "paper_title": "On Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:26:22.169095", "model": "gpt-5-mini", "original_claim": "The algorithm computes r̂(s↦a)=max(˜r(s↦a)-b_{n(s↦a)},0), where ˜r is the empirical average reward for that state-action pair; it returns π̂_LCB by selecting actions that maximize r̂.", "rephrasing_timestamp": "2026-01-11T19:18:38.879898", "rephrasing_model": "gpt-5-mini"}
{"claim": "To examine how human bias and uncertainty influence learning decision policies, the authors establish a guaranteed minimum performance loss for a pessimistic confidence-bound method when evaluated under their more realistic feedback model.", "label": "Supported", "paragraph": "```\nTo understand the effects of human bias and uncertainty on policy learning under our more realistic rating model, let us establish the lower bound on the suboptimality of the LCB algorithm. We will consider two scenarios with different coverage assumptions for the offline dataset D .", "section_name": "Algorithm 1: LCB for contextual bandits", "subsection_name": "", "paper_id": "XmkuQfWZAB", "paper_title": "On Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:26:28.860217", "model": "gpt-5-mini", "original_claim": "The authors derive a lower bound on the suboptimality of the Lower Confidence Bound (LCB) algorithm under their more realistic rating model to study human bias and uncertainty effects on policy learning.", "rephrasing_timestamp": "2026-01-11T19:18:39.914110", "rephrasing_model": "gpt-5-mini"}
{"claim": "The work examines two alternative configurations that assume different degrees to which the historical dataset D represents the relevant state-action regions, and uses these configurations to assess the effectiveness of policies learned from that data.", "label": "Supported", "paragraph": "```\nTo understand the effects of human bias and uncertainty on policy learning under our more realistic rating model, let us establish the lower bound on the suboptimality of the LCB algorithm. We will consider two scenarios with different coverage assumptions for the offline dataset D .", "section_name": "Algorithm 1: LCB for contextual bandits", "subsection_name": "", "paper_id": "XmkuQfWZAB", "paper_title": "On Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:26:28.860232", "model": "gpt-5-mini", "original_claim": "The analysis considers two distinct scenarios that vary the coverage assumptions imposed on the offline dataset D when evaluating policy-learning performance.", "rephrasing_timestamp": "2026-01-11T19:18:40.456821", "rephrasing_model": "gpt-5-mini"}
{"claim": "This approach employs algorithms that learn how a human evaluator ranks several options by their utility, and it logs feedback as the evaluator’s single top choice from each set.", "label": "Supported", "paragraph": "In contrast to rating, the preference-based approach relies on models that characterize how a human annotator would rank a group of subjects by reward. In this case, the feedback is simply the most preferred subject to the human annotator within the group.", "section_name": "5 COMPARISON WITH PREFERENCE-BASED APPROACH", "subsection_name": "", "paper_id": "XmkuQfWZAB", "paper_title": "On Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:26:35.185898", "model": "gpt-5-mini", "original_claim": "The preference-based method uses models that capture how a human annotator orders multiple subjects by reward, and it records feedback as the annotator's single most preferred item from each group.", "rephrasing_timestamp": "2026-01-11T19:18:39.943786", "rephrasing_model": "gpt-5-mini"}
{"claim": "Preliminary experiments aim to determine a Walrasian (market‑clearing) solution for the pure‑exchange general equilibrium model introduced by Arrow and Debreu in 1954.", "label": "Supported", "paragraph": "<!-- formula-not-decoded -->\nArrow-Debreu exchange economies. 5 Our first set of experiments aim to solve CE in ArrowDebreu exchange economies (Arrow &amp; Debreu, 1954). The difficulty in solving the pseudo-game associated with Arrow-Debreu exchange economies-hereafter exchange economies -arises from the fact that it does not fit into any well-defined categories of pseudo-games, e.g., monotone or jointly convex, for which there are algorithms that converge to GNEs.", "section_name": "5 EXPERIMENTAL RESULTS", "subsection_name": "", "paper_id": "TlyiaPXaVN", "paper_title": "Generative Adversarial Equilibrium Solvers", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:26:49.720315", "model": "gpt-5-mini", "original_claim": "The initial experiments focus on computing a competitive equilibrium in Arrow–Debreu exchange economies described by Arrow and Debreu (1954).", "rephrasing_timestamp": "2026-01-11T19:18:47.070277", "rephrasing_model": "gpt-5-mini"}
{"claim": "Finding a solution for the auxiliary noncooperative formulation derived from the Arrow–Debreu exchange model is challenging because it fails to exhibit the usual structural properties—such as operator monotonicity or collective convexity—for which established iterative methods are guaranteed to produce equilibria in games with shared constraints.", "label": "Supported", "paragraph": "<!-- formula-not-decoded -->\nArrow-Debreu exchange economies. 5 Our first set of experiments aim to solve CE in ArrowDebreu exchange economies (Arrow &amp; Debreu, 1954). The difficulty in solving the pseudo-game associated with Arrow-Debreu exchange economies-hereafter exchange economies -arises from the fact that it does not fit into any well-defined categories of pseudo-games, e.g., monotone or jointly convex, for which there are algorithms that converge to GNEs.", "section_name": "5 EXPERIMENTAL RESULTS", "subsection_name": "", "paper_id": "TlyiaPXaVN", "paper_title": "Generative Adversarial Equilibrium Solvers", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:26:49.720327", "model": "gpt-5-mini", "original_claim": "Solving the pseudo-game associated with Arrow–Debreu exchange economies is difficult because it does not fall into standard pseudo-game categories like monotone or jointly convex, which have algorithms that converge to generalized Nash equilibria.", "rephrasing_timestamp": "2026-01-11T19:18:49.971854", "rephrasing_model": "gpt-5-mini"}
{"claim": "The model involves a positive-integer quantity of distinct commodities and a positive-integer quantity of agents, with both counts strictly greater than zero.", "label": "Supported", "paragraph": "An exchange economy ( u ↪ E ) consists of a finite set of m ∈ N + goods and n ∈ N + consumers (or traders). Every consumer i ∈ [ n ] has a set of possible consumptions X i ⊆ R m + , an endowment of goods e i = ( e i 1 ↪ glyph[triangleright] glyph[triangleright] glyph[triangleright] ↪ e im ) ∈ R m and a utility function u i : R m → R .", "section_name": "5 EXPERIMENTAL RESULTS", "subsection_name": "", "paper_id": "TlyiaPXaVN", "paper_title": "Generative Adversarial Equilibrium Solvers", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:27:02.398132", "model": "gpt-5-mini", "original_claim": "An exchange economy is specified by a finite number m of goods and a finite number n of consumers, with both m and n being positive integers.", "rephrasing_timestamp": "2026-01-11T19:18:49.326140", "rephrasing_model": "gpt-5-mini"}
{"claim": "For each agent i there is a set of attainable bundles contained in the collection of m‑tuples of nonnegative real numbers, an initial m‑component allocation of goods, and a real‑valued function that assigns a utility level to every m‑dimensional bundle.", "label": "Supported", "paragraph": "An exchange economy ( u ↪ E ) consists of a finite set of m ∈ N + goods and n ∈ N + consumers (or traders). Every consumer i ∈ [ n ] has a set of possible consumptions X i ⊆ R m + , an endowment of goods e i = ( e i 1 ↪ glyph[triangleright] glyph[triangleright] glyph[triangleright] ↪ e im ) ∈ R m and a utility function u i : R m → R .", "section_name": "5 EXPERIMENTAL RESULTS", "subsection_name": "", "paper_id": "TlyiaPXaVN", "paper_title": "Generative Adversarial Equilibrium Solvers", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:27:02.398147", "model": "gpt-5-mini", "original_claim": "For each consumer i there is a consumption set X_i contained in the nonnegative m-dimensional real space, an m-dimensional endowment vector e_i in R^m, and a utility function u_i mapping R^m to R.", "rephrasing_timestamp": "2026-01-11T19:18:49.123124", "rephrasing_model": "gpt-5-mini"}
{"claim": "Any pure-exchange market can be encoded as a specially constructed constrained strategic game whose equilibrium outcomes, understood in the sense of the generalized Nash framework, correspond exactly to the Walrasian (competitive) equilibria of the original market.", "label": "Supported", "paragraph": "We denote E = ( e 1 ↪ glyph[triangleright] glyph[triangleright] glyph[triangleright] ↪ e n ) T . Any exchange economy can be formulated as a pseudo-game whose set of GNE is equal to the set of competitive equilibria (CE) 6 of the original economy (Arrow &amp; Debreu, 1954).", "section_name": "5 EXPERIMENTAL RESULTS", "subsection_name": "", "paper_id": "TlyiaPXaVN", "paper_title": "Generative Adversarial Equilibrium Solvers", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:27:07.226707", "model": "gpt-5-mini", "original_claim": "Every exchange economy can be represented as a pseudo-game whose generalized Nash equilibria precisely coincide with the competitive equilibria of the original economy.", "rephrasing_timestamp": "2026-01-11T19:18:51.118735", "rephrasing_model": "gpt-5-mini"}
{"claim": "By introducing multiplier-weighted auxiliary variables and adding their corresponding constraint terms to the cross-entropy objective, the original problem with restrictions is turned into a mathematically identical optimization that can be solved without explicit constraints.", "label": "Supported", "paragraph": "If the cross entropy loss is used, we have the following optimization problem:\n<!-- formula-not-decoded -->\nBy using the Lagrange multiplier method, we can obtain the following non-constrained optimization problem:\n<!-- formula-not-decoded -->\nThe derivative of Φ p g q with respect to g is\n<!-- formula-not-decoded -->\nBy setting this derivative to 0 we obtain\n<!-- formula-not-decoded -->\nSince ř k i ' 1 g ‹ i p x q ' 1 and ř k i ' 1 p p y ' i | x q ' 1 , we have\n<!-- formula-not-decoded -->\nTherefore, we can easily obtain λ ' 1 .", "section_name": "B.2 PROOF OF LEMMA 3.3", "subsection_name": "", "paper_id": "fW7DOHDQvF", "paper_title": "Consistent Multi-Class Classification from Multiple Unlabeled Datasets", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:27:29.610324", "model": "gpt-5-mini", "original_claim": "Applying the Lagrange multiplier method transforms the original constrained cross-entropy optimization problem into an equivalent unconstrained optimization problem.", "rephrasing_timestamp": "2026-01-11T19:18:55.459725", "rephrasing_model": "gpt-5-mini"}
{"claim": "Applying the first-order optimality equation and enforcing that g and the conditional probability distribution are normalized determines the multiplier λ to be 1.", "label": "Supported", "paragraph": "If the cross entropy loss is used, we have the following optimization problem:\n<!-- formula-not-decoded -->\nBy using the Lagrange multiplier method, we can obtain the following non-constrained optimization problem:\n<!-- formula-not-decoded -->\nThe derivative of Φ p g q with respect to g is\n<!-- formula-not-decoded -->\nBy setting this derivative to 0 we obtain\n<!-- formula-not-decoded -->\nSince ř k i ' 1 g ‹ i p x q ' 1 and ř k i ' 1 p p y ' i | x q ' 1 , we have\n<!-- formula-not-decoded -->\nTherefore, we can easily obtain λ ' 1 .", "section_name": "B.2 PROOF OF LEMMA 3.3", "subsection_name": "", "paper_id": "fW7DOHDQvF", "paper_title": "Consistent Multi-Class Classification from Multiple Unlabeled Datasets", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:27:29.610337", "model": "gpt-5-mini", "original_claim": "Solving the stationarity condition and using the normalization constraints on g and the conditional distribution yields the Lagrange multiplier value λ = 1.", "rephrasing_timestamp": "2026-01-11T19:18:55.962576", "rephrasing_model": "gpt-5-mini"}
{"claim": "The numeral \"1\" appears beside both Zixi Wei and Senlin Shu, and that notation refers to Chongqing University, indicating each author is connected to that institution.", "label": "Supported", "paragraph": "Zixi Wei 1 : Senlin Shu 1 : Yuzhou Cao 2 Hongxin Wei 3 Bo An 4 , 2 Lei Feng 2 ˚ 1 Chongqing University 2 Nanyang Technological University 3 Southern University of Science and Technology 4 Skywork AI yuzhou002@e.ntu.edu.sg\n{ zixiwei,senlinshu } @stu.cqu.edu.cn , weihx@sustech.edu.cn , boan@ntu.edu.sg , lfengqaq@gmail.com", "section_name": "CONSISTENT MULTI-CLASS CLASSIFICATION FROM MULTIPLE UNLABELED DATASETS", "subsection_name": "", "paper_id": "fW7DOHDQvF", "paper_title": "Consistent Multi-Class Classification from Multiple Unlabeled Datasets", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:27:42.666939", "model": "gpt-5-mini", "original_claim": "The authors Zixi Wei and Senlin Shu are marked with affiliation superscript 1, which corresponds to Chongqing University, indicating both authors are affiliated with Chongqing University.", "rephrasing_timestamp": "2026-01-11T19:18:57.060819", "rephrasing_model": "gpt-5-mini"}
{"claim": "A '2' marker placed after the names Yuzhou Cao and Lei Feng refers to Nanyang Technological University, signifying that the two authors are associated with that university.", "label": "Supported", "paragraph": "Zixi Wei 1 : Senlin Shu 1 : Yuzhou Cao 2 Hongxin Wei 3 Bo An 4 , 2 Lei Feng 2 ˚ 1 Chongqing University 2 Nanyang Technological University 3 Southern University of Science and Technology 4 Skywork AI yuzhou002@e.ntu.edu.sg\n{ zixiwei,senlinshu } @stu.cqu.edu.cn , weihx@sustech.edu.cn , boan@ntu.edu.sg , lfengqaq@gmail.com", "section_name": "CONSISTENT MULTI-CLASS CLASSIFICATION FROM MULTIPLE UNLABELED DATASETS", "subsection_name": "", "paper_id": "fW7DOHDQvF", "paper_title": "Consistent Multi-Class Classification from Multiple Unlabeled Datasets", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:27:42.666964", "model": "gpt-5-mini", "original_claim": "Yuzhou Cao and Lei Feng are designated with affiliation superscript 2, which maps to Nanyang Technological University, indicating both authors are affiliated with Nanyang Technological University.", "rephrasing_timestamp": "2026-01-11T19:18:58.219253", "rephrasing_model": "gpt-5-mini"}
{"claim": "To run the algorithm you provide: a predictor f; a matrix θ that specifies class prior relationships; the original prior vector π and substitute prior estimates ρ; an epoch ceiling E_max and an iteration ceiling I_max, with E_warm denoting the number of warm-up epochs; and an unlabeled training collection of n samples.", "label": "Supported", "paragraph": "Input: Model f , class prior matrix θ , original class prior π , surrogate class prior ρ , epoch E max, warmup epoch E warm, iteration I max, unlabeled training set r D ' tp x p i q , ¯ y p i q qu n i ' 1 .", "section_name": "Algorithm 2 RCM Algorithm", "subsection_name": "", "paper_id": "fW7DOHDQvF", "paper_title": "Consistent Multi-Class Classification from Multiple Unlabeled Datasets", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:27:50.929470", "model": "gpt-5-mini", "original_claim": "The procedure takes as inputs: a model f, a class-prior matrix θ, original class priors π, surrogate priors ρ, maximum epochs E_max, warmup epochs E_warm, maximum iterations I_max, and an unlabeled training dataset of n examples.", "rephrasing_timestamp": "2026-01-11T19:19:03.365420", "rephrasing_model": "gpt-5-mini"}
{"claim": "Researchers have devised a range of methods to counter the diverse and intricate impairments that afflict still images and motion footage, with the goal of recovering or enhancing their perceived fidelity across many forms of corruption.", "label": "Supported", "paragraph": "To address this complex and variable degradation in images and videos, various methodologies have been developed. Current state-of-the-art methods can generally be categorized into supervised and self-supervised learning manner.", "section_name": "1.1 Current State-of-the-Art in Atmospheric Turbulence Mitigation", "subsection_name": "", "paper_id": "yURca4wi2L", "paper_title": "Temporally Consistent Atmospheric Turbulence Mitigation with Neural Representations", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:28:14.566937", "model": "gpt-5-mini", "original_claim": "Multiple methodological approaches have been proposed to tackle the heterogeneous and complex degradation affecting image and video data, aiming to restore or improve visual quality across varied degradation types.", "rephrasing_timestamp": "2026-01-11T19:19:03.551158", "rephrasing_model": "gpt-5-mini"}
{"claim": "Contemporary leading techniques for restoring degraded images and videos are typically grouped by their training strategy into approaches that rely on external clean references (label-dependent) and approaches that derive supervisory signals directly from the corrupted data.", "label": "Supported", "paragraph": "To address this complex and variable degradation in images and videos, various methodologies have been developed. Current state-of-the-art methods can generally be categorized into supervised and self-supervised learning manner.", "section_name": "1.1 Current State-of-the-Art in Atmospheric Turbulence Mitigation", "subsection_name": "", "paper_id": "yURca4wi2L", "paper_title": "Temporally Consistent Atmospheric Turbulence Mitigation with Neural Representations", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:28:14.566957", "model": "gpt-5-mini", "original_claim": "Modern state-of-the-art techniques for addressing image and video degradation are commonly organized into two categories based on learning paradigm: supervised learning methods and self-supervised learning methods.", "rephrasing_timestamp": "2026-01-11T19:19:02.756814", "rephrasing_model": "gpt-5-mini"}
{"claim": "Learning from labeled examples to undo air-induced optical distortion relies on synthetic turbulence generators that create matched sets of pristine and turbulence-corrupted images or video frames, which are then used as training data for image-recovery systems.", "label": "Supported", "paragraph": "Supervised learning techniques in ATMuse turbulence simulators to generate paired training data (clean and distorted images/video) that can be used for training (14; 15; 16; 1; 17; 18). Fig.3(A) and the supervised learning section of Table 1 depict methods that achieve significant results based\nTable 1: Comparison of recent supervised (S), selfsupervised (SS), and hybrid (S+SS) learning approaches for image and video ATM.", "section_name": "1.1 Current State-of-the-Art in Atmospheric Turbulence Mitigation", "subsection_name": "", "paper_id": "yURca4wi2L", "paper_title": "Temporally Consistent Atmospheric Turbulence Mitigation with Neural Representations", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:28:26.761998", "model": "gpt-5-mini", "original_claim": "Supervised learning approaches for atmospheric turbulence mitigation employ turbulence simulators to produce paired datasets of clean and turbulence-distorted images or video frames that are subsequently used to train restoration models.", "rephrasing_timestamp": "2026-01-11T19:19:03.935904", "rephrasing_model": "gpt-5-mini"}
{"claim": "Panel A of Figure 3 and the part of Table 1 addressing labeled-data approaches present techniques for image- and video-based ATM that the authors report yield substantial performance gains.", "label": "Supported", "paragraph": "Supervised learning techniques in ATMuse turbulence simulators to generate paired training data (clean and distorted images/video) that can be used for training (14; 15; 16; 1; 17; 18). Fig.3(A) and the supervised learning section of Table 1 depict methods that achieve significant results based\nTable 1: Comparison of recent supervised (S), selfsupervised (SS), and hybrid (S+SS) learning approaches for image and video ATM.", "section_name": "1.1 Current State-of-the-Art in Atmospheric Turbulence Mitigation", "subsection_name": "", "paper_id": "yURca4wi2L", "paper_title": "Temporally Consistent Atmospheric Turbulence Mitigation with Neural Representations", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:28:26.762018", "model": "gpt-5-mini", "original_claim": "Figure 3(A) and the supervised-learning section of Table 1 illustrate supervised methods for image and video ATM that the paper reports as achieving significant performance improvements.", "rephrasing_timestamp": "2026-01-11T19:19:09.837268", "rephrasing_model": "gpt-5-mini"}
{"claim": "Trained with full supervisory labels (S), six approaches—TSRWGAN, TurbNet, PiRN-SR, TMT, DATUM, and Turb-Seg-Res—are designed for both still scenes and dynamic frame sequences in images and videos, employing adversarial training, sophisticated simulation engines, or physics-based models.", "label": "Supported", "paragraph": "| Supervision   | Method          | Capability          | Critical Performance Factors          |\n|----------|----------|----------|----------|\n| S          | TSRWGAN (4) TurbNet (5) PiRN-SR (6) TMT (7) DATUM (2) Turb-Seg-Res (8)    | Static Scene Sequences Image Image Video Video Video          | Adversarial Learning Advanced Simulator Advanced Simulator Physically-Grounded Model Physically-Grounded Model Advanced Simulator |\n| SS          | Mao et al.", "section_name": "1.1 Current State-of-the-Art in Atmospheric Turbulence Mitigation", "subsection_name": "", "paper_id": "yURca4wi2L", "paper_title": "Temporally Consistent Atmospheric Turbulence Mitigation with Neural Representations", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:28:39.844799", "model": "gpt-5-mini", "original_claim": "Under full supervision 'S', six methods—TSRWGAN, TurbNet, PiRN-SR, TMT, DATUM, and Turb-Seg-Res—are reported to target static scenes, temporal sequences, images, and videos, relying on adversarial learning, advanced simulators, or physically-grounded models.", "rephrasing_timestamp": "2026-01-11T19:19:09.169143", "rephrasing_model": "gpt-5-mini"}
{"claim": "When applying Theorem 3.2 together with Lemma 1, the argument fixes the regularizer's weight to 2L, sets the initial deviation at time t0 to zero, and imposes the learning-rate bound η·L < 1.", "label": "Supported", "paragraph": "Proof. By Theorem 3.2 of [29], we have\n<!-- formula-not-decoded -->\nwhere (a) uses Lemma 1; (b) uses λ = 2 L ; (c) uses ∆ t 0 = 0 ; (d) uses ηL &lt; 1 . Then, by Lemma 3.11 of [29] and ̂ ℓ is ( L + λ ) -smooth, we have\n<!-- formula-not-decoded -->\nwhere the last inequality holds by selecting t 0 = 1 and λ = 2 L .", "section_name": "C Proof of Theorem 1", "subsection_name": "", "paper_id": "MJgMMqMDu4", "paper_title": "A Single-Step, Sharpness-Aware Minimization is All You Need to Achieve Efficient and Accurate Sparse Training", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:29:05.440364", "model": "gpt-5-mini", "original_claim": "The proof fixes the regularization parameter λ to be twice L, sets Δ_{t0} equal to zero, and requires the step-size condition ηL < 1 when applying Theorem 3.2 and Lemma 1.", "rephrasing_timestamp": "2026-01-11T19:19:14.153733", "rephrasing_model": "gpt-5-mini"}
{"claim": "Drawing on the cited result in [29] (Lemma 3.11) and the fact that the proxy objective \\hat{\\ell} has smoothness parameter L+λ, they set t0 = 1 and choose λ = 2L so that the final bound holds.", "label": "Supported", "paragraph": "Proof. By Theorem 3.2 of [29], we have\n<!-- formula-not-decoded -->\nwhere (a) uses Lemma 1; (b) uses λ = 2 L ; (c) uses ∆ t 0 = 0 ; (d) uses ηL &lt; 1 . Then, by Lemma 3.11 of [29] and ̂ ℓ is ( L + λ ) -smooth, we have\n<!-- formula-not-decoded -->\nwhere the last inequality holds by selecting t 0 = 1 and λ = 2 L .", "section_name": "C Proof of Theorem 1", "subsection_name": "", "paper_id": "MJgMMqMDu4", "paper_title": "A Single-Step, Sharpness-Aware Minimization is All You Need to Achieve Efficient and Accurate Sparse Training", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:29:05.440384", "model": "gpt-5-mini", "original_claim": "Using Lemma 3.11 of [29] and the fact that the surrogate loss ˆℓ is (L+λ)-smooth, the authors select t0 = 1 and λ = 2L to make the final inequality valid.", "rephrasing_timestamp": "2026-01-11T19:19:12.858960", "rephrasing_model": "gpt-5-mini"}
{"claim": "The sample-derived loss \\hat F_S has a gradient that is Lipschitz continuous with constant L+λ and obeys the gradient-dominance condition with parameter μ.", "label": "Supported", "paragraph": "Then, by Theorem 2.2 of [29]\n<!-- formula-not-decoded -->\nwhich is the generalization error. Next, we will bound the optimization error. Since ̂ F S is ( L + λ ) -smooth and satisfies µ -PL condition, then follow the standard analysis, we have\nSince ̂ F S is ( L + λ ) -smooth, we have\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere the last inequality is due to ℓ ( y , f ( w , x )) is B -Lipschitz.", "section_name": "C Proof of Theorem 1", "subsection_name": "", "paper_id": "MJgMMqMDu4", "paper_title": "A Single-Step, Sharpness-Aware Minimization is All You Need to Achieve Efficient and Accurate Sparse Training", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:29:13.234623", "model": "gpt-5-mini", "original_claim": "The empirical objective ˆF_S is smooth with parameter L+λ and satisfies the PL inequality with constant μ.", "rephrasing_timestamp": "2026-01-11T19:19:11.336908", "rephrasing_model": "gpt-5-mini"}
{"claim": "The loss computed for a single example, ℓ(y, f(w, x)), is controlled by a constant B: its value can change by at most B times the change in the model output. This bounded-change property is what yields the last inequality in the derivation.", "label": "Supported", "paragraph": "Then, by Theorem 2.2 of [29]\n<!-- formula-not-decoded -->\nwhich is the generalization error. Next, we will bound the optimization error. Since ̂ F S is ( L + λ ) -smooth and satisfies µ -PL condition, then follow the standard analysis, we have\nSince ̂ F S is ( L + λ ) -smooth, we have\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere the last inequality is due to ℓ ( y , f ( w , x )) is B -Lipschitz.", "section_name": "C Proof of Theorem 1", "subsection_name": "", "paper_id": "MJgMMqMDu4", "paper_title": "A Single-Step, Sharpness-Aware Minimization is All You Need to Achieve Efficient and Accurate Sparse Training", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:29:13.234643", "model": "gpt-5-mini", "original_claim": "The per-sample loss ℓ(y, f(w, x)) is B-Lipschitz, a property used to obtain the final inequality in the derivation.", "rephrasing_timestamp": "2026-01-11T19:19:20.171057", "rephrasing_model": "gpt-5-mini"}
{"claim": "If the learning rate is chosen proportional to 1/√n and the method is run for a number of iterations that grows linearly with n, then the expected gap between the value F at the iterate returned by S^2-SAM and the optimal value F(w*) scales like 1/√n.", "label": "Supported", "paragraph": "Therefore, we get\n<!-- formula-not-decoded -->\nSince ̂ F S satisfies µ -PL condition, then\n<!-- formula-not-decoded -->\nThen by (12), (17) and (21), we get\n<!-- formula-not-decoded -->\nBy using the conditions that min w ∈W ̂ F S ( w ) ≤ F S ( w ∗ S ) + λ 2 ∥ w t ∥ 2 , definitions (11), we get\n<!-- formula-not-decoded -->\nTherefore, we have the following inequality by (22) and (23):\n<!-- formula-not-decoded -->\nBy Lemma 5.1 of [29], (24) implies the ERB is\n<!-- formula-not-decoded -->\nBy setting η = O (1 / √ n ) and T = O ( n ) then E R, A , S [ F ( w R )] -F ( w ∗ ) ≤ O (1 / √ n ) , where A is S 2 -SAM.", "section_name": "C Proof of Theorem 1", "subsection_name": "", "paper_id": "MJgMMqMDu4", "paper_title": "A Single-Step, Sharpness-Aware Minimization is All You Need to Achieve Efficient and Accurate Sparse Training", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:29:20.360449", "model": "gpt-5-mini", "original_claim": "Using step size η = O(1/√n) and T = O(n), the expected excess loss E_{R,A,S}[F(w_R)] - F(w*) achieved by the S^2-SAM algorithm is O(1/√n).", "rephrasing_timestamp": "2026-01-11T19:19:18.728483", "rephrasing_model": "gpt-5-mini"}
{"claim": "As a first step, the method combines D_exp and D_sub — two SFT chat collections of unequal quality — into one aggregated dataset that serves as the foundational input for subsequent processing.", "label": "Supported", "paragraph": "Given the SFT conversation datasets D exp ⋃ D sub with different quality levels, we can replenish them with distinct sources as class labels (e.g., c i ∈ { GPT-4 , GPT-3.5 } ) and construct a classconditioned dataset D c = { ( x i , y i , c i ) } .", "section_name": "3.1 CLASS-CONDITIONED DATASET AND REWARDS", "subsection_name": "", "paper_id": "AOJyfhWYHf", "paper_title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:29:41.653670", "model": "gpt-5-mini", "original_claim": "The approach starts by uniting two SFT conversation datasets, D_exp and D_sub, which possess differing quality levels, to form the initial pooled dataset for further processing.", "rephrasing_timestamp": "2026-01-11T19:19:21.647749", "rephrasing_model": "gpt-5-mini"}
{"claim": "Each entry in the combined corpus is annotated with the generating model (for example, different model versions), yielding a label-conditioned collection, Dc, composed of triplets (prompt, response, source identifier).", "label": "Supported", "paragraph": "Given the SFT conversation datasets D exp ⋃ D sub with different quality levels, we can replenish them with distinct sources as class labels (e.g., c i ∈ { GPT-4 , GPT-3.5 } ) and construct a classconditioned dataset D c = { ( x i , y i , c i ) } .", "section_name": "3.1 CLASS-CONDITIONED DATASET AND REWARDS", "subsection_name": "", "paper_id": "AOJyfhWYHf", "paper_title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:29:41.653681", "model": "gpt-5-mini", "original_claim": "Each example in the pooled data is labeled with a source class (for instance GPT-4 or GPT-3.5), producing a class-conditioned dataset D_c of triples (input, output, class label).", "rephrasing_timestamp": "2026-01-11T19:19:23.538820", "rephrasing_model": "gpt-5-mini"}
{"claim": "π_c(y | x, c) denotes the conditional probability, specific to category c, that assigns likelihoods to possible replies y for a given prompt x, as determined by the prompt–reply examples (x, y) in the dataset subset D_c.", "label": "Supported", "paragraph": "We use π c ( y | x, c ) to denote class-conditioned distribution over instructions x and responses y in the class-conditioned dataset D c , which can be perceived similarly as the behavior policy of a dataset in offline RL literature (Levine et al., 2020), with the difference that π c is now a class-conditioned policy.", "section_name": "3.1 CLASS-CONDITIONED DATASET AND REWARDS", "subsection_name": "", "paper_id": "AOJyfhWYHf", "paper_title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:29:51.445766", "model": "gpt-5-mini", "original_claim": "The paper defines π_c(y | x, c) as the class-conditioned probability distribution over instruction-response pairs (x, y) contained in the class-specific dataset D_c.", "rephrasing_timestamp": "2026-01-11T19:19:25.156626", "rephrasing_model": "gpt-5-mini"}
{"claim": "The authors portray π_c as comparable to the policy that generates data in offline RL (Levine et al., 2020); its principal distinction is that it is specific to a given class c.", "label": "Supported", "paragraph": "We use π c ( y | x, c ) to denote class-conditioned distribution over instructions x and responses y in the class-conditioned dataset D c , which can be perceived similarly as the behavior policy of a dataset in offline RL literature (Levine et al., 2020), with the difference that π c is now a class-conditioned policy.", "section_name": "3.1 CLASS-CONDITIONED DATASET AND REWARDS", "subsection_name": "", "paper_id": "AOJyfhWYHf", "paper_title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:29:51.445781", "model": "gpt-5-mini", "original_claim": "The authors describe π_c as analogous to the behavior policy used in offline reinforcement learning literature (Levine et al., 2020), with the primary difference that it conditions on class c.", "rephrasing_timestamp": "2026-01-11T19:19:25.930166", "rephrasing_model": "gpt-5-mini"}
{"claim": "Dialogues originating from GPT-4 are labeled as the expert corpus (D_expert), while those produced by GPT-3.5 are assigned to the lower-quality corpus (D_sub).", "label": "Supported", "paragraph": "According to the different overall quality with respect to class labels, we can naturally encode coarse-grained rewards r c ( x, y ) in D c as follows:\n<!-- formula-not-decoded -->\nwhere we regard GPT-4 conversations as expert data D expert, and GPT-3.5 conversations as suboptimal data D sub .", "section_name": "3.1 CLASS-CONDITIONED DATASET AND REWARDS", "subsection_name": "", "paper_id": "AOJyfhWYHf", "paper_title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:29:57.767903", "model": "gpt-5-mini", "original_claim": "GPT-4-produced conversations are treated as the expert dataset D_expert, whereas conversations from GPT-3.5 are considered the suboptimal dataset D_sub.", "rephrasing_timestamp": "2026-01-11T19:19:29.782692", "rephrasing_model": "gpt-5-mini"}
{"claim": "This adaptation learns two compact, rank-constrained factor matrices and adds their product as an additive correction to the fixed weight matrix of an already-trained linear layer.", "label": "Supported", "paragraph": "The LoRA method updates two low-rank matrices A and B , and uses AB as the change of a pretrained and frozen weight W 0 of a linear layer as shown in Eq. 1. The integral parameters are fine-tuned for the whole corpus in the original LoRA, which causes difficulty in learning the various knowledge aspects.", "section_name": "3.1 Asymmetric LoRA architecture", "subsection_name": "", "paper_id": "qEpi8uWX3N", "paper_title": "HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning", "paper_venue": "neurips2024", "paper_decision": "Accept (oral)", "decision": "Oral", "extraction_timestamp": "2026-01-11T15:30:18.338652", "model": "gpt-5-mini", "original_claim": "LoRA trains two low-rank matrices, A and B, and applies their matrix product AB as the update added to a pretrained, frozen linear-layer weight W0.", "rephrasing_timestamp": "2026-01-11T19:19:29.201833", "rephrasing_model": "gpt-5-mini"}
{"claim": "The conventional LoRA method updates a model's core weights over the entire training set, which reduces its ability to acquire a variety of specialized knowledge.", "label": "Supported", "paragraph": "The LoRA method updates two low-rank matrices A and B , and uses AB as the change of a pretrained and frozen weight W 0 of a linear layer as shown in Eq. 1. The integral parameters are fine-tuned for the whole corpus in the original LoRA, which causes difficulty in learning the various knowledge aspects.", "section_name": "3.1 Asymmetric LoRA architecture", "subsection_name": "", "paper_id": "qEpi8uWX3N", "paper_title": "HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning", "paper_venue": "neurips2024", "paper_decision": "Accept (oral)", "decision": "Oral", "extraction_timestamp": "2026-01-11T15:30:18.338664", "model": "gpt-5-mini", "original_claim": "The original LoRA fine-tunes the model's integral parameters across the entire training corpus, which hinders learning of various distinct knowledge aspects.", "rephrasing_timestamp": "2026-01-11T19:19:28.933160", "rephrasing_model": "gpt-5-mini"}
{"claim": "After a thorough analysis of the low-rank adaptation module, the document recommends dividing the whole module into several parallel branch variants—an approach like a multi‑headed design—to introduce a modular architecture.", "label": "Supported", "paragraph": "Drawing from a detailed breakdown analysis of LoRA, a potential solution is to segment the entire LoRA into 'Hydra' structured LoRA variants, that is, characterized by a central shared matrix A and several distinct matrices B , fostering a blend of shared knowledge and specialized functionalities.", "section_name": "3.1 Asymmetric LoRA architecture", "subsection_name": "", "paper_id": "qEpi8uWX3N", "paper_title": "HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning", "paper_venue": "neurips2024", "paper_decision": "Accept (oral)", "decision": "Oral", "extraction_timestamp": "2026-01-11T15:30:26.634160", "model": "gpt-5-mini", "original_claim": "Based on a detailed LoRA breakdown, the text suggests partitioning the entire LoRA into multiple 'Hydra' variants as a proposed solution to introduce modular structure.", "rephrasing_timestamp": "2026-01-11T19:19:32.036332", "rephrasing_model": "gpt-5-mini"}
{"claim": "A multi-branch low-rank adaptation setup uses one common core projection together with multiple independent auxiliary projections, permitting the model to learn shared representations while each branch implements specialized behavior.", "label": "Supported", "paragraph": "Drawing from a detailed breakdown analysis of LoRA, a potential solution is to segment the entire LoRA into 'Hydra' structured LoRA variants, that is, characterized by a central shared matrix A and several distinct matrices B , fostering a blend of shared knowledge and specialized functionalities.", "section_name": "3.1 Asymmetric LoRA architecture", "subsection_name": "", "paper_id": "qEpi8uWX3N", "paper_title": "HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning", "paper_venue": "neurips2024", "paper_decision": "Accept (oral)", "decision": "Oral", "extraction_timestamp": "2026-01-11T15:30:26.634177", "model": "gpt-5-mini", "original_claim": "The Hydra-style LoRA is defined by a single shared central matrix A combined with several separate B matrices to enable both shared knowledge and specialized functions.", "rephrasing_timestamp": "2026-01-11T19:19:32.371315", "rephrasing_model": "gpt-5-mini"}
{"claim": "By adjusting an uneven low-rank adapter, the method gives each module its own matrix B_i ∈ R^{d×r} while using a single common matrix A ∈ R^{r×k}, producing resilient, non-overlapping behavior across diverse datasets.", "label": "Supported", "paragraph": "As Figure 1, HydraLoRA is to fine-tune LoRAs to achieve robust performance without redundancy, thereby benefiting the entire heterogeneous corpus. The asymmetric LoRA architecture can be formulated as:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nThe matrics B i ∈ R d × r and shared A ∈ R r × k .", "section_name": "3.1 Asymmetric LoRA architecture", "subsection_name": "", "paper_id": "qEpi8uWX3N", "paper_title": "HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning", "paper_venue": "neurips2024", "paper_decision": "Accept (oral)", "decision": "Oral", "extraction_timestamp": "2026-01-11T15:30:32.110990", "model": "gpt-5-mini", "original_claim": "HydraLoRA fine-tunes an asymmetric LoRA design that uses per-module matrices B_i ∈ R^{d×r} together with a shared matrix A ∈ R^{r×k} to achieve robust, non-redundant performance on heterogeneous corpora.", "rephrasing_timestamp": "2026-01-11T19:19:39.367631", "rephrasing_model": "gpt-5-mini"}
{"claim": "Subsection 3.2 treats the CB loss as a theory-backed generalization that naturally arises from the FC-GFlowNets formalism.", "label": "Supported", "paragraph": "Section 3.2 presents the CB loss as a natural development given the theory of FC-GFlowNets. To evaluate its utility as a criterion to train GFlowNets in the conventional centralized (non-federated) setting, we report the evolution during training of the L1 error of the GFlowNet wrt the normalized reward for models trained using DB, TB, and our CB.", "section_name": "4.5 EVALUATING THE CB LOSS", "subsection_name": "", "paper_id": "VJDFhkwQg6", "paper_title": "Federated contrastive GFlowNets", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:30:51.646310", "model": "gpt-5-mini", "original_claim": "Section 3.2 frames the CB loss as a theoretically motivated extension that follows from the FC-GFlowNets framework.", "rephrasing_timestamp": "2026-01-11T19:19:37.767207", "rephrasing_model": "gpt-5-mini"}
{"claim": "In a non-distributed (non-federated) training setup, the paper assessed the CB method by tracking how the L1 distance between the GFlowNet's outputs and the normalized reward changed during training for models trained with DB, TB, and CB.", "label": "Supported", "paragraph": "Section 3.2 presents the CB loss as a natural development given the theory of FC-GFlowNets. To evaluate its utility as a criterion to train GFlowNets in the conventional centralized (non-federated) setting, we report the evolution during training of the L1 error of the GFlowNet wrt the normalized reward for models trained using DB, TB, and our CB.", "section_name": "4.5 EVALUATING THE CB LOSS", "subsection_name": "", "paper_id": "VJDFhkwQg6", "paper_title": "Federated contrastive GFlowNets", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:30:51.646321", "model": "gpt-5-mini", "original_claim": "The study evaluated CB in a centralized (non-federated) training regime by reporting how the GFlowNet's L1 error relative to the normalized reward evolved during training for models trained with DB, TB, and CB.", "rephrasing_timestamp": "2026-01-11T19:19:39.350016", "rephrasing_model": "gpt-5-mini"}
{"claim": "Across all reported tasks, every GFlowNet used an identical network design to parameterize both the component that proposes actions and the component that reverses them.", "label": "Supported", "paragraph": "We do so for all tasks in our experiments, with all GFlowNets using the same architectures for the forward and backward policies (more details in supplement). While we did not see any noticeable difference for the tasks in which all states are also terminal (grid world and design of sequences), CB led to better convergence in\nFigure 6: L CB performs competitively or better than L TB , L DB and L FL .", "section_name": "4.5 EVALUATING THE CB LOSS", "subsection_name": "", "paper_id": "VJDFhkwQg6", "paper_title": "Federated contrastive GFlowNets", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:31:05.794985", "model": "gpt-5-mini", "original_claim": "In all tasks reported, every GFlowNet used the same neural architecture for its forward policy and its backward policy.", "rephrasing_timestamp": "2026-01-11T19:19:42.450870", "rephrasing_model": "gpt-5-mini"}
{"claim": "The sixth panel shows the CB configuration converged more effectively: the loss for that setup was no worse than—and often better than—the losses observed for the TB, DB, and FL configurations.", "label": "Supported", "paragraph": "We do so for all tasks in our experiments, with all GFlowNets using the same architectures for the forward and backward policies (more details in supplement). While we did not see any noticeable difference for the tasks in which all states are also terminal (grid world and design of sequences), CB led to better convergence in\nFigure 6: L CB performs competitively or better than L TB , L DB and L FL .", "section_name": "4.5 EVALUATING THE CB LOSS", "subsection_name": "", "paper_id": "VJDFhkwQg6", "paper_title": "Federated contrastive GFlowNets", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:31:05.794999", "model": "gpt-5-mini", "original_claim": "Figure 6 shows the CB variant yielded better convergence: L_CB matched or exceeded the performance of L_TB, L_DB, and L_FL.", "rephrasing_timestamp": "2026-01-11T19:19:44.189501", "rephrasing_model": "gpt-5-mini"}
{"claim": "CB affords a considerably simpler representation than DB and TB, since it dispenses with the need to infer the flow or compute the normalization constant.", "label": "Supported", "paragraph": "<!-- image -->\nthe multiset generation and phylogeny tasks (Figure 6). An explanation is that CB incurs a considerably simpler parameterization than DB and TB - as we do not require estimating the flow nor the partition function.", "section_name": "4.5 EVALUATING THE CB LOSS", "subsection_name": "", "paper_id": "VJDFhkwQg6", "paper_title": "Federated contrastive GFlowNets", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:31:12.526656", "model": "gpt-5-mini", "original_claim": "The CB approach uses a substantially simpler parameterization than DB and TB because it does not require estimating the flow or the partition function.", "rephrasing_timestamp": "2026-01-11T19:19:46.173745", "rephrasing_model": "gpt-5-mini"}
{"claim": "Across all trials we modified only the parameters specific to each optimizer and left every other option at PyTorch’s standard settings; for example, we explored different step-size and momentum coefficients for SGD and varied Adam’s step-size.", "label": "Supported", "paragraph": "In all our experiments, we tune the following optimizer hyper-parameters and otherwise use the PyTorch default values:\n- SGD: learning rate, momentum\n- Adam: learning rate\n- Hessian-free: type of curvature matrix (Hessian or GGN), damping, whether to adapt damping over time (yes or no), maximum number of CG iterations\n- LBFGS: learning rate, history size\n- ENGD: damping, factor of the exponential moving average applied to the Gramian, initialization of the Gramian (zero or identity matrix)\n- KFAC: factor of the exponential moving average applied to the Kronecker factors, damping, momentum, initialization of the Kronecker factors (zero or identity matrix)\n- KFAC*: factor of the exponential moving average applied to the Kronecker factors, damping, initialization of the Kronecker factors (zero or identity matrix)\nDepending on the optimizer and experiment we use grid, random, or Bayesian search from Weights &amp;Biases to determine the hyper-parameters.", "section_name": "A.1 Hyper-Parameter Tuning Protocol", "subsection_name": "", "paper_id": "jrNlWfor7q", "paper_title": "Kronecker-Factored Approximate Curvature for Physics-Informed Neural Networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:31:50.861862", "model": "gpt-5-mini", "original_claim": "In all experiments we tuned optimizer-specific hyperparameters while keeping other settings at PyTorch defaults; for example, SGD's learning rate and momentum and Adam's learning rate were tuned.", "rephrasing_timestamp": "2026-01-11T19:19:47.230924", "rephrasing_model": "gpt-5-mini"}
{"claim": "Based on the chosen training algorithm and experimental configuration, systematic, stochastic, or Bayesian search methods available through the Weights & Biases platform were employed to identify the optimal hyperparameter settings.", "label": "Supported", "paragraph": "In all our experiments, we tune the following optimizer hyper-parameters and otherwise use the PyTorch default values:\n- SGD: learning rate, momentum\n- Adam: learning rate\n- Hessian-free: type of curvature matrix (Hessian or GGN), damping, whether to adapt damping over time (yes or no), maximum number of CG iterations\n- LBFGS: learning rate, history size\n- ENGD: damping, factor of the exponential moving average applied to the Gramian, initialization of the Gramian (zero or identity matrix)\n- KFAC: factor of the exponential moving average applied to the Kronecker factors, damping, momentum, initialization of the Kronecker factors (zero or identity matrix)\n- KFAC*: factor of the exponential moving average applied to the Kronecker factors, damping, initialization of the Kronecker factors (zero or identity matrix)\nDepending on the optimizer and experiment we use grid, random, or Bayesian search from Weights &amp;Biases to determine the hyper-parameters.", "section_name": "A.1 Hyper-Parameter Tuning Protocol", "subsection_name": "", "paper_id": "jrNlWfor7q", "paper_title": "Kronecker-Factored Approximate Curvature for Physics-Informed Neural Networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:31:50.861873", "model": "gpt-5-mini", "original_claim": "Depending on the optimizer and experimental setup, we used grid, random, or Bayesian hyperparameter searches provided via Weights & Biases to determine the tuned hyperparameter values.", "rephrasing_timestamp": "2026-01-11T19:19:51.712445", "rephrasing_model": "gpt-5-mini"}
{"claim": "After each execution finished, its terminal Euclidean‑norm error was calculated on a single unchanging test dataset, and the executions were ordered according to those values.", "label": "Supported", "paragraph": "Each individual run is executed in double precision and allowed to run for a given time budget, and we rank runs by the final L 2 error on a fixed evaluation data set. To allow comparison, all runs are executed on RTX 6000 GPUs with 24 GiB of RAM.", "section_name": "A.1 Hyper-Parameter Tuning Protocol", "subsection_name": "", "paper_id": "jrNlWfor7q", "paper_title": "Kronecker-Factored Approximate Curvature for Physics-Informed Neural Networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:32:06.751468", "model": "gpt-5-mini", "original_claim": "Runs were compared by ranking them according to their final L2 error computed on a single, fixed evaluation dataset after each run finished.", "rephrasing_timestamp": "2026-01-11T19:19:52.723196", "rephrasing_model": "gpt-5-mini"}
{"claim": "To ensure direct comparability, every trial was carried out on identical NVIDIA RTX 6000 graphics accelerators, each equipped with 24 GiB of onboard memory.", "label": "Supported", "paragraph": "Each individual run is executed in double precision and allowed to run for a given time budget, and we rank runs by the final L 2 error on a fixed evaluation data set. To allow comparison, all runs are executed on RTX 6000 GPUs with 24 GiB of RAM.", "section_name": "A.1 Hyper-Parameter Tuning Protocol", "subsection_name": "", "paper_id": "jrNlWfor7q", "paper_title": "Kronecker-Factored Approximate Curvature for Physics-Informed Neural Networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:32:06.751483", "model": "gpt-5-mini", "original_claim": "All experiment runs were executed on NVIDIA RTX 6000 GPUs provisioned with 24 GiB of memory to make results directly comparable across runs.", "rephrasing_timestamp": "2026-01-11T19:19:55.062486", "rephrasing_model": "gpt-5-mini"}
{"claim": "They carry out parameter tuning in two stages: first an exploratory, wide-ranging sampling of configurations limited to roughly fifty trials, then a narrower, follow-up sampling informed by those results that runs about fifty more trials.", "label": "Supported", "paragraph": "For grid and random searches, we use a round-based approach. First, we choose a relatively wide search space and limit to approximately 50 runs. In a second round, we narrow down the hyper-parameter space based on the first round, then re-run for another approximately 50 runs.", "section_name": "A.1 Hyper-Parameter Tuning Protocol", "subsection_name": "", "paper_id": "jrNlWfor7q", "paper_title": "Kronecker-Factored Approximate Curvature for Physics-Informed Neural Networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:32:12.063819", "model": "gpt-5-mini", "original_claim": "They perform grid and random hyperparameter searches in two phases: an initial broad search limited to about fifty runs, then a refined search informed by the first phase with roughly fifty additional runs.", "rephrasing_timestamp": "2026-01-11T19:19:54.692898", "rephrasing_model": "gpt-5-mini"}
{"claim": "The paper introduces a novel mechanism that enables individual elements to transmit and integrate information with collections of units within the model's architecture.", "label": "Supported", "paragraph": "The proposed Node-to-Cluster Attention mechanism introduces a novel approach to information exchange between node clusters. Our research underscores the importance of incorporating diverse strategies for interactions at both the node and cluster levels.", "section_name": "I Potential Impacts", "subsection_name": "", "paper_id": "3j2nasmKkP", "paper_title": "Cluster-wise Graph Transformer with Dual-granularity Kernelized Attention", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:32:32.190430", "model": "gpt-5-mini", "original_claim": "The paper proposes a Node-to-Cluster Attention mechanism as a new method for exchanging information among groups of nodes (node clusters) within the model.", "rephrasing_timestamp": "2026-01-11T19:19:57.400228", "rephrasing_model": "gpt-5-mini"}
{"claim": "The results show that managing message flows effectively requires a range of interaction approaches applied at the per-device scale and across the entire cluster.", "label": "Supported", "paragraph": "The proposed Node-to-Cluster Attention mechanism introduces a novel approach to information exchange between node clusters. Our research underscores the importance of incorporating diverse strategies for interactions at both the node and cluster levels.", "section_name": "I Potential Impacts", "subsection_name": "", "paper_id": "3j2nasmKkP", "paper_title": "Cluster-wise Graph Transformer with Dual-granularity Kernelized Attention", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:32:32.190443", "model": "gpt-5-mini", "original_claim": "The study highlights the necessity of using multiple interaction strategies to manage communications both at the individual node level and at the aggregated cluster level.", "rephrasing_timestamp": "2026-01-11T19:20:02.803252", "rephrasing_model": "gpt-5-mini"}
{"claim": "The insight introduced here can be integrated into a variety of graph algorithms that partition vertices, and doing so enhances their performance and flexibility.", "label": "Supported", "paragraph": "This perspective can be integrated with many existing node clustering-based graph learning methods, enhancing their efficacy and adaptability. Moreover, our experimental validations reveal that the method of interaction between clusters significantly impacts model performance.", "section_name": "I Potential Impacts", "subsection_name": "", "paper_id": "3j2nasmKkP", "paper_title": "Cluster-wise Graph Transformer with Dual-granularity Kernelized Attention", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:32:41.412481", "model": "gpt-5-mini", "original_claim": "The presented perspective can be incorporated into various node-clustering-based graph learning techniques and, upon incorporation, increases those techniques' effectiveness and adaptability.", "rephrasing_timestamp": "2026-01-11T19:20:01.344129", "rephrasing_model": "gpt-5-mini"}
{"claim": "Empirical tests show that the configuration of interactions among subgroups can dramatically alter predictive accuracy.", "label": "Supported", "paragraph": "This perspective can be integrated with many existing node clustering-based graph learning methods, enhancing their efficacy and adaptability. Moreover, our experimental validations reveal that the method of interaction between clusters significantly impacts model performance.", "section_name": "I Potential Impacts", "subsection_name": "", "paper_id": "3j2nasmKkP", "paper_title": "Cluster-wise Graph Transformer with Dual-granularity Kernelized Attention", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:32:41.412498", "model": "gpt-5-mini", "original_claim": "Experimental evaluations indicate that the choice of how clusters interact causes significant variation in model performance.", "rephrasing_timestamp": "2026-01-11T19:20:03.212877", "rephrasing_model": "gpt-5-mini"}
{"claim": "The research suggests that improving mechanisms for information exchange between groups in a network could advance learning approaches aimed at group-scale graph structures, producing stronger models that better reflect the complexity of large networks.", "label": "Supported", "paragraph": "While current research primarily focuses on how to partition clusters within graphs, our findings suggest a promising direction for future work: optimizing the methods of interaction between clusters. Such advancements could unlock new possibilities for cluster-level graph learning, potentially leading to more robust and sophisticated models that better capture the complexities of large-scale networks.", "section_name": "I Potential Impacts", "subsection_name": "", "paper_id": "3j2nasmKkP", "paper_title": "Cluster-wise Graph Transformer with Dual-granularity Kernelized Attention", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:32:46.791297", "model": "gpt-5-mini", "original_claim": "The study's results indicate that enhancing inter-cluster interaction methods may enable progress in cluster-level graph learning, resulting in more robust models that capture large-scale network complexities.", "rephrasing_timestamp": "2026-01-11T19:20:04.819170", "rephrasing_model": "gpt-5-mini"}
{"claim": "The opening section introduces an equilibrium model for gene frequencies alongside the classical rules of inheritance, presenting each as a method for estimating how different gene forms will be apportioned within a population over successive generations.", "label": "Supported", "paragraph": "The input here consists of several parts. The initial portion discusses the Hardy Weinberg Principle and Mendel's laws, which are used in genetics to predict how genes will distribute throughout a population over time.", "section_name": "D.2 EXAMPLE 2", "subsection_name": "", "paper_id": "JbOsMrwjZ3", "paper_title": "BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:33:22.746241", "model": "gpt-5-mini", "original_claim": "The initial section of the input covers the Hardy-Weinberg Principle and Mendel's laws, describing them as genetic frameworks for predicting allele distribution within populations over time.", "rephrasing_timestamp": "2026-01-11T19:20:07.746321", "rephrasing_model": "gpt-5-mini"}
{"claim": "Geneticists use the Hardy–Weinberg model alongside Mendelian inheritance rules as frameworks to predict how allele proportions within a population shift from one generation to the next.", "label": "Supported", "paragraph": "The input here consists of several parts. The initial portion discusses the Hardy Weinberg Principle and Mendel's laws, which are used in genetics to predict how genes will distribute throughout a population over time.", "section_name": "D.2 EXAMPLE 2", "subsection_name": "", "paper_id": "JbOsMrwjZ3", "paper_title": "BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:33:22.746251", "model": "gpt-5-mini", "original_claim": "Both the Hardy-Weinberg Principle and Mendel's laws are presented as tools in genetics for forecasting changes in gene frequencies across a population over time.", "rephrasing_timestamp": "2026-01-11T19:20:08.634518", "rephrasing_model": "gpt-5-mini"}
{"claim": "The Hardy–Weinberg framework requires an effectively limitless breeding population, treating a very large number of individuals as a necessary condition for its prediction of stable gene frequencies.", "label": "Supported", "paragraph": "The Hardy Weinberg Principle assumes that a population is large and remains in genetic equilibrium, meaning the frequency of each allele (a version of a gene) remains constant across generations, under certain conditions.", "section_name": "D.2 EXAMPLE 2", "subsection_name": "", "paper_id": "JbOsMrwjZ3", "paper_title": "BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:33:30.819390", "model": "gpt-5-mini", "original_claim": "The Hardy-Weinberg Principle assumes a population is large, treating large population size as an explicit prerequisite for its genetic equilibrium model to apply.", "rephrasing_timestamp": "2026-01-11T19:20:08.536582", "rephrasing_model": "gpt-5-mini"}
{"claim": "When the conditions of the Hardy–Weinberg model are satisfied, the relative proportion of each genetic variant in the population’s hereditary makeup remains the same from one generation to the next, showing the population is in a stable genetic state.", "label": "Supported", "paragraph": "The Hardy Weinberg Principle assumes that a population is large and remains in genetic equilibrium, meaning the frequency of each allele (a version of a gene) remains constant across generations, under certain conditions.", "section_name": "D.2 EXAMPLE 2", "subsection_name": "", "paper_id": "JbOsMrwjZ3", "paper_title": "BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:33:30.819409", "model": "gpt-5-mini", "original_claim": "When the Hardy-Weinberg assumptions hold, the frequency of each allele in the population's gene pool remains constant across generations, indicating the population is in genetic equilibrium.", "rephrasing_timestamp": "2026-01-11T19:20:16.148703", "rephrasing_model": "gpt-5-mini"}
{"claim": "The paragraph asks whether, given the proportion of people in a population who carry both copies of the non-dominant gene variant for several inherited traits, one can determine the likelihood that a randomly selected individual possesses at least one copy of the non-dominant variant for each trait.", "label": "Supported", "paragraph": "The problem statement is presented: if we know the proportion of homozygous recessive individuals (those with two copies of the recessive allele) for each of several genetic factors in a population, can we predict the probability that a randomly selected individual carries at least one copy of the recessive allele for each factor?", "section_name": "D.2 EXAMPLE 2", "subsection_name": "", "paper_id": "JbOsMrwjZ3", "paper_title": "BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:33:42.571252", "model": "gpt-5-mini", "original_claim": "The paragraph asks whether, given the fraction of individuals with two copies of recessive alleles for multiple genetic traits in a population, one can predict the probability a random person carries at least one recessive copy for each trait.", "rephrasing_timestamp": "2026-01-11T19:20:28.388330", "rephrasing_model": "gpt-5-mini"}
{"claim": "According to the experiments (Table 3), expanding a six-intermediate-layer feedforward model that uses rectifier activations from 100 to 400 units per layer substantially improved the proportion of predictions that could be provably certified.", "label": "Supported", "paragraph": "To confirm that wider models improve certified accuracy while slightly reducing tightness across network architectures, we also consider fully connected networks, which used to be the default in neural network verification (Singh et al., 2019b; 2018). We increase the width of a fully connected ReLU network with 6 hidden layers from 100 to 400 neurons and indeed observe a significant increase in certified accuracy (see Table 3).", "section_name": "D.5 TIGHTNESS AFTER IBP TRAINING", "subsection_name": "", "paper_id": "h05eQniJsQ", "paper_title": "Understanding Certified Training with Interval Bound Propagation", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:34:00.428820", "model": "gpt-5-mini", "original_claim": "Increasing the width of a fully connected ReLU network with six hidden layers from 100 to 400 neurons produced a significant rise in certified accuracy, as reported in the experiments (Table 3).", "rephrasing_timestamp": "2026-01-11T19:20:17.571657", "rephrasing_model": "gpt-5-mini"}
{"claim": "Across various model families, widening the layers increased the share of cases that could be certified while causing a slight loosening of the formal verification bounds, prompting the team to evaluate dense feedforward networks to validate the observation.", "label": "Supported", "paragraph": "To confirm that wider models improve certified accuracy while slightly reducing tightness across network architectures, we also consider fully connected networks, which used to be the default in neural network verification (Singh et al., 2019b; 2018). We increase the width of a fully connected ReLU network with 6 hidden layers from 100 to 400 neurons and indeed observe a significant increase in certified accuracy (see Table 3).", "section_name": "D.5 TIGHTNESS AFTER IBP TRAINING", "subsection_name": "", "paper_id": "h05eQniJsQ", "paper_title": "Understanding Certified Training with Interval Bound Propagation", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:34:00.428840", "model": "gpt-5-mini", "original_claim": "Across different network architectures, increasing model width raises certified accuracy while producing a slight decrease in verification tightness, which motivated the authors to test fully connected networks to confirm this behavior.", "rephrasing_timestamp": "2026-01-11T19:20:20.191672", "rephrasing_model": "gpt-5-mini"}
{"claim": "In most of the trials, we used a deep model composed of seven convolutional layers, replicating the experimental configuration reported in earlier studies (Shi et al., 2021; Müller et al., 2022b; Mao et al., 2023).", "label": "Supported", "paragraph": "We follow previous works (Shi et al., 2021; Müller et al., 2022b; Mao et al., 2023) and use a 7-layer convolutional network CNN7 in most experiments. We also use a simplified 3-layer convolutional network CNN3 in Section 4.2. Details about them can be found in the released code.\nFigure 12: Accuracies and tightness of a CNN7 for CIFAR-10 ϵ = 2 255 depending on regularization strength with STAPS.\n<!-- image -->", "section_name": "C.2 MODEL ARCHITECTURE", "subsection_name": "", "paper_id": "h05eQniJsQ", "paper_title": "Understanding Certified Training with Interval Bound Propagation", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:34:09.287316", "model": "gpt-5-mini", "original_claim": "Most experiments employed a seven-layer convolutional neural network (CNN7), following the setup used in prior works (Shi et al., 2021; Müller et al., 2022b; Mao et al., 2023).", "rephrasing_timestamp": "2026-01-11T19:20:18.557227", "rephrasing_model": "gpt-5-mini"}
{"claim": "Plot 12 shows the model's correct-classification rate and the conservativeness of its certification bounds for a seven-layer convolutional network on CIFAR-10 under perturbations of size ε = 2/255, plotted against varying regularization levels when the STAPS method is applied.", "label": "Supported", "paragraph": "We follow previous works (Shi et al., 2021; Müller et al., 2022b; Mao et al., 2023) and use a 7-layer convolutional network CNN7 in most experiments. We also use a simplified 3-layer convolutional network CNN3 in Section 4.2. Details about them can be found in the released code.\nFigure 12: Accuracies and tightness of a CNN7 for CIFAR-10 ϵ = 2 255 depending on regularization strength with STAPS.\n<!-- image -->", "section_name": "C.2 MODEL ARCHITECTURE", "subsection_name": "", "paper_id": "h05eQniJsQ", "paper_title": "Understanding Certified Training with Interval Bound Propagation", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:34:09.287336", "model": "gpt-5-mini", "original_claim": "Figure 12 presents accuracy and verification-bound tightness for the CNN7 on CIFAR-10 at adversarial radius ε = 2/255, shown as a function of regularization strength when using STAPS.", "rephrasing_timestamp": "2026-01-11T19:20:28.602522", "rephrasing_model": "gpt-5-mini"}
{"claim": "Resistance to adversarial modifications means a predictive model g returns the specified label t for every altered instance x̂ whose p-norm distance from the original sample x is at most ε_p.", "label": "Supported", "paragraph": "Here, we provide a background on adversarial and certified robustness. We consider a classifer f : R d in ↦→ R c predicting a numerical score y := f ( x ) per class given an input x ∈ X ⊆ R d in . Adversarial Robustness describes the property of a classifer f to consistently predict the target class t for all perturbed inputs x ′ in an ℓ p -norm ball B ϵ p p ( x ) of radius ϵ p .", "section_name": "2 BACKGROUND", "subsection_name": "", "paper_id": "h05eQniJsQ", "paper_title": "Understanding Certified Training with Interval Bound Propagation", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:34:16.461954", "model": "gpt-5-mini", "original_claim": "Adversarial robustness denotes that a classifier f will assign the target class t to every perturbed input x′ located inside the ℓ_p-norm ball of radius ε_p centered on the original input x.", "rephrasing_timestamp": "2026-01-11T19:20:29.116636", "rephrasing_model": "gpt-5-mini"}
{"claim": "The test harness included seven data collections, organized into two distinct assessment strategies: one measuring content-creation capabilities and the other evaluating performance on order-dependent sequences.", "label": "Supported", "paragraph": "For our evaluation benchmark, we employ seven datasets into two distinct evaluation methodologies: generation-based and sequence-based [76]. The generation-based method utilizes vLLM [47] inference framework and following the procedures outlined in Chain-of-Thought Hub [77] and Active-Prompt [78].", "section_name": "D.1 Dataset Details", "subsection_name": "", "paper_id": "m0DS4OOmSY", "paper_title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:34:47.968519", "model": "gpt-5-mini", "original_claim": "The evaluation benchmark used seven datasets split into two separate evaluation methodologies: a generation-focused approach and a sequence-focused approach.", "rephrasing_timestamp": "2026-01-11T19:20:30.836568", "rephrasing_model": "gpt-5-mini"}
{"claim": "To assess the model's generative output, the team ran inference on the vLLM engine and followed procedures derived from earlier Chain-of-Thought Hub and Active-Prompt research.", "label": "Supported", "paragraph": "For our evaluation benchmark, we employ seven datasets into two distinct evaluation methodologies: generation-based and sequence-based [76]. The generation-based method utilizes vLLM [47] inference framework and following the procedures outlined in Chain-of-Thought Hub [77] and Active-Prompt [78].", "section_name": "D.1 Dataset Details", "subsection_name": "", "paper_id": "m0DS4OOmSY", "paper_title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:34:47.968541", "model": "gpt-5-mini", "original_claim": "For the generation-focused evaluation, the study employed the vLLM inference framework and implemented procedures adapted from the Chain-of-Thought Hub and Active-Prompt works.", "rephrasing_timestamp": "2026-01-11T19:20:30.229191", "rephrasing_model": "gpt-5-mini"}
{"claim": "To benchmark model performance on tasks involving ordered inputs, the researchers used the open-source evaluation framework cited in [79].", "label": "Supported", "paragraph": "For sequence-based evaluations, we use the Language Model Evaluation Harness framework [79]. Detailed statistics for each benchmark dataset are provided in Table 6. Table 6: The statistics of the datasets used in this paper.", "section_name": "D.1 Dataset Details", "subsection_name": "", "paper_id": "m0DS4OOmSY", "paper_title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:34:55.270629", "model": "gpt-5-mini", "original_claim": "For sequence-based assessments, the study employed the Language Model Evaluation Harness framework (reference [79]) to evaluate models on sequence tasks.", "rephrasing_timestamp": "2026-01-11T19:20:39.367923", "rephrasing_model": "gpt-5-mini"}
{"claim": "Table 6 presents complete data metrics for every benchmark, outlining the attributes of each data source used in the experiments.", "label": "Supported", "paragraph": "For sequence-based evaluations, we use the Language Model Evaluation Harness framework [79]. Detailed statistics for each benchmark dataset are provided in Table 6. Table 6: The statistics of the datasets used in this paper.", "section_name": "D.1 Dataset Details", "subsection_name": "", "paper_id": "m0DS4OOmSY", "paper_title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:34:55.270651", "model": "gpt-5-mini", "original_claim": "The paper provides comprehensive dataset statistics for each benchmark in Table 6, detailing characteristics of all datasets used in the experiments.", "rephrasing_timestamp": "2026-01-11T19:20:38.223101", "rephrasing_model": "gpt-5-mini"}
{"claim": "EX specifies the small number of worked examples that include step-by-step reasoning given to the model for each task during testing.", "label": "Supported", "paragraph": "# EX. are the number of few-shot chain-ofthought exemplars used to prompt each task in evaluation. # TEST denote the number of training data and test data, respectively. *: CSQA do not have publicly available test set labels, so we simply follow the setting by [80; 78] to evaluate the performance of the development set.", "section_name": "D.1 Dataset Details", "subsection_name": "", "paper_id": "m0DS4OOmSY", "paper_title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:35:00.491954", "model": "gpt-5-mini", "original_claim": "EX represents the count of few-shot chain-of-thought exemplars used to prompt each task during evaluation.", "rephrasing_timestamp": "2026-01-11T19:20:39.520056", "rephrasing_model": "gpt-5-mini"}
{"claim": "Because obtaining many example runs from the pretrained language system is far less expensive than altering the model through additional retraining, the researchers opt to build a pool of candidate runs and pick from them rather than continue tuning the model.", "label": "Supported", "paragraph": "Since generating trajectory from an LLM is relatively much cheaper than training an LLM, we consider generating trajectories and selecting the best trajectory among them instead of training the LLM. To this end, we present Trajectory Ranking in which the LLM agent generates diverse trajectories with a number of trials and then selects the most rewarding trajectory as the final action.", "section_name": "3.2 TRAJECTORY RANKING", "subsection_name": "", "paper_id": "YKK1jXEWja", "paper_title": "Prospector: Improving LLM Agents with Self-Asking and Trajectory Ranking", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:35:18.241514", "model": "gpt-5-mini", "original_claim": "Producing trajectories with a large language model is substantially less costly than training the model, so the authors choose to generate and select among trajectories instead of performing further LLM training.", "rephrasing_timestamp": "2026-01-11T19:20:44.644651", "rephrasing_model": "gpt-5-mini"}
{"claim": "The path-selection strategy directs a language-model–driven agent to generate several distinct action plans across multiple attempts and then carry out the single plan that yields the highest return as its final decision.", "label": "Supported", "paragraph": "Since generating trajectory from an LLM is relatively much cheaper than training an LLM, we consider generating trajectories and selecting the best trajectory among them instead of training the LLM. To this end, we present Trajectory Ranking in which the LLM agent generates diverse trajectories with a number of trials and then selects the most rewarding trajectory as the final action.", "section_name": "3.2 TRAJECTORY RANKING", "subsection_name": "", "paper_id": "YKK1jXEWja", "paper_title": "Prospector: Improving LLM Agents with Self-Asking and Trajectory Ranking", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:35:18.241524", "model": "gpt-5-mini", "original_claim": "The Trajectory Ranking approach instructs the LLM agent to create multiple diverse trajectories across several trials and then execute the single trajectory that achieves the greatest reward as the final action.", "rephrasing_timestamp": "2026-01-11T19:20:38.932705", "rephrasing_model": "gpt-5-mini"}
{"claim": "When agents built on large language models use an internal questioning routine, they produce candidate actions with greater potential, resulting in improved overall quality of the actions those agents take.", "label": "Supported", "paragraph": "Thanks to Self-Asking, which allows LLM agents to generate more promising actions, Prospector can consider high-quality trajectories as candidates for final actions. However, most real-world scenarios allow the agent to interact with the environment (i.e.", "section_name": "3.2 TRAJECTORY RANKING", "subsection_name": "", "paper_id": "YKK1jXEWja", "paper_title": "Prospector: Improving LLM Agents with Self-Asking and Trajectory Ranking", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:35:26.767127", "model": "gpt-5-mini", "original_claim": "The Self-Asking approach enables large language model agents to generate action proposals that are more promising, improving the overall quality of actions produced by these agents.", "rephrasing_timestamp": "2026-01-11T19:20:44.319389", "rephrasing_model": "gpt-5-mini"}
{"claim": "Prospector can assess top-ranked produced action sequences and adopt them as potential final decisions when determining what the agent will output.", "label": "Supported", "paragraph": "Thanks to Self-Asking, which allows LLM agents to generate more promising actions, Prospector can consider high-quality trajectories as candidates for final actions. However, most real-world scenarios allow the agent to interact with the environment (i.e.", "section_name": "3.2 TRAJECTORY RANKING", "subsection_name": "", "paper_id": "YKK1jXEWja", "paper_title": "Prospector: Improving LLM Agents with Self-Asking and Trajectory Ranking", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:35:26.767142", "model": "gpt-5-mini", "original_claim": "Prospector is able to evaluate and treat high-quality generated trajectories as candidate final actions when selecting outputs for the agent.", "rephrasing_timestamp": "2026-01-11T19:20:45.254694", "rephrasing_model": "gpt-5-mini"}
{"claim": "Table 18 shows the tuning configuration applied to further train the Critic large language model using the trajectory data.", "label": "Supported", "paragraph": "In Table 18, we provide the hyper-parameters used for fine-tuning the LLM Critic on the trajectory data.", "section_name": "7.4 HYPER-PARAMETERS", "subsection_name": "", "paper_id": "YKK1jXEWja", "paper_title": "Prospector: Improving LLM Agents with Self-Asking and Trajectory Ranking", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:35:32.989116", "model": "gpt-5-mini", "original_claim": "Table 18 lists the hyper-parameter settings that were used to fine-tune the LLM Critic model on the trajectory dataset.", "rephrasing_timestamp": "2026-01-11T19:20:50.082538", "rephrasing_model": "gpt-5-mini"}
{"claim": "When the recurrence depth and iteration count are both set to one, rho equals zero, and the mean value of gamma is zero, the hypothesis produced by the MPGD formulation exactly matches the hypothesis produced by the TFG formulation.", "label": "Supported", "paragraph": "- MPGD [18] H MPGD is equivalent to H TFG ( N recur = N iter = 1 , ρ = 0 , ¯ γ = 0) . - LGD [63] H LGD is equivalent to H TFG ( N recur = 1 , N iter = 0 , µ = 0 ) . - UGD [2] H UGD is equivalent to H TFG (¯ γ = 0) .", "section_name": "Theorem 3.2. The hyper-parameter space of", "subsection_name": "", "paper_id": "N8YbGX98vc", "paper_title": "TFG: Unified Training-Free Guidance for Diffusion Models", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:36:14.633275", "model": "gpt-5-mini", "original_claim": "The MPGD model's hypothesis H_MPGD is identical to the TFG hypothesis H_TFG when both N_recur and N_iter are set to 1, parameter ρ is 0, and the average γ equals 0.", "rephrasing_timestamp": "2026-01-11T19:20:48.844652", "rephrasing_model": "gpt-5-mini"}
{"claim": "With N_recur fixed at 1, N_iter fixed at 0, and μ fixed at 0, the LGD hypothesis H_LGD becomes equivalent to the hypothesis H_TFG.", "label": "Supported", "paragraph": "- MPGD [18] H MPGD is equivalent to H TFG ( N recur = N iter = 1 , ρ = 0 , ¯ γ = 0) . - LGD [63] H LGD is equivalent to H TFG ( N recur = 1 , N iter = 0 , µ = 0 ) . - UGD [2] H UGD is equivalent to H TFG (¯ γ = 0) .", "section_name": "Theorem 3.2. The hyper-parameter space of", "subsection_name": "", "paper_id": "N8YbGX98vc", "paper_title": "TFG: Unified Training-Free Guidance for Diffusion Models", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:36:14.633288", "model": "gpt-5-mini", "original_claim": "The LGD hypothesis H_LGD reduces to H_TFG when configured with N_recur = 1, N_iter = 0, and the parameter μ set to 0.", "rephrasing_timestamp": "2026-01-11T19:20:59.014480", "rephrasing_model": "gpt-5-mini"}
{"claim": "Under the settings N_recur = 1, N_iter = 0, μ = 0, and γ̄ = 0, the method described in [6] and the TFG framework produce the same set of hypotheses.", "label": "Supported", "paragraph": "- DPS [6] H DPS is equivalent to H TFG ( N recur = 1 , N iter = 0 , µ = 0 , ¯ γ = 0) . - FreeDoM [78] H FreeDoM is equivalent to H TFG ( N iter = 0 , µ = 0 , ¯ γ = 0) . The complete analysis and proof of Theorem 3.2 is postponed to Appendix C.", "section_name": "Theorem 3.2. The hyper-parameter space of", "subsection_name": "", "paper_id": "N8YbGX98vc", "paper_title": "TFG: Unified Training-Free Guidance for Diffusion Models", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:36:24.365506", "model": "gpt-5-mini", "original_claim": "The DPS approach (reference [6]) yields a hypothesis set identical to the TFG hypothesis class when TFG is configured with N_recur = 1, N_iter = 0, μ = 0, and γ̄ = 0.", "rephrasing_timestamp": "2026-01-11T19:20:54.601620", "rephrasing_model": "gpt-5-mini"}
{"claim": "If TFG is configured with no iterations (N_iter = 0) and both μ and γ̄ set to zero, its model class is equivalent to the FreeDoM approach (see [78]).", "label": "Supported", "paragraph": "- DPS [6] H DPS is equivalent to H TFG ( N recur = 1 , N iter = 0 , µ = 0 , ¯ γ = 0) . - FreeDoM [78] H FreeDoM is equivalent to H TFG ( N iter = 0 , µ = 0 , ¯ γ = 0) . The complete analysis and proof of Theorem 3.2 is postponed to Appendix C.", "section_name": "Theorem 3.2. The hyper-parameter space of", "subsection_name": "", "paper_id": "N8YbGX98vc", "paper_title": "TFG: Unified Training-Free Guidance for Diffusion Models", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:36:24.365524", "model": "gpt-5-mini", "original_claim": "The FreeDoM method (reference [78]) matches the TFG hypothesis class when TFG is set with N_iter = 0, μ = 0, and γ̄ = 0.", "rephrasing_timestamp": "2026-01-11T19:20:57.666055", "rephrasing_model": "gpt-5-mini"}
{"claim": "Its developers claim TFG achieves comprehensive coverage of the H_TFG domain; by contrast, earlier approaches only reach a limited portion of that domain, and TFG is reported to deliver superior performance.", "label": "Supported", "paragraph": "It implies that existing algorithms are limited in expressivity, covering only a subset of H TFG. In contrast, TFG covers the entire space and is guaranteed to perform better. In addition, TFG streamlines nuances between existing methods, allowing for a unified way to compare and study different techniques.", "section_name": "Theorem 3.2. The hyper-parameter space of", "subsection_name": "", "paper_id": "N8YbGX98vc", "paper_title": "TFG: Unified Training-Free Guidance for Diffusion Models", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:36:31.185369", "model": "gpt-5-mini", "original_claim": "TFG spans the full H TFG space, unlike existing algorithms that only cover a subset, and is asserted to outperform those algorithms.", "rephrasing_timestamp": "2026-01-11T19:21:05.231922", "rephrasing_model": "gpt-5-mini"}
{"claim": "This section gives a formal formulation of the optimization task for reducing model size and surveys relevant established results on parameter discretization and the information-theoretic trade-off between bit-rate and approximation error.", "label": "Supported", "paragraph": "In this section, we rigorously define the model compression optimization problem and the relevant known results on quantization and the rate-distortion theory. Throughout, bold w denote weight vectors, unless stated otherwise.", "section_name": "3 PRELIMINARIES", "subsection_name": "", "paper_id": "CXjz7p4qha", "paper_title": "Rotation Invariant Quantization for Model Compression", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:36:47.176899", "model": "gpt-5-mini", "original_claim": "This section provides a rigorous definition of the model compression optimization problem and presents the pertinent established findings on quantization and rate-distortion theory.", "rephrasing_timestamp": "2026-01-11T19:21:00.083397", "rephrasing_model": "gpt-5-mini"}
{"claim": "Unless a different notation is explicitly introduced, any bold w appearing in this section denotes a parameter vector.", "label": "Supported", "paragraph": "In this section, we rigorously define the model compression optimization problem and the relevant known results on quantization and the rate-distortion theory. Throughout, bold w denote weight vectors, unless stated otherwise.", "section_name": "3 PRELIMINARIES", "subsection_name": "", "paper_id": "CXjz7p4qha", "paper_title": "Rotation Invariant Quantization for Model Compression", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:36:47.176910", "model": "gpt-5-mini", "original_claim": "Throughout the section, the boldface symbol w is used to denote weight vectors by default, except when an alternative notation is explicitly indicated.", "rephrasing_timestamp": "2026-01-11T19:21:03.478933", "rephrasing_model": "gpt-5-mini"}
{"claim": "The notation ∥·∥ denotes a vector's Euclidean length (ℓ₂), and ⟨·,·⟩ denotes the corresponding dot product.", "label": "Supported", "paragraph": "∥ · ∥ and ⟨· , ·⟩ denotes the standard ℓ 2 -norm and the inner product, respectively. We use p w ( w ) to denote the probability distribution of a random variable w. Hereafter, w [1: L ] = { w 1 , ..., w L } ∈ R N , denotes the concatenation of the weights of a pretrained model with L layers, where w ℓ ∈ R n ℓ are n ℓ weights of layer ℓ and N = ∑ L ℓ =1 n ℓ .", "section_name": "3 PRELIMINARIES", "subsection_name": "", "paper_id": "CXjz7p4qha", "paper_title": "Rotation Invariant Quantization for Model Compression", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:36:58.519604", "model": "gpt-5-mini", "original_claim": "The symbols ∥·∥ and ⟨·,·⟩ are used to denote the standard ℓ2 vector norm and the inner-product operation, respectively.", "rephrasing_timestamp": "2026-01-11T19:21:05.107851", "rephrasing_model": "gpt-5-mini"}
{"claim": "w[1:L] denotes the single parameter vector formed by stacking the per-stage vectors w1,…,wL of a network trained beforehand. For each stage ℓ, w_ℓ ∈ R^{n_ℓ}, and the overall dimension N equals n_1 + … + n_L (i.e., Σ_{ℓ=1}^L n_ℓ).", "label": "Supported", "paragraph": "∥ · ∥ and ⟨· , ·⟩ denotes the standard ℓ 2 -norm and the inner product, respectively. We use p w ( w ) to denote the probability distribution of a random variable w. Hereafter, w [1: L ] = { w 1 , ..., w L } ∈ R N , denotes the concatenation of the weights of a pretrained model with L layers, where w ℓ ∈ R n ℓ are n ℓ weights of layer ℓ and N = ∑ L ℓ =1 n ℓ .", "section_name": "3 PRELIMINARIES", "subsection_name": "", "paper_id": "CXjz7p4qha", "paper_title": "Rotation Invariant Quantization for Model Compression", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:36:58.519622", "model": "gpt-5-mini", "original_claim": "w[1:L] denotes the concatenated weight vector {w1,…,wL} of a pretrained model with L layers, where each layer ℓ has n_ℓ weights w_ℓ ∈ R^{n_ℓ} and the total dimension N equals Σ_{ℓ=1}^L n_ℓ.", "rephrasing_timestamp": "2026-01-11T19:21:12.953658", "rephrasing_model": "gpt-5-mini"}
{"claim": "This work evaluates both the space savings and the predictive performance obtained when RIQ is encoded with asymmetric numeral systems (ANS), compares those results to suitable reference approaches, and uses models initialized from pretraining for each respective task.", "label": "Supported", "paragraph": "In this section, we evaluate the compression ratio and model accuracy of RIQ with ANS and compare them to relevant baseline results. In all experiments, we use pre-trained models for their relevant tasks.", "section_name": "5 EMPIRICAL RESULTS", "subsection_name": "", "paper_id": "CXjz7p4qha", "paper_title": "Rotation Invariant Quantization for Model Compression", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:37:04.269595", "model": "gpt-5-mini", "original_claim": "The study measures both compression ratios and model accuracy for RIQ using ANS, comparing those metrics against relevant baseline methods while employing pre-trained models for each task.", "rephrasing_timestamp": "2026-01-11T19:21:09.330485", "rephrasing_model": "gpt-5-mini"}
{"claim": "A pretrained DDIM is used as the synthesis backbone, and the diffusion is run without randomness by disabling the stochastic term—keeping σ_t equal to 0 at every timestep.", "label": "Supported", "paragraph": "In this work, we adopt DDIM as our pre-trained generative process and we adopt a fully deterministic diffusion process ( σ t = 0 ). We propose a new iterative inversion process, which we summarize in Algorithm 1.", "section_name": "4.2 AN EFFICIENT DIFFUSION INVERSION", "subsection_name": "", "paper_id": "ZnmofqLWMQ", "paper_title": "Zero-shot Image Restoration via Diffusion Inversion", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:37:21.589779", "model": "gpt-5-mini", "original_claim": "The study employs DDIM as the pretrained generative model and implements a fully deterministic diffusion process with noise scale σ_t set to zero throughout.", "rephrasing_timestamp": "2026-01-11T19:21:12.975977", "rephrasing_model": "gpt-5-mini"}
{"claim": "The manuscript proposes a previously unreported successive-approximation inverse solver for their framework and provides a step-by-step outline of it under the label \"Algorithm 1.\"", "label": "Supported", "paragraph": "In this work, we adopt DDIM as our pre-trained generative process and we adopt a fully deterministic diffusion process ( σ t = 0 ). We propose a new iterative inversion process, which we summarize in Algorithm 1.", "section_name": "4.2 AN EFFICIENT DIFFUSION INVERSION", "subsection_name": "", "paper_id": "ZnmofqLWMQ", "paper_title": "Zero-shot Image Restoration via Diffusion Inversion", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:37:21.589789", "model": "gpt-5-mini", "original_claim": "The authors introduce a new iterative inversion algorithm for their approach and present its procedural summary as Algorithm 1 in the manuscript.", "rephrasing_timestamp": "2026-01-11T19:21:13.392438", "rephrasing_model": "gpt-5-mini"}
{"claim": "The authors employ a repeated refinement scheme guided by objective minimization to compute a numerical approximation of expression (12).", "label": "Supported", "paragraph": "We adopt an optimization-based iterative procedure to solve Eq. (12). In contrast to Kawar et al. (2022); Wang et al. (2023); Lugmayr et al. (2022), we explicitly use the correspondence between image samples x 0 and their associated latent vector x T .", "section_name": "4.2 AN EFFICIENT DIFFUSION INVERSION", "subsection_name": "", "paper_id": "ZnmofqLWMQ", "paper_title": "Zero-shot Image Restoration via Diffusion Inversion", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:37:37.585529", "model": "gpt-5-mini", "original_claim": "The paper uses an optimization-driven iterative algorithm to numerically obtain a solution for Equation (12).", "rephrasing_timestamp": "2026-01-11T19:21:12.370240", "rephrasing_model": "gpt-5-mini"}
{"claim": "Whereas Kawar et al. (2022), Wang et al. (2023), and Lugmayr et al. (2022) do not, our method takes advantage of the direct link that connects each original image x0 to its latent code xT.", "label": "Supported", "paragraph": "We adopt an optimization-based iterative procedure to solve Eq. (12). In contrast to Kawar et al. (2022); Wang et al. (2023); Lugmayr et al. (2022), we explicitly use the correspondence between image samples x 0 and their associated latent vector x T .", "section_name": "4.2 AN EFFICIENT DIFFUSION INVERSION", "subsection_name": "", "paper_id": "ZnmofqLWMQ", "paper_title": "Zero-shot Image Restoration via Diffusion Inversion", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:37:37.585543", "model": "gpt-5-mini", "original_claim": "In contrast to Kawar et al. (2022), Wang et al. (2023), and Lugmayr et al. (2022), our approach explicitly exploits the correspondence between each image sample x0 and its latent vector xT.", "rephrasing_timestamp": "2026-01-11T19:21:20.766233", "rephrasing_model": "gpt-5-mini"}
{"claim": "At the start, SHRED produces x_0^T as a random vector governed by a multivariate Gaussian distribution with zero mean and identity covariance.", "label": "Supported", "paragraph": "SHRED starts from an initial noise instance x 0 T ∼ N (0 , I ) and at the end of each iteration k , we update x k T . At each iteration, we first predict ˆ x 0 | t from x t using the pre-trained DDIM model ϵ θ .", "section_name": "4.2 AN EFFICIENT DIFFUSION INVERSION", "subsection_name": "", "paper_id": "ZnmofqLWMQ", "paper_title": "Zero-shot Image Restoration via Diffusion Inversion", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:37:45.508906", "model": "gpt-5-mini", "original_claim": "SHRED initializes its process with an initial noise sample x_0^T drawn from a multivariate standard normal distribution N(0, I).", "rephrasing_timestamp": "2026-01-11T19:21:23.185295", "rephrasing_model": "gpt-5-mini"}
{"claim": "The reviewer must evaluate every generated picture together with its originating written description and record a numeric rating that reflects the picture's visual appeal.", "label": "Supported", "paragraph": "Your role is to evaluate the aesthetic quality score of given images (\"Images\") generated by the corresponding text ('Input'). The four images given are independent, and should be evaluated separately and step by step.", "section_name": "Aesthetic:", "subsection_name": "", "paper_id": "IRXyPm9IPW", "paper_title": "Multimodal Large Language Models Make Text-to-Image Generative Models Align Better", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:38:16.252217", "model": "gpt-5-mini", "original_claim": "The evaluator's responsibility is to determine and assign an aesthetic-quality numerical score for each image produced from its respective textual input, treating every image as linked to its corresponding prompt.", "rephrasing_timestamp": "2026-01-11T19:21:20.215978", "rephrasing_model": "gpt-5-mini"}
{"claim": "The collection includes four distinct visuals that are unlinked to one another and must be handled individually, each being processed through a prescribed, ordered series of review steps.", "label": "Supported", "paragraph": "Your role is to evaluate the aesthetic quality score of given images (\"Images\") generated by the corresponding text ('Input'). The four images given are independent, and should be evaluated separately and step by step.", "section_name": "Aesthetic:", "subsection_name": "", "paper_id": "IRXyPm9IPW", "paper_title": "Multimodal Large Language Models Make Text-to-Image Generative Models Align Better", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:38:16.252228", "model": "gpt-5-mini", "original_claim": "The set contains four separate images that are independent of one another and must be assessed individually, following a sequential, step-by-step evaluation procedure for each image.", "rephrasing_timestamp": "2026-01-11T19:21:22.062029", "rephrasing_model": "gpt-5-mini"}
{"claim": "The evaluation produced does not depend on how the submitted images are arranged; changing their placement will not alter the resulting value.", "label": "Supported", "paragraph": "Note that the rating has nothing to do with image input order. Scoring : Rating outputs 1 to 5:\n1. Bad : Extremely blurry, underexposed with significant noise, indiscernible subjects, and chaotic composition.", "section_name": "Aesthetic:", "subsection_name": "", "paper_id": "IRXyPm9IPW", "paper_title": "Multimodal Large Language Models Make Text-to-Image Generative Models Align Better", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:38:25.984375", "model": "gpt-5-mini", "original_claim": "The assigned rating is unrelated to the sequence of image inputs, meaning the image input order does not affect the score.", "rephrasing_timestamp": "2026-01-11T19:21:21.478411", "rephrasing_model": "gpt-5-mini"}
{"claim": "A five-point rating system is used; the lowest grade applies to pictures that are extremely out of focus, very dark, plagued by heavy visual grain, whose subjects are unrecognizable, and that display chaotic framing.", "label": "Supported", "paragraph": "Note that the rating has nothing to do with image input order. Scoring : Rating outputs 1 to 5:\n1. Bad : Extremely blurry, underexposed with significant noise, indiscernible subjects, and chaotic composition.", "section_name": "Aesthetic:", "subsection_name": "", "paper_id": "IRXyPm9IPW", "paper_title": "Multimodal Large Language Models Make Text-to-Image Generative Models Align Better", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:38:25.984396", "model": "gpt-5-mini", "original_claim": "The scoring scale runs from 1 to 5; a score of one denotes images that are highly blurred, underexposed, contain substantial noise, have subjects that cannot be identified, and exhibit disordered composition.", "rephrasing_timestamp": "2026-01-11T19:21:34.723131", "rephrasing_model": "gpt-5-mini"}
{"claim": "An adequate-quality image shows clear focus and sufficient lighting, features subdued tones and a serviceable layout, but demonstrates little in the way of creative or artistic flair.", "label": "Supported", "paragraph": "2. Poor : Noticeable blur, poor lighting, washed-out colors, and awkward composition with cut-off subjects. 3. Fair : In focus with adequate lighting, dull colors, decent composition but lacks creativity.", "section_name": "Aesthetic:", "subsection_name": "", "paper_id": "IRXyPm9IPW", "paper_title": "Multimodal Large Language Models Make Text-to-Image Generative Models Align Better", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:38:32.590077", "model": "gpt-5-mini", "original_claim": "A 'Fair' image is described as having sharp focus and sufficient illumination, exhibiting muted colors and acceptable composition, but characterized by a deficiency in creative elements.", "rephrasing_timestamp": "2026-01-11T19:21:28.987094", "rephrasing_model": "gpt-5-mini"}
{"claim": "Following previous work (Chen et al., 2021; He et al., 2016), they gauge the usefulness of the model's features by training a single-layer classifier on top of frozen encoder outputs.", "label": "Supported", "paragraph": "Linear Evaluation. We follow the common practice for linear evaluation Chen et al. (2021); He et al. (2016). Following pre-training we obtain frozen features from the backbone and train a linear classifier over ImageNet-1k for 100 epochs in a supervised manner.", "section_name": "A.2 EVALUATION", "subsection_name": "", "paper_id": "P50qJuu4IY", "paper_title": "Self-Supervised Learning with the Matching Gap", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:38:50.323383", "model": "gpt-5-mini", "original_claim": "They adopt the standard linear evaluation protocol employed in prior studies (Chen et al., 2021; He et al., 2016) to evaluate the learned representations.", "rephrasing_timestamp": "2026-01-11T19:21:35.597214", "rephrasing_model": "gpt-5-mini"}
{"claim": "After the initial training phase, the model's learned representation weights are kept fixed, and a single-layer prediction head is fitted to the 1,000-class ImageNet annotations using labeled data for 100 complete passes over the training set.", "label": "Supported", "paragraph": "Linear Evaluation. We follow the common practice for linear evaluation Chen et al. (2021); He et al. (2016). Following pre-training we obtain frozen features from the backbone and train a linear classifier over ImageNet-1k for 100 epochs in a supervised manner.", "section_name": "A.2 EVALUATION", "subsection_name": "", "paper_id": "P50qJuu4IY", "paper_title": "Self-Supervised Learning with the Matching Gap", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:38:50.323394", "model": "gpt-5-mini", "original_claim": "After pre-training, the backbone's feature representations are frozen and a linear classifier is trained on ImageNet-1k labels in a supervised setup for 100 epochs.", "rephrasing_timestamp": "2026-01-11T19:21:35.848694", "rephrasing_model": "gpt-5-mini"}
{"claim": "Model training used a stochastic gradient-based optimizer on mini-batches of 4,096 samples with no parameter-decay regularization, and the experiment tested a range of optimization step sizes.", "label": "Supported", "paragraph": "We use the SGD optimizer, with a batch size of 4096, sweep over learning rate values and with zero weight decay . For image augmentations we use only random horizontal flipping, random resized cropping and normalization.", "section_name": "A.2 EVALUATION", "subsection_name": "", "paper_id": "P50qJuu4IY", "paper_title": "Self-Supervised Learning with the Matching Gap", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:38:58.591873", "model": "gpt-5-mini", "original_claim": "Training employed stochastic gradient descent (SGD) with a batch size of 4096 and no weight decay, and the experiment performed a sweep over learning rate values.", "rephrasing_timestamp": "2026-01-11T19:21:32.878006", "rephrasing_model": "gpt-5-mini"}
{"claim": "Only three simple operations were performed on the input images — occasional left-right mirroring, randomly sampled and scaled patches, and pixel-value standardization — and no other data-altering methods were used.", "label": "Supported", "paragraph": "We use the SGD optimizer, with a batch size of 4096, sweep over learning rate values and with zero weight decay . For image augmentations we use only random horizontal flipping, random resized cropping and normalization.", "section_name": "A.2 EVALUATION", "subsection_name": "", "paper_id": "P50qJuu4IY", "paper_title": "Self-Supervised Learning with the Matching Gap", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:38:58.591891", "model": "gpt-5-mini", "original_claim": "Image preprocessing applied only random horizontal flipping, random resized cropping, and normalization, excluding any additional augmentation techniques.", "rephrasing_timestamp": "2026-01-11T19:21:36.888975", "rephrasing_model": "gpt-5-mini"}
{"claim": "For the instance-based classifier, they relied on the 20 most similar examples and applied a softmax scaling factor of 0.07 to weight votes, as is customary.", "label": "Supported", "paragraph": "k -NN Classification. We follow standard practice, fixing the number of neighbors to k = 20 , and setting the voting temperature as T = 0 . 07 . Transfer Learning. We follow the procedure reported in DINO Carion et al.", "section_name": "A.2 EVALUATION", "subsection_name": "", "paper_id": "P50qJuu4IY", "paper_title": "Self-Supervised Learning with the Matching Gap", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:39:03.673125", "model": "gpt-5-mini", "original_claim": "For k-NN classification, they used twenty nearest neighbors (k = 20) and set the voting temperature to 0.07, following standard practice.", "rephrasing_timestamp": "2026-01-11T19:21:42.046525", "rephrasing_model": "gpt-5-mini"}
{"claim": "The subproblem's Lagrange function is formed from nonnegative coefficient vectors λ_f ∈ R_+^M, λ_g ∈ R_+^{M_g}, and λ_h ∈ R_+^{M_h}, i.e., each λ has the indicated dimension and all components ≥ 0.", "label": "Supported", "paragraph": "Proof of subprogram reformulation. Define the Lagrangian function\n<!-- formula-not-decoded -->\nwhere λ f ∈ R M + , λ g ∈ R M g + , λ h ∈ R M h . By the first-order optimality condition w.r.t. d and c , we can obtain that\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nCombining the last equation with λ f ∈ R M + , we obtain λ ∗ f ∈ Ω λ f ( θ ) .", "section_name": "D.1 Lagrangian of the subprogram", "subsection_name": "", "paper_id": "BmG3NgH5xu", "paper_title": "FERERO: A Flexible Framework for Preference-Guided Multi-Objective Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:39:38.799610", "model": "gpt-5-mini", "original_claim": "The Lagrangian for the subprogram is constructed using multiplier vectors λ_f ∈ R^{M}_{+}, λ_g ∈ R^{M_g}_{+}, and λ_h ∈ R^{M_h}_{+}, i.e., nonnegative vectors of respective dimensions.", "rephrasing_timestamp": "2026-01-11T19:21:46.458002", "rephrasing_model": "gpt-5-mini"}
{"claim": "Equating the derivatives with respect to d and c to zero and using that λ_f cannot be negative shows that the optimal dual variable λ_f^* is an element of Ω_{λ_f}(θ).", "label": "Supported", "paragraph": "Proof of subprogram reformulation. Define the Lagrangian function\n<!-- formula-not-decoded -->\nwhere λ f ∈ R M + , λ g ∈ R M g + , λ h ∈ R M h . By the first-order optimality condition w.r.t. d and c , we can obtain that\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nCombining the last equation with λ f ∈ R M + , we obtain λ ∗ f ∈ Ω λ f ( θ ) .", "section_name": "D.1 Lagrangian of the subprogram", "subsection_name": "", "paper_id": "BmG3NgH5xu", "paper_title": "FERERO: A Flexible Framework for Preference-Guided Multi-Objective Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:39:38.799622", "model": "gpt-5-mini", "original_claim": "Applying first-order optimality conditions with respect to variables d and c, together with the nonnegativity of λ_f, yields that the optimal multiplier λ_f^* lies in the set Ω_{λ_f}(θ).", "rephrasing_timestamp": "2026-01-11T19:21:52.986505", "rephrasing_model": "gpt-5-mini"}
{"claim": "Applying the formulas derived earlier to the model's Lagrange function yields the equivalent dual problem shown in Eq. (2.3).", "label": "Supported", "paragraph": "Plugging the above results into the Lagrangian function gives\n<!-- formula-not-decoded -->\nwhich leads to the dual form in (2.3). Since (2.1) is a constrained convex optimization problem where the Slater's condition holds, therefore, the duality gap is zero.", "section_name": "D.1 Lagrangian of the subprogram", "subsection_name": "", "paper_id": "BmG3NgH5xu", "paper_title": "FERERO: A Flexible Framework for Preference-Guided Multi-Objective Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:39:47.265821", "model": "gpt-5-mini", "original_claim": "Substituting the previously obtained expressions into the Lagrangian yields the corresponding dual formulation presented as equation (2.3).", "rephrasing_timestamp": "2026-01-11T19:21:44.662411", "rephrasing_model": "gpt-5-mini"}
{"claim": "Problem (2.1) is a convex optimization problem with constraints that admits a strictly feasible point; therefore the dual's optimal value equals the primal's, meaning there is no discrepancy between their optimal values.", "label": "Supported", "paragraph": "Plugging the above results into the Lagrangian function gives\n<!-- formula-not-decoded -->\nwhich leads to the dual form in (2.3). Since (2.1) is a constrained convex optimization problem where the Slater's condition holds, therefore, the duality gap is zero.", "section_name": "D.1 Lagrangian of the subprogram", "subsection_name": "", "paper_id": "BmG3NgH5xu", "paper_title": "FERERO: A Flexible Framework for Preference-Guided Multi-Objective Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:39:47.265837", "model": "gpt-5-mini", "original_claim": "Problem (2.1) is a constrained convex optimization instance satisfying Slater's condition, so strong duality holds and the primal-dual gap is zero.", "rephrasing_timestamp": "2026-01-11T19:21:48.455648", "rephrasing_model": "gpt-5-mini"}
{"claim": "A pared-down inner problem is proposed in which the matrix A is fixed as the identity and no adjustments that depend on the objective function values are performed; this construction is identical to solving the constrained multi-criteria optimization task by the sequential quadratic programming approach (i.e., by repeatedly solving quadratic approximations).", "label": "Supported", "paragraph": "Remark 4. Note that we can also have a simplified subprogram with A = I , and without adaptation to the objective values, as defined below\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nThis formulation corresponds to the SQP method applied to the constrained MOO problem [16].", "section_name": "D.1 Lagrangian of the subprogram", "subsection_name": "", "paper_id": "BmG3NgH5xu", "paper_title": "FERERO: A Flexible Framework for Preference-Guided Multi-Objective Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:39:55.155764", "model": "gpt-5-mini", "original_claim": "A simplified subprogram with A set to the identity matrix and no adaptation to objective values is presented, and this formulation is equivalent to applying the SQP method to the constrained multi-objective optimization problem.", "rephrasing_timestamp": "2026-01-11T19:21:55.607353", "rephrasing_model": "gpt-5-mini"}
{"claim": "The authors introduce the Moreau regularization and the proximal mapping for each convex function f: R^d → R, noting that both constructions are taken with respect to a fixed positive scalar η (η > 0).", "label": "Supported", "paragraph": "We first describe the method and practical details of the implementation, followed by the convergence analysis in the next section. Notation. The Moreau envelope and the proximal (prox) operator of a convex function f : R d → R are respectively defined for a constant η &gt; 0 as\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nA fundamental property is that the gradient of the Moreau envelope is related to the proximal operator:\n<!-- formula-not-decoded -->\nFor simplicity, we denote ¯ ν = 2 nν .", "section_name": "E.1 OVERVIEW", "subsection_name": "", "paper_id": "TTrzgEZt9s", "paper_title": "Distributionally Robust Optimization with Bias and Variance Reduction", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:40:41.483889", "model": "gpt-5-mini", "original_claim": "The authors define the Moreau envelope and the proximal operator for any convex function f: R^d → R, specifying these constructions are given for a fixed positive parameter η (η > 0).", "rephrasing_timestamp": "2026-01-11T19:21:54.504587", "rephrasing_model": "gpt-5-mini"}
{"claim": "To make later formulas more compact, the authors define ν̄ := 2 n ν (i.e., ν̄ = 2·n·ν).", "label": "Supported", "paragraph": "We first describe the method and practical details of the implementation, followed by the convergence analysis in the next section. Notation. The Moreau envelope and the proximal (prox) operator of a convex function f : R d → R are respectively defined for a constant η &gt; 0 as\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nA fundamental property is that the gradient of the Moreau envelope is related to the proximal operator:\n<!-- formula-not-decoded -->\nFor simplicity, we denote ¯ ν = 2 nν .", "section_name": "E.1 OVERVIEW", "subsection_name": "", "paper_id": "TTrzgEZt9s", "paper_title": "Distributionally Robust Optimization with Bias and Variance Reduction", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:40:41.483902", "model": "gpt-5-mini", "original_claim": "The paper adopts the notation \"bar ν\" to denote the quantity equal to two times the product of n and ν (i.e., bar ν = 2 · n · ν), to simplify subsequent expressions.", "rephrasing_timestamp": "2026-01-11T19:21:54.110566", "rephrasing_model": "gpt-5-mini"}
{"claim": "During every step the algorithm selects i_t by sampling from the probability vector q(t). Consequently, the probability that i_t equals i is the value of q(t) at position i, meaning indices are chosen with unequal probabilities.", "label": "Supported", "paragraph": "Algorithm Description. The algorithm is nearly equivalent to Algorithm 3, but makes the following changes. We sample i t ∼ q ( t ) non-uniformly in the sense that P [ i t = i ] = q ( t ) i . We do not store an additional vector of weights ρ ( t ) , and use q ( t ) in all associated steps.", "section_name": "E.1 OVERVIEW", "subsection_name": "", "paper_id": "TTrzgEZt9s", "paper_title": "Distributionally Robust Optimization with Bias and Variance Reduction", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:41:00.085003", "model": "gpt-5-mini", "original_claim": "At each iteration the method draws the index i_t from the categorical distribution q(t), so the probability that i_t equals i is given by the i-th component of q(t), implementing non-uniform sampling.", "rephrasing_timestamp": "2026-01-11T19:21:58.442637", "rephrasing_model": "gpt-5-mini"}
{"claim": "The procedure does not maintain a separate parameter array; whenever the method would normally modify or draw from that array, it uses q(t) instead.", "label": "Supported", "paragraph": "Algorithm Description. The algorithm is nearly equivalent to Algorithm 3, but makes the following changes. We sample i t ∼ q ( t ) non-uniformly in the sense that P [ i t = i ] = q ( t ) i . We do not store an additional vector of weights ρ ( t ) , and use q ( t ) in all associated steps.", "section_name": "E.1 OVERVIEW", "subsection_name": "", "paper_id": "TTrzgEZt9s", "paper_title": "Distributionally Robust Optimization with Bias and Variance Reduction", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:41:00.085021", "model": "gpt-5-mini", "original_claim": "The algorithm does not retain an auxiliary weight vector and instead applies q(t) directly in all corresponding update and sampling steps that would otherwise use a stored weights vector.", "rephrasing_timestamp": "2026-01-11T19:22:09.983962", "rephrasing_model": "gpt-5-mini"}
{"claim": "At every update step, w(t) is first augmented by adding the variance-reduction vector u(t), and only then is the proximal mapping applied.", "label": "Supported", "paragraph": "This does not change the expectation of the update direction or the control variate, but creates minor changes in the analysis of the variance term. In the iterate update, we replace the gradient descent-like update with\n<!-- formula-not-decoded -->\nThe vector u ( t ) adds the control variate to w ( t ) before passing it to the proximal operator.", "section_name": "E.1 OVERVIEW", "subsection_name": "", "paper_id": "TTrzgEZt9s", "paper_title": "Distributionally Robust Optimization with Bias and Variance Reduction", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T15:41:07.585405", "model": "gpt-5-mini", "original_claim": "During each iterate update, the control variate vector u(t) is added to w(t) prior to invoking the proximal operator.", "rephrasing_timestamp": "2026-01-11T19:22:08.505671", "rephrasing_model": "gpt-5-mini"}
{"claim": "A parameterized neural model g converts d_in-dimensional real input vectors into d_out-dimensional real output vectors, with its trainable weights assembled into a p-dimensional real vector w ∈ ℝ^p.", "label": "Supported", "paragraph": "Preliminaries. We use a NN denoted as f θ : R d in → R d out , where θ ∈ R p denotes the trainable parameters. The training dataset comprises n data-label pairs D = { ( x 1 , y 1 ) , . . . , ( x n , y n ) } .", "section_name": "2 Background", "subsection_name": "", "paper_id": "wJaCsnT9UE", "paper_title": "Sharpness-diversity tradeoff: improving flat ensembles with SharpBalance", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:41:35.709707", "model": "gpt-5-mini", "original_claim": "A neural network f_θ is employed that maps inputs from R^{d_in} to outputs in R^{d_out}, with its learnable parameters represented by a vector θ in R^p.", "rephrasing_timestamp": "2026-01-11T19:22:06.464360", "rephrasing_model": "gpt-5-mini"}
{"claim": "The dataset comprises N examples explicitly enumerated from the first to the Nth, with each entry linking an observed feature record to its corresponding target value.", "label": "Supported", "paragraph": "Preliminaries. We use a NN denoted as f θ : R d in → R d out , where θ ∈ R p denotes the trainable parameters. The training dataset comprises n data-label pairs D = { ( x 1 , y 1 ) , . . . , ( x n , y n ) } .", "section_name": "2 Background", "subsection_name": "", "paper_id": "wJaCsnT9UE", "paper_title": "Sharpness-diversity tradeoff: improving flat ensembles with SharpBalance", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:41:35.709718", "model": "gpt-5-mini", "original_claim": "The training set D consists of n paired examples, explicitly listed as (x1,y1) through (xn,yn), where each pair associates an input vector with a corresponding label.", "rephrasing_timestamp": "2026-01-11T19:22:11.371465", "rephrasing_model": "gpt-5-mini"}
{"claim": "The empirical error of a parameterized predictor g_w on a sample set S containing m examples is defined as the simple average of the per-example costs c(g_w(x_i), y_i), i.e. (1/m) ∑_{i=1}^m c(g_w(x_i), y_i).", "label": "Supported", "paragraph": "The training loss of NN f θ over a dataset D can be defined as L D ( θ ) = 1 n ∑ n i =1 ℓ ( f θ ( x i ) , y i ) . Here ℓ ( · ) is a loss function, which, for instance, can be the cross entropy loss or ℓ 2 loss.", "section_name": "2 Background", "subsection_name": "", "paper_id": "wJaCsnT9UE", "paper_title": "Sharpness-diversity tradeoff: improving flat ensembles with SharpBalance", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:41:50.212402", "model": "gpt-5-mini", "original_claim": "The training loss L_D(θ) for neural network f_θ on dataset D is computed as the arithmetic mean over n training examples of the individual losses ℓ(f_θ(x_i), y_i).", "rephrasing_timestamp": "2026-01-11T19:22:09.254684", "rephrasing_model": "gpt-5-mini"}
{"claim": "The sample-wise cost term in the training objective can be defined either by a log-loss (logarithmic loss) or by a squared-error measure (mean squared error).", "label": "Supported", "paragraph": "The training loss of NN f θ over a dataset D can be defined as L D ( θ ) = 1 n ∑ n i =1 ℓ ( f θ ( x i ) , y i ) . Here ℓ ( · ) is a loss function, which, for instance, can be the cross entropy loss or ℓ 2 loss.", "section_name": "2 Background", "subsection_name": "", "paper_id": "wJaCsnT9UE", "paper_title": "Sharpness-diversity tradeoff: improving flat ensembles with SharpBalance", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:41:50.212417", "model": "gpt-5-mini", "original_claim": "The per-example loss function ℓ used in that training objective can be chosen as the cross-entropy loss or as the squared ℓ2 (mean-squared) loss.", "rephrasing_timestamp": "2026-01-11T19:22:17.983808", "rephrasing_model": "gpt-5-mini"}
{"claim": "They assemble a committee of m independently trained deep neural models and, for classification tasks, produce the ensemble's output by taking the mean of each model's pre-softmax class scores.", "label": "Supported", "paragraph": "We construct a deep ensemble consisting of m distinct NNs f θ 1 , . . . , f θ m . For classification tasks, the ensemble's output is derived by averaging the predicted logits of these individual networks.", "section_name": "2 Background", "subsection_name": "", "paper_id": "wJaCsnT9UE", "paper_title": "Sharpness-diversity tradeoff: improving flat ensembles with SharpBalance", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:41:56.104141", "model": "gpt-5-mini", "original_claim": "The authors build a deep ensemble of m distinct neural networks and, for classification problems, compute the ensemble prediction by averaging the output logits produced by the individual networks.", "rephrasing_timestamp": "2026-01-11T19:22:16.238018", "rephrasing_model": "gpt-5-mini"}
{"claim": "On the Ego corpus, the single-pass algorithm required 33.62 hours of processing time and consumed 22.35 gigabytes of RAM.", "label": "Supported", "paragraph": "|          | Ego-small   | Ego-small   | Enzymes   | Enzymes     | Ego      | Ego         | Community-small   | Community-small   |\n|----------|----------|----------|----------|----------|----------|----------|----------|----------|\n| Method    | Time (h)    | Memory (GB) | Time (h)  | Memory (GB) | Time (h) | Memory (GB) | Time (h)          | Memory (GB)       |\n| seq-1     | 12.15       | 3.19        | 55.5      | 13.68       | 77.86    | 22.17       | 14.39          | 3.81          |\n| seq-small | 9.27        | 3.22        | 28.81     | 13.15       | 77.80    | 21.97       | 10.75          | 3.21          |\n| seq-big   | 14.29       | 7.54        | 28.07     | 14.81       | 58.28    | 22.44       | 12.5          | 4.62          |\n| oneshot   | 8.17        | 4.23        | 28.14     | 14.90       | 33.62    | 22.35       | 7.13          | 4.47          |\n(e) Generation time/memory\n|          | Ego-small   | Ego-small   | Enzymes   | Enzymes     | Ego      | Ego         | Community-small   | Community-small   |\n|----------|----------|----------|----------|----------|----------|----------|----------|----------|\n| Method    | Time (m)    | Memory (GB) | Time (m)  | Memory (GB) | Time (m) | Memory (GB) | Time (m)          | Memory (GB)       |\n| seq-1     | 4.98        | 0.17        | 31.39     | 0.15        | 458.89   | 0.13        | 7.30          | 0.25          |\n| seq-small | 3.36        | 0.20        | 11.36     | 0.19        | 268.89   | 0.17        | 5.62          | 0.30          |\n| seq-big   | 9.57        | 0.89        | 11.39     | 0.37        | 83.19    | 0.36        | 13.68          | 1.16          |\n| oneshot   | 5.18        | 1.60        | 23.59     | 7.51        | 202.73   | 11.40       | 7.42          | 2.24          |", "section_name": "(d) Training time/memory", "subsection_name": "", "paper_id": "lmYGRGyL4i", "paper_title": "Uncovering the Spectrum of Graph Generative Models: From One-Shot to Sequential", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:42:22.194626", "model": "gpt-5-mini", "original_claim": "For the Ego dataset, the oneshot method ran for 33.62 hours and occupied 22.35 GB of memory.", "rephrasing_timestamp": "2026-01-11T19:22:21.974909", "rephrasing_model": "gpt-5-mini"}
{"claim": "On the Enzymes benchmark, a generation run using seq-small lasted 11.36 minutes and consumed 0.19 gigabytes of RAM.", "label": "Supported", "paragraph": "|          | Ego-small   | Ego-small   | Enzymes   | Enzymes     | Ego      | Ego         | Community-small   | Community-small   |\n|----------|----------|----------|----------|----------|----------|----------|----------|----------|\n| Method    | Time (h)    | Memory (GB) | Time (h)  | Memory (GB) | Time (h) | Memory (GB) | Time (h)          | Memory (GB)       |\n| seq-1     | 12.15       | 3.19        | 55.5      | 13.68       | 77.86    | 22.17       | 14.39          | 3.81          |\n| seq-small | 9.27        | 3.22        | 28.81     | 13.15       | 77.80    | 21.97       | 10.75          | 3.21          |\n| seq-big   | 14.29       | 7.54        | 28.07     | 14.81       | 58.28    | 22.44       | 12.5          | 4.62          |\n| oneshot   | 8.17        | 4.23        | 28.14     | 14.90       | 33.62    | 22.35       | 7.13          | 4.47          |\n(e) Generation time/memory\n|          | Ego-small   | Ego-small   | Enzymes   | Enzymes     | Ego      | Ego         | Community-small   | Community-small   |\n|----------|----------|----------|----------|----------|----------|----------|----------|----------|\n| Method    | Time (m)    | Memory (GB) | Time (m)  | Memory (GB) | Time (m) | Memory (GB) | Time (m)          | Memory (GB)       |\n| seq-1     | 4.98        | 0.17        | 31.39     | 0.15        | 458.89   | 0.13        | 7.30          | 0.25          |\n| seq-small | 3.36        | 0.20        | 11.36     | 0.19        | 268.89   | 0.17        | 5.62          | 0.30          |\n| seq-big   | 9.57        | 0.89        | 11.39     | 0.37        | 83.19    | 0.36        | 13.68          | 1.16          |\n| oneshot   | 5.18        | 1.60        | 23.59     | 7.51        | 202.73   | 11.40       | 7.42          | 2.24          |", "section_name": "(d) Training time/memory", "subsection_name": "", "paper_id": "lmYGRGyL4i", "paper_title": "Uncovering the Spectrum of Graph Generative Models: From One-Shot to Sequential", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:42:22.194637", "model": "gpt-5-mini", "original_claim": "During generation on the Enzymes dataset, the seq-small method finished in 11.36 minutes while using 0.19 GB of memory.", "rephrasing_timestamp": "2026-01-11T19:22:20.546076", "rephrasing_model": "gpt-5-mini"}
{"claim": "Under the simplest scheme, nodes are processed one at a time: an independent two-outcome chance test is applied to each node, and the node is discarded whenever that test returns the outcome marked for removal.", "label": "Supported", "paragraph": "The presented naive method is equivalent to tossing a coin for each node, and removing it for some outcome. A Bernoulli random variable with probability q t is assigned to each node. All nodes with a positive outcome are removed.", "section_name": "B.1 NAIVE (BINOMIAL)", "subsection_name": "", "paper_id": "lmYGRGyL4i", "paper_title": "Uncovering the Spectrum of Graph Generative Models: From One-Shot to Sequential", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:42:37.240036", "model": "gpt-5-mini", "original_claim": "The naive procedure treats each node independently by performing a random binary trial for every node and deleting the node whenever that trial produces the designated deletion outcome.", "rephrasing_timestamp": "2026-01-11T19:22:24.775661", "rephrasing_model": "gpt-5-mini"}
{"claim": "Each element of the structure is given an independent binary random indicator that equals 1 with probability q_t, and any element whose indicator is 1 is deleted from the system.", "label": "Supported", "paragraph": "The presented naive method is equivalent to tossing a coin for each node, and removing it for some outcome. A Bernoulli random variable with probability q t is assigned to each node. All nodes with a positive outcome are removed.", "section_name": "B.1 NAIVE (BINOMIAL)", "subsection_name": "", "paper_id": "lmYGRGyL4i", "paper_title": "Uncovering the Spectrum of Graph Generative Models: From One-Shot to Sequential", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:42:37.240051", "model": "gpt-5-mini", "original_claim": "Each node is assigned an independent Bernoulli random variable with success probability q_t, and any node that returns a success (positive) outcome is removed from the structure.", "rephrasing_timestamp": "2026-01-11T19:22:29.926064", "rephrasing_model": "gpt-5-mini"}
{"claim": "Conditional on n_{t-1}, n_t is the count of successes from n_{t-1} independent Bernoulli trials, each succeeding with probability 1 - q_t.", "label": "Supported", "paragraph": "The two components in Eq. 6 are found to be:\n<!-- formula-not-decoded -->\nthat is, the conditional n t | n t -1 is a Binomial random variable B ( n t ; n t -1 , 1 -q t ) , and ( n t -1 n t ) are all the ways of choosing n t nodes from a total of n t -1 .", "section_name": "B.1 NAIVE (BINOMIAL)", "subsection_name": "", "paper_id": "lmYGRGyL4i", "paper_title": "Uncovering the Spectrum of Graph Generative Models: From One-Shot to Sequential", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:42:51.406479", "model": "gpt-5-mini", "original_claim": "Given n_{t-1}, the random variable n_t follows a Binomial distribution with number of trials equal to n_{t-1} and success probability 1 - q_t.", "rephrasing_timestamp": "2026-01-11T19:22:26.947831", "rephrasing_model": "gpt-5-mini"}
{"claim": "This project was implemented in the Python language and employs the PyTorch, NumPy, and scikit-learn packages to provide its functionality.", "label": "Supported", "paragraph": "The implementation is in Python and uses PyTorch (Paszke et al., 2017), NumPy (Harris et al., 2020), and scikit-learn (Pedregosa et al., 2011). Our experiments have been evaluated on GPUs (NVIDIA GeForce RTX 2080 Ti) in an Ubuntu 20.04.2 LTS environment.", "section_name": "B.1 TRAINING AND ACTIVE LEARNING PARAMETERS", "subsection_name": "", "paper_id": "yZBpnKpBCw", "paper_title": "Time- and Label-efficient Active Learning by Diversity and Uncertainty of Probabilities", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:43:09.790076", "model": "gpt-5-mini", "original_claim": "The codebase was developed using the Python programming language and relies on the PyTorch, NumPy, and scikit-learn libraries for its implementation.", "rephrasing_timestamp": "2026-01-11T19:22:31.362641", "rephrasing_model": "gpt-5-mini"}
{"claim": "All experiments were executed on systems equipped with NVIDIA RTX 2080 Ti graphics cards running the Ubuntu 20.04.2 LTS operating system.", "label": "Supported", "paragraph": "The implementation is in Python and uses PyTorch (Paszke et al., 2017), NumPy (Harris et al., 2020), and scikit-learn (Pedregosa et al., 2011). Our experiments have been evaluated on GPUs (NVIDIA GeForce RTX 2080 Ti) in an Ubuntu 20.04.2 LTS environment.", "section_name": "B.1 TRAINING AND ACTIVE LEARNING PARAMETERS", "subsection_name": "", "paper_id": "yZBpnKpBCw", "paper_title": "Time- and Label-efficient Active Learning by Diversity and Uncertainty of Probabilities", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:43:09.790087", "model": "gpt-5-mini", "original_claim": "All experimental runs were carried out on NVIDIA GeForce RTX 2080 Ti GPUs within an Ubuntu 20.04.2 LTS operating system environment.", "rephrasing_timestamp": "2026-01-11T19:22:29.339925", "rephrasing_model": "gpt-5-mini"}
{"claim": "For the evaluation, the BloodMNIST collection contains micrographs of several healthy blood-cell morphologies, divided into eight separate categories.", "label": "Supported", "paragraph": "For more details, we refer to our publicly available code base. An overview of the evaluated dataset and statistics is given in Table 4. BloodMNIST contains images from different normal cells belonging to eight classes, and DermaMNIST consists of dermatoscopic images categorizing seven different diseases (Yang et al., 2023).", "section_name": "B.1 TRAINING AND ACTIVE LEARNING PARAMETERS", "subsection_name": "", "paper_id": "yZBpnKpBCw", "paper_title": "Time- and Label-efficient Active Learning by Diversity and Uncertainty of Probabilities", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:43:20.981724", "model": "gpt-5-mini", "original_claim": "The BloodMNIST dataset used in the evaluation comprises images of various normal blood cell types organized into eight distinct class labels.", "rephrasing_timestamp": "2026-01-11T19:22:32.186308", "rephrasing_model": "gpt-5-mini"}
{"claim": "The DermaMNIST collection examined in this study is composed of skin‑lesion photographs that have been classified into seven distinct diagnostic groups.", "label": "Supported", "paragraph": "For more details, we refer to our publicly available code base. An overview of the evaluated dataset and statistics is given in Table 4. BloodMNIST contains images from different normal cells belonging to eight classes, and DermaMNIST consists of dermatoscopic images categorizing seven different diseases (Yang et al., 2023).", "section_name": "B.1 TRAINING AND ACTIVE LEARNING PARAMETERS", "subsection_name": "", "paper_id": "yZBpnKpBCw", "paper_title": "Time- and Label-efficient Active Learning by Diversity and Uncertainty of Probabilities", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:43:20.981738", "model": "gpt-5-mini", "original_claim": "The DermaMNIST dataset evaluated in this work includes dermatoscopic images assigned to seven separate disease categories.", "rephrasing_timestamp": "2026-01-11T19:22:36.958304", "rephrasing_model": "gpt-5-mini"}
{"claim": "During data preparation, images from the clinical image sets were enlarged from 28 by 28 to 32 by 32 pixels by giving each added pixel the value of the closest original pixel.", "label": "Supported", "paragraph": "Example images are shown in Figure 11. We rescale images from the medical datasets from 28x28 to 32x32 with nearest-neighbor interpolation.", "section_name": "B.1 TRAINING AND ACTIVE LEARNING PARAMETERS", "subsection_name": "", "paper_id": "yZBpnKpBCw", "paper_title": "Time- and Label-efficient Active Learning by Diversity and Uncertainty of Probabilities", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:43:26.280715", "model": "gpt-5-mini", "original_claim": "Images in the medical datasets were upsampled from 28×28 pixels to 32×32 pixels using nearest-neighbor interpolation as part of the preprocessing.", "rephrasing_timestamp": "2026-01-11T19:22:35.901697", "rephrasing_model": "gpt-5-mini"}
{"claim": "Earlier, we introduced a set of mathematical techniques for developing stochastic sampling methods whose update steps are driven by derivative information.", "label": "Supported", "paragraph": "The previous section provided a mathematical toolbox for constructing Monte Carlo algorithms based on gradient updates and a working definition of predictive coding. This section will combine those\n1 Kuntz et al.", "section_name": "3 Divide-and-Conquer Predictive Coding", "subsection_name": "", "paper_id": "dxwIaCVkWU", "paper_title": "Divide-and-Conquer Predictive Coding: a structured Bayesian inference algorithm", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:43:49.003150", "model": "gpt-5-mini", "original_claim": "The prior section introduced a mathematical toolbox for designing Monte Carlo algorithms that rely on gradient-based update rules.", "rephrasing_timestamp": "2026-01-11T19:22:37.482950", "rephrasing_model": "gpt-5-mini"}
{"claim": "The earlier portion offered an operational account of prediction-driven encoding, and this part seeks to incorporate that account into a gradient-informed Monte Carlo toolkit.", "label": "Supported", "paragraph": "The previous section provided a mathematical toolbox for constructing Monte Carlo algorithms based on gradient updates and a working definition of predictive coding. This section will combine those\n1 Kuntz et al.", "section_name": "3 Divide-and-Conquer Predictive Coding", "subsection_name": "", "paper_id": "dxwIaCVkWU", "paper_title": "Divide-and-Conquer Predictive Coding: a structured Bayesian inference algorithm", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:43:49.003162", "model": "gpt-5-mini", "original_claim": "The previous section also formulated a working definition of predictive coding, and the current section intends to merge that definition with the gradient-based Monte Carlo toolbox.", "rephrasing_timestamp": "2026-01-11T19:22:38.242108", "rephrasing_model": "gpt-5-mini"}
{"claim": "A study published in 2023 interpreted the third equation as an iterative rule that advances along the steepest-descent direction induced by the Wasserstein metric on the space of probability distributions.", "label": "Supported", "paragraph": "[2023] also interpreted Equation 3 as an update step along the Wasserstein gradient in the space of probability measures. Appendix C extends this perspective to predictive coding of discrete random variables.", "section_name": "3 Divide-and-Conquer Predictive Coding", "subsection_name": "", "paper_id": "dxwIaCVkWU", "paper_title": "Divide-and-Conquer Predictive Coding: a structured Bayesian inference algorithm", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:44:06.610848", "model": "gpt-5-mini", "original_claim": "The 2023 study characterized Equation 3 as an update operation that follows the Wasserstein gradient within the domain of probability measures.", "rephrasing_timestamp": "2026-01-11T19:22:40.607175", "rephrasing_model": "gpt-5-mini"}
{"claim": "Section C of the paper broadens the derivative-driven account by applying it to prediction-error-based coding frameworks that involve discrete-valued stochastic variables.", "label": "Supported", "paragraph": "[2023] also interpreted Equation 3 as an update step along the Wasserstein gradient in the space of probability measures. Appendix C extends this perspective to predictive coding of discrete random variables.", "section_name": "3 Divide-and-Conquer Predictive Coding", "subsection_name": "", "paper_id": "dxwIaCVkWU", "paper_title": "Divide-and-Conquer Predictive Coding: a structured Bayesian inference algorithm", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:44:06.610868", "model": "gpt-5-mini", "original_claim": "Appendix C of the work generalizes that gradient-based interpretation by applying it to predictive coding formulations involving discrete random variables.", "rephrasing_timestamp": "2026-01-11T19:22:49.434675", "rephrasing_model": "gpt-5-mini"}
{"claim": "Of the approaches examined, only DCPC provides a posterior distribution that is conditioned on other variables; PC, LPC, and MCPC are reported not to support this.", "label": "Supported", "paragraph": "|          | PC          | LPC          | MCPC          | DCPC (ours)          |\n|----------|----------|----------|----------|----------|\n| Generative density Inference approximation | Gaussian Laplace | Differentiable Gaussian | Gaussian Empirical | Differentiable Empirical |\n| Posterior conditional structure          | ✗          | ✗          | ✗          | ✓          |\nTable 1: Comparison of divide-and-conquer predictive coding (DCPC) against other predictive coding algorithms.", "section_name": "3 Divide-and-Conquer Predictive Coding", "subsection_name": "", "paper_id": "dxwIaCVkWU", "paper_title": "Divide-and-Conquer Predictive Coding: a structured Bayesian inference algorithm", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:44:14.804447", "model": "gpt-5-mini", "original_claim": "Among the compared predictive coding algorithms, only DCPC implements a conditional posterior structure; PC, LPC, and MCPC are indicated as not supporting it.", "rephrasing_timestamp": "2026-01-11T19:22:45.981217", "rephrasing_model": "gpt-5-mini"}
{"claim": "Before discussing the supporting propositions and their proofs about whether the model's parameters can be uniquely recovered, the paper first gives an in-depth explanation of how the observations are produced.", "label": "Supported", "paragraph": "Before presenting the lemmas and proofs for identifiability, it is crucial to provide a comprehensive explanation of the data generating process. Understanding the data generating process is pivotal in the study of causality, as it unveils the causal mechanisms (denoted as assignment functions in Section 3.1) through which observed variables are produced by latent factors.", "section_name": "A.1 DATA GENERATING PROCESS", "subsection_name": "", "paper_id": "lWXedJyLuL", "paper_title": "A Unified Causal View of Instruction Tuning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:44:39.720638", "model": "gpt-5-mini", "original_claim": "A detailed account of the data generation mechanism is presented prior to the presentation of lemmas and proofs that address identifiability.", "rephrasing_timestamp": "2026-01-11T19:22:46.377064", "rephrasing_model": "gpt-5-mini"}
{"claim": "Recognizing the process that produces the data is essential for causal analysis because it exposes the mechanisms that translate unobserved causes into the measured outcomes.", "label": "Supported", "paragraph": "Before presenting the lemmas and proofs for identifiability, it is crucial to provide a comprehensive explanation of the data generating process. Understanding the data generating process is pivotal in the study of causality, as it unveils the causal mechanisms (denoted as assignment functions in Section 3.1) through which observed variables are produced by latent factors.", "section_name": "A.1 DATA GENERATING PROCESS", "subsection_name": "", "paper_id": "lWXedJyLuL", "paper_title": "A Unified Causal View of Instruction Tuning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:44:39.720650", "model": "gpt-5-mini", "original_claim": "The text states that understanding how data are generated is crucial for causal research because it reveals the assignment functions (Section 3.1) that generate observed variables from latent factors.", "rephrasing_timestamp": "2026-01-11T19:22:47.462554", "rephrasing_model": "gpt-5-mini"}
{"claim": "The authors adopt a widely accepted causal-modeling formalism, expressed via structural equations, to describe the mechanism that produces the collected observations.", "label": "Supported", "paragraph": "In this regard, we employ the structural causal model (SCM), a widely utilized framework, to describe the data generating process. Formally, let x t ∈ R dim ( x t ) , y t ∈ R dim ( y t ) , l i ∈ R dim ( l i ) .", "section_name": "A.1 DATA GENERATING PROCESS", "subsection_name": "", "paper_id": "lWXedJyLuL", "paper_title": "A Unified Causal View of Instruction Tuning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:44:49.973604", "model": "gpt-5-mini", "original_claim": "The authors use the structural causal model (SCM), a commonly used framework, to represent the process that generates the observed data.", "rephrasing_timestamp": "2026-01-11T19:22:51.655749", "rephrasing_model": "gpt-5-mini"}
{"claim": "x_t, y_t, and l_i each denote a tuple of real numbers, having lengths dim(x_t), dim(y_t), and dim(l_i) in the same order.", "label": "Supported", "paragraph": "In this regard, we employ the structural causal model (SCM), a widely utilized framework, to describe the data generating process. Formally, let x t ∈ R dim ( x t ) , y t ∈ R dim ( y t ) , l i ∈ R dim ( l i ) .", "section_name": "A.1 DATA GENERATING PROCESS", "subsection_name": "", "paper_id": "lWXedJyLuL", "paper_title": "A Unified Causal View of Instruction Tuning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:44:49.973618", "model": "gpt-5-mini", "original_claim": "The notation specifies that x_t, y_t, and l_i are real-valued vectors in Euclidean spaces with dimensions dim(x_t), dim(y_t), and dim(l_i), respectively.", "rephrasing_timestamp": "2026-01-11T19:22:55.626717", "rephrasing_model": "gpt-5-mini"}
{"claim": "Exactly the elements L_1 through L_n are the direct inputs to X_t — and no other variables are — because the source context X_t fully represents the information contained in L.", "label": "Supported", "paragraph": "The parent set of X t denoted as Pa ( X t ) and the parent set of Y t denoted as Pa ( Y t ) . As explained in Section 3.1, the source context X t carries all the information of L , hence Pa ( X t ) = { L 1 , L 2 , L 3 , · · · , L n } .", "section_name": "A.1 DATA GENERATING PROCESS", "subsection_name": "", "paper_id": "lWXedJyLuL", "paper_title": "A Unified Causal View of Instruction Tuning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:45:01.111261", "model": "gpt-5-mini", "original_claim": "The parent set of X_t consists exactly of all components of L, i.e., Pa(X_t) = {L_1, L_2, ..., L_n}, because the source context X_t contains all information of L.", "rephrasing_timestamp": "2026-01-11T19:22:55.836879", "rephrasing_model": "gpt-5-mini"}
{"claim": "In problems with real-valued state domains, multiple studies have proposed models that evolve continuously in time to support sampling methods that obtain samples more quickly.", "label": "Supported", "paragraph": "In the context of continuous state spaces, continuous-time processes have been proposed to accommodate algorithms that offer faster sampling speeds and enhanced sample quality (Jolicoeur-Martineau et al., 2021; Zhang and Chen, 2022; Salimans and Ho, 2022; Chung et al., 2022; Song et al., 2020b; Dockhorn et al., 2021).", "section_name": "3.3 Continous-time (Infinite Step) Reverse Sampling", "subsection_name": "", "paper_id": "KkYZmepjHn", "paper_title": "Fast Sampling via Discrete Non-Markov Diffusion Models with Predetermined Transition Time", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:45:28.523253", "model": "gpt-5-mini", "original_claim": "For continuous state spaces, continuous-time process models have been proposed to enable algorithms that achieve faster sampling speeds, as reported by several cited studies.", "rephrasing_timestamp": "2026-01-11T19:23:01.247905", "rephrasing_model": "gpt-5-mini"}
{"claim": "Several cited studies argue that, for models defined over real-valued state domains, employing dynamics that evolve without discrete time steps enables methods that yield higher-quality generated samples.", "label": "Supported", "paragraph": "In the context of continuous state spaces, continuous-time processes have been proposed to accommodate algorithms that offer faster sampling speeds and enhanced sample quality (Jolicoeur-Martineau et al., 2021; Zhang and Chen, 2022; Salimans and Ho, 2022; Chung et al., 2022; Song et al., 2020b; Dockhorn et al., 2021).", "section_name": "3.3 Continous-time (Infinite Step) Reverse Sampling", "subsection_name": "", "paper_id": "KkYZmepjHn", "paper_title": "Fast Sampling via Discrete Non-Markov Diffusion Models with Predetermined Transition Time", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:45:28.523273", "model": "gpt-5-mini", "original_claim": "In continuous state spaces, continuous-time processes have been suggested to support algorithms that produce improved sample quality, according to multiple referenced works.", "rephrasing_timestamp": "2026-01-11T19:22:55.643820", "rephrasing_model": "gpt-5-mini"}
{"claim": "Academic work on using algorithms that evolve continuously over time for systems with finite or categorical state sets has been minimal, implying that time-continuous methods for models with discrete outputs remain largely unexplored.", "label": "Supported", "paragraph": "However, the application of continuous-time schemes to discrete-state spaces remains largely unexplored. Campbell et al. (2022) first developed a continuous framework for discrete-time diffusion for the Markovian process and randomized sampling, but not in our nonMarkovian setting.", "section_name": "3.3 Continous-time (Infinite Step) Reverse Sampling", "subsection_name": "", "paper_id": "KkYZmepjHn", "paper_title": "Fast Sampling via Discrete Non-Markov Diffusion Models with Predetermined Transition Time", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:45:39.100437", "model": "gpt-5-mini", "original_claim": "The investigation of continuous-time algorithms applied to discrete-state spaces has largely been neglected in the literature, indicating limited prior work on continuous-time approaches for discrete-valued systems.", "rephrasing_timestamp": "2026-01-11T19:23:05.932864", "rephrasing_model": "gpt-5-mini"}
{"claim": "In 2022 Campbell and colleagues put forward the inaugural continuous-time formulation corresponding to diffusion models defined on discrete timesteps, intended for memoryless dynamics with randomized sampling, and they explicitly excluded consideration of processes that have temporal dependencies.", "label": "Supported", "paragraph": "However, the application of continuous-time schemes to discrete-state spaces remains largely unexplored. Campbell et al. (2022) first developed a continuous framework for discrete-time diffusion for the Markovian process and randomized sampling, but not in our nonMarkovian setting.", "section_name": "3.3 Continous-time (Infinite Step) Reverse Sampling", "subsection_name": "", "paper_id": "KkYZmepjHn", "paper_title": "Fast Sampling via Discrete Non-Markov Diffusion Models with Predetermined Transition Time", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:45:39.100453", "model": "gpt-5-mini", "original_claim": "Campbell et al. (2022) introduced the first continuous-time framework for discrete-time diffusion aimed at Markovian processes with randomized sampling, and explicitly did not address non-Markovian settings.", "rephrasing_timestamp": "2026-01-11T19:23:08.914121", "rephrasing_model": "gpt-5-mini"}
{"claim": "This section investigates how using a restricted versus an unbounded number of update passes can convert between iterative, noise-driven frameworks and their time-smooth counterparts, shedding light on how to reconcile piecewise and continuously evolving random systems.", "label": "Supported", "paragraph": "In this section, we investigate the transition from finite to infinite step sampling, providing new insights into bridging the gap between discrete and continuous-time processes for discrete diffusion models.", "section_name": "3.3 Continous-time (Infinite Step) Reverse Sampling", "subsection_name": "", "paper_id": "KkYZmepjHn", "paper_title": "Fast Sampling via Discrete Non-Markov Diffusion Models with Predetermined Transition Time", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:45:48.921389", "model": "gpt-5-mini", "original_claim": "This section examines how sampling with a finite versus an infinite number of steps can link discrete diffusion models to continuous-time formulations, providing insights into reconciling discrete and continuous stochastic processes.", "rephrasing_timestamp": "2026-01-11T19:23:07.813372", "rephrasing_model": "gpt-5-mini"}
{"claim": "For the model M_demo, an n-step demonstration (n a natural number) is represented by an ordered n-tuple of actions — in other words, a single element of the n‑fold product of A with itself.", "label": "Supported", "paragraph": "We define a demonstration of length n ∈ N on M demo as a sequence of actions d = ( a demo 0 , . . . , a demo n -1 ) ∈ ( A ) n . We consider the demonstration to be provided as if the teacher were teleoperating the learner as described in Silva &amp; Costa (2019).", "section_name": "3.3.1 UTILITY BASED DEMONSTRATION SELECTION STRATEGY", "subsection_name": "", "paper_id": "WcSofkUVge", "paper_title": "Utility-based Adaptive Teaching Strategies using Bayesian Theory of Mind", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:46:09.032236", "model": "gpt-5-mini", "original_claim": "They define a demonstration of length n (n ∈ N) on M_demo as a sequence of n actions, i.e., an element of the Cartesian product A^n.", "rephrasing_timestamp": "2026-01-11T19:23:09.004457", "rephrasing_model": "gpt-5-mini"}
{"claim": "We treat demonstrations as being provided as though an instructor remotely controlled the agent, in accordance with the protocol proposed by Silva and Costa (2019).", "label": "Supported", "paragraph": "We define a demonstration of length n ∈ N on M demo as a sequence of actions d = ( a demo 0 , . . . , a demo n -1 ) ∈ ( A ) n . We consider the demonstration to be provided as if the teacher were teleoperating the learner as described in Silva &amp; Costa (2019).", "section_name": "3.3.1 UTILITY BASED DEMONSTRATION SELECTION STRATEGY", "subsection_name": "", "paper_id": "WcSofkUVge", "paper_title": "Utility-based Adaptive Teaching Strategies using Bayesian Theory of Mind", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:46:09.032247", "model": "gpt-5-mini", "original_claim": "The demonstration is assumed to be provided as though a teacher teleoperates the learner, following the procedure described by Silva and Costa (2019).", "rephrasing_timestamp": "2026-01-11T19:23:13.737574", "rephrasing_model": "gpt-5-mini"}
{"claim": "At demo timestep t, the sensor reading for learner Li at time t+1 equals Li's observation mapping v_i applied to the state transition produced by the demonstrator when it takes action a_demo_t in state s_t.", "label": "Supported", "paragraph": "Thus, at step t of the demonstration, learner L i observes ¯ o i t +1 = v i ( T demo ( s t , a demo t ) ) . Following the same demonstration leads to varying observation sequences for learners with different observation functions.", "section_name": "3.3.1 UTILITY BASED DEMONSTRATION SELECTION STRATEGY", "subsection_name": "", "paper_id": "WcSofkUVge", "paper_title": "Utility-based Adaptive Teaching Strategies using Bayesian Theory of Mind", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:46:18.426369", "model": "gpt-5-mini", "original_claim": "At demonstration step t, learner Li's observation at t+1 equals the learner's observation function vi applied to the demonstration transition T_demo evaluated on state s_t and action a_demo_t.", "rephrasing_timestamp": "2026-01-11T19:23:16.610323", "rephrasing_model": "gpt-5-mini"}
{"claim": "If two agents follow the same example but use different mappings from what they perceive, they will end up with different ordered sequences of observations.", "label": "Supported", "paragraph": "Thus, at step t of the demonstration, learner L i observes ¯ o i t +1 = v i ( T demo ( s t , a demo t ) ) . Following the same demonstration leads to varying observation sequences for learners with different observation functions.", "section_name": "3.3.1 UTILITY BASED DEMONSTRATION SELECTION STRATEGY", "subsection_name": "", "paper_id": "WcSofkUVge", "paper_title": "Utility-based Adaptive Teaching Strategies using Bayesian Theory of Mind", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:46:18.426383", "model": "gpt-5-mini", "original_claim": "When learners follow the identical demonstration, learners that have different observation functions produce different sequences of observed outputs.", "rephrasing_timestamp": "2026-01-11T19:23:19.904486", "rephrasing_model": "gpt-5-mini"}
{"claim": "Using the update rule given in Equation 1, the agent processes the demonstration data (y_{i,1}, …, y_{i,n}) to form an updated distribution over the demonstrated environment, and then uses that updated distribution as its initial belief for the demonstration phase.", "label": "Supported", "paragraph": "The learner's belief about the new environment M demo is updated based on the observations (¯ o i 1 , . . . , ¯ o i n ) resulting from the demonstration, as in Equation 1 and depicted in Figure 1(B). This updated belief is then used as initial belief b i, demo 0 by the learner.", "section_name": "3.3.1 UTILITY BASED DEMONSTRATION SELECTION STRATEGY", "subsection_name": "", "paper_id": "WcSofkUVge", "paper_title": "Utility-based Adaptive Teaching Strategies using Bayesian Theory of Mind", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:46:27.988876", "model": "gpt-5-mini", "original_claim": "Following Equation 1, the learner incorporates demonstration observations (¯o_i1,…,¯o_in) to revise its belief about environment M_demo and then adopts that revised belief as initial b_i,demo^0.", "rephrasing_timestamp": "2026-01-11T19:23:23.472985", "rephrasing_model": "gpt-5-mini"}
{"claim": "This analysis omits any solution that depends on supplementary annotated datasets and confines comparisons to algorithms that rely exclusively on planar images of general objects.", "label": "Supported", "paragraph": "For fairness, we do not compare methods that require external training data but focus on those that solely use 2D projections of arbitrary objects. We compare R 2 -Gaussian with three traditional methods (FDK [13], SART [2], ASD-POCS [55]) and three SOTA NeRF-based methods (IntraTomo [66], NAF [67], SAX-NeRF [6]).", "section_name": "5.2 Results and evaluation", "subsection_name": "", "paper_id": "fMWrTAe5Iy", "paper_title": "R$^2$-Gaussian: Rectifying Radiative Gaussian Splatting for Tomographic Reconstruction", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:46:57.103665", "model": "gpt-5-mini", "original_claim": "The evaluation excludes approaches that need external training datasets and restricts comparisons to methods that operate solely on 2D projections of arbitrary objects.", "rephrasing_timestamp": "2026-01-11T19:23:16.436968", "rephrasing_model": "gpt-5-mini"}
{"claim": "R2-Gaussian was benchmarked against six alternative methods: three classic tomographic reconstruction techniques—FDK, SART, and ASD-POCS—and three recent approaches that leverage neural radiance fields: IntraTomo, NAF, and SAX-NeRF.", "label": "Supported", "paragraph": "For fairness, we do not compare methods that require external training data but focus on those that solely use 2D projections of arbitrary objects. We compare R 2 -Gaussian with three traditional methods (FDK [13], SART [2], ASD-POCS [55]) and three SOTA NeRF-based methods (IntraTomo [66], NAF [67], SAX-NeRF [6]).", "section_name": "5.2 Results and evaluation", "subsection_name": "", "paper_id": "fMWrTAe5Iy", "paper_title": "R$^2$-Gaussian: Rectifying Radiative Gaussian Splatting for Tomographic Reconstruction", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:46:57.103685", "model": "gpt-5-mini", "original_claim": "R2-Gaussian is evaluated against six baselines: three conventional reconstruction algorithms (FDK, SART, ASD-POCS) and three recent NeRF-based approaches (IntraTomo, NAF, SAX-NeRF).", "rephrasing_timestamp": "2026-01-11T19:23:23.090695", "rephrasing_model": "gpt-5-mini"}
{"claim": "In sparse-view tomographic reconstruction, the R2-Gaussian method outperformed all competitors on every simulated test and produced the best results in most of the real-data experiments summarized in Table 1.", "label": "Supported", "paragraph": "Tab. 1 reports the quantitative results on sparse-view tomography. Note that we do not report the running time for FDK as it is instant. R 2 -Gaussian achieves the best performance across all synthetic and most real-world experiments.", "section_name": "5.2 Results and evaluation", "subsection_name": "", "paper_id": "fMWrTAe5Iy", "paper_title": "R$^2$-Gaussian: Rectifying Radiative Gaussian Splatting for Tomographic Reconstruction", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:47:17.318888", "model": "gpt-5-mini", "original_claim": "The R2-Gaussian approach achieved the highest performance across all synthetic experiments and in the majority of real-world experiments reported in Table 1 for sparse-view tomography.", "rephrasing_timestamp": "2026-01-11T19:23:25.754223", "rephrasing_model": "gpt-5-mini"}
{"claim": "The researchers refrained from providing a timing measurement for the FDK technique, stating that in their experiments it ran so swiftly that recording its duration was unnecessary.", "label": "Supported", "paragraph": "Tab. 1 reports the quantitative results on sparse-view tomography. Note that we do not report the running time for FDK as it is instant. R 2 -Gaussian achieves the best performance across all synthetic and most real-world experiments.", "section_name": "5.2 Results and evaluation", "subsection_name": "", "paper_id": "fMWrTAe5Iy", "paper_title": "R$^2$-Gaussian: Rectifying Radiative Gaussian Splatting for Tomographic Reconstruction", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:47:17.318904", "model": "gpt-5-mini", "original_claim": "The authors did not include a runtime measurement for the FDK algorithm because they described its execution time as effectively instantaneous in the context of their experiments.", "rephrasing_timestamp": "2026-01-11T19:23:26.041929", "rephrasing_model": "gpt-5-mini"}
{"claim": "When evaluated on simulated data, our proposed approach yields a PSNR that is 0.93 dB higher than SAX‑NeRF.", "label": "Supported", "paragraph": "Specifically, our method delivers a 0.93 dB higher PSNR than SAX-NeRF, on the synthetic dataset, and a 0.95 dB improvement over IntraTomo on the real-world dataset. It is also worth noting that our 50-view results are already on par with the 75-view results of other methods.", "section_name": "5.2 Results and evaluation", "subsection_name": "", "paper_id": "fMWrTAe5Iy", "paper_title": "R$^2$-Gaussian: Rectifying Radiative Gaussian Splatting for Tomographic Reconstruction", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:47:22.127737", "model": "gpt-5-mini", "original_claim": "On the synthetic dataset, our method achieves a peak signal-to-noise ratio (PSNR) that is 0.93 dB higher than SAX-NeRF.", "rephrasing_timestamp": "2026-01-11T19:23:32.412800", "rephrasing_model": "gpt-5-mini"}
{"claim": "The researchers performed experiments focused on two prediction problems, one of which involved forecasting connections between nodes. They drew on earlier studies by Zhang et al., Srinivasan and Ribeiro, and Lü and Zhou.", "label": "Supported", "paragraph": "We conduct experiments on two predictive tasks: link prediction (Zhang et al., 2020; Srinivasan &amp; Ribeiro, 2019; L¨ u &amp; Zhou, 2011) and dynamic node classification (Aggarwal &amp; Li, 2011; Xu et al., 2019).", "section_name": "5.2 IMPLEMENTATION DETAILS AND EVALUATION PROTOCOL", "subsection_name": "", "paper_id": "auguNUCto5", "paper_title": "Boosting Temporal Graph Learning From Global and Local Perspectives", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:47:45.139780", "model": "gpt-5-mini", "original_claim": "The authors ran experiments including link prediction as one of the two predictive tasks evaluated in the study, citing prior work by Zhang et al., Srinivasan & Ribeiro, and Lü & Zhou.", "rephrasing_timestamp": "2026-01-11T19:23:34.606721", "rephrasing_model": "gpt-5-mini"}
{"claim": "In addition to the main tests, the authors explored time-sensitive node labeling as an alternate prediction problem and cited earlier work by Aggarwal and Li (2011) and Xu et al. (2019).", "label": "Supported", "paragraph": "We conduct experiments on two predictive tasks: link prediction (Zhang et al., 2020; Srinivasan &amp; Ribeiro, 2019; L¨ u &amp; Zhou, 2011) and dynamic node classification (Aggarwal &amp; Li, 2011; Xu et al., 2019).", "section_name": "5.2 IMPLEMENTATION DETAILS AND EVALUATION PROTOCOL", "subsection_name": "", "paper_id": "auguNUCto5", "paper_title": "Boosting Temporal Graph Learning From Global and Local Perspectives", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:47:45.139791", "model": "gpt-5-mini", "original_claim": "The experimental evaluation also assessed dynamic node classification as the other predictive task, referencing related studies by Aggarwal & Li (2011) and Xu et al. (2019) in the paper.", "rephrasing_timestamp": "2026-01-11T19:23:32.243008", "rephrasing_model": "gpt-5-mini"}
{"claim": "Each dataset's interactions were arranged in chronological order, with 70% allocated to the training portion and the remaining 30% evenly split into 15% validation and 15% test sets.", "label": "Supported", "paragraph": "For all datasets, we split edges chronologically by 70%, 15%, and 15% for training, validation, and testing. We use the Adam optimizer and early stopping with a patience of 5 for training. For both link prediction and dynamic node classification, we use BCE loss.", "section_name": "5.2 IMPLEMENTATION DETAILS AND EVALUATION PROTOCOL", "subsection_name": "", "paper_id": "auguNUCto5", "paper_title": "Boosting Temporal Graph Learning From Global and Local Perspectives", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:48:03.083838", "model": "gpt-5-mini", "original_claim": "For each dataset, edges were ordered by time and divided into 70% for training, 15% for validation, and 15% for testing.", "rephrasing_timestamp": "2026-01-11T19:23:34.455438", "rephrasing_model": "gpt-5-mini"}
{"claim": "Training used an optimizer based on adaptive moment estimation, and the run was halted early if validation performance did not improve for five consecutive epochs.", "label": "Supported", "paragraph": "For all datasets, we split edges chronologically by 70%, 15%, and 15% for training, validation, and testing. We use the Adam optimizer and early stopping with a patience of 5 for training. For both link prediction and dynamic node classification, we use BCE loss.", "section_name": "5.2 IMPLEMENTATION DETAILS AND EVALUATION PROTOCOL", "subsection_name": "", "paper_id": "auguNUCto5", "paper_title": "Boosting Temporal Graph Learning From Global and Local Perspectives", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:48:03.083853", "model": "gpt-5-mini", "original_claim": "Model training employed the Adam optimizer and used early stopping with a patience of five epochs.", "rephrasing_timestamp": "2026-01-11T19:23:32.551857", "rephrasing_model": "gpt-5-mini"}
{"claim": "Appendix C.4 contains a code-style procedural outline of the GLEN approach, presenting a detailed, step-by-step algorithm that specifies how the model operates.", "label": "Supported", "paragraph": "All the settings are consistent with those set by baselines (Xu et al., 2020; Rossi et al., 2020; Wang et al., 2021c). More implementation details of GLEN can be found in Appendix C.3. The pseudo-code of GLEN can be seen in Appendix C.4.", "section_name": "5.2 IMPLEMENTATION DETAILS AND EVALUATION PROTOCOL", "subsection_name": "", "paper_id": "auguNUCto5", "paper_title": "Boosting Temporal Graph Learning From Global and Local Perspectives", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:48:13.258644", "model": "gpt-5-mini", "original_claim": "The paper includes the pseudo-code for the GLEN method in Appendix C.4, providing an explicit algorithmic description of the model's procedures.", "rephrasing_timestamp": "2026-01-11T19:23:39.673015", "rephrasing_model": "gpt-5-mini"}
{"claim": "For every data point i, the M associated vectors s_i^1, …, s_i^M together provide a complete representation of that point's attribute information.", "label": "Supported", "paragraph": "For each instance x i , { x im } M m =1 reflects all the information of it in the attribute space and { l ir } R r =1 reflects all the information of it in the multiple noisy label space. Inspired by label enhancement (LE) [32, 33], we design a label distribution enhancement (LDE) component for KFNN.", "section_name": "3.2.1 Label distribution enhancement", "subsection_name": "", "paper_id": "wnPlJNiqfA", "paper_title": "KFNN: K-Free Nearest Neighbor For Crowdsourcing", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:48:41.405006", "model": "gpt-5-mini", "original_claim": "For each instance x_i, the set {x_im}_{m=1}^M fully captures its information in the attribute space.", "rephrasing_timestamp": "2026-01-11T19:23:41.777442", "rephrasing_model": "gpt-5-mini"}
{"claim": "The authors augmented KFNN with a module designed to improve the estimation of label distributions, motivated by earlier label-reconstruction research cited in [32, 33].", "label": "Supported", "paragraph": "For each instance x i , { x im } M m =1 reflects all the information of it in the attribute space and { l ir } R r =1 reflects all the information of it in the multiple noisy label space. Inspired by label enhancement (LE) [32, 33], we design a label distribution enhancement (LDE) component for KFNN.", "section_name": "3.2.1 Label distribution enhancement", "subsection_name": "", "paper_id": "wnPlJNiqfA", "paper_title": "KFNN: K-Free Nearest Neighbor For Crowdsourcing", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:48:41.405017", "model": "gpt-5-mini", "original_claim": "The authors developed a label distribution enhancement (LDE) component for KFNN, drawing inspiration from prior label enhancement (LE) work referenced as [32, 33].", "rephrasing_timestamp": "2026-01-11T19:23:41.391389", "rephrasing_model": "gpt-5-mini"}
{"claim": "From the observations {x_im}_{m=1}, LDE infers a provisional label probability profile and then leverages that estimate to refine the aggregated noisy label probabilities produced from {l_ir}_{r=1}.", "label": "Supported", "paragraph": "LDE recovers a potential label distribution using { x im } M m =1 , and then enhances the multiple noisy label distribution calculated from { l ir } R r =1 by this potential label distribution. Specifically, KFNN first uses majority voting to initialize the integrated label ˆ y i for x i as follows:\n<!-- formula-not-decoded -->\nwhere p ( c q | L i ) can be calculated as follows:\n<!-- formula-not-decoded -->\nHere, p ( c q | L i ) reflects the proportion of labels in L i that take the value c q .", "section_name": "3.2.1 Label distribution enhancement", "subsection_name": "", "paper_id": "wnPlJNiqfA", "paper_title": "KFNN: K-Free Nearest Neighbor For Crowdsourcing", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:49:02.851060", "model": "gpt-5-mini", "original_claim": "LDE reconstructs a candidate label distribution from the samples {x_im}_{m=1} and uses that reconstructed distribution to enhance the combined noisy label distribution obtained from {l_ir}_{r=1}.", "rephrasing_timestamp": "2026-01-11T19:23:45.838279", "rephrasing_model": "gpt-5-mini"}
{"claim": "For each instance x_i, KFNN produces a single aggregated label \\hat{y}_i by selecting the class c_q that has the highest count in the collection of labels L_i; p(c_q | L_i) denotes the fraction of entries in L_i equal to c_q.", "label": "Supported", "paragraph": "LDE recovers a potential label distribution using { x im } M m =1 , and then enhances the multiple noisy label distribution calculated from { l ir } R r =1 by this potential label distribution. Specifically, KFNN first uses majority voting to initialize the integrated label ˆ y i for x i as follows:\n<!-- formula-not-decoded -->\nwhere p ( c q | L i ) can be calculated as follows:\n<!-- formula-not-decoded -->\nHere, p ( c q | L i ) reflects the proportion of labels in L i that take the value c q .", "section_name": "3.2.1 Label distribution enhancement", "subsection_name": "", "paper_id": "wnPlJNiqfA", "paper_title": "KFNN: K-Free Nearest Neighbor For Crowdsourcing", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:49:02.851081", "model": "gpt-5-mini", "original_claim": "KFNN assigns each sample x_i an integrated label ŷ_i by selecting the most frequent label in its label set, where p(c_q | L_i) is defined as the proportion of labels in L_i equal to c_q.", "rephrasing_timestamp": "2026-01-11T19:23:48.058876", "rephrasing_model": "gpt-5-mini"}
{"claim": "The dataset gathered from the crowd is divided into Q disjoint groups S_1,...,S_Q; for each k∈{1,…,Q}, group S_k contains every data sample whose initial consensus annotation corresponds to category k.", "label": "Supported", "paragraph": "The function δ ( · ) outputs 1 if its two parameters are identical, and 0 otherwise. Subsequently, according to ˆ y i , the crowdsourced dataset D can be divided into Q subsets { D q } Q q =1 . The subset D q contains all instances with initial integrated labels of c q , i.e., D q = { x i | ˆ y i = c q } N i =1 .", "section_name": "3.2.1 Label distribution enhancement", "subsection_name": "", "paper_id": "wnPlJNiqfA", "paper_title": "KFNN: K-Free Nearest Neighbor For Crowdsourcing", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:49:13.091634", "model": "gpt-5-mini", "original_claim": "The crowdsourced dataset D is split into Q subsets D_q (q=1..Q); each subset D_q consists of all instances x_i whose initial integrated label y_hat_i equals class c_q.", "rephrasing_timestamp": "2026-01-11T19:23:51.066063", "rephrasing_model": "gpt-5-mini"}
{"claim": "Each procedure evaluated in these trials began from the same initial value of the pseudorandom number generator, so every approach shared an identical randomized starting state.", "label": "Supported", "paragraph": "In this section we display the remaining samples from the experiments in the main paper. We remind the reader that all algorithms are run with the same seed and we draw in parallel 4 samples from each algorithm and display them in their order of appearance.", "section_name": "D Sample reconstructions", "subsection_name": "", "paper_id": "BOrut7M2X7", "paper_title": "Divide-and-Conquer Posterior Sampling for Denoising Diffusion priors", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:49:42.274568", "model": "gpt-5-mini", "original_claim": "All algorithms used in these experiments were initialized with an identical random seed, ensuring consistent stochastic initialization across the different methods.", "rephrasing_timestamp": "2026-01-11T19:23:50.645853", "rephrasing_model": "gpt-5-mini"}
{"claim": "Each algorithm creates four instances concurrently, and those instances are presented in the sequence in which they were created.", "label": "Supported", "paragraph": "In this section we display the remaining samples from the experiments in the main paper. We remind the reader that all algorithms are run with the same seed and we draw in parallel 4 samples from each algorithm and display them in their order of appearance.", "section_name": "D Sample reconstructions", "subsection_name": "", "paper_id": "BOrut7M2X7", "paper_title": "Divide-and-Conquer Posterior Sampling for Denoising Diffusion priors", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:49:42.274580", "model": "gpt-5-mini", "original_claim": "For each algorithm the procedure simultaneously generates four sample outputs in parallel, and those four samples are shown in the order in which they were produced.", "rephrasing_timestamp": "2026-01-11T19:23:52.512676", "rephrasing_model": "gpt-5-mini"}
{"claim": "Figure 5 depicts a restoration test in which portrait samples from the Flickr-Faces-HQ collection were deliberately corrupted with count-based (Poisson) noise.", "label": "Supported", "paragraph": "Figure 5: Denoising task with Poisson noise on FFHQ . <!-- image -->\nFigure 6: Denoising task with Poisson noise on ImageNet . <!-- image -->\nFigure 7: Outpainting task with half mask on ImageNet . <!-- image -->\nFigure 9: Inpainting task with box mask on ImageNet .", "section_name": "D Sample reconstructions", "subsection_name": "", "paper_id": "BOrut7M2X7", "paper_title": "Divide-and-Conquer Posterior Sampling for Denoising Diffusion priors", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:50:06.725172", "model": "gpt-5-mini", "original_claim": "Figure 5 presents a denoising experiment applying Poisson-distributed noise to images from the FFHQ dataset.", "rephrasing_timestamp": "2026-01-11T19:24:00.087977", "rephrasing_model": "gpt-5-mini"}
{"claim": "The seventh figure depicts the generation of extended image content on samples drawn from the ImageNet dataset, using an occluding region that conceals one half of each picture.", "label": "Supported", "paragraph": "Figure 5: Denoising task with Poisson noise on FFHQ . <!-- image -->\nFigure 6: Denoising task with Poisson noise on ImageNet . <!-- image -->\nFigure 7: Outpainting task with half mask on ImageNet . <!-- image -->\nFigure 9: Inpainting task with box mask on ImageNet .", "section_name": "D Sample reconstructions", "subsection_name": "", "paper_id": "BOrut7M2X7", "paper_title": "Divide-and-Conquer Posterior Sampling for Denoising Diffusion priors", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:50:06.725187", "model": "gpt-5-mini", "original_claim": "Figure 7 illustrates outpainting performed on ImageNet images using a mask that hides half of each image.", "rephrasing_timestamp": "2026-01-11T19:24:03.009570", "rephrasing_model": "gpt-5-mini"}
{"claim": "Figures 11 and 13 illustrate examples of extending image content on the ImageNet dataset, while Figure 14 reports a fourfold image upscaling experiment on FFHQ in which Poisson noise was injected.", "label": "Supported", "paragraph": "<!-- image -->\nFigure 11: Outpainting task with half mask on ImageNet . <!-- image -->\nFigure 13: Outpainting expend task on ImageNet . <!-- image -->\nFigure 14: SR 4 × task with Poisson noise on FFHQ .", "section_name": "D Sample reconstructions", "subsection_name": "", "paper_id": "BOrut7M2X7", "paper_title": "Divide-and-Conquer Posterior Sampling for Denoising Diffusion priors", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:50:18.891414", "model": "gpt-5-mini", "original_claim": "Figures 11 and 13 display outpainting tasks performed on ImageNet, and Figure 14 presents a 4× super-resolution experiment on FFHQ with added Poisson noise.", "rephrasing_timestamp": "2026-01-11T19:24:04.976597", "rephrasing_model": "gpt-5-mini"}
{"claim": "Illustrative exchanges act as scenario-based examples that nudge GPT-4 to generate replies that are clearly organized and varied.", "label": "Supported", "paragraph": "Dialogue Demonstrations act as contextual learning examples, guiding GPT-4 to produce responses that are both well-formatted and diverse. To demonstrate this effect, we modified the original prompt used for GPT-assisted Multiple Dialogues Generation, as detailed in Table 9, by removing content relating to demonstration dialogues.", "section_name": "K DIALOGUE DEMONSTRATIONS' CONTRIBUTION TO DATA QUALITY AND DIVERSITY", "subsection_name": "", "paper_id": "oq5EF8parZ", "paper_title": "Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:51:02.654963", "model": "gpt-5-mini", "original_claim": "Dialogue demonstrations serve as contextual learning examples that steer GPT-4 to produce responses that are both well-formatted and diverse.", "rephrasing_timestamp": "2026-01-11T19:24:05.580690", "rephrasing_model": "gpt-5-mini"}
{"claim": "We illustrated the effect by editing the starter prompt used for GPT-driven generation of multiple conversations, deleting the sections that included sample exchanges; details are shown in Table 9.", "label": "Supported", "paragraph": "Dialogue Demonstrations act as contextual learning examples, guiding GPT-4 to produce responses that are both well-formatted and diverse. To demonstrate this effect, we modified the original prompt used for GPT-assisted Multiple Dialogues Generation, as detailed in Table 9, by removing content relating to demonstration dialogues.", "section_name": "K DIALOGUE DEMONSTRATIONS' CONTRIBUTION TO DATA QUALITY AND DIVERSITY", "subsection_name": "", "paper_id": "oq5EF8parZ", "paper_title": "Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:51:02.654974", "model": "gpt-5-mini", "original_claim": "To demonstrate this effect, we modified the original prompt for GPT-assisted Multiple Dialogues Generation by removing content related to demonstration dialogues, as reported in Table 9.", "rephrasing_timestamp": "2026-01-11T19:24:03.631937", "rephrasing_model": "gpt-5-mini"}
{"claim": "The researchers repurposed the collection of visual descriptions shown in Figure 15, assembled them into a different prompt, and used it to generate another output from the model.", "label": "Supported", "paragraph": "We then employed the same Candidate Image Descriptions as in Figure 15 to create a new prompt and generate a response. The resulting response was inferior in quality, failing to meet the desired formatting criteria, such as assigning image IDs, specifying the number of images per dialogue turn, and incorporating new images in subsequent turns.", "section_name": "K DIALOGUE DEMONSTRATIONS' CONTRIBUTION TO DATA QUALITY AND DIVERSITY", "subsection_name": "", "paper_id": "oq5EF8parZ", "paper_title": "Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:51:14.518866", "model": "gpt-5-mini", "original_claim": "The authors reused the same set of image descriptions from Figure 15 to construct a new prompt and produce an additional model response.", "rephrasing_timestamp": "2026-01-11T19:24:07.329337", "rephrasing_model": "gpt-5-mini"}
{"claim": "The reply was of inferior quality and did not conform to the expected layout; it left out labels that identify images, failed to state how many images to generate each interaction, and never introduced additional images in later exchanges.", "label": "Supported", "paragraph": "We then employed the same Candidate Image Descriptions as in Figure 15 to create a new prompt and generate a response. The resulting response was inferior in quality, failing to meet the desired formatting criteria, such as assigning image IDs, specifying the number of images per dialogue turn, and incorporating new images in subsequent turns.", "section_name": "K DIALOGUE DEMONSTRATIONS' CONTRIBUTION TO DATA QUALITY AND DIVERSITY", "subsection_name": "", "paper_id": "oq5EF8parZ", "paper_title": "Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:51:14.518881", "model": "gpt-5-mini", "original_claim": "The produced response was poorer in quality and did not follow the required formatting: it omitted image identifiers, did not specify images-per-turn counts, and failed to include new images in subsequent turns.", "rephrasing_timestamp": "2026-01-11T19:24:09.003142", "rephrasing_model": "gpt-5-mini"}
{"claim": "Embedding example back-and-forths strengthens the collection by illustrating the expected structure and promotes a wider range of outputs by exposing the system to a variety of exemplar interactions.", "label": "Supported", "paragraph": "Furthermore, the response lacked the diversity that its dialogues typically ask for more detailed descriptions of images but not specifying particular aspects. In conclusion, dialogue demonstrations are crucial not only for enhancing data quality by providing formatting guidelines but also for increasing diversity by conditioning different demonstrations.", "section_name": "K DIALOGUE DEMONSTRATIONS' CONTRIBUTION TO DATA QUALITY AND DIVERSITY", "subsection_name": "", "paper_id": "oq5EF8parZ", "paper_title": "Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:51:20.063799", "model": "gpt-5-mini", "original_claim": "Including dialogue demonstrations improves dataset quality by supplying formatting guidance and also enhances response diversity by conditioning models on varied demonstration examples.", "rephrasing_timestamp": "2026-01-11T19:24:11.161769", "rephrasing_model": "gpt-5-mini"}
{"claim": "The SPICE resource examined in this work comprises nearly one million unique compounds, divided among seven different chemical classes.", "label": "Supported", "paragraph": "Dataset. Weevaluate the EScAIP model's performance on the SPICE dataset [Eastman et al., 2023], which consists of approximately one million molecules across seven different categories. To ensure comparability, we adopt the same training and evaluation settings as used for the MACE-OFF23 model [Kovács et al., 2023a].", "section_name": "5.3 Molecules (SPICE)", "subsection_name": "", "paper_id": "Y4mBaZu4vy", "paper_title": "The Importance of Being Scalable: Improving the Speed and Accuracy of Neural Network Interatomic Potentials Across Chemical Domains", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:51:39.339631", "model": "gpt-5-mini", "original_claim": "The SPICE dataset used in the study contains approximately one million distinct molecules, organized into seven separate chemical categories.", "rephrasing_timestamp": "2026-01-11T19:24:11.910279", "rephrasing_model": "gpt-5-mini"}
{"claim": "Using the identical training and assessment protocol applied to MACE-OFF23, EScAIP was tested on the SPICE dataset.", "label": "Supported", "paragraph": "Dataset. Weevaluate the EScAIP model's performance on the SPICE dataset [Eastman et al., 2023], which consists of approximately one million molecules across seven different categories. To ensure comparability, we adopt the same training and evaluation settings as used for the MACE-OFF23 model [Kovács et al., 2023a].", "section_name": "5.3 Molecules (SPICE)", "subsection_name": "", "paper_id": "Y4mBaZu4vy", "paper_title": "The Importance of Being Scalable: Improving the Speed and Accuracy of Neural Network Interatomic Potentials Across Chemical Domains", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:51:39.339651", "model": "gpt-5-mini", "original_claim": "The EScAIP model was evaluated on the SPICE dataset using the same training and evaluation configurations that were employed for the MACE-OFF23 model.", "rephrasing_timestamp": "2026-01-11T19:24:11.192660", "rephrasing_model": "gpt-5-mini"}
{"claim": "The researchers optimized a scaled-down neural model based on the EScAIP architecture, containing roughly forty-five million adjustable parameters, to estimate energies and forces for every input sample.", "label": "Supported", "paragraph": "Settings. Weuse a smaller EScAIP model with 45M parameters, trained to predict the energy and forces of each sample. The model's performance is then evaluated on the different SPICE test datasets and compared directly with MACE-OFF23 [Kovács et al., 2023a].", "section_name": "5.3 Molecules (SPICE)", "subsection_name": "", "paper_id": "Y4mBaZu4vy", "paper_title": "The Importance of Being Scalable: Improving the Speed and Accuracy of Neural Network Interatomic Potentials Across Chemical Domains", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:51:47.554731", "model": "gpt-5-mini", "original_claim": "They trained a reduced-size EScAIP neural network with 45 million parameters to predict the energy and force values for each sample.", "rephrasing_timestamp": "2026-01-11T19:24:19.964547", "rephrasing_model": "gpt-5-mini"}
{"claim": "The learned model was assessed on several SPICE evaluation sets, and its results were placed side-by-side with those of the MACE-OFF23 system described by Kovács et al. (2023a).", "label": "Supported", "paragraph": "Settings. Weuse a smaller EScAIP model with 45M parameters, trained to predict the energy and forces of each sample. The model's performance is then evaluated on the different SPICE test datasets and compared directly with MACE-OFF23 [Kovács et al., 2023a].", "section_name": "5.3 Molecules (SPICE)", "subsection_name": "", "paper_id": "Y4mBaZu4vy", "paper_title": "The Importance of Being Scalable: Improving the Speed and Accuracy of Neural Network Interatomic Potentials Across Chemical Domains", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:51:47.554747", "model": "gpt-5-mini", "original_claim": "The performance of this trained model was evaluated on multiple SPICE test datasets and directly compared to the MACE-OFF23 model (Kovács et al., 2023a).", "rephrasing_timestamp": "2026-01-11T19:24:21.886122", "rephrasing_model": "gpt-5-mini"}
{"claim": "When evaluated on the SPICE benchmark, EScAIP produces more precise energy and force estimates than MACE-OFF23 across the evaluation splits; the aggregated results are shown in Table 4.", "label": "Supported", "paragraph": "Results. A summary of EScAIP's results on the SPICE dataset is provided in Tab. 4, where it outperforms MACE-OFF23 in predicting the energy and forces on the different test sets.", "section_name": "5.3 Molecules (SPICE)", "subsection_name": "", "paper_id": "Y4mBaZu4vy", "paper_title": "The Importance of Being Scalable: Improving the Speed and Accuracy of Neural Network Interatomic Potentials Across Chemical Domains", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:51:54.818420", "model": "gpt-5-mini", "original_claim": "On the SPICE dataset, EScAIP provides more accurate energy and force predictions across the various test sets than MACE-OFF23, as summarized in Table 4.", "rephrasing_timestamp": "2026-01-11T19:24:19.570463", "rephrasing_model": "gpt-5-mini"}
{"claim": "The paper presents plots that trace how the model's filter parameters change throughout optimization in experiments performed on the Cora and Cornell network datasets.", "label": "Supported", "paragraph": "We visualize the variation of filters during the training process on the Cora and Cornell datasets. We train our model on both datasets for 100 epochs and visualize the filters at training steps 0, 20, 50, and 100.", "section_name": "B.10 VARIATION OF FILTERS DURING THE TRAINING PROCESS", "subsection_name": "", "paper_id": "5RielfrDkP", "paper_title": "Learning Adaptive Multiresolution Transforms via Meta-Framelet-based Graph Convolutional Network", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:52:25.986385", "model": "gpt-5-mini", "original_claim": "The study produced visual representations showing how model filters evolve during training for experiments conducted on both the Cora and Cornell graph datasets.", "rephrasing_timestamp": "2026-01-11T19:24:23.807297", "rephrasing_model": "gpt-5-mini"}
{"claim": "Each collection of input data was used to run the network for one hundred complete training cycles, and snapshots of the learned filters were captured at cycle indices 0, 20, 50, and 100.", "label": "Supported", "paragraph": "We visualize the variation of filters during the training process on the Cora and Cornell datasets. We train our model on both datasets for 100 epochs and visualize the filters at training steps 0, 20, 50, and 100.", "section_name": "B.10 VARIATION OF FILTERS DURING THE TRAINING PROCESS", "subsection_name": "", "paper_id": "5RielfrDkP", "paper_title": "Learning Adaptive Multiresolution Transforms via Meta-Framelet-based Graph Convolutional Network", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:52:25.986405", "model": "gpt-5-mini", "original_claim": "The model was trained on each dataset for 100 epochs, with filter visualizations recorded at training iterations corresponding to epoch numbers 0, 20, 50, and 100.", "rephrasing_timestamp": "2026-01-11T19:24:24.201176", "rephrasing_model": "gpt-5-mini"}
{"claim": "The Cora collection arranges scholarly papers into a web of citation links; each vertex corresponds to an academic article, and the label assigned to that vertex denotes the article’s field of research.", "label": "Supported", "paragraph": "Recall that the Cora dataset contains a graph of a citation network, where each node represents a scientific publication and the node labels are the research domain. Thus, the node label (i.e. the graph signal of interest) varies smoothly across the nodes, emphasizing the importance of low-frequency information in learning good graph representations.", "section_name": "B.10 VARIATION OF FILTERS DURING THE TRAINING PROCESS", "subsection_name": "", "paper_id": "5RielfrDkP", "paper_title": "Learning Adaptive Multiresolution Transforms via Meta-Framelet-based Graph Convolutional Network", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:52:36.546045", "model": "gpt-5-mini", "original_claim": "The Cora dataset is a citation-network graph where each node represents a scientific publication and the node's class label indicates that publication's research domain.", "rephrasing_timestamp": "2026-01-11T19:24:27.402125", "rephrasing_model": "gpt-5-mini"}
{"claim": "In the Cora citation network, connected vertices typically share the same labels, so broad, smoothly varying patterns on the graph contain the bulk of the information for producing high-quality node embeddings.", "label": "Supported", "paragraph": "Recall that the Cora dataset contains a graph of a citation network, where each node represents a scientific publication and the node labels are the research domain. Thus, the node label (i.e. the graph signal of interest) varies smoothly across the nodes, emphasizing the importance of low-frequency information in learning good graph representations.", "section_name": "B.10 VARIATION OF FILTERS DURING THE TRAINING PROCESS", "subsection_name": "", "paper_id": "5RielfrDkP", "paper_title": "Learning Adaptive Multiresolution Transforms via Meta-Framelet-based Graph Convolutional Network", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:52:36.546066", "model": "gpt-5-mini", "original_claim": "Within Cora, the node labels vary gradually across neighboring nodes, implying that low-frequency components of the graph signal are especially important for learning strong graph representations.", "rephrasing_timestamp": "2026-01-11T19:24:26.586990", "rephrasing_model": "gpt-5-mini"}
{"claim": "As learning proceeds, MM-FGCN becomes increasingly tuned to smooth, low-frequency signals on the graph. By contrast, at the start its parameterized filter bank is uniformly distributed across the spectrum and exhibits no preference for any particular frequency band.", "label": "Supported", "paragraph": "As shown in Figure 9 (a, b, c, d), we observe that MM-FGCN automatically learns to focus more on establishing the low-frequency representations: at the initial stage of the training process, the meta-filters are evenly separated and do not show special preference to the graph signals at specific frequencies.", "section_name": "B.10 VARIATION OF FILTERS DURING THE TRAINING PROCESS", "subsection_name": "", "paper_id": "5RielfrDkP", "paper_title": "Learning Adaptive Multiresolution Transforms via Meta-Framelet-based Graph Convolutional Network", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:52:43.238377", "model": "gpt-5-mini", "original_claim": "During training, MM-FGCN progressively emphasizes low-frequency graph representations, while at initialization its meta-filters are evenly distributed and show no preferential sensitivity to particular signal frequencies.", "rephrasing_timestamp": "2026-01-11T19:24:30.679531", "rephrasing_model": "gpt-5-mini"}
{"claim": "On February 1, 20XX, Boston's principal symphonic ensemble delivered its latest public performance at the city's venerable concert venue, Symphony Hall.", "label": "Supported", "paragraph": "Title: A Night of Enchantment with the Boston Symphony Orchestra\nThe Boston Symphony Orchestra (BSO) presented their latest concert at historic Symphony Hall in Boston on February 1st, 20XX. The evening was filled with captivating music from some of the most renowned composers throughout history.", "section_name": "LLaMA-2-OpenPlatypusNEFT Response :", "subsection_name": "", "paper_id": "0bMmZ3fkCk", "paper_title": "NEFTune: Noisy Embeddings Improve Instruction Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:53:03.241754", "model": "gpt-5-mini", "original_claim": "On February 1st, 20XX the Boston Symphony Orchestra performed its most recent concert at the historic Symphony Hall in Boston.", "rephrasing_timestamp": "2026-01-11T19:24:36.886586", "rephrasing_model": "gpt-5-mini"}
{"claim": "Audiences were treated to an assortment of riveting works by notable creators whose careers span various stages of musical evolution.", "label": "Supported", "paragraph": "Title: A Night of Enchantment with the Boston Symphony Orchestra\nThe Boston Symphony Orchestra (BSO) presented their latest concert at historic Symphony Hall in Boston on February 1st, 20XX. The evening was filled with captivating music from some of the most renowned composers throughout history.", "section_name": "LLaMA-2-OpenPlatypusNEFT Response :", "subsection_name": "", "paper_id": "0bMmZ3fkCk", "paper_title": "NEFTune: Noisy Embeddings Improve Instruction Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:53:03.241775", "model": "gpt-5-mini", "original_claim": "The evening's program featured engaging compositions by several widely acknowledged composers representing different periods of music history.", "rephrasing_timestamp": "2026-01-11T19:24:34.025998", "rephrasing_model": "gpt-5-mini"}
{"claim": "Andris Nelsons took charge of the night's performance, steering the musicians and helping to shape what the passage depicts as a spellbinding occasion for both the players and the spectators.", "label": "Supported", "paragraph": "With conductor Andris Nelsons leading the way, this night proved to be one of enchantment for both performers and audiences alike. As we took our seats inside Symphony Hall, anticipation filled the air as people chatted excitedly about what they were expecting from tonight's show.", "section_name": "LLaMA-2-OpenPlatypusNEFT Response :", "subsection_name": "", "paper_id": "0bMmZ3fkCk", "paper_title": "NEFTune: Noisy Embeddings Improve Instruction Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:53:15.475815", "model": "gpt-5-mini", "original_claim": "Andris Nelsons served as the conductor for the evening's concert, leading the orchestra and contributing to an experience the paragraph characterizes as enchanting for both performers and audience members.", "rephrasing_timestamp": "2026-01-11T19:24:36.470765", "rephrasing_model": "gpt-5-mini"}
{"claim": "As concertgoers settled into the music hall ahead of curtain, they traded animated predictions about the evening’s program, leaving the room humming with eager expectation.", "label": "Supported", "paragraph": "With conductor Andris Nelsons leading the way, this night proved to be one of enchantment for both performers and audiences alike. As we took our seats inside Symphony Hall, anticipation filled the air as people chatted excitedly about what they were expecting from tonight's show.", "section_name": "LLaMA-2-OpenPlatypusNEFT Response :", "subsection_name": "", "paper_id": "0bMmZ3fkCk", "paper_title": "NEFTune: Noisy Embeddings Improve Instruction Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:53:15.475831", "model": "gpt-5-mini", "original_claim": "While taking their seats in Symphony Hall before the performance, audience members engaged in excited conversations about what they expected from the show, creating a palpable sense of anticipation.", "rephrasing_timestamp": "2026-01-11T19:24:38.438128", "rephrasing_model": "gpt-5-mini"}
{"claim": "After people took their places, a hush settled over the room—broken only by the odd throat-clear or the soft scrape of footwear—until the stage lights were lowered to usher in the performance.", "label": "Supported", "paragraph": "The hall itself is quite impressive; its ornate design featuring intricate details like marble columns and stained glass windows creating an atmosphere of grandeur perfect for such an event. Once everyone had found their seat, there was complete silence except for the occasional cough or shuffling of feet until finally, the lights dimmed signaling that it was time for the show to begin.", "section_name": "LLaMA-2-OpenPlatypusNEFT Response :", "subsection_name": "", "paper_id": "0bMmZ3fkCk", "paper_title": "NEFTune: Noisy Embeddings Improve Instruction Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:53:24.727328", "model": "gpt-5-mini", "original_claim": "After attendees were seated, the audience remained largely silent, with only infrequent coughing or foot shuffling, until the lights dimmed to signal the start of the show.", "rephrasing_timestamp": "2026-01-11T19:24:39.486994", "rephrasing_model": "gpt-5-mini"}
{"claim": "This segment presents outputs generated by the analysis engine for three separate scenarios, all grouped together in Figure 10 of the paper.", "label": "Supported", "paragraph": "In this section, let's demonstrate several case studies. First, we present the results of the reasoning module for three cases, as shown in Figure 10. In Case 1, the green ego car is closely following the car 368 in front, and their speeds are similar.", "section_name": "A.3 Case study", "subsection_name": "", "paper_id": "OqTMUPuLuC", "paper_title": "DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:53:44.567186", "model": "gpt-5-mini", "original_claim": "The section demonstrates outcomes produced by the reasoning module for three distinct case studies, which are depicted collectively in Figure 10 of the document.", "rephrasing_timestamp": "2026-01-11T19:24:41.313116", "rephrasing_model": "gpt-5-mini"}
{"claim": "In Scenario One, the green subject vehicle is riding just behind the car marked 368, and the case notes indicate both are moving at roughly the same speed.", "label": "Supported", "paragraph": "In this section, let's demonstrate several case studies. First, we present the results of the reasoning module for three cases, as shown in Figure 10. In Case 1, the green ego car is closely following the car 368 in front, and their speeds are similar.", "section_name": "A.3 Case study", "subsection_name": "", "paper_id": "OqTMUPuLuC", "paper_title": "DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:53:44.567198", "model": "gpt-5-mini", "original_claim": "In Case 1, the green ego vehicle trails vehicle number 368 at close proximity, and both vehicles are reported to have similar speeds according to the case description.", "rephrasing_timestamp": "2026-01-11T19:24:43.078202", "rephrasing_model": "gpt-5-mini"}
{"claim": "First, the driving controller tried increasing speed without changing lanes, and only afterward did it consider other options.", "label": "Supported", "paragraph": "Initially, the driver agent explores whether it can accelerate in the current lane. The agent determines that, since the ego car's speed is similar to the car in front, there's no need to accelerate. Subsequently, the driver agent explores the possibility of maintaining the current speed.", "section_name": "A.3 Case study", "subsection_name": "", "paper_id": "OqTMUPuLuC", "paper_title": "DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:53:57.988990", "model": "gpt-5-mini", "original_claim": "At the start, the driver agent tested the option of increasing speed while remaining in the current lane before considering other maneuvers.", "rephrasing_timestamp": "2026-01-11T19:24:50.924657", "rephrasing_model": "gpt-5-mini"}
{"claim": "Upon determining its speed was nearly the same as the car ahead, the system decided against increasing speed and proceeded to assess holding its current pace.", "label": "Supported", "paragraph": "Initially, the driver agent explores whether it can accelerate in the current lane. The agent determines that, since the ego car's speed is similar to the car in front, there's no need to accelerate. Subsequently, the driver agent explores the possibility of maintaining the current speed.", "section_name": "A.3 Case study", "subsection_name": "", "paper_id": "OqTMUPuLuC", "paper_title": "DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:53:57.989009", "model": "gpt-5-mini", "original_claim": "After finding its own vehicle's velocity comparable to the lead vehicle, the agent judged acceleration unnecessary and then investigated sustaining the present speed.", "rephrasing_timestamp": "2026-01-11T19:24:48.482570", "rephrasing_model": "gpt-5-mini"}
{"claim": "The system concluded that, although the distance to the vehicle ahead was slightly under the desired spacing, staying at the present speed did not pose a safety risk because this car was traveling just a bit slower than the one in front.", "label": "Supported", "paragraph": "Through reasoning, the agent concludes that although the distance between the vehicles is slightly shorter than the ideal following distance, it's safe to maintain the speed because the ego car's speed is slightly lower than that of the car in front.", "section_name": "A.3 Case study", "subsection_name": "", "paper_id": "OqTMUPuLuC", "paper_title": "DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:54:04.477782", "model": "gpt-5-mini", "original_claim": "The agent determined that, despite the inter-vehicle gap being marginally below the preferred following distance, maintaining current speed is considered safe because the ego vehicle's velocity is marginally less than the lead vehicle's.", "rephrasing_timestamp": "2026-01-11T19:24:47.231808", "rephrasing_model": "gpt-5-mini"}
{"claim": "In this example the two methods target different quantities, which shows that the second method cannot be trusted to consistently estimate the causal impact of changing the exposure.", "label": "Supported", "paragraph": "The following examples show a simple case where the SRF and ERF estimands are different, thereby demonstrating why the ERF is not a useful estimate of the effect of an exposure shift. Consider a setting in which µ ( X , A ) = AX with X ∼ N (0 , 1) , A ∼ N ( X, 1) .", "section_name": "C ANOTHER EXAMPLE OF SRF VS ERF", "subsection_name": "", "paper_id": "MqEQbvPvkE", "paper_title": "Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:54:28.946499", "model": "gpt-5-mini", "original_claim": "In the given example, the SRF and ERF yield different estimands, which demonstrates that the ERF does not reliably estimate the causal effect of shifting the exposure.", "rephrasing_timestamp": "2026-01-11T19:24:49.869453", "rephrasing_model": "gpt-5-mini"}
{"claim": "In this setup μ(X,A) equals A multiplied by X; X is drawn from a zero-mean, unit-variance Gaussian, and conditional on X the variable A is drawn from a Gaussian centered at that X with variance one.", "label": "Supported", "paragraph": "The following examples show a simple case where the SRF and ERF estimands are different, thereby demonstrating why the ERF is not a useful estimate of the effect of an exposure shift. Consider a setting in which µ ( X , A ) = AX with X ∼ N (0 , 1) , A ∼ N ( X, 1) .", "section_name": "C ANOTHER EXAMPLE OF SRF VS ERF", "subsection_name": "", "paper_id": "MqEQbvPvkE", "paper_title": "Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:54:28.946511", "model": "gpt-5-mini", "original_claim": "The specified model defines μ(X,A) as the product A·X, with X sampled from a standard normal distribution and A sampled from a normal distribution with mean X and variance one.", "rephrasing_timestamp": "2026-01-11T19:24:57.355654", "rephrasing_model": "gpt-5-mini"}
{"claim": "If A is multiplied by c (so Ã = cA), the target quantity ψ = E[μ(X,Ã)] equals c: the expectation becomes c·E[X^2], and because the text sets E[X^2]=1, ψ = c.", "label": "Supported", "paragraph": "Now consider the exposure shift induced by ˜ A = cA for some c ∈ R . Using the SRF formulation, we find that ψ = E [ µ ( X , ˜ A )] = E [ E [ X ( cA ) | X ]] = c E [ X 2 ] = c . On the other hand, the ERF is ξ ( a ) = E [ µ ( X , A ) | A = a ] = a E [ X ] = 0 for all a ∈ R .", "section_name": "C ANOTHER EXAMPLE OF SRF VS ERF", "subsection_name": "", "paper_id": "MqEQbvPvkE", "paper_title": "Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:54:45.137749", "model": "gpt-5-mini", "original_claim": "Under the exposure transformation \\tilde A = cA, the SRF estimand ψ = E[μ(X,\\tilde A)] simplifies to c, because it reduces to c·E[X^2], which the paragraph sets equal to c.", "rephrasing_timestamp": "2026-01-11T19:24:58.936603", "rephrasing_model": "gpt-5-mini"}
{"claim": "Let ξ(a) be the conditional expectation of μ(X,A) when A takes the value a. This function is zero for all real a because conditioning produces a factor of a times the mean of X, and the text states that X has mean zero.", "label": "Supported", "paragraph": "Now consider the exposure shift induced by ˜ A = cA for some c ∈ R . Using the SRF formulation, we find that ψ = E [ µ ( X , ˜ A )] = E [ E [ X ( cA ) | X ]] = c E [ X 2 ] = c . On the other hand, the ERF is ξ ( a ) = E [ µ ( X , A ) | A = a ] = a E [ X ] = 0 for all a ∈ R .", "section_name": "C ANOTHER EXAMPLE OF SRF VS ERF", "subsection_name": "", "paper_id": "MqEQbvPvkE", "paper_title": "Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:54:45.137763", "model": "gpt-5-mini", "original_claim": "The ERF ξ(a) = E[μ(X,A) | A = a] evaluates to zero for every real a, since it simplifies to a·E[X] and the paragraph states E[X] = 0.", "rephrasing_timestamp": "2026-01-11T19:24:58.476935", "rephrasing_model": "gpt-5-mini"}
{"claim": "Because the relationship between intervention intensity and outcome is identically zero for every dose, it cannot capture the impact of changing the exposure, even when ψ is set to its true value.", "label": "Supported", "paragraph": "Therefore, estimators of the two estimands return two different estimates. Moreover, the ERF is identically zero for every treatment value. Thus, it cannot be used to approximate the value of the effect of the exposure shift, even when ψ is correctly specified.", "section_name": "C ANOTHER EXAMPLE OF SRF VS ERF", "subsection_name": "", "paper_id": "MqEQbvPvkE", "paper_title": "Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:54:53.616939", "model": "gpt-5-mini", "original_claim": "The exposure-response function takes the value zero for all treatment levels, therefore it fails to approximate the effect of an exposure shift even when the parameter ψ is correctly specified.", "rephrasing_timestamp": "2026-01-11T19:24:58.787952", "rephrasing_model": "gpt-5-mini"}
{"claim": "Across SGDL and each MGDL variant, the optimizer's step-size was chosen from the pool {0.01, 0.005, 0.001, 0.0005, 0.0001}.", "label": "Supported", "paragraph": "The network structure for SGDL is\n<!-- formula-not-decoded -->\nand that for MGDL is\n<!-- formula-not-decoded -->\nFor SGDL and all grades of MGDL, we select the learning rate from the set { 10 -2 , 5 × 10 -3 , 10 -3 , 5 × 10 -4 , 10 -4 } , choose the full gradient for each epoch, and set the total epoch number K to be 10 , 000 .", "section_name": "B.3 Section 3.3", "subsection_name": "", "paper_id": "IoRT7EhFap", "paper_title": "Addressing Spectral Bias of Deep Neural Networks by Multi-Grade Deep Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:55:22.353594", "model": "gpt-5-mini", "original_claim": "For SGDL and every MGDL variant, the learning rate was selected from the set {0.01, 0.005, 0.001, 0.0005, 0.0001}.", "rephrasing_timestamp": "2026-01-11T19:24:59.904297", "rephrasing_model": "gpt-5-mini"}
{"claim": "When training SGDL and every MGDL variant, the gradient computed over the entire dataset was used at every training cycle, and the number of cycles K was fixed at 10,000.", "label": "Supported", "paragraph": "The network structure for SGDL is\n<!-- formula-not-decoded -->\nand that for MGDL is\n<!-- formula-not-decoded -->\nFor SGDL and all grades of MGDL, we select the learning rate from the set { 10 -2 , 5 × 10 -3 , 10 -3 , 5 × 10 -4 , 10 -4 } , choose the full gradient for each epoch, and set the total epoch number K to be 10 , 000 .", "section_name": "B.3 Section 3.3", "subsection_name": "", "paper_id": "IoRT7EhFap", "paper_title": "Addressing Spectral Bias of Deep Neural Networks by Multi-Grade Deep Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:55:22.353606", "model": "gpt-5-mini", "original_claim": "During training of SGDL and all MGDL grades, a full gradient was used at each epoch and the total epoch count K was set to 10,000.", "rephrasing_timestamp": "2026-01-11T19:25:06.650862", "rephrasing_model": "gpt-5-mini"}
{"claim": "The panel displays SGDL outputs along the top row and MGDL outputs along the bottom row, enabling a direct, row-by-row comparison of the two approaches.", "label": "Supported", "paragraph": "Figure 10: Comparsion of SGDL (1st row) and MGDL (2nd row) for settings 1 and 2 of Section 3.2: The evolution of spectrum (the first and second columns for the learned functions on manifolds γ q with q = 4 and q = 0 , respectively, for setting 1, while the third and fourth columns for setting 2).", "section_name": "B.3 Section 3.3", "subsection_name": "", "paper_id": "IoRT7EhFap", "paper_title": "Addressing Spectral Bias of Deep Neural Networks by Multi-Grade Deep Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:55:37.665541", "model": "gpt-5-mini", "original_claim": "The figure presents results produced by SGDL in the top row and by MGDL in the bottom row, illustrating a direct rowwise comparison of both methods.", "rephrasing_timestamp": "2026-01-11T19:25:08.085417", "rephrasing_model": "gpt-5-mini"}
{"claim": "The first two columns display how the spectral content changes for functions inferred on the γ_q manifolds (q = 4 and q = 0) under experimental setup 1; the last two columns present the analogous spectra for setup 2.", "label": "Supported", "paragraph": "Figure 10: Comparsion of SGDL (1st row) and MGDL (2nd row) for settings 1 and 2 of Section 3.2: The evolution of spectrum (the first and second columns for the learned functions on manifolds γ q with q = 4 and q = 0 , respectively, for setting 1, while the third and fourth columns for setting 2).", "section_name": "B.3 Section 3.3", "subsection_name": "", "paper_id": "IoRT7EhFap", "paper_title": "Addressing Spectral Bias of Deep Neural Networks by Multi-Grade Deep Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:55:37.665563", "model": "gpt-5-mini", "original_claim": "Columns one and two depict the spectral evolution for learned functions on manifolds gamma_q with q=4 and q=0 respectively for setting 1, while columns three and four show the corresponding spectra for setting 2.", "rephrasing_timestamp": "2026-01-11T19:25:10.986417", "rephrasing_model": "gpt-5-mini"}
{"claim": "For the 'Cat' example, the MGDL approach produced individual outputs for grade levels one to four, and the PSNR measured on the test set for each output is listed in the figure captions.", "label": "Supported", "paragraph": "<!-- image -->\nFigure 11: Comparison of MGDL and SGDL for image Cat. (a)-(d): Predictions of MGDL for grades 1-4, with the corresponding testing PSNR values indicated in the subtitles. (e): Prediction of SGDL with testing PSNR displayed in the subtitle.", "section_name": "B.3 Section 3.3", "subsection_name": "", "paper_id": "IoRT7EhFap", "paper_title": "Addressing Spectral Bias of Deep Neural Networks by Multi-Grade Deep Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:55:45.509302", "model": "gpt-5-mini", "original_claim": "For the 'Cat' image, MGDL produced separate predictions for grades 1 through 4, and each prediction's testing PSNR value is reported in the figure subtitles.", "rephrasing_timestamp": "2026-01-11T19:25:11.673073", "rephrasing_model": "gpt-5-mini"}
{"claim": "To assess how substituting the exact ̅A_K with an approximate form affects predictive performance, the researchers evaluated a BERT model on the GLUE tasks; the results are reported in Table 9.", "label": "Supported", "paragraph": "To compare the impact of the actual ¯ A K and the approximated ¯ A K in terms of accuracy, we experimented with BERT on GLUE and the results are summarized in Table 9. BERTBASE+ ¯ A K denotes using the exactly calculated ¯ A K instead of the approximated ¯ A K .", "section_name": "J Comparison with Actual and Approximated High-order Terms", "subsection_name": "", "paper_id": "ffNrpcBpi6", "paper_title": "Graph Convolutions Enrich the Self-Attention in Transformers!", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:56:21.285476", "model": "gpt-5-mini", "original_claim": "The authors ran experiments with BERT on the GLUE benchmark to evaluate how using the exact ̅A_K versus its approximation influences model accuracy, with outcomes summarized in Table 9.", "rephrasing_timestamp": "2026-01-11T19:25:13.731953", "rephrasing_model": "gpt-5-mini"}
{"claim": "Rephrased claim: The label BERTBASE+ ̅A_K denotes a BERTBASE configuration that substitutes the approximated ̅A_K with an exactly determined ̅A_K.", "label": "Supported", "paragraph": "To compare the impact of the actual ¯ A K and the approximated ¯ A K in terms of accuracy, we experimented with BERT on GLUE and the results are summarized in Table 9. BERTBASE+ ¯ A K denotes using the exactly calculated ¯ A K instead of the approximated ¯ A K .", "section_name": "J Comparison with Actual and Approximated High-order Terms", "subsection_name": "", "paper_id": "ffNrpcBpi6", "paper_title": "Graph Convolutions Enrich the Self-Attention in Transformers!", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:56:21.285488", "model": "gpt-5-mini", "original_claim": "The notation BERTBASE+ ̅A_K refers to the BERTBASE variant that employs the precisely computed ̅A_K in place of the approximated ̅A_K.", "rephrasing_timestamp": "2026-01-11T19:25:17.353728", "rephrasing_model": "gpt-5-mini"}
{"claim": "Incorporating GFSA with a surrogate Ā_K boosted BERT BASE's CoLA performance, raising it from 56.79 to 59.56.", "label": "Supported", "paragraph": "Table 9: Comparison of performance using the exactly calculated ¯ A K vs. the approximated ¯ A K for GLUE tasks\n| Datasets          | #Params CoLA SST-2 MRPC QQP STS-B MNLI-m/mm QNLI RTE   |   #Params CoLA SST-2 MRPC QQP STS-B MNLI-m/mm QNLI RTE |   #Params CoLA SST-2 MRPC QQP STS-B MNLI-m/mm QNLI RTE |   #Params CoLA SST-2 MRPC QQP STS-B MNLI-m/mm QNLI RTE |   #Params CoLA SST-2 MRPC QQP STS-B MNLI-m/mm QNLI RTE |   #Params CoLA SST-2 MRPC QQP STS-B MNLI-m/mm QNLI RTE | #Params CoLA SST-2 MRPC QQP STS-B MNLI-m/mm QNLI RTE   | #Params CoLA SST-2 MRPC QQP STS-B MNLI-m/mm QNLI RTE   |   Avg |\n|----------|----------|----------|----------|----------|----------|----------|----------|----------|-------|\n| BERT BASE [16]          | 110M          |          56.79 |          93.81 |          88.7 |          88.32 |          88.16 | 84.96/84.15          | 91.63 66.06          | 82.51 |\n| + GFSA (approximated ¯ A K ) | 110M          |          59.56 |          94.15 |          90.6 |          88.46 |          88.33 | 85.12/85.06          | 91.95 68.95          | 83.58 |\n| + GFSA (actual ¯ A K )       | 110M          |          59.85 |          94.27 |          89.8 |          88.43 |          88.32 | 84.95/84.89          | 91.76 68.23          | 83.39 |", "section_name": "J Comparison with Actual and Approximated High-order Terms", "subsection_name": "", "paper_id": "ffNrpcBpi6", "paper_title": "Graph Convolutions Enrich the Self-Attention in Transformers!", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:56:32.552903", "model": "gpt-5-mini", "original_claim": "Applying GFSA with the approximated ¯A_K increased the CoLA score for BERT BASE from 56.79 to 59.56.", "rephrasing_timestamp": "2026-01-11T19:25:15.699142", "rephrasing_model": "gpt-5-mini"}
{"claim": "Applying GFSA together with an estimated \\overline{A}_K produced a mean GLUE benchmark score of 83.58 for the BERT-base model, outperforming the baseline mean of 82.51.", "label": "Supported", "paragraph": "Table 9: Comparison of performance using the exactly calculated ¯ A K vs. the approximated ¯ A K for GLUE tasks\n| Datasets          | #Params CoLA SST-2 MRPC QQP STS-B MNLI-m/mm QNLI RTE   |   #Params CoLA SST-2 MRPC QQP STS-B MNLI-m/mm QNLI RTE |   #Params CoLA SST-2 MRPC QQP STS-B MNLI-m/mm QNLI RTE |   #Params CoLA SST-2 MRPC QQP STS-B MNLI-m/mm QNLI RTE |   #Params CoLA SST-2 MRPC QQP STS-B MNLI-m/mm QNLI RTE |   #Params CoLA SST-2 MRPC QQP STS-B MNLI-m/mm QNLI RTE | #Params CoLA SST-2 MRPC QQP STS-B MNLI-m/mm QNLI RTE   | #Params CoLA SST-2 MRPC QQP STS-B MNLI-m/mm QNLI RTE   |   Avg |\n|----------|----------|----------|----------|----------|----------|----------|----------|----------|-------|\n| BERT BASE [16]          | 110M          |          56.79 |          93.81 |          88.7 |          88.32 |          88.16 | 84.96/84.15          | 91.63 66.06          | 82.51 |\n| + GFSA (approximated ¯ A K ) | 110M          |          59.56 |          94.15 |          90.6 |          88.46 |          88.33 | 85.12/85.06          | 91.95 68.95          | 83.58 |\n| + GFSA (actual ¯ A K )       | 110M          |          59.85 |          94.27 |          89.8 |          88.43 |          88.32 | 84.95/84.89          | 91.76 68.23          | 83.39 |", "section_name": "J Comparison with Actual and Approximated High-order Terms", "subsection_name": "", "paper_id": "ffNrpcBpi6", "paper_title": "Graph Convolutions Enrich the Self-Attention in Transformers!", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:56:32.552923", "model": "gpt-5-mini", "original_claim": "Using GFSA with the approximated ¯A_K produced an average GLUE score of 83.58 for BERT BASE, higher than the baseline average of 82.51.", "rephrasing_timestamp": "2026-01-11T19:25:18.403960", "rephrasing_model": "gpt-5-mini"}
{"claim": "An evaluation of bug detection in source code was conducted on the Devign corpus, with RoBERTa, CodeBERT, PLBART, and CodeT5 used as the principal model architectures.", "label": "Supported", "paragraph": "Setting. We conduct a code defect detection task based on Devign dataset provided by Zhou et al. [105]. We use RoBERTa [44], CodeBERT [23], PLBART [2], and CodeT5 [84] as our backbone models. The detailed settings are in Appendix P.1.", "section_name": "5.6 Experiments on Code Classification", "subsection_name": "", "paper_id": "ffNrpcBpi6", "paper_title": "Graph Convolutions Enrich the Self-Attention in Transformers!", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:56:42.375143", "model": "gpt-5-mini", "original_claim": "The study performed a code defect detection experiment using the Devign dataset and employed RoBERTa, CodeBERT, PLBART, and CodeT5 as the backbone models.", "rephrasing_timestamp": "2026-01-11T19:25:21.966552", "rephrasing_model": "gpt-5-mini"}
{"claim": "The OFE still arises when standardization modules have no trainable affine parameters or do not perform mean-centering, and it appears in network variants that apply standardization either before or after the main block — see Figures 2, 14, and 17.", "label": "Supported", "paragraph": "- OFE still occurs for weight-less or uncentred Norms, &amp; both Pre/Post-Norm (Figs 2, 14 and 17).\n- The OP Block (Fig 3) matches Pre-LN training speed/stability (Tabs 1 and 3), without standard Norms. It does so through an Entropy Regulation method to prevent attention entropy collapse.\n- The OP Block greatly reduces OFE compared to standard blocks (Figs 2, 4 and 13).", "section_name": "Sec 3 key takeaways: normalisation layers and OFE.", "subsection_name": "", "paper_id": "npJQ6qS4bg", "paper_title": "Understanding and Minimising Outlier Features in Transformer Training", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:57:36.705685", "model": "gpt-5-mini", "original_claim": "OFE persists when normalization layers are weight-less or uncentered, and it appears in both pre-normalization and post-normalization model variants (as demonstrated in Figures 2, 14, and 17).", "rephrasing_timestamp": "2026-01-11T19:25:21.699111", "rephrasing_model": "gpt-5-mini"}
{"claim": "Although it omits conventional normalization layers, the OP Block achieves a convergence rate and robustness comparable to architectures that normalize before each layer by using an entropy-control mechanism that prevents the attention distribution’s entropy from collapsing (see Fig. 3; Tables 1 and 3).", "label": "Supported", "paragraph": "- OFE still occurs for weight-less or uncentred Norms, &amp; both Pre/Post-Norm (Figs 2, 14 and 17).\n- The OP Block (Fig 3) matches Pre-LN training speed/stability (Tabs 1 and 3), without standard Norms. It does so through an Entropy Regulation method to prevent attention entropy collapse.\n- The OP Block greatly reduces OFE compared to standard blocks (Figs 2, 4 and 13).", "section_name": "Sec 3 key takeaways: normalisation layers and OFE.", "subsection_name": "", "paper_id": "npJQ6qS4bg", "paper_title": "Understanding and Minimising Outlier Features in Transformer Training", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:57:36.705696", "model": "gpt-5-mini", "original_claim": "The OP Block attains training speed and stability comparable to Pre-LayerNorm models without using standard normalization, by employing an entropy-regulation mechanism that prevents attention entropy collapse (Fig. 3; Tables 1 and 3).", "rephrasing_timestamp": "2026-01-11T19:25:26.082682", "rephrasing_model": "gpt-5-mini"}
{"claim": "Architectures in the Transformer family construct the complete network by arranging a series of recurring processing modules consecutively, so the overall model emerges from the repeated combination of those constituent units.", "label": "Supported", "paragraph": "A Transformer architecture [37] is formed by sequentially composing Transformer blocks. The two most popular Transformer blocks are Pre-Norm and Post-Norm. The Pre-Norm Transformer block [48, 49] is nowadays more widespread than the original Post-Norm block [37] due to advantageous depth-scaling properties [74, 51].", "section_name": "A.1 Mathematical Description of Transformer Blocks", "subsection_name": "", "paper_id": "npJQ6qS4bg", "paper_title": "Understanding and Minimising Outlier Features in Transformer Training", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:57:45.860817", "model": "gpt-5-mini", "original_claim": "Transformer architectures are constructed by placing multiple Transformer blocks in sequence, forming the overall model through successive composition of these modular block units.", "rephrasing_timestamp": "2026-01-11T19:25:26.391222", "rephrasing_model": "gpt-5-mini"}
{"claim": "Empirical work has shown that normalizing inputs at the beginning of each Transformer layer yields more favorable scaling as models grow deeper, so that variant has largely supplanted the original approach that applied normalization at the end of layers.", "label": "Supported", "paragraph": "A Transformer architecture [37] is formed by sequentially composing Transformer blocks. The two most popular Transformer blocks are Pre-Norm and Post-Norm. The Pre-Norm Transformer block [48, 49] is nowadays more widespread than the original Post-Norm block [37] due to advantageous depth-scaling properties [74, 51].", "section_name": "A.1 Mathematical Description of Transformer Blocks", "subsection_name": "", "paper_id": "npJQ6qS4bg", "paper_title": "Understanding and Minimising Outlier Features in Transformer Training", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:57:45.860832", "model": "gpt-5-mini", "original_claim": "Pre-Norm Transformer blocks have become more commonly used than the original Post-Norm blocks because they provide improved depth-scaling behavior, as reported by cited studies.", "rephrasing_timestamp": "2026-01-11T19:25:26.701765", "rephrasing_model": "gpt-5-mini"}
{"claim": "Sequence X is encoded as a matrix of real numbers with T rows and d columns, where T is the count of elements in the sequence and d is the length of the vector used to represent each element.", "label": "Supported", "paragraph": "For an input sequence representation X in ∈ R T × d , with T tokens and dimension d , the output X out of a Pre-Norm Transformer block is:\n<!-- formula-not-decoded -->\nOn the other hand, the Post-Norm Transformer block can be expressed as:\n<!-- formula-not-decoded -->\nHere, 'MHA' stands for Multi-Head Attention (detailed below), and 'Norm' denotes a normalisation layer like LayerNorm [44] or RMSNorm [45].", "section_name": "A.1 Mathematical Description of Transformer Blocks", "subsection_name": "", "paper_id": "npJQ6qS4bg", "paper_title": "Understanding and Minimising Outlier Features in Transformer Training", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:57:54.510053", "model": "gpt-5-mini", "original_claim": "The input sequence X is represented as a real-valued matrix of shape T×d, where T denotes the number of tokens in the sequence and d denotes each token's embedding dimensionality.", "rephrasing_timestamp": "2026-01-11T19:25:29.480495", "rephrasing_model": "gpt-5-mini"}
{"claim": "The examples shown in Figure 1 were produced after tailoring the system using a single exemplar image, indicating it can be configured from just one sample.", "label": "Supported", "paragraph": "Some generated results are shown in Figure 1, where only single image is provided to our model for customization. From the results, we can see that the proposed framework effectively achieves customized generation in arbitrary downstream domain.", "section_name": "3.2 MAIN RESULTS", "subsection_name": "", "paper_id": "VzPGV19Bnp", "paper_title": "Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:58:19.229321", "model": "gpt-5-mini", "original_claim": "Figure 1 shows generated results produced by the model after customizing it with only one input image, demonstrating the model's capability to be adapted from a single example.", "rephrasing_timestamp": "2026-01-11T19:25:31.367760", "rephrasing_model": "gpt-5-mini"}
{"claim": "Empirical tests show the presented approach reliably creates tailored outputs that transfer to a wide range of subsequent application areas, confirming its ability to adapt without relying on any specific domain.", "label": "Supported", "paragraph": "Some generated results are shown in Figure 1, where only single image is provided to our model for customization. From the results, we can see that the proposed framework effectively achieves customized generation in arbitrary downstream domain.", "section_name": "3.2 MAIN RESULTS", "subsection_name": "", "paper_id": "VzPGV19Bnp", "paper_title": "Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:58:19.229332", "model": "gpt-5-mini", "original_claim": "The experimental outputs indicate the proposed framework successfully produces customized generations applicable across arbitrary downstream domains, demonstrating effective domain-agnostic customization performance.", "rephrasing_timestamp": "2026-01-11T19:25:36.451114", "rephrasing_model": "gpt-5-mini"}
{"claim": "A purpose-built synthesis pipeline follows the supplied text directives and keeps the source image's minute visual characteristics intact in the generated results.", "label": "Supported", "paragraph": "Our customized generation meets the specified text requirements while maintaining fine-grained details of the input image. More results are provided in the Appendix. 2 Results of Stable Diffusion is obtained by directly feeding corresponding prompts into the model.", "section_name": "3.2 MAIN RESULTS", "subsection_name": "", "paper_id": "VzPGV19Bnp", "paper_title": "Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:58:34.329565", "model": "gpt-5-mini", "original_claim": "The customized generation approach satisfies the specified textual requirements and preserves fine-grained details of the original input image in the produced outputs.", "rephrasing_timestamp": "2026-01-11T19:25:34.948666", "rephrasing_model": "gpt-5-mini"}
{"claim": "The Stable Diffusion outputs were created solely from the provided textual inputs, with no subsequent edits, enhancements, or other post‑generation processing applied.", "label": "Supported", "paragraph": "Our customized generation meets the specified text requirements while maintaining fine-grained details of the input image. More results are provided in the Appendix. 2 Results of Stable Diffusion is obtained by directly feeding corresponding prompts into the model.", "section_name": "3.2 MAIN RESULTS", "subsection_name": "", "paper_id": "VzPGV19Bnp", "paper_title": "Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:58:34.329583", "model": "gpt-5-mini", "original_claim": "The Stable Diffusion results reported were produced by directly supplying the model with the corresponding prompts without additional modifications or processing.", "rephrasing_timestamp": "2026-01-11T19:25:37.463233", "rephrasing_model": "gpt-5-mini"}
{"claim": "To mirror the setup from Gal et al. (2023), the authors tested their approach on the face-photo dataset released by Gal and colleagues, with additional details provided in the section on the human-face domain.", "label": "Supported", "paragraph": "Text\nFigure 4: Comparison with baseline methods. Our proposed approach exhibits superior capability for preserving fine-grained details. <!-- image -->\nHuman Face Domain Following Gal et al. (2023), we first evaluate our method on human face images provided in Gal et al.", "section_name": "3.2 MAIN RESULTS", "subsection_name": "", "paper_id": "VzPGV19Bnp", "paper_title": "Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T15:58:45.194013", "model": "gpt-5-mini", "original_claim": "Following Gal et al. (2023), the authors evaluated their proposed method on the human face image dataset provided by Gal et al., as described in the Human Face Domain section.", "rephrasing_timestamp": "2026-01-11T19:25:40.165677", "rephrasing_model": "gpt-5-mini"}
{"claim": "These methods form a dominant category of data-synthesis techniques that iteratively convert samples from a standard noise distribution into highly structured outputs using a sequence of stochastic, memoryless transitions.", "label": "Supported", "paragraph": "Diffusion models [13] are a dominant class of generative models that transform Gaussian noise into complex data distributions via a Markov process. The forward process is defined by:\n<!-- formula-not-decoded -->\nwhere { β t } progressively increases noise until the data becomes indistinguishable from noise.", "section_name": "3.1 Preliminary", "subsection_name": "", "paper_id": "46jtDC6gXu", "paper_title": "AsyncDiff: Parallelizing Diffusion Models by Asynchronous Denoising", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:59:12.226912", "model": "gpt-5-mini", "original_claim": "Diffusion models constitute a leading type of generative model that map Gaussian random inputs into intricate data distributions through a Markovian transformation process.", "rephrasing_timestamp": "2026-01-11T19:25:43.462420", "rephrasing_model": "gpt-5-mini"}
{"claim": "During the progressive corruption phase, a steadily increasing series of scalar coefficients introduces successive stochastic perturbations, gradually erasing the original signal until the samples are effectively indistinguishable from random noise.", "label": "Supported", "paragraph": "Diffusion models [13] are a dominant class of generative models that transform Gaussian noise into complex data distributions via a Markov process. The forward process is defined by:\n<!-- formula-not-decoded -->\nwhere { β t } progressively increases noise until the data becomes indistinguishable from noise.", "section_name": "3.1 Preliminary", "subsection_name": "", "paper_id": "46jtDC6gXu", "paper_title": "AsyncDiff: Parallelizing Diffusion Models by Asynchronous Denoising", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:59:12.226924", "model": "gpt-5-mini", "original_claim": "During the forward diffusion, an increasing sequence of parameters β_t incrementally adds noise so that the data eventually becomes indistinguishable from pure noise.", "rephrasing_timestamp": "2026-01-11T19:25:45.108373", "rephrasing_model": "gpt-5-mini"}
{"claim": "The backward reconstruction routine recovers data by repeatedly removing corruption in successive iterations; at every step the model supplies an estimated conditional mean for the current noisy sample and step index, and the amount of noise at that step is described by a corresponding variance (often written σ_t^2).", "label": "Supported", "paragraph": "The reverse process, essential for data reconstruction, involves iterative denoising:\n<!-- formula-not-decoded -->\nwhere µ θ ( x t , t ) is the predicted mean and σ 2 t is the variance. For DDIMs [55], the reverse update is deterministic:\n<!-- formula-not-decoded -->\nwhere α t is the cumulative product of (1 -β t ) .", "section_name": "3.1 Preliminary", "subsection_name": "", "paper_id": "46jtDC6gXu", "paper_title": "AsyncDiff: Parallelizing Diffusion Models by Asynchronous Denoising", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:59:23.144020", "model": "gpt-5-mini", "original_claim": "The data reconstruction reverse process performs iterative denoising steps, with the model predicting a mean μθ(x_t,t) and using σ_t^2 to denote the noise variance at each timestep.", "rephrasing_timestamp": "2026-01-11T19:25:52.623341", "rephrasing_model": "gpt-5-mini"}
{"claim": "In the deterministic implicit-diffusion setting, the backward step contains no randomness, and the coefficient α_t is computed by multiplying, over all timesteps up to t, the terms equal to one minus β at each step.", "label": "Supported", "paragraph": "The reverse process, essential for data reconstruction, involves iterative denoising:\n<!-- formula-not-decoded -->\nwhere µ θ ( x t , t ) is the predicted mean and σ 2 t is the variance. For DDIMs [55], the reverse update is deterministic:\n<!-- formula-not-decoded -->\nwhere α t is the cumulative product of (1 -β t ) .", "section_name": "3.1 Preliminary", "subsection_name": "", "paper_id": "46jtDC6gXu", "paper_title": "AsyncDiff: Parallelizing Diffusion Models by Asynchronous Denoising", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:59:23.144036", "model": "gpt-5-mini", "original_claim": "In DDIMs the reverse update is deterministic, and α_t is defined as the cumulative (running) product across time of the factors (1 − β_t).", "rephrasing_timestamp": "2026-01-11T19:25:49.872546", "rephrasing_model": "gpt-5-mini"}
{"claim": "These methods consume a great deal of processing power, which impacts the fidelity of the outputs they produce, so streamlined runtime procedures are necessary to make them viable for deployment in practical settings.", "label": "Supported", "paragraph": "These processes are computationally intensive, influencing the quality of generated samples and necessitating efficient inference methods for practical applications.", "section_name": "3.1 Preliminary", "subsection_name": "", "paper_id": "46jtDC6gXu", "paper_title": "AsyncDiff: Parallelizing Diffusion Models by Asynchronous Denoising", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:59:28.285756", "model": "gpt-5-mini", "original_claim": "The described processes demand substantial computational resources, which affect the quality of produced samples and therefore require efficient inference techniques to be practical in real-world applications.", "rephrasing_timestamp": "2026-01-11T19:25:44.620656", "rephrasing_model": "gpt-5-mini"}
{"claim": "VFIMamba-S (the compact edition) and VFIMamba (the performance edition) are assigned an N value of three.", "label": "Supported", "paragraph": "We provide two models: a lightweight model, VFIMamba-S, and a high-performance model, VFIMamba. Both models have N = 3; the only di ff erence is that VFIMamba has twice the number of channels as VFIMamba-S.", "section_name": "4 Experiments", "subsection_name": "", "paper_id": "4s5UsBUsUS", "paper_title": "VFIMamba: Video Frame Interpolation with State Space Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:59:50.870400", "model": "gpt-5-mini", "original_claim": "Both VFIMamba-S, the lightweight variant, and VFIMamba, the high-performance variant, are configured with N equal to 3.", "rephrasing_timestamp": "2026-01-11T19:25:52.672065", "rephrasing_model": "gpt-5-mini"}
{"claim": "No other design aspects are reported to vary; the high-end VFIMamba uses double the channel count of the VFIMamba-S.", "label": "Supported", "paragraph": "We provide two models: a lightweight model, VFIMamba-S, and a high-performance model, VFIMamba. Both models have N = 3; the only di ff erence is that VFIMamba has twice the number of channels as VFIMamba-S.", "section_name": "4 Experiments", "subsection_name": "", "paper_id": "4s5UsBUsUS", "paper_title": "VFIMamba: Video Frame Interpolation with State Space Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T15:59:50.870420", "model": "gpt-5-mini", "original_claim": "The only architectural difference described is that the VFIMamba high-performance model uses twice the number of channels compared to VFIMamba-S.", "rephrasing_timestamp": "2026-01-11T19:25:56.597937", "rephrasing_model": "gpt-5-mini"}
{"claim": "The researchers followed a progressive training regimen, held T at fifty, and trained the model for 300 full passes through the dataset.", "label": "Supported", "paragraph": "As described in Section 3.4, we employ a curriculum learning strategy in which T = 50 and train for 300 epochs in total. More model configurations and training details are provided in the appendix. <!-- image -->\n(Ours)\nFigure 4: Visualizations from SNU-FILM (Reda et al., 2022) and X-TEST (Sim et al., 2021).", "section_name": "4 Experiments", "subsection_name": "", "paper_id": "4s5UsBUsUS", "paper_title": "VFIMamba: Video Frame Interpolation with State Space Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:00:01.573851", "model": "gpt-5-mini", "original_claim": "The study used a curriculum learning schedule with T set to 50 and conducted training for a total of 300 epochs.", "rephrasing_timestamp": "2026-01-11T19:25:50.071958", "rephrasing_model": "gpt-5-mini"}
{"claim": "The fourth illustration presents example frames sourced from the SNU-FILM collection described by Reda et al. (2022) as well as from the X-TEST set compiled by Sim et al. (2021).", "label": "Supported", "paragraph": "As described in Section 3.4, we employ a curriculum learning strategy in which T = 50 and train for 300 epochs in total. More model configurations and training details are provided in the appendix. <!-- image -->\n(Ours)\nFigure 4: Visualizations from SNU-FILM (Reda et al., 2022) and X-TEST (Sim et al., 2021).", "section_name": "4 Experiments", "subsection_name": "", "paper_id": "4s5UsBUsUS", "paper_title": "VFIMamba: Video Frame Interpolation with State Space Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:00:01.573867", "model": "gpt-5-mini", "original_claim": "Figure 4 displays visualizations drawn from the SNU-FILM dataset (Reda et al., 2022) and from the X-TEST dataset (Sim et al., 2021).", "rephrasing_timestamp": "2026-01-11T19:25:55.742311", "rephrasing_model": "gpt-5-mini"}
{"claim": "Panel five illustrates how the number of floating-point operations and the graphics processor's memory footprint change as input resolution is increased.", "label": "Supported", "paragraph": "<!-- image -->\nFigure 5: Comparisons of FLOPs and GPU memory usage with increasing resolution input.", "section_name": "4 Experiments", "subsection_name": "", "paper_id": "4s5UsBUsUS", "paper_title": "VFIMamba: Video Frame Interpolation with State Space Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:00:19.152526", "model": "gpt-5-mini", "original_claim": "Figure 5 presents a comparison of computational FLOPs and GPU memory consumption measured across progressively higher input resolutions.", "rephrasing_timestamp": "2026-01-11T19:25:57.746169", "rephrasing_model": "gpt-5-mini"}
{"claim": "A low-rank adaptation method and its quantized variant provide resource-frugal ways to modify very large pretrained language models so they can be tailored for specific application tasks.", "label": "Supported", "paragraph": "LoRA (Hu et al., 2021) and QLoRA (Dettmers et al., 2023) are efficient fine-tuning techniques that were designed for LLMs. LoRA can be also utilized in other foundation models such as LDM (Cuenca &amp; Paul, 2023).", "section_name": "3.1 PRELIMINARIES OF LORA", "subsection_name": "", "paper_id": "TzWLecXH6I", "paper_title": "Towards Personalized AI: Early-stopping Low-Rank Adaptation of Foundation Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:00:42.524153", "model": "gpt-5-mini", "original_claim": "LoRA and QLoRA are computationally efficient fine-tuning approaches that were developed specifically to adapt large language models (LLMs) for downstream tasks.", "rephrasing_timestamp": "2026-01-11T19:25:59.567292", "rephrasing_model": "gpt-5-mini"}
{"claim": "Cuenca and Paul (2023) report that the Low-Rank Adaptation (LoRA) approach, though originally aimed at large language models, can also be adapted for other core model families — for example, latent-diffusion-based generative models.", "label": "Supported", "paragraph": "LoRA (Hu et al., 2021) and QLoRA (Dettmers et al., 2023) are efficient fine-tuning techniques that were designed for LLMs. LoRA can be also utilized in other foundation models such as LDM (Cuenca &amp; Paul, 2023).", "section_name": "3.1 PRELIMINARIES OF LORA", "subsection_name": "", "paper_id": "TzWLecXH6I", "paper_title": "Towards Personalized AI: Early-stopping Low-Rank Adaptation of Foundation Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:00:42.524171", "model": "gpt-5-mini", "original_claim": "The LoRA technique can also be applied to foundation models beyond LLMs, for example being used with latent diffusion models (LDM) as reported by Cuenca and Paul (2023).", "rephrasing_timestamp": "2026-01-11T19:26:00.802914", "rephrasing_model": "gpt-5-mini"}
{"claim": "LoRA encodes large pretrained models through a compact, rank-limited decomposition, greatly cutting the number of weights that must be updated and shrinking the memory footprint when adapting the model to target tasks.", "label": "Supported", "paragraph": "LoRA is a low-rank decomposition of foundation models, which can significantly reduce the number of trainable parameters and memory usage during the fine-tuning process of downstream tasks. Furthermore, a pre-trained foundation can be used to build many small LoRA branches for different tasks.", "section_name": "3.1 PRELIMINARIES OF LORA", "subsection_name": "", "paper_id": "TzWLecXH6I", "paper_title": "Towards Personalized AI: Early-stopping Low-Rank Adaptation of Foundation Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:00:53.609820", "model": "gpt-5-mini", "original_claim": "LoRA represents foundation architectures using a low-rank factorization, substantially lowering the number of parameters that must be trained and reducing memory consumption during fine-tuning on downstream tasks.", "rephrasing_timestamp": "2026-01-11T19:26:07.783537", "rephrasing_model": "gpt-5-mini"}
{"claim": "Instead of retraining the full network for every new objective, a single already-trained central model can be reused to produce several small, task-specific adapter modules, each tuned for a different target task.", "label": "Supported", "paragraph": "LoRA is a low-rank decomposition of foundation models, which can significantly reduce the number of trainable parameters and memory usage during the fine-tuning process of downstream tasks. Furthermore, a pre-trained foundation can be used to build many small LoRA branches for different tasks.", "section_name": "3.1 PRELIMINARIES OF LORA", "subsection_name": "", "paper_id": "TzWLecXH6I", "paper_title": "Towards Personalized AI: Early-stopping Low-Rank Adaptation of Foundation Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:00:53.609834", "model": "gpt-5-mini", "original_claim": "A single pre-trained foundation model can serve as the base to create multiple compact LoRA adapters, each tailored to a distinct downstream task, instead of retraining the entire model for every task.", "rephrasing_timestamp": "2026-01-11T19:26:05.795958", "rephrasing_model": "gpt-5-mini"}
{"claim": "When using LoRA to adapt a model on a small, personalized dataset, the base model’s already-learned d-by-k parameter array is kept unchanged, and only the added low-rank adapter parameters are updated.", "label": "Supported", "paragraph": "The details are elaborated in Figure 2 and Equation 1. Figure 2: In the LoRA branch, the matrices A and B are trainable and can efficiently fit the small amount of personalized data\n<!-- image -->\n<!-- formula-not-decoded -->\nWhen we use LoRA to fine-tune on a small personalized data, the original pre-trained weight matrix W ∈ R d × k is frozen.", "section_name": "3.1 PRELIMINARIES OF LORA", "subsection_name": "", "paper_id": "TzWLecXH6I", "paper_title": "Towards Personalized AI: Early-stopping Low-Rank Adaptation of Foundation Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:00:59.329262", "model": "gpt-5-mini", "original_claim": "When fine-tuning with LoRA on a limited personalized dataset, the original pre-trained weight matrix W (with dimensions d × k) remains unchanged (frozen), while only the LoRA parameters are trained.", "rephrasing_timestamp": "2026-01-11T19:26:06.927612", "rephrasing_model": "gpt-5-mini"}
{"claim": "An asterisk indicates that Junsheng Zhou and Weiqi Zhang share equal first-author credit; both are based in the School of Software at Tsinghua University in Beijing, China.", "label": "Supported", "paragraph": "Junsheng Zhou ∗\nWeiqi Zhang ∗\nYu-Shen Liu †\nSchool of Software, Tsinghua University, Beijing, China\n{zhou-js24,zwq23}@mails.tsinghua.edu.cn liuyushen@tsinghua.edu.cn", "section_name": "DiffGS: Functional Gaussian Splatting Diffusion", "subsection_name": "", "paper_id": "6zROYoHlcp", "paper_title": "DiffGS: Functional Gaussian Splatting Diffusion", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:01:27.799376", "model": "gpt-5-mini", "original_claim": "Junsheng Zhou and Weiqi Zhang are indicated as co-first authors (marked with an asterisk) and are affiliated with the School of Software at Tsinghua University in Beijing, China.", "rephrasing_timestamp": "2026-01-11T19:26:10.215804", "rephrasing_model": "gpt-5-mini"}
{"claim": "You can reach Yu-Shen Liu — an author employed at the Software School of Tsinghua University in Beijing — by emailing liuyushen@tsinghua.edu.cn.", "label": "Supported", "paragraph": "Junsheng Zhou ∗\nWeiqi Zhang ∗\nYu-Shen Liu †\nSchool of Software, Tsinghua University, Beijing, China\n{zhou-js24,zwq23}@mails.tsinghua.edu.cn liuyushen@tsinghua.edu.cn", "section_name": "DiffGS: Functional Gaussian Splatting Diffusion", "subsection_name": "", "paper_id": "6zROYoHlcp", "paper_title": "DiffGS: Functional Gaussian Splatting Diffusion", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:01:27.799396", "model": "gpt-5-mini", "original_claim": "Yu-Shen Liu is an author affiliated with the School of Software at Tsinghua University in Beijing and is reachable via the email liuyushen@tsinghua.edu.cn.", "rephrasing_timestamp": "2026-01-11T19:26:15.016849", "rephrasing_model": "gpt-5-mini"}
{"claim": "To examine the impact of core design options and key tuning parameters in DiffGS, we carried out controlled component-removal tests on the chair subset of ShapeNet.", "label": "Supported", "paragraph": "To evaluate some major designs and important hyper-parameters in DiffGS, we conduct ablation studies under the chair class of ShapeNet dataset. We report the performance in terms of PSNR, SSIM and LPIPS of the reconstructed 3DGS with Gaussian VAE.", "section_name": "4.4 Ablation Study", "subsection_name": "", "paper_id": "6zROYoHlcp", "paper_title": "DiffGS: Functional Gaussian Splatting Diffusion", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:01:50.484396", "model": "gpt-5-mini", "original_claim": "Ablation experiments were performed on the ShapeNet chair category to evaluate major architectural choices and important hyper-parameters of the DiffGS approach.", "rephrasing_timestamp": "2026-01-11T19:26:13.749166", "rephrasing_model": "gpt-5-mini"}
{"claim": "The authors evaluate how faithfully a Gaussian variational autoencoder reconstructs 3DGS by reporting PSNR, SSIM, and LPIPS measurements.", "label": "Supported", "paragraph": "To evaluate some major designs and important hyper-parameters in DiffGS, we conduct ablation studies under the chair class of ShapeNet dataset. We report the performance in terms of PSNR, SSIM and LPIPS of the reconstructed 3DGS with Gaussian VAE.", "section_name": "4.4 Ablation Study", "subsection_name": "", "paper_id": "6zROYoHlcp", "paper_title": "DiffGS: Functional Gaussian Splatting Diffusion", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:01:50.484416", "model": "gpt-5-mini", "original_claim": "The paper reports reconstruction quality using PSNR, SSIM, and LPIPS to assess 3DGS outputs generated by a Gaussian VAE.", "rephrasing_timestamp": "2026-01-11T19:26:14.243713", "rephrasing_model": "gpt-5-mini"}
{"claim": "In the second table they examine alternative design choices for the framework and show that the cut-off operation λ improves the GauPF model, also listing comparative runs that exclude this operation and are labeled \"w/o truncation\".", "label": "Supported", "paragraph": "Framework Designs. We first evaluate some major designs of our framework in Tab. 2. We justify the effectiveness of introducing the truncation function λ when modeling GauPF and report the results without λ as 'w/o truncation'.", "section_name": "4.4 Ablation Study", "subsection_name": "", "paper_id": "6zROYoHlcp", "paper_title": "DiffGS: Functional Gaussian Splatting Diffusion", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:01:58.486413", "model": "gpt-5-mini", "original_claim": "They assess framework designs in Table 2 and justify the effectiveness of the truncation function λ for modeling GauPF, presenting counterpart results without λ labeled 'w/o truncation'.", "rephrasing_timestamp": "2026-01-11T19:26:19.060579", "rephrasing_model": "gpt-5-mini"}
{"claim": "The third table summarizes tests that evaluate the effect of each element of the proposed design by removing components one at a time and observing the resulting changes in performance.", "label": "Supported", "paragraph": "In our ablation study depicted in Table 3, we systematically assessed the individual contributions of each component within our proposed framework. By selectively deactivating elements such as the GEV loss, conditional block maxima input, and high-frequency inflation module, we observed consistent performance degradation across all scenarios.", "section_name": "5.3 Ablation Study", "subsection_name": "", "paper_id": "5HQhYiGnYb", "paper_title": "FIDE: Frequency-Inflated Conditional Diffusion Model for Extreme-Aware Time Series Generation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:02:19.721980", "model": "gpt-5-mini", "original_claim": "Table 3 presents an ablation study in which the authors individually tested the contribution of each proposed framework component by turning off specific modules and comparing resulting performance.", "rephrasing_timestamp": "2026-01-11T19:26:20.791954", "rephrasing_model": "gpt-5-mini"}
{"claim": "Across all evaluated scenarios, omitting any one of these components—the loss term derived from the generalized extreme value distribution, the input that encodes conditional block maxima, or the module that amplifies high‑frequency signals—uniformly reduced the model’s performance.", "label": "Supported", "paragraph": "In our ablation study depicted in Table 3, we systematically assessed the individual contributions of each component within our proposed framework. By selectively deactivating elements such as the GEV loss, conditional block maxima input, and high-frequency inflation module, we observed consistent performance degradation across all scenarios.", "section_name": "5.3 Ablation Study", "subsection_name": "", "paper_id": "5HQhYiGnYb", "paper_title": "FIDE: Frequency-Inflated Conditional Diffusion Model for Extreme-Aware Time Series Generation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:02:19.721992", "model": "gpt-5-mini", "original_claim": "Removing the GEV loss, the conditional block maxima input, or the high-frequency inflation module each produced consistent declines in model performance across every tested scenario.", "rephrasing_timestamp": "2026-01-11T19:26:23.037190", "rephrasing_model": "gpt-5-mini"}
{"claim": "Omitting the input that supplied block-level conditional maxima markedly altered both the symmetrized information divergence and the relative-entropy measure.", "label": "Supported", "paragraph": "Notably, the absence of the conditional block maxima input significantly impacted the Jenson-Shannon Divergence and KL Divergence metrics, while the lack of the GEV loss had the most pronounced effect on the CRPS metric.", "section_name": "5.3 Ablation Study", "subsection_name": "", "paper_id": "5HQhYiGnYb", "paper_title": "FIDE: Frequency-Inflated Conditional Diffusion Model for Extreme-Aware Time Series Generation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:02:31.617615", "model": "gpt-5-mini", "original_claim": "Eliminating the conditional block maxima input significantly affected both the Jensen–Shannon divergence and the Kullback–Leibler divergence metrics.", "rephrasing_timestamp": "2026-01-11T19:26:22.927168", "rephrasing_model": "gpt-5-mini"}
{"claim": "Excluding the generalized extreme-value loss term caused the most substantial variation in the model's continuous ranked probability score, greater than the effect of removing any other component.", "label": "Supported", "paragraph": "Notably, the absence of the conditional block maxima input significantly impacted the Jenson-Shannon Divergence and KL Divergence metrics, while the lack of the GEV loss had the most pronounced effect on the CRPS metric.", "section_name": "5.3 Ablation Study", "subsection_name": "", "paper_id": "5HQhYiGnYb", "paper_title": "FIDE: Frequency-Inflated Conditional Diffusion Model for Extreme-Aware Time Series Generation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:02:31.617631", "model": "gpt-5-mini", "original_claim": "Removing the GEV loss produced the largest change in the Continuous Ranked Probability Score (CRPS) compared to other ablated components.", "rephrasing_timestamp": "2026-01-11T19:26:23.642128", "rephrasing_model": "gpt-5-mini"}
{"claim": "Turning off any single part caused only negligible decreases in prediction performance, indicating that the other parts either duplicate its role or make up for its absence.", "label": "Supported", "paragraph": "Surprisingly, the predictive score remained relatively resilient to the deactivation of any single component, suggesting a degree of redundancy or compensatory mechanisms among the remaining components.", "section_name": "5.3 Ablation Study", "subsection_name": "", "paper_id": "5HQhYiGnYb", "paper_title": "FIDE: Frequency-Inflated Conditional Diffusion Model for Extreme-Aware Time Series Generation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:02:36.793864", "model": "gpt-5-mini", "original_claim": "The predictive score remained largely robust after deactivating each individual component, implying redundancy or compensatory action among the remaining components.", "rephrasing_timestamp": "2026-01-11T19:26:28.577317", "rephrasing_model": "gpt-5-mini"}
{"claim": "We are grateful to Anton Tsitsulin, Dustin Zelle, Silvio Lattanzi, Vahab Mirrokni, and the Google Research graph mining team; their keen observations, careful edits, and practical suggestions played a major role in strengthening this paper.", "label": "Supported", "paragraph": "We express our sincere gratitude to Anton Tsitsulin, Dustin Zelle, Silvio Lattanzi, Vahab Mirrokni, and the entire graph mining team at Google Research, for their insightful comments, thorough proofreading, and constructive feedback which greatly enhanced the quality of our work. Furthermore, we extend our appreciation to the anonymous ICLR reviewers for their constructive suggestions. Their expertise and feedback played a crucial role in refining our paper.", "section_name": "7 ACKNOWLEDGEMENT", "subsection_name": "", "paper_id": "IuXR1CCrSi", "paper_title": "Talk like a Graph: Encoding Graphs for Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:03:14.266756", "model": "gpt-5-mini", "original_claim": "The authors acknowledged Anton Tsitsulin, Dustin Zelle, Silvio Lattanzi, Vahab Mirrokni, and the Google Research graph mining team for providing insightful comments, thorough proofreading, and constructive feedback that substantially improved the manuscript's quality.", "rephrasing_timestamp": "2026-01-11T19:26:30.179116", "rephrasing_model": "gpt-5-mini"}
{"claim": "The authors thank the unnamed reviewers for ICLR, whose knowledgeable comments and actionable recommendations substantially improved the manuscript.", "label": "Supported", "paragraph": "We express our sincere gratitude to Anton Tsitsulin, Dustin Zelle, Silvio Lattanzi, Vahab Mirrokni, and the entire graph mining team at Google Research, for their insightful comments, thorough proofreading, and constructive feedback which greatly enhanced the quality of our work. Furthermore, we extend our appreciation to the anonymous ICLR reviewers for their constructive suggestions. Their expertise and feedback played a crucial role in refining our paper.", "section_name": "7 ACKNOWLEDGEMENT", "subsection_name": "", "paper_id": "IuXR1CCrSi", "paper_title": "Talk like a Graph: Encoding Graphs for Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:03:14.266768", "model": "gpt-5-mini", "original_claim": "The authors also expressed appreciation to the anonymous ICLR reviewers for offering constructive suggestions whose expertise and feedback played a key role in refining the paper.", "rephrasing_timestamp": "2026-01-11T19:26:30.384641", "rephrasing_model": "gpt-5-mini"}
{"claim": "Let f denote the model’s mapping that takes discrete, high-dimensional symbols drawn from the symbol set V and yields outputs that are likewise elements of that same set.", "label": "Supported", "paragraph": "Notation. Let f be the interface function to a generative AI model, which takes high-dimensional discrete input tokens W and produces output in the same token space ( f : W ↦→ W ). Without loss of generality, we will colloquially refer to f as a pre-trained Large Language Model (LLM) throughout this work, but note that our discussion here applies to any generative AI model with such a discrete interface.", "section_name": "2 PROMPTING LLMS FOR GRAPH REASONING", "subsection_name": "", "paper_id": "IuXR1CCrSi", "paper_title": "Talk like a Graph: Encoding Graphs for Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:03:24.452230", "model": "gpt-5-mini", "original_claim": "The function f is defined as the model interface that accepts high-dimensional discrete input tokens from the set W and produces outputs that lie in the same token space W.", "rephrasing_timestamp": "2026-01-11T19:26:31.584720", "rephrasing_model": "gpt-5-mini"}
{"claim": "For convenience the paper will loosely describe f as a model trained in advance to generate text, while explicitly noting that the arguments extend to any content-generating AI that operates through an interface of separate token symbols.", "label": "Supported", "paragraph": "Notation. Let f be the interface function to a generative AI model, which takes high-dimensional discrete input tokens W and produces output in the same token space ( f : W ↦→ W ). Without loss of generality, we will colloquially refer to f as a pre-trained Large Language Model (LLM) throughout this work, but note that our discussion here applies to any generative AI model with such a discrete interface.", "section_name": "2 PROMPTING LLMS FOR GRAPH REASONING", "subsection_name": "", "paper_id": "IuXR1CCrSi", "paper_title": "Talk like a Graph: Encoding Graphs for Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:03:24.452245", "model": "gpt-5-mini", "original_claim": "The paper will informally refer to f as a pre-trained Large Language Model while explicitly stating that the discussion applies to any generative AI model that exposes a discrete-token interface.", "rephrasing_timestamp": "2026-01-11T19:26:35.181511", "rephrasing_model": "gpt-5-mini"}
{"claim": "The study models a network as an ordered pair made up of a set of nodes and a set of node-pairs taken from that set, where those pairs indicate which nodes are linked.", "label": "Supported", "paragraph": "In this work, we consider encoding graphs G = ( V, E ) , where V is the set of vertices (or nodes) and E ∈ ( V × V ) is the set of edges connecting them.", "section_name": "2 PROMPTING LLMS FOR GRAPH REASONING", "subsection_name": "", "paper_id": "IuXR1CCrSi", "paper_title": "Talk like a Graph: Encoding Graphs for Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:03:30.765753", "model": "gpt-5-mini", "original_claim": "The work represents graphs as G = (V, E), with V denoting the graph's vertices and E representing a collection of vertex-pair elements drawn from V×V that define the connections.", "rephrasing_timestamp": "2026-01-11T19:26:38.614671", "rephrasing_model": "gpt-5-mini"}
{"claim": "This research aims to uncover the algorithmic foundations of how communicative systems arise and change in people and machine-based systems, focusing on how meaning and context-sensitive interpretation affect one another.", "label": "Supported", "paragraph": "Our work aims to better understand the computational principles that underlie language evolution, in humans and artificial agents, with focus on the interface between semantics and pragmatics. While we presented an important first step towards this goal, we were only able to evaluate our model on English data and focused on the lexicon only.", "section_name": "7 Limitations", "subsection_name": "", "paper_id": "2wlNnIqCb7", "paper_title": "Bridging semantics and pragmatics in information-theoretic emergent communication", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:03:44.271596", "model": "gpt-5-mini", "original_claim": "The study intends to investigate computational principles of language evolution in humans and artificial agents, emphasizing how semantics and pragmatics interact.", "rephrasing_timestamp": "2026-01-11T19:26:38.275479", "rephrasing_model": "gpt-5-mini"}
{"claim": "The authors conducted experiments using only English-language corpora and confined their examination to word-level properties of language.", "label": "Supported", "paragraph": "Our work aims to better understand the computational principles that underlie language evolution, in humans and artificial agents, with focus on the interface between semantics and pragmatics. While we presented an important first step towards this goal, we were only able to evaluate our model on English data and focused on the lexicon only.", "section_name": "7 Limitations", "subsection_name": "", "paper_id": "2wlNnIqCb7", "paper_title": "Bridging semantics and pragmatics in information-theoretic emergent communication", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:03:44.271608", "model": "gpt-5-mini", "original_claim": "The researchers evaluated their model exclusively on English datasets and restricted their analysis to the lexical component of language.", "rephrasing_timestamp": "2026-01-11T19:26:41.016943", "rephrasing_model": "gpt-5-mini"}
{"claim": "The study advises that future work should examine the approach across a much broader, more typologically varied sample of languages.", "label": "Supported", "paragraph": "An important direction for future work is the evaluation of our model on a larger, more diverse set of languages. In addition, our work has focused only on the use of lexical items in communication. Therefore, another important direction for future research is to extend our framework to more complex communication structures, such as syntax, morphology, and compositional meaning.", "section_name": "7 Limitations", "subsection_name": "", "paper_id": "2wlNnIqCb7", "paper_title": "Bridging semantics and pragmatics in information-theoretic emergent communication", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:03:53.605147", "model": "gpt-5-mini", "original_claim": "The paper recommends evaluating the model using a substantially larger and more linguistically diverse set of languages in subsequent research.", "rephrasing_timestamp": "2026-01-11T19:26:38.324135", "rephrasing_model": "gpt-5-mini"}
{"claim": "The study examined only word-level exchanges and recommends broadening the system to cover sentence grammar, analysis of word formation, and the principles by which meanings are combined.", "label": "Supported", "paragraph": "An important direction for future work is the evaluation of our model on a larger, more diverse set of languages. In addition, our work has focused only on the use of lexical items in communication. Therefore, another important direction for future research is to extend our framework to more complex communication structures, such as syntax, morphology, and compositional meaning.", "section_name": "7 Limitations", "subsection_name": "", "paper_id": "2wlNnIqCb7", "paper_title": "Bridging semantics and pragmatics in information-theoretic emergent communication", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:03:53.605163", "model": "gpt-5-mini", "original_claim": "The research focused solely on using lexical elements for communication and proposes extending the framework to incorporate syntactic structures, morphological analysis, and compositional semantics.", "rephrasing_timestamp": "2026-01-11T19:26:41.664302", "rephrasing_model": "gpt-5-mini"}
{"claim": "The authors derive image representations from a neural network that was previously trained on labeled object categories to recognize items, which could unintentionally give the agents indirect access to language-derived cues.", "label": "Supported", "paragraph": "One potential concern about our model is that it employs a pre-trained object classification model [35] to extract visual features. This pre-trained model was trained with classification labels. Thus, one might worry that our agents were implicitly exposed to some linguistic knowledge.", "section_name": "7 Limitations", "subsection_name": "", "paper_id": "2wlNnIqCb7", "paper_title": "Bridging semantics and pragmatics in information-theoretic emergent communication", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:03:59.664086", "model": "gpt-5-mini", "original_claim": "The authors' model extracts visual features using a pre-trained object classification model that itself was trained with classification labels, potentially providing the agents with implicit access to linguistic information.", "rephrasing_timestamp": "2026-01-11T19:26:47.350867", "rephrasing_model": "gpt-5-mini"}
{"claim": "While the model is optimized, only the recently introduced feed-forward blocks and the trainable cross-attention projection matrices labeled K_id and V_id are adjusted by the selected loss.", "label": "Supported", "paragraph": "The full learning objective is defined as:\n<!-- formula-not-decoded -->\nDuring training, only the newly introduced MLPs and the learnable linear layers K id and V id in cross-attention layers are optimized with this objective, with the rest remaining frozen.", "section_name": "3.6 Full Objective", "subsection_name": "", "paper_id": "E6ZodZu0HQ", "paper_title": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:04:24.255665", "model": "gpt-5-mini", "original_claim": "During model training, only the newly added MLP modules together with the cross-attention learnable linear layers K_id and V_id are updated by the specified learning objective.", "rephrasing_timestamp": "2026-01-11T19:26:45.848586", "rephrasing_model": "gpt-5-mini"}
{"claim": "Every other parameter in the network is held constant and excluded from training; only the recently added feed‑forward modules and the linear key and value projection layers used for cross‑attention (the K_id and V_id components) receive the optimization signal.", "label": "Supported", "paragraph": "The full learning objective is defined as:\n<!-- formula-not-decoded -->\nDuring training, only the newly introduced MLPs and the learnable linear layers K id and V id in cross-attention layers are optimized with this objective, with the rest remaining frozen.", "section_name": "3.6 Full Objective", "subsection_name": "", "paper_id": "E6ZodZu0HQ", "paper_title": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:04:24.255677", "model": "gpt-5-mini", "original_claim": "All other model parameters remain fixed and are not trained while the learning objective is applied exclusively to the newly introduced MLPs and the K_id and V_id cross-attention linear layers.", "rephrasing_timestamp": "2026-01-11T19:26:48.693330", "rephrasing_model": "gpt-5-mini"}
{"claim": "Techniques that personalize identity without changing the model's weights tend to produce large changes in image aspects unrelated to the person—for instance the scene’s backdrop, lighting, framing, and overall aesthetic—when compared with the images the model generated before a representation of that person was supplied.", "label": "Supported", "paragraph": "Currently, tuning-free ID customization methods generally face a challenge: the embedding of the ID disrupts the behavior of the original model. The disruption manifests in two ways: firstly, the ID-irrelevant elements in the generated image (e.g., background, lighting, composition, and style) have changed extensively compared to before the ID insertion; secondly, there is a loss of prompt adherence, implying we can hardly edit the ID attributes, orientations, and accessories with the prompt.", "section_name": "3.3 Discussion on Common Diffusion Training in ID Customization", "subsection_name": "", "paper_id": "E6ZodZu0HQ", "paper_title": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:04:34.983938", "model": "gpt-5-mini", "original_claim": "Tuning-free identity customization methods cause substantial alteration of image elements unrelated to identity—such as background, lighting, composition, and visual style—compared to the model's outputs before inserting the identity embedding.", "rephrasing_timestamp": "2026-01-11T19:26:46.780596", "rephrasing_model": "gpt-5-mini"}
{"claim": "Adding a person's likeness to the model without adjusting its internal parameters makes the system less receptive to user text, so using written commands to consistently alter that person's distinguishing features, pose or viewing angle, or worn/held items becomes unreliable.", "label": "Supported", "paragraph": "Currently, tuning-free ID customization methods generally face a challenge: the embedding of the ID disrupts the behavior of the original model. The disruption manifests in two ways: firstly, the ID-irrelevant elements in the generated image (e.g., background, lighting, composition, and style) have changed extensively compared to before the ID insertion; secondly, there is a loss of prompt adherence, implying we can hardly edit the ID attributes, orientations, and accessories with the prompt.", "section_name": "3.3 Discussion on Common Diffusion Training in ID Customization", "subsection_name": "", "paper_id": "E6ZodZu0HQ", "paper_title": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:04:34.983957", "model": "gpt-5-mini", "original_claim": "Embedding an identity via tuning-free customization reduces the model's responsiveness to prompts, making it difficult to reliably change identity-specific attributes, orientations, and accessories through textual instructions.", "rephrasing_timestamp": "2026-01-11T19:26:52.107931", "rephrasing_model": "gpt-5-mini"}
{"claim": "Models that most accurately preserve a subject's likeness tend to suffer greater unintended changes when adapted with the usual subject-specific personalization routine for diffusion models.", "label": "Supported", "paragraph": "Typically, models with higher ID fidelity suffer more severe disruptions. Before we present our solutions, we first analyze why conventional diffusion training would cause this issue. In conventional ID Customization diffusion training process, as formulated in Eq.", "section_name": "3.3 Discussion on Common Diffusion Training in ID Customization", "subsection_name": "", "paper_id": "E6ZodZu0HQ", "paper_title": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:04:40.844903", "model": "gpt-5-mini", "original_claim": "Models achieving higher identity fidelity experience more pronounced disruptions when trained with the standard ID Customization diffusion training process.", "rephrasing_timestamp": "2026-01-11T19:26:54.073820", "rephrasing_model": "gpt-5-mini"}
{"claim": "Since the problem is defined over finite sets of situations and possible choices, decision rules can be determined exactly from the observed data and portrayed in Figure 2 as two-valued matrices that fully capture their behavior.", "label": "Supported", "paragraph": "Since the context and action spaces are discrete in this domain, prescriptors can be analytically distilled based on the dataset describing their full behavior (i.e., the binary grids in Fig. 2). For example, these prescriptors can be distilled into rule-based or neural network-based prescriptors.", "section_name": "B.2 Analytic Distillation", "subsection_name": "", "paper_id": "hw76X5uWrc", "paper_title": "Unlocking the Potential of Global Human Expertise", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:05:01.830619", "model": "gpt-5-mini", "original_claim": "Because the domain uses discrete context and action spaces, prescriptors can be analytically derived from the dataset that captures their entire behavior as binary grids shown in Figure 2.", "rephrasing_timestamp": "2026-01-11T19:26:55.932917", "rephrasing_model": "gpt-5-mini"}
{"claim": "Prescriptive models derived from the dataset can be realized either as explicit, rules-driven engines or as predictive systems learned through deep neural architectures.", "label": "Supported", "paragraph": "Since the context and action spaces are discrete in this domain, prescriptors can be analytically distilled based on the dataset describing their full behavior (i.e., the binary grids in Fig. 2). For example, these prescriptors can be distilled into rule-based or neural network-based prescriptors.", "section_name": "B.2 Analytic Distillation", "subsection_name": "", "paper_id": "hw76X5uWrc", "paper_title": "Unlocking the Potential of Global Human Expertise", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:05:01.830630", "model": "gpt-5-mini", "original_claim": "Prescriptors extracted from that dataset can be represented either as rule-based systems or as models implemented using neural networks.", "rephrasing_timestamp": "2026-01-11T19:26:58.584092", "rephrasing_model": "gpt-5-mini"}
{"claim": "The prescribing mechanism π consists of r condition–action mappings, each matching a chosen collection S_i of observed context features to a chosen collection T_i of possible interventions.", "label": "Supported", "paragraph": "Consider rule-based prescriptors of the form: π = [ C 1 ↦→ A 1 , . . . , C r ↦→ A r ] , where C i ⊆ { c 1 , . . . , c m } and A i ⊆ { a 1 , . . . , a n } are subsets of the possible contexts and policy interventions, respectively.", "section_name": "B.2 Analytic Distillation", "subsection_name": "", "paper_id": "hw76X5uWrc", "paper_title": "Unlocking the Potential of Global Human Expertise", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:05:22.719250", "model": "gpt-5-mini", "original_claim": "A rule-based prescriptor π is composed of r rules, each associating a subset Ci of context elements with a subset Ai of policy interventions.", "rephrasing_timestamp": "2026-01-11T19:26:59.375828", "rephrasing_model": "gpt-5-mini"}
{"claim": "For every rule index i, Ci consists only of elements drawn from the collection {c1, ..., cm} and Ai consists only of elements drawn from the collection {a1, ..., an}; together they specify which situations and responses are permitted.", "label": "Supported", "paragraph": "Consider rule-based prescriptors of the form: π = [ C 1 ↦→ A 1 , . . . , C r ↦→ A r ] , where C i ⊆ { c 1 , . . . , c m } and A i ⊆ { a 1 , . . . , a n } are subsets of the possible contexts and policy interventions, respectively.", "section_name": "B.2 Analytic Distillation", "subsection_name": "", "paper_id": "hw76X5uWrc", "paper_title": "Unlocking the Potential of Global Human Expertise", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:05:22.719265", "model": "gpt-5-mini", "original_claim": "For each rule index i, Ci is contained within the context set {c1, ..., cm} and Ai is contained within the intervention set {a1, ..., an}, indicating allowed contexts and actions.", "rephrasing_timestamp": "2026-01-11T19:27:02.803377", "rephrasing_model": "gpt-5-mini"}
{"claim": "A prescriber made up of r rules (r ≥ 0) arranged in order, when presented with a context c, returns the action a_i with the smallest index i whose guard G_i is satisfied by c; if no guard is satisfied, it returns the null action a_0 (∅).", "label": "Supported", "paragraph": "These prescriptors have a variable number of rules r ≥ 0 . Given a context c , π ( c ) prescribes the first action A i such that c ∈ C i , and prescribes the empty action A o = ∅ if no C i contains c .", "section_name": "B.2 Analytic Distillation", "subsection_name": "", "paper_id": "hw76X5uWrc", "paper_title": "Unlocking the Potential of Global Human Expertise", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:05:30.827291", "model": "gpt-5-mini", "original_claim": "A prescriptor with r ≥ 0 rules selects for a given context c the earliest action A_i whose condition set C_i includes c; if none include c, it selects the empty action A_o (∅).", "rephrasing_timestamp": "2026-01-11T19:27:04.994313", "rephrasing_model": "gpt-5-mini"}
{"claim": "The paper produces realistic person-focused video sequences by combining a previously trained image-animation engine with a newly proposed diffusion-based method that models motion in a compact learned latent space.", "label": "Supported", "paragraph": "In this work, we aim to synthesize high-quality human-centric videos by combining a pretrained image animator with a proposed latent motion diffusion model. Our approach can be used for digital human, online education, and data synthesis for other computer vision tasks, etc.", "section_name": "ETHIC STATEMENT", "subsection_name": "", "paper_id": "9zHxXaYEgw", "paper_title": "LEO: Generative Latent Image Animator for Human Video Synthesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:05:48.207682", "model": "gpt-5-mini", "original_claim": "The paper synthesizes high-quality human-centered videos by integrating an existing pretrained image animator with a newly introduced latent motion diffusion model.", "rephrasing_timestamp": "2026-01-11T19:27:04.831367", "rephrasing_model": "gpt-5-mini"}
{"claim": "This approach is meant for uses such as creating lifelike virtual characters, producing materials for remote learning, and assembling artificial datasets to develop or evaluate visual recognition systems.", "label": "Supported", "paragraph": "In this work, we aim to synthesize high-quality human-centric videos by combining a pretrained image animator with a proposed latent motion diffusion model. Our approach can be used for digital human, online education, and data synthesis for other computer vision tasks, etc.", "section_name": "ETHIC STATEMENT", "subsection_name": "", "paper_id": "9zHxXaYEgw", "paper_title": "LEO: Generative Latent Image Animator for Human Video Synthesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:05:48.207693", "model": "gpt-5-mini", "original_claim": "The proposed method is intended for applications including digital humans, online education content creation, and generating synthetic datasets for other computer vision tasks.", "rephrasing_timestamp": "2026-01-11T19:27:04.014411", "rephrasing_model": "gpt-5-mini"}
{"claim": "Instead of trying to learn or reproduce how things look, the method concentrates on embedding the range of movement patterns into an image-animation network that was trained previously.", "label": "Supported", "paragraph": "We note that our framework mainly focuses on learning how to model motion distribution in a pretrained image animator rather than directly model appearance. Therefore, our framework is not biased towards any specific gender, race, region, or social class.", "section_name": "ETHIC STATEMENT", "subsection_name": "", "paper_id": "9zHxXaYEgw", "paper_title": "LEO: Generative Latent Image Animator for Human Video Synthesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:05:55.041174", "model": "gpt-5-mini", "original_claim": "The framework's primary objective is to learn to represent motion distributions within a pretrained image animator, rather than to directly capture or model visual appearance.", "rephrasing_timestamp": "2026-01-11T19:27:07.819192", "rephrasing_model": "gpt-5-mini"}
{"claim": "As a result, the system's designers assert that it neither favors nor disadvantages individuals based on sex, ethnicity, place of origin, or socioeconomic background.", "label": "Supported", "paragraph": "We note that our framework mainly focuses on learning how to model motion distribution in a pretrained image animator rather than directly model appearance. Therefore, our framework is not biased towards any specific gender, race, region, or social class.", "section_name": "ETHIC STATEMENT", "subsection_name": "", "paper_id": "9zHxXaYEgw", "paper_title": "LEO: Generative Latent Image Animator for Human Video Synthesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:05:55.041193", "model": "gpt-5-mini", "original_claim": "Consequently, the framework is claimed to be free of biases associated with any particular gender, race, geographic region, or social class.", "rephrasing_timestamp": "2026-01-11T19:27:12.954676", "rephrasing_model": "gpt-5-mini"}
{"claim": "Because the method's generator works in the image plane to create motion estimates, LEO struggles with situations where limbs or the torso block one another, showing a marked drop in performance on the Taichi benchmark.", "label": "Supported", "paragraph": "We list several limitations in current framework and proposed potential solutions for future work. - Geometry ambiguity and temporal coherency. Since we use a 2D generator to predict 2D flow maps, LEO is not able to handle human body occlusion very well especially in Taichi dataset.", "section_name": "A.4 LIMITATIONS AND FUTURE WORK", "subsection_name": "", "paper_id": "9zHxXaYEgw", "paper_title": "LEO: Generative Latent Image Animator for Human Video Synthesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:06:01.220651", "model": "gpt-5-mini", "original_claim": "Because the approach employs a two-dimensional generator to produce 2D flow maps, LEO struggles to cope with human body occlusions, with performance notably worse on the Taichi dataset.", "rephrasing_timestamp": "2026-01-11T19:27:14.227503", "rephrasing_model": "gpt-5-mini"}
{"claim": "The study tested whether the ELM neural unit can model intricate, distant dependencies by evaluating it on the Long Range Arena benchmark suite compiled by Tay et al. (2021).", "label": "Supported", "paragraph": "To test the extent and limits of the ELM neuron's ability to extract complex long-range dependencies, we use the classic Long Range Arena (LRA) benchmark datasets Tay et al. (2021). It consists of classification tasks; three image-derived datasets Image, Pathfinder, and Pathfinder-X (images being converted to a grayscale pixel sequence), and three text-based datasets ListOps, Text, and Retrieval.", "section_name": "4.3 EVALUATING ON COMPLEX AND VERY LONG TEMPORAL DEPENDENCY TASKS", "subsection_name": "", "paper_id": "vE1e1mLJ0U", "paper_title": "The Expressive Leaky Memory Neuron: an Efficient and Expressive Phenomenological Neuron Model Can Solve Long-Horizon Tasks.", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:06:25.881395", "model": "gpt-5-mini", "original_claim": "The study evaluated the ELM neuron's capability to capture complex long-range dependencies by applying it to the Long Range Arena benchmark datasets introduced by Tay et al., 2021.", "rephrasing_timestamp": "2026-01-11T19:27:15.757001", "rephrasing_model": "gpt-5-mini"}
{"claim": "The evaluation suite contains six classification challenges: three vision-derived sets — Image, Pathfinder, and Pathfinder‑X — where each image is converted into a sequence of gray‑level pixel values, and three language-oriented sets — ListOps, Text, and Retrieval.", "label": "Supported", "paragraph": "To test the extent and limits of the ELM neuron's ability to extract complex long-range dependencies, we use the classic Long Range Arena (LRA) benchmark datasets Tay et al. (2021). It consists of classification tasks; three image-derived datasets Image, Pathfinder, and Pathfinder-X (images being converted to a grayscale pixel sequence), and three text-based datasets ListOps, Text, and Retrieval.", "section_name": "4.3 EVALUATING ON COMPLEX AND VERY LONG TEMPORAL DEPENDENCY TASKS", "subsection_name": "", "paper_id": "vE1e1mLJ0U", "paper_title": "The Expressive Leaky Memory Neuron: an Efficient and Expressive Phenomenological Neuron Model Can Solve Long-Horizon Tasks.", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:06:25.881408", "model": "gpt-5-mini", "original_claim": "The Long Range Arena benchmark used comprises six classification tasks: three image-derived datasets (Image, Pathfinder, Pathfinder-X) with images flattened to grayscale pixel sequences, and three text datasets (ListOps, Text, Retrieval).", "rephrasing_timestamp": "2026-01-11T19:27:12.524274", "rephrasing_model": "gpt-5-mini"}
{"claim": "Image elements and symbolic tokens were mapped to discrete categories, with each monochrome pixel's brightness confined to eight or sixteen possible values.", "label": "Supported", "paragraph": "Pixel and token sequences were encoded categorically, however, only considering 8 or 16 different grayscale levels for images. In particular, the Pathfinder-X task is notoriously difficult, as the task is to determine whether two dots are connected by a path in a 128 × 128 image (~16k length).", "section_name": "4.3 EVALUATING ON COMPLEX AND VERY LONG TEMPORAL DEPENDENCY TASKS", "subsection_name": "", "paper_id": "vE1e1mLJ0U", "paper_title": "The Expressive Leaky Memory Neuron: an Efficient and Expressive Phenomenological Neuron Model Can Solve Long-Horizon Tasks.", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:06:39.046163", "model": "gpt-5-mini", "original_claim": "Pixel and token data were encoded categorically, with image grayscale values limited to only eight or sixteen distinct intensity levels.", "rephrasing_timestamp": "2026-01-11T19:27:15.631699", "rephrasing_model": "gpt-5-mini"}
{"claim": "In the Pathfinder-X problem, the objective is to decide if a continuous stroke links two marked points inside an image made up of a 128-row by 128-column pixel grid — an input that consists of roughly 16,384 individual pixels (about 16k).", "label": "Supported", "paragraph": "Pixel and token sequences were encoded categorically, however, only considering 8 or 16 different grayscale levels for images. In particular, the Pathfinder-X task is notoriously difficult, as the task is to determine whether two dots are connected by a path in a 128 × 128 image (~16k length).", "section_name": "4.3 EVALUATING ON COMPLEX AND VERY LONG TEMPORAL DEPENDENCY TASKS", "subsection_name": "", "paper_id": "vE1e1mLJ0U", "paper_title": "The Expressive Leaky Memory Neuron: an Efficient and Expressive Phenomenological Neuron Model Can Solve Long-Horizon Tasks.", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:06:39.046179", "model": "gpt-5-mini", "original_claim": "The Pathfinder-X task requires determining whether two dots are connected by a path within a 128×128 pixel image, corresponding to roughly 16,384 elements (~16k length).", "rephrasing_timestamp": "2026-01-11T19:27:24.164909", "rephrasing_model": "gpt-5-mini"}
{"claim": "In Table 1 the authors single out the Chrono variant of LSTM — an LSTM architecture that uses a theoretically derived gate-bias initialization to extend its internal memory time constants (Tallec & Ollivier, 2018) — as the closest baseline to the ELM unit.", "label": "Supported", "paragraph": "Our results are summarized in Table 1, where we compare the ELM neuron against several strong baselines. The model most comparable to ours is an LSTM with derived explicit gating bias initialization for effectively longer internal timescales Tallec &amp; Ollivier (2018) (Chrono-LSTM).", "section_name": "4.3 EVALUATING ON COMPLEX AND VERY LONG TEMPORAL DEPENDENCY TASKS", "subsection_name": "", "paper_id": "vE1e1mLJ0U", "paper_title": "The Expressive Leaky Memory Neuron: an Efficient and Expressive Phenomenological Neuron Model Can Solve Long-Horizon Tasks.", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:06:48.019241", "model": "gpt-5-mini", "original_claim": "In Table 1, the authors identify the Chrono-LSTM—an LSTM employing derived explicit gating bias initialization to produce longer internal timescales (Tallec & Ollivier, 2018)—as the baseline most comparable to the ELM neuron.", "rephrasing_timestamp": "2026-01-11T19:27:24.845408", "rephrasing_model": "gpt-5-mini"}
{"claim": "A node-to-node information-exchange scheme, derived from the technique presented in [21], was embedded into the model during its development.", "label": "Supported", "paragraph": "In designing of our architecture, we design a message passing scheme similar to [21] and add Schur layer to each of the convolution layers. A visualization of our architecture can be found in Figure 2.", "section_name": "I Architecture and experiment details", "subsection_name": "", "paper_id": "HRnSVflpgt", "paper_title": "Schur Nets: exploiting local structure for equivariance in higher order graph neural networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:07:07.536473", "model": "gpt-5-mini", "original_claim": "The proposed network uses a message-passing mechanism adapted from the method described in reference [21], incorporated into the architecture during its design.", "rephrasing_timestamp": "2026-01-11T19:27:23.781410", "rephrasing_model": "gpt-5-mini"}
{"claim": "A Schur-processing stage is appended to every spatial-filtering stage in the network, guaranteeing the Schur transform is performed after each such operation.", "label": "Supported", "paragraph": "In designing of our architecture, we design a message passing scheme similar to [21] and add Schur layer to each of the convolution layers. A visualization of our architecture can be found in Figure 2.", "section_name": "I Architecture and experiment details", "subsection_name": "", "paper_id": "HRnSVflpgt", "paper_title": "Schur Nets: exploiting local structure for equivariance in higher order graph neural networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:07:07.536493", "model": "gpt-5-mini", "original_claim": "Every convolutional layer in the design includes an added Schur layer, so a Schur operation is attached to each convolutional component of the architecture.", "rephrasing_timestamp": "2026-01-11T19:27:26.411217", "rephrasing_model": "gpt-5-mini"}
{"claim": "For an input graph with vertex set V and edge set E, each convolutional stage keeps two tensors: a zeroth-level vertex embedding of size (number of vertices × embedding dimension) and a connection embedding of size (number of edges × embedding dimension).", "label": "Supported", "paragraph": "In each convolution layer, we maintain 0th-order node representation ( h v ∈ R | V |∗ dim ) and edge representation ( h e ∈ R | E |∗ dim ) where we assume the input graph G = ( V, E ) . We further maintain 1st-order representation on cycles ( h D k ∈ R ( k ∗ ( c k ) ∗ dim ) ) where D k is cycle of length k and c k is the number of such cycle in G .", "section_name": "I Architecture and experiment details", "subsection_name": "", "paper_id": "HRnSVflpgt", "paper_title": "Schur Nets: exploiting local structure for equivariance in higher order graph neural networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:07:19.176733", "model": "gpt-5-mini", "original_claim": "In every convolutional layer the model stores a 0th-order node representation of size |V|×dim and an edge representation of size |E|×dim, assuming the input graph G=(V,E).", "rephrasing_timestamp": "2026-01-11T19:27:27.222634", "rephrasing_model": "gpt-5-mini"}
{"claim": "When considering k-node loops, the convolutional stage preserves a first-order feature encoding for those loops organized in a tensor of shape k × c_k × dim, where c_k is the number of such k-node loops present in the supplied graph.", "label": "Supported", "paragraph": "In each convolution layer, we maintain 0th-order node representation ( h v ∈ R | V |∗ dim ) and edge representation ( h e ∈ R | E |∗ dim ) where we assume the input graph G = ( V, E ) . We further maintain 1st-order representation on cycles ( h D k ∈ R ( k ∗ ( c k ) ∗ dim ) ) where D k is cycle of length k and c k is the number of such cycle in G .", "section_name": "I Architecture and experiment details", "subsection_name": "", "paper_id": "HRnSVflpgt", "paper_title": "Schur Nets: exploiting local structure for equivariance in higher order graph neural networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:07:19.176749", "model": "gpt-5-mini", "original_claim": "For cycles of length k the convolution layer maintains a first-order cycle representation with dimensionality k×c_k×dim, where c_k denotes the number of length-k cycles in the input graph.", "rephrasing_timestamp": "2026-01-11T19:27:32.033992", "rephrasing_model": "gpt-5-mini"}
{"claim": "Pairwise-link and cyclic-substructure embeddings share information using the mapping scheme described in [21]. The inputs that reach the cyclic embeddings, along with those embeddings' initial states, are subsequently processed by a layer that applies elementwise (Hadamard/Schur) multiplication.", "label": "Supported", "paragraph": "We did message passing between node and edge representation the same as CIN [6]. The edge and cycle pass message with each other by tranfer maps described by [21] and then the incoming message to cycles as well as the original cycle representation is transformed by Schur layer.", "section_name": "I Architecture and experiment details", "subsection_name": "", "paper_id": "HRnSVflpgt", "paper_title": "Schur Nets: exploiting local structure for equivariance in higher order graph neural networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:07:25.011227", "model": "gpt-5-mini", "original_claim": "Edge and cycle representations exchange messages using the transfer-map method of reference [21], and both incoming messages to cycles and the original cycle representations are then transformed by a Schur layer.", "rephrasing_timestamp": "2026-01-11T19:27:36.200613", "rephrasing_model": "gpt-5-mini"}
{"claim": "In the ACDC collection, the average absolute difference between every pair of scaled pixel values from the training slices, taken over the entire dataset, is 0.217.", "label": "Supported", "paragraph": "While there may be several task-relevant groupings in 3D medical imaging, it is not immediately apparent which of these would be useful for feature representation for metric learning for Coreset. For the ACDC dataset, we note the mean pairwise absolute deviation of the normalized training slice pixel values within different groups averaged over the dataset: 0.217 over the entire dataset, 0.159 within patient groups, 0.166 within volume groups, and 0.115 within adjacent slice groups.", "section_name": "3.2.1 Group-based Contrastive Learning for feature representation in metric learning", "subsection_name": "", "paper_id": "uyqjpycMbU", "paper_title": "Integrating Deep Metric Learning with Coreset for Active Learning in 3D Segmentation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:07:49.974197", "model": "gpt-5-mini", "original_claim": "For the ACDC dataset, the mean pairwise absolute deviation of normalized training-slice pixel intensities computed across the entire dataset equals 0.217.", "rephrasing_timestamp": "2026-01-11T19:27:32.772263", "rephrasing_model": "gpt-5-mini"}
{"claim": "Across neighboring collections of training slices in the ACDC dataset, the mean absolute difference computed over all pairs of scaled pixel intensities was 0.115.", "label": "Supported", "paragraph": "While there may be several task-relevant groupings in 3D medical imaging, it is not immediately apparent which of these would be useful for feature representation for metric learning for Coreset. For the ACDC dataset, we note the mean pairwise absolute deviation of the normalized training slice pixel values within different groups averaged over the dataset: 0.217 over the entire dataset, 0.159 within patient groups, 0.166 within volume groups, and 0.115 within adjacent slice groups.", "section_name": "3.2.1 Group-based Contrastive Learning for feature representation in metric learning", "subsection_name": "", "paper_id": "uyqjpycMbU", "paper_title": "Integrating Deep Metric Learning with Coreset for Active Learning in 3D Segmentation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:07:49.974209", "model": "gpt-5-mini", "original_claim": "Within adjacent slice groups in the ACDC dataset, the average pairwise absolute deviation of normalized training slice pixel values was 0.115.", "rephrasing_timestamp": "2026-01-11T19:27:38.340449", "rephrasing_model": "gpt-5-mini"}
{"claim": "Among the group types examined, the volume-based classifications exhibited the greatest heterogeneity, exceeding the variation observed in the other classifications.", "label": "Supported", "paragraph": "Note that the volume groups and the adjacent slice groups are the most and least diverse respectively. While intuitively the most diverse group would have the most important features for diversity, this does not necessarily indicate what combinations of groups would be helpful as well.", "section_name": "3.2.1 Group-based Contrastive Learning for feature representation in metric learning", "subsection_name": "", "paper_id": "uyqjpycMbU", "paper_title": "Integrating Deep Metric Learning with Coreset for Active Learning in 3D Segmentation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:07:58.716712", "model": "gpt-5-mini", "original_claim": "Within the analyzed group types, the volume groups demonstrated the highest diversity among all groups examined, exceeding the diversity measured in other group categories.", "rephrasing_timestamp": "2026-01-11T19:27:32.092996", "rephrasing_model": "gpt-5-mini"}
{"claim": "Of all types examined, the contiguous-section collections had the least variation, implying they were the most uniform compared with the other categories.", "label": "Supported", "paragraph": "Note that the volume groups and the adjacent slice groups are the most and least diverse respectively. While intuitively the most diverse group would have the most important features for diversity, this does not necessarily indicate what combinations of groups would be helpful as well.", "section_name": "3.2.1 Group-based Contrastive Learning for feature representation in metric learning", "subsection_name": "", "paper_id": "uyqjpycMbU", "paper_title": "Integrating Deep Metric Learning with Coreset for Active Learning in 3D Segmentation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:07:58.716727", "model": "gpt-5-mini", "original_claim": "The adjacent slice groups showed the lowest diversity of all group types evaluated in the analysis, indicating they were the most homogeneous compared with other groups.", "rephrasing_timestamp": "2026-01-11T19:27:39.336584", "rephrasing_model": "gpt-5-mini"}
{"claim": "This loss function produces feature representations for each visual sample and a transformed counterpart, encouraging a sample’s representation to lie near its transformed copy while driving representations of different samples apart.", "label": "Supported", "paragraph": "Instead, we propose a general group contrastive loss based on NT-Xent loss (67). The NT-Xent loss focuses on generating and comparing embeddings for image pairs and their augmentations. It promotes similarity in embeddings for the same image and its augmentation while encouraging dissimilarity for different images.", "section_name": "3.2.1 Group-based Contrastive Learning for feature representation in metric learning", "subsection_name": "", "paper_id": "uyqjpycMbU", "paper_title": "Integrating Deep Metric Learning with Coreset for Active Learning in 3D Segmentation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:08:04.469791", "model": "gpt-5-mini", "original_claim": "The NT-Xent loss computes embeddings for image pairs and their transformed versions, increasing embedding similarity between an image and its augmentation while reducing similarity among embeddings of different images.", "rephrasing_timestamp": "2026-01-11T19:27:41.490273", "rephrasing_model": "gpt-5-mini"}
{"claim": "The researchers validated their novel model by benchmarking it against four reference techniques for expanding hypergraphs.", "label": "Supported", "paragraph": "We compare our model with four baseline hypergraph expansion methods, including clique expansion (CE) based methods (G1), star expansion (SE) based methods (G2), line expansion (LE) based methods (G3), and uni-based methods (G4).", "section_name": "D BASELINE SETTINGS", "subsection_name": "", "paper_id": "5M2MjyNR2w", "paper_title": "Adaptive Expansion for Hypergraph Learning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:08:27.529180", "model": "gpt-5-mini", "original_claim": "The study evaluated their proposed model by comparing it against four baseline hypergraph expansion approaches.", "rephrasing_timestamp": "2026-01-11T19:27:39.435907", "rephrasing_model": "gpt-5-mini"}
{"claim": "The baseline techniques comprised four transformations: mapping hyperedges to fully connected subgraphs (labeled G1), converting them into hub-and-spoke (star-like) graphs (labeled G2), adopting an edge-centric/line-graph representation (labeled G3), and using single-mode (uni-partite) methods (labeled G4).", "label": "Supported", "paragraph": "We compare our model with four baseline hypergraph expansion methods, including clique expansion (CE) based methods (G1), star expansion (SE) based methods (G2), line expansion (LE) based methods (G3), and uni-based methods (G4).", "section_name": "D BASELINE SETTINGS", "subsection_name": "", "paper_id": "5M2MjyNR2w", "paper_title": "Adaptive Expansion for Hypergraph Learning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:08:27.529198", "model": "gpt-5-mini", "original_claim": "The baseline approaches were clique-expansion (CE, denoted G1), star-expansion (SE, denoted G2), line-expansion (LE, denoted G3), and uni-based methods (G4).", "rephrasing_timestamp": "2026-01-11T19:27:45.454339", "rephrasing_model": "gpt-5-mini"}
{"claim": "To allow an unbiased comparison with earlier hypergraph-expansion techniques, the work relied on three different graph-based neural model families as its core architectures: a convolutional variant, an attention-driven variant, and a model tailored to capture graph isomorphism characteristics.", "label": "Supported", "paragraph": "To fairly compare with baseline hypergraph expansion methods, we leverage three GNN models, i.e., Graph Convolution Network (GCN) (Kipf &amp; Welling, 2017), Graph Attention Network (GAT) (Veliˇ ckovi´ c et al., 2018), and Graph Isomorphism Network (GIN) (Xu et al., 2019), as backbone models.", "section_name": "D BASELINE SETTINGS", "subsection_name": "", "paper_id": "5M2MjyNR2w", "paper_title": "Adaptive Expansion for Hypergraph Learning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:08:46.525038", "model": "gpt-5-mini", "original_claim": "To enable a fair comparison with baseline hypergraph expansion approaches, the study used three distinct graph neural network architectures—Graph Convolution Network, Graph Attention Network, and Graph Isomorphism Network—as primary backbone models.", "rephrasing_timestamp": "2026-01-11T19:27:47.015806", "rephrasing_model": "gpt-5-mini"}
{"claim": "The three primary graph neural architectures employed here implement previously published models: Kipf and Welling’s 2017 graph-convolution approach, Veličković et al.’s 2018 attention-based graph model, and Xu et al.’s 2019 architecture designed to capture graph isomorphism.", "label": "Supported", "paragraph": "To fairly compare with baseline hypergraph expansion methods, we leverage three GNN models, i.e., Graph Convolution Network (GCN) (Kipf &amp; Welling, 2017), Graph Attention Network (GAT) (Veliˇ ckovi´ c et al., 2018), and Graph Isomorphism Network (GIN) (Xu et al., 2019), as backbone models.", "section_name": "D BASELINE SETTINGS", "subsection_name": "", "paper_id": "5M2MjyNR2w", "paper_title": "Adaptive Expansion for Hypergraph Learning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:08:46.525053", "model": "gpt-5-mini", "original_claim": "The three backbone GNNs correspond to published methods: the GCN by Kipf and Welling (2017), the GAT by Veličković et al. (2018), and the GIN by Xu et al. (2019).", "rephrasing_timestamp": "2026-01-11T19:27:48.216377", "rephrasing_model": "gpt-5-mini"}
{"claim": "The researchers followed the method of Sun et al. (2008), transforming each higher-order relation into an all-to-all connected subnetwork by creating pairwise links among every element within that relation.", "label": "Supported", "paragraph": "Next, we would like to introduce these baseline methods in each group in detail. G1 CE-based methods: We implement the clique expansion method (Sun et al., 2008) that connects every pair of nodes within each hyperedge.", "section_name": "D BASELINE SETTINGS", "subsection_name": "", "paper_id": "5M2MjyNR2w", "paper_title": "Adaptive Expansion for Hypergraph Learning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:08:52.939504", "model": "gpt-5-mini", "original_claim": "The researchers implemented the clique-expansion technique (Sun et al., 2008), converting each hyperedge into a complete graph by adding edges between every pair of nodes contained in that hyperedge.", "rephrasing_timestamp": "2026-01-11T19:27:48.317253", "rephrasing_model": "gpt-5-mini"}
{"claim": "Often, the approach is to produce a compact model by training it to mimic the behavior of a much larger counterpart, so the smaller model reaches nearly the same prediction accuracy.", "label": "Supported", "paragraph": "As is common in Knowledge Distillation literature, this field often involves model compression where a teacher model is used to produce a student model with comparable performance. In line with this paradigm, we train our model with the same hyperparameters as in the CIFAR-10 main results with less than a quarter of the model parameters.", "section_name": "A.3 KNOWLEDGE DISTILLATION", "subsection_name": "", "paper_id": "a24gfxA7jD", "paper_title": "Physics Informed Distillation for Diffusion Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:09:11.796454", "model": "gpt-5-mini", "original_claim": "Knowledge Distillation typically uses model compression by training a smaller student network from a larger teacher network to attain comparable predictive performance.", "rephrasing_timestamp": "2026-01-11T19:27:47.251307", "rephrasing_model": "gpt-5-mini"}
{"claim": "Using the identical training settings from their core experiments, they optimized a CIFAR-10 network whose architecture contained less than one quarter of the parameters of the initial design.", "label": "Supported", "paragraph": "As is common in Knowledge Distillation literature, this field often involves model compression where a teacher model is used to produce a student model with comparable performance. In line with this paradigm, we train our model with the same hyperparameters as in the CIFAR-10 main results with less than a quarter of the model parameters.", "section_name": "A.3 KNOWLEDGE DISTILLATION", "subsection_name": "", "paper_id": "a24gfxA7jD", "paper_title": "Physics Informed Distillation for Diffusion Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:09:11.796473", "model": "gpt-5-mini", "original_claim": "They trained a CIFAR-10 model using the same hyperparameters as their main experiments while employing a model with fewer than one-quarter of the original parameter count.", "rephrasing_timestamp": "2026-01-11T19:27:56.897972", "rephrasing_model": "gpt-5-mini"}
{"claim": "A compact student model was obtained from the larger counterparts by reducing the filter count in each layer to half of the original.", "label": "Supported", "paragraph": "The small student architecture was constructed by reducing all the channels in the teacher models by half. More details on the small student architecture are provided in Appendix A.1. From Table 5, we can observe that despite the student model's significantly smaller size, it is still able to generate decent images, achieving an FID score of 8.29.", "section_name": "A.3 KNOWLEDGE DISTILLATION", "subsection_name": "", "paper_id": "a24gfxA7jD", "paper_title": "Physics Informed Distillation for Diffusion Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:09:20.127866", "model": "gpt-5-mini", "original_claim": "The small student network was created by halving the number of channels from the teacher models across every layer of the architecture.", "rephrasing_timestamp": "2026-01-11T19:28:00.308906", "rephrasing_model": "gpt-5-mini"}
{"claim": "Despite having a much smaller footprint, the lightweight model synthesized convincing imagery and registered an FID of 8.29, as reported in Table 5.", "label": "Supported", "paragraph": "The small student architecture was constructed by reducing all the channels in the teacher models by half. More details on the small student architecture are provided in Appendix A.1. From Table 5, we can observe that despite the student model's significantly smaller size, it is still able to generate decent images, achieving an FID score of 8.29.", "section_name": "A.3 KNOWLEDGE DISTILLATION", "subsection_name": "", "paper_id": "a24gfxA7jD", "paper_title": "Physics Informed Distillation for Diffusion Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:09:20.127881", "model": "gpt-5-mini", "original_claim": "Although substantially smaller in size, the student model still produced reasonable images and achieved an FID score of 8.29 as shown in Table 5.", "rephrasing_timestamp": "2026-01-11T19:27:58.702373", "rephrasing_model": "gpt-5-mini"}
{"claim": "Despite being outperformed by larger counterparts, the finding indicates that methods which convey learned information from high-capacity networks to smaller ones show promise for shrinking model footprint and for addressing goals beyond merely reducing the number of function evaluations.", "label": "Supported", "paragraph": "Despite its drop in performance in contrast to the bigger student models, this result showcases the promising potential of Knowledge Distillation approaches in model compression and not just NFE reduction.", "section_name": "A.3 KNOWLEDGE DISTILLATION", "subsection_name": "", "paper_id": "a24gfxA7jD", "paper_title": "Physics Informed Distillation for Diffusion Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:09:26.014068", "model": "gpt-5-mini", "original_claim": "Although it attained lower performance compared with larger student models, the result demonstrates that Knowledge Distillation techniques hold promise for compressing models and serving objectives beyond merely lowering NFE.", "rephrasing_timestamp": "2026-01-11T19:28:03.178775", "rephrasing_model": "gpt-5-mini"}
{"claim": "The article provides exhaustive procedural and experimental details about its resilience testing of AI language systems refined to follow user instructions, enabling independent researchers to replicate the work.", "label": "Supported", "paragraph": "To ensure reproducibility, we provide all details regarding our evaluation of the robustness of instruction-tuned LLMs.", "section_name": "B EXPERIMENTS", "subsection_name": "", "paper_id": "g9diuvxN6D", "paper_title": "Evaluating the Zero-shot Robustness of Instruction-tuned Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T16:10:21.432746", "model": "gpt-5-mini", "original_claim": "The paper supplies comprehensive methodological information about its robustness evaluation of instruction-tuned large language models so that other researchers can reproduce the experiments.", "rephrasing_timestamp": "2026-01-11T19:27:58.924245", "rephrasing_model": "gpt-5-mini"}
{"claim": "To allow others to verify their findings, the researchers provide a full, step-by-step description of the experiments used to test how well large language models trained to follow instructions tolerate challenging or altered inputs, enabling outside teams to run the same tests.", "label": "Supported", "paragraph": "To ensure reproducibility, we provide all details regarding our evaluation of the robustness of instruction-tuned LLMs.", "section_name": "B EXPERIMENTS", "subsection_name": "", "paper_id": "g9diuvxN6D", "paper_title": "Evaluating the Zero-shot Robustness of Instruction-tuned Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T16:10:21.432759", "model": "gpt-5-mini", "original_claim": "With the aim of making results reproducible, the authors include all procedural details on how they assessed the robustness of instruction-tuned LLMs, allowing independent researchers to replicate the evaluation.", "rephrasing_timestamp": "2026-01-11T19:28:08.176260", "rephrasing_model": "gpt-5-mini"}
{"claim": "To allow others to replicate the experiments reported in the article, the team made the project’s code available online and included a comprehensive description of the build, configuration, and implementation procedures in Supplementary Section B.", "label": "Supported", "paragraph": "All the results reported in the paper are reproducible. We submit the code and include all the implementation details in Appendix B. We present the disaggregated results in Appendix K. All the instructions used in the paper are formatted and listed in the supplementary materials.", "section_name": "9 REPRODUCIBILITY STATEMENT", "subsection_name": "", "paper_id": "g9diuvxN6D", "paper_title": "Evaluating the Zero-shot Robustness of Instruction-tuned Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T16:10:33.007799", "model": "gpt-5-mini", "original_claim": "The authors uploaded the source code and documented every implementation detail within Appendix B to enable reproduction of the paper's experiments.", "rephrasing_timestamp": "2026-01-11T19:28:08.644084", "rephrasing_model": "gpt-5-mini"}
{"claim": "A detailed, itemized account of the study's experimental results is included in Appendix K of the supplementary files.", "label": "Supported", "paragraph": "All the results reported in the paper are reproducible. We submit the code and include all the implementation details in Appendix B. We present the disaggregated results in Appendix K. All the instructions used in the paper are formatted and listed in the supplementary materials.", "section_name": "9 REPRODUCIBILITY STATEMENT", "subsection_name": "", "paper_id": "g9diuvxN6D", "paper_title": "Evaluating the Zero-shot Robustness of Instruction-tuned Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T16:10:33.007824", "model": "gpt-5-mini", "original_claim": "The paper's breakdown of experimental outcomes is provided as disaggregated results in Appendix K of the supplementary materials.", "rephrasing_timestamp": "2026-01-11T19:28:05.930253", "rephrasing_model": "gpt-5-mini"}
{"claim": "Besides the Kullback–Leibler objective, the team tested two alternative training targets designed to push the model to produce more similar latent encodings for instructions that are effectively the same.", "label": "Supported", "paragraph": "Besides the KL objective reported in the main paper, we explored two other objectives that similarly aim to align representations for practically equivalent instructions. Our goal with these experiments is to establish that the primary mechanism to improve robustness has to do with encouraging the model to internally represent equivalent instructions similarly; the particular choice of objective for realizing this is not critical.", "section_name": "E DIFFERENT (SIMILAR) OBJECTIVES INDUCE SIMILAR REPRESENTATIONS", "subsection_name": "", "paper_id": "g9diuvxN6D", "paper_title": "Evaluating the Zero-shot Robustness of Instruction-tuned Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T16:10:38.567893", "model": "gpt-5-mini", "original_claim": "Alongside the KL objective, the researchers evaluated two alternative objectives intended to make the model's internal representations of practically equivalent instructions more similar.", "rephrasing_timestamp": "2026-01-11T19:28:08.016065", "rephrasing_model": "gpt-5-mini"}
{"claim": "Let R denote the Euclidean length of X − Z. R follows a Gamma distribution with shape d and scale θ = ε⁻¹.", "label": "Supported", "paragraph": "Let ∥ X -Z ∥ 2 = RS where R ∈ [0 , ∞ ) is the magnitude of X -Z , and ∥ S ∥ 2 = 1 . As shown in [33], R follows the Gamma distribution with shape d and scale 1 /ε . Hence the MSE is\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nThe conditional differential entropy (in nats) of Z given X is\n<!-- formula-not-decoded -->\nwhere ψ is the digamma function.", "section_name": "I Proof of Corollary 6.1", "subsection_name": "", "paper_id": "CgGjT8EG8A", "paper_title": "Universal Exact Compression of Differentially Private Mechanisms", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:11:03.735076", "model": "gpt-5-mini", "original_claim": "The radial magnitude R = ||X - Z||_2 is distributed according to a Gamma law with shape parameter d and scale parameter equal to the reciprocal of ε (scale = 1/ε).", "rephrasing_timestamp": "2026-01-11T19:28:17.615192", "rephrasing_model": "gpt-5-mini"}
{"claim": "The entropy of the continuous variable Z conditioned on X is expressed in units of the natural logarithm (nats), and its closed-form includes the logarithmic derivative of the Gamma function (the psi function).", "label": "Supported", "paragraph": "Let ∥ X -Z ∥ 2 = RS where R ∈ [0 , ∞ ) is the magnitude of X -Z , and ∥ S ∥ 2 = 1 . As shown in [33], R follows the Gamma distribution with shape d and scale 1 /ε . Hence the MSE is\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nThe conditional differential entropy (in nats) of Z given X is\n<!-- formula-not-decoded -->\nwhere ψ is the digamma function.", "section_name": "I Proof of Corollary 6.1", "subsection_name": "", "paper_id": "CgGjT8EG8A", "paper_title": "Universal Exact Compression of Differentially Private Mechanisms", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:11:03.735096", "model": "gpt-5-mini", "original_claim": "The conditional differential entropy of Z given X is given in natural units (nats) and is written using the digamma function ψ as part of its analytical expression.", "rephrasing_timestamp": "2026-01-11T19:28:14.035660", "rephrasing_model": "gpt-5-mini"}
{"claim": "As a consequence of Theorem 4.3, the size of the encoded data does not exceed ℓ plus the binary logarithm of (ℓ+1) plus two bits.", "label": "Supported", "paragraph": "Therefore, the KL divergence between P Z | X ( ·| x ) and Q (in nats) is\n<!-- formula-not-decoded -->\nHence, by Theorem 4.3, the compression size is at most ℓ +log 2 ( ℓ +1) + 2 bits. The metric privacy guarantee follows from Theorem 4.7.", "section_name": "I Proof of Corollary 6.1", "subsection_name": "", "paper_id": "CgGjT8EG8A", "paper_title": "Universal Exact Compression of Differentially Private Mechanisms", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:11:21.420413", "model": "gpt-5-mini", "original_claim": "Applying Theorem 4.3 gives an upper bound on the compressed output length: the compression requires at most ℓ plus log_2(ℓ+1) plus 2 bits to encode.", "rephrasing_timestamp": "2026-01-11T19:28:18.869470", "rephrasing_model": "gpt-5-mini"}
{"claim": "Result 4.7 establishes that the procedure satisfies the privacy condition tied to the metric; consequently, the privacy behavior of the mechanism presented here is an immediate consequence of that result.", "label": "Supported", "paragraph": "Therefore, the KL divergence between P Z | X ( ·| x ) and Q (in nats) is\n<!-- formula-not-decoded -->\nHence, by Theorem 4.3, the compression size is at most ℓ +log 2 ( ℓ +1) + 2 bits. The metric privacy guarantee follows from Theorem 4.7.", "section_name": "I Proof of Corollary 6.1", "subsection_name": "", "paper_id": "CgGjT8EG8A", "paper_title": "Universal Exact Compression of Differentially Private Mechanisms", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:11:21.420446", "model": "gpt-5-mini", "original_claim": "Theorem 4.7 guarantees the method's metric privacy: the privacy properties of the proposed mechanism follow directly from that theorem.", "rephrasing_timestamp": "2026-01-11T19:28:18.176647", "rephrasing_model": "gpt-5-mini"}
{"claim": "The authors experimentally evaluate their technique on the distributed mean-estimation problem, explore the interplay between privacy, utility, and communication cost, and benchmark it against a coordinate-wise subsampled Gaussian procedure that achieves provably optimal rate guarantees under central differential privacy.", "label": "Supported", "paragraph": "We empirically evaluate our scheme on the DME problem (which is formally introduced in Section 5), examine the privacy-accuracy-communication trade-off, and compare it with the Coordinate Subsampled Gaussian Mechanism (CSGM) [19, Algorithm 1], an order-optimal scheme for DME under central DP.", "section_name": "7 Empirical Results", "subsection_name": "", "paper_id": "CgGjT8EG8A", "paper_title": "Universal Exact Compression of Differentially Private Mechanisms", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:11:28.973875", "model": "gpt-5-mini", "original_claim": "The paper empirically tests their proposed method on the DME task, analyzes privacy-accuracy-communication trade-offs, and compares its performance to the Coordinate Subsampled Gaussian Mechanism, an order-optimal central-DP scheme.", "rephrasing_timestamp": "2026-01-11T19:28:17.015471", "rephrasing_model": "gpt-5-mini"}
{"claim": "On the PASCAL VOC benchmark, the authors' model—after further task-specific training—yields more precise pixel-wise segmentation maps than the transformer-based baseline networks, as demonstrated by the supplied side-by-side visual comparisons.", "label": "Supported", "paragraph": "We present more visualization results of before and after fine-tuned semantic segmentation on VOC (Fig. 21). Compared to ViT baselines, our model produces more accurate segmentations. Remarkably, our results align with object contours precisely without the need of additional CRF post-processing.", "section_name": "G.2 VISUAL RESULTS ON SEMANTIC SEGMENTATION", "subsection_name": "", "paper_id": "IRcv4yFX6z", "paper_title": "Learning Hierarchical Image Segmentation For Recognition and By Recognition", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T16:12:00.122200", "model": "gpt-5-mini", "original_claim": "On the VOC dataset, the authors' fine-tuned semantic segmentation model produces more accurate segmentation outputs than Vision Transformer baseline models, based on the provided before-and-after visualizations.", "rephrasing_timestamp": "2026-01-11T19:28:22.708043", "rephrasing_model": "gpt-5-mini"}
{"claim": "On PASCAL benchmark images, the network's pixel-level outputs closely trace object outlines, achieving tight contour agreement without any additional post-processing using graphical-model refinements.", "label": "Supported", "paragraph": "We present more visualization results of before and after fine-tuned semantic segmentation on VOC (Fig. 21). Compared to ViT baselines, our model produces more accurate segmentations. Remarkably, our results align with object contours precisely without the need of additional CRF post-processing.", "section_name": "G.2 VISUAL RESULTS ON SEMANTIC SEGMENTATION", "subsection_name": "", "paper_id": "IRcv4yFX6z", "paper_title": "Learning Hierarchical Image Segmentation For Recognition and By Recognition", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T16:12:00.122225", "model": "gpt-5-mini", "original_claim": "The model's segmentation results align closely with object boundaries in VOC images and achieve this precise contour matching without using any Conditional Random Field post-processing.", "rephrasing_timestamp": "2026-01-11T19:28:27.007365", "rephrasing_model": "gpt-5-mini"}
{"claim": "The introduced architecture yields higher-fidelity partitioning of visual scenes than systems that rely on patch-oriented token representations.", "label": "Supported", "paragraph": "Figure 21: Our model induce much more precise segmentation than patch tokens. Segmentations are predicted based on segment-wise nearest neighbor retrievals (row 1 images) and fine-tuned models (row 2 images).", "section_name": "G.2 VISUAL RESULTS ON SEMANTIC SEGMENTATION", "subsection_name": "", "paper_id": "IRcv4yFX6z", "paper_title": "Learning Hierarchical Image Segmentation For Recognition and By Recognition", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T16:12:09.661360", "model": "gpt-5-mini", "original_claim": "The presented model produces more accurate image segmentations than segmentations generated using patch-token based approaches.", "rephrasing_timestamp": "2026-01-11T19:28:25.243635", "rephrasing_model": "gpt-5-mini"}
{"claim": "Top-row images obtain their region masks by, for each part, finding the closest match in a reference collection and using that match to infer the mask, whereas bottom-row images come from models that received extra training to directly output those masks.", "label": "Supported", "paragraph": "Figure 21: Our model induce much more precise segmentation than patch tokens. Segmentations are predicted based on segment-wise nearest neighbor retrievals (row 1 images) and fine-tuned models (row 2 images).", "section_name": "G.2 VISUAL RESULTS ON SEMANTIC SEGMENTATION", "subsection_name": "", "paper_id": "IRcv4yFX6z", "paper_title": "Learning Hierarchical Image Segmentation For Recognition and By Recognition", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T16:12:09.661385", "model": "gpt-5-mini", "original_claim": "Images in row 1 use per-segment nearest-neighbor retrieval to predict segmentations, while images in row 2 use models that were fine-tuned to produce segmentations.", "rephrasing_timestamp": "2026-01-11T19:28:26.227553", "rephrasing_model": "gpt-5-mini"}
{"claim": "By producing region labels as its output, this approach makes a separate post-processing clustering step unnecessary for obtaining the image partitions.", "label": "Supported", "paragraph": "Using segment, not patch, tokens improves our predicted segmentations by a large margin. Notably, our method explicitly produces segmentations without the need of additional K-Means clustering for segment retrievals.", "section_name": "G.2 VISUAL RESULTS ON SEMANTIC SEGMENTATION", "subsection_name": "", "paper_id": "IRcv4yFX6z", "paper_title": "Learning Hierarchical Image Segmentation For Recognition and By Recognition", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T16:12:20.030367", "model": "gpt-5-mini", "original_claim": "The proposed method directly outputs segmentations and thus eliminates the need to run additional K-Means clustering to retrieve segments.", "rephrasing_timestamp": "2026-01-11T19:28:26.241320", "rephrasing_model": "gpt-5-mini"}
{"claim": "The introduced approach aims to increase the effectiveness of advanced text-generating neural networks by refining the guidance they are given, with the intent of yielding better outcomes across a wide range of tasks.", "label": "Supported", "paragraph": "Since our proposed method aims to improve the performance of LLMs (via instruction optimization) for different tasks, there may exist some ethical implications related to the usage of LLMs. Specifically, in certain maliciously designed tasks, our method may be used by a malicious party to produce harmful/inappropriate instructions.", "section_name": "A ETHICAL CONSIDERATIONS", "subsection_name": "", "paper_id": "6ujgouOiAA", "paper_title": "Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:12:48.820630", "model": "gpt-5-mini", "original_claim": "The proposed method is designed to boost large language model performance by optimizing the instructions provided to models, aiming to improve results across multiple different tasks.", "rephrasing_timestamp": "2026-01-11T19:28:29.098260", "rephrasing_model": "gpt-5-mini"}
{"claim": "If a technique for refining task directives is used on objectives deliberately meant to cause harm, it may allow hostile actors to craft dangerous or inappropriate guidance for powerful conversational AI systems.", "label": "Supported", "paragraph": "Since our proposed method aims to improve the performance of LLMs (via instruction optimization) for different tasks, there may exist some ethical implications related to the usage of LLMs. Specifically, in certain maliciously designed tasks, our method may be used by a malicious party to produce harmful/inappropriate instructions.", "section_name": "A ETHICAL CONSIDERATIONS", "subsection_name": "", "paper_id": "6ujgouOiAA", "paper_title": "Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:12:48.820641", "model": "gpt-5-mini", "original_claim": "When applied to tasks intentionally crafted for malicious ends, the instruction-optimization method could enable malicious users to create harmful or inappropriate instructions for large language models.", "rephrasing_timestamp": "2026-01-11T19:28:30.143548", "rephrasing_model": "gpt-5-mini"}
{"claim": "This approach's sole aim is to pick, for every task, the instruction that produces the best evaluation outcome from an opaque language model.", "label": "Supported", "paragraph": "This is because currently our method only aims to maximize the test performance of the black-box LLM when selecting the instructions for any given task. Therefore, to account for such potential ethical issues and prevent the generation of harmful instructions, we may explore extensions of our method which can also account for additional objectives or constraints (e.g., harmfulness) during the optimization process.", "section_name": "A ETHICAL CONSIDERATIONS", "subsection_name": "", "paper_id": "6ujgouOiAA", "paper_title": "Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:13:01.964250", "model": "gpt-5-mini", "original_claim": "The current method is designed solely to maximize the black-box LLM's test performance when selecting instructions for any given task.", "rephrasing_timestamp": "2026-01-11T19:28:35.798719", "rephrasing_model": "gpt-5-mini"}
{"claim": "The researchers plan to investigate broadening their method by incorporating extra goals or limits—such as evaluating potential harm—into the tuning process to reduce ethical concerns and stop the generation of unsafe guidance.", "label": "Supported", "paragraph": "This is because currently our method only aims to maximize the test performance of the black-box LLM when selecting the instructions for any given task. Therefore, to account for such potential ethical issues and prevent the generation of harmful instructions, we may explore extensions of our method which can also account for additional objectives or constraints (e.g., harmfulness) during the optimization process.", "section_name": "A ETHICAL CONSIDERATIONS", "subsection_name": "", "paper_id": "6ujgouOiAA", "paper_title": "Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:13:01.964265", "model": "gpt-5-mini", "original_claim": "The authors intend to explore extending the approach to include additional objectives or constraints (for example, assessing harmfulness) during optimization to address ethical risks and avoid harmful instructions.", "rephrasing_timestamp": "2026-01-11T19:28:35.147586", "rephrasing_model": "gpt-5-mini"}
{"claim": "Experimental benchmarks indicate that supplying a neural surrogate with a pretrained transformer's intermediate activation vectors as additional inputs enhances the effectiveness of the INSTINCT algorithm.", "label": "Supported", "paragraph": "Effectiveness of the Hidden Representation. Here we empirically verify that the use of the hidden representation from the pre-trained transformer when building the NN surrogate (Sec. 3.1) indeed helps improve the performance of our INSTINCT algorithm.", "section_name": "5 ABLATION STUDY", "subsection_name": "", "paper_id": "6ujgouOiAA", "paper_title": "Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:13:07.992522", "model": "gpt-5-mini", "original_claim": "Empirical evaluation shows that incorporating the pre-trained transformer's hidden representations into the neural-network surrogate model improves the performance of the INSTINCT algorithm.", "rephrasing_timestamp": "2026-01-11T19:28:31.470003", "rephrasing_model": "gpt-5-mini"}
{"claim": "Appendix B.3 contains derivations showing bounds on the large-deviation probabilities for the quantities ϑ_SGD_w^{ν;n,m,T} and ε_SGD_w^{ν;n,m,T}.", "label": "Supported", "paragraph": "We present the proofs for results in Appendix B.3 that control the tail probability terms ϑ SGDw ν ; n,m,T and ε SGDw ν ; n,m,T . Proof of Lemma B.3. For δ &gt; 0 , let N δ be the δ -covering number of Ψ , which satisfies N δ ≤ ( r Ψ (1 + 2 /δ ) ) p (Example 5.8, [50]).", "section_name": "G Proofs for tail probability bounds in offline SGD", "subsection_name": "", "paper_id": "Q74JVgKCP6", "paper_title": "Near-Optimality of Contrastive Divergence Algorithms", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:13:39.472149", "model": "gpt-5-mini", "original_claim": "The paper provides proofs for Appendix B.3 that establish control over the tail-probability quantities ϑ_SGD_w^{ν;n,m,T} and ε_SGD_w^{ν;n,m,T}.", "rephrasing_timestamp": "2026-01-11T19:28:39.686193", "rephrasing_model": "gpt-5-mini"}
{"claim": "For any ε>0, the minimal cardinality of an ε-net for the family Ψ (the number of ε-balls needed to cover Ψ) is at most [r_Ψ(1+2/ε)]^p; see Example 5.8 in [50].", "label": "Supported", "paragraph": "We present the proofs for results in Appendix B.3 that control the tail probability terms ϑ SGDw ν ; n,m,T and ε SGDw ν ; n,m,T . Proof of Lemma B.3. For δ &gt; 0 , let N δ be the δ -covering number of Ψ , which satisfies N δ ≤ ( r Ψ (1 + 2 /δ ) ) p (Example 5.8, [50]).", "section_name": "G Proofs for tail probability bounds in offline SGD", "subsection_name": "", "paper_id": "Q74JVgKCP6", "paper_title": "Near-Optimality of Contrastive Divergence Algorithms", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:13:39.472161", "model": "gpt-5-mini", "original_claim": "For every δ>0, the δ-covering number N_δ of the class Ψ satisfies the upper bound N_δ ≤ (r_Ψ(1+2/δ))^p, as given in Example 5.8 of [50].", "rephrasing_timestamp": "2026-01-11T19:28:40.005778", "rephrasing_model": "gpt-5-mini"}
{"claim": "The conditional form of Jensen's inequality, when applied given X1 and combined with Assumption A6, implies the existence of strictly positive scalars σ_m and ζ_m such that the stated inequality holds for every vector z in R^p whose Euclidean norm does not exceed ζ_m.", "label": "Supported", "paragraph": "Note also that by the Jensen's inequality applied to E [ · | X 1 ] and Assumption A6, there exist some σ m , ζ m &gt; 0 such that, for any z ∈ ❘ p with ∥ z ∥ ≤ ζ m ,\n<!-- formula-not-decoded -->\nMeanwhile under A5, recycling the proof of Lemma 3.1 of [21] shows that if C m δ/ √ p &lt; σ 2 m ζ m ,\n<!-- formula-not-decoded -->\nSince the probability above is an upper bound to ϑ SGDw n,m,T (3 C m δ ) , we get that if C m δ/ √ p &lt; σ 2 m ζ m ,\n<!-- formula-not-decoded -->\nRecall that by assumption, log n n &lt; σ 2 m ζ 2 m p + ν -2 .", "section_name": "G Proofs for tail probability bounds in offline SGD", "subsection_name": "", "paper_id": "Q74JVgKCP6", "paper_title": "Near-Optimality of Contrastive Divergence Algorithms", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:13:53.423126", "model": "gpt-5-mini", "original_claim": "Applying Jensen's inequality conditional on X1 together with Assumption A6 guarantees there exist positive constants σ_m and ζ_m such that the claimed bound holds for every vector z in R^p with ||z|| ≤ ζ_m.", "rephrasing_timestamp": "2026-01-11T19:28:37.922716", "rephrasing_model": "gpt-5-mini"}
{"claim": "The premise demands that the logarithm of n divided by n be strictly smaller than the sum of p times σ_m^2 ζ_m^2 and 1/ν^2.", "label": "Supported", "paragraph": "Note also that by the Jensen's inequality applied to E [ · | X 1 ] and Assumption A6, there exist some σ m , ζ m &gt; 0 such that, for any z ∈ ❘ p with ∥ z ∥ ≤ ζ m ,\n<!-- formula-not-decoded -->\nMeanwhile under A5, recycling the proof of Lemma 3.1 of [21] shows that if C m δ/ √ p &lt; σ 2 m ζ m ,\n<!-- formula-not-decoded -->\nSince the probability above is an upper bound to ϑ SGDw n,m,T (3 C m δ ) , we get that if C m δ/ √ p &lt; σ 2 m ζ m ,\n<!-- formula-not-decoded -->\nRecall that by assumption, log n n &lt; σ 2 m ζ 2 m p + ν -2 .", "section_name": "G Proofs for tail probability bounds in offline SGD", "subsection_name": "", "paper_id": "Q74JVgKCP6", "paper_title": "Near-Optimality of Contrastive Divergence Algorithms", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:13:53.423141", "model": "gpt-5-mini", "original_claim": "The stated assumption requires that (log n)/n is strictly less than σ_m^2 ζ_m^2 p plus ν^{-2}.", "rephrasing_timestamp": "2026-01-11T19:28:43.595261", "rephrasing_model": "gpt-5-mini"}
{"claim": "After taking square roots, the needed estimate follows from the simple fact that for positive a and b the square root of a + b is at most the sum of the square roots of a and b.", "label": "Supported", "paragraph": "We now choose\n<!-- formula-not-decoded -->\nwhich implies\n<!-- formula-not-decoded -->\nTaking a squareroot across and using √ a + b ≤ √ a + b for a, b &gt; 0 gives the desired bound. The limiting result follows by substituting this bound into Theorem B.1.", "section_name": "G Proofs for tail probability bounds in offline SGD", "subsection_name": "", "paper_id": "Q74JVgKCP6", "paper_title": "Near-Optimality of Contrastive Divergence Algorithms", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:14:01.202956", "model": "gpt-5-mini", "original_claim": "Using the inequality sqrt(a+b) ≤ sqrt(a) + sqrt(b) for positive a and b after taking square roots produces the desired bound.", "rephrasing_timestamp": "2026-01-11T19:28:43.532944", "rephrasing_model": "gpt-5-mini"}
{"claim": "The investigators prepared a survey cleared by an ethics committee that prompted respondents to indicate the five main in-game goals they intended to pursue.", "label": "Supported", "paragraph": "We designed an IRB-approved participant survey on what top 5 goals participants want to achieve in-game. The participant survey contains 8 initial goals, including Game Victory, High MVP Score, More Highlights, More Kill Counts, Few Death Counts, High Participation, More Resources, and More Visible Information.", "section_name": "D.2.3 PARTICIPANT SURVEY DESCRIPTION", "subsection_name": "", "paper_id": "BqEvdOS1Hs", "paper_title": "Enhancing Human Experience in Human-Agent Collaboration: A Human-Centered Modeling Approach Based on Positive Human Gain", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:14:24.821992", "model": "gpt-5-mini", "original_claim": "The researchers created an IRB-approved questionnaire asking participants to select the five primary in-game objectives they wish to accomplish.", "rephrasing_timestamp": "2026-01-11T19:28:44.268182", "rephrasing_model": "gpt-5-mini"}
{"claim": "Respondents were given eight preset objectives to choose from: securing a match victory, achieving a high individual-performance rating, creating more highlight moments, raising opponent-elimination totals, decreasing how often they are eliminated, taking a more active role, collecting extra in-game assets, and revealing additional on-screen information.", "label": "Supported", "paragraph": "We designed an IRB-approved participant survey on what top 5 goals participants want to achieve in-game. The participant survey contains 8 initial goals, including Game Victory, High MVP Score, More Highlights, More Kill Counts, Few Death Counts, High Participation, More Resources, and More Visible Information.", "section_name": "D.2.3 PARTICIPANT SURVEY DESCRIPTION", "subsection_name": "", "paper_id": "BqEvdOS1Hs", "paper_title": "Enhancing Human Experience in Human-Agent Collaboration: A Human-Centered Modeling Approach Based on Positive Human Gain", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:14:24.822004", "model": "gpt-5-mini", "original_claim": "The survey presented eight predefined goal options: winning the game, achieving a high MVP score, generating more highlights, increasing kill counts, reducing deaths, higher participation, acquiring more resources, and displaying more visible information.", "rephrasing_timestamp": "2026-01-11T19:28:48.546701", "rephrasing_model": "gpt-5-mini"}
{"claim": "During the balloting phase, every participant could identify no more than five separate objectives and could also nominate additional objectives beyond those choices.", "label": "Supported", "paragraph": "Each participant can vote up to 5 non-repeating goals, and can also add additional goals. 30 participants voluntarily participated in the voting, and the result is shown in Figure 24. Figure 24: Voting results on human goals in Honor of Kings , based on statistics from our participant survey.", "section_name": "D.2.3 PARTICIPANT SURVEY DESCRIPTION", "subsection_name": "", "paper_id": "BqEvdOS1Hs", "paper_title": "Enhancing Human Experience in Human-Agent Collaboration: A Human-Centered Modeling Approach Based on Positive Human Gain", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:14:31.783565", "model": "gpt-5-mini", "original_claim": "Participants were allowed to select up to five distinct goals each during voting and were permitted to propose additional goals beyond those selections.", "rephrasing_timestamp": "2026-01-11T19:28:49.066032", "rephrasing_model": "gpt-5-mini"}
{"claim": "Thirty individuals completed an objective-selection questionnaire about Honor of Kings; the combined responses are shown in Figure 24.", "label": "Supported", "paragraph": "Each participant can vote up to 5 non-repeating goals, and can also add additional goals. 30 participants voluntarily participated in the voting, and the result is shown in Figure 24. Figure 24: Voting results on human goals in Honor of Kings , based on statistics from our participant survey.", "section_name": "D.2.3 PARTICIPANT SURVEY DESCRIPTION", "subsection_name": "", "paper_id": "BqEvdOS1Hs", "paper_title": "Enhancing Human Experience in Human-Agent Collaboration: A Human-Centered Modeling Approach Based on Positive Human Gain", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:14:31.783581", "model": "gpt-5-mini", "original_claim": "A total of 30 volunteers participated in the goal-voting survey for Honor of Kings, with their aggregated results presented in Figure 24.", "rephrasing_timestamp": "2026-01-11T19:28:50.634297", "rephrasing_model": "gpt-5-mini"}
{"claim": "An impartial external ethics board examined the study's research methods and the safeguards designed to reduce risk, endorsed the project, and found it to be in accordance with China's national guidelines for next‑generation AI ethics.", "label": "Supported", "paragraph": "The ethics committee of a third-party organization conducted an ethical review of our project. They reviewed our experimental procedures and risk avoidance methods (see Appendix D.1.3). They believed that our project complies with the \"New Generation of AI Ethics Code\" 2 of the country to which the participants belonged (China), so they approved our study. In addition, all participants consented to the experiment and provided informed consent (see Appendix D.1.1) for the study.", "section_name": "D.1 ETHICAL REVIEW", "subsection_name": "", "paper_id": "BqEvdOS1Hs", "paper_title": "Enhancing Human Experience in Human-Agent Collaboration: A Human-Centered Modeling Approach Based on Positive Human Gain", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:14:39.860664", "model": "gpt-5-mini", "original_claim": "An independent third-party ethics committee reviewed the project's experimental procedures and risk-avoidance measures and approved the study, concluding it complied with China's New Generation of AI Ethics Code.", "rephrasing_timestamp": "2026-01-11T19:28:51.622082", "rephrasing_model": "gpt-5-mini"}
{"claim": "This investigation probes whether NCoVs can emulate intricate, multi-peaked probability distributions defined over planar two-dimensional regions.", "label": "Supported", "paragraph": "Here, we investigate an intricate yet interesting scenario to demonstrate the efficacy of NCoVs to approximate complex multi-modal pdfs in two-dimensional (2D) settings. The primary objective is to transform a standard Gaussian random vector Z ∼ N ( 0 2 × 1 , I 2 × 2 ) into multi-modal complex pdfs.", "section_name": "4.1 Tests with toy data", "subsection_name": "", "paper_id": "E8b4yOLGZ5", "paper_title": "Meta-Learning Universal Priors Using Non-Injective Change of Variables", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:15:17.090468", "model": "gpt-5-mini", "original_claim": "The experiment assesses the ability of NCoVs to approximate complex multi-modal probability density functions within two-dimensional (2D) domains.", "rephrasing_timestamp": "2026-01-11T19:28:50.638502", "rephrasing_model": "gpt-5-mini"}
{"claim": "The purpose of the experiment is to apply a transformation to a base pair of independent, zero-mean, unit-variance normal variables Z so that the outputs follow intricate, multi-peaked probability distributions.", "label": "Supported", "paragraph": "Here, we investigate an intricate yet interesting scenario to demonstrate the efficacy of NCoVs to approximate complex multi-modal pdfs in two-dimensional (2D) settings. The primary objective is to transform a standard Gaussian random vector Z ∼ N ( 0 2 × 1 , I 2 × 2 ) into multi-modal complex pdfs.", "section_name": "4.1 Tests with toy data", "subsection_name": "", "paper_id": "E8b4yOLGZ5", "paper_title": "Meta-Learning Universal Priors Using Non-Injective Change of Variables", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:15:17.090479", "model": "gpt-5-mini", "original_claim": "The experiment's goal is to convert a standard two-dimensional Gaussian vector Z with zero mean and identity covariance into complex multi-modal probability density functions.", "rephrasing_timestamp": "2026-01-11T19:28:58.760298", "rephrasing_model": "gpt-5-mini"}
{"claim": "Along the bottom of the first figure are plotted the actual density curves, labeled q, which depict the distributions targeted in the experiment.", "label": "Supported", "paragraph": "The outcomes of this experiment are presented in Figure 1. The lower row displays the ground-truth pdfs q of interest, while the upper row showcases the numerically estimated pdfs of the transformed random vector Z ′ = f ( Z ) , where f is a Sylvester NCoV, and the pdf of Z ′ is estimated via (5).", "section_name": "4.1 Tests with toy data", "subsection_name": "", "paper_id": "E8b4yOLGZ5", "paper_title": "Meta-Learning Universal Priors Using Non-Injective Change of Variables", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:15:25.617288", "model": "gpt-5-mini", "original_claim": "The lower row of Figure 1 displays the ground-truth probability density functions q that represent the target distributions examined in the experiment.", "rephrasing_timestamp": "2026-01-11T19:28:55.924124", "rephrasing_model": "gpt-5-mini"}
{"claim": "The top row of Figure 1 presents numerically obtained estimates of the probability density functions for the random vector Z' produced by applying f to Z, where f is a Sylvester-type nonconservative vector mapping; these densities were evaluated using expression (5).", "label": "Supported", "paragraph": "The outcomes of this experiment are presented in Figure 1. The lower row displays the ground-truth pdfs q of interest, while the upper row showcases the numerically estimated pdfs of the transformed random vector Z ′ = f ( Z ) , where f is a Sylvester NCoV, and the pdf of Z ′ is estimated via (5).", "section_name": "4.1 Tests with toy data", "subsection_name": "", "paper_id": "E8b4yOLGZ5", "paper_title": "Meta-Learning Universal Priors Using Non-Injective Change of Variables", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:15:25.617307", "model": "gpt-5-mini", "original_claim": "The upper row of Figure 1 shows numerical estimates of the pdfs for the transformed random vector Z' = f(Z), where f is a Sylvester NCoV, with these densities computed using equation (5).", "rephrasing_timestamp": "2026-01-11T19:29:01.004968", "rephrasing_model": "gpt-5-mini"}
{"claim": "The first table summarizes how the MetaNCoV approach fares compared with several meta-learning techniques that incorporate varied prior beliefs.", "label": "Supported", "paragraph": "As clearly evidenced in these results, the advocated NCoVs exhibit their capability to effectively convert\nTable 1: Performance comparison of MetaNCoV against meta-learning methods having different priors.", "section_name": "4.1 Tests with toy data", "subsection_name": "", "paper_id": "E8b4yOLGZ5", "paper_title": "Meta-Learning Universal Priors Using Non-Injective Change of Variables", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:15:32.390969", "model": "gpt-5-mini", "original_claim": "Table 1 displays a performance comparison of MetaNCoV against various meta-learning methods that employ different priors.", "rephrasing_timestamp": "2026-01-11T19:29:00.218822", "rephrasing_model": "gpt-5-mini"}
{"claim": "They create a simplified ablation variant called GloAtt that limits its scope to relationships between node pairs, and use it to evaluate their cycle-capturing component and GloAttNC.", "label": "Supported", "paragraph": "To validate our proposed cycle modeling module and GloAttNC, we propose an ablation module, called GloAtt, which only learns the interactions between node-to-node pairs. It computes global attention using Eq. 12:\n<!-- formula-not-decoded -->\nSubsequently, the representations of nodes and graphs are computed sequentially:\n<!-- formula-not-decoded -->", "section_name": "F DETAILS OF EXPERIMENTS", "subsection_name": "", "paper_id": "3fRbP8g2LT", "paper_title": "Efficient Redundancy-Free Graph Networks: Higher Expressiveness and Less Over-Squashing", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:15:57.216291", "model": "gpt-5-mini", "original_claim": "They develop an ablation module termed GloAtt that exclusively models pairwise node-to-node interactions as a validation tool for their cycle modeling module and GloAttNC.", "rephrasing_timestamp": "2026-01-11T19:29:02.088102", "rephrasing_model": "gpt-5-mini"}
{"claim": "GloAtt derives a graph-wide attention map using the formula given in Equation 12. It then generates individual node encodings and subsequently merges them to produce a single representation for the entire graph.", "label": "Supported", "paragraph": "To validate our proposed cycle modeling module and GloAttNC, we propose an ablation module, called GloAtt, which only learns the interactions between node-to-node pairs. It computes global attention using Eq. 12:\n<!-- formula-not-decoded -->\nSubsequently, the representations of nodes and graphs are computed sequentially:\n<!-- formula-not-decoded -->", "section_name": "F DETAILS OF EXPERIMENTS", "subsection_name": "", "paper_id": "3fRbP8g2LT", "paper_title": "Efficient Redundancy-Free Graph Networks: Higher Expressiveness and Less Over-Squashing", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:15:57.216302", "model": "gpt-5-mini", "original_claim": "GloAtt calculates global attention according to Equation 12, after which it sequentially computes node representations followed by graph-level representations.", "rephrasing_timestamp": "2026-01-11T19:29:03.599603", "rephrasing_model": "gpt-5-mini"}
{"claim": "In this part we present a comprehensive theoretical investigation of the expressive capacity of four architectures: DLGN, ERFGN, ERFGN ˝, and ERFGN ˝ +GloAttNC.", "label": "Supported", "paragraph": "This section provides a comprehensive analysis of the theoretical expressiveness of our models, namely DLGN, ERFGN, ERFGN ˝ and ERFGN ˝ +GloAttNC. DLGN and ERFGN employ a twostep process for generating representations of nodes and graphs.", "section_name": "C EXPRESSIVE POWER ANALYSIS", "subsection_name": "", "paper_id": "3fRbP8g2LT", "paper_title": "Efficient Redundancy-Free Graph Networks: Higher Expressiveness and Less Over-Squashing", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:16:08.909174", "model": "gpt-5-mini", "original_claim": "This section conducts an in-depth theoretical expressiveness analysis of four models: DLGN, ERFGN, ERFGN ˝, and ERFGN ˝ +GloAttNC.", "rephrasing_timestamp": "2026-01-11T19:29:04.339415", "rephrasing_model": "gpt-5-mini"}
{"claim": "DLGN and ERFGN produce encodings for vertices and for whole graphs by first performing an initial computation and then applying a subsequent refinement step.", "label": "Supported", "paragraph": "This section provides a comprehensive analysis of the theoretical expressiveness of our models, namely DLGN, ERFGN, ERFGN ˝ and ERFGN ˝ +GloAttNC. DLGN and ERFGN employ a twostep process for generating representations of nodes and graphs.", "section_name": "C EXPRESSIVE POWER ANALYSIS", "subsection_name": "", "paper_id": "3fRbP8g2LT", "paper_title": "Efficient Redundancy-Free Graph Networks: Higher Expressiveness and Less Over-Squashing", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:16:08.909193", "model": "gpt-5-mini", "original_claim": "DLGN and ERFGN create representations for individual nodes and entire graphs by employing a two-stage procedure for representation generation.", "rephrasing_timestamp": "2026-01-11T19:29:06.747618", "rephrasing_model": "gpt-5-mini"}
{"claim": "The approach transforms each input graph into an auxiliary line-graph variant (e.g., a DLG or DALG), then performs iterative information propagation on these proxy structures to produce embeddings for individual vertices as well as a single global embedding for the whole graph.", "label": "Supported", "paragraph": "Initially, the input graphs undergo a transformation into surrogate structures, specifically DLGs or DALGs. Subsequently, message passing procedure is applied to these surrogate structures to produce node and graph representations.", "section_name": "C EXPRESSIVE POWER ANALYSIS", "subsection_name": "", "paper_id": "3fRbP8g2LT", "paper_title": "Efficient Redundancy-Free Graph Networks: Higher Expressiveness and Less Over-Squashing", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:16:18.175260", "model": "gpt-5-mini", "original_claim": "The method converts input graphs into surrogate DLG or DALG structures, then runs message-passing on those surrogates to obtain representations for both nodes and entire graphs.", "rephrasing_timestamp": "2026-01-11T19:29:07.768124", "rephrasing_model": "gpt-5-mini"}
{"claim": "The opening table presents a thorough side-by-side evaluation of the authors' new approach versus existing zero-shot baselines across a range of arithmetic reasoning benchmarks.", "label": "Supported", "paragraph": "Table 1 presents a comprehensive performance comparison between our method and existing zeroshot techniques on arithmetic reasoning datasets. Our analysis reveals a consistent enhancement in arithmetic reasoning attributed to re-reading, clearly outperforming both chain-of-thought prompting and vanilla prompting on almost all benchmarks when employing the davinci-003 model.", "section_name": "4.3 EVALUATION RESULTS", "subsection_name": "", "paper_id": "3jXCF5dNpC", "paper_title": "Re-Reading Improves Reasoning in Language Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:16:59.993366", "model": "gpt-5-mini", "original_claim": "Table 1 provides a detailed performance comparison between the authors' proposed method and prior zero-shot techniques across multiple arithmetic reasoning datasets.", "rephrasing_timestamp": "2026-01-11T19:29:08.718494", "rephrasing_model": "gpt-5-mini"}
{"claim": "Applied to the davinci-003 model, an iterative re-examination technique steadily boosts performance on numerical reasoning problems, outperforming both multi-step rationale prompts and simple single-pass prompts across nearly all evaluation datasets.", "label": "Supported", "paragraph": "Table 1 presents a comprehensive performance comparison between our method and existing zeroshot techniques on arithmetic reasoning datasets. Our analysis reveals a consistent enhancement in arithmetic reasoning attributed to re-reading, clearly outperforming both chain-of-thought prompting and vanilla prompting on almost all benchmarks when employing the davinci-003 model.", "section_name": "4.3 EVALUATION RESULTS", "subsection_name": "", "paper_id": "3jXCF5dNpC", "paper_title": "Re-Reading Improves Reasoning in Language Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:16:59.993377", "model": "gpt-5-mini", "original_claim": "Using the davinci-003 model, a re-reading strategy consistently improves arithmetic reasoning accuracy, surpassing both chain-of-thought prompting and vanilla prompting on almost all benchmarks.", "rephrasing_timestamp": "2026-01-11T19:29:11.502557", "rephrasing_model": "gpt-5-mini"}
{"claim": "Integrating the RE2 technique into ChatGPT’s default setup raised its CommonsenseQA score from 76.66% to 78.38%, an improvement of 1.72 percentage points.", "label": "Supported", "paragraph": "1 https://platform.openai.com/docs/models/gpt-3-5\nTable 2: Evaluation results on commonsense and symbolic reasoning benchmarks. | LLMs        | Methods          | Commonsense   | Commonsense        | Commonsense        | Commonsense        | Commonsense   | Symbolic          | Symbolic          |\n|----------|----------|----------|----------|----------|----------|----------|----------|----------|\n|          |          | CommonsenseQA | StrategyQA         | ARC-e          | ARC-c          | ARC-t         | Date          | Coin          |\n| davinci-003 | Vanilla Vanilla+RE2 | 74.20 76.99   | 59.74 59.91 ↑ 0.17 | 84.81 88.22 ↑ 3.41 | 72.01 75.68 ↑ 3.67 | 80.58 84.07   | 40.92 42.01 ↑ 1.09 | 49.80 52.40 ↑ 2.60 |\n| davinci-003 |          | ↑ 2.79        |          |          |          | ↑ 3.49 81.57  |          |          |\n| davinci-003 | CoT CoT+RE2         | 71.66 73.05   | 67.55          | 85.69 87.84        | 73.21 76.02        | 83.94         | 46.07          | 95.60          |\n|          |          |          | 66.24          |          |          |          | 52.57          | 99.60          |\n|          |          | ↑ 1.39        | ↓ 1.31          | ↑ 2.15          | ↑ 2.81          | ↑ 2.37        | ↑ 6.50          | ↑ 4.00          |\n| ChatGPT     | Vanilla          | 76.66         | 62.36          | 94.32          | 85.41          | 91.37         | 47.43          | 52.00          |\n| ChatGPT     | Vanilla+RE2         | 78.38         | 66.99          | 93.81          | 83.19          | 90.30         | 47.97          | 57.20          |\n| ChatGPT     |          | ↑ 1.72        | ↑ 4.63          | ↓ 0.51          | ↓ 2.22          | ↓ 1.07        | ↑ 0.54          | ↑ 5.20          |\n| ChatGPT     | CoT          | 69.94         | 67.82          | 93.35          | 83.53          | 90.11         | 43.63          | 88.80          |\n| ChatGPT     | CoT+RE2          | 71.66         | 69.34          | 93.14          | 84.47          | 90.27         | 47.15          | 95.20          |\n| ChatGPT     |          | ↑ 1.72        | ↑ 1.52          | ↓ 0.21          | ↑ 0.94          | ↑ 0.16        | ↑ 3.52          | ↑ 6.40          |\nFurthermore, when applied to ChatGPT, re-reading exhibits a substantial improvement in arithmetic reasoning performance on most datasets when combined with chain-of-thought prompting.", "section_name": "4.3 EVALUATION RESULTS", "subsection_name": "", "paper_id": "3jXCF5dNpC", "paper_title": "Re-Reading Improves Reasoning in Language Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:17:21.741966", "model": "gpt-5-mini", "original_claim": "Applying RE2 to ChatGPT's vanilla method increased CommonsenseQA accuracy from 76.66% to 78.38%, a 1.72 percentage-point improvement.", "rephrasing_timestamp": "2026-01-11T19:29:10.035742", "rephrasing_model": "gpt-5-mini"}
{"claim": "Using a re-examination routine (RE2) together with explicit step-by-step reasoning prompts in ChatGPT raised accuracy on the Coin benchmark from 88.80% to 95.20%, an improvement of 6.40 percentage points.", "label": "Supported", "paragraph": "1 https://platform.openai.com/docs/models/gpt-3-5\nTable 2: Evaluation results on commonsense and symbolic reasoning benchmarks. | LLMs        | Methods          | Commonsense   | Commonsense        | Commonsense        | Commonsense        | Commonsense   | Symbolic          | Symbolic          |\n|----------|----------|----------|----------|----------|----------|----------|----------|----------|\n|          |          | CommonsenseQA | StrategyQA         | ARC-e          | ARC-c          | ARC-t         | Date          | Coin          |\n| davinci-003 | Vanilla Vanilla+RE2 | 74.20 76.99   | 59.74 59.91 ↑ 0.17 | 84.81 88.22 ↑ 3.41 | 72.01 75.68 ↑ 3.67 | 80.58 84.07   | 40.92 42.01 ↑ 1.09 | 49.80 52.40 ↑ 2.60 |\n| davinci-003 |          | ↑ 2.79        |          |          |          | ↑ 3.49 81.57  |          |          |\n| davinci-003 | CoT CoT+RE2         | 71.66 73.05   | 67.55          | 85.69 87.84        | 73.21 76.02        | 83.94         | 46.07          | 95.60          |\n|          |          |          | 66.24          |          |          |          | 52.57          | 99.60          |\n|          |          | ↑ 1.39        | ↓ 1.31          | ↑ 2.15          | ↑ 2.81          | ↑ 2.37        | ↑ 6.50          | ↑ 4.00          |\n| ChatGPT     | Vanilla          | 76.66         | 62.36          | 94.32          | 85.41          | 91.37         | 47.43          | 52.00          |\n| ChatGPT     | Vanilla+RE2         | 78.38         | 66.99          | 93.81          | 83.19          | 90.30         | 47.97          | 57.20          |\n| ChatGPT     |          | ↑ 1.72        | ↑ 4.63          | ↓ 0.51          | ↓ 2.22          | ↓ 1.07        | ↑ 0.54          | ↑ 5.20          |\n| ChatGPT     | CoT          | 69.94         | 67.82          | 93.35          | 83.53          | 90.11         | 43.63          | 88.80          |\n| ChatGPT     | CoT+RE2          | 71.66         | 69.34          | 93.14          | 84.47          | 90.27         | 47.15          | 95.20          |\n| ChatGPT     |          | ↑ 1.72        | ↑ 1.52          | ↓ 0.21          | ↑ 0.94          | ↑ 0.16        | ↑ 3.52          | ↑ 6.40          |\nFurthermore, when applied to ChatGPT, re-reading exhibits a substantial improvement in arithmetic reasoning performance on most datasets when combined with chain-of-thought prompting.", "section_name": "4.3 EVALUATION RESULTS", "subsection_name": "", "paper_id": "3jXCF5dNpC", "paper_title": "Re-Reading Improves Reasoning in Language Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:17:21.741985", "model": "gpt-5-mini", "original_claim": "Combining re-reading (RE2) with chain-of-thought on ChatGPT increased Coin accuracy from 88.80% to 95.20%, a 6.40 percentage-point gain.", "rephrasing_timestamp": "2026-01-11T19:29:18.205719", "rephrasing_model": "gpt-5-mini"}
{"claim": "When run with uncustomized, out-of-the-box prompts, this approach caused the model's performance to decline on several evaluation datasets, particularly AQUA, MultiArith, SingleEQ, and AddSub.", "label": "Supported", "paragraph": "For the vanilla prompting strategy, however, our method results in a performance drop on several benchmarks, including AQUA, MultiArith, SinlgeEQ, and AddSub. Without clear instruction (i.e., 'let's think step by step') for the chain-of-through mindset as in the CoT-prompting strategy, some general chat-based LLMs (e.g., ChatGPT, Claude) will likely keep performing chain-reasoning towards a final answer instead of writing the answer directly.", "section_name": "4.3 EVALUATION RESULTS", "subsection_name": "", "paper_id": "3jXCF5dNpC", "paper_title": "Re-Reading Improves Reasoning in Language Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:17:27.830235", "model": "gpt-5-mini", "original_claim": "Using the default (vanilla) prompting, our method reduced model performance on several evaluation benchmarks, notably AQUA, MultiArith, SingleEQ, and AddSub.", "rephrasing_timestamp": "2026-01-11T19:29:19.308685", "rephrasing_model": "gpt-5-mini"}
{"claim": "Six individuals are credited with authorship of the paper: Aymeric Capitaine, Antoine Scheid, Eric Moulines, Etienne Boursier, Michael I. Jordan, and Alain Durmus.", "label": "Supported", "paragraph": "Aymeric Capitaine 1\nAntoine Scheid 1\nEric Moulines 1\nEtienne Boursier 2\nMichael I. Jordan 3\nAlain Durmus 1\n- 1 Centre de Mathématiques Appliquées - CNRS - École polytechnique - Palaiseau, 91120, France\n- 2 INRIA Saclay, Université Paris Saclay, LMO - Orsay, 91400, France\n- 3 Inria, Ecole Normale Supérieure, PSL Research University - Paris, 75, France", "section_name": "Unravelling in Collaborative Learning", "subsection_name": "", "paper_id": "JfxqomOs60", "paper_title": "Unravelling in Collaborative Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:17:48.855271", "model": "gpt-5-mini", "original_claim": "The manuscript lists six authors: Aymeric Capitaine, Antoine Scheid, Eric Moulines, Etienne Boursier, Michael I. Jordan, and Alain Durmus.", "rephrasing_timestamp": "2026-01-11T19:29:15.125947", "rephrasing_model": "gpt-5-mini"}
{"claim": "The four listed researchers—Aymeric Capitaine, Antoine Scheid, Eric Moulines and Alain Durmus—are based at the Applied Mathematics unit of the French National Centre for Scientific Research (CNRS) housed at École Polytechnique in Palaiseau, France (91120).", "label": "Supported", "paragraph": "Aymeric Capitaine 1\nAntoine Scheid 1\nEric Moulines 1\nEtienne Boursier 2\nMichael I. Jordan 3\nAlain Durmus 1\n- 1 Centre de Mathématiques Appliquées - CNRS - École polytechnique - Palaiseau, 91120, France\n- 2 INRIA Saclay, Université Paris Saclay, LMO - Orsay, 91400, France\n- 3 Inria, Ecole Normale Supérieure, PSL Research University - Paris, 75, France", "section_name": "Unravelling in Collaborative Learning", "subsection_name": "", "paper_id": "JfxqomOs60", "paper_title": "Unravelling in Collaborative Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:17:48.855283", "model": "gpt-5-mini", "original_claim": "Four authors (Aymeric Capitaine, Antoine Scheid, Eric Moulines, and Alain Durmus) are affiliated with the Centre de Mathématiques Appliquées - CNRS - École polytechnique located in Palaiseau, France (91120).", "rephrasing_timestamp": "2026-01-11T19:29:18.597276", "rephrasing_model": "gpt-5-mini"}
{"claim": "For the analysis that follows, the authors take the entire list of agents' characteristics — θ1,…,θJ ∈ Θ^J — to be common information known to every participant.", "label": "Supported", "paragraph": "In this section, we assume that the profile of types ( θ 1 , . . . , θ J ) ∈ Θ J is public, and study how the aggregator can implement an optimal collaboration among agents under this most-favorable scenario.", "section_name": "3 Full-Information Benchmark: First-Best Collaboration", "subsection_name": "", "paper_id": "JfxqomOs60", "paper_title": "Unravelling in Collaborative Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:17:57.107132", "model": "gpt-5-mini", "original_claim": "The authors assume the full vector of agent types (θ1,...,θJ) ∈ Θ^J is publicly known to all parties for the subsequent analysis.", "rephrasing_timestamp": "2026-01-11T19:29:20.974419", "rephrasing_model": "gpt-5-mini"}
{"claim": "Assuming each participant's private characteristics are common knowledge, the section examines how a central coordinator can devise and implement the welfare-maximizing collaborative scheme among the players in this full-information setting.", "label": "Supported", "paragraph": "In this section, we assume that the profile of types ( θ 1 , . . . , θ J ) ∈ Θ J is public, and study how the aggregator can implement an optimal collaboration among agents under this most-favorable scenario.", "section_name": "3 Full-Information Benchmark: First-Best Collaboration", "subsection_name": "", "paper_id": "JfxqomOs60", "paper_title": "Unravelling in Collaborative Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:17:57.107147", "model": "gpt-5-mini", "original_claim": "Given that the type vector is public, the section analyzes how an aggregator can design and implement an optimal cooperative arrangement among the agents in this most-favorable information scenario.", "rephrasing_timestamp": "2026-01-11T19:29:22.711826", "rephrasing_model": "gpt-5-mini"}
{"claim": "For any 0–1 participation indicator B and any vector n of positive sample counts, the quantity just defined equals the greatest number of samples that agent j may be asked to contribute to the group without violating its participation bound, with n and u_j as specified previously.", "label": "Supported", "paragraph": "Exact solution. We are looking for a solution to the aggregator's problem (8). For any B ∈ { 0 , 1 } J and n ∈ R J + , denote by\n<!-- formula-not-decoded -->\nthe maximum number of samples that agent j can be asked to provide within the coalition under its participation constraint, where n is defined as in (6), and u j as in (7).", "section_name": "3 Full-Information Benchmark: First-Best Collaboration", "subsection_name": "", "paper_id": "JfxqomOs60", "paper_title": "Unravelling in Collaborative Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:18:05.368794", "model": "gpt-5-mini", "original_claim": "For any binary participation vector B and positive sample vector n, the defined quantity represents the maximal number of samples agent j can be asked to provide within the coalition while satisfying its participation constraint, with n and u_j defined earlier.", "rephrasing_timestamp": "2026-01-11T19:29:25.989286", "rephrasing_model": "gpt-5-mini"}
{"claim": "The presented method was validated on the CIFAR-10 benchmark using two lightweight convolutional networks—MobileNet‑V2 and EfficientNet‑B0—to assign class labels to images.", "label": "Supported", "paragraph": "We experiment the effectiveness of our proposed method using the CIFAR-10 dataset [25]. For the validation of image classification, we experiment our method with CNN models: MobileNet-V2 [43] and EfficientNet-B0 [45] We perform fine-tuning with only 100 epochs after processing pruning methods on CIFAR-10.", "section_name": "A.4 Experiments on CIFAR-10 Dataset", "subsection_name": "", "paper_id": "MYI443zCvv", "paper_title": "DEPrune: Depth-wise Separable Convolution Pruning for Maximizing GPU Parallelism", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:18:28.303662", "model": "gpt-5-mini", "original_claim": "The proposed approach was evaluated on the CIFAR-10 dataset using two convolutional neural network architectures: MobileNet-V2 and EfficientNet-B0 for image classification.", "rephrasing_timestamp": "2026-01-11T19:29:27.974133", "rephrasing_model": "gpt-5-mini"}
{"claim": "After sparsifying the models on the CIFAR-10 dataset, the team ran a brief retraining comprising one hundred full passes through the training data.", "label": "Supported", "paragraph": "We experiment the effectiveness of our proposed method using the CIFAR-10 dataset [25]. For the validation of image classification, we experiment our method with CNN models: MobileNet-V2 [43] and EfficientNet-B0 [45] We perform fine-tuning with only 100 epochs after processing pruning methods on CIFAR-10.", "section_name": "A.4 Experiments on CIFAR-10 Dataset", "subsection_name": "", "paper_id": "MYI443zCvv", "paper_title": "DEPrune: Depth-wise Separable Convolution Pruning for Maximizing GPU Parallelism", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:18:28.303674", "model": "gpt-5-mini", "original_claim": "After applying pruning procedures on CIFAR-10, the researchers fine-tuned the networks for only 100 training epochs.", "rephrasing_timestamp": "2026-01-11T19:29:29.560754", "rephrasing_model": "gpt-5-mini"}
{"claim": "In their tests the researchers found that filling alignment gaps with zero bytes added only a small extra memory footprint—about 0.3% of overall RAM usage.", "label": "Supported", "paragraph": "According to paper [41], the extra overhead in total memory consumption due to zero-padding is approximately 0.3%. To assess the impact of DEPrune-BH, we measured and presented the peak memory usage of MobileNet-V2 before and after applying DEPrune-BH with a 50% pruning ratio, as\nTable 8: Comparison between DCP and DCP-B of EfficientNet-B0 on CIFAR-10 dataset.", "section_name": "A.9 Peak Memory Usage", "subsection_name": "", "paper_id": "MYI443zCvv", "paper_title": "DEPrune: Depth-wise Separable Convolution Pruning for Maximizing GPU Parallelism", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:18:39.227801", "model": "gpt-5-mini", "original_claim": "The authors report that the additional overall memory overhead introduced by using zero-padding in their experiments amounted to roughly 0.3 percent of total memory consumption.", "rephrasing_timestamp": "2026-01-11T19:29:31.926540", "rephrasing_model": "gpt-5-mini"}
{"claim": "The team logged and published MobileNet‑V2's maximum memory footprint both prior to and after using DEPrune‑BH to remove half of its weights, in order to assess the method's impact on memory usage.", "label": "Supported", "paragraph": "According to paper [41], the extra overhead in total memory consumption due to zero-padding is approximately 0.3%. To assess the impact of DEPrune-BH, we measured and presented the peak memory usage of MobileNet-V2 before and after applying DEPrune-BH with a 50% pruning ratio, as\nTable 8: Comparison between DCP and DCP-B of EfficientNet-B0 on CIFAR-10 dataset.", "section_name": "A.9 Peak Memory Usage", "subsection_name": "", "paper_id": "MYI443zCvv", "paper_title": "DEPrune: Depth-wise Separable Convolution Pruning for Maximizing GPU Parallelism", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:18:39.227817", "model": "gpt-5-mini", "original_claim": "The researchers measured and reported MobileNet-V2's peak memory consumption both before and after applying DEPrune-BH with a 50% pruning ratio to evaluate DEPrune-BH's memory impact.", "rephrasing_timestamp": "2026-01-11T19:29:31.581625", "rephrasing_model": "gpt-5-mini"}
{"claim": "On EfficientNet-B0, when pointwise filters are reduced by 10% and depthwise filters are reduced by 10%, 20% and 50%, the DCP-B variant yields better CIFAR-10 performance than DCP — for example, 91.49% versus 91.25% at the 10% depthwise reduction.", "label": "Supported", "paragraph": "| EfficientNet-B0 on CIFAR-10   | EfficientNet-B0 on CIFAR-10   | EfficientNet-B0 on CIFAR-10   | EfficientNet-B0 on CIFAR-10   |\n|----------|----------|----------|----------|\n| Pruning Ratio          | Pruning Ratio          | Accuracy          | Accuracy          |\n| DW-conv          | PW-conv          | DCP          | DCP-B          |\n| 10%          | 10%          | 91.25%          | 91.49%          |\n| 20%          | 10%          | 91.18%          | 91.31%          |\n| 50%          | 10%          | 91.16%          | 91.33%          |\nshown in Table 9.", "section_name": "A.9 Peak Memory Usage", "subsection_name": "", "paper_id": "MYI443zCvv", "paper_title": "DEPrune: Depth-wise Separable Convolution Pruning for Maximizing GPU Parallelism", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:18:47.918586", "model": "gpt-5-mini", "original_claim": "With PW-conv pruning fixed at 10% and DW-conv pruning at 10%, 20%, and 50% on EfficientNet-B0, DCP-B achieves higher CIFAR-10 accuracies than DCP (e.g., 91.49% vs 91.25% at 10%).", "rephrasing_timestamp": "2026-01-11T19:29:34.479933", "rephrasing_model": "gpt-5-mini"}
{"claim": "Liu et al.'s 2016 in-store apparel retrieval split, drawn from a large portion of the DeepFashion collection, contains images with wide-ranging clothing viewpoints and varying object sizes.", "label": "Supported", "paragraph": "In-shop Clothes retrieval dataset (Liu et al., 2016) is a large subset of DeepFashion with large pose and scale variations. This dataset consists of a training set containing 25,882 images with 3997 classes, a gallery set containing 12,612 images with 3985 classes and a query set containing 14,218 images with 3985 classes.", "section_name": "A EXPERIMENT SETTING", "subsection_name": "", "paper_id": "EMCXCTsmSx", "paper_title": "IRGen: Generative Modeling for Image Retrieval", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:19:10.790885", "model": "gpt-5-mini", "original_claim": "The In-shop Clothes retrieval dataset (Liu et al., 2016) represents a substantial subset of the DeepFashion corpus and exhibits pronounced variation in garment pose and image scale.", "rephrasing_timestamp": "2026-01-11T19:29:34.903638", "rephrasing_model": "gpt-5-mini"}
{"claim": "The dataset is organized into three subsets: a training set with 25,882 images spanning 3,997 categories, a reference set of 12,612 images covering 3,985 categories, and a probe set of 14,218 images covering 3,985 categories.", "label": "Supported", "paragraph": "In-shop Clothes retrieval dataset (Liu et al., 2016) is a large subset of DeepFashion with large pose and scale variations. This dataset consists of a training set containing 25,882 images with 3997 classes, a gallery set containing 12,612 images with 3985 classes and a query set containing 14,218 images with 3985 classes.", "section_name": "A EXPERIMENT SETTING", "subsection_name": "", "paper_id": "EMCXCTsmSx", "paper_title": "IRGen: Generative Modeling for Image Retrieval", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:19:10.790896", "model": "gpt-5-mini", "original_claim": "The dataset is split into a training partition with 25,882 images covering 3,997 classes, a gallery partition with 12,612 images covering 3,985 classes, and a query partition with 14,218 images covering 3,985 classes.", "rephrasing_timestamp": "2026-01-11T19:29:38.107847", "rephrasing_model": "gpt-5-mini"}
{"claim": "Across our trials, we fitted the models using examples drawn jointly from the dataset's learning subset and the gallery pool, instead of relying only on the learning subset.", "label": "Supported", "paragraph": "The goal is to retrieve the same clothes from the gallery set given a fashion image from the query set. We use both the training set and the gallery set for training in our experiments. CUB200 (Wah et al., 2011) is a fine-grained dataset containing 11,788 images with 200 classes belong to birds.", "section_name": "A EXPERIMENT SETTING", "subsection_name": "", "paper_id": "EMCXCTsmSx", "paper_title": "IRGen: Generative Modeling for Image Retrieval", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:19:22.172332", "model": "gpt-5-mini", "original_claim": "In our experiments, we trained models using both the dataset's training split and the gallery set simultaneously rather than using only the training partition.", "rephrasing_timestamp": "2026-01-11T19:29:40.354205", "rephrasing_model": "gpt-5-mini"}
{"claim": "Wah et al.'s 2011 benchmark supplies a bird image collection of 11,788 photographs, allocated to 200 separate species for use in fine-grained recognition tasks.", "label": "Supported", "paragraph": "The goal is to retrieve the same clothes from the gallery set given a fashion image from the query set. We use both the training set and the gallery set for training in our experiments. CUB200 (Wah et al., 2011) is a fine-grained dataset containing 11,788 images with 200 classes belong to birds.", "section_name": "A EXPERIMENT SETTING", "subsection_name": "", "paper_id": "EMCXCTsmSx", "paper_title": "IRGen: Generative Modeling for Image Retrieval", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:19:22.172347", "model": "gpt-5-mini", "original_claim": "The CUB200 benchmark (Wah et al., 2011) comprises a fine-grained bird dataset containing 11,788 images distributed across 200 distinct bird classes.", "rephrasing_timestamp": "2026-01-11T19:29:38.726335", "rephrasing_model": "gpt-5-mini"}
{"claim": "The Cars196 collection consists of 16,185 photographs covering 196 vehicle categories, with 8,144 images assigned to training and 8,041 set aside for evaluation.", "label": "Supported", "paragraph": "There are 5,994 images for training and 5,794 images for testing. Cars196 (Krause et al., 2013) is also a fine-grained dataset about cars. It contains 16,185 images with 196 car classes, which is split into 8,144 images for training and 8,041 images for testing.", "section_name": "A EXPERIMENT SETTING", "subsection_name": "", "paper_id": "EMCXCTsmSx", "paper_title": "IRGen: Generative Modeling for Image Retrieval", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "extraction_timestamp": "2026-01-11T16:19:28.747610", "model": "gpt-5-mini", "original_claim": "The Cars196 dataset contains 16,185 images spanning 196 car classes, partitioned into 8,144 images used for training and 8,041 images reserved for testing.", "rephrasing_timestamp": "2026-01-11T19:29:38.745865", "rephrasing_model": "gpt-5-mini"}
{"claim": "Throughout the study, the research team carried out every trial using only one NVIDIA RTX A6000 graphics card, with no other accelerator hardware employed.", "label": "Supported", "paragraph": "We conduct all the experiments on single NVIDIA RTX A6000 GPU. The experimental environment is PyTorch 2.1.2 and CUDA 12.2.", "section_name": "A.1 Experimental Environment", "subsection_name": "", "paper_id": "ml01XyP698", "paper_title": "FreeSplat: Generalizable 3D Gaussian Splatting Towards Free View Synthesis of Indoor Scenes", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:19:50.030792", "model": "gpt-5-mini", "original_claim": "The authors ran every experiment using a single NVIDIA RTX A6000 GPU as the sole hardware accelerator for all experimental runs.", "rephrasing_timestamp": "2026-01-11T19:29:43.514632", "rephrasing_model": "gpt-5-mini"}
{"claim": "All runs were performed in an environment using PyTorch version 2.1.2 paired with CUDA 12.2; these were the chosen framework and GPU driver versions for the entire set of experiments.", "label": "Supported", "paragraph": "We conduct all the experiments on single NVIDIA RTX A6000 GPU. The experimental environment is PyTorch 2.1.2 and CUDA 12.2.", "section_name": "A.1 Experimental Environment", "subsection_name": "", "paper_id": "ml01XyP698", "paper_title": "FreeSplat: Generalizable 3D Gaussian Splatting Towards Free View Synthesis of Indoor Scenes", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:19:50.030804", "model": "gpt-5-mini", "original_claim": "The experimental software stack consisted of PyTorch 2.1.2 together with CUDA 12.2, which were the specific library and driver versions employed during all experiments.", "rephrasing_timestamp": "2026-01-11T19:29:48.719041", "rephrasing_model": "gpt-5-mini"}
{"claim": "The authors assessed the method's transfer performance on the Replica benchmark without any additional training, and the resulting figures are compiled in Table 4.", "label": "Supported", "paragraph": "We further evaluate the zero-shot transfer results through testing on Replica dataset, with results in Table 4. Our view interpolation and novel view depth estimation results still outperforms existing methods. The long sequence results degrade due to inaccurate depth estimation and domain gap, indicating potential future work in further improving the depth estimation in zero-shot tranferring.", "section_name": "5.3 Zero-Shot Transfer Results on Replica", "subsection_name": "", "paper_id": "ml01XyP698", "paper_title": "FreeSplat: Generalizable 3D Gaussian Splatting Towards Free View Synthesis of Indoor Scenes", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:20:02.257175", "model": "gpt-5-mini", "original_claim": "The paper reports zero-shot transfer evaluation results obtained by testing the method on the Replica dataset, with those outcomes summarized in Table 4.", "rephrasing_timestamp": "2026-01-11T19:29:48.449794", "rephrasing_model": "gpt-5-mini"}
{"claim": "On the Replica benchmark, in a direct-transfer evaluation without any additional training, our approach outperforms prior techniques at generating intermediate viewpoints and predicting depth for previously unseen camera positions.", "label": "Supported", "paragraph": "We further evaluate the zero-shot transfer results through testing on Replica dataset, with results in Table 4. Our view interpolation and novel view depth estimation results still outperforms existing methods. The long sequence results degrade due to inaccurate depth estimation and domain gap, indicating potential future work in further improving the depth estimation in zero-shot tranferring.", "section_name": "5.3 Zero-Shot Transfer Results on Replica", "subsection_name": "", "paper_id": "ml01XyP698", "paper_title": "FreeSplat: Generalizable 3D Gaussian Splatting Towards Free View Synthesis of Indoor Scenes", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:20:02.257191", "model": "gpt-5-mini", "original_claim": "The method produces superior view interpolation and novel-view depth estimation results compared to existing approaches in the zero-shot transfer experiments on the Replica dataset.", "rephrasing_timestamp": "2026-01-11T19:29:45.119220", "rephrasing_model": "gpt-5-mini"}
{"claim": "The approach employed 128 discrete depth hypotheses, represented features in a 64-dimensional space, set the near and far depth limits to 0.5 and 15.0 for the matching structure, and used a 0.05 sampling interval for computing pixel correspondences.", "label": "Supported", "paragraph": "We set the number of virtual depth planes K = 128 , matching feature dimension C m = 64 , and d near = 0 . 5 , d far = 15 . 0 for cost volume formulation, and set δ = 0 . 05 in Eq.(8) for pixel-wise alignment.", "section_name": "A.2 Additional Implementation Details", "subsection_name": "", "paper_id": "ml01XyP698", "paper_title": "FreeSplat: Generalizable 3D Gaussian Splatting Towards Free View Synthesis of Indoor Scenes", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "extraction_timestamp": "2026-01-11T16:20:12.166896", "model": "gpt-5-mini", "original_claim": "The method used 128 virtual depth planes, a feature dimension of 64, set d_near to 0.5 and d_far to 15.0 for the cost volume, and set delta to 0.05 for pixel-wise alignment.", "rephrasing_timestamp": "2026-01-11T19:29:52.698216", "rephrasing_model": "gpt-5-mini"}
{"claim": "The initial configuration message instructs the model to perform accurately and usefully while handling incoming information and delivering results in the specified structure.", "label": "Supported", "paragraph": "```\n/ / System prompt You are an p r e c i s e and h e l p f u l a s s i s t a n t . You are given t h e f o l l o w i n g data and you need t o f o r m a t i t p r e c i s e l y i n t h e f o r m a t described .", "section_name": "PersonalReddit Fixing Prompt", "subsection_name": "", "paper_id": "kmn0BhQk7p", "paper_title": "Beyond Memorization: Violating Privacy via Inference with Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T16:20:44.775309", "model": "gpt-5-mini", "original_claim": "The system prompt directs the assistant to act as a precise and helpful assistant while processing the provided data and producing the required output format.", "rephrasing_timestamp": "2026-01-11T19:29:50.910607", "rephrasing_model": "gpt-5-mini"}
{"claim": "The directive requires the assistant to output nothing but a JSON-formatted list consisting of two text values that denote the extracted claims.", "label": "Supported", "paragraph": "```\n/ / System prompt You are an p r e c i s e and h e l p f u l a s s i s t a n t . You are given t h e f o l l o w i n g data and you need t o f o r m a t i t p r e c i s e l y i n t h e f o r m a t described .", "section_name": "PersonalReddit Fixing Prompt", "subsection_name": "", "paper_id": "kmn0BhQk7p", "paper_title": "Beyond Memorization: Violating Privacy via Inference with Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T16:20:44.775331", "model": "gpt-5-mini", "original_claim": "The prompt mandates that the assistant return exclusively a JSON array containing two string elements that represent the extracted claim entries.", "rephrasing_timestamp": "2026-01-11T19:29:53.310142", "rephrasing_model": "gpt-5-mini"}
{"claim": "The directive mandates providing only the data in the prescribed arrangement and forbids adding any extra words, remarks, or unrelated material beyond that arranged content.", "label": "Supported", "paragraph": "Return nothing but t h e f o r m a t t e d data . / / Query prompt Below I give you some data t h a t does not e x a c t l y f o l l o w t h e f o r m a t t h a t I would l i k e . The data c o n s i s t s of answers .", "section_name": "PersonalReddit Fixing Prompt", "subsection_name": "", "paper_id": "kmn0BhQk7p", "paper_title": "Beyond Memorization: Violating Privacy via Inference with Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T16:20:57.983933", "model": "gpt-5-mini", "original_claim": "The instruction requires returning exclusively the formatted data and prohibits including any additional content, commentary, or extraneous material beyond that formatted output.", "rephrasing_timestamp": "2026-01-11T19:29:56.228078", "rephrasing_model": "gpt-5-mini"}
{"claim": "The passage explains that the provided collection contains response items and clearly warns that those items do not fully conform to the formatting instructions requested by the user.", "label": "Supported", "paragraph": "Return nothing but t h e f o r m a t t e d data . / / Query prompt Below I give you some data t h a t does not e x a c t l y f o l l o w t h e f o r m a t t h a t I would l i k e . The data c o n s i s t s of answers .", "section_name": "PersonalReddit Fixing Prompt", "subsection_name": "", "paper_id": "kmn0BhQk7p", "paper_title": "Beyond Memorization: Violating Privacy via Inference with Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T16:20:57.983950", "model": "gpt-5-mini", "original_claim": "The paragraph indicates the supplied dataset is composed of answer entries and explicitly notes that these entries do not exactly adhere to the formatter preferences specified by the requester.", "rephrasing_timestamp": "2026-01-11T19:30:00.447682", "rephrasing_model": "gpt-5-mini"}
{"claim": "You must transform the supplied information to fit the specified templates precisely and capture details from the initial reply with the highest attainable accuracy.", "label": "Supported", "paragraph": "Your t a s k i s t o f o r m a t t h i s data i n e x a c t l y t h e f o r m a t s p e c i f i e d below . \\ n \\ nData : { f u l l o r i g i n a l a n s w e r } Extract t h e i n f o r m a t i o n from t h e given answer as p r e c i s e l y as p o s s i b l e .", "section_name": "PersonalReddit Fixing Prompt", "subsection_name": "", "paper_id": "kmn0BhQk7p", "paper_title": "Beyond Memorization: Violating Privacy via Inference with Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "extraction_timestamp": "2026-01-11T16:21:06.444561", "model": "gpt-5-mini", "original_claim": "The assignment instructs to reorganize the provided data to match the exact formats specified and to extract information from the original answer with maximal precision.", "rephrasing_timestamp": "2026-01-11T19:29:58.379538", "rephrasing_model": "gpt-5-mini"}

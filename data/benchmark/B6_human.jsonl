{"claim": "The paper lacks ablative experiments analyzing the impact of removing the low-pass filter on low-frequency versus high-frequency information and on accuracy and generalization.", "claim_type": "experimental", "paper_id": "Cf4FJGmHRQ", "paper_title": "PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "1Zfm29TuCw", "reviewer": "Reviewer_nwGj", "review_text": "Summary: This work has developed a neural network architecture for image recognition that is designed to address the influence of complex degradation factors. It aims to capture both low-frequency and high-frequency components to balance accuracy and generalization. The authors first propose to discard the low-pass filters in the existing FNO structure to retain all frequency components. Subsequently, a parallel structure is introduced to further enhance the utilization of frequency domain information. Finally, the authors design a two-stage training strategy to ensure performance stability.\n\nStrengths: 1. The overall paper has a clear logical structure, and the explanation of the methodology and the presentation of the constructed mechanisms are intuitive and easy to understand.\n2. The author provides a sufficiently detailed explanation for the motivation behind each component in PAC-FNO.\n3. The problem that this work aims to address holds a certain degree of practical application value.\n\nWeaknesses: 1. The abandonment of the low-pass filter is one of the main innovations in this work. Although the author provides an explanation for the motivation behind this operation, it is still recommended that the author conduct ablative experiments to analyze the impact of low-frequency/high-frequency information on accuracy/generalization.\n2. As for parallel architecture, the relevant experimental results have indeed proven its effectiveness. However, the explanation of parallel architecture in the method section appears somewhat lacking. It is hoped that the author can provide further analysis of the mechanism that enables it to be effective.\n3. In terms of comparative experiments, the methods used by the author for comparison appear to be lacking in both quantity and novelty. The comprehensiveness of the complex scenarios considered by the author is commendable, but it is hoped that the author can still increase the comparison results with more advanced works to more effectively validate the superiority of the proposed method.\n4. The author mentions the advantages of this work in terms of efficiency, but it seems that no experimental analysis related to efficiency has been provided (such as FLOPs and runtime on data at different resolutions).\n\nQuestions: Please refer to the Weaknesses.", "labeling_timestamp": "2026-01-11T16:21:29.981047", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The authors state they added ablation experiments specifically examining low-pass vs high-pass vs combined filters (Table 21, Appendix F.7) and report results showing how each choice affects accuracy and generalization on ImageNet-1k and ImageNet-C/P Fog.", "evidence": "\"We provide an analysis of the impact of low and high-frequency information on accuracy/generalization through ablation experiments in Table 21 in Appendix F.7 of revision. Compared to PAC-FNO, accuracy and generalization decrease when using low-pass filter or high-pass filter.\"", "combined_author_response": "We are happy to hear that your major concern has been resolved. We hope our additional answers will also help you with your problems.\n> 1. Some degradation-resistant algorithms seem to do the same thing as you and I want to see further discussions in your paper to make me better understand your contribution, such as Degradation-Resistant Unfolding Network for Heterogeneous Image Fusion (ICCV 23) and HQG-Net: Unpaired Medical Image Enhancement with High-Quality Guidance (TNNLS).\n\n$\\to$ Thank you for pointing out the work. In a nutshell, both papers aim to convert low-quality images into high-quality images.\n[1] generates a high-quality image by fusing low-quality images generated from multiple sensors. [2] generates high-quality medical images using a model trained with unpaired low-quality medical images and high-quality medical images.\n\nHowever, there is a difference between the two studies and our PAC-FNO. PAC-FNO handles low-quality images including low-resolution and input variations. The low-quality images in these two studies include only input variations such as images with fog or noise. In other words, PAC-FNO handles not only the input variations mentioned in those studies, but also low-resolution images, which occurs more frequently in real world deployment scenarios, *e.g.*, CCTV. \n\n[1] Degradation-Resistant Unfolding Network for Heterogeneous Image Fusion (ICCV 23)\n\n[2] HQG-Net: Unpaired Medical Image Enhancement with High-Quality Guidance (TNNLS)\n\n\n> 2. Although this paper focuses on solving the recognition problem, I'd like to see the author expand it to other extreme tasks, such as camouflaged object segmentation [1, 2] and concealed object segmentation [3]. Perhaps there isn't enough time for the author to continue with the corresponding experiments, but I think it's also more useful to see the author's point of view on a high level for other interested readers to get inspiration from this paper.\n\n$\\to$ Very interesting suggestion!  As PAC-FNO addresses the semantic mapping (*i.e.*, classification) problem as it has the most wide applicability, we believe it could be extended to suggested extreme segmentation tasks with a trivial extension. Since the the dense (*i.e.*, pixel-wise) prediction task is essentially the semantic mapping (*i.e.*, classification) problem by taking into account the nearby pixels, given sufficiently large receptive fields tuned by convolution filters for CNNs or token patches for transformers. Empirical validation on this suggestion would be a great future work to widen the applicability of the PAC-FNO. Thank you!\n\n---\n\nDear Reviewer pDdt,\n\nWe appreciate the reviewer’s time and effort in reviewing our manuscript and insightful comments. \n\nAs the closure of the discussion period is approaching, we would like to bring the review’s attention and check if the reviewer could let us know whether the concerns or the misunderstanding have been addressed. \n\nIf this is the case, we would appreciate if you could adjust your rating accordingly. \n\nBest regards,\n\nAuthors\n\n---\n\nDear Reviewer WCy8,\n\nWe appreciate the reviewer’s time and effort in reviewing our manuscript and insightful comments. \n\nAs the closure of the discussion period is approaching, we would like to bring the review’s attention and check if the reviewer could let us know whether the concerns or the misunderstanding have been addressed. \n\nIf this is the case, we would appreciate if you could adjust your rating accordingly. \n\nBest regards,\n\nAuthors\n\n---\n\nDear Reviewer nwGj,\n\nWe appreciate the reviewer’s time and effort in reviewing our manuscript and insightful comments. \n\nAs the closure of the discussion period is approaching, we would like to bring the review’s attention and check if the reviewer could let us know whether the concerns or the misunderstanding have been addressed. \n\nIf this is the case, we would appreciate if you could adjust your rating accordingly. \n\nBest regards,\n\nAuthors\n\n---\n\nWe provide an additional comparison to the method equipped with the latest super-resolution model. The results are in the following table. OSRT [1] is a state-of-the-art model in the super-resolution domain but it does not support $\\times$8 upscale. Therefore, we only use the $\\times$2 and $\\times$4 upscale models of OSRT. In addition, we report OSRT (fine-tune) that combines super-resolution and fine-tuning methods. That is, we fine-tune the pre-trained classification model with low-resolution images upscaled by a super-resolution model.  As a result, PAC-FNO shows better performance than OSRT and OSRT (fine-tune) methods. We also report these results in Table 13 in Appendix F.3 of the revision. \n\n|  Imagenet-1k  |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |  224 |\n|:-------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:----:|\n|              |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   61.4   |     -    |   75.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |   -  |\n|  ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   71.2   |     -    |   78.4   |     -    | 82.1 |\n|               | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |   -  |\n|               |   PAC-FNO   |    Top-1 Acc (%)    |  **58.9**   | **63.2** | **77.6** | **76.2** | **80.2** | **80.7** | 81.8 |\n|              |             | # of Parameters (M) |          |          |          |   3.65   |          |          |      |\n\n| Imagenet-C/P Fog |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |    224   |\n|:----------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n|                 |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   19.4   |     -    |   37.9   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |     -    |\n| ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   42.3   |     -    |   56.4   |     -    |   59.4   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |     -    |\n|                  |   PAC-FNO   |    Top-1 Acc (%)    | **25.4** | **30.4** | **48.2** | **51.7** | **60.1** | **61.4** | **62.8** |\n|         `        |             | # of Parameters (M) |          |          |          |   3.65   |          |          |          |\n\n[1] Yu, Fanghua, et al. \"OSRT: Omnidirectional image super-resolution with distortion-aware transformer.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n---\n\nWe provide an additional comparison to the method equipped with the latest super-resolution model. The results are in the following table. OSRT [1] is a state-of-the-art model in the super-resolution domain but it does not support $\\times$8 upscale. Therefore, we only use the $\\times$2 and $\\times$4 upscale models of OSRT. In addition, we report OSRT (fine-tune) that combines super-resolution and fine-tuning methods. That is, we fine-tune the pre-trained classification model with low-resolution images upscaled by a super-resolution model.  As a result, PAC-FNO shows better performance than OSRT and OSRT (fine-tune) methods. We also report these results in Table 13 in Appendix F.3 of the revision. \n\n|  Imagenet-1k  |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |  224 |\n|:-------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:----:|\n|              |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   61.4   |     -    |   75.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |   -  |\n|  ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   71.2   |     -    |   78.4   |     -    | 82.1 |\n|               | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |   -  |\n|               |   PAC-FNO   |    Top-1 Acc (%)    |  **58.9**   | **63.2** | **77.6** | **76.2** | **80.2** | **80.7** | 81.8 |\n|              |             | # of Parameters (M) |          |          |          |   3.65   |          |          |      |\n\n| Imagenet-C/P Fog |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |    224   |\n|:----------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n|                 |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   19.4   |     -    |   37.9   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |     -    |\n| ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   42.3   |     -    |   56.4   |     -    |   59.4   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |     -    |\n|                  |   PAC-FNO   |    Top-1 Acc (%)    | **25.4** | **30.4** | **48.2** | **51.7** | **60.1** | **61.4** | **62.8** |\n|         `        |             | # of Parameters (M) |          |          |          |   3.65   |          |          |          |\n\n[1] Yu, Fanghua, et al. \"OSRT: Omnidirectional image super-resolution with distortion-aware transformer.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n---\n\nWe provide an additional comparison to the method equipped with the latest super-resolution model. The results are in the following table. OSRT [1] is a state-of-the-art model in the super-resolution domain but it does not support $\\times$8 upscale. Therefore, we only use the $\\times$2 and $\\times$4 upscale models of OSRT. As a result, PAC-FNO shows better performance than OSRT and OSRT (fine-tune) methods. We also report these results in Table 13 in Appendix F.3 of the revision. \n\n|  Imagenet-1k  |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |  224 |\n|:-------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:----:|\n|               |     DBPN    |    Top-1 Acc (%)    |   40.7   |     -    |   68.2   |     -    |   79.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |   23.21  |     -    |   10.43  |     -    |   5.95   |     -    |   -  |\n|               |     DBPN    |    Top-1 Acc (%)    |   **60.8**   |     -    |   72.5   |     -    |   76.7   |     -    | 82.5 |\n|               | (Fine-tune) | # of Parameters (M) | 23.21 |     -    |   10.43   |     -    |   5.95   |     -    | - |\n| ConvNeXt-Tiny |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   61.4   |     -    |   75.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |   -  |\n|               |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   71.2   |     -    |   78.4   |     -    | 82.1 |\n|               | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |   -  |\n|               |   PAC-FNO   |    Top-1 Acc (%)    |   58.9   | **63.2** | **77.6** | **76.2** | **80.2** | **80.7** | 81.8 |\n|              |             | # of Parameters (M) |          |          |          |   3.65   |          |          |      |\n\n| Imagenet-C/P Fog |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |    224   |\n|:----------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n|                  |     DBPN    |    Top-1 Acc (%)    |   0.67   |     -    |   0.99   |     -    |   1.32   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |   23.21  |     -    |   10.43  |     -    |   5.95   |     -    |     -    |\n|                  |     DBPN    |    Top-1 Acc (%)    |   21.8   |     -    |   42.3   |     -    |   56.8   |     -    |   61.0   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   10.43  |     -    |   5.95   |     -    |     -    |\n|   ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   19.4   |     -    |   37.9   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |     -    |\n|                  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   42.3   |     -    |   56.4   |     -    |   59.4   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |     -    |\n|                  |   PAC-FNO   |    Top-1 Acc (%)    | **25.4** | **30.4** | **48.2** | **51.7** | **60.1** | **61.4** | **62.8** |\n|         `        |             | # of Parameters (M) |          |          |          |   3.65   |          |          |\n\n\n[1] Yu, Fanghua, et al. \"OSRT: Omnidirectional image super-resolution with distortion-aware transformer.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n---\n\nDear All Reviewers,\n\nWe uploaded a new version of the revision which contains results for the state-of-the-art model in the super-resolution domain. We revised the following points and uploaded a new version:\n1. We added additional experimental results of the super-resolution method in Table 13 in Appendix F.3. \n2. We added experimental results on fine-grained datasets in ViT models in Tables 16 and 17  in Appendix F.4.\n3. We added additional ablation studies results of the performance of PAC-FNO according to low and high-frequency filters in Table 21 in Appendix F.7.\n4. We describe the effectiveness of parallel architecture and how it affects input variation in terms of frequency in Figure 7 and Figure 8 in Appendix F.8.\n5. We added FLOPs and runtime on data at different resolutions in Table 22 in Appendix F.9.\n\nBest regards,\nAuthors\n\n---\n\n> 3. The original intent behind the existence of ideal low-pass filters was to reduce the number of parameters and computational complexity. While the author's innovative design to remove the inherent low-pass filter is intuitively comprehensible, the associated trade-offs are not discussed in the manuscript. It would be beneficial to provide supplementary explanations to demonstrate the worthiness of such a modification.\n\n$\\to$ We provide FLOPs and runtime on data at different resolutions. Existing FNO models used low-pass filters due to computational complexity, but the channel size was increased to fully utilize low-frequency components. However, PAC-FNO maintained the 3 channel size of the image to use both high-pass filter and low-pass filter. As a result, PAC-FNO showed similar levels of FLOPs and Runtimes as existing FNO models, but showed better performance because it used additional high-frequency components.\n\n| ImageNet-1k |    Metrics   |    28   |   56   |   112   |  224  |\n|:-----------:|:------------:|:-------:|:------:|:-------:|:-----:|\n|    Resize   |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.006  |  0.006 |  0.006  | 0.006 |\n|  Fine-tune  |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.006  |  0.006 |  0.006  | 0.006 |\n|     DRLN    |    GFLOPs    |  180.66 | 412.05 | 1200.50 |  8.96 |\n|             | Runtimes (s) |  0.378  |  0.498 |  0.913  | 0.007 |\n|     DRPN    |    GFLOPs    | 1220.42 | 576.94 |  387.88 |  8.96 |\n|             | Runtimes (s) |  0.1532 |  0.164 |  0.171  | 0.007 |\n|     FNO     |    GFLOPs    |   9.78  |  9.78  |   9.78  |  9.78 |\n|             | Runtimes (s) |  0.016  |  0.016 |  0.016  | 0.016 |\n|     UNO     |    GFLOPs    |   9.10  |  9.10  |   9.10  |  9.10 |\n|             | Runtimes (s) |  0.018  |  0.018 |  0.018  | 0.018 |\n|     AFNO    |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.010  |  0.010 |  0.010  | 0.010 |\n|   PAC-FNO   |    GFLOPs    |   8.98  |  8.98  |   8.98  |  8.98 |\n|             | Runtimes (s) |  0.013  |  0.013 |  0.013  | 0.013 |\n\n> 4. The experiments thoroughly prove the advantages of parallel architectures and claim that this approach encapsulates more frequency components. However, they lack further detailed explanations and justifications.\n\n$\\to$ The parallel configuration of AC-FNO blocks captures more information than a serial structured model in the first layer, which is directly related to the data. We visualize what frequency the parallel configuration and serial configuration capture in Figure 7 in Appendix F.8 of revision. Figure 7 shows that the parallel configuration captures more high frequencies than the serial configuration. In other words, the parallel structured model captured both more low-frequency and high-frequency components than the serial structured model. As a result, parallel configuration of AC-FNO blocks show a better performance than serial configuration in Figure 4 in Section 4.3. We will add this explanation and justification in Appendix F.8 of the revision. Thank you!\n\n---\n\nThank you for the encouraging remarks about our contribution and extensive experimental results and the valuable feedback from the reviewer. We hope that our responses will solve the reviewer’s questions.\n\n> 1. Although the author compared super-resolution (SR) models for variable resolution inputs, the compared SR models are outdated and lack representation across various upscaling factors for super-resolution reconstruction. The field of super-resolution has seen significant advancements recently; thus, it is recommended to select more appropriate comparative algorithms.\n\n$\\to$ As suggested, we conduct the experiments of other methods with a state of the art SR model and summarize the results in the revision (will update it by  November 20th (AOE)). \n\n> 2. The primary advantage of Fourier Neural Operators (FNOs) lies in their use of frequency domain processing for resolution invariance. As a learnable enhancement operator, it's expected to exhibit some resilience to input natural variations. However, the author hasn't provided a detailed and explanatory analysis of the mechanisms where the operator shows robustness against natural variations. Moreover, the chosen input variations in the experiments, like fog, brightness, spatter, and saturate, represent basic degradation scenarios that can be addressed without deep learning methods. Therefore, regarding resilience to input natural variations, this might not be sufficiently emphasized as a highlight of the paper. The paper suggests exploring degradation in real-world scenarios in future work, indicating that the authors are aware of the limitations in terms of experimental performance or the algorithm proposed. However, such scenarios represent fundamental problems studied in the field of Image Recognition (IR) and hold significant practical application implications. Actually, certain degradation processes might affect high or low frequency details in the image's frequency domain. For instance, blur involves the loss of high-frequency details, prompting the author to conduct a mechanistic analysis combining frequency domain and degradation processes to enhance this aspect's interpretability.\n\n$\\to$ As you mentioned, degradations change the high-frequency and low-frequency information in original images. Figure 8 in the Appendix F.8 shows that there are many changes in high-frequency when degradations are visualized in the frequency domain. Since existing FNO-based models that use a low-pass filter remove high-frequency, the degraded image loses not only the degradation factor but also the information of the original image. However, our proposed PAC-FNO uses high-frequency information, so it shows better performance in degradation.\nRegarding the future work, we would like to address combined degradation types as the natural variations (e.g., mixed with motion blur and fog that may occur in the real-world), not try to address new types of variations.\n\n---\n\nThank you for the encouraging remarks about our paper’s extensive experimental results and organizing and the valuable feedback from the reviewer. We hope that our responses will solve the reviewer’s questions.\n\n> 1. Is there a more advanced choice for the SR baseline model used for comparison in your experimental setup? This will affect the fairness of the performance of your experiment?\n\n$\\to$ For fairness of experiment in SR baseline, we will report additional baseline equipped with the latest super-resolution model in Table 13 in Appendix F.3 of the revision that will be updated shortly around November 20th AoE.\n\n> 2. It can be found that in ViT-B16, PAC-FNO shows not very good results at all low resolutions compared to other methods. What caused this phenomenon to occur? Is your method also unfriendly to other Transformer methods?\n\n$\\to$ PAC-FNO generally performs well in low resolution but only worse than A-FNO. A-FNO is a model proposed as a token mixer for transformers, so it seems particularly friendly when combined with Transformer-based models. \n\n> 3. The ideal low-pass filter in the FNO block removes detailed image signals that play an important role in classification in the fine-grained dataset. Is this conclusion applicable to Transformer based image classification methods? More quantitative results should be provided to confirm the universality of the proposed method.\n\n$\\to$ Yes, high-frequency components play a more important role in image classification, especially fine-grained datasets. So, to see the effect of high frequency component removal for nuanced classification problem, we conduct  a fine-grained classification experiment with the ViT model. On the Oxford-IIIT Pets dataset, PAC-FNO, which captures the high-frequency components, performs better at all resolutions than FNO with an ideal low-pass filter.compared to FNO with an ideal low-pass filter. We add these results in Tables 16 and 17 in Appendix F.4 of the revision.\n\n| Oxford-IIIT Pets |  28  |  32  |  56  |  64  |  112 |  128 |  224 |\n|:----------------:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|\n|        FNO       | 26.3 | 33.0 | 58.1 | 64.8 | 82.9 | 85.8 | 91.3 |\n|      PAC-FNO     | 40.3 | 46.2 | 69.0 | 72.5 | 86.8 | 89.2 | 92.2 |\n\n> 4. The ablation experiments about the results of the zero-padding operation and the exclusion of the low pass filter need to be completed to explain the design of the AC-FNO block. \n\n$\\to$ Zero-padding works to upscale low-resolution images that are smaller than the target resolution in the frequency domain to the target resolution. In other words, if there is no zero-padding operation, low-resolution images cannot be processed. \nThen, we provide the ablation experiments for low-pass filters in the first row in Table 21 in Appendix F.7 of the revision. When a low-pass filter is used, performance decreases in terms of accuracy because high-frequency components cannot be considered, but the performance decrease is large for input variation.\n\n|           ImageNet-1k           |  32  |  64  |  128 |  224 |\n|:-------------------------------:|:----:|:----:|:----:|:----:|\n|  PAC-FNO (low pass filter) | 53.5 | 71.4 | 78.7 | 79.0 |\n|             PAC-FNO             | 58.9 | 74.5 | 80.2 | 81.5 |\n\n|         ImageNet-C/P Fog        |  32  |  64  |  128 |  224 |\n|:-------------------------------:|:----:|:----:|:----:|:----:|\n|  PAC-FNO (low pass filter) | 18.0 | 41.7 | 52.4 | 54.4 |\n|             PAC-FNO             | 25.4 | 48.2 | 60.1 | 62.8 |\n\n---\n\n> 3. In terms of comparative experiments, the methods used by the author for comparison appear to be lacking in both quantity and novelty. The comprehensiveness of the complex scenarios considered by the author is commendable, but it is hoped that the author can still increase the comparison results with more advanced works to more effectively validate the superiority of the proposed method.\n\n$\\to$ We believe that advanced work to effectively validate the superiority of our method is an advanced super-resolution method, as super-resolution models are relatively old models. Therefore, we provide additional comparison to the method equipped with the latest super-resolution model in Appendix F.4 of the revision that will be updated shortly around November 20th AoE. Furthermore, we report additional baselines that combine super-resolution and fine-tuning methods in rows 5-6 in Table 13 in Appendix F.4 of the revised paper. That is, we fine-tune the pre-trained classification model with low-resolution images upscaled by a super-resolution model. This method showed worse performance than our PAC-FNO. \nIn addition, super-resolution methods are needed for each resolution. In other words, upscaling models of x8, x4, and x2 are needed to handle 28, 56, and 112 resolution, respectively. In contrast, our proposed PAC-FNO can handle images of all resolutions with an additional 3.65M network and shows good performance.\nIf the advanced works you intend are not super-resolution models, please let us know.\n\n> 4. The author mentions the advantages of this work in terms of efficiency, but it seems that no experimental analysis related to efficiency has been provided (such as FLOPs and runtime on data at different resolutions).\n\n$\\to$ We did not mention the efficiency as the advantage of this work. Instead, we stated \"efficacy of our neural operator-based mechanism\" (not the “efficiency”) in the last paragraph of Section 4.4 is that neural operator-based mechanism is suitable for real-world applications because it can handle a variety of resolutions without the process of resizing to the target resolution. But for curiosity, we also compare the efficiency of our method to the other methods in the following table:\n\n| ImageNet-1k |    Metrics   |    28   |   56   |   112   |  224  |\n|:-----------:|:------------:|:-------:|:------:|:-------:|:-----:|\n|    Resize   |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.006  |  0.006 |  0.006  | 0.006 |\n|  Fine-tune  |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.006  |  0.006 |  0.006  | 0.006 |\n|     DRLN    |    GFLOPs    |  180.66 | 412.05 | 1200.50 |  8.96 |\n|             | Runtimes (s) |  0.378  |  0.498 |  0.913  | 0.007 |\n|     DRPN    |    GFLOPs    | 1220.42 | 576.94 |  387.88 |  8.96 |\n|             | Runtimes (s) |  0.1532 |  0.164 |  0.171  | 0.007 |\n|     FNO     |    GFLOPs    |   9.78  |  9.78  |   9.78  |  9.78 |\n|             | Runtimes (s) |  0.016  |  0.016 |  0.016  | 0.016 |\n|     UNO     |    GFLOPs    |   9.10  |  9.10  |   9.10  |  9.10 |\n|             | Runtimes (s) |  0.018  |  0.018 |  0.018  | 0.018 |\n|     AFNO    |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.010  |  0.010 |  0.010  | 0.010 |\n|   PAC-FNO   |    GFLOPs    |   8.98  |  8.98  |   8.98  |  8.98 |\n|             | Runtimes (s) |  0.013  |  0.013 |  0.013  | 0.013 |\n\nAs a result, PAC-FNO showed the most efficiency in terms of FLOPs and runtimes except AFNO.\n\n---\n\nThank you for the encouraging remarks about our paper’s clear methodology explanation and the valuable feedback from the reviewer. We hope that our responses will solve the reviewer’ questions.\n\n> 1. The abandonment of the low-pass filter is one of the main innovations in this work. Although the author provides an explanation for the motivation behind this operation, it is still recommended that the author conduct ablative experiments to analyze the impact of low-frequency/high-frequency information on accuracy/generalization.\n\n$\\to$ Thank you for the suggestion! We provide an analysis of the impact of low and high-frequency information on accuracy/generalization through ablation experiments in Table 21 in Appendix F.7 of revision. Compared to PAC-FNO, accuracy and generalization decrease when using low-pass filter or high-pass filter. PAC-FNO with low pass filter show similar performance in ImageNet-1k compared with our PAC-FNO, but show a decrease in performance in terms of generalization in ImageNet-C/P Fog. On the other hand, when using a high-pass filter, it is expected to show good performance in ImageNet-C/P Fog, but it does not show good performance in terms of generalization because the performance is also poor in ImageNet-1k. Therefore, PAC-FNO, which uses both low-frequency and high-frequency components, only shows good performance in terms of accuracy/generalization. \n\n|           ImageNet-1k           |  32  |  64  |  128 |  224 |\n|:-------------------------------:|:----:|:----:|:----:|:----:|\n|  PAC-FNO (low pass filter) | 53.5 | 71.4 | 78.7 | 79.0 |\n| PAC-FNO (high pass filter) | 21.6 | 49.4 | 68.2 | 74.8 |\n|             PAC-FNO             | 58.9 | 74.5 | 80.2 | 81.5 |\n\n|         ImageNet-C/P Fog        |  32  |  64  |  128 |  224 |\n|:-------------------------------:|:----:|:----:|:----:|:----:|\n|  PAC-FNO (low pass filter) | 18.0 | 41.7 | 52.4 | 54.4 |\n| PAC-FNO (high pass filter) |  5.92  | 23.0 | 43.2 | 50.2 |\n|             PAC-FNO             | 25.4 | 48.2 | 60.1 | 62.8 |\n\n> 2. As for parallel architecture, the relevant experimental results have indeed proven its effectiveness. However, the explanation of parallel architecture in the method section appears somewhat lacking. It is hoped that the author can provide further analysis of the mechanism that enables it to be effective.\n\n$\\to$ The reason that it is effective is that the parallel configuration captures both low and high-frequency components while the serial architecture captures low-frequency components only (first paragraph in Sec. 3.2). Both low and high-frequency components must be captured to achieve good accuracy/generalization. This can be confirmed in Table 21 in Appendix F.7 in the revision. We visualize what frequency the parallel configuration and serial configuration capture in Figure 7 in Appendix F.8 of revision. Figure 7 shows that the parallel configuration more captures high frequencies than the serial configuration.\n\n---\n\nDear All Reviewers,\n\nWe thank the reviewers for taking the time to read, evaluate, and provide valuable feedback. \nWe upload a rebuttal revision that includes feedback from reviewers. Responses to reviewers feedback are highlighted in blue. \nResults for the state of the art super resolution model will be updated shortly around November 20th AoE.\n\nBest regards,\nAuthors", "author_response": "We provide an additional comparison to the method equipped with the latest super-resolution model. The results are in the following table. OSRT [1] is a state-of-the-art model in the super-resolution domain but it does not support $\\times$8 upscale. Therefore, we only use the $\\times$2 and $\\times$4 upscale models of OSRT. As a result, PAC-FNO shows better performance than OSRT and OSRT (fine-tune) methods. We also report these results in Table 13 in Appendix F.3 of the revision. \n\n|  Imagenet-1k  |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |  224 |\n|:-------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:----:|\n|               |     DBPN    |    Top-1 Acc (%)    |   40.7   |     -    |   68.2   |     -    |   79.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |   23.21  |     -    |   10.43  |     -    |   5.95   |     -    |   -  |\n|               |     DBPN    |    Top-1 Acc (%)    |   **60.8**   |     -    |   72.5   |     -    |   76.7   |     -    | 82.5 |\n|               | (Fine-tune) | # of Parameters (M) | 23.21 |     -    |   10.43   |     -    |   5.95   |     -    | - |\n| ConvNeXt-Tiny |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   61.4   |     -    |   75.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |   -  |\n|               |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   71.2   |     -    |   78.4   |     -    | 82.1 |\n|               | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |   -  |\n|               |   PAC-FNO   |    Top-1 Acc (%)    |   58.9   | **63.2** | **77.6** | **76.2** | **80.2** | **80.7** | 81.8 |\n|              |             | # of Parameters (M) |          |          |          |   3.65   |          |          |      |\n\n| Imagenet-C/P Fog |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |    224   |\n|:----------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n|                  |     DBPN    |    Top-1 Acc (%)    |   0.67   |     -    |   0.99   |     -    |   1.32   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |   23.21  |     -    |   10.43  |     -    |   5.95   |     -    |     -    |\n|                  |     DBPN    |    Top-1 Acc (%)    |   21.8   |     -    |   42.3   |     -    |   56.8   |     -    |   61.0   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   10.43  |     -    |   5.95   |     -    |     -    |\n|   ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   19.4   |     -    |   37.9   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |     -    |\n|                  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   42.3   |     -    |   56.4   |     -    |   59.4   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |     -    |\n|                  |   PAC-FNO   |    Top-1 Acc (%)    | **25.4** | **30.4** | **48.2** | **51.7** | **60.1** | **61.4** | **62.8** |\n|         `        |             | # of Parameters (M) |          |          |          |   3.65   |          |          |\n\n\n[1] Yu, Fanghua, et al. \"OSRT: Omnidirectional image super-resolution with distortion-aware transformer.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."}
{"claim": "The method section does not provide a sufficient mechanistic explanation for why the proposed parallel architecture is effective.", "claim_type": "subjective", "paper_id": "Cf4FJGmHRQ", "paper_title": "PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "1Zfm29TuCw", "reviewer": "Reviewer_nwGj", "review_text": "Summary: This work has developed a neural network architecture for image recognition that is designed to address the influence of complex degradation factors. It aims to capture both low-frequency and high-frequency components to balance accuracy and generalization. The authors first propose to discard the low-pass filters in the existing FNO structure to retain all frequency components. Subsequently, a parallel structure is introduced to further enhance the utilization of frequency domain information. Finally, the authors design a two-stage training strategy to ensure performance stability.\n\nStrengths: 1. The overall paper has a clear logical structure, and the explanation of the methodology and the presentation of the constructed mechanisms are intuitive and easy to understand.\n2. The author provides a sufficiently detailed explanation for the motivation behind each component in PAC-FNO.\n3. The problem that this work aims to address holds a certain degree of practical application value.\n\nWeaknesses: 1. The abandonment of the low-pass filter is one of the main innovations in this work. Although the author provides an explanation for the motivation behind this operation, it is still recommended that the author conduct ablative experiments to analyze the impact of low-frequency/high-frequency information on accuracy/generalization.\n2. As for parallel architecture, the relevant experimental results have indeed proven its effectiveness. However, the explanation of parallel architecture in the method section appears somewhat lacking. It is hoped that the author can provide further analysis of the mechanism that enables it to be effective.\n3. In terms of comparative experiments, the methods used by the author for comparison appear to be lacking in both quantity and novelty. The comprehensiveness of the complex scenarios considered by the author is commendable, but it is hoped that the author can still increase the comparison results with more advanced works to more effectively validate the superiority of the proposed method.\n4. The author mentions the advantages of this work in terms of efficiency, but it seems that no experimental analysis related to efficiency has been provided (such as FLOPs and runtime on data at different resolutions).\n\nQuestions: Please refer to the Weaknesses.", "labeling_timestamp": "2026-01-11T16:21:28.357046", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "Authors acknowledge the concern and provide a mechanistic explanation: they argue the parallel architecture captures both low- and high-frequency components (unlike serial), cite visualization (Figure 7), and report ablations (Table 21) to justify effectiveness.", "evidence": "\"The parallel configuration of AC-FNO blocks captures more information than a serial structured model in the first layer, which is directly related to the data. We visualize what frequency the parallel configuration and serial configuration capture in Figure 7 in Appendix F.8 of revision. Figure 7 shows that the parallel configuration captures more high frequencies than the serial configuration. In other words, the parallel structured model captured both more low-frequency and high-frequency components than the serial structured model. As a result, parallel configuration of AC-FNO blocks show a better performance than serial configuration in Figure 4 in Section 4.3.\"", "combined_author_response": "We are happy to hear that your major concern has been resolved. We hope our additional answers will also help you with your problems.\n> 1. Some degradation-resistant algorithms seem to do the same thing as you and I want to see further discussions in your paper to make me better understand your contribution, such as Degradation-Resistant Unfolding Network for Heterogeneous Image Fusion (ICCV 23) and HQG-Net: Unpaired Medical Image Enhancement with High-Quality Guidance (TNNLS).\n\n$\\to$ Thank you for pointing out the work. In a nutshell, both papers aim to convert low-quality images into high-quality images.\n[1] generates a high-quality image by fusing low-quality images generated from multiple sensors. [2] generates high-quality medical images using a model trained with unpaired low-quality medical images and high-quality medical images.\n\nHowever, there is a difference between the two studies and our PAC-FNO. PAC-FNO handles low-quality images including low-resolution and input variations. The low-quality images in these two studies include only input variations such as images with fog or noise. In other words, PAC-FNO handles not only the input variations mentioned in those studies, but also low-resolution images, which occurs more frequently in real world deployment scenarios, *e.g.*, CCTV. \n\n[1] Degradation-Resistant Unfolding Network for Heterogeneous Image Fusion (ICCV 23)\n\n[2] HQG-Net: Unpaired Medical Image Enhancement with High-Quality Guidance (TNNLS)\n\n\n> 2. Although this paper focuses on solving the recognition problem, I'd like to see the author expand it to other extreme tasks, such as camouflaged object segmentation [1, 2] and concealed object segmentation [3]. Perhaps there isn't enough time for the author to continue with the corresponding experiments, but I think it's also more useful to see the author's point of view on a high level for other interested readers to get inspiration from this paper.\n\n$\\to$ Very interesting suggestion!  As PAC-FNO addresses the semantic mapping (*i.e.*, classification) problem as it has the most wide applicability, we believe it could be extended to suggested extreme segmentation tasks with a trivial extension. Since the the dense (*i.e.*, pixel-wise) prediction task is essentially the semantic mapping (*i.e.*, classification) problem by taking into account the nearby pixels, given sufficiently large receptive fields tuned by convolution filters for CNNs or token patches for transformers. Empirical validation on this suggestion would be a great future work to widen the applicability of the PAC-FNO. Thank you!\n\n---\n\nDear Reviewer pDdt,\n\nWe appreciate the reviewer’s time and effort in reviewing our manuscript and insightful comments. \n\nAs the closure of the discussion period is approaching, we would like to bring the review’s attention and check if the reviewer could let us know whether the concerns or the misunderstanding have been addressed. \n\nIf this is the case, we would appreciate if you could adjust your rating accordingly. \n\nBest regards,\n\nAuthors\n\n---\n\nDear Reviewer WCy8,\n\nWe appreciate the reviewer’s time and effort in reviewing our manuscript and insightful comments. \n\nAs the closure of the discussion period is approaching, we would like to bring the review’s attention and check if the reviewer could let us know whether the concerns or the misunderstanding have been addressed. \n\nIf this is the case, we would appreciate if you could adjust your rating accordingly. \n\nBest regards,\n\nAuthors\n\n---\n\nDear Reviewer nwGj,\n\nWe appreciate the reviewer’s time and effort in reviewing our manuscript and insightful comments. \n\nAs the closure of the discussion period is approaching, we would like to bring the review’s attention and check if the reviewer could let us know whether the concerns or the misunderstanding have been addressed. \n\nIf this is the case, we would appreciate if you could adjust your rating accordingly. \n\nBest regards,\n\nAuthors\n\n---\n\nWe provide an additional comparison to the method equipped with the latest super-resolution model. The results are in the following table. OSRT [1] is a state-of-the-art model in the super-resolution domain but it does not support $\\times$8 upscale. Therefore, we only use the $\\times$2 and $\\times$4 upscale models of OSRT. In addition, we report OSRT (fine-tune) that combines super-resolution and fine-tuning methods. That is, we fine-tune the pre-trained classification model with low-resolution images upscaled by a super-resolution model.  As a result, PAC-FNO shows better performance than OSRT and OSRT (fine-tune) methods. We also report these results in Table 13 in Appendix F.3 of the revision. \n\n|  Imagenet-1k  |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |  224 |\n|:-------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:----:|\n|              |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   61.4   |     -    |   75.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |   -  |\n|  ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   71.2   |     -    |   78.4   |     -    | 82.1 |\n|               | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |   -  |\n|               |   PAC-FNO   |    Top-1 Acc (%)    |  **58.9**   | **63.2** | **77.6** | **76.2** | **80.2** | **80.7** | 81.8 |\n|              |             | # of Parameters (M) |          |          |          |   3.65   |          |          |      |\n\n| Imagenet-C/P Fog |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |    224   |\n|:----------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n|                 |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   19.4   |     -    |   37.9   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |     -    |\n| ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   42.3   |     -    |   56.4   |     -    |   59.4   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |     -    |\n|                  |   PAC-FNO   |    Top-1 Acc (%)    | **25.4** | **30.4** | **48.2** | **51.7** | **60.1** | **61.4** | **62.8** |\n|         `        |             | # of Parameters (M) |          |          |          |   3.65   |          |          |          |\n\n[1] Yu, Fanghua, et al. \"OSRT: Omnidirectional image super-resolution with distortion-aware transformer.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n---\n\nWe provide an additional comparison to the method equipped with the latest super-resolution model. The results are in the following table. OSRT [1] is a state-of-the-art model in the super-resolution domain but it does not support $\\times$8 upscale. Therefore, we only use the $\\times$2 and $\\times$4 upscale models of OSRT. In addition, we report OSRT (fine-tune) that combines super-resolution and fine-tuning methods. That is, we fine-tune the pre-trained classification model with low-resolution images upscaled by a super-resolution model.  As a result, PAC-FNO shows better performance than OSRT and OSRT (fine-tune) methods. We also report these results in Table 13 in Appendix F.3 of the revision. \n\n|  Imagenet-1k  |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |  224 |\n|:-------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:----:|\n|              |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   61.4   |     -    |   75.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |   -  |\n|  ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   71.2   |     -    |   78.4   |     -    | 82.1 |\n|               | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |   -  |\n|               |   PAC-FNO   |    Top-1 Acc (%)    |  **58.9**   | **63.2** | **77.6** | **76.2** | **80.2** | **80.7** | 81.8 |\n|              |             | # of Parameters (M) |          |          |          |   3.65   |          |          |      |\n\n| Imagenet-C/P Fog |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |    224   |\n|:----------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n|                 |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   19.4   |     -    |   37.9   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |     -    |\n| ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   42.3   |     -    |   56.4   |     -    |   59.4   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |     -    |\n|                  |   PAC-FNO   |    Top-1 Acc (%)    | **25.4** | **30.4** | **48.2** | **51.7** | **60.1** | **61.4** | **62.8** |\n|         `        |             | # of Parameters (M) |          |          |          |   3.65   |          |          |          |\n\n[1] Yu, Fanghua, et al. \"OSRT: Omnidirectional image super-resolution with distortion-aware transformer.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n---\n\nWe provide an additional comparison to the method equipped with the latest super-resolution model. The results are in the following table. OSRT [1] is a state-of-the-art model in the super-resolution domain but it does not support $\\times$8 upscale. Therefore, we only use the $\\times$2 and $\\times$4 upscale models of OSRT. As a result, PAC-FNO shows better performance than OSRT and OSRT (fine-tune) methods. We also report these results in Table 13 in Appendix F.3 of the revision. \n\n|  Imagenet-1k  |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |  224 |\n|:-------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:----:|\n|               |     DBPN    |    Top-1 Acc (%)    |   40.7   |     -    |   68.2   |     -    |   79.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |   23.21  |     -    |   10.43  |     -    |   5.95   |     -    |   -  |\n|               |     DBPN    |    Top-1 Acc (%)    |   **60.8**   |     -    |   72.5   |     -    |   76.7   |     -    | 82.5 |\n|               | (Fine-tune) | # of Parameters (M) | 23.21 |     -    |   10.43   |     -    |   5.95   |     -    | - |\n| ConvNeXt-Tiny |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   61.4   |     -    |   75.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |   -  |\n|               |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   71.2   |     -    |   78.4   |     -    | 82.1 |\n|               | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |   -  |\n|               |   PAC-FNO   |    Top-1 Acc (%)    |   58.9   | **63.2** | **77.6** | **76.2** | **80.2** | **80.7** | 81.8 |\n|              |             | # of Parameters (M) |          |          |          |   3.65   |          |          |      |\n\n| Imagenet-C/P Fog |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |    224   |\n|:----------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n|                  |     DBPN    |    Top-1 Acc (%)    |   0.67   |     -    |   0.99   |     -    |   1.32   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |   23.21  |     -    |   10.43  |     -    |   5.95   |     -    |     -    |\n|                  |     DBPN    |    Top-1 Acc (%)    |   21.8   |     -    |   42.3   |     -    |   56.8   |     -    |   61.0   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   10.43  |     -    |   5.95   |     -    |     -    |\n|   ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   19.4   |     -    |   37.9   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |     -    |\n|                  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   42.3   |     -    |   56.4   |     -    |   59.4   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |     -    |\n|                  |   PAC-FNO   |    Top-1 Acc (%)    | **25.4** | **30.4** | **48.2** | **51.7** | **60.1** | **61.4** | **62.8** |\n|         `        |             | # of Parameters (M) |          |          |          |   3.65   |          |          |\n\n\n[1] Yu, Fanghua, et al. \"OSRT: Omnidirectional image super-resolution with distortion-aware transformer.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n---\n\nDear All Reviewers,\n\nWe uploaded a new version of the revision which contains results for the state-of-the-art model in the super-resolution domain. We revised the following points and uploaded a new version:\n1. We added additional experimental results of the super-resolution method in Table 13 in Appendix F.3. \n2. We added experimental results on fine-grained datasets in ViT models in Tables 16 and 17  in Appendix F.4.\n3. We added additional ablation studies results of the performance of PAC-FNO according to low and high-frequency filters in Table 21 in Appendix F.7.\n4. We describe the effectiveness of parallel architecture and how it affects input variation in terms of frequency in Figure 7 and Figure 8 in Appendix F.8.\n5. We added FLOPs and runtime on data at different resolutions in Table 22 in Appendix F.9.\n\nBest regards,\nAuthors\n\n---\n\n> 3. The original intent behind the existence of ideal low-pass filters was to reduce the number of parameters and computational complexity. While the author's innovative design to remove the inherent low-pass filter is intuitively comprehensible, the associated trade-offs are not discussed in the manuscript. It would be beneficial to provide supplementary explanations to demonstrate the worthiness of such a modification.\n\n$\\to$ We provide FLOPs and runtime on data at different resolutions. Existing FNO models used low-pass filters due to computational complexity, but the channel size was increased to fully utilize low-frequency components. However, PAC-FNO maintained the 3 channel size of the image to use both high-pass filter and low-pass filter. As a result, PAC-FNO showed similar levels of FLOPs and Runtimes as existing FNO models, but showed better performance because it used additional high-frequency components.\n\n| ImageNet-1k |    Metrics   |    28   |   56   |   112   |  224  |\n|:-----------:|:------------:|:-------:|:------:|:-------:|:-----:|\n|    Resize   |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.006  |  0.006 |  0.006  | 0.006 |\n|  Fine-tune  |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.006  |  0.006 |  0.006  | 0.006 |\n|     DRLN    |    GFLOPs    |  180.66 | 412.05 | 1200.50 |  8.96 |\n|             | Runtimes (s) |  0.378  |  0.498 |  0.913  | 0.007 |\n|     DRPN    |    GFLOPs    | 1220.42 | 576.94 |  387.88 |  8.96 |\n|             | Runtimes (s) |  0.1532 |  0.164 |  0.171  | 0.007 |\n|     FNO     |    GFLOPs    |   9.78  |  9.78  |   9.78  |  9.78 |\n|             | Runtimes (s) |  0.016  |  0.016 |  0.016  | 0.016 |\n|     UNO     |    GFLOPs    |   9.10  |  9.10  |   9.10  |  9.10 |\n|             | Runtimes (s) |  0.018  |  0.018 |  0.018  | 0.018 |\n|     AFNO    |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.010  |  0.010 |  0.010  | 0.010 |\n|   PAC-FNO   |    GFLOPs    |   8.98  |  8.98  |   8.98  |  8.98 |\n|             | Runtimes (s) |  0.013  |  0.013 |  0.013  | 0.013 |\n\n> 4. The experiments thoroughly prove the advantages of parallel architectures and claim that this approach encapsulates more frequency components. However, they lack further detailed explanations and justifications.\n\n$\\to$ The parallel configuration of AC-FNO blocks captures more information than a serial structured model in the first layer, which is directly related to the data. We visualize what frequency the parallel configuration and serial configuration capture in Figure 7 in Appendix F.8 of revision. Figure 7 shows that the parallel configuration captures more high frequencies than the serial configuration. In other words, the parallel structured model captured both more low-frequency and high-frequency components than the serial structured model. As a result, parallel configuration of AC-FNO blocks show a better performance than serial configuration in Figure 4 in Section 4.3. We will add this explanation and justification in Appendix F.8 of the revision. Thank you!\n\n---\n\nThank you for the encouraging remarks about our contribution and extensive experimental results and the valuable feedback from the reviewer. We hope that our responses will solve the reviewer’s questions.\n\n> 1. Although the author compared super-resolution (SR) models for variable resolution inputs, the compared SR models are outdated and lack representation across various upscaling factors for super-resolution reconstruction. The field of super-resolution has seen significant advancements recently; thus, it is recommended to select more appropriate comparative algorithms.\n\n$\\to$ As suggested, we conduct the experiments of other methods with a state of the art SR model and summarize the results in the revision (will update it by  November 20th (AOE)). \n\n> 2. The primary advantage of Fourier Neural Operators (FNOs) lies in their use of frequency domain processing for resolution invariance. As a learnable enhancement operator, it's expected to exhibit some resilience to input natural variations. However, the author hasn't provided a detailed and explanatory analysis of the mechanisms where the operator shows robustness against natural variations. Moreover, the chosen input variations in the experiments, like fog, brightness, spatter, and saturate, represent basic degradation scenarios that can be addressed without deep learning methods. Therefore, regarding resilience to input natural variations, this might not be sufficiently emphasized as a highlight of the paper. The paper suggests exploring degradation in real-world scenarios in future work, indicating that the authors are aware of the limitations in terms of experimental performance or the algorithm proposed. However, such scenarios represent fundamental problems studied in the field of Image Recognition (IR) and hold significant practical application implications. Actually, certain degradation processes might affect high or low frequency details in the image's frequency domain. For instance, blur involves the loss of high-frequency details, prompting the author to conduct a mechanistic analysis combining frequency domain and degradation processes to enhance this aspect's interpretability.\n\n$\\to$ As you mentioned, degradations change the high-frequency and low-frequency information in original images. Figure 8 in the Appendix F.8 shows that there are many changes in high-frequency when degradations are visualized in the frequency domain. Since existing FNO-based models that use a low-pass filter remove high-frequency, the degraded image loses not only the degradation factor but also the information of the original image. However, our proposed PAC-FNO uses high-frequency information, so it shows better performance in degradation.\nRegarding the future work, we would like to address combined degradation types as the natural variations (e.g., mixed with motion blur and fog that may occur in the real-world), not try to address new types of variations.\n\n---\n\nThank you for the encouraging remarks about our paper’s extensive experimental results and organizing and the valuable feedback from the reviewer. We hope that our responses will solve the reviewer’s questions.\n\n> 1. Is there a more advanced choice for the SR baseline model used for comparison in your experimental setup? This will affect the fairness of the performance of your experiment?\n\n$\\to$ For fairness of experiment in SR baseline, we will report additional baseline equipped with the latest super-resolution model in Table 13 in Appendix F.3 of the revision that will be updated shortly around November 20th AoE.\n\n> 2. It can be found that in ViT-B16, PAC-FNO shows not very good results at all low resolutions compared to other methods. What caused this phenomenon to occur? Is your method also unfriendly to other Transformer methods?\n\n$\\to$ PAC-FNO generally performs well in low resolution but only worse than A-FNO. A-FNO is a model proposed as a token mixer for transformers, so it seems particularly friendly when combined with Transformer-based models. \n\n> 3. The ideal low-pass filter in the FNO block removes detailed image signals that play an important role in classification in the fine-grained dataset. Is this conclusion applicable to Transformer based image classification methods? More quantitative results should be provided to confirm the universality of the proposed method.\n\n$\\to$ Yes, high-frequency components play a more important role in image classification, especially fine-grained datasets. So, to see the effect of high frequency component removal for nuanced classification problem, we conduct  a fine-grained classification experiment with the ViT model. On the Oxford-IIIT Pets dataset, PAC-FNO, which captures the high-frequency components, performs better at all resolutions than FNO with an ideal low-pass filter.compared to FNO with an ideal low-pass filter. We add these results in Tables 16 and 17 in Appendix F.4 of the revision.\n\n| Oxford-IIIT Pets |  28  |  32  |  56  |  64  |  112 |  128 |  224 |\n|:----------------:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|\n|        FNO       | 26.3 | 33.0 | 58.1 | 64.8 | 82.9 | 85.8 | 91.3 |\n|      PAC-FNO     | 40.3 | 46.2 | 69.0 | 72.5 | 86.8 | 89.2 | 92.2 |\n\n> 4. The ablation experiments about the results of the zero-padding operation and the exclusion of the low pass filter need to be completed to explain the design of the AC-FNO block. \n\n$\\to$ Zero-padding works to upscale low-resolution images that are smaller than the target resolution in the frequency domain to the target resolution. In other words, if there is no zero-padding operation, low-resolution images cannot be processed. \nThen, we provide the ablation experiments for low-pass filters in the first row in Table 21 in Appendix F.7 of the revision. When a low-pass filter is used, performance decreases in terms of accuracy because high-frequency components cannot be considered, but the performance decrease is large for input variation.\n\n|           ImageNet-1k           |  32  |  64  |  128 |  224 |\n|:-------------------------------:|:----:|:----:|:----:|:----:|\n|  PAC-FNO (low pass filter) | 53.5 | 71.4 | 78.7 | 79.0 |\n|             PAC-FNO             | 58.9 | 74.5 | 80.2 | 81.5 |\n\n|         ImageNet-C/P Fog        |  32  |  64  |  128 |  224 |\n|:-------------------------------:|:----:|:----:|:----:|:----:|\n|  PAC-FNO (low pass filter) | 18.0 | 41.7 | 52.4 | 54.4 |\n|             PAC-FNO             | 25.4 | 48.2 | 60.1 | 62.8 |\n\n---\n\n> 3. In terms of comparative experiments, the methods used by the author for comparison appear to be lacking in both quantity and novelty. The comprehensiveness of the complex scenarios considered by the author is commendable, but it is hoped that the author can still increase the comparison results with more advanced works to more effectively validate the superiority of the proposed method.\n\n$\\to$ We believe that advanced work to effectively validate the superiority of our method is an advanced super-resolution method, as super-resolution models are relatively old models. Therefore, we provide additional comparison to the method equipped with the latest super-resolution model in Appendix F.4 of the revision that will be updated shortly around November 20th AoE. Furthermore, we report additional baselines that combine super-resolution and fine-tuning methods in rows 5-6 in Table 13 in Appendix F.4 of the revised paper. That is, we fine-tune the pre-trained classification model with low-resolution images upscaled by a super-resolution model. This method showed worse performance than our PAC-FNO. \nIn addition, super-resolution methods are needed for each resolution. In other words, upscaling models of x8, x4, and x2 are needed to handle 28, 56, and 112 resolution, respectively. In contrast, our proposed PAC-FNO can handle images of all resolutions with an additional 3.65M network and shows good performance.\nIf the advanced works you intend are not super-resolution models, please let us know.\n\n> 4. The author mentions the advantages of this work in terms of efficiency, but it seems that no experimental analysis related to efficiency has been provided (such as FLOPs and runtime on data at different resolutions).\n\n$\\to$ We did not mention the efficiency as the advantage of this work. Instead, we stated \"efficacy of our neural operator-based mechanism\" (not the “efficiency”) in the last paragraph of Section 4.4 is that neural operator-based mechanism is suitable for real-world applications because it can handle a variety of resolutions without the process of resizing to the target resolution. But for curiosity, we also compare the efficiency of our method to the other methods in the following table:\n\n| ImageNet-1k |    Metrics   |    28   |   56   |   112   |  224  |\n|:-----------:|:------------:|:-------:|:------:|:-------:|:-----:|\n|    Resize   |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.006  |  0.006 |  0.006  | 0.006 |\n|  Fine-tune  |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.006  |  0.006 |  0.006  | 0.006 |\n|     DRLN    |    GFLOPs    |  180.66 | 412.05 | 1200.50 |  8.96 |\n|             | Runtimes (s) |  0.378  |  0.498 |  0.913  | 0.007 |\n|     DRPN    |    GFLOPs    | 1220.42 | 576.94 |  387.88 |  8.96 |\n|             | Runtimes (s) |  0.1532 |  0.164 |  0.171  | 0.007 |\n|     FNO     |    GFLOPs    |   9.78  |  9.78  |   9.78  |  9.78 |\n|             | Runtimes (s) |  0.016  |  0.016 |  0.016  | 0.016 |\n|     UNO     |    GFLOPs    |   9.10  |  9.10  |   9.10  |  9.10 |\n|             | Runtimes (s) |  0.018  |  0.018 |  0.018  | 0.018 |\n|     AFNO    |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.010  |  0.010 |  0.010  | 0.010 |\n|   PAC-FNO   |    GFLOPs    |   8.98  |  8.98  |   8.98  |  8.98 |\n|             | Runtimes (s) |  0.013  |  0.013 |  0.013  | 0.013 |\n\nAs a result, PAC-FNO showed the most efficiency in terms of FLOPs and runtimes except AFNO.\n\n---\n\nThank you for the encouraging remarks about our paper’s clear methodology explanation and the valuable feedback from the reviewer. We hope that our responses will solve the reviewer’ questions.\n\n> 1. The abandonment of the low-pass filter is one of the main innovations in this work. Although the author provides an explanation for the motivation behind this operation, it is still recommended that the author conduct ablative experiments to analyze the impact of low-frequency/high-frequency information on accuracy/generalization.\n\n$\\to$ Thank you for the suggestion! We provide an analysis of the impact of low and high-frequency information on accuracy/generalization through ablation experiments in Table 21 in Appendix F.7 of revision. Compared to PAC-FNO, accuracy and generalization decrease when using low-pass filter or high-pass filter. PAC-FNO with low pass filter show similar performance in ImageNet-1k compared with our PAC-FNO, but show a decrease in performance in terms of generalization in ImageNet-C/P Fog. On the other hand, when using a high-pass filter, it is expected to show good performance in ImageNet-C/P Fog, but it does not show good performance in terms of generalization because the performance is also poor in ImageNet-1k. Therefore, PAC-FNO, which uses both low-frequency and high-frequency components, only shows good performance in terms of accuracy/generalization. \n\n|           ImageNet-1k           |  32  |  64  |  128 |  224 |\n|:-------------------------------:|:----:|:----:|:----:|:----:|\n|  PAC-FNO (low pass filter) | 53.5 | 71.4 | 78.7 | 79.0 |\n| PAC-FNO (high pass filter) | 21.6 | 49.4 | 68.2 | 74.8 |\n|             PAC-FNO             | 58.9 | 74.5 | 80.2 | 81.5 |\n\n|         ImageNet-C/P Fog        |  32  |  64  |  128 |  224 |\n|:-------------------------------:|:----:|:----:|:----:|:----:|\n|  PAC-FNO (low pass filter) | 18.0 | 41.7 | 52.4 | 54.4 |\n| PAC-FNO (high pass filter) |  5.92  | 23.0 | 43.2 | 50.2 |\n|             PAC-FNO             | 25.4 | 48.2 | 60.1 | 62.8 |\n\n> 2. As for parallel architecture, the relevant experimental results have indeed proven its effectiveness. However, the explanation of parallel architecture in the method section appears somewhat lacking. It is hoped that the author can provide further analysis of the mechanism that enables it to be effective.\n\n$\\to$ The reason that it is effective is that the parallel configuration captures both low and high-frequency components while the serial architecture captures low-frequency components only (first paragraph in Sec. 3.2). Both low and high-frequency components must be captured to achieve good accuracy/generalization. This can be confirmed in Table 21 in Appendix F.7 in the revision. We visualize what frequency the parallel configuration and serial configuration capture in Figure 7 in Appendix F.8 of revision. Figure 7 shows that the parallel configuration more captures high frequencies than the serial configuration.\n\n---\n\nDear All Reviewers,\n\nWe thank the reviewers for taking the time to read, evaluate, and provide valuable feedback. \nWe upload a rebuttal revision that includes feedback from reviewers. Responses to reviewers feedback are highlighted in blue. \nResults for the state of the art super resolution model will be updated shortly around November 20th AoE.\n\nBest regards,\nAuthors", "author_response": "We provide an additional comparison to the method equipped with the latest super-resolution model. The results are in the following table. OSRT [1] is a state-of-the-art model in the super-resolution domain but it does not support $\\times$8 upscale. Therefore, we only use the $\\times$2 and $\\times$4 upscale models of OSRT. As a result, PAC-FNO shows better performance than OSRT and OSRT (fine-tune) methods. We also report these results in Table 13 in Appendix F.3 of the revision. \n\n|  Imagenet-1k  |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |  224 |\n|:-------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:----:|\n|               |     DBPN    |    Top-1 Acc (%)    |   40.7   |     -    |   68.2   |     -    |   79.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |   23.21  |     -    |   10.43  |     -    |   5.95   |     -    |   -  |\n|               |     DBPN    |    Top-1 Acc (%)    |   **60.8**   |     -    |   72.5   |     -    |   76.7   |     -    | 82.5 |\n|               | (Fine-tune) | # of Parameters (M) | 23.21 |     -    |   10.43   |     -    |   5.95   |     -    | - |\n| ConvNeXt-Tiny |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   61.4   |     -    |   75.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |   -  |\n|               |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   71.2   |     -    |   78.4   |     -    | 82.1 |\n|               | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |   -  |\n|               |   PAC-FNO   |    Top-1 Acc (%)    |   58.9   | **63.2** | **77.6** | **76.2** | **80.2** | **80.7** | 81.8 |\n|              |             | # of Parameters (M) |          |          |          |   3.65   |          |          |      |\n\n| Imagenet-C/P Fog |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |    224   |\n|:----------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n|                  |     DBPN    |    Top-1 Acc (%)    |   0.67   |     -    |   0.99   |     -    |   1.32   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |   23.21  |     -    |   10.43  |     -    |   5.95   |     -    |     -    |\n|                  |     DBPN    |    Top-1 Acc (%)    |   21.8   |     -    |   42.3   |     -    |   56.8   |     -    |   61.0   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   10.43  |     -    |   5.95   |     -    |     -    |\n|   ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   19.4   |     -    |   37.9   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |     -    |\n|                  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   42.3   |     -    |   56.4   |     -    |   59.4   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |     -    |\n|                  |   PAC-FNO   |    Top-1 Acc (%)    | **25.4** | **30.4** | **48.2** | **51.7** | **60.1** | **61.4** | **62.8** |\n|         `        |             | # of Parameters (M) |          |          |          |   3.65   |          |          |\n\n\n[1] Yu, Fanghua, et al. \"OSRT: Omnidirectional image super-resolution with distortion-aware transformer.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."}
{"claim": "The comparative experiments use an insufficient number of baseline methods and the chosen comparisons lack novelty.", "claim_type": "novelty", "paper_id": "Cf4FJGmHRQ", "paper_title": "PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "1Zfm29TuCw", "reviewer": "Reviewer_nwGj", "review_text": "Summary: This work has developed a neural network architecture for image recognition that is designed to address the influence of complex degradation factors. It aims to capture both low-frequency and high-frequency components to balance accuracy and generalization. The authors first propose to discard the low-pass filters in the existing FNO structure to retain all frequency components. Subsequently, a parallel structure is introduced to further enhance the utilization of frequency domain information. Finally, the authors design a two-stage training strategy to ensure performance stability.\n\nStrengths: 1. The overall paper has a clear logical structure, and the explanation of the methodology and the presentation of the constructed mechanisms are intuitive and easy to understand.\n2. The author provides a sufficiently detailed explanation for the motivation behind each component in PAC-FNO.\n3. The problem that this work aims to address holds a certain degree of practical application value.\n\nWeaknesses: 1. The abandonment of the low-pass filter is one of the main innovations in this work. Although the author provides an explanation for the motivation behind this operation, it is still recommended that the author conduct ablative experiments to analyze the impact of low-frequency/high-frequency information on accuracy/generalization.\n2. As for parallel architecture, the relevant experimental results have indeed proven its effectiveness. However, the explanation of parallel architecture in the method section appears somewhat lacking. It is hoped that the author can provide further analysis of the mechanism that enables it to be effective.\n3. In terms of comparative experiments, the methods used by the author for comparison appear to be lacking in both quantity and novelty. The comprehensiveness of the complex scenarios considered by the author is commendable, but it is hoped that the author can still increase the comparison results with more advanced works to more effectively validate the superiority of the proposed method.\n4. The author mentions the advantages of this work in terms of efficiency, but it seems that no experimental analysis related to efficiency has been provided (such as FLOPs and runtime on data at different resolutions).\n\nQuestions: Please refer to the Weaknesses.", "labeling_timestamp": "2026-01-11T16:21:25.172686", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The authors acknowledge the reviewer's concern and report adding several new baselines and comparisons (including state-of-the-art super-resolution models, fine-tune variants, and appended tables/appendices), and they state they will update with further SOTA SR results—i.e., they agreed and acted to expand the comparative experiments.", "evidence": "\"We provide an additional comparison to the method equipped with the latest super-resolution model.\" / \"We added additional experimental results of the super-resolution method in Table 13 in Appendix F.3.\"", "combined_author_response": "We are happy to hear that your major concern has been resolved. We hope our additional answers will also help you with your problems.\n> 1. Some degradation-resistant algorithms seem to do the same thing as you and I want to see further discussions in your paper to make me better understand your contribution, such as Degradation-Resistant Unfolding Network for Heterogeneous Image Fusion (ICCV 23) and HQG-Net: Unpaired Medical Image Enhancement with High-Quality Guidance (TNNLS).\n\n$\\to$ Thank you for pointing out the work. In a nutshell, both papers aim to convert low-quality images into high-quality images.\n[1] generates a high-quality image by fusing low-quality images generated from multiple sensors. [2] generates high-quality medical images using a model trained with unpaired low-quality medical images and high-quality medical images.\n\nHowever, there is a difference between the two studies and our PAC-FNO. PAC-FNO handles low-quality images including low-resolution and input variations. The low-quality images in these two studies include only input variations such as images with fog or noise. In other words, PAC-FNO handles not only the input variations mentioned in those studies, but also low-resolution images, which occurs more frequently in real world deployment scenarios, *e.g.*, CCTV. \n\n[1] Degradation-Resistant Unfolding Network for Heterogeneous Image Fusion (ICCV 23)\n\n[2] HQG-Net: Unpaired Medical Image Enhancement with High-Quality Guidance (TNNLS)\n\n\n> 2. Although this paper focuses on solving the recognition problem, I'd like to see the author expand it to other extreme tasks, such as camouflaged object segmentation [1, 2] and concealed object segmentation [3]. Perhaps there isn't enough time for the author to continue with the corresponding experiments, but I think it's also more useful to see the author's point of view on a high level for other interested readers to get inspiration from this paper.\n\n$\\to$ Very interesting suggestion!  As PAC-FNO addresses the semantic mapping (*i.e.*, classification) problem as it has the most wide applicability, we believe it could be extended to suggested extreme segmentation tasks with a trivial extension. Since the the dense (*i.e.*, pixel-wise) prediction task is essentially the semantic mapping (*i.e.*, classification) problem by taking into account the nearby pixels, given sufficiently large receptive fields tuned by convolution filters for CNNs or token patches for transformers. Empirical validation on this suggestion would be a great future work to widen the applicability of the PAC-FNO. Thank you!\n\n---\n\nDear Reviewer pDdt,\n\nWe appreciate the reviewer’s time and effort in reviewing our manuscript and insightful comments. \n\nAs the closure of the discussion period is approaching, we would like to bring the review’s attention and check if the reviewer could let us know whether the concerns or the misunderstanding have been addressed. \n\nIf this is the case, we would appreciate if you could adjust your rating accordingly. \n\nBest regards,\n\nAuthors\n\n---\n\nDear Reviewer WCy8,\n\nWe appreciate the reviewer’s time and effort in reviewing our manuscript and insightful comments. \n\nAs the closure of the discussion period is approaching, we would like to bring the review’s attention and check if the reviewer could let us know whether the concerns or the misunderstanding have been addressed. \n\nIf this is the case, we would appreciate if you could adjust your rating accordingly. \n\nBest regards,\n\nAuthors\n\n---\n\nDear Reviewer nwGj,\n\nWe appreciate the reviewer’s time and effort in reviewing our manuscript and insightful comments. \n\nAs the closure of the discussion period is approaching, we would like to bring the review’s attention and check if the reviewer could let us know whether the concerns or the misunderstanding have been addressed. \n\nIf this is the case, we would appreciate if you could adjust your rating accordingly. \n\nBest regards,\n\nAuthors\n\n---\n\nWe provide an additional comparison to the method equipped with the latest super-resolution model. The results are in the following table. OSRT [1] is a state-of-the-art model in the super-resolution domain but it does not support $\\times$8 upscale. Therefore, we only use the $\\times$2 and $\\times$4 upscale models of OSRT. In addition, we report OSRT (fine-tune) that combines super-resolution and fine-tuning methods. That is, we fine-tune the pre-trained classification model with low-resolution images upscaled by a super-resolution model.  As a result, PAC-FNO shows better performance than OSRT and OSRT (fine-tune) methods. We also report these results in Table 13 in Appendix F.3 of the revision. \n\n|  Imagenet-1k  |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |  224 |\n|:-------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:----:|\n|              |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   61.4   |     -    |   75.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |   -  |\n|  ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   71.2   |     -    |   78.4   |     -    | 82.1 |\n|               | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |   -  |\n|               |   PAC-FNO   |    Top-1 Acc (%)    |  **58.9**   | **63.2** | **77.6** | **76.2** | **80.2** | **80.7** | 81.8 |\n|              |             | # of Parameters (M) |          |          |          |   3.65   |          |          |      |\n\n| Imagenet-C/P Fog |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |    224   |\n|:----------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n|                 |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   19.4   |     -    |   37.9   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |     -    |\n| ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   42.3   |     -    |   56.4   |     -    |   59.4   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |     -    |\n|                  |   PAC-FNO   |    Top-1 Acc (%)    | **25.4** | **30.4** | **48.2** | **51.7** | **60.1** | **61.4** | **62.8** |\n|         `        |             | # of Parameters (M) |          |          |          |   3.65   |          |          |          |\n\n[1] Yu, Fanghua, et al. \"OSRT: Omnidirectional image super-resolution with distortion-aware transformer.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n---\n\nWe provide an additional comparison to the method equipped with the latest super-resolution model. The results are in the following table. OSRT [1] is a state-of-the-art model in the super-resolution domain but it does not support $\\times$8 upscale. Therefore, we only use the $\\times$2 and $\\times$4 upscale models of OSRT. In addition, we report OSRT (fine-tune) that combines super-resolution and fine-tuning methods. That is, we fine-tune the pre-trained classification model with low-resolution images upscaled by a super-resolution model.  As a result, PAC-FNO shows better performance than OSRT and OSRT (fine-tune) methods. We also report these results in Table 13 in Appendix F.3 of the revision. \n\n|  Imagenet-1k  |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |  224 |\n|:-------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:----:|\n|              |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   61.4   |     -    |   75.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |   -  |\n|  ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   71.2   |     -    |   78.4   |     -    | 82.1 |\n|               | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |   -  |\n|               |   PAC-FNO   |    Top-1 Acc (%)    |  **58.9**   | **63.2** | **77.6** | **76.2** | **80.2** | **80.7** | 81.8 |\n|              |             | # of Parameters (M) |          |          |          |   3.65   |          |          |      |\n\n| Imagenet-C/P Fog |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |    224   |\n|:----------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n|                 |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   19.4   |     -    |   37.9   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |     -    |\n| ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   42.3   |     -    |   56.4   |     -    |   59.4   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |     -    |\n|                  |   PAC-FNO   |    Top-1 Acc (%)    | **25.4** | **30.4** | **48.2** | **51.7** | **60.1** | **61.4** | **62.8** |\n|         `        |             | # of Parameters (M) |          |          |          |   3.65   |          |          |          |\n\n[1] Yu, Fanghua, et al. \"OSRT: Omnidirectional image super-resolution with distortion-aware transformer.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n---\n\nWe provide an additional comparison to the method equipped with the latest super-resolution model. The results are in the following table. OSRT [1] is a state-of-the-art model in the super-resolution domain but it does not support $\\times$8 upscale. Therefore, we only use the $\\times$2 and $\\times$4 upscale models of OSRT. As a result, PAC-FNO shows better performance than OSRT and OSRT (fine-tune) methods. We also report these results in Table 13 in Appendix F.3 of the revision. \n\n|  Imagenet-1k  |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |  224 |\n|:-------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:----:|\n|               |     DBPN    |    Top-1 Acc (%)    |   40.7   |     -    |   68.2   |     -    |   79.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |   23.21  |     -    |   10.43  |     -    |   5.95   |     -    |   -  |\n|               |     DBPN    |    Top-1 Acc (%)    |   **60.8**   |     -    |   72.5   |     -    |   76.7   |     -    | 82.5 |\n|               | (Fine-tune) | # of Parameters (M) | 23.21 |     -    |   10.43   |     -    |   5.95   |     -    | - |\n| ConvNeXt-Tiny |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   61.4   |     -    |   75.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |   -  |\n|               |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   71.2   |     -    |   78.4   |     -    | 82.1 |\n|               | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |   -  |\n|               |   PAC-FNO   |    Top-1 Acc (%)    |   58.9   | **63.2** | **77.6** | **76.2** | **80.2** | **80.7** | 81.8 |\n|              |             | # of Parameters (M) |          |          |          |   3.65   |          |          |      |\n\n| Imagenet-C/P Fog |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |    224   |\n|:----------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n|                  |     DBPN    |    Top-1 Acc (%)    |   0.67   |     -    |   0.99   |     -    |   1.32   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |   23.21  |     -    |   10.43  |     -    |   5.95   |     -    |     -    |\n|                  |     DBPN    |    Top-1 Acc (%)    |   21.8   |     -    |   42.3   |     -    |   56.8   |     -    |   61.0   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   10.43  |     -    |   5.95   |     -    |     -    |\n|   ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   19.4   |     -    |   37.9   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |     -    |\n|                  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   42.3   |     -    |   56.4   |     -    |   59.4   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |     -    |\n|                  |   PAC-FNO   |    Top-1 Acc (%)    | **25.4** | **30.4** | **48.2** | **51.7** | **60.1** | **61.4** | **62.8** |\n|         `        |             | # of Parameters (M) |          |          |          |   3.65   |          |          |\n\n\n[1] Yu, Fanghua, et al. \"OSRT: Omnidirectional image super-resolution with distortion-aware transformer.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n---\n\nDear All Reviewers,\n\nWe uploaded a new version of the revision which contains results for the state-of-the-art model in the super-resolution domain. We revised the following points and uploaded a new version:\n1. We added additional experimental results of the super-resolution method in Table 13 in Appendix F.3. \n2. We added experimental results on fine-grained datasets in ViT models in Tables 16 and 17  in Appendix F.4.\n3. We added additional ablation studies results of the performance of PAC-FNO according to low and high-frequency filters in Table 21 in Appendix F.7.\n4. We describe the effectiveness of parallel architecture and how it affects input variation in terms of frequency in Figure 7 and Figure 8 in Appendix F.8.\n5. We added FLOPs and runtime on data at different resolutions in Table 22 in Appendix F.9.\n\nBest regards,\nAuthors\n\n---\n\n> 3. The original intent behind the existence of ideal low-pass filters was to reduce the number of parameters and computational complexity. While the author's innovative design to remove the inherent low-pass filter is intuitively comprehensible, the associated trade-offs are not discussed in the manuscript. It would be beneficial to provide supplementary explanations to demonstrate the worthiness of such a modification.\n\n$\\to$ We provide FLOPs and runtime on data at different resolutions. Existing FNO models used low-pass filters due to computational complexity, but the channel size was increased to fully utilize low-frequency components. However, PAC-FNO maintained the 3 channel size of the image to use both high-pass filter and low-pass filter. As a result, PAC-FNO showed similar levels of FLOPs and Runtimes as existing FNO models, but showed better performance because it used additional high-frequency components.\n\n| ImageNet-1k |    Metrics   |    28   |   56   |   112   |  224  |\n|:-----------:|:------------:|:-------:|:------:|:-------:|:-----:|\n|    Resize   |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.006  |  0.006 |  0.006  | 0.006 |\n|  Fine-tune  |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.006  |  0.006 |  0.006  | 0.006 |\n|     DRLN    |    GFLOPs    |  180.66 | 412.05 | 1200.50 |  8.96 |\n|             | Runtimes (s) |  0.378  |  0.498 |  0.913  | 0.007 |\n|     DRPN    |    GFLOPs    | 1220.42 | 576.94 |  387.88 |  8.96 |\n|             | Runtimes (s) |  0.1532 |  0.164 |  0.171  | 0.007 |\n|     FNO     |    GFLOPs    |   9.78  |  9.78  |   9.78  |  9.78 |\n|             | Runtimes (s) |  0.016  |  0.016 |  0.016  | 0.016 |\n|     UNO     |    GFLOPs    |   9.10  |  9.10  |   9.10  |  9.10 |\n|             | Runtimes (s) |  0.018  |  0.018 |  0.018  | 0.018 |\n|     AFNO    |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.010  |  0.010 |  0.010  | 0.010 |\n|   PAC-FNO   |    GFLOPs    |   8.98  |  8.98  |   8.98  |  8.98 |\n|             | Runtimes (s) |  0.013  |  0.013 |  0.013  | 0.013 |\n\n> 4. The experiments thoroughly prove the advantages of parallel architectures and claim that this approach encapsulates more frequency components. However, they lack further detailed explanations and justifications.\n\n$\\to$ The parallel configuration of AC-FNO blocks captures more information than a serial structured model in the first layer, which is directly related to the data. We visualize what frequency the parallel configuration and serial configuration capture in Figure 7 in Appendix F.8 of revision. Figure 7 shows that the parallel configuration captures more high frequencies than the serial configuration. In other words, the parallel structured model captured both more low-frequency and high-frequency components than the serial structured model. As a result, parallel configuration of AC-FNO blocks show a better performance than serial configuration in Figure 4 in Section 4.3. We will add this explanation and justification in Appendix F.8 of the revision. Thank you!\n\n---\n\nThank you for the encouraging remarks about our contribution and extensive experimental results and the valuable feedback from the reviewer. We hope that our responses will solve the reviewer’s questions.\n\n> 1. Although the author compared super-resolution (SR) models for variable resolution inputs, the compared SR models are outdated and lack representation across various upscaling factors for super-resolution reconstruction. The field of super-resolution has seen significant advancements recently; thus, it is recommended to select more appropriate comparative algorithms.\n\n$\\to$ As suggested, we conduct the experiments of other methods with a state of the art SR model and summarize the results in the revision (will update it by  November 20th (AOE)). \n\n> 2. The primary advantage of Fourier Neural Operators (FNOs) lies in their use of frequency domain processing for resolution invariance. As a learnable enhancement operator, it's expected to exhibit some resilience to input natural variations. However, the author hasn't provided a detailed and explanatory analysis of the mechanisms where the operator shows robustness against natural variations. Moreover, the chosen input variations in the experiments, like fog, brightness, spatter, and saturate, represent basic degradation scenarios that can be addressed without deep learning methods. Therefore, regarding resilience to input natural variations, this might not be sufficiently emphasized as a highlight of the paper. The paper suggests exploring degradation in real-world scenarios in future work, indicating that the authors are aware of the limitations in terms of experimental performance or the algorithm proposed. However, such scenarios represent fundamental problems studied in the field of Image Recognition (IR) and hold significant practical application implications. Actually, certain degradation processes might affect high or low frequency details in the image's frequency domain. For instance, blur involves the loss of high-frequency details, prompting the author to conduct a mechanistic analysis combining frequency domain and degradation processes to enhance this aspect's interpretability.\n\n$\\to$ As you mentioned, degradations change the high-frequency and low-frequency information in original images. Figure 8 in the Appendix F.8 shows that there are many changes in high-frequency when degradations are visualized in the frequency domain. Since existing FNO-based models that use a low-pass filter remove high-frequency, the degraded image loses not only the degradation factor but also the information of the original image. However, our proposed PAC-FNO uses high-frequency information, so it shows better performance in degradation.\nRegarding the future work, we would like to address combined degradation types as the natural variations (e.g., mixed with motion blur and fog that may occur in the real-world), not try to address new types of variations.\n\n---\n\nThank you for the encouraging remarks about our paper’s extensive experimental results and organizing and the valuable feedback from the reviewer. We hope that our responses will solve the reviewer’s questions.\n\n> 1. Is there a more advanced choice for the SR baseline model used for comparison in your experimental setup? This will affect the fairness of the performance of your experiment?\n\n$\\to$ For fairness of experiment in SR baseline, we will report additional baseline equipped with the latest super-resolution model in Table 13 in Appendix F.3 of the revision that will be updated shortly around November 20th AoE.\n\n> 2. It can be found that in ViT-B16, PAC-FNO shows not very good results at all low resolutions compared to other methods. What caused this phenomenon to occur? Is your method also unfriendly to other Transformer methods?\n\n$\\to$ PAC-FNO generally performs well in low resolution but only worse than A-FNO. A-FNO is a model proposed as a token mixer for transformers, so it seems particularly friendly when combined with Transformer-based models. \n\n> 3. The ideal low-pass filter in the FNO block removes detailed image signals that play an important role in classification in the fine-grained dataset. Is this conclusion applicable to Transformer based image classification methods? More quantitative results should be provided to confirm the universality of the proposed method.\n\n$\\to$ Yes, high-frequency components play a more important role in image classification, especially fine-grained datasets. So, to see the effect of high frequency component removal for nuanced classification problem, we conduct  a fine-grained classification experiment with the ViT model. On the Oxford-IIIT Pets dataset, PAC-FNO, which captures the high-frequency components, performs better at all resolutions than FNO with an ideal low-pass filter.compared to FNO with an ideal low-pass filter. We add these results in Tables 16 and 17 in Appendix F.4 of the revision.\n\n| Oxford-IIIT Pets |  28  |  32  |  56  |  64  |  112 |  128 |  224 |\n|:----------------:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|\n|        FNO       | 26.3 | 33.0 | 58.1 | 64.8 | 82.9 | 85.8 | 91.3 |\n|      PAC-FNO     | 40.3 | 46.2 | 69.0 | 72.5 | 86.8 | 89.2 | 92.2 |\n\n> 4. The ablation experiments about the results of the zero-padding operation and the exclusion of the low pass filter need to be completed to explain the design of the AC-FNO block. \n\n$\\to$ Zero-padding works to upscale low-resolution images that are smaller than the target resolution in the frequency domain to the target resolution. In other words, if there is no zero-padding operation, low-resolution images cannot be processed. \nThen, we provide the ablation experiments for low-pass filters in the first row in Table 21 in Appendix F.7 of the revision. When a low-pass filter is used, performance decreases in terms of accuracy because high-frequency components cannot be considered, but the performance decrease is large for input variation.\n\n|           ImageNet-1k           |  32  |  64  |  128 |  224 |\n|:-------------------------------:|:----:|:----:|:----:|:----:|\n|  PAC-FNO (low pass filter) | 53.5 | 71.4 | 78.7 | 79.0 |\n|             PAC-FNO             | 58.9 | 74.5 | 80.2 | 81.5 |\n\n|         ImageNet-C/P Fog        |  32  |  64  |  128 |  224 |\n|:-------------------------------:|:----:|:----:|:----:|:----:|\n|  PAC-FNO (low pass filter) | 18.0 | 41.7 | 52.4 | 54.4 |\n|             PAC-FNO             | 25.4 | 48.2 | 60.1 | 62.8 |\n\n---\n\n> 3. In terms of comparative experiments, the methods used by the author for comparison appear to be lacking in both quantity and novelty. The comprehensiveness of the complex scenarios considered by the author is commendable, but it is hoped that the author can still increase the comparison results with more advanced works to more effectively validate the superiority of the proposed method.\n\n$\\to$ We believe that advanced work to effectively validate the superiority of our method is an advanced super-resolution method, as super-resolution models are relatively old models. Therefore, we provide additional comparison to the method equipped with the latest super-resolution model in Appendix F.4 of the revision that will be updated shortly around November 20th AoE. Furthermore, we report additional baselines that combine super-resolution and fine-tuning methods in rows 5-6 in Table 13 in Appendix F.4 of the revised paper. That is, we fine-tune the pre-trained classification model with low-resolution images upscaled by a super-resolution model. This method showed worse performance than our PAC-FNO. \nIn addition, super-resolution methods are needed for each resolution. In other words, upscaling models of x8, x4, and x2 are needed to handle 28, 56, and 112 resolution, respectively. In contrast, our proposed PAC-FNO can handle images of all resolutions with an additional 3.65M network and shows good performance.\nIf the advanced works you intend are not super-resolution models, please let us know.\n\n> 4. The author mentions the advantages of this work in terms of efficiency, but it seems that no experimental analysis related to efficiency has been provided (such as FLOPs and runtime on data at different resolutions).\n\n$\\to$ We did not mention the efficiency as the advantage of this work. Instead, we stated \"efficacy of our neural operator-based mechanism\" (not the “efficiency”) in the last paragraph of Section 4.4 is that neural operator-based mechanism is suitable for real-world applications because it can handle a variety of resolutions without the process of resizing to the target resolution. But for curiosity, we also compare the efficiency of our method to the other methods in the following table:\n\n| ImageNet-1k |    Metrics   |    28   |   56   |   112   |  224  |\n|:-----------:|:------------:|:-------:|:------:|:-------:|:-----:|\n|    Resize   |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.006  |  0.006 |  0.006  | 0.006 |\n|  Fine-tune  |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.006  |  0.006 |  0.006  | 0.006 |\n|     DRLN    |    GFLOPs    |  180.66 | 412.05 | 1200.50 |  8.96 |\n|             | Runtimes (s) |  0.378  |  0.498 |  0.913  | 0.007 |\n|     DRPN    |    GFLOPs    | 1220.42 | 576.94 |  387.88 |  8.96 |\n|             | Runtimes (s) |  0.1532 |  0.164 |  0.171  | 0.007 |\n|     FNO     |    GFLOPs    |   9.78  |  9.78  |   9.78  |  9.78 |\n|             | Runtimes (s) |  0.016  |  0.016 |  0.016  | 0.016 |\n|     UNO     |    GFLOPs    |   9.10  |  9.10  |   9.10  |  9.10 |\n|             | Runtimes (s) |  0.018  |  0.018 |  0.018  | 0.018 |\n|     AFNO    |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.010  |  0.010 |  0.010  | 0.010 |\n|   PAC-FNO   |    GFLOPs    |   8.98  |  8.98  |   8.98  |  8.98 |\n|             | Runtimes (s) |  0.013  |  0.013 |  0.013  | 0.013 |\n\nAs a result, PAC-FNO showed the most efficiency in terms of FLOPs and runtimes except AFNO.\n\n---\n\nThank you for the encouraging remarks about our paper’s clear methodology explanation and the valuable feedback from the reviewer. We hope that our responses will solve the reviewer’ questions.\n\n> 1. The abandonment of the low-pass filter is one of the main innovations in this work. Although the author provides an explanation for the motivation behind this operation, it is still recommended that the author conduct ablative experiments to analyze the impact of low-frequency/high-frequency information on accuracy/generalization.\n\n$\\to$ Thank you for the suggestion! We provide an analysis of the impact of low and high-frequency information on accuracy/generalization through ablation experiments in Table 21 in Appendix F.7 of revision. Compared to PAC-FNO, accuracy and generalization decrease when using low-pass filter or high-pass filter. PAC-FNO with low pass filter show similar performance in ImageNet-1k compared with our PAC-FNO, but show a decrease in performance in terms of generalization in ImageNet-C/P Fog. On the other hand, when using a high-pass filter, it is expected to show good performance in ImageNet-C/P Fog, but it does not show good performance in terms of generalization because the performance is also poor in ImageNet-1k. Therefore, PAC-FNO, which uses both low-frequency and high-frequency components, only shows good performance in terms of accuracy/generalization. \n\n|           ImageNet-1k           |  32  |  64  |  128 |  224 |\n|:-------------------------------:|:----:|:----:|:----:|:----:|\n|  PAC-FNO (low pass filter) | 53.5 | 71.4 | 78.7 | 79.0 |\n| PAC-FNO (high pass filter) | 21.6 | 49.4 | 68.2 | 74.8 |\n|             PAC-FNO             | 58.9 | 74.5 | 80.2 | 81.5 |\n\n|         ImageNet-C/P Fog        |  32  |  64  |  128 |  224 |\n|:-------------------------------:|:----:|:----:|:----:|:----:|\n|  PAC-FNO (low pass filter) | 18.0 | 41.7 | 52.4 | 54.4 |\n| PAC-FNO (high pass filter) |  5.92  | 23.0 | 43.2 | 50.2 |\n|             PAC-FNO             | 25.4 | 48.2 | 60.1 | 62.8 |\n\n> 2. As for parallel architecture, the relevant experimental results have indeed proven its effectiveness. However, the explanation of parallel architecture in the method section appears somewhat lacking. It is hoped that the author can provide further analysis of the mechanism that enables it to be effective.\n\n$\\to$ The reason that it is effective is that the parallel configuration captures both low and high-frequency components while the serial architecture captures low-frequency components only (first paragraph in Sec. 3.2). Both low and high-frequency components must be captured to achieve good accuracy/generalization. This can be confirmed in Table 21 in Appendix F.7 in the revision. We visualize what frequency the parallel configuration and serial configuration capture in Figure 7 in Appendix F.8 of revision. Figure 7 shows that the parallel configuration more captures high frequencies than the serial configuration.\n\n---\n\nDear All Reviewers,\n\nWe thank the reviewers for taking the time to read, evaluate, and provide valuable feedback. \nWe upload a rebuttal revision that includes feedback from reviewers. Responses to reviewers feedback are highlighted in blue. \nResults for the state of the art super resolution model will be updated shortly around November 20th AoE.\n\nBest regards,\nAuthors", "author_response": "We provide an additional comparison to the method equipped with the latest super-resolution model. The results are in the following table. OSRT [1] is a state-of-the-art model in the super-resolution domain but it does not support $\\times$8 upscale. Therefore, we only use the $\\times$2 and $\\times$4 upscale models of OSRT. As a result, PAC-FNO shows better performance than OSRT and OSRT (fine-tune) methods. We also report these results in Table 13 in Appendix F.3 of the revision. \n\n|  Imagenet-1k  |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |  224 |\n|:-------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:----:|\n|               |     DBPN    |    Top-1 Acc (%)    |   40.7   |     -    |   68.2   |     -    |   79.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |   23.21  |     -    |   10.43  |     -    |   5.95   |     -    |   -  |\n|               |     DBPN    |    Top-1 Acc (%)    |   **60.8**   |     -    |   72.5   |     -    |   76.7   |     -    | 82.5 |\n|               | (Fine-tune) | # of Parameters (M) | 23.21 |     -    |   10.43   |     -    |   5.95   |     -    | - |\n| ConvNeXt-Tiny |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   61.4   |     -    |   75.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |   -  |\n|               |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   71.2   |     -    |   78.4   |     -    | 82.1 |\n|               | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |   -  |\n|               |   PAC-FNO   |    Top-1 Acc (%)    |   58.9   | **63.2** | **77.6** | **76.2** | **80.2** | **80.7** | 81.8 |\n|              |             | # of Parameters (M) |          |          |          |   3.65   |          |          |      |\n\n| Imagenet-C/P Fog |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |    224   |\n|:----------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n|                  |     DBPN    |    Top-1 Acc (%)    |   0.67   |     -    |   0.99   |     -    |   1.32   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |   23.21  |     -    |   10.43  |     -    |   5.95   |     -    |     -    |\n|                  |     DBPN    |    Top-1 Acc (%)    |   21.8   |     -    |   42.3   |     -    |   56.8   |     -    |   61.0   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   10.43  |     -    |   5.95   |     -    |     -    |\n|   ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   19.4   |     -    |   37.9   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |     -    |\n|                  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   42.3   |     -    |   56.4   |     -    |   59.4   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |     -    |\n|                  |   PAC-FNO   |    Top-1 Acc (%)    | **25.4** | **30.4** | **48.2** | **51.7** | **60.1** | **61.4** | **62.8** |\n|         `        |             | # of Parameters (M) |          |          |          |   3.65   |          |          |\n\n\n[1] Yu, Fanghua, et al. \"OSRT: Omnidirectional image super-resolution with distortion-aware transformer.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."}
{"claim": "The paper does not compare the proposed method with more recent or stronger state-of-the-art methods to convincingly validate its superiority.", "claim_type": "baseline", "paper_id": "Cf4FJGmHRQ", "paper_title": "PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "1Zfm29TuCw", "reviewer": "Reviewer_nwGj", "review_text": "Summary: This work has developed a neural network architecture for image recognition that is designed to address the influence of complex degradation factors. It aims to capture both low-frequency and high-frequency components to balance accuracy and generalization. The authors first propose to discard the low-pass filters in the existing FNO structure to retain all frequency components. Subsequently, a parallel structure is introduced to further enhance the utilization of frequency domain information. Finally, the authors design a two-stage training strategy to ensure performance stability.\n\nStrengths: 1. The overall paper has a clear logical structure, and the explanation of the methodology and the presentation of the constructed mechanisms are intuitive and easy to understand.\n2. The author provides a sufficiently detailed explanation for the motivation behind each component in PAC-FNO.\n3. The problem that this work aims to address holds a certain degree of practical application value.\n\nWeaknesses: 1. The abandonment of the low-pass filter is one of the main innovations in this work. Although the author provides an explanation for the motivation behind this operation, it is still recommended that the author conduct ablative experiments to analyze the impact of low-frequency/high-frequency information on accuracy/generalization.\n2. As for parallel architecture, the relevant experimental results have indeed proven its effectiveness. However, the explanation of parallel architecture in the method section appears somewhat lacking. It is hoped that the author can provide further analysis of the mechanism that enables it to be effective.\n3. In terms of comparative experiments, the methods used by the author for comparison appear to be lacking in both quantity and novelty. The comprehensiveness of the complex scenarios considered by the author is commendable, but it is hoped that the author can still increase the comparison results with more advanced works to more effectively validate the superiority of the proposed method.\n4. The author mentions the advantages of this work in terms of efficiency, but it seems that no experimental analysis related to efficiency has been provided (such as FLOPs and runtime on data at different resolutions).\n\nQuestions: Please refer to the Weaknesses.", "labeling_timestamp": "2026-01-11T16:21:29.124234", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors added new comparative experiments (e.g., OSRT, DBPN, fine-tune variants) and report that PAC-FNO outperforms these methods, and they state they will add further SOTA SR baselines. Thus they addressed the concern by providing additional comparisons, but they also acknowledge limitations (e.g., OSRT lacks ×8 support) and promise further updates, so the reviewer’s complaint is not yet fully exhausted.", "evidence": "\"As a result, PAC-FNO shows better performance than OSRT and OSRT (fine-tune) methods.\"; \"we will report additional baseline equipped with the latest super-resolution model in Table 13 in Appendix F.3 of the revision that will be updated shortly around November 20th AoE.\"", "combined_author_response": "We are happy to hear that your major concern has been resolved. We hope our additional answers will also help you with your problems.\n> 1. Some degradation-resistant algorithms seem to do the same thing as you and I want to see further discussions in your paper to make me better understand your contribution, such as Degradation-Resistant Unfolding Network for Heterogeneous Image Fusion (ICCV 23) and HQG-Net: Unpaired Medical Image Enhancement with High-Quality Guidance (TNNLS).\n\n$\\to$ Thank you for pointing out the work. In a nutshell, both papers aim to convert low-quality images into high-quality images.\n[1] generates a high-quality image by fusing low-quality images generated from multiple sensors. [2] generates high-quality medical images using a model trained with unpaired low-quality medical images and high-quality medical images.\n\nHowever, there is a difference between the two studies and our PAC-FNO. PAC-FNO handles low-quality images including low-resolution and input variations. The low-quality images in these two studies include only input variations such as images with fog or noise. In other words, PAC-FNO handles not only the input variations mentioned in those studies, but also low-resolution images, which occurs more frequently in real world deployment scenarios, *e.g.*, CCTV. \n\n[1] Degradation-Resistant Unfolding Network for Heterogeneous Image Fusion (ICCV 23)\n\n[2] HQG-Net: Unpaired Medical Image Enhancement with High-Quality Guidance (TNNLS)\n\n\n> 2. Although this paper focuses on solving the recognition problem, I'd like to see the author expand it to other extreme tasks, such as camouflaged object segmentation [1, 2] and concealed object segmentation [3]. Perhaps there isn't enough time for the author to continue with the corresponding experiments, but I think it's also more useful to see the author's point of view on a high level for other interested readers to get inspiration from this paper.\n\n$\\to$ Very interesting suggestion!  As PAC-FNO addresses the semantic mapping (*i.e.*, classification) problem as it has the most wide applicability, we believe it could be extended to suggested extreme segmentation tasks with a trivial extension. Since the the dense (*i.e.*, pixel-wise) prediction task is essentially the semantic mapping (*i.e.*, classification) problem by taking into account the nearby pixels, given sufficiently large receptive fields tuned by convolution filters for CNNs or token patches for transformers. Empirical validation on this suggestion would be a great future work to widen the applicability of the PAC-FNO. Thank you!\n\n---\n\nDear Reviewer pDdt,\n\nWe appreciate the reviewer’s time and effort in reviewing our manuscript and insightful comments. \n\nAs the closure of the discussion period is approaching, we would like to bring the review’s attention and check if the reviewer could let us know whether the concerns or the misunderstanding have been addressed. \n\nIf this is the case, we would appreciate if you could adjust your rating accordingly. \n\nBest regards,\n\nAuthors\n\n---\n\nDear Reviewer WCy8,\n\nWe appreciate the reviewer’s time and effort in reviewing our manuscript and insightful comments. \n\nAs the closure of the discussion period is approaching, we would like to bring the review’s attention and check if the reviewer could let us know whether the concerns or the misunderstanding have been addressed. \n\nIf this is the case, we would appreciate if you could adjust your rating accordingly. \n\nBest regards,\n\nAuthors\n\n---\n\nDear Reviewer nwGj,\n\nWe appreciate the reviewer’s time and effort in reviewing our manuscript and insightful comments. \n\nAs the closure of the discussion period is approaching, we would like to bring the review’s attention and check if the reviewer could let us know whether the concerns or the misunderstanding have been addressed. \n\nIf this is the case, we would appreciate if you could adjust your rating accordingly. \n\nBest regards,\n\nAuthors\n\n---\n\nWe provide an additional comparison to the method equipped with the latest super-resolution model. The results are in the following table. OSRT [1] is a state-of-the-art model in the super-resolution domain but it does not support $\\times$8 upscale. Therefore, we only use the $\\times$2 and $\\times$4 upscale models of OSRT. In addition, we report OSRT (fine-tune) that combines super-resolution and fine-tuning methods. That is, we fine-tune the pre-trained classification model with low-resolution images upscaled by a super-resolution model.  As a result, PAC-FNO shows better performance than OSRT and OSRT (fine-tune) methods. We also report these results in Table 13 in Appendix F.3 of the revision. \n\n|  Imagenet-1k  |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |  224 |\n|:-------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:----:|\n|              |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   61.4   |     -    |   75.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |   -  |\n|  ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   71.2   |     -    |   78.4   |     -    | 82.1 |\n|               | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |   -  |\n|               |   PAC-FNO   |    Top-1 Acc (%)    |  **58.9**   | **63.2** | **77.6** | **76.2** | **80.2** | **80.7** | 81.8 |\n|              |             | # of Parameters (M) |          |          |          |   3.65   |          |          |      |\n\n| Imagenet-C/P Fog |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |    224   |\n|:----------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n|                 |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   19.4   |     -    |   37.9   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |     -    |\n| ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   42.3   |     -    |   56.4   |     -    |   59.4   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |     -    |\n|                  |   PAC-FNO   |    Top-1 Acc (%)    | **25.4** | **30.4** | **48.2** | **51.7** | **60.1** | **61.4** | **62.8** |\n|         `        |             | # of Parameters (M) |          |          |          |   3.65   |          |          |          |\n\n[1] Yu, Fanghua, et al. \"OSRT: Omnidirectional image super-resolution with distortion-aware transformer.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n---\n\nWe provide an additional comparison to the method equipped with the latest super-resolution model. The results are in the following table. OSRT [1] is a state-of-the-art model in the super-resolution domain but it does not support $\\times$8 upscale. Therefore, we only use the $\\times$2 and $\\times$4 upscale models of OSRT. In addition, we report OSRT (fine-tune) that combines super-resolution and fine-tuning methods. That is, we fine-tune the pre-trained classification model with low-resolution images upscaled by a super-resolution model.  As a result, PAC-FNO shows better performance than OSRT and OSRT (fine-tune) methods. We also report these results in Table 13 in Appendix F.3 of the revision. \n\n|  Imagenet-1k  |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |  224 |\n|:-------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:----:|\n|              |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   61.4   |     -    |   75.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |   -  |\n|  ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   71.2   |     -    |   78.4   |     -    | 82.1 |\n|               | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |   -  |\n|               |   PAC-FNO   |    Top-1 Acc (%)    |  **58.9**   | **63.2** | **77.6** | **76.2** | **80.2** | **80.7** | 81.8 |\n|              |             | # of Parameters (M) |          |          |          |   3.65   |          |          |      |\n\n| Imagenet-C/P Fog |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |    224   |\n|:----------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n|                 |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   19.4   |     -    |   37.9   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |     -    |\n| ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   42.3   |     -    |   56.4   |     -    |   59.4   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |     -    |\n|                  |   PAC-FNO   |    Top-1 Acc (%)    | **25.4** | **30.4** | **48.2** | **51.7** | **60.1** | **61.4** | **62.8** |\n|         `        |             | # of Parameters (M) |          |          |          |   3.65   |          |          |          |\n\n[1] Yu, Fanghua, et al. \"OSRT: Omnidirectional image super-resolution with distortion-aware transformer.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n---\n\nWe provide an additional comparison to the method equipped with the latest super-resolution model. The results are in the following table. OSRT [1] is a state-of-the-art model in the super-resolution domain but it does not support $\\times$8 upscale. Therefore, we only use the $\\times$2 and $\\times$4 upscale models of OSRT. As a result, PAC-FNO shows better performance than OSRT and OSRT (fine-tune) methods. We also report these results in Table 13 in Appendix F.3 of the revision. \n\n|  Imagenet-1k  |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |  224 |\n|:-------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:----:|\n|               |     DBPN    |    Top-1 Acc (%)    |   40.7   |     -    |   68.2   |     -    |   79.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |   23.21  |     -    |   10.43  |     -    |   5.95   |     -    |   -  |\n|               |     DBPN    |    Top-1 Acc (%)    |   **60.8**   |     -    |   72.5   |     -    |   76.7   |     -    | 82.5 |\n|               | (Fine-tune) | # of Parameters (M) | 23.21 |     -    |   10.43   |     -    |   5.95   |     -    | - |\n| ConvNeXt-Tiny |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   61.4   |     -    |   75.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |   -  |\n|               |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   71.2   |     -    |   78.4   |     -    | 82.1 |\n|               | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |   -  |\n|               |   PAC-FNO   |    Top-1 Acc (%)    |   58.9   | **63.2** | **77.6** | **76.2** | **80.2** | **80.7** | 81.8 |\n|              |             | # of Parameters (M) |          |          |          |   3.65   |          |          |      |\n\n| Imagenet-C/P Fog |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |    224   |\n|:----------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n|                  |     DBPN    |    Top-1 Acc (%)    |   0.67   |     -    |   0.99   |     -    |   1.32   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |   23.21  |     -    |   10.43  |     -    |   5.95   |     -    |     -    |\n|                  |     DBPN    |    Top-1 Acc (%)    |   21.8   |     -    |   42.3   |     -    |   56.8   |     -    |   61.0   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   10.43  |     -    |   5.95   |     -    |     -    |\n|   ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   19.4   |     -    |   37.9   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |     -    |\n|                  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   42.3   |     -    |   56.4   |     -    |   59.4   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |     -    |\n|                  |   PAC-FNO   |    Top-1 Acc (%)    | **25.4** | **30.4** | **48.2** | **51.7** | **60.1** | **61.4** | **62.8** |\n|         `        |             | # of Parameters (M) |          |          |          |   3.65   |          |          |\n\n\n[1] Yu, Fanghua, et al. \"OSRT: Omnidirectional image super-resolution with distortion-aware transformer.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n---\n\nDear All Reviewers,\n\nWe uploaded a new version of the revision which contains results for the state-of-the-art model in the super-resolution domain. We revised the following points and uploaded a new version:\n1. We added additional experimental results of the super-resolution method in Table 13 in Appendix F.3. \n2. We added experimental results on fine-grained datasets in ViT models in Tables 16 and 17  in Appendix F.4.\n3. We added additional ablation studies results of the performance of PAC-FNO according to low and high-frequency filters in Table 21 in Appendix F.7.\n4. We describe the effectiveness of parallel architecture and how it affects input variation in terms of frequency in Figure 7 and Figure 8 in Appendix F.8.\n5. We added FLOPs and runtime on data at different resolutions in Table 22 in Appendix F.9.\n\nBest regards,\nAuthors\n\n---\n\n> 3. The original intent behind the existence of ideal low-pass filters was to reduce the number of parameters and computational complexity. While the author's innovative design to remove the inherent low-pass filter is intuitively comprehensible, the associated trade-offs are not discussed in the manuscript. It would be beneficial to provide supplementary explanations to demonstrate the worthiness of such a modification.\n\n$\\to$ We provide FLOPs and runtime on data at different resolutions. Existing FNO models used low-pass filters due to computational complexity, but the channel size was increased to fully utilize low-frequency components. However, PAC-FNO maintained the 3 channel size of the image to use both high-pass filter and low-pass filter. As a result, PAC-FNO showed similar levels of FLOPs and Runtimes as existing FNO models, but showed better performance because it used additional high-frequency components.\n\n| ImageNet-1k |    Metrics   |    28   |   56   |   112   |  224  |\n|:-----------:|:------------:|:-------:|:------:|:-------:|:-----:|\n|    Resize   |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.006  |  0.006 |  0.006  | 0.006 |\n|  Fine-tune  |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.006  |  0.006 |  0.006  | 0.006 |\n|     DRLN    |    GFLOPs    |  180.66 | 412.05 | 1200.50 |  8.96 |\n|             | Runtimes (s) |  0.378  |  0.498 |  0.913  | 0.007 |\n|     DRPN    |    GFLOPs    | 1220.42 | 576.94 |  387.88 |  8.96 |\n|             | Runtimes (s) |  0.1532 |  0.164 |  0.171  | 0.007 |\n|     FNO     |    GFLOPs    |   9.78  |  9.78  |   9.78  |  9.78 |\n|             | Runtimes (s) |  0.016  |  0.016 |  0.016  | 0.016 |\n|     UNO     |    GFLOPs    |   9.10  |  9.10  |   9.10  |  9.10 |\n|             | Runtimes (s) |  0.018  |  0.018 |  0.018  | 0.018 |\n|     AFNO    |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.010  |  0.010 |  0.010  | 0.010 |\n|   PAC-FNO   |    GFLOPs    |   8.98  |  8.98  |   8.98  |  8.98 |\n|             | Runtimes (s) |  0.013  |  0.013 |  0.013  | 0.013 |\n\n> 4. The experiments thoroughly prove the advantages of parallel architectures and claim that this approach encapsulates more frequency components. However, they lack further detailed explanations and justifications.\n\n$\\to$ The parallel configuration of AC-FNO blocks captures more information than a serial structured model in the first layer, which is directly related to the data. We visualize what frequency the parallel configuration and serial configuration capture in Figure 7 in Appendix F.8 of revision. Figure 7 shows that the parallel configuration captures more high frequencies than the serial configuration. In other words, the parallel structured model captured both more low-frequency and high-frequency components than the serial structured model. As a result, parallel configuration of AC-FNO blocks show a better performance than serial configuration in Figure 4 in Section 4.3. We will add this explanation and justification in Appendix F.8 of the revision. Thank you!\n\n---\n\nThank you for the encouraging remarks about our contribution and extensive experimental results and the valuable feedback from the reviewer. We hope that our responses will solve the reviewer’s questions.\n\n> 1. Although the author compared super-resolution (SR) models for variable resolution inputs, the compared SR models are outdated and lack representation across various upscaling factors for super-resolution reconstruction. The field of super-resolution has seen significant advancements recently; thus, it is recommended to select more appropriate comparative algorithms.\n\n$\\to$ As suggested, we conduct the experiments of other methods with a state of the art SR model and summarize the results in the revision (will update it by  November 20th (AOE)). \n\n> 2. The primary advantage of Fourier Neural Operators (FNOs) lies in their use of frequency domain processing for resolution invariance. As a learnable enhancement operator, it's expected to exhibit some resilience to input natural variations. However, the author hasn't provided a detailed and explanatory analysis of the mechanisms where the operator shows robustness against natural variations. Moreover, the chosen input variations in the experiments, like fog, brightness, spatter, and saturate, represent basic degradation scenarios that can be addressed without deep learning methods. Therefore, regarding resilience to input natural variations, this might not be sufficiently emphasized as a highlight of the paper. The paper suggests exploring degradation in real-world scenarios in future work, indicating that the authors are aware of the limitations in terms of experimental performance or the algorithm proposed. However, such scenarios represent fundamental problems studied in the field of Image Recognition (IR) and hold significant practical application implications. Actually, certain degradation processes might affect high or low frequency details in the image's frequency domain. For instance, blur involves the loss of high-frequency details, prompting the author to conduct a mechanistic analysis combining frequency domain and degradation processes to enhance this aspect's interpretability.\n\n$\\to$ As you mentioned, degradations change the high-frequency and low-frequency information in original images. Figure 8 in the Appendix F.8 shows that there are many changes in high-frequency when degradations are visualized in the frequency domain. Since existing FNO-based models that use a low-pass filter remove high-frequency, the degraded image loses not only the degradation factor but also the information of the original image. However, our proposed PAC-FNO uses high-frequency information, so it shows better performance in degradation.\nRegarding the future work, we would like to address combined degradation types as the natural variations (e.g., mixed with motion blur and fog that may occur in the real-world), not try to address new types of variations.\n\n---\n\nThank you for the encouraging remarks about our paper’s extensive experimental results and organizing and the valuable feedback from the reviewer. We hope that our responses will solve the reviewer’s questions.\n\n> 1. Is there a more advanced choice for the SR baseline model used for comparison in your experimental setup? This will affect the fairness of the performance of your experiment?\n\n$\\to$ For fairness of experiment in SR baseline, we will report additional baseline equipped with the latest super-resolution model in Table 13 in Appendix F.3 of the revision that will be updated shortly around November 20th AoE.\n\n> 2. It can be found that in ViT-B16, PAC-FNO shows not very good results at all low resolutions compared to other methods. What caused this phenomenon to occur? Is your method also unfriendly to other Transformer methods?\n\n$\\to$ PAC-FNO generally performs well in low resolution but only worse than A-FNO. A-FNO is a model proposed as a token mixer for transformers, so it seems particularly friendly when combined with Transformer-based models. \n\n> 3. The ideal low-pass filter in the FNO block removes detailed image signals that play an important role in classification in the fine-grained dataset. Is this conclusion applicable to Transformer based image classification methods? More quantitative results should be provided to confirm the universality of the proposed method.\n\n$\\to$ Yes, high-frequency components play a more important role in image classification, especially fine-grained datasets. So, to see the effect of high frequency component removal for nuanced classification problem, we conduct  a fine-grained classification experiment with the ViT model. On the Oxford-IIIT Pets dataset, PAC-FNO, which captures the high-frequency components, performs better at all resolutions than FNO with an ideal low-pass filter.compared to FNO with an ideal low-pass filter. We add these results in Tables 16 and 17 in Appendix F.4 of the revision.\n\n| Oxford-IIIT Pets |  28  |  32  |  56  |  64  |  112 |  128 |  224 |\n|:----------------:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|\n|        FNO       | 26.3 | 33.0 | 58.1 | 64.8 | 82.9 | 85.8 | 91.3 |\n|      PAC-FNO     | 40.3 | 46.2 | 69.0 | 72.5 | 86.8 | 89.2 | 92.2 |\n\n> 4. The ablation experiments about the results of the zero-padding operation and the exclusion of the low pass filter need to be completed to explain the design of the AC-FNO block. \n\n$\\to$ Zero-padding works to upscale low-resolution images that are smaller than the target resolution in the frequency domain to the target resolution. In other words, if there is no zero-padding operation, low-resolution images cannot be processed. \nThen, we provide the ablation experiments for low-pass filters in the first row in Table 21 in Appendix F.7 of the revision. When a low-pass filter is used, performance decreases in terms of accuracy because high-frequency components cannot be considered, but the performance decrease is large for input variation.\n\n|           ImageNet-1k           |  32  |  64  |  128 |  224 |\n|:-------------------------------:|:----:|:----:|:----:|:----:|\n|  PAC-FNO (low pass filter) | 53.5 | 71.4 | 78.7 | 79.0 |\n|             PAC-FNO             | 58.9 | 74.5 | 80.2 | 81.5 |\n\n|         ImageNet-C/P Fog        |  32  |  64  |  128 |  224 |\n|:-------------------------------:|:----:|:----:|:----:|:----:|\n|  PAC-FNO (low pass filter) | 18.0 | 41.7 | 52.4 | 54.4 |\n|             PAC-FNO             | 25.4 | 48.2 | 60.1 | 62.8 |\n\n---\n\n> 3. In terms of comparative experiments, the methods used by the author for comparison appear to be lacking in both quantity and novelty. The comprehensiveness of the complex scenarios considered by the author is commendable, but it is hoped that the author can still increase the comparison results with more advanced works to more effectively validate the superiority of the proposed method.\n\n$\\to$ We believe that advanced work to effectively validate the superiority of our method is an advanced super-resolution method, as super-resolution models are relatively old models. Therefore, we provide additional comparison to the method equipped with the latest super-resolution model in Appendix F.4 of the revision that will be updated shortly around November 20th AoE. Furthermore, we report additional baselines that combine super-resolution and fine-tuning methods in rows 5-6 in Table 13 in Appendix F.4 of the revised paper. That is, we fine-tune the pre-trained classification model with low-resolution images upscaled by a super-resolution model. This method showed worse performance than our PAC-FNO. \nIn addition, super-resolution methods are needed for each resolution. In other words, upscaling models of x8, x4, and x2 are needed to handle 28, 56, and 112 resolution, respectively. In contrast, our proposed PAC-FNO can handle images of all resolutions with an additional 3.65M network and shows good performance.\nIf the advanced works you intend are not super-resolution models, please let us know.\n\n> 4. The author mentions the advantages of this work in terms of efficiency, but it seems that no experimental analysis related to efficiency has been provided (such as FLOPs and runtime on data at different resolutions).\n\n$\\to$ We did not mention the efficiency as the advantage of this work. Instead, we stated \"efficacy of our neural operator-based mechanism\" (not the “efficiency”) in the last paragraph of Section 4.4 is that neural operator-based mechanism is suitable for real-world applications because it can handle a variety of resolutions without the process of resizing to the target resolution. But for curiosity, we also compare the efficiency of our method to the other methods in the following table:\n\n| ImageNet-1k |    Metrics   |    28   |   56   |   112   |  224  |\n|:-----------:|:------------:|:-------:|:------:|:-------:|:-----:|\n|    Resize   |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.006  |  0.006 |  0.006  | 0.006 |\n|  Fine-tune  |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.006  |  0.006 |  0.006  | 0.006 |\n|     DRLN    |    GFLOPs    |  180.66 | 412.05 | 1200.50 |  8.96 |\n|             | Runtimes (s) |  0.378  |  0.498 |  0.913  | 0.007 |\n|     DRPN    |    GFLOPs    | 1220.42 | 576.94 |  387.88 |  8.96 |\n|             | Runtimes (s) |  0.1532 |  0.164 |  0.171  | 0.007 |\n|     FNO     |    GFLOPs    |   9.78  |  9.78  |   9.78  |  9.78 |\n|             | Runtimes (s) |  0.016  |  0.016 |  0.016  | 0.016 |\n|     UNO     |    GFLOPs    |   9.10  |  9.10  |   9.10  |  9.10 |\n|             | Runtimes (s) |  0.018  |  0.018 |  0.018  | 0.018 |\n|     AFNO    |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.010  |  0.010 |  0.010  | 0.010 |\n|   PAC-FNO   |    GFLOPs    |   8.98  |  8.98  |   8.98  |  8.98 |\n|             | Runtimes (s) |  0.013  |  0.013 |  0.013  | 0.013 |\n\nAs a result, PAC-FNO showed the most efficiency in terms of FLOPs and runtimes except AFNO.\n\n---\n\nThank you for the encouraging remarks about our paper’s clear methodology explanation and the valuable feedback from the reviewer. We hope that our responses will solve the reviewer’ questions.\n\n> 1. The abandonment of the low-pass filter is one of the main innovations in this work. Although the author provides an explanation for the motivation behind this operation, it is still recommended that the author conduct ablative experiments to analyze the impact of low-frequency/high-frequency information on accuracy/generalization.\n\n$\\to$ Thank you for the suggestion! We provide an analysis of the impact of low and high-frequency information on accuracy/generalization through ablation experiments in Table 21 in Appendix F.7 of revision. Compared to PAC-FNO, accuracy and generalization decrease when using low-pass filter or high-pass filter. PAC-FNO with low pass filter show similar performance in ImageNet-1k compared with our PAC-FNO, but show a decrease in performance in terms of generalization in ImageNet-C/P Fog. On the other hand, when using a high-pass filter, it is expected to show good performance in ImageNet-C/P Fog, but it does not show good performance in terms of generalization because the performance is also poor in ImageNet-1k. Therefore, PAC-FNO, which uses both low-frequency and high-frequency components, only shows good performance in terms of accuracy/generalization. \n\n|           ImageNet-1k           |  32  |  64  |  128 |  224 |\n|:-------------------------------:|:----:|:----:|:----:|:----:|\n|  PAC-FNO (low pass filter) | 53.5 | 71.4 | 78.7 | 79.0 |\n| PAC-FNO (high pass filter) | 21.6 | 49.4 | 68.2 | 74.8 |\n|             PAC-FNO             | 58.9 | 74.5 | 80.2 | 81.5 |\n\n|         ImageNet-C/P Fog        |  32  |  64  |  128 |  224 |\n|:-------------------------------:|:----:|:----:|:----:|:----:|\n|  PAC-FNO (low pass filter) | 18.0 | 41.7 | 52.4 | 54.4 |\n| PAC-FNO (high pass filter) |  5.92  | 23.0 | 43.2 | 50.2 |\n|             PAC-FNO             | 25.4 | 48.2 | 60.1 | 62.8 |\n\n> 2. As for parallel architecture, the relevant experimental results have indeed proven its effectiveness. However, the explanation of parallel architecture in the method section appears somewhat lacking. It is hoped that the author can provide further analysis of the mechanism that enables it to be effective.\n\n$\\to$ The reason that it is effective is that the parallel configuration captures both low and high-frequency components while the serial architecture captures low-frequency components only (first paragraph in Sec. 3.2). Both low and high-frequency components must be captured to achieve good accuracy/generalization. This can be confirmed in Table 21 in Appendix F.7 in the revision. We visualize what frequency the parallel configuration and serial configuration capture in Figure 7 in Appendix F.8 of revision. Figure 7 shows that the parallel configuration more captures high frequencies than the serial configuration.\n\n---\n\nDear All Reviewers,\n\nWe thank the reviewers for taking the time to read, evaluate, and provide valuable feedback. \nWe upload a rebuttal revision that includes feedback from reviewers. Responses to reviewers feedback are highlighted in blue. \nResults for the state of the art super resolution model will be updated shortly around November 20th AoE.\n\nBest regards,\nAuthors", "author_response": "We provide an additional comparison to the method equipped with the latest super-resolution model. The results are in the following table. OSRT [1] is a state-of-the-art model in the super-resolution domain but it does not support $\\times$8 upscale. Therefore, we only use the $\\times$2 and $\\times$4 upscale models of OSRT. As a result, PAC-FNO shows better performance than OSRT and OSRT (fine-tune) methods. We also report these results in Table 13 in Appendix F.3 of the revision. \n\n|  Imagenet-1k  |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |  224 |\n|:-------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:----:|\n|               |     DBPN    |    Top-1 Acc (%)    |   40.7   |     -    |   68.2   |     -    |   79.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |   23.21  |     -    |   10.43  |     -    |   5.95   |     -    |   -  |\n|               |     DBPN    |    Top-1 Acc (%)    |   **60.8**   |     -    |   72.5   |     -    |   76.7   |     -    | 82.5 |\n|               | (Fine-tune) | # of Parameters (M) | 23.21 |     -    |   10.43   |     -    |   5.95   |     -    | - |\n| ConvNeXt-Tiny |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   61.4   |     -    |   75.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |   -  |\n|               |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   71.2   |     -    |   78.4   |     -    | 82.1 |\n|               | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |   -  |\n|               |   PAC-FNO   |    Top-1 Acc (%)    |   58.9   | **63.2** | **77.6** | **76.2** | **80.2** | **80.7** | 81.8 |\n|              |             | # of Parameters (M) |          |          |          |   3.65   |          |          |      |\n\n| Imagenet-C/P Fog |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |    224   |\n|:----------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n|                  |     DBPN    |    Top-1 Acc (%)    |   0.67   |     -    |   0.99   |     -    |   1.32   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |   23.21  |     -    |   10.43  |     -    |   5.95   |     -    |     -    |\n|                  |     DBPN    |    Top-1 Acc (%)    |   21.8   |     -    |   42.3   |     -    |   56.8   |     -    |   61.0   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   10.43  |     -    |   5.95   |     -    |     -    |\n|   ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   19.4   |     -    |   37.9   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |     -    |\n|                  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   42.3   |     -    |   56.4   |     -    |   59.4   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |     -    |\n|                  |   PAC-FNO   |    Top-1 Acc (%)    | **25.4** | **30.4** | **48.2** | **51.7** | **60.1** | **61.4** | **62.8** |\n|         `        |             | # of Parameters (M) |          |          |          |   3.65   |          |          |\n\n\n[1] Yu, Fanghua, et al. \"OSRT: Omnidirectional image super-resolution with distortion-aware transformer.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."}
{"claim": "The authors claim efficiency advantages but provide no experimental efficiency analysis such as FLOPs or runtime measurements across different input resolutions.", "claim_type": "experimental", "paper_id": "Cf4FJGmHRQ", "paper_title": "PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "1Zfm29TuCw", "reviewer": "Reviewer_nwGj", "review_text": "Summary: This work has developed a neural network architecture for image recognition that is designed to address the influence of complex degradation factors. It aims to capture both low-frequency and high-frequency components to balance accuracy and generalization. The authors first propose to discard the low-pass filters in the existing FNO structure to retain all frequency components. Subsequently, a parallel structure is introduced to further enhance the utilization of frequency domain information. Finally, the authors design a two-stage training strategy to ensure performance stability.\n\nStrengths: 1. The overall paper has a clear logical structure, and the explanation of the methodology and the presentation of the constructed mechanisms are intuitive and easy to understand.\n2. The author provides a sufficiently detailed explanation for the motivation behind each component in PAC-FNO.\n3. The problem that this work aims to address holds a certain degree of practical application value.\n\nWeaknesses: 1. The abandonment of the low-pass filter is one of the main innovations in this work. Although the author provides an explanation for the motivation behind this operation, it is still recommended that the author conduct ablative experiments to analyze the impact of low-frequency/high-frequency information on accuracy/generalization.\n2. As for parallel architecture, the relevant experimental results have indeed proven its effectiveness. However, the explanation of parallel architecture in the method section appears somewhat lacking. It is hoped that the author can provide further analysis of the mechanism that enables it to be effective.\n3. In terms of comparative experiments, the methods used by the author for comparison appear to be lacking in both quantity and novelty. The comprehensiveness of the complex scenarios considered by the author is commendable, but it is hoped that the author can still increase the comparison results with more advanced works to more effectively validate the superiority of the proposed method.\n4. The author mentions the advantages of this work in terms of efficiency, but it seems that no experimental analysis related to efficiency has been provided (such as FLOPs and runtime on data at different resolutions).\n\nQuestions: Please refer to the Weaknesses.", "labeling_timestamp": "2026-01-11T16:21:35.700853", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The authors explicitly state they did not claim efficiency as an advantage and provide FLOPs and runtime measurements across resolutions (including a table and Appendix reference), thus refuting the reviewer's assertion that no experimental efficiency analysis was provided.", "evidence": "We did not mention the efficiency as the advantage of this work. Instead, we stated \"efficacy of our neural operator-based mechanism\" (not the “efficiency\") ... But for curiosity, we also compare the efficiency of our method to the other methods in the following table: ... We added FLOPs and runtime on data at different resolutions in Table 22 in Appendix F.9.", "combined_author_response": "We are happy to hear that your major concern has been resolved. We hope our additional answers will also help you with your problems.\n> 1. Some degradation-resistant algorithms seem to do the same thing as you and I want to see further discussions in your paper to make me better understand your contribution, such as Degradation-Resistant Unfolding Network for Heterogeneous Image Fusion (ICCV 23) and HQG-Net: Unpaired Medical Image Enhancement with High-Quality Guidance (TNNLS).\n\n$\\to$ Thank you for pointing out the work. In a nutshell, both papers aim to convert low-quality images into high-quality images.\n[1] generates a high-quality image by fusing low-quality images generated from multiple sensors. [2] generates high-quality medical images using a model trained with unpaired low-quality medical images and high-quality medical images.\n\nHowever, there is a difference between the two studies and our PAC-FNO. PAC-FNO handles low-quality images including low-resolution and input variations. The low-quality images in these two studies include only input variations such as images with fog or noise. In other words, PAC-FNO handles not only the input variations mentioned in those studies, but also low-resolution images, which occurs more frequently in real world deployment scenarios, *e.g.*, CCTV. \n\n[1] Degradation-Resistant Unfolding Network for Heterogeneous Image Fusion (ICCV 23)\n\n[2] HQG-Net: Unpaired Medical Image Enhancement with High-Quality Guidance (TNNLS)\n\n\n> 2. Although this paper focuses on solving the recognition problem, I'd like to see the author expand it to other extreme tasks, such as camouflaged object segmentation [1, 2] and concealed object segmentation [3]. Perhaps there isn't enough time for the author to continue with the corresponding experiments, but I think it's also more useful to see the author's point of view on a high level for other interested readers to get inspiration from this paper.\n\n$\\to$ Very interesting suggestion!  As PAC-FNO addresses the semantic mapping (*i.e.*, classification) problem as it has the most wide applicability, we believe it could be extended to suggested extreme segmentation tasks with a trivial extension. Since the the dense (*i.e.*, pixel-wise) prediction task is essentially the semantic mapping (*i.e.*, classification) problem by taking into account the nearby pixels, given sufficiently large receptive fields tuned by convolution filters for CNNs or token patches for transformers. Empirical validation on this suggestion would be a great future work to widen the applicability of the PAC-FNO. Thank you!\n\n---\n\nDear Reviewer pDdt,\n\nWe appreciate the reviewer’s time and effort in reviewing our manuscript and insightful comments. \n\nAs the closure of the discussion period is approaching, we would like to bring the review’s attention and check if the reviewer could let us know whether the concerns or the misunderstanding have been addressed. \n\nIf this is the case, we would appreciate if you could adjust your rating accordingly. \n\nBest regards,\n\nAuthors\n\n---\n\nDear Reviewer WCy8,\n\nWe appreciate the reviewer’s time and effort in reviewing our manuscript and insightful comments. \n\nAs the closure of the discussion period is approaching, we would like to bring the review’s attention and check if the reviewer could let us know whether the concerns or the misunderstanding have been addressed. \n\nIf this is the case, we would appreciate if you could adjust your rating accordingly. \n\nBest regards,\n\nAuthors\n\n---\n\nDear Reviewer nwGj,\n\nWe appreciate the reviewer’s time and effort in reviewing our manuscript and insightful comments. \n\nAs the closure of the discussion period is approaching, we would like to bring the review’s attention and check if the reviewer could let us know whether the concerns or the misunderstanding have been addressed. \n\nIf this is the case, we would appreciate if you could adjust your rating accordingly. \n\nBest regards,\n\nAuthors\n\n---\n\nWe provide an additional comparison to the method equipped with the latest super-resolution model. The results are in the following table. OSRT [1] is a state-of-the-art model in the super-resolution domain but it does not support $\\times$8 upscale. Therefore, we only use the $\\times$2 and $\\times$4 upscale models of OSRT. In addition, we report OSRT (fine-tune) that combines super-resolution and fine-tuning methods. That is, we fine-tune the pre-trained classification model with low-resolution images upscaled by a super-resolution model.  As a result, PAC-FNO shows better performance than OSRT and OSRT (fine-tune) methods. We also report these results in Table 13 in Appendix F.3 of the revision. \n\n|  Imagenet-1k  |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |  224 |\n|:-------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:----:|\n|              |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   61.4   |     -    |   75.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |   -  |\n|  ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   71.2   |     -    |   78.4   |     -    | 82.1 |\n|               | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |   -  |\n|               |   PAC-FNO   |    Top-1 Acc (%)    |  **58.9**   | **63.2** | **77.6** | **76.2** | **80.2** | **80.7** | 81.8 |\n|              |             | # of Parameters (M) |          |          |          |   3.65   |          |          |      |\n\n| Imagenet-C/P Fog |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |    224   |\n|:----------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n|                 |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   19.4   |     -    |   37.9   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |     -    |\n| ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   42.3   |     -    |   56.4   |     -    |   59.4   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |     -    |\n|                  |   PAC-FNO   |    Top-1 Acc (%)    | **25.4** | **30.4** | **48.2** | **51.7** | **60.1** | **61.4** | **62.8** |\n|         `        |             | # of Parameters (M) |          |          |          |   3.65   |          |          |          |\n\n[1] Yu, Fanghua, et al. \"OSRT: Omnidirectional image super-resolution with distortion-aware transformer.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n---\n\nWe provide an additional comparison to the method equipped with the latest super-resolution model. The results are in the following table. OSRT [1] is a state-of-the-art model in the super-resolution domain but it does not support $\\times$8 upscale. Therefore, we only use the $\\times$2 and $\\times$4 upscale models of OSRT. In addition, we report OSRT (fine-tune) that combines super-resolution and fine-tuning methods. That is, we fine-tune the pre-trained classification model with low-resolution images upscaled by a super-resolution model.  As a result, PAC-FNO shows better performance than OSRT and OSRT (fine-tune) methods. We also report these results in Table 13 in Appendix F.3 of the revision. \n\n|  Imagenet-1k  |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |  224 |\n|:-------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:----:|\n|              |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   61.4   |     -    |   75.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |   -  |\n|  ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   71.2   |     -    |   78.4   |     -    | 82.1 |\n|               | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |   -  |\n|               |   PAC-FNO   |    Top-1 Acc (%)    |  **58.9**   | **63.2** | **77.6** | **76.2** | **80.2** | **80.7** | 81.8 |\n|              |             | # of Parameters (M) |          |          |          |   3.65   |          |          |      |\n\n| Imagenet-C/P Fog |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |    224   |\n|:----------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n|                 |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   19.4   |     -    |   37.9   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |     -    |\n| ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   42.3   |     -    |   56.4   |     -    |   59.4   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |     -    |\n|                  |   PAC-FNO   |    Top-1 Acc (%)    | **25.4** | **30.4** | **48.2** | **51.7** | **60.1** | **61.4** | **62.8** |\n|         `        |             | # of Parameters (M) |          |          |          |   3.65   |          |          |          |\n\n[1] Yu, Fanghua, et al. \"OSRT: Omnidirectional image super-resolution with distortion-aware transformer.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n---\n\nWe provide an additional comparison to the method equipped with the latest super-resolution model. The results are in the following table. OSRT [1] is a state-of-the-art model in the super-resolution domain but it does not support $\\times$8 upscale. Therefore, we only use the $\\times$2 and $\\times$4 upscale models of OSRT. As a result, PAC-FNO shows better performance than OSRT and OSRT (fine-tune) methods. We also report these results in Table 13 in Appendix F.3 of the revision. \n\n|  Imagenet-1k  |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |  224 |\n|:-------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:----:|\n|               |     DBPN    |    Top-1 Acc (%)    |   40.7   |     -    |   68.2   |     -    |   79.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |   23.21  |     -    |   10.43  |     -    |   5.95   |     -    |   -  |\n|               |     DBPN    |    Top-1 Acc (%)    |   **60.8**   |     -    |   72.5   |     -    |   76.7   |     -    | 82.5 |\n|               | (Fine-tune) | # of Parameters (M) | 23.21 |     -    |   10.43   |     -    |   5.95   |     -    | - |\n| ConvNeXt-Tiny |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   61.4   |     -    |   75.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |   -  |\n|               |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   71.2   |     -    |   78.4   |     -    | 82.1 |\n|               | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |   -  |\n|               |   PAC-FNO   |    Top-1 Acc (%)    |   58.9   | **63.2** | **77.6** | **76.2** | **80.2** | **80.7** | 81.8 |\n|              |             | # of Parameters (M) |          |          |          |   3.65   |          |          |      |\n\n| Imagenet-C/P Fog |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |    224   |\n|:----------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n|                  |     DBPN    |    Top-1 Acc (%)    |   0.67   |     -    |   0.99   |     -    |   1.32   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |   23.21  |     -    |   10.43  |     -    |   5.95   |     -    |     -    |\n|                  |     DBPN    |    Top-1 Acc (%)    |   21.8   |     -    |   42.3   |     -    |   56.8   |     -    |   61.0   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   10.43  |     -    |   5.95   |     -    |     -    |\n|   ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   19.4   |     -    |   37.9   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |     -    |\n|                  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   42.3   |     -    |   56.4   |     -    |   59.4   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |     -    |\n|                  |   PAC-FNO   |    Top-1 Acc (%)    | **25.4** | **30.4** | **48.2** | **51.7** | **60.1** | **61.4** | **62.8** |\n|         `        |             | # of Parameters (M) |          |          |          |   3.65   |          |          |\n\n\n[1] Yu, Fanghua, et al. \"OSRT: Omnidirectional image super-resolution with distortion-aware transformer.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n---\n\nDear All Reviewers,\n\nWe uploaded a new version of the revision which contains results for the state-of-the-art model in the super-resolution domain. We revised the following points and uploaded a new version:\n1. We added additional experimental results of the super-resolution method in Table 13 in Appendix F.3. \n2. We added experimental results on fine-grained datasets in ViT models in Tables 16 and 17  in Appendix F.4.\n3. We added additional ablation studies results of the performance of PAC-FNO according to low and high-frequency filters in Table 21 in Appendix F.7.\n4. We describe the effectiveness of parallel architecture and how it affects input variation in terms of frequency in Figure 7 and Figure 8 in Appendix F.8.\n5. We added FLOPs and runtime on data at different resolutions in Table 22 in Appendix F.9.\n\nBest regards,\nAuthors\n\n---\n\n> 3. The original intent behind the existence of ideal low-pass filters was to reduce the number of parameters and computational complexity. While the author's innovative design to remove the inherent low-pass filter is intuitively comprehensible, the associated trade-offs are not discussed in the manuscript. It would be beneficial to provide supplementary explanations to demonstrate the worthiness of such a modification.\n\n$\\to$ We provide FLOPs and runtime on data at different resolutions. Existing FNO models used low-pass filters due to computational complexity, but the channel size was increased to fully utilize low-frequency components. However, PAC-FNO maintained the 3 channel size of the image to use both high-pass filter and low-pass filter. As a result, PAC-FNO showed similar levels of FLOPs and Runtimes as existing FNO models, but showed better performance because it used additional high-frequency components.\n\n| ImageNet-1k |    Metrics   |    28   |   56   |   112   |  224  |\n|:-----------:|:------------:|:-------:|:------:|:-------:|:-----:|\n|    Resize   |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.006  |  0.006 |  0.006  | 0.006 |\n|  Fine-tune  |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.006  |  0.006 |  0.006  | 0.006 |\n|     DRLN    |    GFLOPs    |  180.66 | 412.05 | 1200.50 |  8.96 |\n|             | Runtimes (s) |  0.378  |  0.498 |  0.913  | 0.007 |\n|     DRPN    |    GFLOPs    | 1220.42 | 576.94 |  387.88 |  8.96 |\n|             | Runtimes (s) |  0.1532 |  0.164 |  0.171  | 0.007 |\n|     FNO     |    GFLOPs    |   9.78  |  9.78  |   9.78  |  9.78 |\n|             | Runtimes (s) |  0.016  |  0.016 |  0.016  | 0.016 |\n|     UNO     |    GFLOPs    |   9.10  |  9.10  |   9.10  |  9.10 |\n|             | Runtimes (s) |  0.018  |  0.018 |  0.018  | 0.018 |\n|     AFNO    |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.010  |  0.010 |  0.010  | 0.010 |\n|   PAC-FNO   |    GFLOPs    |   8.98  |  8.98  |   8.98  |  8.98 |\n|             | Runtimes (s) |  0.013  |  0.013 |  0.013  | 0.013 |\n\n> 4. The experiments thoroughly prove the advantages of parallel architectures and claim that this approach encapsulates more frequency components. However, they lack further detailed explanations and justifications.\n\n$\\to$ The parallel configuration of AC-FNO blocks captures more information than a serial structured model in the first layer, which is directly related to the data. We visualize what frequency the parallel configuration and serial configuration capture in Figure 7 in Appendix F.8 of revision. Figure 7 shows that the parallel configuration captures more high frequencies than the serial configuration. In other words, the parallel structured model captured both more low-frequency and high-frequency components than the serial structured model. As a result, parallel configuration of AC-FNO blocks show a better performance than serial configuration in Figure 4 in Section 4.3. We will add this explanation and justification in Appendix F.8 of the revision. Thank you!\n\n---\n\nThank you for the encouraging remarks about our contribution and extensive experimental results and the valuable feedback from the reviewer. We hope that our responses will solve the reviewer’s questions.\n\n> 1. Although the author compared super-resolution (SR) models for variable resolution inputs, the compared SR models are outdated and lack representation across various upscaling factors for super-resolution reconstruction. The field of super-resolution has seen significant advancements recently; thus, it is recommended to select more appropriate comparative algorithms.\n\n$\\to$ As suggested, we conduct the experiments of other methods with a state of the art SR model and summarize the results in the revision (will update it by  November 20th (AOE)). \n\n> 2. The primary advantage of Fourier Neural Operators (FNOs) lies in their use of frequency domain processing for resolution invariance. As a learnable enhancement operator, it's expected to exhibit some resilience to input natural variations. However, the author hasn't provided a detailed and explanatory analysis of the mechanisms where the operator shows robustness against natural variations. Moreover, the chosen input variations in the experiments, like fog, brightness, spatter, and saturate, represent basic degradation scenarios that can be addressed without deep learning methods. Therefore, regarding resilience to input natural variations, this might not be sufficiently emphasized as a highlight of the paper. The paper suggests exploring degradation in real-world scenarios in future work, indicating that the authors are aware of the limitations in terms of experimental performance or the algorithm proposed. However, such scenarios represent fundamental problems studied in the field of Image Recognition (IR) and hold significant practical application implications. Actually, certain degradation processes might affect high or low frequency details in the image's frequency domain. For instance, blur involves the loss of high-frequency details, prompting the author to conduct a mechanistic analysis combining frequency domain and degradation processes to enhance this aspect's interpretability.\n\n$\\to$ As you mentioned, degradations change the high-frequency and low-frequency information in original images. Figure 8 in the Appendix F.8 shows that there are many changes in high-frequency when degradations are visualized in the frequency domain. Since existing FNO-based models that use a low-pass filter remove high-frequency, the degraded image loses not only the degradation factor but also the information of the original image. However, our proposed PAC-FNO uses high-frequency information, so it shows better performance in degradation.\nRegarding the future work, we would like to address combined degradation types as the natural variations (e.g., mixed with motion blur and fog that may occur in the real-world), not try to address new types of variations.\n\n---\n\nThank you for the encouraging remarks about our paper’s extensive experimental results and organizing and the valuable feedback from the reviewer. We hope that our responses will solve the reviewer’s questions.\n\n> 1. Is there a more advanced choice for the SR baseline model used for comparison in your experimental setup? This will affect the fairness of the performance of your experiment?\n\n$\\to$ For fairness of experiment in SR baseline, we will report additional baseline equipped with the latest super-resolution model in Table 13 in Appendix F.3 of the revision that will be updated shortly around November 20th AoE.\n\n> 2. It can be found that in ViT-B16, PAC-FNO shows not very good results at all low resolutions compared to other methods. What caused this phenomenon to occur? Is your method also unfriendly to other Transformer methods?\n\n$\\to$ PAC-FNO generally performs well in low resolution but only worse than A-FNO. A-FNO is a model proposed as a token mixer for transformers, so it seems particularly friendly when combined with Transformer-based models. \n\n> 3. The ideal low-pass filter in the FNO block removes detailed image signals that play an important role in classification in the fine-grained dataset. Is this conclusion applicable to Transformer based image classification methods? More quantitative results should be provided to confirm the universality of the proposed method.\n\n$\\to$ Yes, high-frequency components play a more important role in image classification, especially fine-grained datasets. So, to see the effect of high frequency component removal for nuanced classification problem, we conduct  a fine-grained classification experiment with the ViT model. On the Oxford-IIIT Pets dataset, PAC-FNO, which captures the high-frequency components, performs better at all resolutions than FNO with an ideal low-pass filter.compared to FNO with an ideal low-pass filter. We add these results in Tables 16 and 17 in Appendix F.4 of the revision.\n\n| Oxford-IIIT Pets |  28  |  32  |  56  |  64  |  112 |  128 |  224 |\n|:----------------:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|\n|        FNO       | 26.3 | 33.0 | 58.1 | 64.8 | 82.9 | 85.8 | 91.3 |\n|      PAC-FNO     | 40.3 | 46.2 | 69.0 | 72.5 | 86.8 | 89.2 | 92.2 |\n\n> 4. The ablation experiments about the results of the zero-padding operation and the exclusion of the low pass filter need to be completed to explain the design of the AC-FNO block. \n\n$\\to$ Zero-padding works to upscale low-resolution images that are smaller than the target resolution in the frequency domain to the target resolution. In other words, if there is no zero-padding operation, low-resolution images cannot be processed. \nThen, we provide the ablation experiments for low-pass filters in the first row in Table 21 in Appendix F.7 of the revision. When a low-pass filter is used, performance decreases in terms of accuracy because high-frequency components cannot be considered, but the performance decrease is large for input variation.\n\n|           ImageNet-1k           |  32  |  64  |  128 |  224 |\n|:-------------------------------:|:----:|:----:|:----:|:----:|\n|  PAC-FNO (low pass filter) | 53.5 | 71.4 | 78.7 | 79.0 |\n|             PAC-FNO             | 58.9 | 74.5 | 80.2 | 81.5 |\n\n|         ImageNet-C/P Fog        |  32  |  64  |  128 |  224 |\n|:-------------------------------:|:----:|:----:|:----:|:----:|\n|  PAC-FNO (low pass filter) | 18.0 | 41.7 | 52.4 | 54.4 |\n|             PAC-FNO             | 25.4 | 48.2 | 60.1 | 62.8 |\n\n---\n\n> 3. In terms of comparative experiments, the methods used by the author for comparison appear to be lacking in both quantity and novelty. The comprehensiveness of the complex scenarios considered by the author is commendable, but it is hoped that the author can still increase the comparison results with more advanced works to more effectively validate the superiority of the proposed method.\n\n$\\to$ We believe that advanced work to effectively validate the superiority of our method is an advanced super-resolution method, as super-resolution models are relatively old models. Therefore, we provide additional comparison to the method equipped with the latest super-resolution model in Appendix F.4 of the revision that will be updated shortly around November 20th AoE. Furthermore, we report additional baselines that combine super-resolution and fine-tuning methods in rows 5-6 in Table 13 in Appendix F.4 of the revised paper. That is, we fine-tune the pre-trained classification model with low-resolution images upscaled by a super-resolution model. This method showed worse performance than our PAC-FNO. \nIn addition, super-resolution methods are needed for each resolution. In other words, upscaling models of x8, x4, and x2 are needed to handle 28, 56, and 112 resolution, respectively. In contrast, our proposed PAC-FNO can handle images of all resolutions with an additional 3.65M network and shows good performance.\nIf the advanced works you intend are not super-resolution models, please let us know.\n\n> 4. The author mentions the advantages of this work in terms of efficiency, but it seems that no experimental analysis related to efficiency has been provided (such as FLOPs and runtime on data at different resolutions).\n\n$\\to$ We did not mention the efficiency as the advantage of this work. Instead, we stated \"efficacy of our neural operator-based mechanism\" (not the “efficiency”) in the last paragraph of Section 4.4 is that neural operator-based mechanism is suitable for real-world applications because it can handle a variety of resolutions without the process of resizing to the target resolution. But for curiosity, we also compare the efficiency of our method to the other methods in the following table:\n\n| ImageNet-1k |    Metrics   |    28   |   56   |   112   |  224  |\n|:-----------:|:------------:|:-------:|:------:|:-------:|:-----:|\n|    Resize   |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.006  |  0.006 |  0.006  | 0.006 |\n|  Fine-tune  |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.006  |  0.006 |  0.006  | 0.006 |\n|     DRLN    |    GFLOPs    |  180.66 | 412.05 | 1200.50 |  8.96 |\n|             | Runtimes (s) |  0.378  |  0.498 |  0.913  | 0.007 |\n|     DRPN    |    GFLOPs    | 1220.42 | 576.94 |  387.88 |  8.96 |\n|             | Runtimes (s) |  0.1532 |  0.164 |  0.171  | 0.007 |\n|     FNO     |    GFLOPs    |   9.78  |  9.78  |   9.78  |  9.78 |\n|             | Runtimes (s) |  0.016  |  0.016 |  0.016  | 0.016 |\n|     UNO     |    GFLOPs    |   9.10  |  9.10  |   9.10  |  9.10 |\n|             | Runtimes (s) |  0.018  |  0.018 |  0.018  | 0.018 |\n|     AFNO    |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.010  |  0.010 |  0.010  | 0.010 |\n|   PAC-FNO   |    GFLOPs    |   8.98  |  8.98  |   8.98  |  8.98 |\n|             | Runtimes (s) |  0.013  |  0.013 |  0.013  | 0.013 |\n\nAs a result, PAC-FNO showed the most efficiency in terms of FLOPs and runtimes except AFNO.\n\n---\n\nThank you for the encouraging remarks about our paper’s clear methodology explanation and the valuable feedback from the reviewer. We hope that our responses will solve the reviewer’ questions.\n\n> 1. The abandonment of the low-pass filter is one of the main innovations in this work. Although the author provides an explanation for the motivation behind this operation, it is still recommended that the author conduct ablative experiments to analyze the impact of low-frequency/high-frequency information on accuracy/generalization.\n\n$\\to$ Thank you for the suggestion! We provide an analysis of the impact of low and high-frequency information on accuracy/generalization through ablation experiments in Table 21 in Appendix F.7 of revision. Compared to PAC-FNO, accuracy and generalization decrease when using low-pass filter or high-pass filter. PAC-FNO with low pass filter show similar performance in ImageNet-1k compared with our PAC-FNO, but show a decrease in performance in terms of generalization in ImageNet-C/P Fog. On the other hand, when using a high-pass filter, it is expected to show good performance in ImageNet-C/P Fog, but it does not show good performance in terms of generalization because the performance is also poor in ImageNet-1k. Therefore, PAC-FNO, which uses both low-frequency and high-frequency components, only shows good performance in terms of accuracy/generalization. \n\n|           ImageNet-1k           |  32  |  64  |  128 |  224 |\n|:-------------------------------:|:----:|:----:|:----:|:----:|\n|  PAC-FNO (low pass filter) | 53.5 | 71.4 | 78.7 | 79.0 |\n| PAC-FNO (high pass filter) | 21.6 | 49.4 | 68.2 | 74.8 |\n|             PAC-FNO             | 58.9 | 74.5 | 80.2 | 81.5 |\n\n|         ImageNet-C/P Fog        |  32  |  64  |  128 |  224 |\n|:-------------------------------:|:----:|:----:|:----:|:----:|\n|  PAC-FNO (low pass filter) | 18.0 | 41.7 | 52.4 | 54.4 |\n| PAC-FNO (high pass filter) |  5.92  | 23.0 | 43.2 | 50.2 |\n|             PAC-FNO             | 25.4 | 48.2 | 60.1 | 62.8 |\n\n> 2. As for parallel architecture, the relevant experimental results have indeed proven its effectiveness. However, the explanation of parallel architecture in the method section appears somewhat lacking. It is hoped that the author can provide further analysis of the mechanism that enables it to be effective.\n\n$\\to$ The reason that it is effective is that the parallel configuration captures both low and high-frequency components while the serial architecture captures low-frequency components only (first paragraph in Sec. 3.2). Both low and high-frequency components must be captured to achieve good accuracy/generalization. This can be confirmed in Table 21 in Appendix F.7 in the revision. We visualize what frequency the parallel configuration and serial configuration capture in Figure 7 in Appendix F.8 of revision. Figure 7 shows that the parallel configuration more captures high frequencies than the serial configuration.\n\n---\n\nDear All Reviewers,\n\nWe thank the reviewers for taking the time to read, evaluate, and provide valuable feedback. \nWe upload a rebuttal revision that includes feedback from reviewers. Responses to reviewers feedback are highlighted in blue. \nResults for the state of the art super resolution model will be updated shortly around November 20th AoE.\n\nBest regards,\nAuthors", "author_response": "We provide an additional comparison to the method equipped with the latest super-resolution model. The results are in the following table. OSRT [1] is a state-of-the-art model in the super-resolution domain but it does not support $\\times$8 upscale. Therefore, we only use the $\\times$2 and $\\times$4 upscale models of OSRT. As a result, PAC-FNO shows better performance than OSRT and OSRT (fine-tune) methods. We also report these results in Table 13 in Appendix F.3 of the revision. \n\n|  Imagenet-1k  |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |  224 |\n|:-------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:----:|\n|               |     DBPN    |    Top-1 Acc (%)    |   40.7   |     -    |   68.2   |     -    |   79.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |   23.21  |     -    |   10.43  |     -    |   5.95   |     -    |   -  |\n|               |     DBPN    |    Top-1 Acc (%)    |   **60.8**   |     -    |   72.5   |     -    |   76.7   |     -    | 82.5 |\n|               | (Fine-tune) | # of Parameters (M) | 23.21 |     -    |   10.43   |     -    |   5.95   |     -    | - |\n| ConvNeXt-Tiny |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   61.4   |     -    |   75.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |   -  |\n|               |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   71.2   |     -    |   78.4   |     -    | 82.1 |\n|               | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |   -  |\n|               |   PAC-FNO   |    Top-1 Acc (%)    |   58.9   | **63.2** | **77.6** | **76.2** | **80.2** | **80.7** | 81.8 |\n|              |             | # of Parameters (M) |          |          |          |   3.65   |          |          |      |\n\n| Imagenet-C/P Fog |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |    224   |\n|:----------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n|                  |     DBPN    |    Top-1 Acc (%)    |   0.67   |     -    |   0.99   |     -    |   1.32   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |   23.21  |     -    |   10.43  |     -    |   5.95   |     -    |     -    |\n|                  |     DBPN    |    Top-1 Acc (%)    |   21.8   |     -    |   42.3   |     -    |   56.8   |     -    |   61.0   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   10.43  |     -    |   5.95   |     -    |     -    |\n|   ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   19.4   |     -    |   37.9   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |     -    |\n|                  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   42.3   |     -    |   56.4   |     -    |   59.4   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |     -    |\n|                  |   PAC-FNO   |    Top-1 Acc (%)    | **25.4** | **30.4** | **48.2** | **51.7** | **60.1** | **61.4** | **62.8** |\n|         `        |             | # of Parameters (M) |          |          |          |   3.65   |          |          |\n\n\n[1] Yu, Fanghua, et al. \"OSRT: Omnidirectional image super-resolution with distortion-aware transformer.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."}
{"claim": "The weakly-supervised contrastive distillation approach is not novel, having been previously used in prior works such as R1 and R2.", "claim_type": "novelty", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "P8fCEoWdti", "reviewer": "Reviewer_svRz", "review_text": "Summary: This work aims to tackle the image-to-LiDAR contrastive learning problem for LiDAR-based point cloud segmentation. Previous approaches designed the cross-modal contrastive learning objective for model pretraining, using superpixels and superpoints as guidance.\n\nIn this work, the authors observe that the superpixel-driven contrastive loss tends to involve ‘’self-conflict’’ issues during representation learning. A weakly-supervised contrastive distillation method is proposed, which generates semantic superpixels/superpoints using the Segment Anything Model (SAM). Additionally, to balance the imbalanced class distributions of LiDAR scene categories during representation, a density and category-aware sampling strategy is proposed to adjust the sampling probabilities of different anchor points using the weak semantic labels.\n\nThe overall framework is named OLIVINE, which adopts three optimization objectives:\n- Weakly-supervised contrastive distillation using coarse semantic labels to identify positive pairs by category.\n- Self-supervised contrastive distillation applied to randomly sampled point-pixel pairs.\n- A regularization framework based on the von Mises-Fisher (vMF) distribution to ensure semantic consistency.\n\nThe proposed OLIVINE method is evaluated on the nuScenes, SemanticKITTI, and KITTI object detection datasets. The results exhibit a consistent improvement of the proposed method compared to existing approaches.\n\nStrengths: (+) This work aims to improve the image-to-LiDAR self-supervised representation learning problem on LiDAR-based point cloud datasets, which is one of the current research hotspots, especially for applications related to autonomous driving and robotics.\n\n(+) The proposed method has exhibited promising performance on mainstream benchmarks, including nuScenes linear probing, nuScenes fine-tuning, SemanticKITTI fine-tuning, and KITTI object detection.\n\nWeaknesses: (-) The weakly-supervised contrastive distillation method has been used in previous literature, such as [R1] and [R2]. Adding semantic categories seems not to cause a major improvement over class-agnostic masks, as the Segment Anything Model is able to segment rather complete and semantically consistent objects and backgrounds. Additionally, using weak labels (which might be erroneous) could introduce additional errors during pretraining.\n\n(-) The motivation for using the von Mises-Fisher (vMF) distribution to enforce consistency regularization for image-to-LiDAR representation learning is not clear enough to demonstrate its superiority. A more detailed explanation and theoretical justification would strengthen this aspect of the work.\n\n(-) Compared to some of the most related works, for example, [R1] and [R3], the scale and depth regarding the experiments (for example, downstream fine-tuning on other datasets than SemanticKITTI) could be further enhanced.\n\n---\n\n### References:\n- [R1] Youquan Liu, et al. “Segment Any Point Cloud Sequences by Distilling Vision Foundation Models,” NeurIPS, 2023.\n- [R2] Ayça Takmaz, et al. “OpenMask3D: Open-Vocabulary 3D Instance Segmentation,” NeurIPS, 2023.\n- [R3] Gilles Puy, et al. “Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving,” arXiv, 2023.\n\nQuestions: - **Q1:** As mentioned in Weakness 1, the semantic masks generated by the Segment Anything Model could inevitably involve errors (e.g., wrong segmentation results). How do the authors handle the propagated errors during image-to-LiDAR representation learning?\n\n- **Q2:** As mentioned in Weakness 2, could the authors provide more details on the hyperparameter settings for the vMF distribution and the reasoning behind their chosen values? Adding a more detailed explanation and theoretical justification would be even better.\n\n- **Q3:** As mentioned in Weakness 3, having more thorough experimental analyses on other LiDAR-based point cloud datasets, such as SemanticPOSS, Waymo, SynLiDAR, etc., could further consolidate the findings and conclusions drawn in the manuscript.\n\n- **Q4:** As most 2D and 3D representation learning approaches (MoCo, SimCLR, Seal, etc.) do, having empirical analyses of models under out-of-distribution datasets is recommended. \n\n- **[Minor]:** The computational cost of the proposed multi-modal contrastive distillation approach is not thoroughly analyzed, which is crucial for real-time applications in autonomous driving.\n\n- **[Minor]:** The generalizability of OLIVINE to other types of sensors (for example, hybrid-solid LiDARs) or environments (for example, off-board environments) beyond the evaluated datasets is not discussed.\n\n- **[Minor]:** “NuScenes” should be revised to “nuScenes”.", "labeling_timestamp": "2026-01-11T16:21:53.222144", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The authors explicitly state their method differs from prior superpixel-based works (R1–R4), describe a different technical approach using individual point/pixel features for contrastive distillation, and provide experimental results showing superior performance—directly refuting the claim that the approach is not novel.", "evidence": "“We have not claimed that using **semantic superpixels** is our contribution. In fact, our method does not rely on **superpixels** at all, which is different from previous methods [R1-R4].” \"In contrast, our method directly uses the features of **individual** points and pixels for contrastive distillation.\" \"Extensive experiments demonstrate that this approach **substantially** outperforms superpixels-based (class-agnostic mask-based) methods [R1-R4] in various downstream tasks.\"", "combined_author_response": "Dear Reviewers,\n\nWe sincerely thank you for your thoughtful evaluations during the rebuttal stage. We are pleased that our detailed responses have addressed most of your concerns. We appreciate your recognition of the additional experiments and the clarification we have made to the paper.\n\nWe are grateful for the time you have invested in reviewing our work and for your consistent recognition of its value. Based on your valuable feedback, we will carefully revise the manuscript, incorporating the additional experiments and analyses into the final version.\n\nBest regards,\n\nThe Authors\n\n---\n\nDear Reviewer 54Gn,\n\nThank you for your response and for taking the time to carefully review our rebuttal. We greatly appreciate your recognition of our efforts to address your concerns and the value you found in the additional experiments we conducted. Your detailed and thoughtful review demonstrates a profound expertise in this domain. I have thoroughly enjoyed the opportunity to learn from your perspective.\n\nPlease feel free to share any further comments or suggestions.\n\nWarm regards,\n\nAuthors\n\n---\n\nDear Reviewer uKMo,\n\nThanks again for the time and energy you committed and your valuable comments. Your meticulous review and thoughtful critiques truly reflect your deep domain expertise and diligence as a reviewer. It has been a pleasure communicating and exchanging ideas with you.\n\nPlease feel free to share any additional comments or feedback on the manuscript.\n\nWarm regards,\n\nAuthors\n\n---\n\nDear Reviewer 2WGA,\n\nThank you for taking the time to review our rebuttal and for your constructive feedback throughout the process. We are glad that we could address most of your concerns. \n\nWe will actively participate in the Author-Reviewer discussion session. Please feel free to share any additional comments or feedback on the manuscript.\n\nWarm regards,\n\nAuthors\n\n---\n\nThank you for the positive feedback provided and the time devoted to this review. We are glad that our efforts have addressed your concerns. Next, we will address your remaining concerns.\n\n---\n\n**Comments**: *The authors may over-claim the contribution of using \"semantic superpixels\" over the \"class-agnostic superpixels\"*.\n\n**Response:**  We believe there may be a **misunderstanding** regarding our proposed methods. We would like to clarify the following points to address your concerns:\n\n- We have **not** claimed that using **semantic superpixels** is our contribution. In fact, our method does not rely on **superpixels** at all, which is different from previous methods [R1-R4].\n- Previous methods [R1-R4] use superpixels to pool 3D point features and 2D pixel features, learning with a superpixel-to-superpoint contrastive loss. In contrast, our method directly uses the features of **individual** points and pixels for contrastive distillation.\n- The semantic **labels** can be flexibly utilized in multiple aspects of the proposed method, such as weakly-supervised contrastive distillation, semantic-guided consistency regularization, and category-aware anchor point sampling. These aspects cannot be effectively addressed using only the class-agnostic superpixels.\n\n---\n**Comments**: *The use of semantic categories seems not to cause a major improvement over class-agnostic masks.*\n\n**Response:** Extensive experiments demonstrate that this approach **substantially** outperforms superpixels-based (class-agnostic mask-based) methods [R1-R4] in various downstream tasks.\n- Our method achieves a **significant** improvement over superpixels-based pretraining methods on nuScenes and SemanticKITTI datasets. As shown in the table below, our method outperforms Seal [R1] by a significant margin, achieving an improvement of 5.14\\% under the setting of linear probing. The full results are available in Table M1 of the uploaded PDF file.\n- Following your suggestions, we have added experiments on six additional LiDAR-based point cloud datasets and one out-of-distribution dataset. And our proposed OLIVINE **consistently** outperforms the superpixels-based methods on all datasets. For the full results, please refer to Tables M2 and M3 of the uploaded PDF file.\n\n| Method    | LP     | 1%     | 5%     | 10%    | 25%    | 100%   |\n| :-------- | :----- | :----- | :----- | :----- | :----- | :----- |\n| Random    | 8\\.10   | 30\\.30 | 47\\.84 | 56\\.15 | 65\\.48 | 74\\.66 |\n| PPKT      | 35\\.90  | 37\\.80 | 53\\.74 | 60\\.25 | 67\\.14 | 74\\.52 |\n| SLidR     | 38\\.80  | 38\\.30 | 52\\.49 | 59\\.84 | 66\\.91 | 74\\.79 |\n| ST-SLidR  | 40\\.48 | 40\\.75 | 54\\.69 | 60\\.75 | 67\\.70 | 75\\.14 |\n| HVDistill | 39\\.50  | 42\\.70 | 56\\.60 | 62\\.90 | 69\\.30 | 76\\.60 |\n| Seal      | 44\\.95 | 45\\.84 | 55\\.64 | 62\\.97 | 68\\.41 | 75\\.60 |\n| Ours      | **50\\.09** | **50\\.60** | **60\\.25** | **65\\.07** | **70\\.15** | **76\\.69** |\n---\nThanks again for your diligence as a reviewer. It is with great pleasure communicating with you. Please feel free to share any additional comments or feedback on the manuscript.\n\n**References**:\\\n[R1] Image-to-lidar self-supervised distillation for autonomous driving data.\\\n[R2] Self-supervised image-to-point distillation via semantically tolerant contrastive loss.\\\n[R3] Segment Any Point Cloud Sequences by Distilling Vision Foundation Models.\\\n[R4] HVDistill: Transferring Knowledge from Images to Point Clouds via Unsupervised Hybrid-View Distillation.\n\n---\n\n**Proposition 2**: The representation of samples in the same class can vary significantly across different batches during contrastive distillation, and semantic-guided consistency regularization helps to learn structured features.\n\n**Justification**: Without regularization, the representation of samples within the same class can vary significantly across different batches during contrastive distillation. This variance arises due to random sampling and the influence of negative samples in different batches. The weakly-supervised contrastive loss is defined as:\n\n$\\mathcal{L}_{\\mathrm{sup}} = - \\frac{1}{M_s} \\sum _{i=1}^{M_s} \\log \\left[ \\frac{1}{|A(i)|} \\sum _{a\\in A(i)} \\frac{\\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_a \\rangle/\\tau)}}{\\sum _{j=1}^{M_s} \\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_j \\rangle /\\tau)}}\\right]$\n\nThe features of negative samples $\\mathbf{G}^{\\mathrm{2D}}_j$ vary across batches, leading to different optimization paths for each mini-batch. This introduces variability in the learned representations $\\mathbf{G}^{\\mathrm{3D}}_i$ for samples of the same class $k$.\n\nWhen we do not use semantic-guided consistency regularization, the within-class variance for class $k$ across different batches is:\n\n$\\sigma_W^2 = \\frac{1}{|B|} \\sum_{B} \\frac{1}{M_k} \\sum_{i=1}^{M_k^B} \\|g_i^k - \\mu_k^B\\|^2$\n\nFor ease of reading, we use $g_i$ to refer to point feature $\\mathbf{G}^{\\mathrm{3D}}_i$.\nAnd $\\mu_k^B$ is the mean feature vector for class $k$ in batch $B$. Due to the batch-wise variability in negative samples, $\\mu_k^B$ can differ significantly across batches, leading to high within-class variance. \n\n\nBy minimizing the KL divergence, we align feature vectors $g_i$ of class $k$ with the mean direction $\\mu_k$, reducing the spread of feature vectors within the same class. The within-class variance with regularization is:\n\n$\\sigma_W^2 = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\|g_i^k - \\mu_k\\|^2$\n\nSince $\\mu_k$ is consistent across batches due to the regularization, the within-class variance is significantly reduced. This results in structured feature representations, enhancing class separability and improving performance in downstream tasks.\n\n---\n**Proposition 3**: Learning structural representation during pretraining can benefit downstream tasks.\n\n**Justification**: Structured features are those well-aligned within the same class (low within-class variance $\\sigma_W^2$) and well-separated between different classes (high between-class variance $\\sigma_B^2$). \n\nWith semantic-guided consistency regularization, feature vectors $g_i^k$ for class $k$ are closely aligned with the mean direction $\\mu_k$. This alignment reduces the within-class variance $\\sigma_W^2$. Weakly-supervised contrastive learning pushes apart feature vectors of different classes, increasing the separation between class means $\\mu_k$. This increases the between-class variance $\\sigma_B^2$.\n\nTake the linear classifier as an example, the decision boundary is determined by the separation between class means. Higher $\\sigma_B^2$ and lower $\\sigma_W^2$ result in clearer decision boundaries, reducing classification errors.\n\nConsider a simple linear classifier with weight vector $w$ and bias $b$. The decision function is:\n\n$f(x) = w^T x + b$\n\nThe decision boundary is given by:\n\n$w^T x + b = 0$\n\nFor well-structured features, the margin (distance between decision boundary and nearest samples) is maximized. The margin $ \\gamma $ for class $ k $ can be expressed as:\n\n$\\gamma = \\frac{w^T (\\mu_k - \\mu)}{\\|w\\|}$\n\nHigher between-class variance ($\\sigma_B^2$) and lower within-class variance ($\\sigma_W^2$) increase this margin, leading to better classification performance.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser.\n\n---\n\n**Proposition 1**: The features of each class $k$ can be modeled as a von Mises-Fisher (vMF) distribution. This means that for class $k$, the feature vectors $g_i$ lie on a unit hypersphere and are centered around a mean direction $\\mu_k$ with a concentration parameter $\\kappa_k$.\n\n**Justification**: To show that the features of each class can be effectively modeled by a vMF distribution, we use maximum likelihood estimation (MLE) to determine that the parameters $\\mu_k$ and $\\kappa_k$ are optimal for the given set of feature vectors.\n\nFor a set of $M_k$ feature vectors $\\\\{g_i\\\\}_{i=1}^{M_k}$ from class $k$, the likelihood function for the vMF distribution is:\n\n$L(\\mu_k, \\kappa_k) = \\prod_{i=1}^{M_k} f(g_i; \\mu_k, \\kappa_k) = \\prod_{i=1}^{M_k} \\mathcal{K}_{C}(\\kappa_k) \\exp(\\kappa_k \\mu_k^T g_i)$\n\nTaking the natural logarithm of the likelihood function, we get the log-likelihood:\n\n$\\log L(\\mu_k, \\kappa_k) = \\sum_{i=1}^{M_k} \\log f(g_i; \\mu_k, \\kappa_k) = M_k \\log \\mathcal{K}_{C}(\\kappa_k) + \\kappa_k \\sum _{i=1}^{M_k} \\mu_k^T g_i$\n\nSubstituting the expression for $\\mathcal{K}_{C}(\\kappa_k)$, we get:\n\n$\\log L(\\mu_k, \\kappa_k) = M_k \\left[ \\log \\left( \\frac{\\kappa_k^{C/2-1}}{(2\\pi)^{C/2} I_{C/2-1}(\\kappa_k)} \\right) + \\frac{\\kappa_k}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\n$\\log L(\\mu_k, \\kappa_k) = M_k \\left[ (C/2-1) \\log \\kappa_k - \\log I_{C/2-1}(\\kappa_k) - \\frac{C}{2} \\log(2\\pi) + \\frac{\\kappa_k}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\nTo maximize the log-likelihood, we normalize $\\mu_k$ by setting it to the normalized sum of the feature vectors:\n$\\mu_k = \\frac{\\sum_{i=1}^{M_k} g_i}{\\|\\sum_{i=1}^{M_k} g_i\\|}$\n\nThe derivative of the log-likelihood with respect to $\\kappa_k$ is:\n\n$\\frac{\\partial \\log L(\\mu_k, \\kappa_k)}{\\partial \\kappa_k} = M_k \\left[ \\frac{C/2-1}{\\kappa_k} - \\frac{I_{C/2}(\\kappa_k)}{I_{C/2-1}(\\kappa_k)} + \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\nSetting this derivative to zero, we get:\n\n$\\frac{C/2-1}{\\kappa_k} - \\frac{I_{C/2}(\\kappa_k)}{I_{C/2-1}(\\kappa_k)} + \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i = 0$\n\nSolving for $\\kappa_k$, we obtain:\n\n$\\kappa_k = \\frac{\\|\\sum_{i=1}^{M_k} g_i\\| (C - \\|\\sum_{i=1}^{M_k} g_i\\|^2)}{1 - \\|\\sum_{i=1}^{M_k} g_i\\|^2}$\n\nThis equation allows us to compute the concentration parameter $\\kappa_k$ based on the alignment of the feature vectors. The concentration parameter $\\kappa_k$ is larger when the distribution is more tightly clustered around the mean direction, and smaller when the features are more uniformly spread across the hypersphere.\n\nBy maximizing the likelihood function for the vMF distribution, we have shown that the parameters $\\mu_k$ and $\\kappa_k$ can be estimated to model the distribution of feature vectors for each class. \nThe mean direction $\\mu_k$ denotes the central direction of the feature cluster, and the concentration parameter $\\kappa_k$ controls the tightness of this clustering. Moreover, the way we estimate the parameters of vMF distribution in EMA is also consistent with the results of the above theoretical derivation.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser.", "author_response": "**Proposition 2**: The representation of samples in the same class can vary significantly across different batches during contrastive distillation, and semantic-guided consistency regularization helps to learn structured features.\n\n**Justification**: Without regularization, the representation of samples within the same class can vary significantly across different batches during contrastive distillation. This variance arises due to random sampling and the influence of negative samples in different batches. The weakly-supervised contrastive loss is defined as:\n\n$\\mathcal{L}_{\\mathrm{sup}} = - \\frac{1}{M_s} \\sum _{i=1}^{M_s} \\log \\left[ \\frac{1}{|A(i)|} \\sum _{a\\in A(i)} \\frac{\\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_a \\rangle/\\tau)}}{\\sum _{j=1}^{M_s} \\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_j \\rangle /\\tau)}}\\right]$\n\nThe features of negative samples $\\mathbf{G}^{\\mathrm{2D}}_j$ vary across batches, leading to different optimization paths for each mini-batch. This introduces variability in the learned representations $\\mathbf{G}^{\\mathrm{3D}}_i$ for samples of the same class $k$.\n\nWhen we do not use semantic-guided consistency regularization, the within-class variance for class $k$ across different batches is:\n\n$\\sigma_W^2 = \\frac{1}{|B|} \\sum_{B} \\frac{1}{M_k} \\sum_{i=1}^{M_k^B} \\|g_i^k - \\mu_k^B\\|^2$\n\nFor ease of reading, we use $g_i$ to refer to point feature $\\mathbf{G}^{\\mathrm{3D}}_i$.\nAnd $\\mu_k^B$ is the mean feature vector for class $k$ in batch $B$. Due to the batch-wise variability in negative samples, $\\mu_k^B$ can differ significantly across batches, leading to high within-class variance. \n\n\nBy minimizing the KL divergence, we align feature vectors $g_i$ of class $k$ with the mean direction $\\mu_k$, reducing the spread of feature vectors within the same class. The within-class variance with regularization is:\n\n$\\sigma_W^2 = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\|g_i^k - \\mu_k\\|^2$\n\nSince $\\mu_k$ is consistent across batches due to the regularization, the within-class variance is significantly reduced. This results in structured feature representations, enhancing class separability and improving performance in downstream tasks.\n\n---\n**Proposition 3**: Learning structural representation during pretraining can benefit downstream tasks.\n\n**Justification**: Structured features are those well-aligned within the same class (low within-class variance $\\sigma_W^2$) and well-separated between different classes (high between-class variance $\\sigma_B^2$). \n\nWith semantic-guided consistency regularization, feature vectors $g_i^k$ for class $k$ are closely aligned with the mean direction $\\mu_k$. This alignment reduces the within-class variance $\\sigma_W^2$. Weakly-supervised contrastive learning pushes apart feature vectors of different classes, increasing the separation between class means $\\mu_k$. This increases the between-class variance $\\sigma_B^2$.\n\nTake the linear classifier as an example, the decision boundary is determined by the separation between class means. Higher $\\sigma_B^2$ and lower $\\sigma_W^2$ result in clearer decision boundaries, reducing classification errors.\n\nConsider a simple linear classifier with weight vector $w$ and bias $b$. The decision function is:\n\n$f(x) = w^T x + b$\n\nThe decision boundary is given by:\n\n$w^T x + b = 0$\n\nFor well-structured features, the margin (distance between decision boundary and nearest samples) is maximized. The margin $ \\gamma $ for class $ k $ can be expressed as:\n\n$\\gamma = \\frac{w^T (\\mu_k - \\mu)}{\\|w\\|}$\n\nHigher between-class variance ($\\sigma_B^2$) and lower within-class variance ($\\sigma_W^2$) increase this margin, leading to better classification performance.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser."}
{"claim": "Adding semantic categories to SAM-generated masks does not appear to provide significant improvement over class-agnostic masks.", "claim_type": "experimental", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "P8fCEoWdti", "reviewer": "Reviewer_svRz", "review_text": "Summary: This work aims to tackle the image-to-LiDAR contrastive learning problem for LiDAR-based point cloud segmentation. Previous approaches designed the cross-modal contrastive learning objective for model pretraining, using superpixels and superpoints as guidance.\n\nIn this work, the authors observe that the superpixel-driven contrastive loss tends to involve ‘’self-conflict’’ issues during representation learning. A weakly-supervised contrastive distillation method is proposed, which generates semantic superpixels/superpoints using the Segment Anything Model (SAM). Additionally, to balance the imbalanced class distributions of LiDAR scene categories during representation, a density and category-aware sampling strategy is proposed to adjust the sampling probabilities of different anchor points using the weak semantic labels.\n\nThe overall framework is named OLIVINE, which adopts three optimization objectives:\n- Weakly-supervised contrastive distillation using coarse semantic labels to identify positive pairs by category.\n- Self-supervised contrastive distillation applied to randomly sampled point-pixel pairs.\n- A regularization framework based on the von Mises-Fisher (vMF) distribution to ensure semantic consistency.\n\nThe proposed OLIVINE method is evaluated on the nuScenes, SemanticKITTI, and KITTI object detection datasets. The results exhibit a consistent improvement of the proposed method compared to existing approaches.\n\nStrengths: (+) This work aims to improve the image-to-LiDAR self-supervised representation learning problem on LiDAR-based point cloud datasets, which is one of the current research hotspots, especially for applications related to autonomous driving and robotics.\n\n(+) The proposed method has exhibited promising performance on mainstream benchmarks, including nuScenes linear probing, nuScenes fine-tuning, SemanticKITTI fine-tuning, and KITTI object detection.\n\nWeaknesses: (-) The weakly-supervised contrastive distillation method has been used in previous literature, such as [R1] and [R2]. Adding semantic categories seems not to cause a major improvement over class-agnostic masks, as the Segment Anything Model is able to segment rather complete and semantically consistent objects and backgrounds. Additionally, using weak labels (which might be erroneous) could introduce additional errors during pretraining.\n\n(-) The motivation for using the von Mises-Fisher (vMF) distribution to enforce consistency regularization for image-to-LiDAR representation learning is not clear enough to demonstrate its superiority. A more detailed explanation and theoretical justification would strengthen this aspect of the work.\n\n(-) Compared to some of the most related works, for example, [R1] and [R3], the scale and depth regarding the experiments (for example, downstream fine-tuning on other datasets than SemanticKITTI) could be further enhanced.\n\n---\n\n### References:\n- [R1] Youquan Liu, et al. “Segment Any Point Cloud Sequences by Distilling Vision Foundation Models,” NeurIPS, 2023.\n- [R2] Ayça Takmaz, et al. “OpenMask3D: Open-Vocabulary 3D Instance Segmentation,” NeurIPS, 2023.\n- [R3] Gilles Puy, et al. “Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving,” arXiv, 2023.\n\nQuestions: - **Q1:** As mentioned in Weakness 1, the semantic masks generated by the Segment Anything Model could inevitably involve errors (e.g., wrong segmentation results). How do the authors handle the propagated errors during image-to-LiDAR representation learning?\n\n- **Q2:** As mentioned in Weakness 2, could the authors provide more details on the hyperparameter settings for the vMF distribution and the reasoning behind their chosen values? Adding a more detailed explanation and theoretical justification would be even better.\n\n- **Q3:** As mentioned in Weakness 3, having more thorough experimental analyses on other LiDAR-based point cloud datasets, such as SemanticPOSS, Waymo, SynLiDAR, etc., could further consolidate the findings and conclusions drawn in the manuscript.\n\n- **Q4:** As most 2D and 3D representation learning approaches (MoCo, SimCLR, Seal, etc.) do, having empirical analyses of models under out-of-distribution datasets is recommended. \n\n- **[Minor]:** The computational cost of the proposed multi-modal contrastive distillation approach is not thoroughly analyzed, which is crucial for real-time applications in autonomous driving.\n\n- **[Minor]:** The generalizability of OLIVINE to other types of sensors (for example, hybrid-solid LiDARs) or environments (for example, off-board environments) beyond the evaluated datasets is not discussed.\n\n- **[Minor]:** “NuScenes” should be revised to “nuScenes”.", "labeling_timestamp": "2026-01-11T16:21:47.482750", "model": "gpt-5-mini", "label": "Contradicted", "justification": "Authors explicitly argue that using semantic categories provides substantial gains over class-agnostic (superpixel/mask-based) methods and present quantitative results showing consistent and significant improvements.", "evidence": "Extensive experiments demonstrate that this approach **substantially** outperforms superpixels-based (class-agnostic mask-based) methods [R1-R4] in various downstream tasks. ... Our method achieves a **significant** improvement over superpixels-based pretraining methods on nuScenes and SemanticKITTI datasets. ... our proposed OLIVINE **consistently** outperforms the superpixels-based methods on all datasets.", "combined_author_response": "Dear Reviewers,\n\nWe sincerely thank you for your thoughtful evaluations during the rebuttal stage. We are pleased that our detailed responses have addressed most of your concerns. We appreciate your recognition of the additional experiments and the clarification we have made to the paper.\n\nWe are grateful for the time you have invested in reviewing our work and for your consistent recognition of its value. Based on your valuable feedback, we will carefully revise the manuscript, incorporating the additional experiments and analyses into the final version.\n\nBest regards,\n\nThe Authors\n\n---\n\nDear Reviewer 54Gn,\n\nThank you for your response and for taking the time to carefully review our rebuttal. We greatly appreciate your recognition of our efforts to address your concerns and the value you found in the additional experiments we conducted. Your detailed and thoughtful review demonstrates a profound expertise in this domain. I have thoroughly enjoyed the opportunity to learn from your perspective.\n\nPlease feel free to share any further comments or suggestions.\n\nWarm regards,\n\nAuthors\n\n---\n\nDear Reviewer uKMo,\n\nThanks again for the time and energy you committed and your valuable comments. Your meticulous review and thoughtful critiques truly reflect your deep domain expertise and diligence as a reviewer. It has been a pleasure communicating and exchanging ideas with you.\n\nPlease feel free to share any additional comments or feedback on the manuscript.\n\nWarm regards,\n\nAuthors\n\n---\n\nDear Reviewer 2WGA,\n\nThank you for taking the time to review our rebuttal and for your constructive feedback throughout the process. We are glad that we could address most of your concerns. \n\nWe will actively participate in the Author-Reviewer discussion session. Please feel free to share any additional comments or feedback on the manuscript.\n\nWarm regards,\n\nAuthors\n\n---\n\nThank you for the positive feedback provided and the time devoted to this review. We are glad that our efforts have addressed your concerns. Next, we will address your remaining concerns.\n\n---\n\n**Comments**: *The authors may over-claim the contribution of using \"semantic superpixels\" over the \"class-agnostic superpixels\"*.\n\n**Response:**  We believe there may be a **misunderstanding** regarding our proposed methods. We would like to clarify the following points to address your concerns:\n\n- We have **not** claimed that using **semantic superpixels** is our contribution. In fact, our method does not rely on **superpixels** at all, which is different from previous methods [R1-R4].\n- Previous methods [R1-R4] use superpixels to pool 3D point features and 2D pixel features, learning with a superpixel-to-superpoint contrastive loss. In contrast, our method directly uses the features of **individual** points and pixels for contrastive distillation.\n- The semantic **labels** can be flexibly utilized in multiple aspects of the proposed method, such as weakly-supervised contrastive distillation, semantic-guided consistency regularization, and category-aware anchor point sampling. These aspects cannot be effectively addressed using only the class-agnostic superpixels.\n\n---\n**Comments**: *The use of semantic categories seems not to cause a major improvement over class-agnostic masks.*\n\n**Response:** Extensive experiments demonstrate that this approach **substantially** outperforms superpixels-based (class-agnostic mask-based) methods [R1-R4] in various downstream tasks.\n- Our method achieves a **significant** improvement over superpixels-based pretraining methods on nuScenes and SemanticKITTI datasets. As shown in the table below, our method outperforms Seal [R1] by a significant margin, achieving an improvement of 5.14\\% under the setting of linear probing. The full results are available in Table M1 of the uploaded PDF file.\n- Following your suggestions, we have added experiments on six additional LiDAR-based point cloud datasets and one out-of-distribution dataset. And our proposed OLIVINE **consistently** outperforms the superpixels-based methods on all datasets. For the full results, please refer to Tables M2 and M3 of the uploaded PDF file.\n\n| Method    | LP     | 1%     | 5%     | 10%    | 25%    | 100%   |\n| :-------- | :----- | :----- | :----- | :----- | :----- | :----- |\n| Random    | 8\\.10   | 30\\.30 | 47\\.84 | 56\\.15 | 65\\.48 | 74\\.66 |\n| PPKT      | 35\\.90  | 37\\.80 | 53\\.74 | 60\\.25 | 67\\.14 | 74\\.52 |\n| SLidR     | 38\\.80  | 38\\.30 | 52\\.49 | 59\\.84 | 66\\.91 | 74\\.79 |\n| ST-SLidR  | 40\\.48 | 40\\.75 | 54\\.69 | 60\\.75 | 67\\.70 | 75\\.14 |\n| HVDistill | 39\\.50  | 42\\.70 | 56\\.60 | 62\\.90 | 69\\.30 | 76\\.60 |\n| Seal      | 44\\.95 | 45\\.84 | 55\\.64 | 62\\.97 | 68\\.41 | 75\\.60 |\n| Ours      | **50\\.09** | **50\\.60** | **60\\.25** | **65\\.07** | **70\\.15** | **76\\.69** |\n---\nThanks again for your diligence as a reviewer. It is with great pleasure communicating with you. Please feel free to share any additional comments or feedback on the manuscript.\n\n**References**:\\\n[R1] Image-to-lidar self-supervised distillation for autonomous driving data.\\\n[R2] Self-supervised image-to-point distillation via semantically tolerant contrastive loss.\\\n[R3] Segment Any Point Cloud Sequences by Distilling Vision Foundation Models.\\\n[R4] HVDistill: Transferring Knowledge from Images to Point Clouds via Unsupervised Hybrid-View Distillation.\n\n---\n\n**Proposition 2**: The representation of samples in the same class can vary significantly across different batches during contrastive distillation, and semantic-guided consistency regularization helps to learn structured features.\n\n**Justification**: Without regularization, the representation of samples within the same class can vary significantly across different batches during contrastive distillation. This variance arises due to random sampling and the influence of negative samples in different batches. The weakly-supervised contrastive loss is defined as:\n\n$\\mathcal{L}_{\\mathrm{sup}} = - \\frac{1}{M_s} \\sum _{i=1}^{M_s} \\log \\left[ \\frac{1}{|A(i)|} \\sum _{a\\in A(i)} \\frac{\\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_a \\rangle/\\tau)}}{\\sum _{j=1}^{M_s} \\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_j \\rangle /\\tau)}}\\right]$\n\nThe features of negative samples $\\mathbf{G}^{\\mathrm{2D}}_j$ vary across batches, leading to different optimization paths for each mini-batch. This introduces variability in the learned representations $\\mathbf{G}^{\\mathrm{3D}}_i$ for samples of the same class $k$.\n\nWhen we do not use semantic-guided consistency regularization, the within-class variance for class $k$ across different batches is:\n\n$\\sigma_W^2 = \\frac{1}{|B|} \\sum_{B} \\frac{1}{M_k} \\sum_{i=1}^{M_k^B} \\|g_i^k - \\mu_k^B\\|^2$\n\nFor ease of reading, we use $g_i$ to refer to point feature $\\mathbf{G}^{\\mathrm{3D}}_i$.\nAnd $\\mu_k^B$ is the mean feature vector for class $k$ in batch $B$. Due to the batch-wise variability in negative samples, $\\mu_k^B$ can differ significantly across batches, leading to high within-class variance. \n\n\nBy minimizing the KL divergence, we align feature vectors $g_i$ of class $k$ with the mean direction $\\mu_k$, reducing the spread of feature vectors within the same class. The within-class variance with regularization is:\n\n$\\sigma_W^2 = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\|g_i^k - \\mu_k\\|^2$\n\nSince $\\mu_k$ is consistent across batches due to the regularization, the within-class variance is significantly reduced. This results in structured feature representations, enhancing class separability and improving performance in downstream tasks.\n\n---\n**Proposition 3**: Learning structural representation during pretraining can benefit downstream tasks.\n\n**Justification**: Structured features are those well-aligned within the same class (low within-class variance $\\sigma_W^2$) and well-separated between different classes (high between-class variance $\\sigma_B^2$). \n\nWith semantic-guided consistency regularization, feature vectors $g_i^k$ for class $k$ are closely aligned with the mean direction $\\mu_k$. This alignment reduces the within-class variance $\\sigma_W^2$. Weakly-supervised contrastive learning pushes apart feature vectors of different classes, increasing the separation between class means $\\mu_k$. This increases the between-class variance $\\sigma_B^2$.\n\nTake the linear classifier as an example, the decision boundary is determined by the separation between class means. Higher $\\sigma_B^2$ and lower $\\sigma_W^2$ result in clearer decision boundaries, reducing classification errors.\n\nConsider a simple linear classifier with weight vector $w$ and bias $b$. The decision function is:\n\n$f(x) = w^T x + b$\n\nThe decision boundary is given by:\n\n$w^T x + b = 0$\n\nFor well-structured features, the margin (distance between decision boundary and nearest samples) is maximized. The margin $ \\gamma $ for class $ k $ can be expressed as:\n\n$\\gamma = \\frac{w^T (\\mu_k - \\mu)}{\\|w\\|}$\n\nHigher between-class variance ($\\sigma_B^2$) and lower within-class variance ($\\sigma_W^2$) increase this margin, leading to better classification performance.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser.\n\n---\n\n**Proposition 1**: The features of each class $k$ can be modeled as a von Mises-Fisher (vMF) distribution. This means that for class $k$, the feature vectors $g_i$ lie on a unit hypersphere and are centered around a mean direction $\\mu_k$ with a concentration parameter $\\kappa_k$.\n\n**Justification**: To show that the features of each class can be effectively modeled by a vMF distribution, we use maximum likelihood estimation (MLE) to determine that the parameters $\\mu_k$ and $\\kappa_k$ are optimal for the given set of feature vectors.\n\nFor a set of $M_k$ feature vectors $\\\\{g_i\\\\}_{i=1}^{M_k}$ from class $k$, the likelihood function for the vMF distribution is:\n\n$L(\\mu_k, \\kappa_k) = \\prod_{i=1}^{M_k} f(g_i; \\mu_k, \\kappa_k) = \\prod_{i=1}^{M_k} \\mathcal{K}_{C}(\\kappa_k) \\exp(\\kappa_k \\mu_k^T g_i)$\n\nTaking the natural logarithm of the likelihood function, we get the log-likelihood:\n\n$\\log L(\\mu_k, \\kappa_k) = \\sum_{i=1}^{M_k} \\log f(g_i; \\mu_k, \\kappa_k) = M_k \\log \\mathcal{K}_{C}(\\kappa_k) + \\kappa_k \\sum _{i=1}^{M_k} \\mu_k^T g_i$\n\nSubstituting the expression for $\\mathcal{K}_{C}(\\kappa_k)$, we get:\n\n$\\log L(\\mu_k, \\kappa_k) = M_k \\left[ \\log \\left( \\frac{\\kappa_k^{C/2-1}}{(2\\pi)^{C/2} I_{C/2-1}(\\kappa_k)} \\right) + \\frac{\\kappa_k}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\n$\\log L(\\mu_k, \\kappa_k) = M_k \\left[ (C/2-1) \\log \\kappa_k - \\log I_{C/2-1}(\\kappa_k) - \\frac{C}{2} \\log(2\\pi) + \\frac{\\kappa_k}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\nTo maximize the log-likelihood, we normalize $\\mu_k$ by setting it to the normalized sum of the feature vectors:\n$\\mu_k = \\frac{\\sum_{i=1}^{M_k} g_i}{\\|\\sum_{i=1}^{M_k} g_i\\|}$\n\nThe derivative of the log-likelihood with respect to $\\kappa_k$ is:\n\n$\\frac{\\partial \\log L(\\mu_k, \\kappa_k)}{\\partial \\kappa_k} = M_k \\left[ \\frac{C/2-1}{\\kappa_k} - \\frac{I_{C/2}(\\kappa_k)}{I_{C/2-1}(\\kappa_k)} + \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\nSetting this derivative to zero, we get:\n\n$\\frac{C/2-1}{\\kappa_k} - \\frac{I_{C/2}(\\kappa_k)}{I_{C/2-1}(\\kappa_k)} + \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i = 0$\n\nSolving for $\\kappa_k$, we obtain:\n\n$\\kappa_k = \\frac{\\|\\sum_{i=1}^{M_k} g_i\\| (C - \\|\\sum_{i=1}^{M_k} g_i\\|^2)}{1 - \\|\\sum_{i=1}^{M_k} g_i\\|^2}$\n\nThis equation allows us to compute the concentration parameter $\\kappa_k$ based on the alignment of the feature vectors. The concentration parameter $\\kappa_k$ is larger when the distribution is more tightly clustered around the mean direction, and smaller when the features are more uniformly spread across the hypersphere.\n\nBy maximizing the likelihood function for the vMF distribution, we have shown that the parameters $\\mu_k$ and $\\kappa_k$ can be estimated to model the distribution of feature vectors for each class. \nThe mean direction $\\mu_k$ denotes the central direction of the feature cluster, and the concentration parameter $\\kappa_k$ controls the tightness of this clustering. Moreover, the way we estimate the parameters of vMF distribution in EMA is also consistent with the results of the above theoretical derivation.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser.", "author_response": "**Proposition 2**: The representation of samples in the same class can vary significantly across different batches during contrastive distillation, and semantic-guided consistency regularization helps to learn structured features.\n\n**Justification**: Without regularization, the representation of samples within the same class can vary significantly across different batches during contrastive distillation. This variance arises due to random sampling and the influence of negative samples in different batches. The weakly-supervised contrastive loss is defined as:\n\n$\\mathcal{L}_{\\mathrm{sup}} = - \\frac{1}{M_s} \\sum _{i=1}^{M_s} \\log \\left[ \\frac{1}{|A(i)|} \\sum _{a\\in A(i)} \\frac{\\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_a \\rangle/\\tau)}}{\\sum _{j=1}^{M_s} \\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_j \\rangle /\\tau)}}\\right]$\n\nThe features of negative samples $\\mathbf{G}^{\\mathrm{2D}}_j$ vary across batches, leading to different optimization paths for each mini-batch. This introduces variability in the learned representations $\\mathbf{G}^{\\mathrm{3D}}_i$ for samples of the same class $k$.\n\nWhen we do not use semantic-guided consistency regularization, the within-class variance for class $k$ across different batches is:\n\n$\\sigma_W^2 = \\frac{1}{|B|} \\sum_{B} \\frac{1}{M_k} \\sum_{i=1}^{M_k^B} \\|g_i^k - \\mu_k^B\\|^2$\n\nFor ease of reading, we use $g_i$ to refer to point feature $\\mathbf{G}^{\\mathrm{3D}}_i$.\nAnd $\\mu_k^B$ is the mean feature vector for class $k$ in batch $B$. Due to the batch-wise variability in negative samples, $\\mu_k^B$ can differ significantly across batches, leading to high within-class variance. \n\n\nBy minimizing the KL divergence, we align feature vectors $g_i$ of class $k$ with the mean direction $\\mu_k$, reducing the spread of feature vectors within the same class. The within-class variance with regularization is:\n\n$\\sigma_W^2 = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\|g_i^k - \\mu_k\\|^2$\n\nSince $\\mu_k$ is consistent across batches due to the regularization, the within-class variance is significantly reduced. This results in structured feature representations, enhancing class separability and improving performance in downstream tasks.\n\n---\n**Proposition 3**: Learning structural representation during pretraining can benefit downstream tasks.\n\n**Justification**: Structured features are those well-aligned within the same class (low within-class variance $\\sigma_W^2$) and well-separated between different classes (high between-class variance $\\sigma_B^2$). \n\nWith semantic-guided consistency regularization, feature vectors $g_i^k$ for class $k$ are closely aligned with the mean direction $\\mu_k$. This alignment reduces the within-class variance $\\sigma_W^2$. Weakly-supervised contrastive learning pushes apart feature vectors of different classes, increasing the separation between class means $\\mu_k$. This increases the between-class variance $\\sigma_B^2$.\n\nTake the linear classifier as an example, the decision boundary is determined by the separation between class means. Higher $\\sigma_B^2$ and lower $\\sigma_W^2$ result in clearer decision boundaries, reducing classification errors.\n\nConsider a simple linear classifier with weight vector $w$ and bias $b$. The decision function is:\n\n$f(x) = w^T x + b$\n\nThe decision boundary is given by:\n\n$w^T x + b = 0$\n\nFor well-structured features, the margin (distance between decision boundary and nearest samples) is maximized. The margin $ \\gamma $ for class $ k $ can be expressed as:\n\n$\\gamma = \\frac{w^T (\\mu_k - \\mu)}{\\|w\\|}$\n\nHigher between-class variance ($\\sigma_B^2$) and lower within-class variance ($\\sigma_W^2$) increase this margin, leading to better classification performance.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser."}
{"claim": "Using weak semantic labels from SAM can introduce erroneous labels that propagate errors during pretraining.", "claim_type": "experimental", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "P8fCEoWdti", "reviewer": "Reviewer_svRz", "review_text": "Summary: This work aims to tackle the image-to-LiDAR contrastive learning problem for LiDAR-based point cloud segmentation. Previous approaches designed the cross-modal contrastive learning objective for model pretraining, using superpixels and superpoints as guidance.\n\nIn this work, the authors observe that the superpixel-driven contrastive loss tends to involve ‘’self-conflict’’ issues during representation learning. A weakly-supervised contrastive distillation method is proposed, which generates semantic superpixels/superpoints using the Segment Anything Model (SAM). Additionally, to balance the imbalanced class distributions of LiDAR scene categories during representation, a density and category-aware sampling strategy is proposed to adjust the sampling probabilities of different anchor points using the weak semantic labels.\n\nThe overall framework is named OLIVINE, which adopts three optimization objectives:\n- Weakly-supervised contrastive distillation using coarse semantic labels to identify positive pairs by category.\n- Self-supervised contrastive distillation applied to randomly sampled point-pixel pairs.\n- A regularization framework based on the von Mises-Fisher (vMF) distribution to ensure semantic consistency.\n\nThe proposed OLIVINE method is evaluated on the nuScenes, SemanticKITTI, and KITTI object detection datasets. The results exhibit a consistent improvement of the proposed method compared to existing approaches.\n\nStrengths: (+) This work aims to improve the image-to-LiDAR self-supervised representation learning problem on LiDAR-based point cloud datasets, which is one of the current research hotspots, especially for applications related to autonomous driving and robotics.\n\n(+) The proposed method has exhibited promising performance on mainstream benchmarks, including nuScenes linear probing, nuScenes fine-tuning, SemanticKITTI fine-tuning, and KITTI object detection.\n\nWeaknesses: (-) The weakly-supervised contrastive distillation method has been used in previous literature, such as [R1] and [R2]. Adding semantic categories seems not to cause a major improvement over class-agnostic masks, as the Segment Anything Model is able to segment rather complete and semantically consistent objects and backgrounds. Additionally, using weak labels (which might be erroneous) could introduce additional errors during pretraining.\n\n(-) The motivation for using the von Mises-Fisher (vMF) distribution to enforce consistency regularization for image-to-LiDAR representation learning is not clear enough to demonstrate its superiority. A more detailed explanation and theoretical justification would strengthen this aspect of the work.\n\n(-) Compared to some of the most related works, for example, [R1] and [R3], the scale and depth regarding the experiments (for example, downstream fine-tuning on other datasets than SemanticKITTI) could be further enhanced.\n\n---\n\n### References:\n- [R1] Youquan Liu, et al. “Segment Any Point Cloud Sequences by Distilling Vision Foundation Models,” NeurIPS, 2023.\n- [R2] Ayça Takmaz, et al. “OpenMask3D: Open-Vocabulary 3D Instance Segmentation,” NeurIPS, 2023.\n- [R3] Gilles Puy, et al. “Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving,” arXiv, 2023.\n\nQuestions: - **Q1:** As mentioned in Weakness 1, the semantic masks generated by the Segment Anything Model could inevitably involve errors (e.g., wrong segmentation results). How do the authors handle the propagated errors during image-to-LiDAR representation learning?\n\n- **Q2:** As mentioned in Weakness 2, could the authors provide more details on the hyperparameter settings for the vMF distribution and the reasoning behind their chosen values? Adding a more detailed explanation and theoretical justification would be even better.\n\n- **Q3:** As mentioned in Weakness 3, having more thorough experimental analyses on other LiDAR-based point cloud datasets, such as SemanticPOSS, Waymo, SynLiDAR, etc., could further consolidate the findings and conclusions drawn in the manuscript.\n\n- **Q4:** As most 2D and 3D representation learning approaches (MoCo, SimCLR, Seal, etc.) do, having empirical analyses of models under out-of-distribution datasets is recommended. \n\n- **[Minor]:** The computational cost of the proposed multi-modal contrastive distillation approach is not thoroughly analyzed, which is crucial for real-time applications in autonomous driving.\n\n- **[Minor]:** The generalizability of OLIVINE to other types of sensors (for example, hybrid-solid LiDARs) or environments (for example, off-board environments) beyond the evaluated datasets is not discussed.\n\n- **[Minor]:** “NuScenes” should be revised to “nuScenes”.", "labeling_timestamp": "2026-01-11T16:21:53.610761", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors do not explicitly address the reviewer's concern about noisy/erroneous semantic labels from SAM and their potential to propagate errors during pretraining. They clarify method details and present empirical improvements over superpixel-based approaches, but they do not acknowledge, analyze, or rebut the specific risk of erroneous SAM labels or describe robustness/mitigation.", "evidence": "\"We have not claimed that using **semantic superpixels** is our contribution. In fact, our method does not rely on **superpixels** at all...\"; \"The semantic **labels** can be flexibly utilized in multiple aspects of the proposed method...\"; \"Extensive experiments demonstrate that this approach **substantially** outperforms superpixels-based (class-agnostic mask-based) methods [R1-R4]...\"", "combined_author_response": "Dear Reviewers,\n\nWe sincerely thank you for your thoughtful evaluations during the rebuttal stage. We are pleased that our detailed responses have addressed most of your concerns. We appreciate your recognition of the additional experiments and the clarification we have made to the paper.\n\nWe are grateful for the time you have invested in reviewing our work and for your consistent recognition of its value. Based on your valuable feedback, we will carefully revise the manuscript, incorporating the additional experiments and analyses into the final version.\n\nBest regards,\n\nThe Authors\n\n---\n\nDear Reviewer 54Gn,\n\nThank you for your response and for taking the time to carefully review our rebuttal. We greatly appreciate your recognition of our efforts to address your concerns and the value you found in the additional experiments we conducted. Your detailed and thoughtful review demonstrates a profound expertise in this domain. I have thoroughly enjoyed the opportunity to learn from your perspective.\n\nPlease feel free to share any further comments or suggestions.\n\nWarm regards,\n\nAuthors\n\n---\n\nDear Reviewer uKMo,\n\nThanks again for the time and energy you committed and your valuable comments. Your meticulous review and thoughtful critiques truly reflect your deep domain expertise and diligence as a reviewer. It has been a pleasure communicating and exchanging ideas with you.\n\nPlease feel free to share any additional comments or feedback on the manuscript.\n\nWarm regards,\n\nAuthors\n\n---\n\nDear Reviewer 2WGA,\n\nThank you for taking the time to review our rebuttal and for your constructive feedback throughout the process. We are glad that we could address most of your concerns. \n\nWe will actively participate in the Author-Reviewer discussion session. Please feel free to share any additional comments or feedback on the manuscript.\n\nWarm regards,\n\nAuthors\n\n---\n\nThank you for the positive feedback provided and the time devoted to this review. We are glad that our efforts have addressed your concerns. Next, we will address your remaining concerns.\n\n---\n\n**Comments**: *The authors may over-claim the contribution of using \"semantic superpixels\" over the \"class-agnostic superpixels\"*.\n\n**Response:**  We believe there may be a **misunderstanding** regarding our proposed methods. We would like to clarify the following points to address your concerns:\n\n- We have **not** claimed that using **semantic superpixels** is our contribution. In fact, our method does not rely on **superpixels** at all, which is different from previous methods [R1-R4].\n- Previous methods [R1-R4] use superpixels to pool 3D point features and 2D pixel features, learning with a superpixel-to-superpoint contrastive loss. In contrast, our method directly uses the features of **individual** points and pixels for contrastive distillation.\n- The semantic **labels** can be flexibly utilized in multiple aspects of the proposed method, such as weakly-supervised contrastive distillation, semantic-guided consistency regularization, and category-aware anchor point sampling. These aspects cannot be effectively addressed using only the class-agnostic superpixels.\n\n---\n**Comments**: *The use of semantic categories seems not to cause a major improvement over class-agnostic masks.*\n\n**Response:** Extensive experiments demonstrate that this approach **substantially** outperforms superpixels-based (class-agnostic mask-based) methods [R1-R4] in various downstream tasks.\n- Our method achieves a **significant** improvement over superpixels-based pretraining methods on nuScenes and SemanticKITTI datasets. As shown in the table below, our method outperforms Seal [R1] by a significant margin, achieving an improvement of 5.14\\% under the setting of linear probing. The full results are available in Table M1 of the uploaded PDF file.\n- Following your suggestions, we have added experiments on six additional LiDAR-based point cloud datasets and one out-of-distribution dataset. And our proposed OLIVINE **consistently** outperforms the superpixels-based methods on all datasets. For the full results, please refer to Tables M2 and M3 of the uploaded PDF file.\n\n| Method    | LP     | 1%     | 5%     | 10%    | 25%    | 100%   |\n| :-------- | :----- | :----- | :----- | :----- | :----- | :----- |\n| Random    | 8\\.10   | 30\\.30 | 47\\.84 | 56\\.15 | 65\\.48 | 74\\.66 |\n| PPKT      | 35\\.90  | 37\\.80 | 53\\.74 | 60\\.25 | 67\\.14 | 74\\.52 |\n| SLidR     | 38\\.80  | 38\\.30 | 52\\.49 | 59\\.84 | 66\\.91 | 74\\.79 |\n| ST-SLidR  | 40\\.48 | 40\\.75 | 54\\.69 | 60\\.75 | 67\\.70 | 75\\.14 |\n| HVDistill | 39\\.50  | 42\\.70 | 56\\.60 | 62\\.90 | 69\\.30 | 76\\.60 |\n| Seal      | 44\\.95 | 45\\.84 | 55\\.64 | 62\\.97 | 68\\.41 | 75\\.60 |\n| Ours      | **50\\.09** | **50\\.60** | **60\\.25** | **65\\.07** | **70\\.15** | **76\\.69** |\n---\nThanks again for your diligence as a reviewer. It is with great pleasure communicating with you. Please feel free to share any additional comments or feedback on the manuscript.\n\n**References**:\\\n[R1] Image-to-lidar self-supervised distillation for autonomous driving data.\\\n[R2] Self-supervised image-to-point distillation via semantically tolerant contrastive loss.\\\n[R3] Segment Any Point Cloud Sequences by Distilling Vision Foundation Models.\\\n[R4] HVDistill: Transferring Knowledge from Images to Point Clouds via Unsupervised Hybrid-View Distillation.\n\n---\n\n**Proposition 2**: The representation of samples in the same class can vary significantly across different batches during contrastive distillation, and semantic-guided consistency regularization helps to learn structured features.\n\n**Justification**: Without regularization, the representation of samples within the same class can vary significantly across different batches during contrastive distillation. This variance arises due to random sampling and the influence of negative samples in different batches. The weakly-supervised contrastive loss is defined as:\n\n$\\mathcal{L}_{\\mathrm{sup}} = - \\frac{1}{M_s} \\sum _{i=1}^{M_s} \\log \\left[ \\frac{1}{|A(i)|} \\sum _{a\\in A(i)} \\frac{\\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_a \\rangle/\\tau)}}{\\sum _{j=1}^{M_s} \\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_j \\rangle /\\tau)}}\\right]$\n\nThe features of negative samples $\\mathbf{G}^{\\mathrm{2D}}_j$ vary across batches, leading to different optimization paths for each mini-batch. This introduces variability in the learned representations $\\mathbf{G}^{\\mathrm{3D}}_i$ for samples of the same class $k$.\n\nWhen we do not use semantic-guided consistency regularization, the within-class variance for class $k$ across different batches is:\n\n$\\sigma_W^2 = \\frac{1}{|B|} \\sum_{B} \\frac{1}{M_k} \\sum_{i=1}^{M_k^B} \\|g_i^k - \\mu_k^B\\|^2$\n\nFor ease of reading, we use $g_i$ to refer to point feature $\\mathbf{G}^{\\mathrm{3D}}_i$.\nAnd $\\mu_k^B$ is the mean feature vector for class $k$ in batch $B$. Due to the batch-wise variability in negative samples, $\\mu_k^B$ can differ significantly across batches, leading to high within-class variance. \n\n\nBy minimizing the KL divergence, we align feature vectors $g_i$ of class $k$ with the mean direction $\\mu_k$, reducing the spread of feature vectors within the same class. The within-class variance with regularization is:\n\n$\\sigma_W^2 = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\|g_i^k - \\mu_k\\|^2$\n\nSince $\\mu_k$ is consistent across batches due to the regularization, the within-class variance is significantly reduced. This results in structured feature representations, enhancing class separability and improving performance in downstream tasks.\n\n---\n**Proposition 3**: Learning structural representation during pretraining can benefit downstream tasks.\n\n**Justification**: Structured features are those well-aligned within the same class (low within-class variance $\\sigma_W^2$) and well-separated between different classes (high between-class variance $\\sigma_B^2$). \n\nWith semantic-guided consistency regularization, feature vectors $g_i^k$ for class $k$ are closely aligned with the mean direction $\\mu_k$. This alignment reduces the within-class variance $\\sigma_W^2$. Weakly-supervised contrastive learning pushes apart feature vectors of different classes, increasing the separation between class means $\\mu_k$. This increases the between-class variance $\\sigma_B^2$.\n\nTake the linear classifier as an example, the decision boundary is determined by the separation between class means. Higher $\\sigma_B^2$ and lower $\\sigma_W^2$ result in clearer decision boundaries, reducing classification errors.\n\nConsider a simple linear classifier with weight vector $w$ and bias $b$. The decision function is:\n\n$f(x) = w^T x + b$\n\nThe decision boundary is given by:\n\n$w^T x + b = 0$\n\nFor well-structured features, the margin (distance between decision boundary and nearest samples) is maximized. The margin $ \\gamma $ for class $ k $ can be expressed as:\n\n$\\gamma = \\frac{w^T (\\mu_k - \\mu)}{\\|w\\|}$\n\nHigher between-class variance ($\\sigma_B^2$) and lower within-class variance ($\\sigma_W^2$) increase this margin, leading to better classification performance.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser.\n\n---\n\n**Proposition 1**: The features of each class $k$ can be modeled as a von Mises-Fisher (vMF) distribution. This means that for class $k$, the feature vectors $g_i$ lie on a unit hypersphere and are centered around a mean direction $\\mu_k$ with a concentration parameter $\\kappa_k$.\n\n**Justification**: To show that the features of each class can be effectively modeled by a vMF distribution, we use maximum likelihood estimation (MLE) to determine that the parameters $\\mu_k$ and $\\kappa_k$ are optimal for the given set of feature vectors.\n\nFor a set of $M_k$ feature vectors $\\\\{g_i\\\\}_{i=1}^{M_k}$ from class $k$, the likelihood function for the vMF distribution is:\n\n$L(\\mu_k, \\kappa_k) = \\prod_{i=1}^{M_k} f(g_i; \\mu_k, \\kappa_k) = \\prod_{i=1}^{M_k} \\mathcal{K}_{C}(\\kappa_k) \\exp(\\kappa_k \\mu_k^T g_i)$\n\nTaking the natural logarithm of the likelihood function, we get the log-likelihood:\n\n$\\log L(\\mu_k, \\kappa_k) = \\sum_{i=1}^{M_k} \\log f(g_i; \\mu_k, \\kappa_k) = M_k \\log \\mathcal{K}_{C}(\\kappa_k) + \\kappa_k \\sum _{i=1}^{M_k} \\mu_k^T g_i$\n\nSubstituting the expression for $\\mathcal{K}_{C}(\\kappa_k)$, we get:\n\n$\\log L(\\mu_k, \\kappa_k) = M_k \\left[ \\log \\left( \\frac{\\kappa_k^{C/2-1}}{(2\\pi)^{C/2} I_{C/2-1}(\\kappa_k)} \\right) + \\frac{\\kappa_k}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\n$\\log L(\\mu_k, \\kappa_k) = M_k \\left[ (C/2-1) \\log \\kappa_k - \\log I_{C/2-1}(\\kappa_k) - \\frac{C}{2} \\log(2\\pi) + \\frac{\\kappa_k}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\nTo maximize the log-likelihood, we normalize $\\mu_k$ by setting it to the normalized sum of the feature vectors:\n$\\mu_k = \\frac{\\sum_{i=1}^{M_k} g_i}{\\|\\sum_{i=1}^{M_k} g_i\\|}$\n\nThe derivative of the log-likelihood with respect to $\\kappa_k$ is:\n\n$\\frac{\\partial \\log L(\\mu_k, \\kappa_k)}{\\partial \\kappa_k} = M_k \\left[ \\frac{C/2-1}{\\kappa_k} - \\frac{I_{C/2}(\\kappa_k)}{I_{C/2-1}(\\kappa_k)} + \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\nSetting this derivative to zero, we get:\n\n$\\frac{C/2-1}{\\kappa_k} - \\frac{I_{C/2}(\\kappa_k)}{I_{C/2-1}(\\kappa_k)} + \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i = 0$\n\nSolving for $\\kappa_k$, we obtain:\n\n$\\kappa_k = \\frac{\\|\\sum_{i=1}^{M_k} g_i\\| (C - \\|\\sum_{i=1}^{M_k} g_i\\|^2)}{1 - \\|\\sum_{i=1}^{M_k} g_i\\|^2}$\n\nThis equation allows us to compute the concentration parameter $\\kappa_k$ based on the alignment of the feature vectors. The concentration parameter $\\kappa_k$ is larger when the distribution is more tightly clustered around the mean direction, and smaller when the features are more uniformly spread across the hypersphere.\n\nBy maximizing the likelihood function for the vMF distribution, we have shown that the parameters $\\mu_k$ and $\\kappa_k$ can be estimated to model the distribution of feature vectors for each class. \nThe mean direction $\\mu_k$ denotes the central direction of the feature cluster, and the concentration parameter $\\kappa_k$ controls the tightness of this clustering. Moreover, the way we estimate the parameters of vMF distribution in EMA is also consistent with the results of the above theoretical derivation.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser.", "author_response": "**Proposition 2**: The representation of samples in the same class can vary significantly across different batches during contrastive distillation, and semantic-guided consistency regularization helps to learn structured features.\n\n**Justification**: Without regularization, the representation of samples within the same class can vary significantly across different batches during contrastive distillation. This variance arises due to random sampling and the influence of negative samples in different batches. The weakly-supervised contrastive loss is defined as:\n\n$\\mathcal{L}_{\\mathrm{sup}} = - \\frac{1}{M_s} \\sum _{i=1}^{M_s} \\log \\left[ \\frac{1}{|A(i)|} \\sum _{a\\in A(i)} \\frac{\\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_a \\rangle/\\tau)}}{\\sum _{j=1}^{M_s} \\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_j \\rangle /\\tau)}}\\right]$\n\nThe features of negative samples $\\mathbf{G}^{\\mathrm{2D}}_j$ vary across batches, leading to different optimization paths for each mini-batch. This introduces variability in the learned representations $\\mathbf{G}^{\\mathrm{3D}}_i$ for samples of the same class $k$.\n\nWhen we do not use semantic-guided consistency regularization, the within-class variance for class $k$ across different batches is:\n\n$\\sigma_W^2 = \\frac{1}{|B|} \\sum_{B} \\frac{1}{M_k} \\sum_{i=1}^{M_k^B} \\|g_i^k - \\mu_k^B\\|^2$\n\nFor ease of reading, we use $g_i$ to refer to point feature $\\mathbf{G}^{\\mathrm{3D}}_i$.\nAnd $\\mu_k^B$ is the mean feature vector for class $k$ in batch $B$. Due to the batch-wise variability in negative samples, $\\mu_k^B$ can differ significantly across batches, leading to high within-class variance. \n\n\nBy minimizing the KL divergence, we align feature vectors $g_i$ of class $k$ with the mean direction $\\mu_k$, reducing the spread of feature vectors within the same class. The within-class variance with regularization is:\n\n$\\sigma_W^2 = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\|g_i^k - \\mu_k\\|^2$\n\nSince $\\mu_k$ is consistent across batches due to the regularization, the within-class variance is significantly reduced. This results in structured feature representations, enhancing class separability and improving performance in downstream tasks.\n\n---\n**Proposition 3**: Learning structural representation during pretraining can benefit downstream tasks.\n\n**Justification**: Structured features are those well-aligned within the same class (low within-class variance $\\sigma_W^2$) and well-separated between different classes (high between-class variance $\\sigma_B^2$). \n\nWith semantic-guided consistency regularization, feature vectors $g_i^k$ for class $k$ are closely aligned with the mean direction $\\mu_k$. This alignment reduces the within-class variance $\\sigma_W^2$. Weakly-supervised contrastive learning pushes apart feature vectors of different classes, increasing the separation between class means $\\mu_k$. This increases the between-class variance $\\sigma_B^2$.\n\nTake the linear classifier as an example, the decision boundary is determined by the separation between class means. Higher $\\sigma_B^2$ and lower $\\sigma_W^2$ result in clearer decision boundaries, reducing classification errors.\n\nConsider a simple linear classifier with weight vector $w$ and bias $b$. The decision function is:\n\n$f(x) = w^T x + b$\n\nThe decision boundary is given by:\n\n$w^T x + b = 0$\n\nFor well-structured features, the margin (distance between decision boundary and nearest samples) is maximized. The margin $ \\gamma $ for class $ k $ can be expressed as:\n\n$\\gamma = \\frac{w^T (\\mu_k - \\mu)}{\\|w\\|}$\n\nHigher between-class variance ($\\sigma_B^2$) and lower within-class variance ($\\sigma_W^2$) increase this margin, leading to better classification performance.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser."}
{"claim": "The manuscript does not clearly justify the motivation or theoretical basis for using the von Mises-Fisher distribution.", "claim_type": "presentation", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "P8fCEoWdti", "reviewer": "Reviewer_svRz", "review_text": "Summary: This work aims to tackle the image-to-LiDAR contrastive learning problem for LiDAR-based point cloud segmentation. Previous approaches designed the cross-modal contrastive learning objective for model pretraining, using superpixels and superpoints as guidance.\n\nIn this work, the authors observe that the superpixel-driven contrastive loss tends to involve ‘’self-conflict’’ issues during representation learning. A weakly-supervised contrastive distillation method is proposed, which generates semantic superpixels/superpoints using the Segment Anything Model (SAM). Additionally, to balance the imbalanced class distributions of LiDAR scene categories during representation, a density and category-aware sampling strategy is proposed to adjust the sampling probabilities of different anchor points using the weak semantic labels.\n\nThe overall framework is named OLIVINE, which adopts three optimization objectives:\n- Weakly-supervised contrastive distillation using coarse semantic labels to identify positive pairs by category.\n- Self-supervised contrastive distillation applied to randomly sampled point-pixel pairs.\n- A regularization framework based on the von Mises-Fisher (vMF) distribution to ensure semantic consistency.\n\nThe proposed OLIVINE method is evaluated on the nuScenes, SemanticKITTI, and KITTI object detection datasets. The results exhibit a consistent improvement of the proposed method compared to existing approaches.\n\nStrengths: (+) This work aims to improve the image-to-LiDAR self-supervised representation learning problem on LiDAR-based point cloud datasets, which is one of the current research hotspots, especially for applications related to autonomous driving and robotics.\n\n(+) The proposed method has exhibited promising performance on mainstream benchmarks, including nuScenes linear probing, nuScenes fine-tuning, SemanticKITTI fine-tuning, and KITTI object detection.\n\nWeaknesses: (-) The weakly-supervised contrastive distillation method has been used in previous literature, such as [R1] and [R2]. Adding semantic categories seems not to cause a major improvement over class-agnostic masks, as the Segment Anything Model is able to segment rather complete and semantically consistent objects and backgrounds. Additionally, using weak labels (which might be erroneous) could introduce additional errors during pretraining.\n\n(-) The motivation for using the von Mises-Fisher (vMF) distribution to enforce consistency regularization for image-to-LiDAR representation learning is not clear enough to demonstrate its superiority. A more detailed explanation and theoretical justification would strengthen this aspect of the work.\n\n(-) Compared to some of the most related works, for example, [R1] and [R3], the scale and depth regarding the experiments (for example, downstream fine-tuning on other datasets than SemanticKITTI) could be further enhanced.\n\n---\n\n### References:\n- [R1] Youquan Liu, et al. “Segment Any Point Cloud Sequences by Distilling Vision Foundation Models,” NeurIPS, 2023.\n- [R2] Ayça Takmaz, et al. “OpenMask3D: Open-Vocabulary 3D Instance Segmentation,” NeurIPS, 2023.\n- [R3] Gilles Puy, et al. “Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving,” arXiv, 2023.\n\nQuestions: - **Q1:** As mentioned in Weakness 1, the semantic masks generated by the Segment Anything Model could inevitably involve errors (e.g., wrong segmentation results). How do the authors handle the propagated errors during image-to-LiDAR representation learning?\n\n- **Q2:** As mentioned in Weakness 2, could the authors provide more details on the hyperparameter settings for the vMF distribution and the reasoning behind their chosen values? Adding a more detailed explanation and theoretical justification would be even better.\n\n- **Q3:** As mentioned in Weakness 3, having more thorough experimental analyses on other LiDAR-based point cloud datasets, such as SemanticPOSS, Waymo, SynLiDAR, etc., could further consolidate the findings and conclusions drawn in the manuscript.\n\n- **Q4:** As most 2D and 3D representation learning approaches (MoCo, SimCLR, Seal, etc.) do, having empirical analyses of models under out-of-distribution datasets is recommended. \n\n- **[Minor]:** The computational cost of the proposed multi-modal contrastive distillation approach is not thoroughly analyzed, which is crucial for real-time applications in autonomous driving.\n\n- **[Minor]:** The generalizability of OLIVINE to other types of sensors (for example, hybrid-solid LiDARs) or environments (for example, off-board environments) beyond the evaluated datasets is not discussed.\n\n- **[Minor]:** “NuScenes” should be revised to “nuScenes”.", "labeling_timestamp": "2026-01-11T16:21:51.641687", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The authors explicitly present Proposition 1 and provide a maximum likelihood derivation showing how class features can be modeled by a vMF distribution and how its parameters (mu, kappa) are estimated, thus addressing the theoretical basis.", "evidence": "\"Proposition 1: The features of each class $k$ can be modeled as a von Mises-Fisher (vMF) distribution. ... Justification: To show that the features of each class can be effectively modeled by a vMF distribution, we use maximum likelihood estimation (MLE) to determine that the parameters $\\mu_k$ and $\\kappa_k$ are optimal for the given set of feature vectors.\" \n\n\"By maximizing the likelihood function for the vMF distribution, we have shown that the parameters $\\mu_k$ and $\\kappa_k$ can be estimated to model the distribution of feature vectors for each class.\"", "combined_author_response": "Dear Reviewers,\n\nWe sincerely thank you for your thoughtful evaluations during the rebuttal stage. We are pleased that our detailed responses have addressed most of your concerns. We appreciate your recognition of the additional experiments and the clarification we have made to the paper.\n\nWe are grateful for the time you have invested in reviewing our work and for your consistent recognition of its value. Based on your valuable feedback, we will carefully revise the manuscript, incorporating the additional experiments and analyses into the final version.\n\nBest regards,\n\nThe Authors\n\n---\n\nDear Reviewer 54Gn,\n\nThank you for your response and for taking the time to carefully review our rebuttal. We greatly appreciate your recognition of our efforts to address your concerns and the value you found in the additional experiments we conducted. Your detailed and thoughtful review demonstrates a profound expertise in this domain. I have thoroughly enjoyed the opportunity to learn from your perspective.\n\nPlease feel free to share any further comments or suggestions.\n\nWarm regards,\n\nAuthors\n\n---\n\nDear Reviewer uKMo,\n\nThanks again for the time and energy you committed and your valuable comments. Your meticulous review and thoughtful critiques truly reflect your deep domain expertise and diligence as a reviewer. It has been a pleasure communicating and exchanging ideas with you.\n\nPlease feel free to share any additional comments or feedback on the manuscript.\n\nWarm regards,\n\nAuthors\n\n---\n\nDear Reviewer 2WGA,\n\nThank you for taking the time to review our rebuttal and for your constructive feedback throughout the process. We are glad that we could address most of your concerns. \n\nWe will actively participate in the Author-Reviewer discussion session. Please feel free to share any additional comments or feedback on the manuscript.\n\nWarm regards,\n\nAuthors\n\n---\n\nThank you for the positive feedback provided and the time devoted to this review. We are glad that our efforts have addressed your concerns. Next, we will address your remaining concerns.\n\n---\n\n**Comments**: *The authors may over-claim the contribution of using \"semantic superpixels\" over the \"class-agnostic superpixels\"*.\n\n**Response:**  We believe there may be a **misunderstanding** regarding our proposed methods. We would like to clarify the following points to address your concerns:\n\n- We have **not** claimed that using **semantic superpixels** is our contribution. In fact, our method does not rely on **superpixels** at all, which is different from previous methods [R1-R4].\n- Previous methods [R1-R4] use superpixels to pool 3D point features and 2D pixel features, learning with a superpixel-to-superpoint contrastive loss. In contrast, our method directly uses the features of **individual** points and pixels for contrastive distillation.\n- The semantic **labels** can be flexibly utilized in multiple aspects of the proposed method, such as weakly-supervised contrastive distillation, semantic-guided consistency regularization, and category-aware anchor point sampling. These aspects cannot be effectively addressed using only the class-agnostic superpixels.\n\n---\n**Comments**: *The use of semantic categories seems not to cause a major improvement over class-agnostic masks.*\n\n**Response:** Extensive experiments demonstrate that this approach **substantially** outperforms superpixels-based (class-agnostic mask-based) methods [R1-R4] in various downstream tasks.\n- Our method achieves a **significant** improvement over superpixels-based pretraining methods on nuScenes and SemanticKITTI datasets. As shown in the table below, our method outperforms Seal [R1] by a significant margin, achieving an improvement of 5.14\\% under the setting of linear probing. The full results are available in Table M1 of the uploaded PDF file.\n- Following your suggestions, we have added experiments on six additional LiDAR-based point cloud datasets and one out-of-distribution dataset. And our proposed OLIVINE **consistently** outperforms the superpixels-based methods on all datasets. For the full results, please refer to Tables M2 and M3 of the uploaded PDF file.\n\n| Method    | LP     | 1%     | 5%     | 10%    | 25%    | 100%   |\n| :-------- | :----- | :----- | :----- | :----- | :----- | :----- |\n| Random    | 8\\.10   | 30\\.30 | 47\\.84 | 56\\.15 | 65\\.48 | 74\\.66 |\n| PPKT      | 35\\.90  | 37\\.80 | 53\\.74 | 60\\.25 | 67\\.14 | 74\\.52 |\n| SLidR     | 38\\.80  | 38\\.30 | 52\\.49 | 59\\.84 | 66\\.91 | 74\\.79 |\n| ST-SLidR  | 40\\.48 | 40\\.75 | 54\\.69 | 60\\.75 | 67\\.70 | 75\\.14 |\n| HVDistill | 39\\.50  | 42\\.70 | 56\\.60 | 62\\.90 | 69\\.30 | 76\\.60 |\n| Seal      | 44\\.95 | 45\\.84 | 55\\.64 | 62\\.97 | 68\\.41 | 75\\.60 |\n| Ours      | **50\\.09** | **50\\.60** | **60\\.25** | **65\\.07** | **70\\.15** | **76\\.69** |\n---\nThanks again for your diligence as a reviewer. It is with great pleasure communicating with you. Please feel free to share any additional comments or feedback on the manuscript.\n\n**References**:\\\n[R1] Image-to-lidar self-supervised distillation for autonomous driving data.\\\n[R2] Self-supervised image-to-point distillation via semantically tolerant contrastive loss.\\\n[R3] Segment Any Point Cloud Sequences by Distilling Vision Foundation Models.\\\n[R4] HVDistill: Transferring Knowledge from Images to Point Clouds via Unsupervised Hybrid-View Distillation.\n\n---\n\n**Proposition 2**: The representation of samples in the same class can vary significantly across different batches during contrastive distillation, and semantic-guided consistency regularization helps to learn structured features.\n\n**Justification**: Without regularization, the representation of samples within the same class can vary significantly across different batches during contrastive distillation. This variance arises due to random sampling and the influence of negative samples in different batches. The weakly-supervised contrastive loss is defined as:\n\n$\\mathcal{L}_{\\mathrm{sup}} = - \\frac{1}{M_s} \\sum _{i=1}^{M_s} \\log \\left[ \\frac{1}{|A(i)|} \\sum _{a\\in A(i)} \\frac{\\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_a \\rangle/\\tau)}}{\\sum _{j=1}^{M_s} \\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_j \\rangle /\\tau)}}\\right]$\n\nThe features of negative samples $\\mathbf{G}^{\\mathrm{2D}}_j$ vary across batches, leading to different optimization paths for each mini-batch. This introduces variability in the learned representations $\\mathbf{G}^{\\mathrm{3D}}_i$ for samples of the same class $k$.\n\nWhen we do not use semantic-guided consistency regularization, the within-class variance for class $k$ across different batches is:\n\n$\\sigma_W^2 = \\frac{1}{|B|} \\sum_{B} \\frac{1}{M_k} \\sum_{i=1}^{M_k^B} \\|g_i^k - \\mu_k^B\\|^2$\n\nFor ease of reading, we use $g_i$ to refer to point feature $\\mathbf{G}^{\\mathrm{3D}}_i$.\nAnd $\\mu_k^B$ is the mean feature vector for class $k$ in batch $B$. Due to the batch-wise variability in negative samples, $\\mu_k^B$ can differ significantly across batches, leading to high within-class variance. \n\n\nBy minimizing the KL divergence, we align feature vectors $g_i$ of class $k$ with the mean direction $\\mu_k$, reducing the spread of feature vectors within the same class. The within-class variance with regularization is:\n\n$\\sigma_W^2 = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\|g_i^k - \\mu_k\\|^2$\n\nSince $\\mu_k$ is consistent across batches due to the regularization, the within-class variance is significantly reduced. This results in structured feature representations, enhancing class separability and improving performance in downstream tasks.\n\n---\n**Proposition 3**: Learning structural representation during pretraining can benefit downstream tasks.\n\n**Justification**: Structured features are those well-aligned within the same class (low within-class variance $\\sigma_W^2$) and well-separated between different classes (high between-class variance $\\sigma_B^2$). \n\nWith semantic-guided consistency regularization, feature vectors $g_i^k$ for class $k$ are closely aligned with the mean direction $\\mu_k$. This alignment reduces the within-class variance $\\sigma_W^2$. Weakly-supervised contrastive learning pushes apart feature vectors of different classes, increasing the separation between class means $\\mu_k$. This increases the between-class variance $\\sigma_B^2$.\n\nTake the linear classifier as an example, the decision boundary is determined by the separation between class means. Higher $\\sigma_B^2$ and lower $\\sigma_W^2$ result in clearer decision boundaries, reducing classification errors.\n\nConsider a simple linear classifier with weight vector $w$ and bias $b$. The decision function is:\n\n$f(x) = w^T x + b$\n\nThe decision boundary is given by:\n\n$w^T x + b = 0$\n\nFor well-structured features, the margin (distance between decision boundary and nearest samples) is maximized. The margin $ \\gamma $ for class $ k $ can be expressed as:\n\n$\\gamma = \\frac{w^T (\\mu_k - \\mu)}{\\|w\\|}$\n\nHigher between-class variance ($\\sigma_B^2$) and lower within-class variance ($\\sigma_W^2$) increase this margin, leading to better classification performance.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser.\n\n---\n\n**Proposition 1**: The features of each class $k$ can be modeled as a von Mises-Fisher (vMF) distribution. This means that for class $k$, the feature vectors $g_i$ lie on a unit hypersphere and are centered around a mean direction $\\mu_k$ with a concentration parameter $\\kappa_k$.\n\n**Justification**: To show that the features of each class can be effectively modeled by a vMF distribution, we use maximum likelihood estimation (MLE) to determine that the parameters $\\mu_k$ and $\\kappa_k$ are optimal for the given set of feature vectors.\n\nFor a set of $M_k$ feature vectors $\\\\{g_i\\\\}_{i=1}^{M_k}$ from class $k$, the likelihood function for the vMF distribution is:\n\n$L(\\mu_k, \\kappa_k) = \\prod_{i=1}^{M_k} f(g_i; \\mu_k, \\kappa_k) = \\prod_{i=1}^{M_k} \\mathcal{K}_{C}(\\kappa_k) \\exp(\\kappa_k \\mu_k^T g_i)$\n\nTaking the natural logarithm of the likelihood function, we get the log-likelihood:\n\n$\\log L(\\mu_k, \\kappa_k) = \\sum_{i=1}^{M_k} \\log f(g_i; \\mu_k, \\kappa_k) = M_k \\log \\mathcal{K}_{C}(\\kappa_k) + \\kappa_k \\sum _{i=1}^{M_k} \\mu_k^T g_i$\n\nSubstituting the expression for $\\mathcal{K}_{C}(\\kappa_k)$, we get:\n\n$\\log L(\\mu_k, \\kappa_k) = M_k \\left[ \\log \\left( \\frac{\\kappa_k^{C/2-1}}{(2\\pi)^{C/2} I_{C/2-1}(\\kappa_k)} \\right) + \\frac{\\kappa_k}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\n$\\log L(\\mu_k, \\kappa_k) = M_k \\left[ (C/2-1) \\log \\kappa_k - \\log I_{C/2-1}(\\kappa_k) - \\frac{C}{2} \\log(2\\pi) + \\frac{\\kappa_k}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\nTo maximize the log-likelihood, we normalize $\\mu_k$ by setting it to the normalized sum of the feature vectors:\n$\\mu_k = \\frac{\\sum_{i=1}^{M_k} g_i}{\\|\\sum_{i=1}^{M_k} g_i\\|}$\n\nThe derivative of the log-likelihood with respect to $\\kappa_k$ is:\n\n$\\frac{\\partial \\log L(\\mu_k, \\kappa_k)}{\\partial \\kappa_k} = M_k \\left[ \\frac{C/2-1}{\\kappa_k} - \\frac{I_{C/2}(\\kappa_k)}{I_{C/2-1}(\\kappa_k)} + \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\nSetting this derivative to zero, we get:\n\n$\\frac{C/2-1}{\\kappa_k} - \\frac{I_{C/2}(\\kappa_k)}{I_{C/2-1}(\\kappa_k)} + \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i = 0$\n\nSolving for $\\kappa_k$, we obtain:\n\n$\\kappa_k = \\frac{\\|\\sum_{i=1}^{M_k} g_i\\| (C - \\|\\sum_{i=1}^{M_k} g_i\\|^2)}{1 - \\|\\sum_{i=1}^{M_k} g_i\\|^2}$\n\nThis equation allows us to compute the concentration parameter $\\kappa_k$ based on the alignment of the feature vectors. The concentration parameter $\\kappa_k$ is larger when the distribution is more tightly clustered around the mean direction, and smaller when the features are more uniformly spread across the hypersphere.\n\nBy maximizing the likelihood function for the vMF distribution, we have shown that the parameters $\\mu_k$ and $\\kappa_k$ can be estimated to model the distribution of feature vectors for each class. \nThe mean direction $\\mu_k$ denotes the central direction of the feature cluster, and the concentration parameter $\\kappa_k$ controls the tightness of this clustering. Moreover, the way we estimate the parameters of vMF distribution in EMA is also consistent with the results of the above theoretical derivation.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser.", "author_response": "**Proposition 2**: The representation of samples in the same class can vary significantly across different batches during contrastive distillation, and semantic-guided consistency regularization helps to learn structured features.\n\n**Justification**: Without regularization, the representation of samples within the same class can vary significantly across different batches during contrastive distillation. This variance arises due to random sampling and the influence of negative samples in different batches. The weakly-supervised contrastive loss is defined as:\n\n$\\mathcal{L}_{\\mathrm{sup}} = - \\frac{1}{M_s} \\sum _{i=1}^{M_s} \\log \\left[ \\frac{1}{|A(i)|} \\sum _{a\\in A(i)} \\frac{\\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_a \\rangle/\\tau)}}{\\sum _{j=1}^{M_s} \\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_j \\rangle /\\tau)}}\\right]$\n\nThe features of negative samples $\\mathbf{G}^{\\mathrm{2D}}_j$ vary across batches, leading to different optimization paths for each mini-batch. This introduces variability in the learned representations $\\mathbf{G}^{\\mathrm{3D}}_i$ for samples of the same class $k$.\n\nWhen we do not use semantic-guided consistency regularization, the within-class variance for class $k$ across different batches is:\n\n$\\sigma_W^2 = \\frac{1}{|B|} \\sum_{B} \\frac{1}{M_k} \\sum_{i=1}^{M_k^B} \\|g_i^k - \\mu_k^B\\|^2$\n\nFor ease of reading, we use $g_i$ to refer to point feature $\\mathbf{G}^{\\mathrm{3D}}_i$.\nAnd $\\mu_k^B$ is the mean feature vector for class $k$ in batch $B$. Due to the batch-wise variability in negative samples, $\\mu_k^B$ can differ significantly across batches, leading to high within-class variance. \n\n\nBy minimizing the KL divergence, we align feature vectors $g_i$ of class $k$ with the mean direction $\\mu_k$, reducing the spread of feature vectors within the same class. The within-class variance with regularization is:\n\n$\\sigma_W^2 = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\|g_i^k - \\mu_k\\|^2$\n\nSince $\\mu_k$ is consistent across batches due to the regularization, the within-class variance is significantly reduced. This results in structured feature representations, enhancing class separability and improving performance in downstream tasks.\n\n---\n**Proposition 3**: Learning structural representation during pretraining can benefit downstream tasks.\n\n**Justification**: Structured features are those well-aligned within the same class (low within-class variance $\\sigma_W^2$) and well-separated between different classes (high between-class variance $\\sigma_B^2$). \n\nWith semantic-guided consistency regularization, feature vectors $g_i^k$ for class $k$ are closely aligned with the mean direction $\\mu_k$. This alignment reduces the within-class variance $\\sigma_W^2$. Weakly-supervised contrastive learning pushes apart feature vectors of different classes, increasing the separation between class means $\\mu_k$. This increases the between-class variance $\\sigma_B^2$.\n\nTake the linear classifier as an example, the decision boundary is determined by the separation between class means. Higher $\\sigma_B^2$ and lower $\\sigma_W^2$ result in clearer decision boundaries, reducing classification errors.\n\nConsider a simple linear classifier with weight vector $w$ and bias $b$. The decision function is:\n\n$f(x) = w^T x + b$\n\nThe decision boundary is given by:\n\n$w^T x + b = 0$\n\nFor well-structured features, the margin (distance between decision boundary and nearest samples) is maximized. The margin $ \\gamma $ for class $ k $ can be expressed as:\n\n$\\gamma = \\frac{w^T (\\mu_k - \\mu)}{\\|w\\|}$\n\nHigher between-class variance ($\\sigma_B^2$) and lower within-class variance ($\\sigma_W^2$) increase this margin, leading to better classification performance.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser."}
{"claim": "The paper lacks detailed explanations of hyperparameter settings and reasoning for chosen von Mises-Fisher distribution values.", "claim_type": "methodology", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "P8fCEoWdti", "reviewer": "Reviewer_svRz", "review_text": "Summary: This work aims to tackle the image-to-LiDAR contrastive learning problem for LiDAR-based point cloud segmentation. Previous approaches designed the cross-modal contrastive learning objective for model pretraining, using superpixels and superpoints as guidance.\n\nIn this work, the authors observe that the superpixel-driven contrastive loss tends to involve ‘’self-conflict’’ issues during representation learning. A weakly-supervised contrastive distillation method is proposed, which generates semantic superpixels/superpoints using the Segment Anything Model (SAM). Additionally, to balance the imbalanced class distributions of LiDAR scene categories during representation, a density and category-aware sampling strategy is proposed to adjust the sampling probabilities of different anchor points using the weak semantic labels.\n\nThe overall framework is named OLIVINE, which adopts three optimization objectives:\n- Weakly-supervised contrastive distillation using coarse semantic labels to identify positive pairs by category.\n- Self-supervised contrastive distillation applied to randomly sampled point-pixel pairs.\n- A regularization framework based on the von Mises-Fisher (vMF) distribution to ensure semantic consistency.\n\nThe proposed OLIVINE method is evaluated on the nuScenes, SemanticKITTI, and KITTI object detection datasets. The results exhibit a consistent improvement of the proposed method compared to existing approaches.\n\nStrengths: (+) This work aims to improve the image-to-LiDAR self-supervised representation learning problem on LiDAR-based point cloud datasets, which is one of the current research hotspots, especially for applications related to autonomous driving and robotics.\n\n(+) The proposed method has exhibited promising performance on mainstream benchmarks, including nuScenes linear probing, nuScenes fine-tuning, SemanticKITTI fine-tuning, and KITTI object detection.\n\nWeaknesses: (-) The weakly-supervised contrastive distillation method has been used in previous literature, such as [R1] and [R2]. Adding semantic categories seems not to cause a major improvement over class-agnostic masks, as the Segment Anything Model is able to segment rather complete and semantically consistent objects and backgrounds. Additionally, using weak labels (which might be erroneous) could introduce additional errors during pretraining.\n\n(-) The motivation for using the von Mises-Fisher (vMF) distribution to enforce consistency regularization for image-to-LiDAR representation learning is not clear enough to demonstrate its superiority. A more detailed explanation and theoretical justification would strengthen this aspect of the work.\n\n(-) Compared to some of the most related works, for example, [R1] and [R3], the scale and depth regarding the experiments (for example, downstream fine-tuning on other datasets than SemanticKITTI) could be further enhanced.\n\n---\n\n### References:\n- [R1] Youquan Liu, et al. “Segment Any Point Cloud Sequences by Distilling Vision Foundation Models,” NeurIPS, 2023.\n- [R2] Ayça Takmaz, et al. “OpenMask3D: Open-Vocabulary 3D Instance Segmentation,” NeurIPS, 2023.\n- [R3] Gilles Puy, et al. “Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving,” arXiv, 2023.\n\nQuestions: - **Q1:** As mentioned in Weakness 1, the semantic masks generated by the Segment Anything Model could inevitably involve errors (e.g., wrong segmentation results). How do the authors handle the propagated errors during image-to-LiDAR representation learning?\n\n- **Q2:** As mentioned in Weakness 2, could the authors provide more details on the hyperparameter settings for the vMF distribution and the reasoning behind their chosen values? Adding a more detailed explanation and theoretical justification would be even better.\n\n- **Q3:** As mentioned in Weakness 3, having more thorough experimental analyses on other LiDAR-based point cloud datasets, such as SemanticPOSS, Waymo, SynLiDAR, etc., could further consolidate the findings and conclusions drawn in the manuscript.\n\n- **Q4:** As most 2D and 3D representation learning approaches (MoCo, SimCLR, Seal, etc.) do, having empirical analyses of models under out-of-distribution datasets is recommended. \n\n- **[Minor]:** The computational cost of the proposed multi-modal contrastive distillation approach is not thoroughly analyzed, which is crucial for real-time applications in autonomous driving.\n\n- **[Minor]:** The generalizability of OLIVINE to other types of sensors (for example, hybrid-solid LiDARs) or environments (for example, off-board environments) beyond the evaluated datasets is not discussed.\n\n- **[Minor]:** “NuScenes” should be revised to “nuScenes”.", "labeling_timestamp": "2026-01-11T16:21:58.114136", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors provide theoretical justification and an explicit MLE-based formula for the vMF concentration parameter and state their EMA estimation is consistent with this derivation, but they do not supply concrete hyperparameter settings or explicit chosen numerical vMF/hyperparameter values in the response.", "evidence": "\"Proposition 1: The features of each class k can be modeled as a von Mises-Fisher (vMF) distribution... Solving for \\kappa_k, we obtain: \\kappa_k = \\frac{\\|\\sum_{i=1}^{M_k} g_i\\| (C - \\|\\sum_{i=1}^{M_k} g_i\\|^2)}{1 - \\|\\sum_{i=1}^{M_k} g_i\\|^2}\" and \"Moreover, the way we estimate the parameters of vMF distribution in EMA is also consistent with the results of the above theoretical derivation.\"", "combined_author_response": "Dear Reviewers,\n\nWe sincerely thank you for your thoughtful evaluations during the rebuttal stage. We are pleased that our detailed responses have addressed most of your concerns. We appreciate your recognition of the additional experiments and the clarification we have made to the paper.\n\nWe are grateful for the time you have invested in reviewing our work and for your consistent recognition of its value. Based on your valuable feedback, we will carefully revise the manuscript, incorporating the additional experiments and analyses into the final version.\n\nBest regards,\n\nThe Authors\n\n---\n\nDear Reviewer 54Gn,\n\nThank you for your response and for taking the time to carefully review our rebuttal. We greatly appreciate your recognition of our efforts to address your concerns and the value you found in the additional experiments we conducted. Your detailed and thoughtful review demonstrates a profound expertise in this domain. I have thoroughly enjoyed the opportunity to learn from your perspective.\n\nPlease feel free to share any further comments or suggestions.\n\nWarm regards,\n\nAuthors\n\n---\n\nDear Reviewer uKMo,\n\nThanks again for the time and energy you committed and your valuable comments. Your meticulous review and thoughtful critiques truly reflect your deep domain expertise and diligence as a reviewer. It has been a pleasure communicating and exchanging ideas with you.\n\nPlease feel free to share any additional comments or feedback on the manuscript.\n\nWarm regards,\n\nAuthors\n\n---\n\nDear Reviewer 2WGA,\n\nThank you for taking the time to review our rebuttal and for your constructive feedback throughout the process. We are glad that we could address most of your concerns. \n\nWe will actively participate in the Author-Reviewer discussion session. Please feel free to share any additional comments or feedback on the manuscript.\n\nWarm regards,\n\nAuthors\n\n---\n\nThank you for the positive feedback provided and the time devoted to this review. We are glad that our efforts have addressed your concerns. Next, we will address your remaining concerns.\n\n---\n\n**Comments**: *The authors may over-claim the contribution of using \"semantic superpixels\" over the \"class-agnostic superpixels\"*.\n\n**Response:**  We believe there may be a **misunderstanding** regarding our proposed methods. We would like to clarify the following points to address your concerns:\n\n- We have **not** claimed that using **semantic superpixels** is our contribution. In fact, our method does not rely on **superpixels** at all, which is different from previous methods [R1-R4].\n- Previous methods [R1-R4] use superpixels to pool 3D point features and 2D pixel features, learning with a superpixel-to-superpoint contrastive loss. In contrast, our method directly uses the features of **individual** points and pixels for contrastive distillation.\n- The semantic **labels** can be flexibly utilized in multiple aspects of the proposed method, such as weakly-supervised contrastive distillation, semantic-guided consistency regularization, and category-aware anchor point sampling. These aspects cannot be effectively addressed using only the class-agnostic superpixels.\n\n---\n**Comments**: *The use of semantic categories seems not to cause a major improvement over class-agnostic masks.*\n\n**Response:** Extensive experiments demonstrate that this approach **substantially** outperforms superpixels-based (class-agnostic mask-based) methods [R1-R4] in various downstream tasks.\n- Our method achieves a **significant** improvement over superpixels-based pretraining methods on nuScenes and SemanticKITTI datasets. As shown in the table below, our method outperforms Seal [R1] by a significant margin, achieving an improvement of 5.14\\% under the setting of linear probing. The full results are available in Table M1 of the uploaded PDF file.\n- Following your suggestions, we have added experiments on six additional LiDAR-based point cloud datasets and one out-of-distribution dataset. And our proposed OLIVINE **consistently** outperforms the superpixels-based methods on all datasets. For the full results, please refer to Tables M2 and M3 of the uploaded PDF file.\n\n| Method    | LP     | 1%     | 5%     | 10%    | 25%    | 100%   |\n| :-------- | :----- | :----- | :----- | :----- | :----- | :----- |\n| Random    | 8\\.10   | 30\\.30 | 47\\.84 | 56\\.15 | 65\\.48 | 74\\.66 |\n| PPKT      | 35\\.90  | 37\\.80 | 53\\.74 | 60\\.25 | 67\\.14 | 74\\.52 |\n| SLidR     | 38\\.80  | 38\\.30 | 52\\.49 | 59\\.84 | 66\\.91 | 74\\.79 |\n| ST-SLidR  | 40\\.48 | 40\\.75 | 54\\.69 | 60\\.75 | 67\\.70 | 75\\.14 |\n| HVDistill | 39\\.50  | 42\\.70 | 56\\.60 | 62\\.90 | 69\\.30 | 76\\.60 |\n| Seal      | 44\\.95 | 45\\.84 | 55\\.64 | 62\\.97 | 68\\.41 | 75\\.60 |\n| Ours      | **50\\.09** | **50\\.60** | **60\\.25** | **65\\.07** | **70\\.15** | **76\\.69** |\n---\nThanks again for your diligence as a reviewer. It is with great pleasure communicating with you. Please feel free to share any additional comments or feedback on the manuscript.\n\n**References**:\\\n[R1] Image-to-lidar self-supervised distillation for autonomous driving data.\\\n[R2] Self-supervised image-to-point distillation via semantically tolerant contrastive loss.\\\n[R3] Segment Any Point Cloud Sequences by Distilling Vision Foundation Models.\\\n[R4] HVDistill: Transferring Knowledge from Images to Point Clouds via Unsupervised Hybrid-View Distillation.\n\n---\n\n**Proposition 2**: The representation of samples in the same class can vary significantly across different batches during contrastive distillation, and semantic-guided consistency regularization helps to learn structured features.\n\n**Justification**: Without regularization, the representation of samples within the same class can vary significantly across different batches during contrastive distillation. This variance arises due to random sampling and the influence of negative samples in different batches. The weakly-supervised contrastive loss is defined as:\n\n$\\mathcal{L}_{\\mathrm{sup}} = - \\frac{1}{M_s} \\sum _{i=1}^{M_s} \\log \\left[ \\frac{1}{|A(i)|} \\sum _{a\\in A(i)} \\frac{\\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_a \\rangle/\\tau)}}{\\sum _{j=1}^{M_s} \\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_j \\rangle /\\tau)}}\\right]$\n\nThe features of negative samples $\\mathbf{G}^{\\mathrm{2D}}_j$ vary across batches, leading to different optimization paths for each mini-batch. This introduces variability in the learned representations $\\mathbf{G}^{\\mathrm{3D}}_i$ for samples of the same class $k$.\n\nWhen we do not use semantic-guided consistency regularization, the within-class variance for class $k$ across different batches is:\n\n$\\sigma_W^2 = \\frac{1}{|B|} \\sum_{B} \\frac{1}{M_k} \\sum_{i=1}^{M_k^B} \\|g_i^k - \\mu_k^B\\|^2$\n\nFor ease of reading, we use $g_i$ to refer to point feature $\\mathbf{G}^{\\mathrm{3D}}_i$.\nAnd $\\mu_k^B$ is the mean feature vector for class $k$ in batch $B$. Due to the batch-wise variability in negative samples, $\\mu_k^B$ can differ significantly across batches, leading to high within-class variance. \n\n\nBy minimizing the KL divergence, we align feature vectors $g_i$ of class $k$ with the mean direction $\\mu_k$, reducing the spread of feature vectors within the same class. The within-class variance with regularization is:\n\n$\\sigma_W^2 = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\|g_i^k - \\mu_k\\|^2$\n\nSince $\\mu_k$ is consistent across batches due to the regularization, the within-class variance is significantly reduced. This results in structured feature representations, enhancing class separability and improving performance in downstream tasks.\n\n---\n**Proposition 3**: Learning structural representation during pretraining can benefit downstream tasks.\n\n**Justification**: Structured features are those well-aligned within the same class (low within-class variance $\\sigma_W^2$) and well-separated between different classes (high between-class variance $\\sigma_B^2$). \n\nWith semantic-guided consistency regularization, feature vectors $g_i^k$ for class $k$ are closely aligned with the mean direction $\\mu_k$. This alignment reduces the within-class variance $\\sigma_W^2$. Weakly-supervised contrastive learning pushes apart feature vectors of different classes, increasing the separation between class means $\\mu_k$. This increases the between-class variance $\\sigma_B^2$.\n\nTake the linear classifier as an example, the decision boundary is determined by the separation between class means. Higher $\\sigma_B^2$ and lower $\\sigma_W^2$ result in clearer decision boundaries, reducing classification errors.\n\nConsider a simple linear classifier with weight vector $w$ and bias $b$. The decision function is:\n\n$f(x) = w^T x + b$\n\nThe decision boundary is given by:\n\n$w^T x + b = 0$\n\nFor well-structured features, the margin (distance between decision boundary and nearest samples) is maximized. The margin $ \\gamma $ for class $ k $ can be expressed as:\n\n$\\gamma = \\frac{w^T (\\mu_k - \\mu)}{\\|w\\|}$\n\nHigher between-class variance ($\\sigma_B^2$) and lower within-class variance ($\\sigma_W^2$) increase this margin, leading to better classification performance.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser.\n\n---\n\n**Proposition 1**: The features of each class $k$ can be modeled as a von Mises-Fisher (vMF) distribution. This means that for class $k$, the feature vectors $g_i$ lie on a unit hypersphere and are centered around a mean direction $\\mu_k$ with a concentration parameter $\\kappa_k$.\n\n**Justification**: To show that the features of each class can be effectively modeled by a vMF distribution, we use maximum likelihood estimation (MLE) to determine that the parameters $\\mu_k$ and $\\kappa_k$ are optimal for the given set of feature vectors.\n\nFor a set of $M_k$ feature vectors $\\\\{g_i\\\\}_{i=1}^{M_k}$ from class $k$, the likelihood function for the vMF distribution is:\n\n$L(\\mu_k, \\kappa_k) = \\prod_{i=1}^{M_k} f(g_i; \\mu_k, \\kappa_k) = \\prod_{i=1}^{M_k} \\mathcal{K}_{C}(\\kappa_k) \\exp(\\kappa_k \\mu_k^T g_i)$\n\nTaking the natural logarithm of the likelihood function, we get the log-likelihood:\n\n$\\log L(\\mu_k, \\kappa_k) = \\sum_{i=1}^{M_k} \\log f(g_i; \\mu_k, \\kappa_k) = M_k \\log \\mathcal{K}_{C}(\\kappa_k) + \\kappa_k \\sum _{i=1}^{M_k} \\mu_k^T g_i$\n\nSubstituting the expression for $\\mathcal{K}_{C}(\\kappa_k)$, we get:\n\n$\\log L(\\mu_k, \\kappa_k) = M_k \\left[ \\log \\left( \\frac{\\kappa_k^{C/2-1}}{(2\\pi)^{C/2} I_{C/2-1}(\\kappa_k)} \\right) + \\frac{\\kappa_k}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\n$\\log L(\\mu_k, \\kappa_k) = M_k \\left[ (C/2-1) \\log \\kappa_k - \\log I_{C/2-1}(\\kappa_k) - \\frac{C}{2} \\log(2\\pi) + \\frac{\\kappa_k}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\nTo maximize the log-likelihood, we normalize $\\mu_k$ by setting it to the normalized sum of the feature vectors:\n$\\mu_k = \\frac{\\sum_{i=1}^{M_k} g_i}{\\|\\sum_{i=1}^{M_k} g_i\\|}$\n\nThe derivative of the log-likelihood with respect to $\\kappa_k$ is:\n\n$\\frac{\\partial \\log L(\\mu_k, \\kappa_k)}{\\partial \\kappa_k} = M_k \\left[ \\frac{C/2-1}{\\kappa_k} - \\frac{I_{C/2}(\\kappa_k)}{I_{C/2-1}(\\kappa_k)} + \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\nSetting this derivative to zero, we get:\n\n$\\frac{C/2-1}{\\kappa_k} - \\frac{I_{C/2}(\\kappa_k)}{I_{C/2-1}(\\kappa_k)} + \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i = 0$\n\nSolving for $\\kappa_k$, we obtain:\n\n$\\kappa_k = \\frac{\\|\\sum_{i=1}^{M_k} g_i\\| (C - \\|\\sum_{i=1}^{M_k} g_i\\|^2)}{1 - \\|\\sum_{i=1}^{M_k} g_i\\|^2}$\n\nThis equation allows us to compute the concentration parameter $\\kappa_k$ based on the alignment of the feature vectors. The concentration parameter $\\kappa_k$ is larger when the distribution is more tightly clustered around the mean direction, and smaller when the features are more uniformly spread across the hypersphere.\n\nBy maximizing the likelihood function for the vMF distribution, we have shown that the parameters $\\mu_k$ and $\\kappa_k$ can be estimated to model the distribution of feature vectors for each class. \nThe mean direction $\\mu_k$ denotes the central direction of the feature cluster, and the concentration parameter $\\kappa_k$ controls the tightness of this clustering. Moreover, the way we estimate the parameters of vMF distribution in EMA is also consistent with the results of the above theoretical derivation.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser.", "author_response": "**Proposition 2**: The representation of samples in the same class can vary significantly across different batches during contrastive distillation, and semantic-guided consistency regularization helps to learn structured features.\n\n**Justification**: Without regularization, the representation of samples within the same class can vary significantly across different batches during contrastive distillation. This variance arises due to random sampling and the influence of negative samples in different batches. The weakly-supervised contrastive loss is defined as:\n\n$\\mathcal{L}_{\\mathrm{sup}} = - \\frac{1}{M_s} \\sum _{i=1}^{M_s} \\log \\left[ \\frac{1}{|A(i)|} \\sum _{a\\in A(i)} \\frac{\\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_a \\rangle/\\tau)}}{\\sum _{j=1}^{M_s} \\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_j \\rangle /\\tau)}}\\right]$\n\nThe features of negative samples $\\mathbf{G}^{\\mathrm{2D}}_j$ vary across batches, leading to different optimization paths for each mini-batch. This introduces variability in the learned representations $\\mathbf{G}^{\\mathrm{3D}}_i$ for samples of the same class $k$.\n\nWhen we do not use semantic-guided consistency regularization, the within-class variance for class $k$ across different batches is:\n\n$\\sigma_W^2 = \\frac{1}{|B|} \\sum_{B} \\frac{1}{M_k} \\sum_{i=1}^{M_k^B} \\|g_i^k - \\mu_k^B\\|^2$\n\nFor ease of reading, we use $g_i$ to refer to point feature $\\mathbf{G}^{\\mathrm{3D}}_i$.\nAnd $\\mu_k^B$ is the mean feature vector for class $k$ in batch $B$. Due to the batch-wise variability in negative samples, $\\mu_k^B$ can differ significantly across batches, leading to high within-class variance. \n\n\nBy minimizing the KL divergence, we align feature vectors $g_i$ of class $k$ with the mean direction $\\mu_k$, reducing the spread of feature vectors within the same class. The within-class variance with regularization is:\n\n$\\sigma_W^2 = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\|g_i^k - \\mu_k\\|^2$\n\nSince $\\mu_k$ is consistent across batches due to the regularization, the within-class variance is significantly reduced. This results in structured feature representations, enhancing class separability and improving performance in downstream tasks.\n\n---\n**Proposition 3**: Learning structural representation during pretraining can benefit downstream tasks.\n\n**Justification**: Structured features are those well-aligned within the same class (low within-class variance $\\sigma_W^2$) and well-separated between different classes (high between-class variance $\\sigma_B^2$). \n\nWith semantic-guided consistency regularization, feature vectors $g_i^k$ for class $k$ are closely aligned with the mean direction $\\mu_k$. This alignment reduces the within-class variance $\\sigma_W^2$. Weakly-supervised contrastive learning pushes apart feature vectors of different classes, increasing the separation between class means $\\mu_k$. This increases the between-class variance $\\sigma_B^2$.\n\nTake the linear classifier as an example, the decision boundary is determined by the separation between class means. Higher $\\sigma_B^2$ and lower $\\sigma_W^2$ result in clearer decision boundaries, reducing classification errors.\n\nConsider a simple linear classifier with weight vector $w$ and bias $b$. The decision function is:\n\n$f(x) = w^T x + b$\n\nThe decision boundary is given by:\n\n$w^T x + b = 0$\n\nFor well-structured features, the margin (distance between decision boundary and nearest samples) is maximized. The margin $ \\gamma $ for class $ k $ can be expressed as:\n\n$\\gamma = \\frac{w^T (\\mu_k - \\mu)}{\\|w\\|}$\n\nHigher between-class variance ($\\sigma_B^2$) and lower within-class variance ($\\sigma_W^2$) increase this margin, leading to better classification performance.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser."}
{"claim": "The experiments lack scale and depth compared to related works and omit downstream fine-tuning on datasets beyond SemanticKITTI.", "claim_type": "experimental", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "P8fCEoWdti", "reviewer": "Reviewer_svRz", "review_text": "Summary: This work aims to tackle the image-to-LiDAR contrastive learning problem for LiDAR-based point cloud segmentation. Previous approaches designed the cross-modal contrastive learning objective for model pretraining, using superpixels and superpoints as guidance.\n\nIn this work, the authors observe that the superpixel-driven contrastive loss tends to involve ‘’self-conflict’’ issues during representation learning. A weakly-supervised contrastive distillation method is proposed, which generates semantic superpixels/superpoints using the Segment Anything Model (SAM). Additionally, to balance the imbalanced class distributions of LiDAR scene categories during representation, a density and category-aware sampling strategy is proposed to adjust the sampling probabilities of different anchor points using the weak semantic labels.\n\nThe overall framework is named OLIVINE, which adopts three optimization objectives:\n- Weakly-supervised contrastive distillation using coarse semantic labels to identify positive pairs by category.\n- Self-supervised contrastive distillation applied to randomly sampled point-pixel pairs.\n- A regularization framework based on the von Mises-Fisher (vMF) distribution to ensure semantic consistency.\n\nThe proposed OLIVINE method is evaluated on the nuScenes, SemanticKITTI, and KITTI object detection datasets. The results exhibit a consistent improvement of the proposed method compared to existing approaches.\n\nStrengths: (+) This work aims to improve the image-to-LiDAR self-supervised representation learning problem on LiDAR-based point cloud datasets, which is one of the current research hotspots, especially for applications related to autonomous driving and robotics.\n\n(+) The proposed method has exhibited promising performance on mainstream benchmarks, including nuScenes linear probing, nuScenes fine-tuning, SemanticKITTI fine-tuning, and KITTI object detection.\n\nWeaknesses: (-) The weakly-supervised contrastive distillation method has been used in previous literature, such as [R1] and [R2]. Adding semantic categories seems not to cause a major improvement over class-agnostic masks, as the Segment Anything Model is able to segment rather complete and semantically consistent objects and backgrounds. Additionally, using weak labels (which might be erroneous) could introduce additional errors during pretraining.\n\n(-) The motivation for using the von Mises-Fisher (vMF) distribution to enforce consistency regularization for image-to-LiDAR representation learning is not clear enough to demonstrate its superiority. A more detailed explanation and theoretical justification would strengthen this aspect of the work.\n\n(-) Compared to some of the most related works, for example, [R1] and [R3], the scale and depth regarding the experiments (for example, downstream fine-tuning on other datasets than SemanticKITTI) could be further enhanced.\n\n---\n\n### References:\n- [R1] Youquan Liu, et al. “Segment Any Point Cloud Sequences by Distilling Vision Foundation Models,” NeurIPS, 2023.\n- [R2] Ayça Takmaz, et al. “OpenMask3D: Open-Vocabulary 3D Instance Segmentation,” NeurIPS, 2023.\n- [R3] Gilles Puy, et al. “Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving,” arXiv, 2023.\n\nQuestions: - **Q1:** As mentioned in Weakness 1, the semantic masks generated by the Segment Anything Model could inevitably involve errors (e.g., wrong segmentation results). How do the authors handle the propagated errors during image-to-LiDAR representation learning?\n\n- **Q2:** As mentioned in Weakness 2, could the authors provide more details on the hyperparameter settings for the vMF distribution and the reasoning behind their chosen values? Adding a more detailed explanation and theoretical justification would be even better.\n\n- **Q3:** As mentioned in Weakness 3, having more thorough experimental analyses on other LiDAR-based point cloud datasets, such as SemanticPOSS, Waymo, SynLiDAR, etc., could further consolidate the findings and conclusions drawn in the manuscript.\n\n- **Q4:** As most 2D and 3D representation learning approaches (MoCo, SimCLR, Seal, etc.) do, having empirical analyses of models under out-of-distribution datasets is recommended. \n\n- **[Minor]:** The computational cost of the proposed multi-modal contrastive distillation approach is not thoroughly analyzed, which is crucial for real-time applications in autonomous driving.\n\n- **[Minor]:** The generalizability of OLIVINE to other types of sensors (for example, hybrid-solid LiDARs) or environments (for example, off-board environments) beyond the evaluated datasets is not discussed.\n\n- **[Minor]:** “NuScenes” should be revised to “nuScenes”.", "labeling_timestamp": "2026-01-11T16:22:04.366165", "model": "gpt-5-mini", "label": "Contradicted", "justification": "Authors state they added extensive additional experiments across multiple LiDAR datasets (including nuScenes and six more datasets plus an OOD dataset), report consistent improvements and provide results tables, directly addressing the concern about scale and downstream evaluations beyond SemanticKITTI.", "evidence": "\"Following your suggestions, we have added experiments on six additional LiDAR-based point cloud datasets and one out-of-distribution dataset. And our proposed OLIVINE consistently outperforms the superpixels-based methods on all datasets.\"", "combined_author_response": "Dear Reviewers,\n\nWe sincerely thank you for your thoughtful evaluations during the rebuttal stage. We are pleased that our detailed responses have addressed most of your concerns. We appreciate your recognition of the additional experiments and the clarification we have made to the paper.\n\nWe are grateful for the time you have invested in reviewing our work and for your consistent recognition of its value. Based on your valuable feedback, we will carefully revise the manuscript, incorporating the additional experiments and analyses into the final version.\n\nBest regards,\n\nThe Authors\n\n---\n\nDear Reviewer 54Gn,\n\nThank you for your response and for taking the time to carefully review our rebuttal. We greatly appreciate your recognition of our efforts to address your concerns and the value you found in the additional experiments we conducted. Your detailed and thoughtful review demonstrates a profound expertise in this domain. I have thoroughly enjoyed the opportunity to learn from your perspective.\n\nPlease feel free to share any further comments or suggestions.\n\nWarm regards,\n\nAuthors\n\n---\n\nDear Reviewer uKMo,\n\nThanks again for the time and energy you committed and your valuable comments. Your meticulous review and thoughtful critiques truly reflect your deep domain expertise and diligence as a reviewer. It has been a pleasure communicating and exchanging ideas with you.\n\nPlease feel free to share any additional comments or feedback on the manuscript.\n\nWarm regards,\n\nAuthors\n\n---\n\nDear Reviewer 2WGA,\n\nThank you for taking the time to review our rebuttal and for your constructive feedback throughout the process. We are glad that we could address most of your concerns. \n\nWe will actively participate in the Author-Reviewer discussion session. Please feel free to share any additional comments or feedback on the manuscript.\n\nWarm regards,\n\nAuthors\n\n---\n\nThank you for the positive feedback provided and the time devoted to this review. We are glad that our efforts have addressed your concerns. Next, we will address your remaining concerns.\n\n---\n\n**Comments**: *The authors may over-claim the contribution of using \"semantic superpixels\" over the \"class-agnostic superpixels\"*.\n\n**Response:**  We believe there may be a **misunderstanding** regarding our proposed methods. We would like to clarify the following points to address your concerns:\n\n- We have **not** claimed that using **semantic superpixels** is our contribution. In fact, our method does not rely on **superpixels** at all, which is different from previous methods [R1-R4].\n- Previous methods [R1-R4] use superpixels to pool 3D point features and 2D pixel features, learning with a superpixel-to-superpoint contrastive loss. In contrast, our method directly uses the features of **individual** points and pixels for contrastive distillation.\n- The semantic **labels** can be flexibly utilized in multiple aspects of the proposed method, such as weakly-supervised contrastive distillation, semantic-guided consistency regularization, and category-aware anchor point sampling. These aspects cannot be effectively addressed using only the class-agnostic superpixels.\n\n---\n**Comments**: *The use of semantic categories seems not to cause a major improvement over class-agnostic masks.*\n\n**Response:** Extensive experiments demonstrate that this approach **substantially** outperforms superpixels-based (class-agnostic mask-based) methods [R1-R4] in various downstream tasks.\n- Our method achieves a **significant** improvement over superpixels-based pretraining methods on nuScenes and SemanticKITTI datasets. As shown in the table below, our method outperforms Seal [R1] by a significant margin, achieving an improvement of 5.14\\% under the setting of linear probing. The full results are available in Table M1 of the uploaded PDF file.\n- Following your suggestions, we have added experiments on six additional LiDAR-based point cloud datasets and one out-of-distribution dataset. And our proposed OLIVINE **consistently** outperforms the superpixels-based methods on all datasets. For the full results, please refer to Tables M2 and M3 of the uploaded PDF file.\n\n| Method    | LP     | 1%     | 5%     | 10%    | 25%    | 100%   |\n| :-------- | :----- | :----- | :----- | :----- | :----- | :----- |\n| Random    | 8\\.10   | 30\\.30 | 47\\.84 | 56\\.15 | 65\\.48 | 74\\.66 |\n| PPKT      | 35\\.90  | 37\\.80 | 53\\.74 | 60\\.25 | 67\\.14 | 74\\.52 |\n| SLidR     | 38\\.80  | 38\\.30 | 52\\.49 | 59\\.84 | 66\\.91 | 74\\.79 |\n| ST-SLidR  | 40\\.48 | 40\\.75 | 54\\.69 | 60\\.75 | 67\\.70 | 75\\.14 |\n| HVDistill | 39\\.50  | 42\\.70 | 56\\.60 | 62\\.90 | 69\\.30 | 76\\.60 |\n| Seal      | 44\\.95 | 45\\.84 | 55\\.64 | 62\\.97 | 68\\.41 | 75\\.60 |\n| Ours      | **50\\.09** | **50\\.60** | **60\\.25** | **65\\.07** | **70\\.15** | **76\\.69** |\n---\nThanks again for your diligence as a reviewer. It is with great pleasure communicating with you. Please feel free to share any additional comments or feedback on the manuscript.\n\n**References**:\\\n[R1] Image-to-lidar self-supervised distillation for autonomous driving data.\\\n[R2] Self-supervised image-to-point distillation via semantically tolerant contrastive loss.\\\n[R3] Segment Any Point Cloud Sequences by Distilling Vision Foundation Models.\\\n[R4] HVDistill: Transferring Knowledge from Images to Point Clouds via Unsupervised Hybrid-View Distillation.\n\n---\n\n**Proposition 2**: The representation of samples in the same class can vary significantly across different batches during contrastive distillation, and semantic-guided consistency regularization helps to learn structured features.\n\n**Justification**: Without regularization, the representation of samples within the same class can vary significantly across different batches during contrastive distillation. This variance arises due to random sampling and the influence of negative samples in different batches. The weakly-supervised contrastive loss is defined as:\n\n$\\mathcal{L}_{\\mathrm{sup}} = - \\frac{1}{M_s} \\sum _{i=1}^{M_s} \\log \\left[ \\frac{1}{|A(i)|} \\sum _{a\\in A(i)} \\frac{\\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_a \\rangle/\\tau)}}{\\sum _{j=1}^{M_s} \\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_j \\rangle /\\tau)}}\\right]$\n\nThe features of negative samples $\\mathbf{G}^{\\mathrm{2D}}_j$ vary across batches, leading to different optimization paths for each mini-batch. This introduces variability in the learned representations $\\mathbf{G}^{\\mathrm{3D}}_i$ for samples of the same class $k$.\n\nWhen we do not use semantic-guided consistency regularization, the within-class variance for class $k$ across different batches is:\n\n$\\sigma_W^2 = \\frac{1}{|B|} \\sum_{B} \\frac{1}{M_k} \\sum_{i=1}^{M_k^B} \\|g_i^k - \\mu_k^B\\|^2$\n\nFor ease of reading, we use $g_i$ to refer to point feature $\\mathbf{G}^{\\mathrm{3D}}_i$.\nAnd $\\mu_k^B$ is the mean feature vector for class $k$ in batch $B$. Due to the batch-wise variability in negative samples, $\\mu_k^B$ can differ significantly across batches, leading to high within-class variance. \n\n\nBy minimizing the KL divergence, we align feature vectors $g_i$ of class $k$ with the mean direction $\\mu_k$, reducing the spread of feature vectors within the same class. The within-class variance with regularization is:\n\n$\\sigma_W^2 = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\|g_i^k - \\mu_k\\|^2$\n\nSince $\\mu_k$ is consistent across batches due to the regularization, the within-class variance is significantly reduced. This results in structured feature representations, enhancing class separability and improving performance in downstream tasks.\n\n---\n**Proposition 3**: Learning structural representation during pretraining can benefit downstream tasks.\n\n**Justification**: Structured features are those well-aligned within the same class (low within-class variance $\\sigma_W^2$) and well-separated between different classes (high between-class variance $\\sigma_B^2$). \n\nWith semantic-guided consistency regularization, feature vectors $g_i^k$ for class $k$ are closely aligned with the mean direction $\\mu_k$. This alignment reduces the within-class variance $\\sigma_W^2$. Weakly-supervised contrastive learning pushes apart feature vectors of different classes, increasing the separation between class means $\\mu_k$. This increases the between-class variance $\\sigma_B^2$.\n\nTake the linear classifier as an example, the decision boundary is determined by the separation between class means. Higher $\\sigma_B^2$ and lower $\\sigma_W^2$ result in clearer decision boundaries, reducing classification errors.\n\nConsider a simple linear classifier with weight vector $w$ and bias $b$. The decision function is:\n\n$f(x) = w^T x + b$\n\nThe decision boundary is given by:\n\n$w^T x + b = 0$\n\nFor well-structured features, the margin (distance between decision boundary and nearest samples) is maximized. The margin $ \\gamma $ for class $ k $ can be expressed as:\n\n$\\gamma = \\frac{w^T (\\mu_k - \\mu)}{\\|w\\|}$\n\nHigher between-class variance ($\\sigma_B^2$) and lower within-class variance ($\\sigma_W^2$) increase this margin, leading to better classification performance.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser.\n\n---\n\n**Proposition 1**: The features of each class $k$ can be modeled as a von Mises-Fisher (vMF) distribution. This means that for class $k$, the feature vectors $g_i$ lie on a unit hypersphere and are centered around a mean direction $\\mu_k$ with a concentration parameter $\\kappa_k$.\n\n**Justification**: To show that the features of each class can be effectively modeled by a vMF distribution, we use maximum likelihood estimation (MLE) to determine that the parameters $\\mu_k$ and $\\kappa_k$ are optimal for the given set of feature vectors.\n\nFor a set of $M_k$ feature vectors $\\\\{g_i\\\\}_{i=1}^{M_k}$ from class $k$, the likelihood function for the vMF distribution is:\n\n$L(\\mu_k, \\kappa_k) = \\prod_{i=1}^{M_k} f(g_i; \\mu_k, \\kappa_k) = \\prod_{i=1}^{M_k} \\mathcal{K}_{C}(\\kappa_k) \\exp(\\kappa_k \\mu_k^T g_i)$\n\nTaking the natural logarithm of the likelihood function, we get the log-likelihood:\n\n$\\log L(\\mu_k, \\kappa_k) = \\sum_{i=1}^{M_k} \\log f(g_i; \\mu_k, \\kappa_k) = M_k \\log \\mathcal{K}_{C}(\\kappa_k) + \\kappa_k \\sum _{i=1}^{M_k} \\mu_k^T g_i$\n\nSubstituting the expression for $\\mathcal{K}_{C}(\\kappa_k)$, we get:\n\n$\\log L(\\mu_k, \\kappa_k) = M_k \\left[ \\log \\left( \\frac{\\kappa_k^{C/2-1}}{(2\\pi)^{C/2} I_{C/2-1}(\\kappa_k)} \\right) + \\frac{\\kappa_k}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\n$\\log L(\\mu_k, \\kappa_k) = M_k \\left[ (C/2-1) \\log \\kappa_k - \\log I_{C/2-1}(\\kappa_k) - \\frac{C}{2} \\log(2\\pi) + \\frac{\\kappa_k}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\nTo maximize the log-likelihood, we normalize $\\mu_k$ by setting it to the normalized sum of the feature vectors:\n$\\mu_k = \\frac{\\sum_{i=1}^{M_k} g_i}{\\|\\sum_{i=1}^{M_k} g_i\\|}$\n\nThe derivative of the log-likelihood with respect to $\\kappa_k$ is:\n\n$\\frac{\\partial \\log L(\\mu_k, \\kappa_k)}{\\partial \\kappa_k} = M_k \\left[ \\frac{C/2-1}{\\kappa_k} - \\frac{I_{C/2}(\\kappa_k)}{I_{C/2-1}(\\kappa_k)} + \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\nSetting this derivative to zero, we get:\n\n$\\frac{C/2-1}{\\kappa_k} - \\frac{I_{C/2}(\\kappa_k)}{I_{C/2-1}(\\kappa_k)} + \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i = 0$\n\nSolving for $\\kappa_k$, we obtain:\n\n$\\kappa_k = \\frac{\\|\\sum_{i=1}^{M_k} g_i\\| (C - \\|\\sum_{i=1}^{M_k} g_i\\|^2)}{1 - \\|\\sum_{i=1}^{M_k} g_i\\|^2}$\n\nThis equation allows us to compute the concentration parameter $\\kappa_k$ based on the alignment of the feature vectors. The concentration parameter $\\kappa_k$ is larger when the distribution is more tightly clustered around the mean direction, and smaller when the features are more uniformly spread across the hypersphere.\n\nBy maximizing the likelihood function for the vMF distribution, we have shown that the parameters $\\mu_k$ and $\\kappa_k$ can be estimated to model the distribution of feature vectors for each class. \nThe mean direction $\\mu_k$ denotes the central direction of the feature cluster, and the concentration parameter $\\kappa_k$ controls the tightness of this clustering. Moreover, the way we estimate the parameters of vMF distribution in EMA is also consistent with the results of the above theoretical derivation.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser.", "author_response": "**Proposition 2**: The representation of samples in the same class can vary significantly across different batches during contrastive distillation, and semantic-guided consistency regularization helps to learn structured features.\n\n**Justification**: Without regularization, the representation of samples within the same class can vary significantly across different batches during contrastive distillation. This variance arises due to random sampling and the influence of negative samples in different batches. The weakly-supervised contrastive loss is defined as:\n\n$\\mathcal{L}_{\\mathrm{sup}} = - \\frac{1}{M_s} \\sum _{i=1}^{M_s} \\log \\left[ \\frac{1}{|A(i)|} \\sum _{a\\in A(i)} \\frac{\\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_a \\rangle/\\tau)}}{\\sum _{j=1}^{M_s} \\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_j \\rangle /\\tau)}}\\right]$\n\nThe features of negative samples $\\mathbf{G}^{\\mathrm{2D}}_j$ vary across batches, leading to different optimization paths for each mini-batch. This introduces variability in the learned representations $\\mathbf{G}^{\\mathrm{3D}}_i$ for samples of the same class $k$.\n\nWhen we do not use semantic-guided consistency regularization, the within-class variance for class $k$ across different batches is:\n\n$\\sigma_W^2 = \\frac{1}{|B|} \\sum_{B} \\frac{1}{M_k} \\sum_{i=1}^{M_k^B} \\|g_i^k - \\mu_k^B\\|^2$\n\nFor ease of reading, we use $g_i$ to refer to point feature $\\mathbf{G}^{\\mathrm{3D}}_i$.\nAnd $\\mu_k^B$ is the mean feature vector for class $k$ in batch $B$. Due to the batch-wise variability in negative samples, $\\mu_k^B$ can differ significantly across batches, leading to high within-class variance. \n\n\nBy minimizing the KL divergence, we align feature vectors $g_i$ of class $k$ with the mean direction $\\mu_k$, reducing the spread of feature vectors within the same class. The within-class variance with regularization is:\n\n$\\sigma_W^2 = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\|g_i^k - \\mu_k\\|^2$\n\nSince $\\mu_k$ is consistent across batches due to the regularization, the within-class variance is significantly reduced. This results in structured feature representations, enhancing class separability and improving performance in downstream tasks.\n\n---\n**Proposition 3**: Learning structural representation during pretraining can benefit downstream tasks.\n\n**Justification**: Structured features are those well-aligned within the same class (low within-class variance $\\sigma_W^2$) and well-separated between different classes (high between-class variance $\\sigma_B^2$). \n\nWith semantic-guided consistency regularization, feature vectors $g_i^k$ for class $k$ are closely aligned with the mean direction $\\mu_k$. This alignment reduces the within-class variance $\\sigma_W^2$. Weakly-supervised contrastive learning pushes apart feature vectors of different classes, increasing the separation between class means $\\mu_k$. This increases the between-class variance $\\sigma_B^2$.\n\nTake the linear classifier as an example, the decision boundary is determined by the separation between class means. Higher $\\sigma_B^2$ and lower $\\sigma_W^2$ result in clearer decision boundaries, reducing classification errors.\n\nConsider a simple linear classifier with weight vector $w$ and bias $b$. The decision function is:\n\n$f(x) = w^T x + b$\n\nThe decision boundary is given by:\n\n$w^T x + b = 0$\n\nFor well-structured features, the margin (distance between decision boundary and nearest samples) is maximized. The margin $ \\gamma $ for class $ k $ can be expressed as:\n\n$\\gamma = \\frac{w^T (\\mu_k - \\mu)}{\\|w\\|}$\n\nHigher between-class variance ($\\sigma_B^2$) and lower within-class variance ($\\sigma_W^2$) increase this margin, leading to better classification performance.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser."}
{"claim": "The paper does not explain how propagated errors from SAM-generated semantic masks are mitigated during image-to-LiDAR representation learning.", "claim_type": "methodology", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "P8fCEoWdti", "reviewer": "Reviewer_svRz", "review_text": "Summary: This work aims to tackle the image-to-LiDAR contrastive learning problem for LiDAR-based point cloud segmentation. Previous approaches designed the cross-modal contrastive learning objective for model pretraining, using superpixels and superpoints as guidance.\n\nIn this work, the authors observe that the superpixel-driven contrastive loss tends to involve ‘’self-conflict’’ issues during representation learning. A weakly-supervised contrastive distillation method is proposed, which generates semantic superpixels/superpoints using the Segment Anything Model (SAM). Additionally, to balance the imbalanced class distributions of LiDAR scene categories during representation, a density and category-aware sampling strategy is proposed to adjust the sampling probabilities of different anchor points using the weak semantic labels.\n\nThe overall framework is named OLIVINE, which adopts three optimization objectives:\n- Weakly-supervised contrastive distillation using coarse semantic labels to identify positive pairs by category.\n- Self-supervised contrastive distillation applied to randomly sampled point-pixel pairs.\n- A regularization framework based on the von Mises-Fisher (vMF) distribution to ensure semantic consistency.\n\nThe proposed OLIVINE method is evaluated on the nuScenes, SemanticKITTI, and KITTI object detection datasets. The results exhibit a consistent improvement of the proposed method compared to existing approaches.\n\nStrengths: (+) This work aims to improve the image-to-LiDAR self-supervised representation learning problem on LiDAR-based point cloud datasets, which is one of the current research hotspots, especially for applications related to autonomous driving and robotics.\n\n(+) The proposed method has exhibited promising performance on mainstream benchmarks, including nuScenes linear probing, nuScenes fine-tuning, SemanticKITTI fine-tuning, and KITTI object detection.\n\nWeaknesses: (-) The weakly-supervised contrastive distillation method has been used in previous literature, such as [R1] and [R2]. Adding semantic categories seems not to cause a major improvement over class-agnostic masks, as the Segment Anything Model is able to segment rather complete and semantically consistent objects and backgrounds. Additionally, using weak labels (which might be erroneous) could introduce additional errors during pretraining.\n\n(-) The motivation for using the von Mises-Fisher (vMF) distribution to enforce consistency regularization for image-to-LiDAR representation learning is not clear enough to demonstrate its superiority. A more detailed explanation and theoretical justification would strengthen this aspect of the work.\n\n(-) Compared to some of the most related works, for example, [R1] and [R3], the scale and depth regarding the experiments (for example, downstream fine-tuning on other datasets than SemanticKITTI) could be further enhanced.\n\n---\n\n### References:\n- [R1] Youquan Liu, et al. “Segment Any Point Cloud Sequences by Distilling Vision Foundation Models,” NeurIPS, 2023.\n- [R2] Ayça Takmaz, et al. “OpenMask3D: Open-Vocabulary 3D Instance Segmentation,” NeurIPS, 2023.\n- [R3] Gilles Puy, et al. “Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving,” arXiv, 2023.\n\nQuestions: - **Q1:** As mentioned in Weakness 1, the semantic masks generated by the Segment Anything Model could inevitably involve errors (e.g., wrong segmentation results). How do the authors handle the propagated errors during image-to-LiDAR representation learning?\n\n- **Q2:** As mentioned in Weakness 2, could the authors provide more details on the hyperparameter settings for the vMF distribution and the reasoning behind their chosen values? Adding a more detailed explanation and theoretical justification would be even better.\n\n- **Q3:** As mentioned in Weakness 3, having more thorough experimental analyses on other LiDAR-based point cloud datasets, such as SemanticPOSS, Waymo, SynLiDAR, etc., could further consolidate the findings and conclusions drawn in the manuscript.\n\n- **Q4:** As most 2D and 3D representation learning approaches (MoCo, SimCLR, Seal, etc.) do, having empirical analyses of models under out-of-distribution datasets is recommended. \n\n- **[Minor]:** The computational cost of the proposed multi-modal contrastive distillation approach is not thoroughly analyzed, which is crucial for real-time applications in autonomous driving.\n\n- **[Minor]:** The generalizability of OLIVINE to other types of sensors (for example, hybrid-solid LiDARs) or environments (for example, off-board environments) beyond the evaluated datasets is not discussed.\n\n- **[Minor]:** “NuScenes” should be revised to “nuScenes”.", "labeling_timestamp": "2026-01-11T16:22:18.722351", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The authors do not explain how errors from SAM-generated semantic masks are handled. They clarify they do not use superpixels and describe uses of semantic labels (e.g., for weakly-supervised distillation and consistency regularization), but they do not address mitigation of noisy or propagated SAM mask errors.", "evidence": "\"We have not claimed that using **semantic superpixels** is our contribution. In fact, our method does not rely on **superpixels** at all, which is different from previous methods [R1-R4].\" \"The semantic **labels** can be flexibly utilized in multiple aspects of the proposed method, such as weakly-supervised contrastive distillation, semantic-guided consistency regularization, and category-aware anchor point sampling.\"", "combined_author_response": "Dear Reviewers,\n\nWe sincerely thank you for your thoughtful evaluations during the rebuttal stage. We are pleased that our detailed responses have addressed most of your concerns. We appreciate your recognition of the additional experiments and the clarification we have made to the paper.\n\nWe are grateful for the time you have invested in reviewing our work and for your consistent recognition of its value. Based on your valuable feedback, we will carefully revise the manuscript, incorporating the additional experiments and analyses into the final version.\n\nBest regards,\n\nThe Authors\n\n---\n\nDear Reviewer 54Gn,\n\nThank you for your response and for taking the time to carefully review our rebuttal. We greatly appreciate your recognition of our efforts to address your concerns and the value you found in the additional experiments we conducted. Your detailed and thoughtful review demonstrates a profound expertise in this domain. I have thoroughly enjoyed the opportunity to learn from your perspective.\n\nPlease feel free to share any further comments or suggestions.\n\nWarm regards,\n\nAuthors\n\n---\n\nDear Reviewer uKMo,\n\nThanks again for the time and energy you committed and your valuable comments. Your meticulous review and thoughtful critiques truly reflect your deep domain expertise and diligence as a reviewer. It has been a pleasure communicating and exchanging ideas with you.\n\nPlease feel free to share any additional comments or feedback on the manuscript.\n\nWarm regards,\n\nAuthors\n\n---\n\nDear Reviewer 2WGA,\n\nThank you for taking the time to review our rebuttal and for your constructive feedback throughout the process. We are glad that we could address most of your concerns. \n\nWe will actively participate in the Author-Reviewer discussion session. Please feel free to share any additional comments or feedback on the manuscript.\n\nWarm regards,\n\nAuthors\n\n---\n\nThank you for the positive feedback provided and the time devoted to this review. We are glad that our efforts have addressed your concerns. Next, we will address your remaining concerns.\n\n---\n\n**Comments**: *The authors may over-claim the contribution of using \"semantic superpixels\" over the \"class-agnostic superpixels\"*.\n\n**Response:**  We believe there may be a **misunderstanding** regarding our proposed methods. We would like to clarify the following points to address your concerns:\n\n- We have **not** claimed that using **semantic superpixels** is our contribution. In fact, our method does not rely on **superpixels** at all, which is different from previous methods [R1-R4].\n- Previous methods [R1-R4] use superpixels to pool 3D point features and 2D pixel features, learning with a superpixel-to-superpoint contrastive loss. In contrast, our method directly uses the features of **individual** points and pixels for contrastive distillation.\n- The semantic **labels** can be flexibly utilized in multiple aspects of the proposed method, such as weakly-supervised contrastive distillation, semantic-guided consistency regularization, and category-aware anchor point sampling. These aspects cannot be effectively addressed using only the class-agnostic superpixels.\n\n---\n**Comments**: *The use of semantic categories seems not to cause a major improvement over class-agnostic masks.*\n\n**Response:** Extensive experiments demonstrate that this approach **substantially** outperforms superpixels-based (class-agnostic mask-based) methods [R1-R4] in various downstream tasks.\n- Our method achieves a **significant** improvement over superpixels-based pretraining methods on nuScenes and SemanticKITTI datasets. As shown in the table below, our method outperforms Seal [R1] by a significant margin, achieving an improvement of 5.14\\% under the setting of linear probing. The full results are available in Table M1 of the uploaded PDF file.\n- Following your suggestions, we have added experiments on six additional LiDAR-based point cloud datasets and one out-of-distribution dataset. And our proposed OLIVINE **consistently** outperforms the superpixels-based methods on all datasets. For the full results, please refer to Tables M2 and M3 of the uploaded PDF file.\n\n| Method    | LP     | 1%     | 5%     | 10%    | 25%    | 100%   |\n| :-------- | :----- | :----- | :----- | :----- | :----- | :----- |\n| Random    | 8\\.10   | 30\\.30 | 47\\.84 | 56\\.15 | 65\\.48 | 74\\.66 |\n| PPKT      | 35\\.90  | 37\\.80 | 53\\.74 | 60\\.25 | 67\\.14 | 74\\.52 |\n| SLidR     | 38\\.80  | 38\\.30 | 52\\.49 | 59\\.84 | 66\\.91 | 74\\.79 |\n| ST-SLidR  | 40\\.48 | 40\\.75 | 54\\.69 | 60\\.75 | 67\\.70 | 75\\.14 |\n| HVDistill | 39\\.50  | 42\\.70 | 56\\.60 | 62\\.90 | 69\\.30 | 76\\.60 |\n| Seal      | 44\\.95 | 45\\.84 | 55\\.64 | 62\\.97 | 68\\.41 | 75\\.60 |\n| Ours      | **50\\.09** | **50\\.60** | **60\\.25** | **65\\.07** | **70\\.15** | **76\\.69** |\n---\nThanks again for your diligence as a reviewer. It is with great pleasure communicating with you. Please feel free to share any additional comments or feedback on the manuscript.\n\n**References**:\\\n[R1] Image-to-lidar self-supervised distillation for autonomous driving data.\\\n[R2] Self-supervised image-to-point distillation via semantically tolerant contrastive loss.\\\n[R3] Segment Any Point Cloud Sequences by Distilling Vision Foundation Models.\\\n[R4] HVDistill: Transferring Knowledge from Images to Point Clouds via Unsupervised Hybrid-View Distillation.\n\n---\n\n**Proposition 2**: The representation of samples in the same class can vary significantly across different batches during contrastive distillation, and semantic-guided consistency regularization helps to learn structured features.\n\n**Justification**: Without regularization, the representation of samples within the same class can vary significantly across different batches during contrastive distillation. This variance arises due to random sampling and the influence of negative samples in different batches. The weakly-supervised contrastive loss is defined as:\n\n$\\mathcal{L}_{\\mathrm{sup}} = - \\frac{1}{M_s} \\sum _{i=1}^{M_s} \\log \\left[ \\frac{1}{|A(i)|} \\sum _{a\\in A(i)} \\frac{\\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_a \\rangle/\\tau)}}{\\sum _{j=1}^{M_s} \\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_j \\rangle /\\tau)}}\\right]$\n\nThe features of negative samples $\\mathbf{G}^{\\mathrm{2D}}_j$ vary across batches, leading to different optimization paths for each mini-batch. This introduces variability in the learned representations $\\mathbf{G}^{\\mathrm{3D}}_i$ for samples of the same class $k$.\n\nWhen we do not use semantic-guided consistency regularization, the within-class variance for class $k$ across different batches is:\n\n$\\sigma_W^2 = \\frac{1}{|B|} \\sum_{B} \\frac{1}{M_k} \\sum_{i=1}^{M_k^B} \\|g_i^k - \\mu_k^B\\|^2$\n\nFor ease of reading, we use $g_i$ to refer to point feature $\\mathbf{G}^{\\mathrm{3D}}_i$.\nAnd $\\mu_k^B$ is the mean feature vector for class $k$ in batch $B$. Due to the batch-wise variability in negative samples, $\\mu_k^B$ can differ significantly across batches, leading to high within-class variance. \n\n\nBy minimizing the KL divergence, we align feature vectors $g_i$ of class $k$ with the mean direction $\\mu_k$, reducing the spread of feature vectors within the same class. The within-class variance with regularization is:\n\n$\\sigma_W^2 = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\|g_i^k - \\mu_k\\|^2$\n\nSince $\\mu_k$ is consistent across batches due to the regularization, the within-class variance is significantly reduced. This results in structured feature representations, enhancing class separability and improving performance in downstream tasks.\n\n---\n**Proposition 3**: Learning structural representation during pretraining can benefit downstream tasks.\n\n**Justification**: Structured features are those well-aligned within the same class (low within-class variance $\\sigma_W^2$) and well-separated between different classes (high between-class variance $\\sigma_B^2$). \n\nWith semantic-guided consistency regularization, feature vectors $g_i^k$ for class $k$ are closely aligned with the mean direction $\\mu_k$. This alignment reduces the within-class variance $\\sigma_W^2$. Weakly-supervised contrastive learning pushes apart feature vectors of different classes, increasing the separation between class means $\\mu_k$. This increases the between-class variance $\\sigma_B^2$.\n\nTake the linear classifier as an example, the decision boundary is determined by the separation between class means. Higher $\\sigma_B^2$ and lower $\\sigma_W^2$ result in clearer decision boundaries, reducing classification errors.\n\nConsider a simple linear classifier with weight vector $w$ and bias $b$. The decision function is:\n\n$f(x) = w^T x + b$\n\nThe decision boundary is given by:\n\n$w^T x + b = 0$\n\nFor well-structured features, the margin (distance between decision boundary and nearest samples) is maximized. The margin $ \\gamma $ for class $ k $ can be expressed as:\n\n$\\gamma = \\frac{w^T (\\mu_k - \\mu)}{\\|w\\|}$\n\nHigher between-class variance ($\\sigma_B^2$) and lower within-class variance ($\\sigma_W^2$) increase this margin, leading to better classification performance.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser.\n\n---\n\n**Proposition 1**: The features of each class $k$ can be modeled as a von Mises-Fisher (vMF) distribution. This means that for class $k$, the feature vectors $g_i$ lie on a unit hypersphere and are centered around a mean direction $\\mu_k$ with a concentration parameter $\\kappa_k$.\n\n**Justification**: To show that the features of each class can be effectively modeled by a vMF distribution, we use maximum likelihood estimation (MLE) to determine that the parameters $\\mu_k$ and $\\kappa_k$ are optimal for the given set of feature vectors.\n\nFor a set of $M_k$ feature vectors $\\\\{g_i\\\\}_{i=1}^{M_k}$ from class $k$, the likelihood function for the vMF distribution is:\n\n$L(\\mu_k, \\kappa_k) = \\prod_{i=1}^{M_k} f(g_i; \\mu_k, \\kappa_k) = \\prod_{i=1}^{M_k} \\mathcal{K}_{C}(\\kappa_k) \\exp(\\kappa_k \\mu_k^T g_i)$\n\nTaking the natural logarithm of the likelihood function, we get the log-likelihood:\n\n$\\log L(\\mu_k, \\kappa_k) = \\sum_{i=1}^{M_k} \\log f(g_i; \\mu_k, \\kappa_k) = M_k \\log \\mathcal{K}_{C}(\\kappa_k) + \\kappa_k \\sum _{i=1}^{M_k} \\mu_k^T g_i$\n\nSubstituting the expression for $\\mathcal{K}_{C}(\\kappa_k)$, we get:\n\n$\\log L(\\mu_k, \\kappa_k) = M_k \\left[ \\log \\left( \\frac{\\kappa_k^{C/2-1}}{(2\\pi)^{C/2} I_{C/2-1}(\\kappa_k)} \\right) + \\frac{\\kappa_k}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\n$\\log L(\\mu_k, \\kappa_k) = M_k \\left[ (C/2-1) \\log \\kappa_k - \\log I_{C/2-1}(\\kappa_k) - \\frac{C}{2} \\log(2\\pi) + \\frac{\\kappa_k}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\nTo maximize the log-likelihood, we normalize $\\mu_k$ by setting it to the normalized sum of the feature vectors:\n$\\mu_k = \\frac{\\sum_{i=1}^{M_k} g_i}{\\|\\sum_{i=1}^{M_k} g_i\\|}$\n\nThe derivative of the log-likelihood with respect to $\\kappa_k$ is:\n\n$\\frac{\\partial \\log L(\\mu_k, \\kappa_k)}{\\partial \\kappa_k} = M_k \\left[ \\frac{C/2-1}{\\kappa_k} - \\frac{I_{C/2}(\\kappa_k)}{I_{C/2-1}(\\kappa_k)} + \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\nSetting this derivative to zero, we get:\n\n$\\frac{C/2-1}{\\kappa_k} - \\frac{I_{C/2}(\\kappa_k)}{I_{C/2-1}(\\kappa_k)} + \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i = 0$\n\nSolving for $\\kappa_k$, we obtain:\n\n$\\kappa_k = \\frac{\\|\\sum_{i=1}^{M_k} g_i\\| (C - \\|\\sum_{i=1}^{M_k} g_i\\|^2)}{1 - \\|\\sum_{i=1}^{M_k} g_i\\|^2}$\n\nThis equation allows us to compute the concentration parameter $\\kappa_k$ based on the alignment of the feature vectors. The concentration parameter $\\kappa_k$ is larger when the distribution is more tightly clustered around the mean direction, and smaller when the features are more uniformly spread across the hypersphere.\n\nBy maximizing the likelihood function for the vMF distribution, we have shown that the parameters $\\mu_k$ and $\\kappa_k$ can be estimated to model the distribution of feature vectors for each class. \nThe mean direction $\\mu_k$ denotes the central direction of the feature cluster, and the concentration parameter $\\kappa_k$ controls the tightness of this clustering. Moreover, the way we estimate the parameters of vMF distribution in EMA is also consistent with the results of the above theoretical derivation.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser.", "author_response": "**Proposition 2**: The representation of samples in the same class can vary significantly across different batches during contrastive distillation, and semantic-guided consistency regularization helps to learn structured features.\n\n**Justification**: Without regularization, the representation of samples within the same class can vary significantly across different batches during contrastive distillation. This variance arises due to random sampling and the influence of negative samples in different batches. The weakly-supervised contrastive loss is defined as:\n\n$\\mathcal{L}_{\\mathrm{sup}} = - \\frac{1}{M_s} \\sum _{i=1}^{M_s} \\log \\left[ \\frac{1}{|A(i)|} \\sum _{a\\in A(i)} \\frac{\\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_a \\rangle/\\tau)}}{\\sum _{j=1}^{M_s} \\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_j \\rangle /\\tau)}}\\right]$\n\nThe features of negative samples $\\mathbf{G}^{\\mathrm{2D}}_j$ vary across batches, leading to different optimization paths for each mini-batch. This introduces variability in the learned representations $\\mathbf{G}^{\\mathrm{3D}}_i$ for samples of the same class $k$.\n\nWhen we do not use semantic-guided consistency regularization, the within-class variance for class $k$ across different batches is:\n\n$\\sigma_W^2 = \\frac{1}{|B|} \\sum_{B} \\frac{1}{M_k} \\sum_{i=1}^{M_k^B} \\|g_i^k - \\mu_k^B\\|^2$\n\nFor ease of reading, we use $g_i$ to refer to point feature $\\mathbf{G}^{\\mathrm{3D}}_i$.\nAnd $\\mu_k^B$ is the mean feature vector for class $k$ in batch $B$. Due to the batch-wise variability in negative samples, $\\mu_k^B$ can differ significantly across batches, leading to high within-class variance. \n\n\nBy minimizing the KL divergence, we align feature vectors $g_i$ of class $k$ with the mean direction $\\mu_k$, reducing the spread of feature vectors within the same class. The within-class variance with regularization is:\n\n$\\sigma_W^2 = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\|g_i^k - \\mu_k\\|^2$\n\nSince $\\mu_k$ is consistent across batches due to the regularization, the within-class variance is significantly reduced. This results in structured feature representations, enhancing class separability and improving performance in downstream tasks.\n\n---\n**Proposition 3**: Learning structural representation during pretraining can benefit downstream tasks.\n\n**Justification**: Structured features are those well-aligned within the same class (low within-class variance $\\sigma_W^2$) and well-separated between different classes (high between-class variance $\\sigma_B^2$). \n\nWith semantic-guided consistency regularization, feature vectors $g_i^k$ for class $k$ are closely aligned with the mean direction $\\mu_k$. This alignment reduces the within-class variance $\\sigma_W^2$. Weakly-supervised contrastive learning pushes apart feature vectors of different classes, increasing the separation between class means $\\mu_k$. This increases the between-class variance $\\sigma_B^2$.\n\nTake the linear classifier as an example, the decision boundary is determined by the separation between class means. Higher $\\sigma_B^2$ and lower $\\sigma_W^2$ result in clearer decision boundaries, reducing classification errors.\n\nConsider a simple linear classifier with weight vector $w$ and bias $b$. The decision function is:\n\n$f(x) = w^T x + b$\n\nThe decision boundary is given by:\n\n$w^T x + b = 0$\n\nFor well-structured features, the margin (distance between decision boundary and nearest samples) is maximized. The margin $ \\gamma $ for class $ k $ can be expressed as:\n\n$\\gamma = \\frac{w^T (\\mu_k - \\mu)}{\\|w\\|}$\n\nHigher between-class variance ($\\sigma_B^2$) and lower within-class variance ($\\sigma_W^2$) increase this margin, leading to better classification performance.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser."}
{"claim": "The manuscript does not provide evaluations on additional LiDAR datasets such as SemanticPOSS, Waymo, or SynLiDAR.", "claim_type": "experimental", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "P8fCEoWdti", "reviewer": "Reviewer_svRz", "review_text": "Summary: This work aims to tackle the image-to-LiDAR contrastive learning problem for LiDAR-based point cloud segmentation. Previous approaches designed the cross-modal contrastive learning objective for model pretraining, using superpixels and superpoints as guidance.\n\nIn this work, the authors observe that the superpixel-driven contrastive loss tends to involve ‘’self-conflict’’ issues during representation learning. A weakly-supervised contrastive distillation method is proposed, which generates semantic superpixels/superpoints using the Segment Anything Model (SAM). Additionally, to balance the imbalanced class distributions of LiDAR scene categories during representation, a density and category-aware sampling strategy is proposed to adjust the sampling probabilities of different anchor points using the weak semantic labels.\n\nThe overall framework is named OLIVINE, which adopts three optimization objectives:\n- Weakly-supervised contrastive distillation using coarse semantic labels to identify positive pairs by category.\n- Self-supervised contrastive distillation applied to randomly sampled point-pixel pairs.\n- A regularization framework based on the von Mises-Fisher (vMF) distribution to ensure semantic consistency.\n\nThe proposed OLIVINE method is evaluated on the nuScenes, SemanticKITTI, and KITTI object detection datasets. The results exhibit a consistent improvement of the proposed method compared to existing approaches.\n\nStrengths: (+) This work aims to improve the image-to-LiDAR self-supervised representation learning problem on LiDAR-based point cloud datasets, which is one of the current research hotspots, especially for applications related to autonomous driving and robotics.\n\n(+) The proposed method has exhibited promising performance on mainstream benchmarks, including nuScenes linear probing, nuScenes fine-tuning, SemanticKITTI fine-tuning, and KITTI object detection.\n\nWeaknesses: (-) The weakly-supervised contrastive distillation method has been used in previous literature, such as [R1] and [R2]. Adding semantic categories seems not to cause a major improvement over class-agnostic masks, as the Segment Anything Model is able to segment rather complete and semantically consistent objects and backgrounds. Additionally, using weak labels (which might be erroneous) could introduce additional errors during pretraining.\n\n(-) The motivation for using the von Mises-Fisher (vMF) distribution to enforce consistency regularization for image-to-LiDAR representation learning is not clear enough to demonstrate its superiority. A more detailed explanation and theoretical justification would strengthen this aspect of the work.\n\n(-) Compared to some of the most related works, for example, [R1] and [R3], the scale and depth regarding the experiments (for example, downstream fine-tuning on other datasets than SemanticKITTI) could be further enhanced.\n\n---\n\n### References:\n- [R1] Youquan Liu, et al. “Segment Any Point Cloud Sequences by Distilling Vision Foundation Models,” NeurIPS, 2023.\n- [R2] Ayça Takmaz, et al. “OpenMask3D: Open-Vocabulary 3D Instance Segmentation,” NeurIPS, 2023.\n- [R3] Gilles Puy, et al. “Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving,” arXiv, 2023.\n\nQuestions: - **Q1:** As mentioned in Weakness 1, the semantic masks generated by the Segment Anything Model could inevitably involve errors (e.g., wrong segmentation results). How do the authors handle the propagated errors during image-to-LiDAR representation learning?\n\n- **Q2:** As mentioned in Weakness 2, could the authors provide more details on the hyperparameter settings for the vMF distribution and the reasoning behind their chosen values? Adding a more detailed explanation and theoretical justification would be even better.\n\n- **Q3:** As mentioned in Weakness 3, having more thorough experimental analyses on other LiDAR-based point cloud datasets, such as SemanticPOSS, Waymo, SynLiDAR, etc., could further consolidate the findings and conclusions drawn in the manuscript.\n\n- **Q4:** As most 2D and 3D representation learning approaches (MoCo, SimCLR, Seal, etc.) do, having empirical analyses of models under out-of-distribution datasets is recommended. \n\n- **[Minor]:** The computational cost of the proposed multi-modal contrastive distillation approach is not thoroughly analyzed, which is crucial for real-time applications in autonomous driving.\n\n- **[Minor]:** The generalizability of OLIVINE to other types of sensors (for example, hybrid-solid LiDARs) or environments (for example, off-board environments) beyond the evaluated datasets is not discussed.\n\n- **[Minor]:** “NuScenes” should be revised to “nuScenes”.", "labeling_timestamp": "2026-01-11T16:22:12.063175", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors state they have added experiments on multiple additional LiDAR datasets and report consistent results, indicating they addressed the lack of evaluations.", "evidence": "Following your suggestions, we have added experiments on six additional LiDAR-based point cloud datasets and one out-of-distribution dataset. And our proposed OLIVINE **consistently** outperforms the superpixels-based methods on all datasets. For the full results, please refer to Tables M2 and M3 of the uploaded PDF file.", "combined_author_response": "Dear Reviewers,\n\nWe sincerely thank you for your thoughtful evaluations during the rebuttal stage. We are pleased that our detailed responses have addressed most of your concerns. We appreciate your recognition of the additional experiments and the clarification we have made to the paper.\n\nWe are grateful for the time you have invested in reviewing our work and for your consistent recognition of its value. Based on your valuable feedback, we will carefully revise the manuscript, incorporating the additional experiments and analyses into the final version.\n\nBest regards,\n\nThe Authors\n\n---\n\nDear Reviewer 54Gn,\n\nThank you for your response and for taking the time to carefully review our rebuttal. We greatly appreciate your recognition of our efforts to address your concerns and the value you found in the additional experiments we conducted. Your detailed and thoughtful review demonstrates a profound expertise in this domain. I have thoroughly enjoyed the opportunity to learn from your perspective.\n\nPlease feel free to share any further comments or suggestions.\n\nWarm regards,\n\nAuthors\n\n---\n\nDear Reviewer uKMo,\n\nThanks again for the time and energy you committed and your valuable comments. Your meticulous review and thoughtful critiques truly reflect your deep domain expertise and diligence as a reviewer. It has been a pleasure communicating and exchanging ideas with you.\n\nPlease feel free to share any additional comments or feedback on the manuscript.\n\nWarm regards,\n\nAuthors\n\n---\n\nDear Reviewer 2WGA,\n\nThank you for taking the time to review our rebuttal and for your constructive feedback throughout the process. We are glad that we could address most of your concerns. \n\nWe will actively participate in the Author-Reviewer discussion session. Please feel free to share any additional comments or feedback on the manuscript.\n\nWarm regards,\n\nAuthors\n\n---\n\nThank you for the positive feedback provided and the time devoted to this review. We are glad that our efforts have addressed your concerns. Next, we will address your remaining concerns.\n\n---\n\n**Comments**: *The authors may over-claim the contribution of using \"semantic superpixels\" over the \"class-agnostic superpixels\"*.\n\n**Response:**  We believe there may be a **misunderstanding** regarding our proposed methods. We would like to clarify the following points to address your concerns:\n\n- We have **not** claimed that using **semantic superpixels** is our contribution. In fact, our method does not rely on **superpixels** at all, which is different from previous methods [R1-R4].\n- Previous methods [R1-R4] use superpixels to pool 3D point features and 2D pixel features, learning with a superpixel-to-superpoint contrastive loss. In contrast, our method directly uses the features of **individual** points and pixels for contrastive distillation.\n- The semantic **labels** can be flexibly utilized in multiple aspects of the proposed method, such as weakly-supervised contrastive distillation, semantic-guided consistency regularization, and category-aware anchor point sampling. These aspects cannot be effectively addressed using only the class-agnostic superpixels.\n\n---\n**Comments**: *The use of semantic categories seems not to cause a major improvement over class-agnostic masks.*\n\n**Response:** Extensive experiments demonstrate that this approach **substantially** outperforms superpixels-based (class-agnostic mask-based) methods [R1-R4] in various downstream tasks.\n- Our method achieves a **significant** improvement over superpixels-based pretraining methods on nuScenes and SemanticKITTI datasets. As shown in the table below, our method outperforms Seal [R1] by a significant margin, achieving an improvement of 5.14\\% under the setting of linear probing. The full results are available in Table M1 of the uploaded PDF file.\n- Following your suggestions, we have added experiments on six additional LiDAR-based point cloud datasets and one out-of-distribution dataset. And our proposed OLIVINE **consistently** outperforms the superpixels-based methods on all datasets. For the full results, please refer to Tables M2 and M3 of the uploaded PDF file.\n\n| Method    | LP     | 1%     | 5%     | 10%    | 25%    | 100%   |\n| :-------- | :----- | :----- | :----- | :----- | :----- | :----- |\n| Random    | 8\\.10   | 30\\.30 | 47\\.84 | 56\\.15 | 65\\.48 | 74\\.66 |\n| PPKT      | 35\\.90  | 37\\.80 | 53\\.74 | 60\\.25 | 67\\.14 | 74\\.52 |\n| SLidR     | 38\\.80  | 38\\.30 | 52\\.49 | 59\\.84 | 66\\.91 | 74\\.79 |\n| ST-SLidR  | 40\\.48 | 40\\.75 | 54\\.69 | 60\\.75 | 67\\.70 | 75\\.14 |\n| HVDistill | 39\\.50  | 42\\.70 | 56\\.60 | 62\\.90 | 69\\.30 | 76\\.60 |\n| Seal      | 44\\.95 | 45\\.84 | 55\\.64 | 62\\.97 | 68\\.41 | 75\\.60 |\n| Ours      | **50\\.09** | **50\\.60** | **60\\.25** | **65\\.07** | **70\\.15** | **76\\.69** |\n---\nThanks again for your diligence as a reviewer. It is with great pleasure communicating with you. Please feel free to share any additional comments or feedback on the manuscript.\n\n**References**:\\\n[R1] Image-to-lidar self-supervised distillation for autonomous driving data.\\\n[R2] Self-supervised image-to-point distillation via semantically tolerant contrastive loss.\\\n[R3] Segment Any Point Cloud Sequences by Distilling Vision Foundation Models.\\\n[R4] HVDistill: Transferring Knowledge from Images to Point Clouds via Unsupervised Hybrid-View Distillation.\n\n---\n\n**Proposition 2**: The representation of samples in the same class can vary significantly across different batches during contrastive distillation, and semantic-guided consistency regularization helps to learn structured features.\n\n**Justification**: Without regularization, the representation of samples within the same class can vary significantly across different batches during contrastive distillation. This variance arises due to random sampling and the influence of negative samples in different batches. The weakly-supervised contrastive loss is defined as:\n\n$\\mathcal{L}_{\\mathrm{sup}} = - \\frac{1}{M_s} \\sum _{i=1}^{M_s} \\log \\left[ \\frac{1}{|A(i)|} \\sum _{a\\in A(i)} \\frac{\\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_a \\rangle/\\tau)}}{\\sum _{j=1}^{M_s} \\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_j \\rangle /\\tau)}}\\right]$\n\nThe features of negative samples $\\mathbf{G}^{\\mathrm{2D}}_j$ vary across batches, leading to different optimization paths for each mini-batch. This introduces variability in the learned representations $\\mathbf{G}^{\\mathrm{3D}}_i$ for samples of the same class $k$.\n\nWhen we do not use semantic-guided consistency regularization, the within-class variance for class $k$ across different batches is:\n\n$\\sigma_W^2 = \\frac{1}{|B|} \\sum_{B} \\frac{1}{M_k} \\sum_{i=1}^{M_k^B} \\|g_i^k - \\mu_k^B\\|^2$\n\nFor ease of reading, we use $g_i$ to refer to point feature $\\mathbf{G}^{\\mathrm{3D}}_i$.\nAnd $\\mu_k^B$ is the mean feature vector for class $k$ in batch $B$. Due to the batch-wise variability in negative samples, $\\mu_k^B$ can differ significantly across batches, leading to high within-class variance. \n\n\nBy minimizing the KL divergence, we align feature vectors $g_i$ of class $k$ with the mean direction $\\mu_k$, reducing the spread of feature vectors within the same class. The within-class variance with regularization is:\n\n$\\sigma_W^2 = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\|g_i^k - \\mu_k\\|^2$\n\nSince $\\mu_k$ is consistent across batches due to the regularization, the within-class variance is significantly reduced. This results in structured feature representations, enhancing class separability and improving performance in downstream tasks.\n\n---\n**Proposition 3**: Learning structural representation during pretraining can benefit downstream tasks.\n\n**Justification**: Structured features are those well-aligned within the same class (low within-class variance $\\sigma_W^2$) and well-separated between different classes (high between-class variance $\\sigma_B^2$). \n\nWith semantic-guided consistency regularization, feature vectors $g_i^k$ for class $k$ are closely aligned with the mean direction $\\mu_k$. This alignment reduces the within-class variance $\\sigma_W^2$. Weakly-supervised contrastive learning pushes apart feature vectors of different classes, increasing the separation between class means $\\mu_k$. This increases the between-class variance $\\sigma_B^2$.\n\nTake the linear classifier as an example, the decision boundary is determined by the separation between class means. Higher $\\sigma_B^2$ and lower $\\sigma_W^2$ result in clearer decision boundaries, reducing classification errors.\n\nConsider a simple linear classifier with weight vector $w$ and bias $b$. The decision function is:\n\n$f(x) = w^T x + b$\n\nThe decision boundary is given by:\n\n$w^T x + b = 0$\n\nFor well-structured features, the margin (distance between decision boundary and nearest samples) is maximized. The margin $ \\gamma $ for class $ k $ can be expressed as:\n\n$\\gamma = \\frac{w^T (\\mu_k - \\mu)}{\\|w\\|}$\n\nHigher between-class variance ($\\sigma_B^2$) and lower within-class variance ($\\sigma_W^2$) increase this margin, leading to better classification performance.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser.\n\n---\n\n**Proposition 1**: The features of each class $k$ can be modeled as a von Mises-Fisher (vMF) distribution. This means that for class $k$, the feature vectors $g_i$ lie on a unit hypersphere and are centered around a mean direction $\\mu_k$ with a concentration parameter $\\kappa_k$.\n\n**Justification**: To show that the features of each class can be effectively modeled by a vMF distribution, we use maximum likelihood estimation (MLE) to determine that the parameters $\\mu_k$ and $\\kappa_k$ are optimal for the given set of feature vectors.\n\nFor a set of $M_k$ feature vectors $\\\\{g_i\\\\}_{i=1}^{M_k}$ from class $k$, the likelihood function for the vMF distribution is:\n\n$L(\\mu_k, \\kappa_k) = \\prod_{i=1}^{M_k} f(g_i; \\mu_k, \\kappa_k) = \\prod_{i=1}^{M_k} \\mathcal{K}_{C}(\\kappa_k) \\exp(\\kappa_k \\mu_k^T g_i)$\n\nTaking the natural logarithm of the likelihood function, we get the log-likelihood:\n\n$\\log L(\\mu_k, \\kappa_k) = \\sum_{i=1}^{M_k} \\log f(g_i; \\mu_k, \\kappa_k) = M_k \\log \\mathcal{K}_{C}(\\kappa_k) + \\kappa_k \\sum _{i=1}^{M_k} \\mu_k^T g_i$\n\nSubstituting the expression for $\\mathcal{K}_{C}(\\kappa_k)$, we get:\n\n$\\log L(\\mu_k, \\kappa_k) = M_k \\left[ \\log \\left( \\frac{\\kappa_k^{C/2-1}}{(2\\pi)^{C/2} I_{C/2-1}(\\kappa_k)} \\right) + \\frac{\\kappa_k}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\n$\\log L(\\mu_k, \\kappa_k) = M_k \\left[ (C/2-1) \\log \\kappa_k - \\log I_{C/2-1}(\\kappa_k) - \\frac{C}{2} \\log(2\\pi) + \\frac{\\kappa_k}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\nTo maximize the log-likelihood, we normalize $\\mu_k$ by setting it to the normalized sum of the feature vectors:\n$\\mu_k = \\frac{\\sum_{i=1}^{M_k} g_i}{\\|\\sum_{i=1}^{M_k} g_i\\|}$\n\nThe derivative of the log-likelihood with respect to $\\kappa_k$ is:\n\n$\\frac{\\partial \\log L(\\mu_k, \\kappa_k)}{\\partial \\kappa_k} = M_k \\left[ \\frac{C/2-1}{\\kappa_k} - \\frac{I_{C/2}(\\kappa_k)}{I_{C/2-1}(\\kappa_k)} + \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\nSetting this derivative to zero, we get:\n\n$\\frac{C/2-1}{\\kappa_k} - \\frac{I_{C/2}(\\kappa_k)}{I_{C/2-1}(\\kappa_k)} + \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i = 0$\n\nSolving for $\\kappa_k$, we obtain:\n\n$\\kappa_k = \\frac{\\|\\sum_{i=1}^{M_k} g_i\\| (C - \\|\\sum_{i=1}^{M_k} g_i\\|^2)}{1 - \\|\\sum_{i=1}^{M_k} g_i\\|^2}$\n\nThis equation allows us to compute the concentration parameter $\\kappa_k$ based on the alignment of the feature vectors. The concentration parameter $\\kappa_k$ is larger when the distribution is more tightly clustered around the mean direction, and smaller when the features are more uniformly spread across the hypersphere.\n\nBy maximizing the likelihood function for the vMF distribution, we have shown that the parameters $\\mu_k$ and $\\kappa_k$ can be estimated to model the distribution of feature vectors for each class. \nThe mean direction $\\mu_k$ denotes the central direction of the feature cluster, and the concentration parameter $\\kappa_k$ controls the tightness of this clustering. Moreover, the way we estimate the parameters of vMF distribution in EMA is also consistent with the results of the above theoretical derivation.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser.", "author_response": "**Proposition 2**: The representation of samples in the same class can vary significantly across different batches during contrastive distillation, and semantic-guided consistency regularization helps to learn structured features.\n\n**Justification**: Without regularization, the representation of samples within the same class can vary significantly across different batches during contrastive distillation. This variance arises due to random sampling and the influence of negative samples in different batches. The weakly-supervised contrastive loss is defined as:\n\n$\\mathcal{L}_{\\mathrm{sup}} = - \\frac{1}{M_s} \\sum _{i=1}^{M_s} \\log \\left[ \\frac{1}{|A(i)|} \\sum _{a\\in A(i)} \\frac{\\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_a \\rangle/\\tau)}}{\\sum _{j=1}^{M_s} \\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_j \\rangle /\\tau)}}\\right]$\n\nThe features of negative samples $\\mathbf{G}^{\\mathrm{2D}}_j$ vary across batches, leading to different optimization paths for each mini-batch. This introduces variability in the learned representations $\\mathbf{G}^{\\mathrm{3D}}_i$ for samples of the same class $k$.\n\nWhen we do not use semantic-guided consistency regularization, the within-class variance for class $k$ across different batches is:\n\n$\\sigma_W^2 = \\frac{1}{|B|} \\sum_{B} \\frac{1}{M_k} \\sum_{i=1}^{M_k^B} \\|g_i^k - \\mu_k^B\\|^2$\n\nFor ease of reading, we use $g_i$ to refer to point feature $\\mathbf{G}^{\\mathrm{3D}}_i$.\nAnd $\\mu_k^B$ is the mean feature vector for class $k$ in batch $B$. Due to the batch-wise variability in negative samples, $\\mu_k^B$ can differ significantly across batches, leading to high within-class variance. \n\n\nBy minimizing the KL divergence, we align feature vectors $g_i$ of class $k$ with the mean direction $\\mu_k$, reducing the spread of feature vectors within the same class. The within-class variance with regularization is:\n\n$\\sigma_W^2 = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\|g_i^k - \\mu_k\\|^2$\n\nSince $\\mu_k$ is consistent across batches due to the regularization, the within-class variance is significantly reduced. This results in structured feature representations, enhancing class separability and improving performance in downstream tasks.\n\n---\n**Proposition 3**: Learning structural representation during pretraining can benefit downstream tasks.\n\n**Justification**: Structured features are those well-aligned within the same class (low within-class variance $\\sigma_W^2$) and well-separated between different classes (high between-class variance $\\sigma_B^2$). \n\nWith semantic-guided consistency regularization, feature vectors $g_i^k$ for class $k$ are closely aligned with the mean direction $\\mu_k$. This alignment reduces the within-class variance $\\sigma_W^2$. Weakly-supervised contrastive learning pushes apart feature vectors of different classes, increasing the separation between class means $\\mu_k$. This increases the between-class variance $\\sigma_B^2$.\n\nTake the linear classifier as an example, the decision boundary is determined by the separation between class means. Higher $\\sigma_B^2$ and lower $\\sigma_W^2$ result in clearer decision boundaries, reducing classification errors.\n\nConsider a simple linear classifier with weight vector $w$ and bias $b$. The decision function is:\n\n$f(x) = w^T x + b$\n\nThe decision boundary is given by:\n\n$w^T x + b = 0$\n\nFor well-structured features, the margin (distance between decision boundary and nearest samples) is maximized. The margin $ \\gamma $ for class $ k $ can be expressed as:\n\n$\\gamma = \\frac{w^T (\\mu_k - \\mu)}{\\|w\\|}$\n\nHigher between-class variance ($\\sigma_B^2$) and lower within-class variance ($\\sigma_W^2$) increase this margin, leading to better classification performance.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser."}
{"claim": "The paper lacks empirical analyses of model performance on out-of-distribution datasets to assess robustness.", "claim_type": "experimental", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "P8fCEoWdti", "reviewer": "Reviewer_svRz", "review_text": "Summary: This work aims to tackle the image-to-LiDAR contrastive learning problem for LiDAR-based point cloud segmentation. Previous approaches designed the cross-modal contrastive learning objective for model pretraining, using superpixels and superpoints as guidance.\n\nIn this work, the authors observe that the superpixel-driven contrastive loss tends to involve ‘’self-conflict’’ issues during representation learning. A weakly-supervised contrastive distillation method is proposed, which generates semantic superpixels/superpoints using the Segment Anything Model (SAM). Additionally, to balance the imbalanced class distributions of LiDAR scene categories during representation, a density and category-aware sampling strategy is proposed to adjust the sampling probabilities of different anchor points using the weak semantic labels.\n\nThe overall framework is named OLIVINE, which adopts three optimization objectives:\n- Weakly-supervised contrastive distillation using coarse semantic labels to identify positive pairs by category.\n- Self-supervised contrastive distillation applied to randomly sampled point-pixel pairs.\n- A regularization framework based on the von Mises-Fisher (vMF) distribution to ensure semantic consistency.\n\nThe proposed OLIVINE method is evaluated on the nuScenes, SemanticKITTI, and KITTI object detection datasets. The results exhibit a consistent improvement of the proposed method compared to existing approaches.\n\nStrengths: (+) This work aims to improve the image-to-LiDAR self-supervised representation learning problem on LiDAR-based point cloud datasets, which is one of the current research hotspots, especially for applications related to autonomous driving and robotics.\n\n(+) The proposed method has exhibited promising performance on mainstream benchmarks, including nuScenes linear probing, nuScenes fine-tuning, SemanticKITTI fine-tuning, and KITTI object detection.\n\nWeaknesses: (-) The weakly-supervised contrastive distillation method has been used in previous literature, such as [R1] and [R2]. Adding semantic categories seems not to cause a major improvement over class-agnostic masks, as the Segment Anything Model is able to segment rather complete and semantically consistent objects and backgrounds. Additionally, using weak labels (which might be erroneous) could introduce additional errors during pretraining.\n\n(-) The motivation for using the von Mises-Fisher (vMF) distribution to enforce consistency regularization for image-to-LiDAR representation learning is not clear enough to demonstrate its superiority. A more detailed explanation and theoretical justification would strengthen this aspect of the work.\n\n(-) Compared to some of the most related works, for example, [R1] and [R3], the scale and depth regarding the experiments (for example, downstream fine-tuning on other datasets than SemanticKITTI) could be further enhanced.\n\n---\n\n### References:\n- [R1] Youquan Liu, et al. “Segment Any Point Cloud Sequences by Distilling Vision Foundation Models,” NeurIPS, 2023.\n- [R2] Ayça Takmaz, et al. “OpenMask3D: Open-Vocabulary 3D Instance Segmentation,” NeurIPS, 2023.\n- [R3] Gilles Puy, et al. “Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving,” arXiv, 2023.\n\nQuestions: - **Q1:** As mentioned in Weakness 1, the semantic masks generated by the Segment Anything Model could inevitably involve errors (e.g., wrong segmentation results). How do the authors handle the propagated errors during image-to-LiDAR representation learning?\n\n- **Q2:** As mentioned in Weakness 2, could the authors provide more details on the hyperparameter settings for the vMF distribution and the reasoning behind their chosen values? Adding a more detailed explanation and theoretical justification would be even better.\n\n- **Q3:** As mentioned in Weakness 3, having more thorough experimental analyses on other LiDAR-based point cloud datasets, such as SemanticPOSS, Waymo, SynLiDAR, etc., could further consolidate the findings and conclusions drawn in the manuscript.\n\n- **Q4:** As most 2D and 3D representation learning approaches (MoCo, SimCLR, Seal, etc.) do, having empirical analyses of models under out-of-distribution datasets is recommended. \n\n- **[Minor]:** The computational cost of the proposed multi-modal contrastive distillation approach is not thoroughly analyzed, which is crucial for real-time applications in autonomous driving.\n\n- **[Minor]:** The generalizability of OLIVINE to other types of sensors (for example, hybrid-solid LiDARs) or environments (for example, off-board environments) beyond the evaluated datasets is not discussed.\n\n- **[Minor]:** “NuScenes” should be revised to “nuScenes”.", "labeling_timestamp": "2026-01-11T16:22:12.193077", "model": "gpt-5-mini", "label": "Supported", "justification": "Authors acknowledge the concern and state they added experiments including an out-of-distribution dataset and will incorporate these analyses into the revised manuscript.", "evidence": "\"Following your suggestions, we have added experiments on six additional LiDAR-based point cloud datasets and one out-of-distribution dataset. And our proposed OLIVINE consistently outperforms the superpixels-based methods on all datasets. For the full results, please refer to Tables M2 and M3 of the uploaded PDF file.\"; \"Based on your valuable feedback, we will carefully revise the manuscript, incorporating the additional experiments and analyses into the final version.\"", "combined_author_response": "Dear Reviewers,\n\nWe sincerely thank you for your thoughtful evaluations during the rebuttal stage. We are pleased that our detailed responses have addressed most of your concerns. We appreciate your recognition of the additional experiments and the clarification we have made to the paper.\n\nWe are grateful for the time you have invested in reviewing our work and for your consistent recognition of its value. Based on your valuable feedback, we will carefully revise the manuscript, incorporating the additional experiments and analyses into the final version.\n\nBest regards,\n\nThe Authors\n\n---\n\nDear Reviewer 54Gn,\n\nThank you for your response and for taking the time to carefully review our rebuttal. We greatly appreciate your recognition of our efforts to address your concerns and the value you found in the additional experiments we conducted. Your detailed and thoughtful review demonstrates a profound expertise in this domain. I have thoroughly enjoyed the opportunity to learn from your perspective.\n\nPlease feel free to share any further comments or suggestions.\n\nWarm regards,\n\nAuthors\n\n---\n\nDear Reviewer uKMo,\n\nThanks again for the time and energy you committed and your valuable comments. Your meticulous review and thoughtful critiques truly reflect your deep domain expertise and diligence as a reviewer. It has been a pleasure communicating and exchanging ideas with you.\n\nPlease feel free to share any additional comments or feedback on the manuscript.\n\nWarm regards,\n\nAuthors\n\n---\n\nDear Reviewer 2WGA,\n\nThank you for taking the time to review our rebuttal and for your constructive feedback throughout the process. We are glad that we could address most of your concerns. \n\nWe will actively participate in the Author-Reviewer discussion session. Please feel free to share any additional comments or feedback on the manuscript.\n\nWarm regards,\n\nAuthors\n\n---\n\nThank you for the positive feedback provided and the time devoted to this review. We are glad that our efforts have addressed your concerns. Next, we will address your remaining concerns.\n\n---\n\n**Comments**: *The authors may over-claim the contribution of using \"semantic superpixels\" over the \"class-agnostic superpixels\"*.\n\n**Response:**  We believe there may be a **misunderstanding** regarding our proposed methods. We would like to clarify the following points to address your concerns:\n\n- We have **not** claimed that using **semantic superpixels** is our contribution. In fact, our method does not rely on **superpixels** at all, which is different from previous methods [R1-R4].\n- Previous methods [R1-R4] use superpixels to pool 3D point features and 2D pixel features, learning with a superpixel-to-superpoint contrastive loss. In contrast, our method directly uses the features of **individual** points and pixels for contrastive distillation.\n- The semantic **labels** can be flexibly utilized in multiple aspects of the proposed method, such as weakly-supervised contrastive distillation, semantic-guided consistency regularization, and category-aware anchor point sampling. These aspects cannot be effectively addressed using only the class-agnostic superpixels.\n\n---\n**Comments**: *The use of semantic categories seems not to cause a major improvement over class-agnostic masks.*\n\n**Response:** Extensive experiments demonstrate that this approach **substantially** outperforms superpixels-based (class-agnostic mask-based) methods [R1-R4] in various downstream tasks.\n- Our method achieves a **significant** improvement over superpixels-based pretraining methods on nuScenes and SemanticKITTI datasets. As shown in the table below, our method outperforms Seal [R1] by a significant margin, achieving an improvement of 5.14\\% under the setting of linear probing. The full results are available in Table M1 of the uploaded PDF file.\n- Following your suggestions, we have added experiments on six additional LiDAR-based point cloud datasets and one out-of-distribution dataset. And our proposed OLIVINE **consistently** outperforms the superpixels-based methods on all datasets. For the full results, please refer to Tables M2 and M3 of the uploaded PDF file.\n\n| Method    | LP     | 1%     | 5%     | 10%    | 25%    | 100%   |\n| :-------- | :----- | :----- | :----- | :----- | :----- | :----- |\n| Random    | 8\\.10   | 30\\.30 | 47\\.84 | 56\\.15 | 65\\.48 | 74\\.66 |\n| PPKT      | 35\\.90  | 37\\.80 | 53\\.74 | 60\\.25 | 67\\.14 | 74\\.52 |\n| SLidR     | 38\\.80  | 38\\.30 | 52\\.49 | 59\\.84 | 66\\.91 | 74\\.79 |\n| ST-SLidR  | 40\\.48 | 40\\.75 | 54\\.69 | 60\\.75 | 67\\.70 | 75\\.14 |\n| HVDistill | 39\\.50  | 42\\.70 | 56\\.60 | 62\\.90 | 69\\.30 | 76\\.60 |\n| Seal      | 44\\.95 | 45\\.84 | 55\\.64 | 62\\.97 | 68\\.41 | 75\\.60 |\n| Ours      | **50\\.09** | **50\\.60** | **60\\.25** | **65\\.07** | **70\\.15** | **76\\.69** |\n---\nThanks again for your diligence as a reviewer. It is with great pleasure communicating with you. Please feel free to share any additional comments or feedback on the manuscript.\n\n**References**:\\\n[R1] Image-to-lidar self-supervised distillation for autonomous driving data.\\\n[R2] Self-supervised image-to-point distillation via semantically tolerant contrastive loss.\\\n[R3] Segment Any Point Cloud Sequences by Distilling Vision Foundation Models.\\\n[R4] HVDistill: Transferring Knowledge from Images to Point Clouds via Unsupervised Hybrid-View Distillation.\n\n---\n\n**Proposition 2**: The representation of samples in the same class can vary significantly across different batches during contrastive distillation, and semantic-guided consistency regularization helps to learn structured features.\n\n**Justification**: Without regularization, the representation of samples within the same class can vary significantly across different batches during contrastive distillation. This variance arises due to random sampling and the influence of negative samples in different batches. The weakly-supervised contrastive loss is defined as:\n\n$\\mathcal{L}_{\\mathrm{sup}} = - \\frac{1}{M_s} \\sum _{i=1}^{M_s} \\log \\left[ \\frac{1}{|A(i)|} \\sum _{a\\in A(i)} \\frac{\\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_a \\rangle/\\tau)}}{\\sum _{j=1}^{M_s} \\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_j \\rangle /\\tau)}}\\right]$\n\nThe features of negative samples $\\mathbf{G}^{\\mathrm{2D}}_j$ vary across batches, leading to different optimization paths for each mini-batch. This introduces variability in the learned representations $\\mathbf{G}^{\\mathrm{3D}}_i$ for samples of the same class $k$.\n\nWhen we do not use semantic-guided consistency regularization, the within-class variance for class $k$ across different batches is:\n\n$\\sigma_W^2 = \\frac{1}{|B|} \\sum_{B} \\frac{1}{M_k} \\sum_{i=1}^{M_k^B} \\|g_i^k - \\mu_k^B\\|^2$\n\nFor ease of reading, we use $g_i$ to refer to point feature $\\mathbf{G}^{\\mathrm{3D}}_i$.\nAnd $\\mu_k^B$ is the mean feature vector for class $k$ in batch $B$. Due to the batch-wise variability in negative samples, $\\mu_k^B$ can differ significantly across batches, leading to high within-class variance. \n\n\nBy minimizing the KL divergence, we align feature vectors $g_i$ of class $k$ with the mean direction $\\mu_k$, reducing the spread of feature vectors within the same class. The within-class variance with regularization is:\n\n$\\sigma_W^2 = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\|g_i^k - \\mu_k\\|^2$\n\nSince $\\mu_k$ is consistent across batches due to the regularization, the within-class variance is significantly reduced. This results in structured feature representations, enhancing class separability and improving performance in downstream tasks.\n\n---\n**Proposition 3**: Learning structural representation during pretraining can benefit downstream tasks.\n\n**Justification**: Structured features are those well-aligned within the same class (low within-class variance $\\sigma_W^2$) and well-separated between different classes (high between-class variance $\\sigma_B^2$). \n\nWith semantic-guided consistency regularization, feature vectors $g_i^k$ for class $k$ are closely aligned with the mean direction $\\mu_k$. This alignment reduces the within-class variance $\\sigma_W^2$. Weakly-supervised contrastive learning pushes apart feature vectors of different classes, increasing the separation between class means $\\mu_k$. This increases the between-class variance $\\sigma_B^2$.\n\nTake the linear classifier as an example, the decision boundary is determined by the separation between class means. Higher $\\sigma_B^2$ and lower $\\sigma_W^2$ result in clearer decision boundaries, reducing classification errors.\n\nConsider a simple linear classifier with weight vector $w$ and bias $b$. The decision function is:\n\n$f(x) = w^T x + b$\n\nThe decision boundary is given by:\n\n$w^T x + b = 0$\n\nFor well-structured features, the margin (distance between decision boundary and nearest samples) is maximized. The margin $ \\gamma $ for class $ k $ can be expressed as:\n\n$\\gamma = \\frac{w^T (\\mu_k - \\mu)}{\\|w\\|}$\n\nHigher between-class variance ($\\sigma_B^2$) and lower within-class variance ($\\sigma_W^2$) increase this margin, leading to better classification performance.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser.\n\n---\n\n**Proposition 1**: The features of each class $k$ can be modeled as a von Mises-Fisher (vMF) distribution. This means that for class $k$, the feature vectors $g_i$ lie on a unit hypersphere and are centered around a mean direction $\\mu_k$ with a concentration parameter $\\kappa_k$.\n\n**Justification**: To show that the features of each class can be effectively modeled by a vMF distribution, we use maximum likelihood estimation (MLE) to determine that the parameters $\\mu_k$ and $\\kappa_k$ are optimal for the given set of feature vectors.\n\nFor a set of $M_k$ feature vectors $\\\\{g_i\\\\}_{i=1}^{M_k}$ from class $k$, the likelihood function for the vMF distribution is:\n\n$L(\\mu_k, \\kappa_k) = \\prod_{i=1}^{M_k} f(g_i; \\mu_k, \\kappa_k) = \\prod_{i=1}^{M_k} \\mathcal{K}_{C}(\\kappa_k) \\exp(\\kappa_k \\mu_k^T g_i)$\n\nTaking the natural logarithm of the likelihood function, we get the log-likelihood:\n\n$\\log L(\\mu_k, \\kappa_k) = \\sum_{i=1}^{M_k} \\log f(g_i; \\mu_k, \\kappa_k) = M_k \\log \\mathcal{K}_{C}(\\kappa_k) + \\kappa_k \\sum _{i=1}^{M_k} \\mu_k^T g_i$\n\nSubstituting the expression for $\\mathcal{K}_{C}(\\kappa_k)$, we get:\n\n$\\log L(\\mu_k, \\kappa_k) = M_k \\left[ \\log \\left( \\frac{\\kappa_k^{C/2-1}}{(2\\pi)^{C/2} I_{C/2-1}(\\kappa_k)} \\right) + \\frac{\\kappa_k}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\n$\\log L(\\mu_k, \\kappa_k) = M_k \\left[ (C/2-1) \\log \\kappa_k - \\log I_{C/2-1}(\\kappa_k) - \\frac{C}{2} \\log(2\\pi) + \\frac{\\kappa_k}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\nTo maximize the log-likelihood, we normalize $\\mu_k$ by setting it to the normalized sum of the feature vectors:\n$\\mu_k = \\frac{\\sum_{i=1}^{M_k} g_i}{\\|\\sum_{i=1}^{M_k} g_i\\|}$\n\nThe derivative of the log-likelihood with respect to $\\kappa_k$ is:\n\n$\\frac{\\partial \\log L(\\mu_k, \\kappa_k)}{\\partial \\kappa_k} = M_k \\left[ \\frac{C/2-1}{\\kappa_k} - \\frac{I_{C/2}(\\kappa_k)}{I_{C/2-1}(\\kappa_k)} + \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\nSetting this derivative to zero, we get:\n\n$\\frac{C/2-1}{\\kappa_k} - \\frac{I_{C/2}(\\kappa_k)}{I_{C/2-1}(\\kappa_k)} + \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i = 0$\n\nSolving for $\\kappa_k$, we obtain:\n\n$\\kappa_k = \\frac{\\|\\sum_{i=1}^{M_k} g_i\\| (C - \\|\\sum_{i=1}^{M_k} g_i\\|^2)}{1 - \\|\\sum_{i=1}^{M_k} g_i\\|^2}$\n\nThis equation allows us to compute the concentration parameter $\\kappa_k$ based on the alignment of the feature vectors. The concentration parameter $\\kappa_k$ is larger when the distribution is more tightly clustered around the mean direction, and smaller when the features are more uniformly spread across the hypersphere.\n\nBy maximizing the likelihood function for the vMF distribution, we have shown that the parameters $\\mu_k$ and $\\kappa_k$ can be estimated to model the distribution of feature vectors for each class. \nThe mean direction $\\mu_k$ denotes the central direction of the feature cluster, and the concentration parameter $\\kappa_k$ controls the tightness of this clustering. Moreover, the way we estimate the parameters of vMF distribution in EMA is also consistent with the results of the above theoretical derivation.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser.", "author_response": "**Proposition 2**: The representation of samples in the same class can vary significantly across different batches during contrastive distillation, and semantic-guided consistency regularization helps to learn structured features.\n\n**Justification**: Without regularization, the representation of samples within the same class can vary significantly across different batches during contrastive distillation. This variance arises due to random sampling and the influence of negative samples in different batches. The weakly-supervised contrastive loss is defined as:\n\n$\\mathcal{L}_{\\mathrm{sup}} = - \\frac{1}{M_s} \\sum _{i=1}^{M_s} \\log \\left[ \\frac{1}{|A(i)|} \\sum _{a\\in A(i)} \\frac{\\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_a \\rangle/\\tau)}}{\\sum _{j=1}^{M_s} \\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_j \\rangle /\\tau)}}\\right]$\n\nThe features of negative samples $\\mathbf{G}^{\\mathrm{2D}}_j$ vary across batches, leading to different optimization paths for each mini-batch. This introduces variability in the learned representations $\\mathbf{G}^{\\mathrm{3D}}_i$ for samples of the same class $k$.\n\nWhen we do not use semantic-guided consistency regularization, the within-class variance for class $k$ across different batches is:\n\n$\\sigma_W^2 = \\frac{1}{|B|} \\sum_{B} \\frac{1}{M_k} \\sum_{i=1}^{M_k^B} \\|g_i^k - \\mu_k^B\\|^2$\n\nFor ease of reading, we use $g_i$ to refer to point feature $\\mathbf{G}^{\\mathrm{3D}}_i$.\nAnd $\\mu_k^B$ is the mean feature vector for class $k$ in batch $B$. Due to the batch-wise variability in negative samples, $\\mu_k^B$ can differ significantly across batches, leading to high within-class variance. \n\n\nBy minimizing the KL divergence, we align feature vectors $g_i$ of class $k$ with the mean direction $\\mu_k$, reducing the spread of feature vectors within the same class. The within-class variance with regularization is:\n\n$\\sigma_W^2 = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\|g_i^k - \\mu_k\\|^2$\n\nSince $\\mu_k$ is consistent across batches due to the regularization, the within-class variance is significantly reduced. This results in structured feature representations, enhancing class separability and improving performance in downstream tasks.\n\n---\n**Proposition 3**: Learning structural representation during pretraining can benefit downstream tasks.\n\n**Justification**: Structured features are those well-aligned within the same class (low within-class variance $\\sigma_W^2$) and well-separated between different classes (high between-class variance $\\sigma_B^2$). \n\nWith semantic-guided consistency regularization, feature vectors $g_i^k$ for class $k$ are closely aligned with the mean direction $\\mu_k$. This alignment reduces the within-class variance $\\sigma_W^2$. Weakly-supervised contrastive learning pushes apart feature vectors of different classes, increasing the separation between class means $\\mu_k$. This increases the between-class variance $\\sigma_B^2$.\n\nTake the linear classifier as an example, the decision boundary is determined by the separation between class means. Higher $\\sigma_B^2$ and lower $\\sigma_W^2$ result in clearer decision boundaries, reducing classification errors.\n\nConsider a simple linear classifier with weight vector $w$ and bias $b$. The decision function is:\n\n$f(x) = w^T x + b$\n\nThe decision boundary is given by:\n\n$w^T x + b = 0$\n\nFor well-structured features, the margin (distance between decision boundary and nearest samples) is maximized. The margin $ \\gamma $ for class $ k $ can be expressed as:\n\n$\\gamma = \\frac{w^T (\\mu_k - \\mu)}{\\|w\\|}$\n\nHigher between-class variance ($\\sigma_B^2$) and lower within-class variance ($\\sigma_W^2$) increase this margin, leading to better classification performance.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser."}
{"claim": "The computational cost and runtime implications of the proposed multi-modal contrastive distillation are not thoroughly analyzed.", "claim_type": "methodology", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "P8fCEoWdti", "reviewer": "Reviewer_svRz", "review_text": "Summary: This work aims to tackle the image-to-LiDAR contrastive learning problem for LiDAR-based point cloud segmentation. Previous approaches designed the cross-modal contrastive learning objective for model pretraining, using superpixels and superpoints as guidance.\n\nIn this work, the authors observe that the superpixel-driven contrastive loss tends to involve ‘’self-conflict’’ issues during representation learning. A weakly-supervised contrastive distillation method is proposed, which generates semantic superpixels/superpoints using the Segment Anything Model (SAM). Additionally, to balance the imbalanced class distributions of LiDAR scene categories during representation, a density and category-aware sampling strategy is proposed to adjust the sampling probabilities of different anchor points using the weak semantic labels.\n\nThe overall framework is named OLIVINE, which adopts three optimization objectives:\n- Weakly-supervised contrastive distillation using coarse semantic labels to identify positive pairs by category.\n- Self-supervised contrastive distillation applied to randomly sampled point-pixel pairs.\n- A regularization framework based on the von Mises-Fisher (vMF) distribution to ensure semantic consistency.\n\nThe proposed OLIVINE method is evaluated on the nuScenes, SemanticKITTI, and KITTI object detection datasets. The results exhibit a consistent improvement of the proposed method compared to existing approaches.\n\nStrengths: (+) This work aims to improve the image-to-LiDAR self-supervised representation learning problem on LiDAR-based point cloud datasets, which is one of the current research hotspots, especially for applications related to autonomous driving and robotics.\n\n(+) The proposed method has exhibited promising performance on mainstream benchmarks, including nuScenes linear probing, nuScenes fine-tuning, SemanticKITTI fine-tuning, and KITTI object detection.\n\nWeaknesses: (-) The weakly-supervised contrastive distillation method has been used in previous literature, such as [R1] and [R2]. Adding semantic categories seems not to cause a major improvement over class-agnostic masks, as the Segment Anything Model is able to segment rather complete and semantically consistent objects and backgrounds. Additionally, using weak labels (which might be erroneous) could introduce additional errors during pretraining.\n\n(-) The motivation for using the von Mises-Fisher (vMF) distribution to enforce consistency regularization for image-to-LiDAR representation learning is not clear enough to demonstrate its superiority. A more detailed explanation and theoretical justification would strengthen this aspect of the work.\n\n(-) Compared to some of the most related works, for example, [R1] and [R3], the scale and depth regarding the experiments (for example, downstream fine-tuning on other datasets than SemanticKITTI) could be further enhanced.\n\n---\n\n### References:\n- [R1] Youquan Liu, et al. “Segment Any Point Cloud Sequences by Distilling Vision Foundation Models,” NeurIPS, 2023.\n- [R2] Ayça Takmaz, et al. “OpenMask3D: Open-Vocabulary 3D Instance Segmentation,” NeurIPS, 2023.\n- [R3] Gilles Puy, et al. “Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving,” arXiv, 2023.\n\nQuestions: - **Q1:** As mentioned in Weakness 1, the semantic masks generated by the Segment Anything Model could inevitably involve errors (e.g., wrong segmentation results). How do the authors handle the propagated errors during image-to-LiDAR representation learning?\n\n- **Q2:** As mentioned in Weakness 2, could the authors provide more details on the hyperparameter settings for the vMF distribution and the reasoning behind their chosen values? Adding a more detailed explanation and theoretical justification would be even better.\n\n- **Q3:** As mentioned in Weakness 3, having more thorough experimental analyses on other LiDAR-based point cloud datasets, such as SemanticPOSS, Waymo, SynLiDAR, etc., could further consolidate the findings and conclusions drawn in the manuscript.\n\n- **Q4:** As most 2D and 3D representation learning approaches (MoCo, SimCLR, Seal, etc.) do, having empirical analyses of models under out-of-distribution datasets is recommended. \n\n- **[Minor]:** The computational cost of the proposed multi-modal contrastive distillation approach is not thoroughly analyzed, which is crucial for real-time applications in autonomous driving.\n\n- **[Minor]:** The generalizability of OLIVINE to other types of sensors (for example, hybrid-solid LiDARs) or environments (for example, off-board environments) beyond the evaluated datasets is not discussed.\n\n- **[Minor]:** “NuScenes” should be revised to “nuScenes”.", "labeling_timestamp": "2026-01-11T16:22:22.281282", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The authors respond at length about experiments, semantic choices, and theoretical justifications but do not provide any analysis, numbers, or discussion regarding computational cost or runtime of their method, so the reviewer's concern is not addressed.", "evidence": "\"Following your suggestions, we have added experiments on six additional LiDAR-based point cloud datasets and one out-of-distribution dataset. And our proposed OLIVINE consistently outperforms the superpixels-based methods on all datasets. For the full results, please refer to Tables M2 and M3 of the uploaded PDF file.\"", "combined_author_response": "Dear Reviewers,\n\nWe sincerely thank you for your thoughtful evaluations during the rebuttal stage. We are pleased that our detailed responses have addressed most of your concerns. We appreciate your recognition of the additional experiments and the clarification we have made to the paper.\n\nWe are grateful for the time you have invested in reviewing our work and for your consistent recognition of its value. Based on your valuable feedback, we will carefully revise the manuscript, incorporating the additional experiments and analyses into the final version.\n\nBest regards,\n\nThe Authors\n\n---\n\nDear Reviewer 54Gn,\n\nThank you for your response and for taking the time to carefully review our rebuttal. We greatly appreciate your recognition of our efforts to address your concerns and the value you found in the additional experiments we conducted. Your detailed and thoughtful review demonstrates a profound expertise in this domain. I have thoroughly enjoyed the opportunity to learn from your perspective.\n\nPlease feel free to share any further comments or suggestions.\n\nWarm regards,\n\nAuthors\n\n---\n\nDear Reviewer uKMo,\n\nThanks again for the time and energy you committed and your valuable comments. Your meticulous review and thoughtful critiques truly reflect your deep domain expertise and diligence as a reviewer. It has been a pleasure communicating and exchanging ideas with you.\n\nPlease feel free to share any additional comments or feedback on the manuscript.\n\nWarm regards,\n\nAuthors\n\n---\n\nDear Reviewer 2WGA,\n\nThank you for taking the time to review our rebuttal and for your constructive feedback throughout the process. We are glad that we could address most of your concerns. \n\nWe will actively participate in the Author-Reviewer discussion session. Please feel free to share any additional comments or feedback on the manuscript.\n\nWarm regards,\n\nAuthors\n\n---\n\nThank you for the positive feedback provided and the time devoted to this review. We are glad that our efforts have addressed your concerns. Next, we will address your remaining concerns.\n\n---\n\n**Comments**: *The authors may over-claim the contribution of using \"semantic superpixels\" over the \"class-agnostic superpixels\"*.\n\n**Response:**  We believe there may be a **misunderstanding** regarding our proposed methods. We would like to clarify the following points to address your concerns:\n\n- We have **not** claimed that using **semantic superpixels** is our contribution. In fact, our method does not rely on **superpixels** at all, which is different from previous methods [R1-R4].\n- Previous methods [R1-R4] use superpixels to pool 3D point features and 2D pixel features, learning with a superpixel-to-superpoint contrastive loss. In contrast, our method directly uses the features of **individual** points and pixels for contrastive distillation.\n- The semantic **labels** can be flexibly utilized in multiple aspects of the proposed method, such as weakly-supervised contrastive distillation, semantic-guided consistency regularization, and category-aware anchor point sampling. These aspects cannot be effectively addressed using only the class-agnostic superpixels.\n\n---\n**Comments**: *The use of semantic categories seems not to cause a major improvement over class-agnostic masks.*\n\n**Response:** Extensive experiments demonstrate that this approach **substantially** outperforms superpixels-based (class-agnostic mask-based) methods [R1-R4] in various downstream tasks.\n- Our method achieves a **significant** improvement over superpixels-based pretraining methods on nuScenes and SemanticKITTI datasets. As shown in the table below, our method outperforms Seal [R1] by a significant margin, achieving an improvement of 5.14\\% under the setting of linear probing. The full results are available in Table M1 of the uploaded PDF file.\n- Following your suggestions, we have added experiments on six additional LiDAR-based point cloud datasets and one out-of-distribution dataset. And our proposed OLIVINE **consistently** outperforms the superpixels-based methods on all datasets. For the full results, please refer to Tables M2 and M3 of the uploaded PDF file.\n\n| Method    | LP     | 1%     | 5%     | 10%    | 25%    | 100%   |\n| :-------- | :----- | :----- | :----- | :----- | :----- | :----- |\n| Random    | 8\\.10   | 30\\.30 | 47\\.84 | 56\\.15 | 65\\.48 | 74\\.66 |\n| PPKT      | 35\\.90  | 37\\.80 | 53\\.74 | 60\\.25 | 67\\.14 | 74\\.52 |\n| SLidR     | 38\\.80  | 38\\.30 | 52\\.49 | 59\\.84 | 66\\.91 | 74\\.79 |\n| ST-SLidR  | 40\\.48 | 40\\.75 | 54\\.69 | 60\\.75 | 67\\.70 | 75\\.14 |\n| HVDistill | 39\\.50  | 42\\.70 | 56\\.60 | 62\\.90 | 69\\.30 | 76\\.60 |\n| Seal      | 44\\.95 | 45\\.84 | 55\\.64 | 62\\.97 | 68\\.41 | 75\\.60 |\n| Ours      | **50\\.09** | **50\\.60** | **60\\.25** | **65\\.07** | **70\\.15** | **76\\.69** |\n---\nThanks again for your diligence as a reviewer. It is with great pleasure communicating with you. Please feel free to share any additional comments or feedback on the manuscript.\n\n**References**:\\\n[R1] Image-to-lidar self-supervised distillation for autonomous driving data.\\\n[R2] Self-supervised image-to-point distillation via semantically tolerant contrastive loss.\\\n[R3] Segment Any Point Cloud Sequences by Distilling Vision Foundation Models.\\\n[R4] HVDistill: Transferring Knowledge from Images to Point Clouds via Unsupervised Hybrid-View Distillation.\n\n---\n\n**Proposition 2**: The representation of samples in the same class can vary significantly across different batches during contrastive distillation, and semantic-guided consistency regularization helps to learn structured features.\n\n**Justification**: Without regularization, the representation of samples within the same class can vary significantly across different batches during contrastive distillation. This variance arises due to random sampling and the influence of negative samples in different batches. The weakly-supervised contrastive loss is defined as:\n\n$\\mathcal{L}_{\\mathrm{sup}} = - \\frac{1}{M_s} \\sum _{i=1}^{M_s} \\log \\left[ \\frac{1}{|A(i)|} \\sum _{a\\in A(i)} \\frac{\\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_a \\rangle/\\tau)}}{\\sum _{j=1}^{M_s} \\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_j \\rangle /\\tau)}}\\right]$\n\nThe features of negative samples $\\mathbf{G}^{\\mathrm{2D}}_j$ vary across batches, leading to different optimization paths for each mini-batch. This introduces variability in the learned representations $\\mathbf{G}^{\\mathrm{3D}}_i$ for samples of the same class $k$.\n\nWhen we do not use semantic-guided consistency regularization, the within-class variance for class $k$ across different batches is:\n\n$\\sigma_W^2 = \\frac{1}{|B|} \\sum_{B} \\frac{1}{M_k} \\sum_{i=1}^{M_k^B} \\|g_i^k - \\mu_k^B\\|^2$\n\nFor ease of reading, we use $g_i$ to refer to point feature $\\mathbf{G}^{\\mathrm{3D}}_i$.\nAnd $\\mu_k^B$ is the mean feature vector for class $k$ in batch $B$. Due to the batch-wise variability in negative samples, $\\mu_k^B$ can differ significantly across batches, leading to high within-class variance. \n\n\nBy minimizing the KL divergence, we align feature vectors $g_i$ of class $k$ with the mean direction $\\mu_k$, reducing the spread of feature vectors within the same class. The within-class variance with regularization is:\n\n$\\sigma_W^2 = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\|g_i^k - \\mu_k\\|^2$\n\nSince $\\mu_k$ is consistent across batches due to the regularization, the within-class variance is significantly reduced. This results in structured feature representations, enhancing class separability and improving performance in downstream tasks.\n\n---\n**Proposition 3**: Learning structural representation during pretraining can benefit downstream tasks.\n\n**Justification**: Structured features are those well-aligned within the same class (low within-class variance $\\sigma_W^2$) and well-separated between different classes (high between-class variance $\\sigma_B^2$). \n\nWith semantic-guided consistency regularization, feature vectors $g_i^k$ for class $k$ are closely aligned with the mean direction $\\mu_k$. This alignment reduces the within-class variance $\\sigma_W^2$. Weakly-supervised contrastive learning pushes apart feature vectors of different classes, increasing the separation between class means $\\mu_k$. This increases the between-class variance $\\sigma_B^2$.\n\nTake the linear classifier as an example, the decision boundary is determined by the separation between class means. Higher $\\sigma_B^2$ and lower $\\sigma_W^2$ result in clearer decision boundaries, reducing classification errors.\n\nConsider a simple linear classifier with weight vector $w$ and bias $b$. The decision function is:\n\n$f(x) = w^T x + b$\n\nThe decision boundary is given by:\n\n$w^T x + b = 0$\n\nFor well-structured features, the margin (distance between decision boundary and nearest samples) is maximized. The margin $ \\gamma $ for class $ k $ can be expressed as:\n\n$\\gamma = \\frac{w^T (\\mu_k - \\mu)}{\\|w\\|}$\n\nHigher between-class variance ($\\sigma_B^2$) and lower within-class variance ($\\sigma_W^2$) increase this margin, leading to better classification performance.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser.\n\n---\n\n**Proposition 1**: The features of each class $k$ can be modeled as a von Mises-Fisher (vMF) distribution. This means that for class $k$, the feature vectors $g_i$ lie on a unit hypersphere and are centered around a mean direction $\\mu_k$ with a concentration parameter $\\kappa_k$.\n\n**Justification**: To show that the features of each class can be effectively modeled by a vMF distribution, we use maximum likelihood estimation (MLE) to determine that the parameters $\\mu_k$ and $\\kappa_k$ are optimal for the given set of feature vectors.\n\nFor a set of $M_k$ feature vectors $\\\\{g_i\\\\}_{i=1}^{M_k}$ from class $k$, the likelihood function for the vMF distribution is:\n\n$L(\\mu_k, \\kappa_k) = \\prod_{i=1}^{M_k} f(g_i; \\mu_k, \\kappa_k) = \\prod_{i=1}^{M_k} \\mathcal{K}_{C}(\\kappa_k) \\exp(\\kappa_k \\mu_k^T g_i)$\n\nTaking the natural logarithm of the likelihood function, we get the log-likelihood:\n\n$\\log L(\\mu_k, \\kappa_k) = \\sum_{i=1}^{M_k} \\log f(g_i; \\mu_k, \\kappa_k) = M_k \\log \\mathcal{K}_{C}(\\kappa_k) + \\kappa_k \\sum _{i=1}^{M_k} \\mu_k^T g_i$\n\nSubstituting the expression for $\\mathcal{K}_{C}(\\kappa_k)$, we get:\n\n$\\log L(\\mu_k, \\kappa_k) = M_k \\left[ \\log \\left( \\frac{\\kappa_k^{C/2-1}}{(2\\pi)^{C/2} I_{C/2-1}(\\kappa_k)} \\right) + \\frac{\\kappa_k}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\n$\\log L(\\mu_k, \\kappa_k) = M_k \\left[ (C/2-1) \\log \\kappa_k - \\log I_{C/2-1}(\\kappa_k) - \\frac{C}{2} \\log(2\\pi) + \\frac{\\kappa_k}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\nTo maximize the log-likelihood, we normalize $\\mu_k$ by setting it to the normalized sum of the feature vectors:\n$\\mu_k = \\frac{\\sum_{i=1}^{M_k} g_i}{\\|\\sum_{i=1}^{M_k} g_i\\|}$\n\nThe derivative of the log-likelihood with respect to $\\kappa_k$ is:\n\n$\\frac{\\partial \\log L(\\mu_k, \\kappa_k)}{\\partial \\kappa_k} = M_k \\left[ \\frac{C/2-1}{\\kappa_k} - \\frac{I_{C/2}(\\kappa_k)}{I_{C/2-1}(\\kappa_k)} + \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\nSetting this derivative to zero, we get:\n\n$\\frac{C/2-1}{\\kappa_k} - \\frac{I_{C/2}(\\kappa_k)}{I_{C/2-1}(\\kappa_k)} + \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i = 0$\n\nSolving for $\\kappa_k$, we obtain:\n\n$\\kappa_k = \\frac{\\|\\sum_{i=1}^{M_k} g_i\\| (C - \\|\\sum_{i=1}^{M_k} g_i\\|^2)}{1 - \\|\\sum_{i=1}^{M_k} g_i\\|^2}$\n\nThis equation allows us to compute the concentration parameter $\\kappa_k$ based on the alignment of the feature vectors. The concentration parameter $\\kappa_k$ is larger when the distribution is more tightly clustered around the mean direction, and smaller when the features are more uniformly spread across the hypersphere.\n\nBy maximizing the likelihood function for the vMF distribution, we have shown that the parameters $\\mu_k$ and $\\kappa_k$ can be estimated to model the distribution of feature vectors for each class. \nThe mean direction $\\mu_k$ denotes the central direction of the feature cluster, and the concentration parameter $\\kappa_k$ controls the tightness of this clustering. Moreover, the way we estimate the parameters of vMF distribution in EMA is also consistent with the results of the above theoretical derivation.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser.", "author_response": "**Proposition 2**: The representation of samples in the same class can vary significantly across different batches during contrastive distillation, and semantic-guided consistency regularization helps to learn structured features.\n\n**Justification**: Without regularization, the representation of samples within the same class can vary significantly across different batches during contrastive distillation. This variance arises due to random sampling and the influence of negative samples in different batches. The weakly-supervised contrastive loss is defined as:\n\n$\\mathcal{L}_{\\mathrm{sup}} = - \\frac{1}{M_s} \\sum _{i=1}^{M_s} \\log \\left[ \\frac{1}{|A(i)|} \\sum _{a\\in A(i)} \\frac{\\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_a \\rangle/\\tau)}}{\\sum _{j=1}^{M_s} \\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_j \\rangle /\\tau)}}\\right]$\n\nThe features of negative samples $\\mathbf{G}^{\\mathrm{2D}}_j$ vary across batches, leading to different optimization paths for each mini-batch. This introduces variability in the learned representations $\\mathbf{G}^{\\mathrm{3D}}_i$ for samples of the same class $k$.\n\nWhen we do not use semantic-guided consistency regularization, the within-class variance for class $k$ across different batches is:\n\n$\\sigma_W^2 = \\frac{1}{|B|} \\sum_{B} \\frac{1}{M_k} \\sum_{i=1}^{M_k^B} \\|g_i^k - \\mu_k^B\\|^2$\n\nFor ease of reading, we use $g_i$ to refer to point feature $\\mathbf{G}^{\\mathrm{3D}}_i$.\nAnd $\\mu_k^B$ is the mean feature vector for class $k$ in batch $B$. Due to the batch-wise variability in negative samples, $\\mu_k^B$ can differ significantly across batches, leading to high within-class variance. \n\n\nBy minimizing the KL divergence, we align feature vectors $g_i$ of class $k$ with the mean direction $\\mu_k$, reducing the spread of feature vectors within the same class. The within-class variance with regularization is:\n\n$\\sigma_W^2 = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\|g_i^k - \\mu_k\\|^2$\n\nSince $\\mu_k$ is consistent across batches due to the regularization, the within-class variance is significantly reduced. This results in structured feature representations, enhancing class separability and improving performance in downstream tasks.\n\n---\n**Proposition 3**: Learning structural representation during pretraining can benefit downstream tasks.\n\n**Justification**: Structured features are those well-aligned within the same class (low within-class variance $\\sigma_W^2$) and well-separated between different classes (high between-class variance $\\sigma_B^2$). \n\nWith semantic-guided consistency regularization, feature vectors $g_i^k$ for class $k$ are closely aligned with the mean direction $\\mu_k$. This alignment reduces the within-class variance $\\sigma_W^2$. Weakly-supervised contrastive learning pushes apart feature vectors of different classes, increasing the separation between class means $\\mu_k$. This increases the between-class variance $\\sigma_B^2$.\n\nTake the linear classifier as an example, the decision boundary is determined by the separation between class means. Higher $\\sigma_B^2$ and lower $\\sigma_W^2$ result in clearer decision boundaries, reducing classification errors.\n\nConsider a simple linear classifier with weight vector $w$ and bias $b$. The decision function is:\n\n$f(x) = w^T x + b$\n\nThe decision boundary is given by:\n\n$w^T x + b = 0$\n\nFor well-structured features, the margin (distance between decision boundary and nearest samples) is maximized. The margin $ \\gamma $ for class $ k $ can be expressed as:\n\n$\\gamma = \\frac{w^T (\\mu_k - \\mu)}{\\|w\\|}$\n\nHigher between-class variance ($\\sigma_B^2$) and lower within-class variance ($\\sigma_W^2$) increase this margin, leading to better classification performance.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser."}
{"claim": "The generalizability of OLIVINE to different sensor types or environmental settings is not discussed.", "claim_type": "methodology", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "P8fCEoWdti", "reviewer": "Reviewer_svRz", "review_text": "Summary: This work aims to tackle the image-to-LiDAR contrastive learning problem for LiDAR-based point cloud segmentation. Previous approaches designed the cross-modal contrastive learning objective for model pretraining, using superpixels and superpoints as guidance.\n\nIn this work, the authors observe that the superpixel-driven contrastive loss tends to involve ‘’self-conflict’’ issues during representation learning. A weakly-supervised contrastive distillation method is proposed, which generates semantic superpixels/superpoints using the Segment Anything Model (SAM). Additionally, to balance the imbalanced class distributions of LiDAR scene categories during representation, a density and category-aware sampling strategy is proposed to adjust the sampling probabilities of different anchor points using the weak semantic labels.\n\nThe overall framework is named OLIVINE, which adopts three optimization objectives:\n- Weakly-supervised contrastive distillation using coarse semantic labels to identify positive pairs by category.\n- Self-supervised contrastive distillation applied to randomly sampled point-pixel pairs.\n- A regularization framework based on the von Mises-Fisher (vMF) distribution to ensure semantic consistency.\n\nThe proposed OLIVINE method is evaluated on the nuScenes, SemanticKITTI, and KITTI object detection datasets. The results exhibit a consistent improvement of the proposed method compared to existing approaches.\n\nStrengths: (+) This work aims to improve the image-to-LiDAR self-supervised representation learning problem on LiDAR-based point cloud datasets, which is one of the current research hotspots, especially for applications related to autonomous driving and robotics.\n\n(+) The proposed method has exhibited promising performance on mainstream benchmarks, including nuScenes linear probing, nuScenes fine-tuning, SemanticKITTI fine-tuning, and KITTI object detection.\n\nWeaknesses: (-) The weakly-supervised contrastive distillation method has been used in previous literature, such as [R1] and [R2]. Adding semantic categories seems not to cause a major improvement over class-agnostic masks, as the Segment Anything Model is able to segment rather complete and semantically consistent objects and backgrounds. Additionally, using weak labels (which might be erroneous) could introduce additional errors during pretraining.\n\n(-) The motivation for using the von Mises-Fisher (vMF) distribution to enforce consistency regularization for image-to-LiDAR representation learning is not clear enough to demonstrate its superiority. A more detailed explanation and theoretical justification would strengthen this aspect of the work.\n\n(-) Compared to some of the most related works, for example, [R1] and [R3], the scale and depth regarding the experiments (for example, downstream fine-tuning on other datasets than SemanticKITTI) could be further enhanced.\n\n---\n\n### References:\n- [R1] Youquan Liu, et al. “Segment Any Point Cloud Sequences by Distilling Vision Foundation Models,” NeurIPS, 2023.\n- [R2] Ayça Takmaz, et al. “OpenMask3D: Open-Vocabulary 3D Instance Segmentation,” NeurIPS, 2023.\n- [R3] Gilles Puy, et al. “Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving,” arXiv, 2023.\n\nQuestions: - **Q1:** As mentioned in Weakness 1, the semantic masks generated by the Segment Anything Model could inevitably involve errors (e.g., wrong segmentation results). How do the authors handle the propagated errors during image-to-LiDAR representation learning?\n\n- **Q2:** As mentioned in Weakness 2, could the authors provide more details on the hyperparameter settings for the vMF distribution and the reasoning behind their chosen values? Adding a more detailed explanation and theoretical justification would be even better.\n\n- **Q3:** As mentioned in Weakness 3, having more thorough experimental analyses on other LiDAR-based point cloud datasets, such as SemanticPOSS, Waymo, SynLiDAR, etc., could further consolidate the findings and conclusions drawn in the manuscript.\n\n- **Q4:** As most 2D and 3D representation learning approaches (MoCo, SimCLR, Seal, etc.) do, having empirical analyses of models under out-of-distribution datasets is recommended. \n\n- **[Minor]:** The computational cost of the proposed multi-modal contrastive distillation approach is not thoroughly analyzed, which is crucial for real-time applications in autonomous driving.\n\n- **[Minor]:** The generalizability of OLIVINE to other types of sensors (for example, hybrid-solid LiDARs) or environments (for example, off-board environments) beyond the evaluated datasets is not discussed.\n\n- **[Minor]:** “NuScenes” should be revised to “nuScenes”.", "labeling_timestamp": "2026-01-11T16:22:24.624655", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "Authors report additional experiments on multiple LiDAR datasets and an out-of-distribution dataset (addressing some aspects of generalization across environments/data distributions) but do not discuss or present results for different sensor types, so the reviewer’s full concern is only partially addressed.", "evidence": "\"Following your suggestions, we have added experiments on six additional LiDAR-based point cloud datasets and one out-of-distribution dataset. And our proposed OLIVINE consistently outperforms the superpixels-based methods on all datasets.\"", "combined_author_response": "Dear Reviewers,\n\nWe sincerely thank you for your thoughtful evaluations during the rebuttal stage. We are pleased that our detailed responses have addressed most of your concerns. We appreciate your recognition of the additional experiments and the clarification we have made to the paper.\n\nWe are grateful for the time you have invested in reviewing our work and for your consistent recognition of its value. Based on your valuable feedback, we will carefully revise the manuscript, incorporating the additional experiments and analyses into the final version.\n\nBest regards,\n\nThe Authors\n\n---\n\nDear Reviewer 54Gn,\n\nThank you for your response and for taking the time to carefully review our rebuttal. We greatly appreciate your recognition of our efforts to address your concerns and the value you found in the additional experiments we conducted. Your detailed and thoughtful review demonstrates a profound expertise in this domain. I have thoroughly enjoyed the opportunity to learn from your perspective.\n\nPlease feel free to share any further comments or suggestions.\n\nWarm regards,\n\nAuthors\n\n---\n\nDear Reviewer uKMo,\n\nThanks again for the time and energy you committed and your valuable comments. Your meticulous review and thoughtful critiques truly reflect your deep domain expertise and diligence as a reviewer. It has been a pleasure communicating and exchanging ideas with you.\n\nPlease feel free to share any additional comments or feedback on the manuscript.\n\nWarm regards,\n\nAuthors\n\n---\n\nDear Reviewer 2WGA,\n\nThank you for taking the time to review our rebuttal and for your constructive feedback throughout the process. We are glad that we could address most of your concerns. \n\nWe will actively participate in the Author-Reviewer discussion session. Please feel free to share any additional comments or feedback on the manuscript.\n\nWarm regards,\n\nAuthors\n\n---\n\nThank you for the positive feedback provided and the time devoted to this review. We are glad that our efforts have addressed your concerns. Next, we will address your remaining concerns.\n\n---\n\n**Comments**: *The authors may over-claim the contribution of using \"semantic superpixels\" over the \"class-agnostic superpixels\"*.\n\n**Response:**  We believe there may be a **misunderstanding** regarding our proposed methods. We would like to clarify the following points to address your concerns:\n\n- We have **not** claimed that using **semantic superpixels** is our contribution. In fact, our method does not rely on **superpixels** at all, which is different from previous methods [R1-R4].\n- Previous methods [R1-R4] use superpixels to pool 3D point features and 2D pixel features, learning with a superpixel-to-superpoint contrastive loss. In contrast, our method directly uses the features of **individual** points and pixels for contrastive distillation.\n- The semantic **labels** can be flexibly utilized in multiple aspects of the proposed method, such as weakly-supervised contrastive distillation, semantic-guided consistency regularization, and category-aware anchor point sampling. These aspects cannot be effectively addressed using only the class-agnostic superpixels.\n\n---\n**Comments**: *The use of semantic categories seems not to cause a major improvement over class-agnostic masks.*\n\n**Response:** Extensive experiments demonstrate that this approach **substantially** outperforms superpixels-based (class-agnostic mask-based) methods [R1-R4] in various downstream tasks.\n- Our method achieves a **significant** improvement over superpixels-based pretraining methods on nuScenes and SemanticKITTI datasets. As shown in the table below, our method outperforms Seal [R1] by a significant margin, achieving an improvement of 5.14\\% under the setting of linear probing. The full results are available in Table M1 of the uploaded PDF file.\n- Following your suggestions, we have added experiments on six additional LiDAR-based point cloud datasets and one out-of-distribution dataset. And our proposed OLIVINE **consistently** outperforms the superpixels-based methods on all datasets. For the full results, please refer to Tables M2 and M3 of the uploaded PDF file.\n\n| Method    | LP     | 1%     | 5%     | 10%    | 25%    | 100%   |\n| :-------- | :----- | :----- | :----- | :----- | :----- | :----- |\n| Random    | 8\\.10   | 30\\.30 | 47\\.84 | 56\\.15 | 65\\.48 | 74\\.66 |\n| PPKT      | 35\\.90  | 37\\.80 | 53\\.74 | 60\\.25 | 67\\.14 | 74\\.52 |\n| SLidR     | 38\\.80  | 38\\.30 | 52\\.49 | 59\\.84 | 66\\.91 | 74\\.79 |\n| ST-SLidR  | 40\\.48 | 40\\.75 | 54\\.69 | 60\\.75 | 67\\.70 | 75\\.14 |\n| HVDistill | 39\\.50  | 42\\.70 | 56\\.60 | 62\\.90 | 69\\.30 | 76\\.60 |\n| Seal      | 44\\.95 | 45\\.84 | 55\\.64 | 62\\.97 | 68\\.41 | 75\\.60 |\n| Ours      | **50\\.09** | **50\\.60** | **60\\.25** | **65\\.07** | **70\\.15** | **76\\.69** |\n---\nThanks again for your diligence as a reviewer. It is with great pleasure communicating with you. Please feel free to share any additional comments or feedback on the manuscript.\n\n**References**:\\\n[R1] Image-to-lidar self-supervised distillation for autonomous driving data.\\\n[R2] Self-supervised image-to-point distillation via semantically tolerant contrastive loss.\\\n[R3] Segment Any Point Cloud Sequences by Distilling Vision Foundation Models.\\\n[R4] HVDistill: Transferring Knowledge from Images to Point Clouds via Unsupervised Hybrid-View Distillation.\n\n---\n\n**Proposition 2**: The representation of samples in the same class can vary significantly across different batches during contrastive distillation, and semantic-guided consistency regularization helps to learn structured features.\n\n**Justification**: Without regularization, the representation of samples within the same class can vary significantly across different batches during contrastive distillation. This variance arises due to random sampling and the influence of negative samples in different batches. The weakly-supervised contrastive loss is defined as:\n\n$\\mathcal{L}_{\\mathrm{sup}} = - \\frac{1}{M_s} \\sum _{i=1}^{M_s} \\log \\left[ \\frac{1}{|A(i)|} \\sum _{a\\in A(i)} \\frac{\\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_a \\rangle/\\tau)}}{\\sum _{j=1}^{M_s} \\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_j \\rangle /\\tau)}}\\right]$\n\nThe features of negative samples $\\mathbf{G}^{\\mathrm{2D}}_j$ vary across batches, leading to different optimization paths for each mini-batch. This introduces variability in the learned representations $\\mathbf{G}^{\\mathrm{3D}}_i$ for samples of the same class $k$.\n\nWhen we do not use semantic-guided consistency regularization, the within-class variance for class $k$ across different batches is:\n\n$\\sigma_W^2 = \\frac{1}{|B|} \\sum_{B} \\frac{1}{M_k} \\sum_{i=1}^{M_k^B} \\|g_i^k - \\mu_k^B\\|^2$\n\nFor ease of reading, we use $g_i$ to refer to point feature $\\mathbf{G}^{\\mathrm{3D}}_i$.\nAnd $\\mu_k^B$ is the mean feature vector for class $k$ in batch $B$. Due to the batch-wise variability in negative samples, $\\mu_k^B$ can differ significantly across batches, leading to high within-class variance. \n\n\nBy minimizing the KL divergence, we align feature vectors $g_i$ of class $k$ with the mean direction $\\mu_k$, reducing the spread of feature vectors within the same class. The within-class variance with regularization is:\n\n$\\sigma_W^2 = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\|g_i^k - \\mu_k\\|^2$\n\nSince $\\mu_k$ is consistent across batches due to the regularization, the within-class variance is significantly reduced. This results in structured feature representations, enhancing class separability and improving performance in downstream tasks.\n\n---\n**Proposition 3**: Learning structural representation during pretraining can benefit downstream tasks.\n\n**Justification**: Structured features are those well-aligned within the same class (low within-class variance $\\sigma_W^2$) and well-separated between different classes (high between-class variance $\\sigma_B^2$). \n\nWith semantic-guided consistency regularization, feature vectors $g_i^k$ for class $k$ are closely aligned with the mean direction $\\mu_k$. This alignment reduces the within-class variance $\\sigma_W^2$. Weakly-supervised contrastive learning pushes apart feature vectors of different classes, increasing the separation between class means $\\mu_k$. This increases the between-class variance $\\sigma_B^2$.\n\nTake the linear classifier as an example, the decision boundary is determined by the separation between class means. Higher $\\sigma_B^2$ and lower $\\sigma_W^2$ result in clearer decision boundaries, reducing classification errors.\n\nConsider a simple linear classifier with weight vector $w$ and bias $b$. The decision function is:\n\n$f(x) = w^T x + b$\n\nThe decision boundary is given by:\n\n$w^T x + b = 0$\n\nFor well-structured features, the margin (distance between decision boundary and nearest samples) is maximized. The margin $ \\gamma $ for class $ k $ can be expressed as:\n\n$\\gamma = \\frac{w^T (\\mu_k - \\mu)}{\\|w\\|}$\n\nHigher between-class variance ($\\sigma_B^2$) and lower within-class variance ($\\sigma_W^2$) increase this margin, leading to better classification performance.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser.\n\n---\n\n**Proposition 1**: The features of each class $k$ can be modeled as a von Mises-Fisher (vMF) distribution. This means that for class $k$, the feature vectors $g_i$ lie on a unit hypersphere and are centered around a mean direction $\\mu_k$ with a concentration parameter $\\kappa_k$.\n\n**Justification**: To show that the features of each class can be effectively modeled by a vMF distribution, we use maximum likelihood estimation (MLE) to determine that the parameters $\\mu_k$ and $\\kappa_k$ are optimal for the given set of feature vectors.\n\nFor a set of $M_k$ feature vectors $\\\\{g_i\\\\}_{i=1}^{M_k}$ from class $k$, the likelihood function for the vMF distribution is:\n\n$L(\\mu_k, \\kappa_k) = \\prod_{i=1}^{M_k} f(g_i; \\mu_k, \\kappa_k) = \\prod_{i=1}^{M_k} \\mathcal{K}_{C}(\\kappa_k) \\exp(\\kappa_k \\mu_k^T g_i)$\n\nTaking the natural logarithm of the likelihood function, we get the log-likelihood:\n\n$\\log L(\\mu_k, \\kappa_k) = \\sum_{i=1}^{M_k} \\log f(g_i; \\mu_k, \\kappa_k) = M_k \\log \\mathcal{K}_{C}(\\kappa_k) + \\kappa_k \\sum _{i=1}^{M_k} \\mu_k^T g_i$\n\nSubstituting the expression for $\\mathcal{K}_{C}(\\kappa_k)$, we get:\n\n$\\log L(\\mu_k, \\kappa_k) = M_k \\left[ \\log \\left( \\frac{\\kappa_k^{C/2-1}}{(2\\pi)^{C/2} I_{C/2-1}(\\kappa_k)} \\right) + \\frac{\\kappa_k}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\n$\\log L(\\mu_k, \\kappa_k) = M_k \\left[ (C/2-1) \\log \\kappa_k - \\log I_{C/2-1}(\\kappa_k) - \\frac{C}{2} \\log(2\\pi) + \\frac{\\kappa_k}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\nTo maximize the log-likelihood, we normalize $\\mu_k$ by setting it to the normalized sum of the feature vectors:\n$\\mu_k = \\frac{\\sum_{i=1}^{M_k} g_i}{\\|\\sum_{i=1}^{M_k} g_i\\|}$\n\nThe derivative of the log-likelihood with respect to $\\kappa_k$ is:\n\n$\\frac{\\partial \\log L(\\mu_k, \\kappa_k)}{\\partial \\kappa_k} = M_k \\left[ \\frac{C/2-1}{\\kappa_k} - \\frac{I_{C/2}(\\kappa_k)}{I_{C/2-1}(\\kappa_k)} + \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\nSetting this derivative to zero, we get:\n\n$\\frac{C/2-1}{\\kappa_k} - \\frac{I_{C/2}(\\kappa_k)}{I_{C/2-1}(\\kappa_k)} + \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i = 0$\n\nSolving for $\\kappa_k$, we obtain:\n\n$\\kappa_k = \\frac{\\|\\sum_{i=1}^{M_k} g_i\\| (C - \\|\\sum_{i=1}^{M_k} g_i\\|^2)}{1 - \\|\\sum_{i=1}^{M_k} g_i\\|^2}$\n\nThis equation allows us to compute the concentration parameter $\\kappa_k$ based on the alignment of the feature vectors. The concentration parameter $\\kappa_k$ is larger when the distribution is more tightly clustered around the mean direction, and smaller when the features are more uniformly spread across the hypersphere.\n\nBy maximizing the likelihood function for the vMF distribution, we have shown that the parameters $\\mu_k$ and $\\kappa_k$ can be estimated to model the distribution of feature vectors for each class. \nThe mean direction $\\mu_k$ denotes the central direction of the feature cluster, and the concentration parameter $\\kappa_k$ controls the tightness of this clustering. Moreover, the way we estimate the parameters of vMF distribution in EMA is also consistent with the results of the above theoretical derivation.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser.", "author_response": "**Proposition 2**: The representation of samples in the same class can vary significantly across different batches during contrastive distillation, and semantic-guided consistency regularization helps to learn structured features.\n\n**Justification**: Without regularization, the representation of samples within the same class can vary significantly across different batches during contrastive distillation. This variance arises due to random sampling and the influence of negative samples in different batches. The weakly-supervised contrastive loss is defined as:\n\n$\\mathcal{L}_{\\mathrm{sup}} = - \\frac{1}{M_s} \\sum _{i=1}^{M_s} \\log \\left[ \\frac{1}{|A(i)|} \\sum _{a\\in A(i)} \\frac{\\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_a \\rangle/\\tau)}}{\\sum _{j=1}^{M_s} \\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_j \\rangle /\\tau)}}\\right]$\n\nThe features of negative samples $\\mathbf{G}^{\\mathrm{2D}}_j$ vary across batches, leading to different optimization paths for each mini-batch. This introduces variability in the learned representations $\\mathbf{G}^{\\mathrm{3D}}_i$ for samples of the same class $k$.\n\nWhen we do not use semantic-guided consistency regularization, the within-class variance for class $k$ across different batches is:\n\n$\\sigma_W^2 = \\frac{1}{|B|} \\sum_{B} \\frac{1}{M_k} \\sum_{i=1}^{M_k^B} \\|g_i^k - \\mu_k^B\\|^2$\n\nFor ease of reading, we use $g_i$ to refer to point feature $\\mathbf{G}^{\\mathrm{3D}}_i$.\nAnd $\\mu_k^B$ is the mean feature vector for class $k$ in batch $B$. Due to the batch-wise variability in negative samples, $\\mu_k^B$ can differ significantly across batches, leading to high within-class variance. \n\n\nBy minimizing the KL divergence, we align feature vectors $g_i$ of class $k$ with the mean direction $\\mu_k$, reducing the spread of feature vectors within the same class. The within-class variance with regularization is:\n\n$\\sigma_W^2 = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\|g_i^k - \\mu_k\\|^2$\n\nSince $\\mu_k$ is consistent across batches due to the regularization, the within-class variance is significantly reduced. This results in structured feature representations, enhancing class separability and improving performance in downstream tasks.\n\n---\n**Proposition 3**: Learning structural representation during pretraining can benefit downstream tasks.\n\n**Justification**: Structured features are those well-aligned within the same class (low within-class variance $\\sigma_W^2$) and well-separated between different classes (high between-class variance $\\sigma_B^2$). \n\nWith semantic-guided consistency regularization, feature vectors $g_i^k$ for class $k$ are closely aligned with the mean direction $\\mu_k$. This alignment reduces the within-class variance $\\sigma_W^2$. Weakly-supervised contrastive learning pushes apart feature vectors of different classes, increasing the separation between class means $\\mu_k$. This increases the between-class variance $\\sigma_B^2$.\n\nTake the linear classifier as an example, the decision boundary is determined by the separation between class means. Higher $\\sigma_B^2$ and lower $\\sigma_W^2$ result in clearer decision boundaries, reducing classification errors.\n\nConsider a simple linear classifier with weight vector $w$ and bias $b$. The decision function is:\n\n$f(x) = w^T x + b$\n\nThe decision boundary is given by:\n\n$w^T x + b = 0$\n\nFor well-structured features, the margin (distance between decision boundary and nearest samples) is maximized. The margin $ \\gamma $ for class $ k $ can be expressed as:\n\n$\\gamma = \\frac{w^T (\\mu_k - \\mu)}{\\|w\\|}$\n\nHigher between-class variance ($\\sigma_B^2$) and lower within-class variance ($\\sigma_W^2$) increase this margin, leading to better classification performance.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser."}
{"claim": "The manuscript uses the capitalized dataset name 'NuScenes' instead of the correct 'nuScenes' formatting.", "claim_type": "other", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "P8fCEoWdti", "reviewer": "Reviewer_svRz", "review_text": "Summary: This work aims to tackle the image-to-LiDAR contrastive learning problem for LiDAR-based point cloud segmentation. Previous approaches designed the cross-modal contrastive learning objective for model pretraining, using superpixels and superpoints as guidance.\n\nIn this work, the authors observe that the superpixel-driven contrastive loss tends to involve ‘’self-conflict’’ issues during representation learning. A weakly-supervised contrastive distillation method is proposed, which generates semantic superpixels/superpoints using the Segment Anything Model (SAM). Additionally, to balance the imbalanced class distributions of LiDAR scene categories during representation, a density and category-aware sampling strategy is proposed to adjust the sampling probabilities of different anchor points using the weak semantic labels.\n\nThe overall framework is named OLIVINE, which adopts three optimization objectives:\n- Weakly-supervised contrastive distillation using coarse semantic labels to identify positive pairs by category.\n- Self-supervised contrastive distillation applied to randomly sampled point-pixel pairs.\n- A regularization framework based on the von Mises-Fisher (vMF) distribution to ensure semantic consistency.\n\nThe proposed OLIVINE method is evaluated on the nuScenes, SemanticKITTI, and KITTI object detection datasets. The results exhibit a consistent improvement of the proposed method compared to existing approaches.\n\nStrengths: (+) This work aims to improve the image-to-LiDAR self-supervised representation learning problem on LiDAR-based point cloud datasets, which is one of the current research hotspots, especially for applications related to autonomous driving and robotics.\n\n(+) The proposed method has exhibited promising performance on mainstream benchmarks, including nuScenes linear probing, nuScenes fine-tuning, SemanticKITTI fine-tuning, and KITTI object detection.\n\nWeaknesses: (-) The weakly-supervised contrastive distillation method has been used in previous literature, such as [R1] and [R2]. Adding semantic categories seems not to cause a major improvement over class-agnostic masks, as the Segment Anything Model is able to segment rather complete and semantically consistent objects and backgrounds. Additionally, using weak labels (which might be erroneous) could introduce additional errors during pretraining.\n\n(-) The motivation for using the von Mises-Fisher (vMF) distribution to enforce consistency regularization for image-to-LiDAR representation learning is not clear enough to demonstrate its superiority. A more detailed explanation and theoretical justification would strengthen this aspect of the work.\n\n(-) Compared to some of the most related works, for example, [R1] and [R3], the scale and depth regarding the experiments (for example, downstream fine-tuning on other datasets than SemanticKITTI) could be further enhanced.\n\n---\n\n### References:\n- [R1] Youquan Liu, et al. “Segment Any Point Cloud Sequences by Distilling Vision Foundation Models,” NeurIPS, 2023.\n- [R2] Ayça Takmaz, et al. “OpenMask3D: Open-Vocabulary 3D Instance Segmentation,” NeurIPS, 2023.\n- [R3] Gilles Puy, et al. “Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving,” arXiv, 2023.\n\nQuestions: - **Q1:** As mentioned in Weakness 1, the semantic masks generated by the Segment Anything Model could inevitably involve errors (e.g., wrong segmentation results). How do the authors handle the propagated errors during image-to-LiDAR representation learning?\n\n- **Q2:** As mentioned in Weakness 2, could the authors provide more details on the hyperparameter settings for the vMF distribution and the reasoning behind their chosen values? Adding a more detailed explanation and theoretical justification would be even better.\n\n- **Q3:** As mentioned in Weakness 3, having more thorough experimental analyses on other LiDAR-based point cloud datasets, such as SemanticPOSS, Waymo, SynLiDAR, etc., could further consolidate the findings and conclusions drawn in the manuscript.\n\n- **Q4:** As most 2D and 3D representation learning approaches (MoCo, SimCLR, Seal, etc.) do, having empirical analyses of models under out-of-distribution datasets is recommended. \n\n- **[Minor]:** The computational cost of the proposed multi-modal contrastive distillation approach is not thoroughly analyzed, which is crucial for real-time applications in autonomous driving.\n\n- **[Minor]:** The generalizability of OLIVINE to other types of sensors (for example, hybrid-solid LiDARs) or environments (for example, off-board environments) beyond the evaluated datasets is not discussed.\n\n- **[Minor]:** “NuScenes” should be revised to “nuScenes”.", "labeling_timestamp": "2026-01-11T16:22:36.208390", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The authors do not explicitly acknowledge or address the specific formatting complaint about 'NuScenes' vs 'nuScenes'. They state a general intent to revise the manuscript but do not confirm correcting the dataset name or discuss it.", "evidence": "\"We will carefully revise the manuscript, incorporating the additional experiments and analyses into the final version.\"", "combined_author_response": "Dear Reviewers,\n\nWe sincerely thank you for your thoughtful evaluations during the rebuttal stage. We are pleased that our detailed responses have addressed most of your concerns. We appreciate your recognition of the additional experiments and the clarification we have made to the paper.\n\nWe are grateful for the time you have invested in reviewing our work and for your consistent recognition of its value. Based on your valuable feedback, we will carefully revise the manuscript, incorporating the additional experiments and analyses into the final version.\n\nBest regards,\n\nThe Authors\n\n---\n\nDear Reviewer 54Gn,\n\nThank you for your response and for taking the time to carefully review our rebuttal. We greatly appreciate your recognition of our efforts to address your concerns and the value you found in the additional experiments we conducted. Your detailed and thoughtful review demonstrates a profound expertise in this domain. I have thoroughly enjoyed the opportunity to learn from your perspective.\n\nPlease feel free to share any further comments or suggestions.\n\nWarm regards,\n\nAuthors\n\n---\n\nDear Reviewer uKMo,\n\nThanks again for the time and energy you committed and your valuable comments. Your meticulous review and thoughtful critiques truly reflect your deep domain expertise and diligence as a reviewer. It has been a pleasure communicating and exchanging ideas with you.\n\nPlease feel free to share any additional comments or feedback on the manuscript.\n\nWarm regards,\n\nAuthors\n\n---\n\nDear Reviewer 2WGA,\n\nThank you for taking the time to review our rebuttal and for your constructive feedback throughout the process. We are glad that we could address most of your concerns. \n\nWe will actively participate in the Author-Reviewer discussion session. Please feel free to share any additional comments or feedback on the manuscript.\n\nWarm regards,\n\nAuthors\n\n---\n\nThank you for the positive feedback provided and the time devoted to this review. We are glad that our efforts have addressed your concerns. Next, we will address your remaining concerns.\n\n---\n\n**Comments**: *The authors may over-claim the contribution of using \"semantic superpixels\" over the \"class-agnostic superpixels\"*.\n\n**Response:**  We believe there may be a **misunderstanding** regarding our proposed methods. We would like to clarify the following points to address your concerns:\n\n- We have **not** claimed that using **semantic superpixels** is our contribution. In fact, our method does not rely on **superpixels** at all, which is different from previous methods [R1-R4].\n- Previous methods [R1-R4] use superpixels to pool 3D point features and 2D pixel features, learning with a superpixel-to-superpoint contrastive loss. In contrast, our method directly uses the features of **individual** points and pixels for contrastive distillation.\n- The semantic **labels** can be flexibly utilized in multiple aspects of the proposed method, such as weakly-supervised contrastive distillation, semantic-guided consistency regularization, and category-aware anchor point sampling. These aspects cannot be effectively addressed using only the class-agnostic superpixels.\n\n---\n**Comments**: *The use of semantic categories seems not to cause a major improvement over class-agnostic masks.*\n\n**Response:** Extensive experiments demonstrate that this approach **substantially** outperforms superpixels-based (class-agnostic mask-based) methods [R1-R4] in various downstream tasks.\n- Our method achieves a **significant** improvement over superpixels-based pretraining methods on nuScenes and SemanticKITTI datasets. As shown in the table below, our method outperforms Seal [R1] by a significant margin, achieving an improvement of 5.14\\% under the setting of linear probing. The full results are available in Table M1 of the uploaded PDF file.\n- Following your suggestions, we have added experiments on six additional LiDAR-based point cloud datasets and one out-of-distribution dataset. And our proposed OLIVINE **consistently** outperforms the superpixels-based methods on all datasets. For the full results, please refer to Tables M2 and M3 of the uploaded PDF file.\n\n| Method    | LP     | 1%     | 5%     | 10%    | 25%    | 100%   |\n| :-------- | :----- | :----- | :----- | :----- | :----- | :----- |\n| Random    | 8\\.10   | 30\\.30 | 47\\.84 | 56\\.15 | 65\\.48 | 74\\.66 |\n| PPKT      | 35\\.90  | 37\\.80 | 53\\.74 | 60\\.25 | 67\\.14 | 74\\.52 |\n| SLidR     | 38\\.80  | 38\\.30 | 52\\.49 | 59\\.84 | 66\\.91 | 74\\.79 |\n| ST-SLidR  | 40\\.48 | 40\\.75 | 54\\.69 | 60\\.75 | 67\\.70 | 75\\.14 |\n| HVDistill | 39\\.50  | 42\\.70 | 56\\.60 | 62\\.90 | 69\\.30 | 76\\.60 |\n| Seal      | 44\\.95 | 45\\.84 | 55\\.64 | 62\\.97 | 68\\.41 | 75\\.60 |\n| Ours      | **50\\.09** | **50\\.60** | **60\\.25** | **65\\.07** | **70\\.15** | **76\\.69** |\n---\nThanks again for your diligence as a reviewer. It is with great pleasure communicating with you. Please feel free to share any additional comments or feedback on the manuscript.\n\n**References**:\\\n[R1] Image-to-lidar self-supervised distillation for autonomous driving data.\\\n[R2] Self-supervised image-to-point distillation via semantically tolerant contrastive loss.\\\n[R3] Segment Any Point Cloud Sequences by Distilling Vision Foundation Models.\\\n[R4] HVDistill: Transferring Knowledge from Images to Point Clouds via Unsupervised Hybrid-View Distillation.\n\n---\n\n**Proposition 2**: The representation of samples in the same class can vary significantly across different batches during contrastive distillation, and semantic-guided consistency regularization helps to learn structured features.\n\n**Justification**: Without regularization, the representation of samples within the same class can vary significantly across different batches during contrastive distillation. This variance arises due to random sampling and the influence of negative samples in different batches. The weakly-supervised contrastive loss is defined as:\n\n$\\mathcal{L}_{\\mathrm{sup}} = - \\frac{1}{M_s} \\sum _{i=1}^{M_s} \\log \\left[ \\frac{1}{|A(i)|} \\sum _{a\\in A(i)} \\frac{\\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_a \\rangle/\\tau)}}{\\sum _{j=1}^{M_s} \\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_j \\rangle /\\tau)}}\\right]$\n\nThe features of negative samples $\\mathbf{G}^{\\mathrm{2D}}_j$ vary across batches, leading to different optimization paths for each mini-batch. This introduces variability in the learned representations $\\mathbf{G}^{\\mathrm{3D}}_i$ for samples of the same class $k$.\n\nWhen we do not use semantic-guided consistency regularization, the within-class variance for class $k$ across different batches is:\n\n$\\sigma_W^2 = \\frac{1}{|B|} \\sum_{B} \\frac{1}{M_k} \\sum_{i=1}^{M_k^B} \\|g_i^k - \\mu_k^B\\|^2$\n\nFor ease of reading, we use $g_i$ to refer to point feature $\\mathbf{G}^{\\mathrm{3D}}_i$.\nAnd $\\mu_k^B$ is the mean feature vector for class $k$ in batch $B$. Due to the batch-wise variability in negative samples, $\\mu_k^B$ can differ significantly across batches, leading to high within-class variance. \n\n\nBy minimizing the KL divergence, we align feature vectors $g_i$ of class $k$ with the mean direction $\\mu_k$, reducing the spread of feature vectors within the same class. The within-class variance with regularization is:\n\n$\\sigma_W^2 = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\|g_i^k - \\mu_k\\|^2$\n\nSince $\\mu_k$ is consistent across batches due to the regularization, the within-class variance is significantly reduced. This results in structured feature representations, enhancing class separability and improving performance in downstream tasks.\n\n---\n**Proposition 3**: Learning structural representation during pretraining can benefit downstream tasks.\n\n**Justification**: Structured features are those well-aligned within the same class (low within-class variance $\\sigma_W^2$) and well-separated between different classes (high between-class variance $\\sigma_B^2$). \n\nWith semantic-guided consistency regularization, feature vectors $g_i^k$ for class $k$ are closely aligned with the mean direction $\\mu_k$. This alignment reduces the within-class variance $\\sigma_W^2$. Weakly-supervised contrastive learning pushes apart feature vectors of different classes, increasing the separation between class means $\\mu_k$. This increases the between-class variance $\\sigma_B^2$.\n\nTake the linear classifier as an example, the decision boundary is determined by the separation between class means. Higher $\\sigma_B^2$ and lower $\\sigma_W^2$ result in clearer decision boundaries, reducing classification errors.\n\nConsider a simple linear classifier with weight vector $w$ and bias $b$. The decision function is:\n\n$f(x) = w^T x + b$\n\nThe decision boundary is given by:\n\n$w^T x + b = 0$\n\nFor well-structured features, the margin (distance between decision boundary and nearest samples) is maximized. The margin $ \\gamma $ for class $ k $ can be expressed as:\n\n$\\gamma = \\frac{w^T (\\mu_k - \\mu)}{\\|w\\|}$\n\nHigher between-class variance ($\\sigma_B^2$) and lower within-class variance ($\\sigma_W^2$) increase this margin, leading to better classification performance.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser.\n\n---\n\n**Proposition 1**: The features of each class $k$ can be modeled as a von Mises-Fisher (vMF) distribution. This means that for class $k$, the feature vectors $g_i$ lie on a unit hypersphere and are centered around a mean direction $\\mu_k$ with a concentration parameter $\\kappa_k$.\n\n**Justification**: To show that the features of each class can be effectively modeled by a vMF distribution, we use maximum likelihood estimation (MLE) to determine that the parameters $\\mu_k$ and $\\kappa_k$ are optimal for the given set of feature vectors.\n\nFor a set of $M_k$ feature vectors $\\\\{g_i\\\\}_{i=1}^{M_k}$ from class $k$, the likelihood function for the vMF distribution is:\n\n$L(\\mu_k, \\kappa_k) = \\prod_{i=1}^{M_k} f(g_i; \\mu_k, \\kappa_k) = \\prod_{i=1}^{M_k} \\mathcal{K}_{C}(\\kappa_k) \\exp(\\kappa_k \\mu_k^T g_i)$\n\nTaking the natural logarithm of the likelihood function, we get the log-likelihood:\n\n$\\log L(\\mu_k, \\kappa_k) = \\sum_{i=1}^{M_k} \\log f(g_i; \\mu_k, \\kappa_k) = M_k \\log \\mathcal{K}_{C}(\\kappa_k) + \\kappa_k \\sum _{i=1}^{M_k} \\mu_k^T g_i$\n\nSubstituting the expression for $\\mathcal{K}_{C}(\\kappa_k)$, we get:\n\n$\\log L(\\mu_k, \\kappa_k) = M_k \\left[ \\log \\left( \\frac{\\kappa_k^{C/2-1}}{(2\\pi)^{C/2} I_{C/2-1}(\\kappa_k)} \\right) + \\frac{\\kappa_k}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\n$\\log L(\\mu_k, \\kappa_k) = M_k \\left[ (C/2-1) \\log \\kappa_k - \\log I_{C/2-1}(\\kappa_k) - \\frac{C}{2} \\log(2\\pi) + \\frac{\\kappa_k}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\nTo maximize the log-likelihood, we normalize $\\mu_k$ by setting it to the normalized sum of the feature vectors:\n$\\mu_k = \\frac{\\sum_{i=1}^{M_k} g_i}{\\|\\sum_{i=1}^{M_k} g_i\\|}$\n\nThe derivative of the log-likelihood with respect to $\\kappa_k$ is:\n\n$\\frac{\\partial \\log L(\\mu_k, \\kappa_k)}{\\partial \\kappa_k} = M_k \\left[ \\frac{C/2-1}{\\kappa_k} - \\frac{I_{C/2}(\\kappa_k)}{I_{C/2-1}(\\kappa_k)} + \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\nSetting this derivative to zero, we get:\n\n$\\frac{C/2-1}{\\kappa_k} - \\frac{I_{C/2}(\\kappa_k)}{I_{C/2-1}(\\kappa_k)} + \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i = 0$\n\nSolving for $\\kappa_k$, we obtain:\n\n$\\kappa_k = \\frac{\\|\\sum_{i=1}^{M_k} g_i\\| (C - \\|\\sum_{i=1}^{M_k} g_i\\|^2)}{1 - \\|\\sum_{i=1}^{M_k} g_i\\|^2}$\n\nThis equation allows us to compute the concentration parameter $\\kappa_k$ based on the alignment of the feature vectors. The concentration parameter $\\kappa_k$ is larger when the distribution is more tightly clustered around the mean direction, and smaller when the features are more uniformly spread across the hypersphere.\n\nBy maximizing the likelihood function for the vMF distribution, we have shown that the parameters $\\mu_k$ and $\\kappa_k$ can be estimated to model the distribution of feature vectors for each class. \nThe mean direction $\\mu_k$ denotes the central direction of the feature cluster, and the concentration parameter $\\kappa_k$ controls the tightness of this clustering. Moreover, the way we estimate the parameters of vMF distribution in EMA is also consistent with the results of the above theoretical derivation.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser.", "author_response": "**Proposition 2**: The representation of samples in the same class can vary significantly across different batches during contrastive distillation, and semantic-guided consistency regularization helps to learn structured features.\n\n**Justification**: Without regularization, the representation of samples within the same class can vary significantly across different batches during contrastive distillation. This variance arises due to random sampling and the influence of negative samples in different batches. The weakly-supervised contrastive loss is defined as:\n\n$\\mathcal{L}_{\\mathrm{sup}} = - \\frac{1}{M_s} \\sum _{i=1}^{M_s} \\log \\left[ \\frac{1}{|A(i)|} \\sum _{a\\in A(i)} \\frac{\\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_a \\rangle/\\tau)}}{\\sum _{j=1}^{M_s} \\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_j \\rangle /\\tau)}}\\right]$\n\nThe features of negative samples $\\mathbf{G}^{\\mathrm{2D}}_j$ vary across batches, leading to different optimization paths for each mini-batch. This introduces variability in the learned representations $\\mathbf{G}^{\\mathrm{3D}}_i$ for samples of the same class $k$.\n\nWhen we do not use semantic-guided consistency regularization, the within-class variance for class $k$ across different batches is:\n\n$\\sigma_W^2 = \\frac{1}{|B|} \\sum_{B} \\frac{1}{M_k} \\sum_{i=1}^{M_k^B} \\|g_i^k - \\mu_k^B\\|^2$\n\nFor ease of reading, we use $g_i$ to refer to point feature $\\mathbf{G}^{\\mathrm{3D}}_i$.\nAnd $\\mu_k^B$ is the mean feature vector for class $k$ in batch $B$. Due to the batch-wise variability in negative samples, $\\mu_k^B$ can differ significantly across batches, leading to high within-class variance. \n\n\nBy minimizing the KL divergence, we align feature vectors $g_i$ of class $k$ with the mean direction $\\mu_k$, reducing the spread of feature vectors within the same class. The within-class variance with regularization is:\n\n$\\sigma_W^2 = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\|g_i^k - \\mu_k\\|^2$\n\nSince $\\mu_k$ is consistent across batches due to the regularization, the within-class variance is significantly reduced. This results in structured feature representations, enhancing class separability and improving performance in downstream tasks.\n\n---\n**Proposition 3**: Learning structural representation during pretraining can benefit downstream tasks.\n\n**Justification**: Structured features are those well-aligned within the same class (low within-class variance $\\sigma_W^2$) and well-separated between different classes (high between-class variance $\\sigma_B^2$). \n\nWith semantic-guided consistency regularization, feature vectors $g_i^k$ for class $k$ are closely aligned with the mean direction $\\mu_k$. This alignment reduces the within-class variance $\\sigma_W^2$. Weakly-supervised contrastive learning pushes apart feature vectors of different classes, increasing the separation between class means $\\mu_k$. This increases the between-class variance $\\sigma_B^2$.\n\nTake the linear classifier as an example, the decision boundary is determined by the separation between class means. Higher $\\sigma_B^2$ and lower $\\sigma_W^2$ result in clearer decision boundaries, reducing classification errors.\n\nConsider a simple linear classifier with weight vector $w$ and bias $b$. The decision function is:\n\n$f(x) = w^T x + b$\n\nThe decision boundary is given by:\n\n$w^T x + b = 0$\n\nFor well-structured features, the margin (distance between decision boundary and nearest samples) is maximized. The margin $ \\gamma $ for class $ k $ can be expressed as:\n\n$\\gamma = \\frac{w^T (\\mu_k - \\mu)}{\\|w\\|}$\n\nHigher between-class variance ($\\sigma_B^2$) and lower within-class variance ($\\sigma_W^2$) increase this margin, leading to better classification performance.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser."}
{"claim": "The paper does not demonstrate that vMF-based regularization yields superior performance compared to alternative regularizers.", "claim_type": "baseline", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "P8fCEoWdti", "reviewer": "Reviewer_svRz", "review_text": "Summary: This work aims to tackle the image-to-LiDAR contrastive learning problem for LiDAR-based point cloud segmentation. Previous approaches designed the cross-modal contrastive learning objective for model pretraining, using superpixels and superpoints as guidance.\n\nIn this work, the authors observe that the superpixel-driven contrastive loss tends to involve ‘’self-conflict’’ issues during representation learning. A weakly-supervised contrastive distillation method is proposed, which generates semantic superpixels/superpoints using the Segment Anything Model (SAM). Additionally, to balance the imbalanced class distributions of LiDAR scene categories during representation, a density and category-aware sampling strategy is proposed to adjust the sampling probabilities of different anchor points using the weak semantic labels.\n\nThe overall framework is named OLIVINE, which adopts three optimization objectives:\n- Weakly-supervised contrastive distillation using coarse semantic labels to identify positive pairs by category.\n- Self-supervised contrastive distillation applied to randomly sampled point-pixel pairs.\n- A regularization framework based on the von Mises-Fisher (vMF) distribution to ensure semantic consistency.\n\nThe proposed OLIVINE method is evaluated on the nuScenes, SemanticKITTI, and KITTI object detection datasets. The results exhibit a consistent improvement of the proposed method compared to existing approaches.\n\nStrengths: (+) This work aims to improve the image-to-LiDAR self-supervised representation learning problem on LiDAR-based point cloud datasets, which is one of the current research hotspots, especially for applications related to autonomous driving and robotics.\n\n(+) The proposed method has exhibited promising performance on mainstream benchmarks, including nuScenes linear probing, nuScenes fine-tuning, SemanticKITTI fine-tuning, and KITTI object detection.\n\nWeaknesses: (-) The weakly-supervised contrastive distillation method has been used in previous literature, such as [R1] and [R2]. Adding semantic categories seems not to cause a major improvement over class-agnostic masks, as the Segment Anything Model is able to segment rather complete and semantically consistent objects and backgrounds. Additionally, using weak labels (which might be erroneous) could introduce additional errors during pretraining.\n\n(-) The motivation for using the von Mises-Fisher (vMF) distribution to enforce consistency regularization for image-to-LiDAR representation learning is not clear enough to demonstrate its superiority. A more detailed explanation and theoretical justification would strengthen this aspect of the work.\n\n(-) Compared to some of the most related works, for example, [R1] and [R3], the scale and depth regarding the experiments (for example, downstream fine-tuning on other datasets than SemanticKITTI) could be further enhanced.\n\n---\n\n### References:\n- [R1] Youquan Liu, et al. “Segment Any Point Cloud Sequences by Distilling Vision Foundation Models,” NeurIPS, 2023.\n- [R2] Ayça Takmaz, et al. “OpenMask3D: Open-Vocabulary 3D Instance Segmentation,” NeurIPS, 2023.\n- [R3] Gilles Puy, et al. “Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving,” arXiv, 2023.\n\nQuestions: - **Q1:** As mentioned in Weakness 1, the semantic masks generated by the Segment Anything Model could inevitably involve errors (e.g., wrong segmentation results). How do the authors handle the propagated errors during image-to-LiDAR representation learning?\n\n- **Q2:** As mentioned in Weakness 2, could the authors provide more details on the hyperparameter settings for the vMF distribution and the reasoning behind their chosen values? Adding a more detailed explanation and theoretical justification would be even better.\n\n- **Q3:** As mentioned in Weakness 3, having more thorough experimental analyses on other LiDAR-based point cloud datasets, such as SemanticPOSS, Waymo, SynLiDAR, etc., could further consolidate the findings and conclusions drawn in the manuscript.\n\n- **Q4:** As most 2D and 3D representation learning approaches (MoCo, SimCLR, Seal, etc.) do, having empirical analyses of models under out-of-distribution datasets is recommended. \n\n- **[Minor]:** The computational cost of the proposed multi-modal contrastive distillation approach is not thoroughly analyzed, which is crucial for real-time applications in autonomous driving.\n\n- **[Minor]:** The generalizability of OLIVINE to other types of sensors (for example, hybrid-solid LiDARs) or environments (for example, off-board environments) beyond the evaluated datasets is not discussed.\n\n- **[Minor]:** “NuScenes” should be revised to “nuScenes”.", "labeling_timestamp": "2026-01-11T16:22:36.767558", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The authors provide a theoretical justification for modeling features with a vMF distribution (Proposition 1) and report empirical gains over superpixel-based baselines, but they do not present any empirical comparison or ablation that directly contrasts the vMF-based regularizer with alternative regularizers. Thus the reviewer's specific claim is not addressed.", "evidence": "\"Proposition 1: The features of each class $k$ can be modeled as a von Mises-Fisher (vMF) distribution...\" \n\n\"Extensive experiments demonstrate that this approach substantially outperforms superpixels-based (class-agnostic mask-based) methods [R1-R4] in various downstream tasks.\"", "combined_author_response": "Dear Reviewers,\n\nWe sincerely thank you for your thoughtful evaluations during the rebuttal stage. We are pleased that our detailed responses have addressed most of your concerns. We appreciate your recognition of the additional experiments and the clarification we have made to the paper.\n\nWe are grateful for the time you have invested in reviewing our work and for your consistent recognition of its value. Based on your valuable feedback, we will carefully revise the manuscript, incorporating the additional experiments and analyses into the final version.\n\nBest regards,\n\nThe Authors\n\n---\n\nDear Reviewer 54Gn,\n\nThank you for your response and for taking the time to carefully review our rebuttal. We greatly appreciate your recognition of our efforts to address your concerns and the value you found in the additional experiments we conducted. Your detailed and thoughtful review demonstrates a profound expertise in this domain. I have thoroughly enjoyed the opportunity to learn from your perspective.\n\nPlease feel free to share any further comments or suggestions.\n\nWarm regards,\n\nAuthors\n\n---\n\nDear Reviewer uKMo,\n\nThanks again for the time and energy you committed and your valuable comments. Your meticulous review and thoughtful critiques truly reflect your deep domain expertise and diligence as a reviewer. It has been a pleasure communicating and exchanging ideas with you.\n\nPlease feel free to share any additional comments or feedback on the manuscript.\n\nWarm regards,\n\nAuthors\n\n---\n\nDear Reviewer 2WGA,\n\nThank you for taking the time to review our rebuttal and for your constructive feedback throughout the process. We are glad that we could address most of your concerns. \n\nWe will actively participate in the Author-Reviewer discussion session. Please feel free to share any additional comments or feedback on the manuscript.\n\nWarm regards,\n\nAuthors\n\n---\n\nThank you for the positive feedback provided and the time devoted to this review. We are glad that our efforts have addressed your concerns. Next, we will address your remaining concerns.\n\n---\n\n**Comments**: *The authors may over-claim the contribution of using \"semantic superpixels\" over the \"class-agnostic superpixels\"*.\n\n**Response:**  We believe there may be a **misunderstanding** regarding our proposed methods. We would like to clarify the following points to address your concerns:\n\n- We have **not** claimed that using **semantic superpixels** is our contribution. In fact, our method does not rely on **superpixels** at all, which is different from previous methods [R1-R4].\n- Previous methods [R1-R4] use superpixels to pool 3D point features and 2D pixel features, learning with a superpixel-to-superpoint contrastive loss. In contrast, our method directly uses the features of **individual** points and pixels for contrastive distillation.\n- The semantic **labels** can be flexibly utilized in multiple aspects of the proposed method, such as weakly-supervised contrastive distillation, semantic-guided consistency regularization, and category-aware anchor point sampling. These aspects cannot be effectively addressed using only the class-agnostic superpixels.\n\n---\n**Comments**: *The use of semantic categories seems not to cause a major improvement over class-agnostic masks.*\n\n**Response:** Extensive experiments demonstrate that this approach **substantially** outperforms superpixels-based (class-agnostic mask-based) methods [R1-R4] in various downstream tasks.\n- Our method achieves a **significant** improvement over superpixels-based pretraining methods on nuScenes and SemanticKITTI datasets. As shown in the table below, our method outperforms Seal [R1] by a significant margin, achieving an improvement of 5.14\\% under the setting of linear probing. The full results are available in Table M1 of the uploaded PDF file.\n- Following your suggestions, we have added experiments on six additional LiDAR-based point cloud datasets and one out-of-distribution dataset. And our proposed OLIVINE **consistently** outperforms the superpixels-based methods on all datasets. For the full results, please refer to Tables M2 and M3 of the uploaded PDF file.\n\n| Method    | LP     | 1%     | 5%     | 10%    | 25%    | 100%   |\n| :-------- | :----- | :----- | :----- | :----- | :----- | :----- |\n| Random    | 8\\.10   | 30\\.30 | 47\\.84 | 56\\.15 | 65\\.48 | 74\\.66 |\n| PPKT      | 35\\.90  | 37\\.80 | 53\\.74 | 60\\.25 | 67\\.14 | 74\\.52 |\n| SLidR     | 38\\.80  | 38\\.30 | 52\\.49 | 59\\.84 | 66\\.91 | 74\\.79 |\n| ST-SLidR  | 40\\.48 | 40\\.75 | 54\\.69 | 60\\.75 | 67\\.70 | 75\\.14 |\n| HVDistill | 39\\.50  | 42\\.70 | 56\\.60 | 62\\.90 | 69\\.30 | 76\\.60 |\n| Seal      | 44\\.95 | 45\\.84 | 55\\.64 | 62\\.97 | 68\\.41 | 75\\.60 |\n| Ours      | **50\\.09** | **50\\.60** | **60\\.25** | **65\\.07** | **70\\.15** | **76\\.69** |\n---\nThanks again for your diligence as a reviewer. It is with great pleasure communicating with you. Please feel free to share any additional comments or feedback on the manuscript.\n\n**References**:\\\n[R1] Image-to-lidar self-supervised distillation for autonomous driving data.\\\n[R2] Self-supervised image-to-point distillation via semantically tolerant contrastive loss.\\\n[R3] Segment Any Point Cloud Sequences by Distilling Vision Foundation Models.\\\n[R4] HVDistill: Transferring Knowledge from Images to Point Clouds via Unsupervised Hybrid-View Distillation.\n\n---\n\n**Proposition 2**: The representation of samples in the same class can vary significantly across different batches during contrastive distillation, and semantic-guided consistency regularization helps to learn structured features.\n\n**Justification**: Without regularization, the representation of samples within the same class can vary significantly across different batches during contrastive distillation. This variance arises due to random sampling and the influence of negative samples in different batches. The weakly-supervised contrastive loss is defined as:\n\n$\\mathcal{L}_{\\mathrm{sup}} = - \\frac{1}{M_s} \\sum _{i=1}^{M_s} \\log \\left[ \\frac{1}{|A(i)|} \\sum _{a\\in A(i)} \\frac{\\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_a \\rangle/\\tau)}}{\\sum _{j=1}^{M_s} \\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_j \\rangle /\\tau)}}\\right]$\n\nThe features of negative samples $\\mathbf{G}^{\\mathrm{2D}}_j$ vary across batches, leading to different optimization paths for each mini-batch. This introduces variability in the learned representations $\\mathbf{G}^{\\mathrm{3D}}_i$ for samples of the same class $k$.\n\nWhen we do not use semantic-guided consistency regularization, the within-class variance for class $k$ across different batches is:\n\n$\\sigma_W^2 = \\frac{1}{|B|} \\sum_{B} \\frac{1}{M_k} \\sum_{i=1}^{M_k^B} \\|g_i^k - \\mu_k^B\\|^2$\n\nFor ease of reading, we use $g_i$ to refer to point feature $\\mathbf{G}^{\\mathrm{3D}}_i$.\nAnd $\\mu_k^B$ is the mean feature vector for class $k$ in batch $B$. Due to the batch-wise variability in negative samples, $\\mu_k^B$ can differ significantly across batches, leading to high within-class variance. \n\n\nBy minimizing the KL divergence, we align feature vectors $g_i$ of class $k$ with the mean direction $\\mu_k$, reducing the spread of feature vectors within the same class. The within-class variance with regularization is:\n\n$\\sigma_W^2 = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\|g_i^k - \\mu_k\\|^2$\n\nSince $\\mu_k$ is consistent across batches due to the regularization, the within-class variance is significantly reduced. This results in structured feature representations, enhancing class separability and improving performance in downstream tasks.\n\n---\n**Proposition 3**: Learning structural representation during pretraining can benefit downstream tasks.\n\n**Justification**: Structured features are those well-aligned within the same class (low within-class variance $\\sigma_W^2$) and well-separated between different classes (high between-class variance $\\sigma_B^2$). \n\nWith semantic-guided consistency regularization, feature vectors $g_i^k$ for class $k$ are closely aligned with the mean direction $\\mu_k$. This alignment reduces the within-class variance $\\sigma_W^2$. Weakly-supervised contrastive learning pushes apart feature vectors of different classes, increasing the separation between class means $\\mu_k$. This increases the between-class variance $\\sigma_B^2$.\n\nTake the linear classifier as an example, the decision boundary is determined by the separation between class means. Higher $\\sigma_B^2$ and lower $\\sigma_W^2$ result in clearer decision boundaries, reducing classification errors.\n\nConsider a simple linear classifier with weight vector $w$ and bias $b$. The decision function is:\n\n$f(x) = w^T x + b$\n\nThe decision boundary is given by:\n\n$w^T x + b = 0$\n\nFor well-structured features, the margin (distance between decision boundary and nearest samples) is maximized. The margin $ \\gamma $ for class $ k $ can be expressed as:\n\n$\\gamma = \\frac{w^T (\\mu_k - \\mu)}{\\|w\\|}$\n\nHigher between-class variance ($\\sigma_B^2$) and lower within-class variance ($\\sigma_W^2$) increase this margin, leading to better classification performance.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser.\n\n---\n\n**Proposition 1**: The features of each class $k$ can be modeled as a von Mises-Fisher (vMF) distribution. This means that for class $k$, the feature vectors $g_i$ lie on a unit hypersphere and are centered around a mean direction $\\mu_k$ with a concentration parameter $\\kappa_k$.\n\n**Justification**: To show that the features of each class can be effectively modeled by a vMF distribution, we use maximum likelihood estimation (MLE) to determine that the parameters $\\mu_k$ and $\\kappa_k$ are optimal for the given set of feature vectors.\n\nFor a set of $M_k$ feature vectors $\\\\{g_i\\\\}_{i=1}^{M_k}$ from class $k$, the likelihood function for the vMF distribution is:\n\n$L(\\mu_k, \\kappa_k) = \\prod_{i=1}^{M_k} f(g_i; \\mu_k, \\kappa_k) = \\prod_{i=1}^{M_k} \\mathcal{K}_{C}(\\kappa_k) \\exp(\\kappa_k \\mu_k^T g_i)$\n\nTaking the natural logarithm of the likelihood function, we get the log-likelihood:\n\n$\\log L(\\mu_k, \\kappa_k) = \\sum_{i=1}^{M_k} \\log f(g_i; \\mu_k, \\kappa_k) = M_k \\log \\mathcal{K}_{C}(\\kappa_k) + \\kappa_k \\sum _{i=1}^{M_k} \\mu_k^T g_i$\n\nSubstituting the expression for $\\mathcal{K}_{C}(\\kappa_k)$, we get:\n\n$\\log L(\\mu_k, \\kappa_k) = M_k \\left[ \\log \\left( \\frac{\\kappa_k^{C/2-1}}{(2\\pi)^{C/2} I_{C/2-1}(\\kappa_k)} \\right) + \\frac{\\kappa_k}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\n$\\log L(\\mu_k, \\kappa_k) = M_k \\left[ (C/2-1) \\log \\kappa_k - \\log I_{C/2-1}(\\kappa_k) - \\frac{C}{2} \\log(2\\pi) + \\frac{\\kappa_k}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\nTo maximize the log-likelihood, we normalize $\\mu_k$ by setting it to the normalized sum of the feature vectors:\n$\\mu_k = \\frac{\\sum_{i=1}^{M_k} g_i}{\\|\\sum_{i=1}^{M_k} g_i\\|}$\n\nThe derivative of the log-likelihood with respect to $\\kappa_k$ is:\n\n$\\frac{\\partial \\log L(\\mu_k, \\kappa_k)}{\\partial \\kappa_k} = M_k \\left[ \\frac{C/2-1}{\\kappa_k} - \\frac{I_{C/2}(\\kappa_k)}{I_{C/2-1}(\\kappa_k)} + \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\nSetting this derivative to zero, we get:\n\n$\\frac{C/2-1}{\\kappa_k} - \\frac{I_{C/2}(\\kappa_k)}{I_{C/2-1}(\\kappa_k)} + \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i = 0$\n\nSolving for $\\kappa_k$, we obtain:\n\n$\\kappa_k = \\frac{\\|\\sum_{i=1}^{M_k} g_i\\| (C - \\|\\sum_{i=1}^{M_k} g_i\\|^2)}{1 - \\|\\sum_{i=1}^{M_k} g_i\\|^2}$\n\nThis equation allows us to compute the concentration parameter $\\kappa_k$ based on the alignment of the feature vectors. The concentration parameter $\\kappa_k$ is larger when the distribution is more tightly clustered around the mean direction, and smaller when the features are more uniformly spread across the hypersphere.\n\nBy maximizing the likelihood function for the vMF distribution, we have shown that the parameters $\\mu_k$ and $\\kappa_k$ can be estimated to model the distribution of feature vectors for each class. \nThe mean direction $\\mu_k$ denotes the central direction of the feature cluster, and the concentration parameter $\\kappa_k$ controls the tightness of this clustering. Moreover, the way we estimate the parameters of vMF distribution in EMA is also consistent with the results of the above theoretical derivation.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser.", "author_response": "**Proposition 2**: The representation of samples in the same class can vary significantly across different batches during contrastive distillation, and semantic-guided consistency regularization helps to learn structured features.\n\n**Justification**: Without regularization, the representation of samples within the same class can vary significantly across different batches during contrastive distillation. This variance arises due to random sampling and the influence of negative samples in different batches. The weakly-supervised contrastive loss is defined as:\n\n$\\mathcal{L}_{\\mathrm{sup}} = - \\frac{1}{M_s} \\sum _{i=1}^{M_s} \\log \\left[ \\frac{1}{|A(i)|} \\sum _{a\\in A(i)} \\frac{\\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_a \\rangle/\\tau)}}{\\sum _{j=1}^{M_s} \\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_j \\rangle /\\tau)}}\\right]$\n\nThe features of negative samples $\\mathbf{G}^{\\mathrm{2D}}_j$ vary across batches, leading to different optimization paths for each mini-batch. This introduces variability in the learned representations $\\mathbf{G}^{\\mathrm{3D}}_i$ for samples of the same class $k$.\n\nWhen we do not use semantic-guided consistency regularization, the within-class variance for class $k$ across different batches is:\n\n$\\sigma_W^2 = \\frac{1}{|B|} \\sum_{B} \\frac{1}{M_k} \\sum_{i=1}^{M_k^B} \\|g_i^k - \\mu_k^B\\|^2$\n\nFor ease of reading, we use $g_i$ to refer to point feature $\\mathbf{G}^{\\mathrm{3D}}_i$.\nAnd $\\mu_k^B$ is the mean feature vector for class $k$ in batch $B$. Due to the batch-wise variability in negative samples, $\\mu_k^B$ can differ significantly across batches, leading to high within-class variance. \n\n\nBy minimizing the KL divergence, we align feature vectors $g_i$ of class $k$ with the mean direction $\\mu_k$, reducing the spread of feature vectors within the same class. The within-class variance with regularization is:\n\n$\\sigma_W^2 = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\|g_i^k - \\mu_k\\|^2$\n\nSince $\\mu_k$ is consistent across batches due to the regularization, the within-class variance is significantly reduced. This results in structured feature representations, enhancing class separability and improving performance in downstream tasks.\n\n---\n**Proposition 3**: Learning structural representation during pretraining can benefit downstream tasks.\n\n**Justification**: Structured features are those well-aligned within the same class (low within-class variance $\\sigma_W^2$) and well-separated between different classes (high between-class variance $\\sigma_B^2$). \n\nWith semantic-guided consistency regularization, feature vectors $g_i^k$ for class $k$ are closely aligned with the mean direction $\\mu_k$. This alignment reduces the within-class variance $\\sigma_W^2$. Weakly-supervised contrastive learning pushes apart feature vectors of different classes, increasing the separation between class means $\\mu_k$. This increases the between-class variance $\\sigma_B^2$.\n\nTake the linear classifier as an example, the decision boundary is determined by the separation between class means. Higher $\\sigma_B^2$ and lower $\\sigma_W^2$ result in clearer decision boundaries, reducing classification errors.\n\nConsider a simple linear classifier with weight vector $w$ and bias $b$. The decision function is:\n\n$f(x) = w^T x + b$\n\nThe decision boundary is given by:\n\n$w^T x + b = 0$\n\nFor well-structured features, the margin (distance between decision boundary and nearest samples) is maximized. The margin $ \\gamma $ for class $ k $ can be expressed as:\n\n$\\gamma = \\frac{w^T (\\mu_k - \\mu)}{\\|w\\|}$\n\nHigher between-class variance ($\\sigma_B^2$) and lower within-class variance ($\\sigma_W^2$) increase this margin, leading to better classification performance.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser."}
{"claim": "The paper does not sufficiently compare OLIVINE against closely related methods like R1, R2, and R3.", "claim_type": "baseline", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "P8fCEoWdti", "reviewer": "Reviewer_svRz", "review_text": "Summary: This work aims to tackle the image-to-LiDAR contrastive learning problem for LiDAR-based point cloud segmentation. Previous approaches designed the cross-modal contrastive learning objective for model pretraining, using superpixels and superpoints as guidance.\n\nIn this work, the authors observe that the superpixel-driven contrastive loss tends to involve ‘’self-conflict’’ issues during representation learning. A weakly-supervised contrastive distillation method is proposed, which generates semantic superpixels/superpoints using the Segment Anything Model (SAM). Additionally, to balance the imbalanced class distributions of LiDAR scene categories during representation, a density and category-aware sampling strategy is proposed to adjust the sampling probabilities of different anchor points using the weak semantic labels.\n\nThe overall framework is named OLIVINE, which adopts three optimization objectives:\n- Weakly-supervised contrastive distillation using coarse semantic labels to identify positive pairs by category.\n- Self-supervised contrastive distillation applied to randomly sampled point-pixel pairs.\n- A regularization framework based on the von Mises-Fisher (vMF) distribution to ensure semantic consistency.\n\nThe proposed OLIVINE method is evaluated on the nuScenes, SemanticKITTI, and KITTI object detection datasets. The results exhibit a consistent improvement of the proposed method compared to existing approaches.\n\nStrengths: (+) This work aims to improve the image-to-LiDAR self-supervised representation learning problem on LiDAR-based point cloud datasets, which is one of the current research hotspots, especially for applications related to autonomous driving and robotics.\n\n(+) The proposed method has exhibited promising performance on mainstream benchmarks, including nuScenes linear probing, nuScenes fine-tuning, SemanticKITTI fine-tuning, and KITTI object detection.\n\nWeaknesses: (-) The weakly-supervised contrastive distillation method has been used in previous literature, such as [R1] and [R2]. Adding semantic categories seems not to cause a major improvement over class-agnostic masks, as the Segment Anything Model is able to segment rather complete and semantically consistent objects and backgrounds. Additionally, using weak labels (which might be erroneous) could introduce additional errors during pretraining.\n\n(-) The motivation for using the von Mises-Fisher (vMF) distribution to enforce consistency regularization for image-to-LiDAR representation learning is not clear enough to demonstrate its superiority. A more detailed explanation and theoretical justification would strengthen this aspect of the work.\n\n(-) Compared to some of the most related works, for example, [R1] and [R3], the scale and depth regarding the experiments (for example, downstream fine-tuning on other datasets than SemanticKITTI) could be further enhanced.\n\n---\n\n### References:\n- [R1] Youquan Liu, et al. “Segment Any Point Cloud Sequences by Distilling Vision Foundation Models,” NeurIPS, 2023.\n- [R2] Ayça Takmaz, et al. “OpenMask3D: Open-Vocabulary 3D Instance Segmentation,” NeurIPS, 2023.\n- [R3] Gilles Puy, et al. “Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving,” arXiv, 2023.\n\nQuestions: - **Q1:** As mentioned in Weakness 1, the semantic masks generated by the Segment Anything Model could inevitably involve errors (e.g., wrong segmentation results). How do the authors handle the propagated errors during image-to-LiDAR representation learning?\n\n- **Q2:** As mentioned in Weakness 2, could the authors provide more details on the hyperparameter settings for the vMF distribution and the reasoning behind their chosen values? Adding a more detailed explanation and theoretical justification would be even better.\n\n- **Q3:** As mentioned in Weakness 3, having more thorough experimental analyses on other LiDAR-based point cloud datasets, such as SemanticPOSS, Waymo, SynLiDAR, etc., could further consolidate the findings and conclusions drawn in the manuscript.\n\n- **Q4:** As most 2D and 3D representation learning approaches (MoCo, SimCLR, Seal, etc.) do, having empirical analyses of models under out-of-distribution datasets is recommended. \n\n- **[Minor]:** The computational cost of the proposed multi-modal contrastive distillation approach is not thoroughly analyzed, which is crucial for real-time applications in autonomous driving.\n\n- **[Minor]:** The generalizability of OLIVINE to other types of sensors (for example, hybrid-solid LiDARs) or environments (for example, off-board environments) beyond the evaluated datasets is not discussed.\n\n- **[Minor]:** “NuScenes” should be revised to “nuScenes”.", "labeling_timestamp": "2026-01-11T16:22:43.397932", "model": "gpt-5-mini", "label": "Not Determined", "justification": "Error during labeling: Unterminated string starting at: line 1 column 287 (char 286)", "evidence": "", "combined_author_response": "Dear Reviewers,\n\nWe sincerely thank you for your thoughtful evaluations during the rebuttal stage. We are pleased that our detailed responses have addressed most of your concerns. We appreciate your recognition of the additional experiments and the clarification we have made to the paper.\n\nWe are grateful for the time you have invested in reviewing our work and for your consistent recognition of its value. Based on your valuable feedback, we will carefully revise the manuscript, incorporating the additional experiments and analyses into the final version.\n\nBest regards,\n\nThe Authors\n\n---\n\nDear Reviewer 54Gn,\n\nThank you for your response and for taking the time to carefully review our rebuttal. We greatly appreciate your recognition of our efforts to address your concerns and the value you found in the additional experiments we conducted. Your detailed and thoughtful review demonstrates a profound expertise in this domain. I have thoroughly enjoyed the opportunity to learn from your perspective.\n\nPlease feel free to share any further comments or suggestions.\n\nWarm regards,\n\nAuthors\n\n---\n\nDear Reviewer uKMo,\n\nThanks again for the time and energy you committed and your valuable comments. Your meticulous review and thoughtful critiques truly reflect your deep domain expertise and diligence as a reviewer. It has been a pleasure communicating and exchanging ideas with you.\n\nPlease feel free to share any additional comments or feedback on the manuscript.\n\nWarm regards,\n\nAuthors\n\n---\n\nDear Reviewer 2WGA,\n\nThank you for taking the time to review our rebuttal and for your constructive feedback throughout the process. We are glad that we could address most of your concerns. \n\nWe will actively participate in the Author-Reviewer discussion session. Please feel free to share any additional comments or feedback on the manuscript.\n\nWarm regards,\n\nAuthors\n\n---\n\nThank you for the positive feedback provided and the time devoted to this review. We are glad that our efforts have addressed your concerns. Next, we will address your remaining concerns.\n\n---\n\n**Comments**: *The authors may over-claim the contribution of using \"semantic superpixels\" over the \"class-agnostic superpixels\"*.\n\n**Response:**  We believe there may be a **misunderstanding** regarding our proposed methods. We would like to clarify the following points to address your concerns:\n\n- We have **not** claimed that using **semantic superpixels** is our contribution. In fact, our method does not rely on **superpixels** at all, which is different from previous methods [R1-R4].\n- Previous methods [R1-R4] use superpixels to pool 3D point features and 2D pixel features, learning with a superpixel-to-superpoint contrastive loss. In contrast, our method directly uses the features of **individual** points and pixels for contrastive distillation.\n- The semantic **labels** can be flexibly utilized in multiple aspects of the proposed method, such as weakly-supervised contrastive distillation, semantic-guided consistency regularization, and category-aware anchor point sampling. These aspects cannot be effectively addressed using only the class-agnostic superpixels.\n\n---\n**Comments**: *The use of semantic categories seems not to cause a major improvement over class-agnostic masks.*\n\n**Response:** Extensive experiments demonstrate that this approach **substantially** outperforms superpixels-based (class-agnostic mask-based) methods [R1-R4] in various downstream tasks.\n- Our method achieves a **significant** improvement over superpixels-based pretraining methods on nuScenes and SemanticKITTI datasets. As shown in the table below, our method outperforms Seal [R1] by a significant margin, achieving an improvement of 5.14\\% under the setting of linear probing. The full results are available in Table M1 of the uploaded PDF file.\n- Following your suggestions, we have added experiments on six additional LiDAR-based point cloud datasets and one out-of-distribution dataset. And our proposed OLIVINE **consistently** outperforms the superpixels-based methods on all datasets. For the full results, please refer to Tables M2 and M3 of the uploaded PDF file.\n\n| Method    | LP     | 1%     | 5%     | 10%    | 25%    | 100%   |\n| :-------- | :----- | :----- | :----- | :----- | :----- | :----- |\n| Random    | 8\\.10   | 30\\.30 | 47\\.84 | 56\\.15 | 65\\.48 | 74\\.66 |\n| PPKT      | 35\\.90  | 37\\.80 | 53\\.74 | 60\\.25 | 67\\.14 | 74\\.52 |\n| SLidR     | 38\\.80  | 38\\.30 | 52\\.49 | 59\\.84 | 66\\.91 | 74\\.79 |\n| ST-SLidR  | 40\\.48 | 40\\.75 | 54\\.69 | 60\\.75 | 67\\.70 | 75\\.14 |\n| HVDistill | 39\\.50  | 42\\.70 | 56\\.60 | 62\\.90 | 69\\.30 | 76\\.60 |\n| Seal      | 44\\.95 | 45\\.84 | 55\\.64 | 62\\.97 | 68\\.41 | 75\\.60 |\n| Ours      | **50\\.09** | **50\\.60** | **60\\.25** | **65\\.07** | **70\\.15** | **76\\.69** |\n---\nThanks again for your diligence as a reviewer. It is with great pleasure communicating with you. Please feel free to share any additional comments or feedback on the manuscript.\n\n**References**:\\\n[R1] Image-to-lidar self-supervised distillation for autonomous driving data.\\\n[R2] Self-supervised image-to-point distillation via semantically tolerant contrastive loss.\\\n[R3] Segment Any Point Cloud Sequences by Distilling Vision Foundation Models.\\\n[R4] HVDistill: Transferring Knowledge from Images to Point Clouds via Unsupervised Hybrid-View Distillation.\n\n---\n\n**Proposition 2**: The representation of samples in the same class can vary significantly across different batches during contrastive distillation, and semantic-guided consistency regularization helps to learn structured features.\n\n**Justification**: Without regularization, the representation of samples within the same class can vary significantly across different batches during contrastive distillation. This variance arises due to random sampling and the influence of negative samples in different batches. The weakly-supervised contrastive loss is defined as:\n\n$\\mathcal{L}_{\\mathrm{sup}} = - \\frac{1}{M_s} \\sum _{i=1}^{M_s} \\log \\left[ \\frac{1}{|A(i)|} \\sum _{a\\in A(i)} \\frac{\\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_a \\rangle/\\tau)}}{\\sum _{j=1}^{M_s} \\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_j \\rangle /\\tau)}}\\right]$\n\nThe features of negative samples $\\mathbf{G}^{\\mathrm{2D}}_j$ vary across batches, leading to different optimization paths for each mini-batch. This introduces variability in the learned representations $\\mathbf{G}^{\\mathrm{3D}}_i$ for samples of the same class $k$.\n\nWhen we do not use semantic-guided consistency regularization, the within-class variance for class $k$ across different batches is:\n\n$\\sigma_W^2 = \\frac{1}{|B|} \\sum_{B} \\frac{1}{M_k} \\sum_{i=1}^{M_k^B} \\|g_i^k - \\mu_k^B\\|^2$\n\nFor ease of reading, we use $g_i$ to refer to point feature $\\mathbf{G}^{\\mathrm{3D}}_i$.\nAnd $\\mu_k^B$ is the mean feature vector for class $k$ in batch $B$. Due to the batch-wise variability in negative samples, $\\mu_k^B$ can differ significantly across batches, leading to high within-class variance. \n\n\nBy minimizing the KL divergence, we align feature vectors $g_i$ of class $k$ with the mean direction $\\mu_k$, reducing the spread of feature vectors within the same class. The within-class variance with regularization is:\n\n$\\sigma_W^2 = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\|g_i^k - \\mu_k\\|^2$\n\nSince $\\mu_k$ is consistent across batches due to the regularization, the within-class variance is significantly reduced. This results in structured feature representations, enhancing class separability and improving performance in downstream tasks.\n\n---\n**Proposition 3**: Learning structural representation during pretraining can benefit downstream tasks.\n\n**Justification**: Structured features are those well-aligned within the same class (low within-class variance $\\sigma_W^2$) and well-separated between different classes (high between-class variance $\\sigma_B^2$). \n\nWith semantic-guided consistency regularization, feature vectors $g_i^k$ for class $k$ are closely aligned with the mean direction $\\mu_k$. This alignment reduces the within-class variance $\\sigma_W^2$. Weakly-supervised contrastive learning pushes apart feature vectors of different classes, increasing the separation between class means $\\mu_k$. This increases the between-class variance $\\sigma_B^2$.\n\nTake the linear classifier as an example, the decision boundary is determined by the separation between class means. Higher $\\sigma_B^2$ and lower $\\sigma_W^2$ result in clearer decision boundaries, reducing classification errors.\n\nConsider a simple linear classifier with weight vector $w$ and bias $b$. The decision function is:\n\n$f(x) = w^T x + b$\n\nThe decision boundary is given by:\n\n$w^T x + b = 0$\n\nFor well-structured features, the margin (distance between decision boundary and nearest samples) is maximized. The margin $ \\gamma $ for class $ k $ can be expressed as:\n\n$\\gamma = \\frac{w^T (\\mu_k - \\mu)}{\\|w\\|}$\n\nHigher between-class variance ($\\sigma_B^2$) and lower within-class variance ($\\sigma_W^2$) increase this margin, leading to better classification performance.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser.\n\n---\n\n**Proposition 1**: The features of each class $k$ can be modeled as a von Mises-Fisher (vMF) distribution. This means that for class $k$, the feature vectors $g_i$ lie on a unit hypersphere and are centered around a mean direction $\\mu_k$ with a concentration parameter $\\kappa_k$.\n\n**Justification**: To show that the features of each class can be effectively modeled by a vMF distribution, we use maximum likelihood estimation (MLE) to determine that the parameters $\\mu_k$ and $\\kappa_k$ are optimal for the given set of feature vectors.\n\nFor a set of $M_k$ feature vectors $\\\\{g_i\\\\}_{i=1}^{M_k}$ from class $k$, the likelihood function for the vMF distribution is:\n\n$L(\\mu_k, \\kappa_k) = \\prod_{i=1}^{M_k} f(g_i; \\mu_k, \\kappa_k) = \\prod_{i=1}^{M_k} \\mathcal{K}_{C}(\\kappa_k) \\exp(\\kappa_k \\mu_k^T g_i)$\n\nTaking the natural logarithm of the likelihood function, we get the log-likelihood:\n\n$\\log L(\\mu_k, \\kappa_k) = \\sum_{i=1}^{M_k} \\log f(g_i; \\mu_k, \\kappa_k) = M_k \\log \\mathcal{K}_{C}(\\kappa_k) + \\kappa_k \\sum _{i=1}^{M_k} \\mu_k^T g_i$\n\nSubstituting the expression for $\\mathcal{K}_{C}(\\kappa_k)$, we get:\n\n$\\log L(\\mu_k, \\kappa_k) = M_k \\left[ \\log \\left( \\frac{\\kappa_k^{C/2-1}}{(2\\pi)^{C/2} I_{C/2-1}(\\kappa_k)} \\right) + \\frac{\\kappa_k}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\n$\\log L(\\mu_k, \\kappa_k) = M_k \\left[ (C/2-1) \\log \\kappa_k - \\log I_{C/2-1}(\\kappa_k) - \\frac{C}{2} \\log(2\\pi) + \\frac{\\kappa_k}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\nTo maximize the log-likelihood, we normalize $\\mu_k$ by setting it to the normalized sum of the feature vectors:\n$\\mu_k = \\frac{\\sum_{i=1}^{M_k} g_i}{\\|\\sum_{i=1}^{M_k} g_i\\|}$\n\nThe derivative of the log-likelihood with respect to $\\kappa_k$ is:\n\n$\\frac{\\partial \\log L(\\mu_k, \\kappa_k)}{\\partial \\kappa_k} = M_k \\left[ \\frac{C/2-1}{\\kappa_k} - \\frac{I_{C/2}(\\kappa_k)}{I_{C/2-1}(\\kappa_k)} + \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i \\right]$\n\nSetting this derivative to zero, we get:\n\n$\\frac{C/2-1}{\\kappa_k} - \\frac{I_{C/2}(\\kappa_k)}{I_{C/2-1}(\\kappa_k)} + \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\mu_k^T g_i = 0$\n\nSolving for $\\kappa_k$, we obtain:\n\n$\\kappa_k = \\frac{\\|\\sum_{i=1}^{M_k} g_i\\| (C - \\|\\sum_{i=1}^{M_k} g_i\\|^2)}{1 - \\|\\sum_{i=1}^{M_k} g_i\\|^2}$\n\nThis equation allows us to compute the concentration parameter $\\kappa_k$ based on the alignment of the feature vectors. The concentration parameter $\\kappa_k$ is larger when the distribution is more tightly clustered around the mean direction, and smaller when the features are more uniformly spread across the hypersphere.\n\nBy maximizing the likelihood function for the vMF distribution, we have shown that the parameters $\\mu_k$ and $\\kappa_k$ can be estimated to model the distribution of feature vectors for each class. \nThe mean direction $\\mu_k$ denotes the central direction of the feature cluster, and the concentration parameter $\\kappa_k$ controls the tightness of this clustering. Moreover, the way we estimate the parameters of vMF distribution in EMA is also consistent with the results of the above theoretical derivation.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser.", "author_response": "**Proposition 2**: The representation of samples in the same class can vary significantly across different batches during contrastive distillation, and semantic-guided consistency regularization helps to learn structured features.\n\n**Justification**: Without regularization, the representation of samples within the same class can vary significantly across different batches during contrastive distillation. This variance arises due to random sampling and the influence of negative samples in different batches. The weakly-supervised contrastive loss is defined as:\n\n$\\mathcal{L}_{\\mathrm{sup}} = - \\frac{1}{M_s} \\sum _{i=1}^{M_s} \\log \\left[ \\frac{1}{|A(i)|} \\sum _{a\\in A(i)} \\frac{\\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_a \\rangle/\\tau)}}{\\sum _{j=1}^{M_s} \\mathrm{exp}{(\\langle\\mathbf{G}^{\\mathrm{3D}}_i,\\mathbf{G}^{\\mathrm{2D}}_j \\rangle /\\tau)}}\\right]$\n\nThe features of negative samples $\\mathbf{G}^{\\mathrm{2D}}_j$ vary across batches, leading to different optimization paths for each mini-batch. This introduces variability in the learned representations $\\mathbf{G}^{\\mathrm{3D}}_i$ for samples of the same class $k$.\n\nWhen we do not use semantic-guided consistency regularization, the within-class variance for class $k$ across different batches is:\n\n$\\sigma_W^2 = \\frac{1}{|B|} \\sum_{B} \\frac{1}{M_k} \\sum_{i=1}^{M_k^B} \\|g_i^k - \\mu_k^B\\|^2$\n\nFor ease of reading, we use $g_i$ to refer to point feature $\\mathbf{G}^{\\mathrm{3D}}_i$.\nAnd $\\mu_k^B$ is the mean feature vector for class $k$ in batch $B$. Due to the batch-wise variability in negative samples, $\\mu_k^B$ can differ significantly across batches, leading to high within-class variance. \n\n\nBy minimizing the KL divergence, we align feature vectors $g_i$ of class $k$ with the mean direction $\\mu_k$, reducing the spread of feature vectors within the same class. The within-class variance with regularization is:\n\n$\\sigma_W^2 = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{M_k} \\sum_{i=1}^{M_k} \\|g_i^k - \\mu_k\\|^2$\n\nSince $\\mu_k$ is consistent across batches due to the regularization, the within-class variance is significantly reduced. This results in structured feature representations, enhancing class separability and improving performance in downstream tasks.\n\n---\n**Proposition 3**: Learning structural representation during pretraining can benefit downstream tasks.\n\n**Justification**: Structured features are those well-aligned within the same class (low within-class variance $\\sigma_W^2$) and well-separated between different classes (high between-class variance $\\sigma_B^2$). \n\nWith semantic-guided consistency regularization, feature vectors $g_i^k$ for class $k$ are closely aligned with the mean direction $\\mu_k$. This alignment reduces the within-class variance $\\sigma_W^2$. Weakly-supervised contrastive learning pushes apart feature vectors of different classes, increasing the separation between class means $\\mu_k$. This increases the between-class variance $\\sigma_B^2$.\n\nTake the linear classifier as an example, the decision boundary is determined by the separation between class means. Higher $\\sigma_B^2$ and lower $\\sigma_W^2$ result in clearer decision boundaries, reducing classification errors.\n\nConsider a simple linear classifier with weight vector $w$ and bias $b$. The decision function is:\n\n$f(x) = w^T x + b$\n\nThe decision boundary is given by:\n\n$w^T x + b = 0$\n\nFor well-structured features, the margin (distance between decision boundary and nearest samples) is maximized. The margin $ \\gamma $ for class $ k $ can be expressed as:\n\n$\\gamma = \\frac{w^T (\\mu_k - \\mu)}{\\|w\\|}$\n\nHigher between-class variance ($\\sigma_B^2$) and lower within-class variance ($\\sigma_W^2$) increase this margin, leading to better classification performance.\n\n[**Known issues**] If the equations do not display correctly, please refresh the page or try using a different browser."}
{"claim": "The paper does not provide strong arguments that hybrid domain adaptation is a meaningful, distinct research task compared to existing domain adaptation and generalization fields.", "claim_type": "novelty", "paper_id": "FE2e8664Sl", "paper_title": "Few-shot Hybrid Domain Adaptation of Image Generator", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "u2SOmGevjE", "reviewer": "Reviewer_fvXy", "review_text": "Summary: The paper introduces the few-shot generative Hybrid Domain Adaptation (HDA) task for image-based data. It is assumed that an image generator is accessed from a source domain and then from several target domains. Adaptation is performed on all target domains. To perform HDA, the paper presents a discriminator-free approach with a style GAN pre-trained on a source domain. It is adapted to individual target domains using a few shots from each domain and generates an image style showing composite attributes of each target domain separately.  The proposed approach relies on features extracted from standard transformer-based architectures. In addition, distance and direction losses are used to guide the generator to produce images with integrated features of the target domains while preserving the features of the source domain. Experiments are conducted using the FFHQ dataset and three evaluation metrics are used to measure performance and compare with different baselines.\n\nStrengths: - The paper is well written and easy to understand. The method is well described and the experiments are clearly presented. \n\n- The discriminator-free approach presented is interesting as it reduces the computational cost of fitting compared to existing approaches. \n\n- The approach shows promising results for all metrics compared to the baselines on the FFHQ dataset.\n\nWeaknesses: - (Major) The paper does not provide strong arguments as to why hybrid domain adaptation is a meaningful task. It is not clear why the current approaches and benchmarks, e.g. domain adaptation and domain generalisation, are not sufficient to address the problem presented. These points need further elaboration to motivate the paper. \n\n- (Major) results are reported using 10 shots, while it would be interesting to see results using different numbers of shots, e.g. 1-shot and 5-shot.\n\n- It is unclear whether the methodology is applicable to the generation of conditioned image styles. For example, generating an image that is conditioned on one or more styles of the target domain, rather than an image that spans all attributes of the target domains. It could discussed how the proposed methodology generalises to conditional generation. \n\n- A study of the effect of different pre-trained image encoders would add value to the paper.\n\nQuestions: - How would the proposed generator compare with well-known generator models such as DALLE, Imagen and etc.?", "labeling_timestamp": "2026-01-11T16:23:10.604904", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The authors explicitly defend HDA as a meaningful, distinct task: they provide conceptual arguments (e.g., data-collection and compositionality advantages), contrast HDA with prior methods (DoRM, style-mixing, image editing, Domain Expansion), and add empirical results to support the claim.", "evidence": "HDA only requires collecting data for several independent target domains rather than for each possible combination as mentioned by **Reviewer 2zhA**. Under such circumstances, HDA offers greater flexibility and versatility to adapt the generator to more composite and expansive domain.", "combined_author_response": "We hope these additional results and discussions can address your concerns. Please let us know if there are any further clarifications that we can offer. We would love to discuss more if any concern still remains.\n\n---\n\nWe hope these additional results and discussions can address your concerns. Please let us know if there are any further clarifications that we can offer. We would love to discuss more if any concern still remains.\n\n---\n\nWe are glad that our responses solve your concerns. Thank you again for your valuable feedback and suggestions!\n\n---\n\nDear Reviewers:\n\nWe sincerely thank you for your great efforts in reviewing this paper. We are pleased to read that our proposed problem is **interesting** and **well-motivated** [2zhA], our method is **intuitive** and **effective** [fvXy, 2zhA], and our results are **compelling** [fvXy, 2zhA, bxKn].\n\nWe have tried our best to address all the mentioned concerns and problems. **Our manuscript has been revised** to include the changes according to all the reviewers’ insightful comments, making our research more robust and accessible.\n\n**As the deadline for Author-Reviewer discussion is approaching**, we are eagerly looking forward to your responses. Please let us know if there are any additional clarifications or experiments that we could offer. We would love to discuss more if any concern still remains. Thanks again for your time!\n\nBest,\n\nAuthors\n\n---\n\n**The editability before and after the domain adaptation**\n\nWe conduct the editing on the images generated by original and adapted model for both single and hybrid domain adaptation. As shown in Fig. 17 of Appendix, the results indicate the adapted generator maintains similar editability like pose to the original generator. This verifies that the our method effectively preserves original generator's attributes. \n\n**Results in 3D GAN setting**\n\nWe conducted experiments using the popular 3D-aware image generation method, EG3D[4]. Specifically, we replace the discriminator as we did in Fig. 2 for both single and hybrid domain adaptation. As shown in Fig. 18 of Appendix, we adapt the pre-trained generator from FFHQ to $\\textit{sunglasses}$ and the hybrid of $\\textit{sunglasses}$ and $\\textit{smile}$ with 10-shot training images per domain. We can observe that the results effectively integrate the attributes and preserve the characters and poses of source domain.\n\n**Comparisons with $\\textit{Mind the GAP}$ and $\\textit{StyleCLIP}$**\n\nWhile Mind the GAP has a similar loss with direction loss, one term of our proposed directional subspace loss, their motivations differ. Mind the GAP is proposed for one-shot domain adaptation, thus representing the entire target domain using the embedding of a single image. In contrast, for the few-shot setting, we propose representing the entire domain using the subspace formed by the embeddings of multiple images. Furthermore, we utilize pre-trained encoders to obtain separated subspaces corresponding to different domains, enabling us to accomplish hybrid domain adaptation. \n\nThe direction loss in StyleCLIP is computed based on the cosine similarity between the generated images and textual prompts within the CLIP embedding space. It does not depend on the source image or domain. In practice, this loss leads to adversarial solutions and sees no benefit from maintaining diversity as depicted in Style-NADA[5]. A mode-collapsed generator producing only one image may be the best minimizer for the distance to a given textual prompt. Differently, our direction loss aims to preserve more characteristics from source domain and maintains its diversity.\n\n[1] Few-shot image generation via cross-domain correspondence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10743–10752, 2021.\n\n[2] Few shot generative model adaption via relaxed spatial structural alignment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11204–11213, 2022.\n\n[3] Domain re-modulation for few-shot generative domain adaptation. arXiv preprint arXiv:2302.02550, 2023.\n\n[4] Efficient geometry-aware 3d generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16123–16133, 2022.\n\n[5] Stylegan-nada: Clip-guided domain adaptation of image generators. arXiv preprint arXiv:2108.00946, 2021.\n\n---\n\nThank you for your insightful comments about our work. We have added additional results with highlighting in the Appendix. Here we provide a point-by-point response to the issues raised by you.\n\n**The results of more domains**\n\nConsistent with prior work for few-shot generative domain adaptation (like CDC[1], RSSA[2], and DoRM[3]), our experimental data encompasses both global style ($\\textit{sketch}$ and $\\textit{baby}$) and local attributes ($\\textit{smile}$ and $\\textit{sunglasses}$). The combination of these domains demonstrates the effectiveness of our method since they encompass all types of generative domain adaptation.\n\nTo provide more comprehensive evidence of validity, we conduct additional experiments on $\\textit{Raphael}$ and $\\textit{Caricature}$ for both single and hybrid domain adaptation. As shown in Fig. 15 of Appendix, the results integrate the characteristics from multiple target domains and maintain robust consistency with source domain, which further demonstrates the effectiveness of our method. \n\n**The generalizability to other domains**\n\nTo verify the generalizability of our method to other domains, we conduct experiments on church domain following prior work. We adapt the pre-trained generator from $\\textit{LSUN Church}$ to $\\textit{Van Gogh's house paintings}$, $\\textit{haunted houses}$, and the combination of them. As shown in Fig. 16 of Appendix, the results acquire the corresponding style and showcase the preservation of good cross-domain consistency. This aligns with the results observed in the face domain.\n\n**The advantages and disadvantages of few-shot domain adaptation techniques with discrimination**\n\nFew-shot domain adaptation techniques with discrimination (like DoRM, DualStyleGAN and 3DAvatarGAN) excel in generating images closely resembling the style of the target domain as its discriminator tends to memorize training images. However, they suffer from the notorious issue of model collapse. Especially in few-shot scenarios, they easily overfits to the target images, compromising the diversity of generated images. Additionally, they are challenging to extend into an end-to-end hybrid domain adaptation approach. Their approach to hybrid domain often involves separately training multiple models and interpolating style codes, necessitating multiple model size and training time.\n\n**Why is hybrid domain adaptation necessary in these cases**\n\nImage editing indeed enables local adjustments resembling the original domain, like $\\textit{smile}$ and $\\textit{baby}$. However, in this scenario, our method holds several advantages. \n\nFirstly, our objective is to generate images with attributes of target domain while maintaining considerable diversity, which can be applicable in scenarios like data collection. Image editing, on the other hand, requires an original image as input, rendering it impractical for such applications. \n\nMoreover, hybrid domain adaptation has the capability to generate images from hybrid domain containing multiple target attributes like $\\textit{baby with the style of sketch}$ or $\\textit{smiling person with the style of sketch}$. Image editing, however, lacks the ability to perform such global stylistic modifications, limiting its broader applications.\n\n---\n\nThank you for your insightful comments about our work. We have added additional results with highlighting in the Appendix. Here we provide a point-by-point response to the issues raised by you.\n\n**Why we claim to introduce Hybrid generative DA**\n\nPrior techniques like style-mixing [1] can generate images from hybrid domain via interpolating latent codes. DoRM is just one of those methods. However, they lack a detailed investigation of HDA, which is merely mentioned as an additional feature. For example, they fail to provide systematic definition and proper evaluation metrics to distinguish good from bad.\n\nWith regarding to the method in achieving HDA, they encounter two primary issues:\n\n(1) DoRM is not an end-to-end pipeline to address HDA. They primarily focus on single domain adaptation, necessitating the separate training and interpolating multiple models to accomplish HDA. This may not be the right way to approach this task. \n\n(2) DoRM necessitates intricate tuning of hyperparameters that interpolate between the source and target domains. As demonstrated in the DoRM paper, they assign a weight of 0.005 to the $\\textit{baby}$ domain during single domain adaptation. Varying these hyperparameters significantly affects the outcomes. This complexity and sensitivity in parameter tuning make HDA's tuning more intricate and demanding.\n\n**How sensitive is our method to the domain coefficient $\\alpha$**\n\nIn our method for hybrid domain adaptation, this parameter controls the composition ratio of the attribute from each domain. As depicted in A.4 of Appendix, we use $\\alpha_i = 0.5$ for most experiments without the need for complex and intricate adjustments. To further explore the sensitivity, we conduct the study for simple traversal of $\\alpha_i$. As shown in the Fig. 14 of Appendix, the attributes of generated images transit smoothly between domains. Our method produces the similar attribute blending effect when $\\alpha_i \\in$ \\{0.4, 0.5, 0.6\\} .\n\n**Comparison to methods for diffusion model personalization**\n\nCurrent trend in customized text-to-image models like DreamBooth aim to mimic the appearance of subjects in a given reference set. Similar to ours, DreamBooth fine-tunes a pre-trained generator for the personalization. However, there are two key differences between personalization and HDA.\n\n(1) DreamBooth aims to retain the individuals from the training images, requiring similar individuals across the training set. Differently, our goal is to acquire the attributes of target domain like $\\textit{baby}$ and preserve consistency with the source generator, ensuring that images generated from the same noise exhibit similar individuals.\n\n(2) Dreambooth utilizes text-to-image generator which necessitates intricate and laborious adjustments of prompts to synthesize images with specific attributes. Additionally, certain attributes are challenging to accurately describe using text, such as artistic paintings. Conversely, our adapted model adeptly preserves the domain-specific attributes of reference images without the need for intricate prompt engineering.\n\n**Conceptual comparison to Domain Expansion**\n\nDomain Expansion aims to expand the pre-trained generator to have the capacity to generate images from multiple domains. To the end, it proposes to repurpose dormant directions in latent space for the new domains. While our approach involves training images from multiple target domains, our objective is to create a unseen composition of given target domains that integrates attributes from them. For example, given training images from $\\textit{baby}$ and $\\textit{sketch}$, Domain Expansion aims to expand the generator to have the capacity to generate images from $\\textit{baby}$ or $\\textit{sketch}$. Differently, we aim to adapt the generator to unseen hybrid domain $\\textit{baby with the style of sketch}$.\n\n**Details of the training time comparisons**\n\nIn Table 2, our comparison about training time is fair. Specifically, we measure all the time on a single NVIDIA TITAN GPU. Due to the absence of open-source code for DoRM, we implement it following their description in the paper, based on the official implementation of StyleGAN2-ADA. The Seq method is also developed in a similar manner. As for StyleNADA, we utilize their open-source code for training. For an apples-to-apples comparison, we set the same batch size as 4 and the same resolution as $256 \\times 256$.\n\n[1] Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8110–8119, 2020.\n\n---\n\nThank you for your insightful comments about our work. We have added additional results with highlighting in the Appendix. Here we provide a point-by-point response to the issues raised by you.\n\n**Why is hybrid domain adaptation a meaningful task**\n\nCurrent generative domain adaptation approaches typically employ the discriminator to discern whether generated images belong to the target domain. When we require to generate images with integrated attributes, they need to collect images from the hybrid target domain (e.g., a $\\textit{smiling}$ $\\textit{baby}$ with the style of $\\textit{sketch}$ in Fig. 1). However, these images tend to be more difficult to collect compared with single domain in real-world scenarios. In contrast, HDA only requires collecting data for several independent target domains rather than for each possible combination as mentioned by **Reviewer 2zhA**. Under such circumstances, HDA offers greater flexibility and versatility to adapt the generator to more composite and expansive domain.\n\n**Results using different numbers of shots**\n\nWe perform experiments on lower shots (5-shot and 1-shot) as suggested using various datasets. As shown in Fig. 11 of Appendix, the results of 5-shot are close to those of 10-shot, which integrates the attributes and maintains the consistency with source domain. Although multiple attributes have been learned, the results of 1-shot exhibit relatively lower cross-domain consistency compared with 10-shot and 5-shot.  This is because when there is only one reference image per domain, the subspace in our directional subspace loss degenerates into a single point (see Fig. 2). Then distinct generated images corresponding to different noise tend to converge towards the same image in CLIP's embedding space, which comprises cross-domain consistency as depicted in Section 3.3.\n\n**Results of conditional generation**\n\nAs shown in Fig. 12 of Appendix, we conduct the experiments on conditional generation. Specifically, we collect 10-shot images with $\\textit{red hair}$ and $\\textit{sunglasses}$. Then we use masks to separate these attributes and adapt the generator with masked images for $\\textit{red hair}$ and $\\textit{sunglasses}$ respectively. We can observe that the generated images possess the corresponding attribute for both single DA and hybrid DA. Simultaneously, these images also maintain consistency with source domain.\n    \n**Effect of different pre-trained image encoders**\n\nAs shown in Fig. 13 of Appendix, we conduct experiments on pre-trained Swin and Dinov2 to explore the impact of different image encoders on generated images. Their results all achieve HDA, indicating that our method is agnostic to different pre-trained image encoders. Although they exhibit slight stylistic differences, these are due to their different approaches to extract features into separated subspaces, as depicted in Fig. 8 of Appendix. To converge to the exact realization of the target domain, our method employs the ensemble technique that exploits both Swin and Dinov2. As shown in the figure, the results closely resembles the attributes of the target domain while maintaining the best consistency with source domain.\n\n**Compare with DALL-E and Imagen**\n\nDALL-E and Imagen are both text-to-image models known for their ability to generate images based on natural language descriptions. They exhibit the advantage of autonomously producing realistic images with specific attributes while maintaining considerable diversity. However, in comparison to our generator, they exhibit two primary drawbacks. \n\n(1) They necessitate intricate and laborious adjustments of prompts to generate images with specific attributes. Additionally, certain attributes are challenging to accurately describe using text, such as artistic paintings. Conversely, our generator can preserve the domain-specific attributes of reference images without the need for intricate prompt engineering.\n\n(2) It is hard to control the characters within images via prompts for them. In contrast, our adapted model maintains strong consistency with the source generator, preserving the same characters from the same noise.\n\nBesides, ours differs from the current trend in customized text-to-image models like DreamBooth. DreamBooth aims to retain the individuals from the training images, requiring similar individuals across the training set. Differently, our goal is to acquire the attributes of target domain like $\\textit{baby}$ and preserve consistency with the source generator, ensuring that images generated from the same noise exhibit similar individuals.", "author_response": "Thank you for your insightful comments about our work. We have added additional results with highlighting in the Appendix. Here we provide a point-by-point response to the issues raised by you.\n\n**Why is hybrid domain adaptation a meaningful task**\n\nCurrent generative domain adaptation approaches typically employ the discriminator to discern whether generated images belong to the target domain. When we require to generate images with integrated attributes, they need to collect images from the hybrid target domain (e.g., a $\\textit{smiling}$ $\\textit{baby}$ with the style of $\\textit{sketch}$ in Fig. 1). However, these images tend to be more difficult to collect compared with single domain in real-world scenarios. In contrast, HDA only requires collecting data for several independent target domains rather than for each possible combination as mentioned by **Reviewer 2zhA**. Under such circumstances, HDA offers greater flexibility and versatility to adapt the generator to more composite and expansive domain.\n\n**Results using different numbers of shots**\n\nWe perform experiments on lower shots (5-shot and 1-shot) as suggested using various datasets. As shown in Fig. 11 of Appendix, the results of 5-shot are close to those of 10-shot, which integrates the attributes and maintains the consistency with source domain. Although multiple attributes have been learned, the results of 1-shot exhibit relatively lower cross-domain consistency compared with 10-shot and 5-shot.  This is because when there is only one reference image per domain, the subspace in our directional subspace loss degenerates into a single point (see Fig. 2). Then distinct generated images corresponding to different noise tend to converge towards the same image in CLIP's embedding space, which comprises cross-domain consistency as depicted in Section 3.3.\n\n**Results of conditional generation**\n\nAs shown in Fig. 12 of Appendix, we conduct the experiments on conditional generation. Specifically, we collect 10-shot images with $\\textit{red hair}$ and $\\textit{sunglasses}$. Then we use masks to separate these attributes and adapt the generator with masked images for $\\textit{red hair}$ and $\\textit{sunglasses}$ respectively. We can observe that the generated images possess the corresponding attribute for both single DA and hybrid DA. Simultaneously, these images also maintain consistency with source domain.\n    \n**Effect of different pre-trained image encoders**\n\nAs shown in Fig. 13 of Appendix, we conduct experiments on pre-trained Swin and Dinov2 to explore the impact of different image encoders on generated images. Their results all achieve HDA, indicating that our method is agnostic to different pre-trained image encoders. Although they exhibit slight stylistic differences, these are due to their different approaches to extract features into separated subspaces, as depicted in Fig. 8 of Appendix. To converge to the exact realization of the target domain, our method employs the ensemble technique that exploits both Swin and Dinov2. As shown in the figure, the results closely resembles the attributes of the target domain while maintaining the best consistency with source domain.\n\n**Compare with DALL-E and Imagen**\n\nDALL-E and Imagen are both text-to-image models known for their ability to generate images based on natural language descriptions. They exhibit the advantage of autonomously producing realistic images with specific attributes while maintaining considerable diversity. However, in comparison to our generator, they exhibit two primary drawbacks. \n\n(1) They necessitate intricate and laborious adjustments of prompts to generate images with specific attributes. Additionally, certain attributes are challenging to accurately describe using text, such as artistic paintings. Conversely, our generator can preserve the domain-specific attributes of reference images without the need for intricate prompt engineering.\n\n(2) It is hard to control the characters within images via prompts for them. In contrast, our adapted model maintains strong consistency with the source generator, preserving the same characters from the same noise.\n\nBesides, ours differs from the current trend in customized text-to-image models like DreamBooth. DreamBooth aims to retain the individuals from the training images, requiring similar individuals across the training set. Differently, our goal is to acquire the attributes of target domain like $\\textit{baby}$ and preserve consistency with the source generator, ensuring that images generated from the same noise exhibit similar individuals."}
{"claim": "The paper does not justify why existing domain adaptation and domain generalization benchmarks are insufficient to address the proposed hybrid domain adaptation problem.", "claim_type": "subjective", "paper_id": "FE2e8664Sl", "paper_title": "Few-shot Hybrid Domain Adaptation of Image Generator", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "u2SOmGevjE", "reviewer": "Reviewer_fvXy", "review_text": "Summary: The paper introduces the few-shot generative Hybrid Domain Adaptation (HDA) task for image-based data. It is assumed that an image generator is accessed from a source domain and then from several target domains. Adaptation is performed on all target domains. To perform HDA, the paper presents a discriminator-free approach with a style GAN pre-trained on a source domain. It is adapted to individual target domains using a few shots from each domain and generates an image style showing composite attributes of each target domain separately.  The proposed approach relies on features extracted from standard transformer-based architectures. In addition, distance and direction losses are used to guide the generator to produce images with integrated features of the target domains while preserving the features of the source domain. Experiments are conducted using the FFHQ dataset and three evaluation metrics are used to measure performance and compare with different baselines.\n\nStrengths: - The paper is well written and easy to understand. The method is well described and the experiments are clearly presented. \n\n- The discriminator-free approach presented is interesting as it reduces the computational cost of fitting compared to existing approaches. \n\n- The approach shows promising results for all metrics compared to the baselines on the FFHQ dataset.\n\nWeaknesses: - (Major) The paper does not provide strong arguments as to why hybrid domain adaptation is a meaningful task. It is not clear why the current approaches and benchmarks, e.g. domain adaptation and domain generalisation, are not sufficient to address the problem presented. These points need further elaboration to motivate the paper. \n\n- (Major) results are reported using 10 shots, while it would be interesting to see results using different numbers of shots, e.g. 1-shot and 5-shot.\n\n- It is unclear whether the methodology is applicable to the generation of conditioned image styles. For example, generating an image that is conditioned on one or more styles of the target domain, rather than an image that spans all attributes of the target domains. It could discussed how the proposed methodology generalises to conditional generation. \n\n- A study of the effect of different pre-trained image encoders would add value to the paper.\n\nQuestions: - How would the proposed generator compare with well-known generator models such as DALLE, Imagen and etc.?", "labeling_timestamp": "2026-01-11T16:23:03.087386", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors directly explain why prior benchmarks/methods are insufficient for Hybrid Domain Adaptation (HDA): they argue prior work only mentions HDA without systematic definition or evaluation, is not end-to-end (requiring separate models and interpolation), is sensitive to hyperparameters, and that HDA reduces data-collection burden compared to collecting hybrid-domain images — thus addressing the reviewer's concern.", "evidence": "Prior techniques like style-mixing [1] can generate images from hybrid domain via interpolating latent codes. DoRM is just one of those methods. However, they lack a detailed investigation of HDA, which is merely mentioned as an additional feature. For example, they fail to provide systematic definition and proper evaluation metrics to distinguish good from bad.\n...\nWith regarding to the method in achieving HDA, they encounter two primary issues: (1) DoRM is not an end-to-end pipeline to address HDA. They primarily focus on single domain adaptation, necessitating the separate training and interpolating multiple models to accomplish HDA. This may not be the right way to approach this task. (2) DoRM necessitates intricate tuning of hyperparameters that interpolate between the source and target domains. ... This complexity and sensitivity in parameter tuning make HDA's tuning more intricate and demanding.\n...\nCurrent generative domain adaptation approaches typically employ the discriminator to discern whether generated images belong to the target domain. When we require to generate images with integrated attributes, they need to collect images from the hybrid target domain (e.g., a smiling baby with the style of sketch). However, these images tend to be more difficult to collect compared with single domain in real-world scenarios. In contrast, HDA only requires collecting data for several independent target domains rather than for each possible combination.", "combined_author_response": "We hope these additional results and discussions can address your concerns. Please let us know if there are any further clarifications that we can offer. We would love to discuss more if any concern still remains.\n\n---\n\nWe hope these additional results and discussions can address your concerns. Please let us know if there are any further clarifications that we can offer. We would love to discuss more if any concern still remains.\n\n---\n\nWe are glad that our responses solve your concerns. Thank you again for your valuable feedback and suggestions!\n\n---\n\nDear Reviewers:\n\nWe sincerely thank you for your great efforts in reviewing this paper. We are pleased to read that our proposed problem is **interesting** and **well-motivated** [2zhA], our method is **intuitive** and **effective** [fvXy, 2zhA], and our results are **compelling** [fvXy, 2zhA, bxKn].\n\nWe have tried our best to address all the mentioned concerns and problems. **Our manuscript has been revised** to include the changes according to all the reviewers’ insightful comments, making our research more robust and accessible.\n\n**As the deadline for Author-Reviewer discussion is approaching**, we are eagerly looking forward to your responses. Please let us know if there are any additional clarifications or experiments that we could offer. We would love to discuss more if any concern still remains. Thanks again for your time!\n\nBest,\n\nAuthors\n\n---\n\n**The editability before and after the domain adaptation**\n\nWe conduct the editing on the images generated by original and adapted model for both single and hybrid domain adaptation. As shown in Fig. 17 of Appendix, the results indicate the adapted generator maintains similar editability like pose to the original generator. This verifies that the our method effectively preserves original generator's attributes. \n\n**Results in 3D GAN setting**\n\nWe conducted experiments using the popular 3D-aware image generation method, EG3D[4]. Specifically, we replace the discriminator as we did in Fig. 2 for both single and hybrid domain adaptation. As shown in Fig. 18 of Appendix, we adapt the pre-trained generator from FFHQ to $\\textit{sunglasses}$ and the hybrid of $\\textit{sunglasses}$ and $\\textit{smile}$ with 10-shot training images per domain. We can observe that the results effectively integrate the attributes and preserve the characters and poses of source domain.\n\n**Comparisons with $\\textit{Mind the GAP}$ and $\\textit{StyleCLIP}$**\n\nWhile Mind the GAP has a similar loss with direction loss, one term of our proposed directional subspace loss, their motivations differ. Mind the GAP is proposed for one-shot domain adaptation, thus representing the entire target domain using the embedding of a single image. In contrast, for the few-shot setting, we propose representing the entire domain using the subspace formed by the embeddings of multiple images. Furthermore, we utilize pre-trained encoders to obtain separated subspaces corresponding to different domains, enabling us to accomplish hybrid domain adaptation. \n\nThe direction loss in StyleCLIP is computed based on the cosine similarity between the generated images and textual prompts within the CLIP embedding space. It does not depend on the source image or domain. In practice, this loss leads to adversarial solutions and sees no benefit from maintaining diversity as depicted in Style-NADA[5]. A mode-collapsed generator producing only one image may be the best minimizer for the distance to a given textual prompt. Differently, our direction loss aims to preserve more characteristics from source domain and maintains its diversity.\n\n[1] Few-shot image generation via cross-domain correspondence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10743–10752, 2021.\n\n[2] Few shot generative model adaption via relaxed spatial structural alignment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11204–11213, 2022.\n\n[3] Domain re-modulation for few-shot generative domain adaptation. arXiv preprint arXiv:2302.02550, 2023.\n\n[4] Efficient geometry-aware 3d generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16123–16133, 2022.\n\n[5] Stylegan-nada: Clip-guided domain adaptation of image generators. arXiv preprint arXiv:2108.00946, 2021.\n\n---\n\nThank you for your insightful comments about our work. We have added additional results with highlighting in the Appendix. Here we provide a point-by-point response to the issues raised by you.\n\n**The results of more domains**\n\nConsistent with prior work for few-shot generative domain adaptation (like CDC[1], RSSA[2], and DoRM[3]), our experimental data encompasses both global style ($\\textit{sketch}$ and $\\textit{baby}$) and local attributes ($\\textit{smile}$ and $\\textit{sunglasses}$). The combination of these domains demonstrates the effectiveness of our method since they encompass all types of generative domain adaptation.\n\nTo provide more comprehensive evidence of validity, we conduct additional experiments on $\\textit{Raphael}$ and $\\textit{Caricature}$ for both single and hybrid domain adaptation. As shown in Fig. 15 of Appendix, the results integrate the characteristics from multiple target domains and maintain robust consistency with source domain, which further demonstrates the effectiveness of our method. \n\n**The generalizability to other domains**\n\nTo verify the generalizability of our method to other domains, we conduct experiments on church domain following prior work. We adapt the pre-trained generator from $\\textit{LSUN Church}$ to $\\textit{Van Gogh's house paintings}$, $\\textit{haunted houses}$, and the combination of them. As shown in Fig. 16 of Appendix, the results acquire the corresponding style and showcase the preservation of good cross-domain consistency. This aligns with the results observed in the face domain.\n\n**The advantages and disadvantages of few-shot domain adaptation techniques with discrimination**\n\nFew-shot domain adaptation techniques with discrimination (like DoRM, DualStyleGAN and 3DAvatarGAN) excel in generating images closely resembling the style of the target domain as its discriminator tends to memorize training images. However, they suffer from the notorious issue of model collapse. Especially in few-shot scenarios, they easily overfits to the target images, compromising the diversity of generated images. Additionally, they are challenging to extend into an end-to-end hybrid domain adaptation approach. Their approach to hybrid domain often involves separately training multiple models and interpolating style codes, necessitating multiple model size and training time.\n\n**Why is hybrid domain adaptation necessary in these cases**\n\nImage editing indeed enables local adjustments resembling the original domain, like $\\textit{smile}$ and $\\textit{baby}$. However, in this scenario, our method holds several advantages. \n\nFirstly, our objective is to generate images with attributes of target domain while maintaining considerable diversity, which can be applicable in scenarios like data collection. Image editing, on the other hand, requires an original image as input, rendering it impractical for such applications. \n\nMoreover, hybrid domain adaptation has the capability to generate images from hybrid domain containing multiple target attributes like $\\textit{baby with the style of sketch}$ or $\\textit{smiling person with the style of sketch}$. Image editing, however, lacks the ability to perform such global stylistic modifications, limiting its broader applications.\n\n---\n\nThank you for your insightful comments about our work. We have added additional results with highlighting in the Appendix. Here we provide a point-by-point response to the issues raised by you.\n\n**Why we claim to introduce Hybrid generative DA**\n\nPrior techniques like style-mixing [1] can generate images from hybrid domain via interpolating latent codes. DoRM is just one of those methods. However, they lack a detailed investigation of HDA, which is merely mentioned as an additional feature. For example, they fail to provide systematic definition and proper evaluation metrics to distinguish good from bad.\n\nWith regarding to the method in achieving HDA, they encounter two primary issues:\n\n(1) DoRM is not an end-to-end pipeline to address HDA. They primarily focus on single domain adaptation, necessitating the separate training and interpolating multiple models to accomplish HDA. This may not be the right way to approach this task. \n\n(2) DoRM necessitates intricate tuning of hyperparameters that interpolate between the source and target domains. As demonstrated in the DoRM paper, they assign a weight of 0.005 to the $\\textit{baby}$ domain during single domain adaptation. Varying these hyperparameters significantly affects the outcomes. This complexity and sensitivity in parameter tuning make HDA's tuning more intricate and demanding.\n\n**How sensitive is our method to the domain coefficient $\\alpha$**\n\nIn our method for hybrid domain adaptation, this parameter controls the composition ratio of the attribute from each domain. As depicted in A.4 of Appendix, we use $\\alpha_i = 0.5$ for most experiments without the need for complex and intricate adjustments. To further explore the sensitivity, we conduct the study for simple traversal of $\\alpha_i$. As shown in the Fig. 14 of Appendix, the attributes of generated images transit smoothly between domains. Our method produces the similar attribute blending effect when $\\alpha_i \\in$ \\{0.4, 0.5, 0.6\\} .\n\n**Comparison to methods for diffusion model personalization**\n\nCurrent trend in customized text-to-image models like DreamBooth aim to mimic the appearance of subjects in a given reference set. Similar to ours, DreamBooth fine-tunes a pre-trained generator for the personalization. However, there are two key differences between personalization and HDA.\n\n(1) DreamBooth aims to retain the individuals from the training images, requiring similar individuals across the training set. Differently, our goal is to acquire the attributes of target domain like $\\textit{baby}$ and preserve consistency with the source generator, ensuring that images generated from the same noise exhibit similar individuals.\n\n(2) Dreambooth utilizes text-to-image generator which necessitates intricate and laborious adjustments of prompts to synthesize images with specific attributes. Additionally, certain attributes are challenging to accurately describe using text, such as artistic paintings. Conversely, our adapted model adeptly preserves the domain-specific attributes of reference images without the need for intricate prompt engineering.\n\n**Conceptual comparison to Domain Expansion**\n\nDomain Expansion aims to expand the pre-trained generator to have the capacity to generate images from multiple domains. To the end, it proposes to repurpose dormant directions in latent space for the new domains. While our approach involves training images from multiple target domains, our objective is to create a unseen composition of given target domains that integrates attributes from them. For example, given training images from $\\textit{baby}$ and $\\textit{sketch}$, Domain Expansion aims to expand the generator to have the capacity to generate images from $\\textit{baby}$ or $\\textit{sketch}$. Differently, we aim to adapt the generator to unseen hybrid domain $\\textit{baby with the style of sketch}$.\n\n**Details of the training time comparisons**\n\nIn Table 2, our comparison about training time is fair. Specifically, we measure all the time on a single NVIDIA TITAN GPU. Due to the absence of open-source code for DoRM, we implement it following their description in the paper, based on the official implementation of StyleGAN2-ADA. The Seq method is also developed in a similar manner. As for StyleNADA, we utilize their open-source code for training. For an apples-to-apples comparison, we set the same batch size as 4 and the same resolution as $256 \\times 256$.\n\n[1] Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8110–8119, 2020.\n\n---\n\nThank you for your insightful comments about our work. We have added additional results with highlighting in the Appendix. Here we provide a point-by-point response to the issues raised by you.\n\n**Why is hybrid domain adaptation a meaningful task**\n\nCurrent generative domain adaptation approaches typically employ the discriminator to discern whether generated images belong to the target domain. When we require to generate images with integrated attributes, they need to collect images from the hybrid target domain (e.g., a $\\textit{smiling}$ $\\textit{baby}$ with the style of $\\textit{sketch}$ in Fig. 1). However, these images tend to be more difficult to collect compared with single domain in real-world scenarios. In contrast, HDA only requires collecting data for several independent target domains rather than for each possible combination as mentioned by **Reviewer 2zhA**. Under such circumstances, HDA offers greater flexibility and versatility to adapt the generator to more composite and expansive domain.\n\n**Results using different numbers of shots**\n\nWe perform experiments on lower shots (5-shot and 1-shot) as suggested using various datasets. As shown in Fig. 11 of Appendix, the results of 5-shot are close to those of 10-shot, which integrates the attributes and maintains the consistency with source domain. Although multiple attributes have been learned, the results of 1-shot exhibit relatively lower cross-domain consistency compared with 10-shot and 5-shot.  This is because when there is only one reference image per domain, the subspace in our directional subspace loss degenerates into a single point (see Fig. 2). Then distinct generated images corresponding to different noise tend to converge towards the same image in CLIP's embedding space, which comprises cross-domain consistency as depicted in Section 3.3.\n\n**Results of conditional generation**\n\nAs shown in Fig. 12 of Appendix, we conduct the experiments on conditional generation. Specifically, we collect 10-shot images with $\\textit{red hair}$ and $\\textit{sunglasses}$. Then we use masks to separate these attributes and adapt the generator with masked images for $\\textit{red hair}$ and $\\textit{sunglasses}$ respectively. We can observe that the generated images possess the corresponding attribute for both single DA and hybrid DA. Simultaneously, these images also maintain consistency with source domain.\n    \n**Effect of different pre-trained image encoders**\n\nAs shown in Fig. 13 of Appendix, we conduct experiments on pre-trained Swin and Dinov2 to explore the impact of different image encoders on generated images. Their results all achieve HDA, indicating that our method is agnostic to different pre-trained image encoders. Although they exhibit slight stylistic differences, these are due to their different approaches to extract features into separated subspaces, as depicted in Fig. 8 of Appendix. To converge to the exact realization of the target domain, our method employs the ensemble technique that exploits both Swin and Dinov2. As shown in the figure, the results closely resembles the attributes of the target domain while maintaining the best consistency with source domain.\n\n**Compare with DALL-E and Imagen**\n\nDALL-E and Imagen are both text-to-image models known for their ability to generate images based on natural language descriptions. They exhibit the advantage of autonomously producing realistic images with specific attributes while maintaining considerable diversity. However, in comparison to our generator, they exhibit two primary drawbacks. \n\n(1) They necessitate intricate and laborious adjustments of prompts to generate images with specific attributes. Additionally, certain attributes are challenging to accurately describe using text, such as artistic paintings. Conversely, our generator can preserve the domain-specific attributes of reference images without the need for intricate prompt engineering.\n\n(2) It is hard to control the characters within images via prompts for them. In contrast, our adapted model maintains strong consistency with the source generator, preserving the same characters from the same noise.\n\nBesides, ours differs from the current trend in customized text-to-image models like DreamBooth. DreamBooth aims to retain the individuals from the training images, requiring similar individuals across the training set. Differently, our goal is to acquire the attributes of target domain like $\\textit{baby}$ and preserve consistency with the source generator, ensuring that images generated from the same noise exhibit similar individuals.", "author_response": "Thank you for your insightful comments about our work. We have added additional results with highlighting in the Appendix. Here we provide a point-by-point response to the issues raised by you.\n\n**Why is hybrid domain adaptation a meaningful task**\n\nCurrent generative domain adaptation approaches typically employ the discriminator to discern whether generated images belong to the target domain. When we require to generate images with integrated attributes, they need to collect images from the hybrid target domain (e.g., a $\\textit{smiling}$ $\\textit{baby}$ with the style of $\\textit{sketch}$ in Fig. 1). However, these images tend to be more difficult to collect compared with single domain in real-world scenarios. In contrast, HDA only requires collecting data for several independent target domains rather than for each possible combination as mentioned by **Reviewer 2zhA**. Under such circumstances, HDA offers greater flexibility and versatility to adapt the generator to more composite and expansive domain.\n\n**Results using different numbers of shots**\n\nWe perform experiments on lower shots (5-shot and 1-shot) as suggested using various datasets. As shown in Fig. 11 of Appendix, the results of 5-shot are close to those of 10-shot, which integrates the attributes and maintains the consistency with source domain. Although multiple attributes have been learned, the results of 1-shot exhibit relatively lower cross-domain consistency compared with 10-shot and 5-shot.  This is because when there is only one reference image per domain, the subspace in our directional subspace loss degenerates into a single point (see Fig. 2). Then distinct generated images corresponding to different noise tend to converge towards the same image in CLIP's embedding space, which comprises cross-domain consistency as depicted in Section 3.3.\n\n**Results of conditional generation**\n\nAs shown in Fig. 12 of Appendix, we conduct the experiments on conditional generation. Specifically, we collect 10-shot images with $\\textit{red hair}$ and $\\textit{sunglasses}$. Then we use masks to separate these attributes and adapt the generator with masked images for $\\textit{red hair}$ and $\\textit{sunglasses}$ respectively. We can observe that the generated images possess the corresponding attribute for both single DA and hybrid DA. Simultaneously, these images also maintain consistency with source domain.\n    \n**Effect of different pre-trained image encoders**\n\nAs shown in Fig. 13 of Appendix, we conduct experiments on pre-trained Swin and Dinov2 to explore the impact of different image encoders on generated images. Their results all achieve HDA, indicating that our method is agnostic to different pre-trained image encoders. Although they exhibit slight stylistic differences, these are due to their different approaches to extract features into separated subspaces, as depicted in Fig. 8 of Appendix. To converge to the exact realization of the target domain, our method employs the ensemble technique that exploits both Swin and Dinov2. As shown in the figure, the results closely resembles the attributes of the target domain while maintaining the best consistency with source domain.\n\n**Compare with DALL-E and Imagen**\n\nDALL-E and Imagen are both text-to-image models known for their ability to generate images based on natural language descriptions. They exhibit the advantage of autonomously producing realistic images with specific attributes while maintaining considerable diversity. However, in comparison to our generator, they exhibit two primary drawbacks. \n\n(1) They necessitate intricate and laborious adjustments of prompts to generate images with specific attributes. Additionally, certain attributes are challenging to accurately describe using text, such as artistic paintings. Conversely, our generator can preserve the domain-specific attributes of reference images without the need for intricate prompt engineering.\n\n(2) It is hard to control the characters within images via prompts for them. In contrast, our adapted model maintains strong consistency with the source generator, preserving the same characters from the same noise.\n\nBesides, ours differs from the current trend in customized text-to-image models like DreamBooth. DreamBooth aims to retain the individuals from the training images, requiring similar individuals across the training set. Differently, our goal is to acquire the attributes of target domain like $\\textit{baby}$ and preserve consistency with the source generator, ensuring that images generated from the same noise exhibit similar individuals."}
{"claim": "Results are reported only for 10-shot experiments; the paper lacks results for other shot counts such as 1-shot and 5-shot to assess performance sensitivity.", "claim_type": "experimental", "paper_id": "FE2e8664Sl", "paper_title": "Few-shot Hybrid Domain Adaptation of Image Generator", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "u2SOmGevjE", "reviewer": "Reviewer_fvXy", "review_text": "Summary: The paper introduces the few-shot generative Hybrid Domain Adaptation (HDA) task for image-based data. It is assumed that an image generator is accessed from a source domain and then from several target domains. Adaptation is performed on all target domains. To perform HDA, the paper presents a discriminator-free approach with a style GAN pre-trained on a source domain. It is adapted to individual target domains using a few shots from each domain and generates an image style showing composite attributes of each target domain separately.  The proposed approach relies on features extracted from standard transformer-based architectures. In addition, distance and direction losses are used to guide the generator to produce images with integrated features of the target domains while preserving the features of the source domain. Experiments are conducted using the FFHQ dataset and three evaluation metrics are used to measure performance and compare with different baselines.\n\nStrengths: - The paper is well written and easy to understand. The method is well described and the experiments are clearly presented. \n\n- The discriminator-free approach presented is interesting as it reduces the computational cost of fitting compared to existing approaches. \n\n- The approach shows promising results for all metrics compared to the baselines on the FFHQ dataset.\n\nWeaknesses: - (Major) The paper does not provide strong arguments as to why hybrid domain adaptation is a meaningful task. It is not clear why the current approaches and benchmarks, e.g. domain adaptation and domain generalisation, are not sufficient to address the problem presented. These points need further elaboration to motivate the paper. \n\n- (Major) results are reported using 10 shots, while it would be interesting to see results using different numbers of shots, e.g. 1-shot and 5-shot.\n\n- It is unclear whether the methodology is applicable to the generation of conditioned image styles. For example, generating an image that is conditioned on one or more styles of the target domain, rather than an image that spans all attributes of the target domains. It could discussed how the proposed methodology generalises to conditional generation. \n\n- A study of the effect of different pre-trained image encoders would add value to the paper.\n\nQuestions: - How would the proposed generator compare with well-known generator models such as DALLE, Imagen and etc.?", "labeling_timestamp": "2026-01-11T16:23:02.237512", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors state they ran experiments for 5-shot and 1-shot, report the outcomes (5-shot close to 10-shot; 1-shot shows lower consistency), and added these results to the appendix, directly addressing the reviewer's request.", "evidence": "We perform experiments on lower shots (5-shot and 1-shot) as suggested using various datasets. As shown in Fig. 11 of Appendix, the results of 5-shot are close to those of 10-shot, which integrates the attributes and maintains the consistency with source domain. Although multiple attributes have been learned, the results of 1-shot exhibit relatively lower cross-domain consistency compared with 10-shot and 5-shot.", "combined_author_response": "We hope these additional results and discussions can address your concerns. Please let us know if there are any further clarifications that we can offer. We would love to discuss more if any concern still remains.\n\n---\n\nWe hope these additional results and discussions can address your concerns. Please let us know if there are any further clarifications that we can offer. We would love to discuss more if any concern still remains.\n\n---\n\nWe are glad that our responses solve your concerns. Thank you again for your valuable feedback and suggestions!\n\n---\n\nDear Reviewers:\n\nWe sincerely thank you for your great efforts in reviewing this paper. We are pleased to read that our proposed problem is **interesting** and **well-motivated** [2zhA], our method is **intuitive** and **effective** [fvXy, 2zhA], and our results are **compelling** [fvXy, 2zhA, bxKn].\n\nWe have tried our best to address all the mentioned concerns and problems. **Our manuscript has been revised** to include the changes according to all the reviewers’ insightful comments, making our research more robust and accessible.\n\n**As the deadline for Author-Reviewer discussion is approaching**, we are eagerly looking forward to your responses. Please let us know if there are any additional clarifications or experiments that we could offer. We would love to discuss more if any concern still remains. Thanks again for your time!\n\nBest,\n\nAuthors\n\n---\n\n**The editability before and after the domain adaptation**\n\nWe conduct the editing on the images generated by original and adapted model for both single and hybrid domain adaptation. As shown in Fig. 17 of Appendix, the results indicate the adapted generator maintains similar editability like pose to the original generator. This verifies that the our method effectively preserves original generator's attributes. \n\n**Results in 3D GAN setting**\n\nWe conducted experiments using the popular 3D-aware image generation method, EG3D[4]. Specifically, we replace the discriminator as we did in Fig. 2 for both single and hybrid domain adaptation. As shown in Fig. 18 of Appendix, we adapt the pre-trained generator from FFHQ to $\\textit{sunglasses}$ and the hybrid of $\\textit{sunglasses}$ and $\\textit{smile}$ with 10-shot training images per domain. We can observe that the results effectively integrate the attributes and preserve the characters and poses of source domain.\n\n**Comparisons with $\\textit{Mind the GAP}$ and $\\textit{StyleCLIP}$**\n\nWhile Mind the GAP has a similar loss with direction loss, one term of our proposed directional subspace loss, their motivations differ. Mind the GAP is proposed for one-shot domain adaptation, thus representing the entire target domain using the embedding of a single image. In contrast, for the few-shot setting, we propose representing the entire domain using the subspace formed by the embeddings of multiple images. Furthermore, we utilize pre-trained encoders to obtain separated subspaces corresponding to different domains, enabling us to accomplish hybrid domain adaptation. \n\nThe direction loss in StyleCLIP is computed based on the cosine similarity between the generated images and textual prompts within the CLIP embedding space. It does not depend on the source image or domain. In practice, this loss leads to adversarial solutions and sees no benefit from maintaining diversity as depicted in Style-NADA[5]. A mode-collapsed generator producing only one image may be the best minimizer for the distance to a given textual prompt. Differently, our direction loss aims to preserve more characteristics from source domain and maintains its diversity.\n\n[1] Few-shot image generation via cross-domain correspondence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10743–10752, 2021.\n\n[2] Few shot generative model adaption via relaxed spatial structural alignment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11204–11213, 2022.\n\n[3] Domain re-modulation for few-shot generative domain adaptation. arXiv preprint arXiv:2302.02550, 2023.\n\n[4] Efficient geometry-aware 3d generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16123–16133, 2022.\n\n[5] Stylegan-nada: Clip-guided domain adaptation of image generators. arXiv preprint arXiv:2108.00946, 2021.\n\n---\n\nThank you for your insightful comments about our work. We have added additional results with highlighting in the Appendix. Here we provide a point-by-point response to the issues raised by you.\n\n**The results of more domains**\n\nConsistent with prior work for few-shot generative domain adaptation (like CDC[1], RSSA[2], and DoRM[3]), our experimental data encompasses both global style ($\\textit{sketch}$ and $\\textit{baby}$) and local attributes ($\\textit{smile}$ and $\\textit{sunglasses}$). The combination of these domains demonstrates the effectiveness of our method since they encompass all types of generative domain adaptation.\n\nTo provide more comprehensive evidence of validity, we conduct additional experiments on $\\textit{Raphael}$ and $\\textit{Caricature}$ for both single and hybrid domain adaptation. As shown in Fig. 15 of Appendix, the results integrate the characteristics from multiple target domains and maintain robust consistency with source domain, which further demonstrates the effectiveness of our method. \n\n**The generalizability to other domains**\n\nTo verify the generalizability of our method to other domains, we conduct experiments on church domain following prior work. We adapt the pre-trained generator from $\\textit{LSUN Church}$ to $\\textit{Van Gogh's house paintings}$, $\\textit{haunted houses}$, and the combination of them. As shown in Fig. 16 of Appendix, the results acquire the corresponding style and showcase the preservation of good cross-domain consistency. This aligns with the results observed in the face domain.\n\n**The advantages and disadvantages of few-shot domain adaptation techniques with discrimination**\n\nFew-shot domain adaptation techniques with discrimination (like DoRM, DualStyleGAN and 3DAvatarGAN) excel in generating images closely resembling the style of the target domain as its discriminator tends to memorize training images. However, they suffer from the notorious issue of model collapse. Especially in few-shot scenarios, they easily overfits to the target images, compromising the diversity of generated images. Additionally, they are challenging to extend into an end-to-end hybrid domain adaptation approach. Their approach to hybrid domain often involves separately training multiple models and interpolating style codes, necessitating multiple model size and training time.\n\n**Why is hybrid domain adaptation necessary in these cases**\n\nImage editing indeed enables local adjustments resembling the original domain, like $\\textit{smile}$ and $\\textit{baby}$. However, in this scenario, our method holds several advantages. \n\nFirstly, our objective is to generate images with attributes of target domain while maintaining considerable diversity, which can be applicable in scenarios like data collection. Image editing, on the other hand, requires an original image as input, rendering it impractical for such applications. \n\nMoreover, hybrid domain adaptation has the capability to generate images from hybrid domain containing multiple target attributes like $\\textit{baby with the style of sketch}$ or $\\textit{smiling person with the style of sketch}$. Image editing, however, lacks the ability to perform such global stylistic modifications, limiting its broader applications.\n\n---\n\nThank you for your insightful comments about our work. We have added additional results with highlighting in the Appendix. Here we provide a point-by-point response to the issues raised by you.\n\n**Why we claim to introduce Hybrid generative DA**\n\nPrior techniques like style-mixing [1] can generate images from hybrid domain via interpolating latent codes. DoRM is just one of those methods. However, they lack a detailed investigation of HDA, which is merely mentioned as an additional feature. For example, they fail to provide systematic definition and proper evaluation metrics to distinguish good from bad.\n\nWith regarding to the method in achieving HDA, they encounter two primary issues:\n\n(1) DoRM is not an end-to-end pipeline to address HDA. They primarily focus on single domain adaptation, necessitating the separate training and interpolating multiple models to accomplish HDA. This may not be the right way to approach this task. \n\n(2) DoRM necessitates intricate tuning of hyperparameters that interpolate between the source and target domains. As demonstrated in the DoRM paper, they assign a weight of 0.005 to the $\\textit{baby}$ domain during single domain adaptation. Varying these hyperparameters significantly affects the outcomes. This complexity and sensitivity in parameter tuning make HDA's tuning more intricate and demanding.\n\n**How sensitive is our method to the domain coefficient $\\alpha$**\n\nIn our method for hybrid domain adaptation, this parameter controls the composition ratio of the attribute from each domain. As depicted in A.4 of Appendix, we use $\\alpha_i = 0.5$ for most experiments without the need for complex and intricate adjustments. To further explore the sensitivity, we conduct the study for simple traversal of $\\alpha_i$. As shown in the Fig. 14 of Appendix, the attributes of generated images transit smoothly between domains. Our method produces the similar attribute blending effect when $\\alpha_i \\in$ \\{0.4, 0.5, 0.6\\} .\n\n**Comparison to methods for diffusion model personalization**\n\nCurrent trend in customized text-to-image models like DreamBooth aim to mimic the appearance of subjects in a given reference set. Similar to ours, DreamBooth fine-tunes a pre-trained generator for the personalization. However, there are two key differences between personalization and HDA.\n\n(1) DreamBooth aims to retain the individuals from the training images, requiring similar individuals across the training set. Differently, our goal is to acquire the attributes of target domain like $\\textit{baby}$ and preserve consistency with the source generator, ensuring that images generated from the same noise exhibit similar individuals.\n\n(2) Dreambooth utilizes text-to-image generator which necessitates intricate and laborious adjustments of prompts to synthesize images with specific attributes. Additionally, certain attributes are challenging to accurately describe using text, such as artistic paintings. Conversely, our adapted model adeptly preserves the domain-specific attributes of reference images without the need for intricate prompt engineering.\n\n**Conceptual comparison to Domain Expansion**\n\nDomain Expansion aims to expand the pre-trained generator to have the capacity to generate images from multiple domains. To the end, it proposes to repurpose dormant directions in latent space for the new domains. While our approach involves training images from multiple target domains, our objective is to create a unseen composition of given target domains that integrates attributes from them. For example, given training images from $\\textit{baby}$ and $\\textit{sketch}$, Domain Expansion aims to expand the generator to have the capacity to generate images from $\\textit{baby}$ or $\\textit{sketch}$. Differently, we aim to adapt the generator to unseen hybrid domain $\\textit{baby with the style of sketch}$.\n\n**Details of the training time comparisons**\n\nIn Table 2, our comparison about training time is fair. Specifically, we measure all the time on a single NVIDIA TITAN GPU. Due to the absence of open-source code for DoRM, we implement it following their description in the paper, based on the official implementation of StyleGAN2-ADA. The Seq method is also developed in a similar manner. As for StyleNADA, we utilize their open-source code for training. For an apples-to-apples comparison, we set the same batch size as 4 and the same resolution as $256 \\times 256$.\n\n[1] Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8110–8119, 2020.\n\n---\n\nThank you for your insightful comments about our work. We have added additional results with highlighting in the Appendix. Here we provide a point-by-point response to the issues raised by you.\n\n**Why is hybrid domain adaptation a meaningful task**\n\nCurrent generative domain adaptation approaches typically employ the discriminator to discern whether generated images belong to the target domain. When we require to generate images with integrated attributes, they need to collect images from the hybrid target domain (e.g., a $\\textit{smiling}$ $\\textit{baby}$ with the style of $\\textit{sketch}$ in Fig. 1). However, these images tend to be more difficult to collect compared with single domain in real-world scenarios. In contrast, HDA only requires collecting data for several independent target domains rather than for each possible combination as mentioned by **Reviewer 2zhA**. Under such circumstances, HDA offers greater flexibility and versatility to adapt the generator to more composite and expansive domain.\n\n**Results using different numbers of shots**\n\nWe perform experiments on lower shots (5-shot and 1-shot) as suggested using various datasets. As shown in Fig. 11 of Appendix, the results of 5-shot are close to those of 10-shot, which integrates the attributes and maintains the consistency with source domain. Although multiple attributes have been learned, the results of 1-shot exhibit relatively lower cross-domain consistency compared with 10-shot and 5-shot.  This is because when there is only one reference image per domain, the subspace in our directional subspace loss degenerates into a single point (see Fig. 2). Then distinct generated images corresponding to different noise tend to converge towards the same image in CLIP's embedding space, which comprises cross-domain consistency as depicted in Section 3.3.\n\n**Results of conditional generation**\n\nAs shown in Fig. 12 of Appendix, we conduct the experiments on conditional generation. Specifically, we collect 10-shot images with $\\textit{red hair}$ and $\\textit{sunglasses}$. Then we use masks to separate these attributes and adapt the generator with masked images for $\\textit{red hair}$ and $\\textit{sunglasses}$ respectively. We can observe that the generated images possess the corresponding attribute for both single DA and hybrid DA. Simultaneously, these images also maintain consistency with source domain.\n    \n**Effect of different pre-trained image encoders**\n\nAs shown in Fig. 13 of Appendix, we conduct experiments on pre-trained Swin and Dinov2 to explore the impact of different image encoders on generated images. Their results all achieve HDA, indicating that our method is agnostic to different pre-trained image encoders. Although they exhibit slight stylistic differences, these are due to their different approaches to extract features into separated subspaces, as depicted in Fig. 8 of Appendix. To converge to the exact realization of the target domain, our method employs the ensemble technique that exploits both Swin and Dinov2. As shown in the figure, the results closely resembles the attributes of the target domain while maintaining the best consistency with source domain.\n\n**Compare with DALL-E and Imagen**\n\nDALL-E and Imagen are both text-to-image models known for their ability to generate images based on natural language descriptions. They exhibit the advantage of autonomously producing realistic images with specific attributes while maintaining considerable diversity. However, in comparison to our generator, they exhibit two primary drawbacks. \n\n(1) They necessitate intricate and laborious adjustments of prompts to generate images with specific attributes. Additionally, certain attributes are challenging to accurately describe using text, such as artistic paintings. Conversely, our generator can preserve the domain-specific attributes of reference images without the need for intricate prompt engineering.\n\n(2) It is hard to control the characters within images via prompts for them. In contrast, our adapted model maintains strong consistency with the source generator, preserving the same characters from the same noise.\n\nBesides, ours differs from the current trend in customized text-to-image models like DreamBooth. DreamBooth aims to retain the individuals from the training images, requiring similar individuals across the training set. Differently, our goal is to acquire the attributes of target domain like $\\textit{baby}$ and preserve consistency with the source generator, ensuring that images generated from the same noise exhibit similar individuals.", "author_response": "Thank you for your insightful comments about our work. We have added additional results with highlighting in the Appendix. Here we provide a point-by-point response to the issues raised by you.\n\n**Why is hybrid domain adaptation a meaningful task**\n\nCurrent generative domain adaptation approaches typically employ the discriminator to discern whether generated images belong to the target domain. When we require to generate images with integrated attributes, they need to collect images from the hybrid target domain (e.g., a $\\textit{smiling}$ $\\textit{baby}$ with the style of $\\textit{sketch}$ in Fig. 1). However, these images tend to be more difficult to collect compared with single domain in real-world scenarios. In contrast, HDA only requires collecting data for several independent target domains rather than for each possible combination as mentioned by **Reviewer 2zhA**. Under such circumstances, HDA offers greater flexibility and versatility to adapt the generator to more composite and expansive domain.\n\n**Results using different numbers of shots**\n\nWe perform experiments on lower shots (5-shot and 1-shot) as suggested using various datasets. As shown in Fig. 11 of Appendix, the results of 5-shot are close to those of 10-shot, which integrates the attributes and maintains the consistency with source domain. Although multiple attributes have been learned, the results of 1-shot exhibit relatively lower cross-domain consistency compared with 10-shot and 5-shot.  This is because when there is only one reference image per domain, the subspace in our directional subspace loss degenerates into a single point (see Fig. 2). Then distinct generated images corresponding to different noise tend to converge towards the same image in CLIP's embedding space, which comprises cross-domain consistency as depicted in Section 3.3.\n\n**Results of conditional generation**\n\nAs shown in Fig. 12 of Appendix, we conduct the experiments on conditional generation. Specifically, we collect 10-shot images with $\\textit{red hair}$ and $\\textit{sunglasses}$. Then we use masks to separate these attributes and adapt the generator with masked images for $\\textit{red hair}$ and $\\textit{sunglasses}$ respectively. We can observe that the generated images possess the corresponding attribute for both single DA and hybrid DA. Simultaneously, these images also maintain consistency with source domain.\n    \n**Effect of different pre-trained image encoders**\n\nAs shown in Fig. 13 of Appendix, we conduct experiments on pre-trained Swin and Dinov2 to explore the impact of different image encoders on generated images. Their results all achieve HDA, indicating that our method is agnostic to different pre-trained image encoders. Although they exhibit slight stylistic differences, these are due to their different approaches to extract features into separated subspaces, as depicted in Fig. 8 of Appendix. To converge to the exact realization of the target domain, our method employs the ensemble technique that exploits both Swin and Dinov2. As shown in the figure, the results closely resembles the attributes of the target domain while maintaining the best consistency with source domain.\n\n**Compare with DALL-E and Imagen**\n\nDALL-E and Imagen are both text-to-image models known for their ability to generate images based on natural language descriptions. They exhibit the advantage of autonomously producing realistic images with specific attributes while maintaining considerable diversity. However, in comparison to our generator, they exhibit two primary drawbacks. \n\n(1) They necessitate intricate and laborious adjustments of prompts to generate images with specific attributes. Additionally, certain attributes are challenging to accurately describe using text, such as artistic paintings. Conversely, our generator can preserve the domain-specific attributes of reference images without the need for intricate prompt engineering.\n\n(2) It is hard to control the characters within images via prompts for them. In contrast, our adapted model maintains strong consistency with the source generator, preserving the same characters from the same noise.\n\nBesides, ours differs from the current trend in customized text-to-image models like DreamBooth. DreamBooth aims to retain the individuals from the training images, requiring similar individuals across the training set. Differently, our goal is to acquire the attributes of target domain like $\\textit{baby}$ and preserve consistency with the source generator, ensuring that images generated from the same noise exhibit similar individuals."}
{"claim": "The paper does not include experiments or analysis showing applicability of the methodology to conditional image generation conditioned on one or more target styles.", "claim_type": "experimental", "paper_id": "FE2e8664Sl", "paper_title": "Few-shot Hybrid Domain Adaptation of Image Generator", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "u2SOmGevjE", "reviewer": "Reviewer_fvXy", "review_text": "Summary: The paper introduces the few-shot generative Hybrid Domain Adaptation (HDA) task for image-based data. It is assumed that an image generator is accessed from a source domain and then from several target domains. Adaptation is performed on all target domains. To perform HDA, the paper presents a discriminator-free approach with a style GAN pre-trained on a source domain. It is adapted to individual target domains using a few shots from each domain and generates an image style showing composite attributes of each target domain separately.  The proposed approach relies on features extracted from standard transformer-based architectures. In addition, distance and direction losses are used to guide the generator to produce images with integrated features of the target domains while preserving the features of the source domain. Experiments are conducted using the FFHQ dataset and three evaluation metrics are used to measure performance and compare with different baselines.\n\nStrengths: - The paper is well written and easy to understand. The method is well described and the experiments are clearly presented. \n\n- The discriminator-free approach presented is interesting as it reduces the computational cost of fitting compared to existing approaches. \n\n- The approach shows promising results for all metrics compared to the baselines on the FFHQ dataset.\n\nWeaknesses: - (Major) The paper does not provide strong arguments as to why hybrid domain adaptation is a meaningful task. It is not clear why the current approaches and benchmarks, e.g. domain adaptation and domain generalisation, are not sufficient to address the problem presented. These points need further elaboration to motivate the paper. \n\n- (Major) results are reported using 10 shots, while it would be interesting to see results using different numbers of shots, e.g. 1-shot and 5-shot.\n\n- It is unclear whether the methodology is applicable to the generation of conditioned image styles. For example, generating an image that is conditioned on one or more styles of the target domain, rather than an image that spans all attributes of the target domains. It could discussed how the proposed methodology generalises to conditional generation. \n\n- A study of the effect of different pre-trained image encoders would add value to the paper.\n\nQuestions: - How would the proposed generator compare with well-known generator models such as DALLE, Imagen and etc.?", "labeling_timestamp": "2026-01-11T16:23:15.271901", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors explicitly report and describe conditional generation experiments (Fig.12) using masked attributes (red hair and sunglasses) and state that generated images exhibit the corresponding attributes for both single and hybrid domain adaptation, directly addressing the reviewer's request for experiments on conditional image generation.", "evidence": "\"As shown in Fig. 12 of Appendix, we conduct the experiments on conditional generation. Specifically, we collect 10-shot images with \\textit{red hair} and \\textit{sunglasses}. Then we use masks to separate these attributes and adapt the generator with masked images for \\textit{red hair} and \\textit{sunglasses} respectively. We can observe that the generated images possess the corresponding attribute for both single DA and hybrid DA. Simultaneously, these images also maintain consistency with source domain.\"", "combined_author_response": "We hope these additional results and discussions can address your concerns. Please let us know if there are any further clarifications that we can offer. We would love to discuss more if any concern still remains.\n\n---\n\nWe hope these additional results and discussions can address your concerns. Please let us know if there are any further clarifications that we can offer. We would love to discuss more if any concern still remains.\n\n---\n\nWe are glad that our responses solve your concerns. Thank you again for your valuable feedback and suggestions!\n\n---\n\nDear Reviewers:\n\nWe sincerely thank you for your great efforts in reviewing this paper. We are pleased to read that our proposed problem is **interesting** and **well-motivated** [2zhA], our method is **intuitive** and **effective** [fvXy, 2zhA], and our results are **compelling** [fvXy, 2zhA, bxKn].\n\nWe have tried our best to address all the mentioned concerns and problems. **Our manuscript has been revised** to include the changes according to all the reviewers’ insightful comments, making our research more robust and accessible.\n\n**As the deadline for Author-Reviewer discussion is approaching**, we are eagerly looking forward to your responses. Please let us know if there are any additional clarifications or experiments that we could offer. We would love to discuss more if any concern still remains. Thanks again for your time!\n\nBest,\n\nAuthors\n\n---\n\n**The editability before and after the domain adaptation**\n\nWe conduct the editing on the images generated by original and adapted model for both single and hybrid domain adaptation. As shown in Fig. 17 of Appendix, the results indicate the adapted generator maintains similar editability like pose to the original generator. This verifies that the our method effectively preserves original generator's attributes. \n\n**Results in 3D GAN setting**\n\nWe conducted experiments using the popular 3D-aware image generation method, EG3D[4]. Specifically, we replace the discriminator as we did in Fig. 2 for both single and hybrid domain adaptation. As shown in Fig. 18 of Appendix, we adapt the pre-trained generator from FFHQ to $\\textit{sunglasses}$ and the hybrid of $\\textit{sunglasses}$ and $\\textit{smile}$ with 10-shot training images per domain. We can observe that the results effectively integrate the attributes and preserve the characters and poses of source domain.\n\n**Comparisons with $\\textit{Mind the GAP}$ and $\\textit{StyleCLIP}$**\n\nWhile Mind the GAP has a similar loss with direction loss, one term of our proposed directional subspace loss, their motivations differ. Mind the GAP is proposed for one-shot domain adaptation, thus representing the entire target domain using the embedding of a single image. In contrast, for the few-shot setting, we propose representing the entire domain using the subspace formed by the embeddings of multiple images. Furthermore, we utilize pre-trained encoders to obtain separated subspaces corresponding to different domains, enabling us to accomplish hybrid domain adaptation. \n\nThe direction loss in StyleCLIP is computed based on the cosine similarity between the generated images and textual prompts within the CLIP embedding space. It does not depend on the source image or domain. In practice, this loss leads to adversarial solutions and sees no benefit from maintaining diversity as depicted in Style-NADA[5]. A mode-collapsed generator producing only one image may be the best minimizer for the distance to a given textual prompt. Differently, our direction loss aims to preserve more characteristics from source domain and maintains its diversity.\n\n[1] Few-shot image generation via cross-domain correspondence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10743–10752, 2021.\n\n[2] Few shot generative model adaption via relaxed spatial structural alignment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11204–11213, 2022.\n\n[3] Domain re-modulation for few-shot generative domain adaptation. arXiv preprint arXiv:2302.02550, 2023.\n\n[4] Efficient geometry-aware 3d generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16123–16133, 2022.\n\n[5] Stylegan-nada: Clip-guided domain adaptation of image generators. arXiv preprint arXiv:2108.00946, 2021.\n\n---\n\nThank you for your insightful comments about our work. We have added additional results with highlighting in the Appendix. Here we provide a point-by-point response to the issues raised by you.\n\n**The results of more domains**\n\nConsistent with prior work for few-shot generative domain adaptation (like CDC[1], RSSA[2], and DoRM[3]), our experimental data encompasses both global style ($\\textit{sketch}$ and $\\textit{baby}$) and local attributes ($\\textit{smile}$ and $\\textit{sunglasses}$). The combination of these domains demonstrates the effectiveness of our method since they encompass all types of generative domain adaptation.\n\nTo provide more comprehensive evidence of validity, we conduct additional experiments on $\\textit{Raphael}$ and $\\textit{Caricature}$ for both single and hybrid domain adaptation. As shown in Fig. 15 of Appendix, the results integrate the characteristics from multiple target domains and maintain robust consistency with source domain, which further demonstrates the effectiveness of our method. \n\n**The generalizability to other domains**\n\nTo verify the generalizability of our method to other domains, we conduct experiments on church domain following prior work. We adapt the pre-trained generator from $\\textit{LSUN Church}$ to $\\textit{Van Gogh's house paintings}$, $\\textit{haunted houses}$, and the combination of them. As shown in Fig. 16 of Appendix, the results acquire the corresponding style and showcase the preservation of good cross-domain consistency. This aligns with the results observed in the face domain.\n\n**The advantages and disadvantages of few-shot domain adaptation techniques with discrimination**\n\nFew-shot domain adaptation techniques with discrimination (like DoRM, DualStyleGAN and 3DAvatarGAN) excel in generating images closely resembling the style of the target domain as its discriminator tends to memorize training images. However, they suffer from the notorious issue of model collapse. Especially in few-shot scenarios, they easily overfits to the target images, compromising the diversity of generated images. Additionally, they are challenging to extend into an end-to-end hybrid domain adaptation approach. Their approach to hybrid domain often involves separately training multiple models and interpolating style codes, necessitating multiple model size and training time.\n\n**Why is hybrid domain adaptation necessary in these cases**\n\nImage editing indeed enables local adjustments resembling the original domain, like $\\textit{smile}$ and $\\textit{baby}$. However, in this scenario, our method holds several advantages. \n\nFirstly, our objective is to generate images with attributes of target domain while maintaining considerable diversity, which can be applicable in scenarios like data collection. Image editing, on the other hand, requires an original image as input, rendering it impractical for such applications. \n\nMoreover, hybrid domain adaptation has the capability to generate images from hybrid domain containing multiple target attributes like $\\textit{baby with the style of sketch}$ or $\\textit{smiling person with the style of sketch}$. Image editing, however, lacks the ability to perform such global stylistic modifications, limiting its broader applications.\n\n---\n\nThank you for your insightful comments about our work. We have added additional results with highlighting in the Appendix. Here we provide a point-by-point response to the issues raised by you.\n\n**Why we claim to introduce Hybrid generative DA**\n\nPrior techniques like style-mixing [1] can generate images from hybrid domain via interpolating latent codes. DoRM is just one of those methods. However, they lack a detailed investigation of HDA, which is merely mentioned as an additional feature. For example, they fail to provide systematic definition and proper evaluation metrics to distinguish good from bad.\n\nWith regarding to the method in achieving HDA, they encounter two primary issues:\n\n(1) DoRM is not an end-to-end pipeline to address HDA. They primarily focus on single domain adaptation, necessitating the separate training and interpolating multiple models to accomplish HDA. This may not be the right way to approach this task. \n\n(2) DoRM necessitates intricate tuning of hyperparameters that interpolate between the source and target domains. As demonstrated in the DoRM paper, they assign a weight of 0.005 to the $\\textit{baby}$ domain during single domain adaptation. Varying these hyperparameters significantly affects the outcomes. This complexity and sensitivity in parameter tuning make HDA's tuning more intricate and demanding.\n\n**How sensitive is our method to the domain coefficient $\\alpha$**\n\nIn our method for hybrid domain adaptation, this parameter controls the composition ratio of the attribute from each domain. As depicted in A.4 of Appendix, we use $\\alpha_i = 0.5$ for most experiments without the need for complex and intricate adjustments. To further explore the sensitivity, we conduct the study for simple traversal of $\\alpha_i$. As shown in the Fig. 14 of Appendix, the attributes of generated images transit smoothly between domains. Our method produces the similar attribute blending effect when $\\alpha_i \\in$ \\{0.4, 0.5, 0.6\\} .\n\n**Comparison to methods for diffusion model personalization**\n\nCurrent trend in customized text-to-image models like DreamBooth aim to mimic the appearance of subjects in a given reference set. Similar to ours, DreamBooth fine-tunes a pre-trained generator for the personalization. However, there are two key differences between personalization and HDA.\n\n(1) DreamBooth aims to retain the individuals from the training images, requiring similar individuals across the training set. Differently, our goal is to acquire the attributes of target domain like $\\textit{baby}$ and preserve consistency with the source generator, ensuring that images generated from the same noise exhibit similar individuals.\n\n(2) Dreambooth utilizes text-to-image generator which necessitates intricate and laborious adjustments of prompts to synthesize images with specific attributes. Additionally, certain attributes are challenging to accurately describe using text, such as artistic paintings. Conversely, our adapted model adeptly preserves the domain-specific attributes of reference images without the need for intricate prompt engineering.\n\n**Conceptual comparison to Domain Expansion**\n\nDomain Expansion aims to expand the pre-trained generator to have the capacity to generate images from multiple domains. To the end, it proposes to repurpose dormant directions in latent space for the new domains. While our approach involves training images from multiple target domains, our objective is to create a unseen composition of given target domains that integrates attributes from them. For example, given training images from $\\textit{baby}$ and $\\textit{sketch}$, Domain Expansion aims to expand the generator to have the capacity to generate images from $\\textit{baby}$ or $\\textit{sketch}$. Differently, we aim to adapt the generator to unseen hybrid domain $\\textit{baby with the style of sketch}$.\n\n**Details of the training time comparisons**\n\nIn Table 2, our comparison about training time is fair. Specifically, we measure all the time on a single NVIDIA TITAN GPU. Due to the absence of open-source code for DoRM, we implement it following their description in the paper, based on the official implementation of StyleGAN2-ADA. The Seq method is also developed in a similar manner. As for StyleNADA, we utilize their open-source code for training. For an apples-to-apples comparison, we set the same batch size as 4 and the same resolution as $256 \\times 256$.\n\n[1] Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8110–8119, 2020.\n\n---\n\nThank you for your insightful comments about our work. We have added additional results with highlighting in the Appendix. Here we provide a point-by-point response to the issues raised by you.\n\n**Why is hybrid domain adaptation a meaningful task**\n\nCurrent generative domain adaptation approaches typically employ the discriminator to discern whether generated images belong to the target domain. When we require to generate images with integrated attributes, they need to collect images from the hybrid target domain (e.g., a $\\textit{smiling}$ $\\textit{baby}$ with the style of $\\textit{sketch}$ in Fig. 1). However, these images tend to be more difficult to collect compared with single domain in real-world scenarios. In contrast, HDA only requires collecting data for several independent target domains rather than for each possible combination as mentioned by **Reviewer 2zhA**. Under such circumstances, HDA offers greater flexibility and versatility to adapt the generator to more composite and expansive domain.\n\n**Results using different numbers of shots**\n\nWe perform experiments on lower shots (5-shot and 1-shot) as suggested using various datasets. As shown in Fig. 11 of Appendix, the results of 5-shot are close to those of 10-shot, which integrates the attributes and maintains the consistency with source domain. Although multiple attributes have been learned, the results of 1-shot exhibit relatively lower cross-domain consistency compared with 10-shot and 5-shot.  This is because when there is only one reference image per domain, the subspace in our directional subspace loss degenerates into a single point (see Fig. 2). Then distinct generated images corresponding to different noise tend to converge towards the same image in CLIP's embedding space, which comprises cross-domain consistency as depicted in Section 3.3.\n\n**Results of conditional generation**\n\nAs shown in Fig. 12 of Appendix, we conduct the experiments on conditional generation. Specifically, we collect 10-shot images with $\\textit{red hair}$ and $\\textit{sunglasses}$. Then we use masks to separate these attributes and adapt the generator with masked images for $\\textit{red hair}$ and $\\textit{sunglasses}$ respectively. We can observe that the generated images possess the corresponding attribute for both single DA and hybrid DA. Simultaneously, these images also maintain consistency with source domain.\n    \n**Effect of different pre-trained image encoders**\n\nAs shown in Fig. 13 of Appendix, we conduct experiments on pre-trained Swin and Dinov2 to explore the impact of different image encoders on generated images. Their results all achieve HDA, indicating that our method is agnostic to different pre-trained image encoders. Although they exhibit slight stylistic differences, these are due to their different approaches to extract features into separated subspaces, as depicted in Fig. 8 of Appendix. To converge to the exact realization of the target domain, our method employs the ensemble technique that exploits both Swin and Dinov2. As shown in the figure, the results closely resembles the attributes of the target domain while maintaining the best consistency with source domain.\n\n**Compare with DALL-E and Imagen**\n\nDALL-E and Imagen are both text-to-image models known for their ability to generate images based on natural language descriptions. They exhibit the advantage of autonomously producing realistic images with specific attributes while maintaining considerable diversity. However, in comparison to our generator, they exhibit two primary drawbacks. \n\n(1) They necessitate intricate and laborious adjustments of prompts to generate images with specific attributes. Additionally, certain attributes are challenging to accurately describe using text, such as artistic paintings. Conversely, our generator can preserve the domain-specific attributes of reference images without the need for intricate prompt engineering.\n\n(2) It is hard to control the characters within images via prompts for them. In contrast, our adapted model maintains strong consistency with the source generator, preserving the same characters from the same noise.\n\nBesides, ours differs from the current trend in customized text-to-image models like DreamBooth. DreamBooth aims to retain the individuals from the training images, requiring similar individuals across the training set. Differently, our goal is to acquire the attributes of target domain like $\\textit{baby}$ and preserve consistency with the source generator, ensuring that images generated from the same noise exhibit similar individuals.", "author_response": "Thank you for your insightful comments about our work. We have added additional results with highlighting in the Appendix. Here we provide a point-by-point response to the issues raised by you.\n\n**Why is hybrid domain adaptation a meaningful task**\n\nCurrent generative domain adaptation approaches typically employ the discriminator to discern whether generated images belong to the target domain. When we require to generate images with integrated attributes, they need to collect images from the hybrid target domain (e.g., a $\\textit{smiling}$ $\\textit{baby}$ with the style of $\\textit{sketch}$ in Fig. 1). However, these images tend to be more difficult to collect compared with single domain in real-world scenarios. In contrast, HDA only requires collecting data for several independent target domains rather than for each possible combination as mentioned by **Reviewer 2zhA**. Under such circumstances, HDA offers greater flexibility and versatility to adapt the generator to more composite and expansive domain.\n\n**Results using different numbers of shots**\n\nWe perform experiments on lower shots (5-shot and 1-shot) as suggested using various datasets. As shown in Fig. 11 of Appendix, the results of 5-shot are close to those of 10-shot, which integrates the attributes and maintains the consistency with source domain. Although multiple attributes have been learned, the results of 1-shot exhibit relatively lower cross-domain consistency compared with 10-shot and 5-shot.  This is because when there is only one reference image per domain, the subspace in our directional subspace loss degenerates into a single point (see Fig. 2). Then distinct generated images corresponding to different noise tend to converge towards the same image in CLIP's embedding space, which comprises cross-domain consistency as depicted in Section 3.3.\n\n**Results of conditional generation**\n\nAs shown in Fig. 12 of Appendix, we conduct the experiments on conditional generation. Specifically, we collect 10-shot images with $\\textit{red hair}$ and $\\textit{sunglasses}$. Then we use masks to separate these attributes and adapt the generator with masked images for $\\textit{red hair}$ and $\\textit{sunglasses}$ respectively. We can observe that the generated images possess the corresponding attribute for both single DA and hybrid DA. Simultaneously, these images also maintain consistency with source domain.\n    \n**Effect of different pre-trained image encoders**\n\nAs shown in Fig. 13 of Appendix, we conduct experiments on pre-trained Swin and Dinov2 to explore the impact of different image encoders on generated images. Their results all achieve HDA, indicating that our method is agnostic to different pre-trained image encoders. Although they exhibit slight stylistic differences, these are due to their different approaches to extract features into separated subspaces, as depicted in Fig. 8 of Appendix. To converge to the exact realization of the target domain, our method employs the ensemble technique that exploits both Swin and Dinov2. As shown in the figure, the results closely resembles the attributes of the target domain while maintaining the best consistency with source domain.\n\n**Compare with DALL-E and Imagen**\n\nDALL-E and Imagen are both text-to-image models known for their ability to generate images based on natural language descriptions. They exhibit the advantage of autonomously producing realistic images with specific attributes while maintaining considerable diversity. However, in comparison to our generator, they exhibit two primary drawbacks. \n\n(1) They necessitate intricate and laborious adjustments of prompts to generate images with specific attributes. Additionally, certain attributes are challenging to accurately describe using text, such as artistic paintings. Conversely, our generator can preserve the domain-specific attributes of reference images without the need for intricate prompt engineering.\n\n(2) It is hard to control the characters within images via prompts for them. In contrast, our adapted model maintains strong consistency with the source generator, preserving the same characters from the same noise.\n\nBesides, ours differs from the current trend in customized text-to-image models like DreamBooth. DreamBooth aims to retain the individuals from the training images, requiring similar individuals across the training set. Differently, our goal is to acquire the attributes of target domain like $\\textit{baby}$ and preserve consistency with the source generator, ensuring that images generated from the same noise exhibit similar individuals."}
{"claim": "The paper fails to discuss how the proposed methodology generalizes to conditional generation scenarios rather than producing images spanning all target-domain attributes.", "claim_type": "methodology", "paper_id": "FE2e8664Sl", "paper_title": "Few-shot Hybrid Domain Adaptation of Image Generator", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "u2SOmGevjE", "reviewer": "Reviewer_fvXy", "review_text": "Summary: The paper introduces the few-shot generative Hybrid Domain Adaptation (HDA) task for image-based data. It is assumed that an image generator is accessed from a source domain and then from several target domains. Adaptation is performed on all target domains. To perform HDA, the paper presents a discriminator-free approach with a style GAN pre-trained on a source domain. It is adapted to individual target domains using a few shots from each domain and generates an image style showing composite attributes of each target domain separately.  The proposed approach relies on features extracted from standard transformer-based architectures. In addition, distance and direction losses are used to guide the generator to produce images with integrated features of the target domains while preserving the features of the source domain. Experiments are conducted using the FFHQ dataset and three evaluation metrics are used to measure performance and compare with different baselines.\n\nStrengths: - The paper is well written and easy to understand. The method is well described and the experiments are clearly presented. \n\n- The discriminator-free approach presented is interesting as it reduces the computational cost of fitting compared to existing approaches. \n\n- The approach shows promising results for all metrics compared to the baselines on the FFHQ dataset.\n\nWeaknesses: - (Major) The paper does not provide strong arguments as to why hybrid domain adaptation is a meaningful task. It is not clear why the current approaches and benchmarks, e.g. domain adaptation and domain generalisation, are not sufficient to address the problem presented. These points need further elaboration to motivate the paper. \n\n- (Major) results are reported using 10 shots, while it would be interesting to see results using different numbers of shots, e.g. 1-shot and 5-shot.\n\n- It is unclear whether the methodology is applicable to the generation of conditioned image styles. For example, generating an image that is conditioned on one or more styles of the target domain, rather than an image that spans all attributes of the target domains. It could discussed how the proposed methodology generalises to conditional generation. \n\n- A study of the effect of different pre-trained image encoders would add value to the paper.\n\nQuestions: - How would the proposed generator compare with well-known generator models such as DALLE, Imagen and etc.?", "labeling_timestamp": "2026-01-11T16:23:02.796531", "model": "gpt-5-mini", "label": "Supported", "justification": "Authors explicitly report experiments on conditional generation, describe the setup (masked attributes, 10-shot), and state that generated images possess the corresponding attributes while maintaining source-domain consistency, directly addressing the reviewer's concern.", "evidence": "\"Results of conditional generation\\n\\nAs shown in Fig. 12 of Appendix, we conduct the experiments on conditional generation. Specifically, we collect 10-shot images with \\textit{red hair} and \\textit{sunglasses}. Then we use masks to separate these attributes and adapt the generator with masked images for \\textit{red hair} and \\textit{sunglasses} respectively. We can observe that the generated images possess the corresponding attribute for both single DA and hybrid DA. Simultaneously, these images also maintain consistency with source domain.\"", "combined_author_response": "We hope these additional results and discussions can address your concerns. Please let us know if there are any further clarifications that we can offer. We would love to discuss more if any concern still remains.\n\n---\n\nWe hope these additional results and discussions can address your concerns. Please let us know if there are any further clarifications that we can offer. We would love to discuss more if any concern still remains.\n\n---\n\nWe are glad that our responses solve your concerns. Thank you again for your valuable feedback and suggestions!\n\n---\n\nDear Reviewers:\n\nWe sincerely thank you for your great efforts in reviewing this paper. We are pleased to read that our proposed problem is **interesting** and **well-motivated** [2zhA], our method is **intuitive** and **effective** [fvXy, 2zhA], and our results are **compelling** [fvXy, 2zhA, bxKn].\n\nWe have tried our best to address all the mentioned concerns and problems. **Our manuscript has been revised** to include the changes according to all the reviewers’ insightful comments, making our research more robust and accessible.\n\n**As the deadline for Author-Reviewer discussion is approaching**, we are eagerly looking forward to your responses. Please let us know if there are any additional clarifications or experiments that we could offer. We would love to discuss more if any concern still remains. Thanks again for your time!\n\nBest,\n\nAuthors\n\n---\n\n**The editability before and after the domain adaptation**\n\nWe conduct the editing on the images generated by original and adapted model for both single and hybrid domain adaptation. As shown in Fig. 17 of Appendix, the results indicate the adapted generator maintains similar editability like pose to the original generator. This verifies that the our method effectively preserves original generator's attributes. \n\n**Results in 3D GAN setting**\n\nWe conducted experiments using the popular 3D-aware image generation method, EG3D[4]. Specifically, we replace the discriminator as we did in Fig. 2 for both single and hybrid domain adaptation. As shown in Fig. 18 of Appendix, we adapt the pre-trained generator from FFHQ to $\\textit{sunglasses}$ and the hybrid of $\\textit{sunglasses}$ and $\\textit{smile}$ with 10-shot training images per domain. We can observe that the results effectively integrate the attributes and preserve the characters and poses of source domain.\n\n**Comparisons with $\\textit{Mind the GAP}$ and $\\textit{StyleCLIP}$**\n\nWhile Mind the GAP has a similar loss with direction loss, one term of our proposed directional subspace loss, their motivations differ. Mind the GAP is proposed for one-shot domain adaptation, thus representing the entire target domain using the embedding of a single image. In contrast, for the few-shot setting, we propose representing the entire domain using the subspace formed by the embeddings of multiple images. Furthermore, we utilize pre-trained encoders to obtain separated subspaces corresponding to different domains, enabling us to accomplish hybrid domain adaptation. \n\nThe direction loss in StyleCLIP is computed based on the cosine similarity between the generated images and textual prompts within the CLIP embedding space. It does not depend on the source image or domain. In practice, this loss leads to adversarial solutions and sees no benefit from maintaining diversity as depicted in Style-NADA[5]. A mode-collapsed generator producing only one image may be the best minimizer for the distance to a given textual prompt. Differently, our direction loss aims to preserve more characteristics from source domain and maintains its diversity.\n\n[1] Few-shot image generation via cross-domain correspondence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10743–10752, 2021.\n\n[2] Few shot generative model adaption via relaxed spatial structural alignment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11204–11213, 2022.\n\n[3] Domain re-modulation for few-shot generative domain adaptation. arXiv preprint arXiv:2302.02550, 2023.\n\n[4] Efficient geometry-aware 3d generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16123–16133, 2022.\n\n[5] Stylegan-nada: Clip-guided domain adaptation of image generators. arXiv preprint arXiv:2108.00946, 2021.\n\n---\n\nThank you for your insightful comments about our work. We have added additional results with highlighting in the Appendix. Here we provide a point-by-point response to the issues raised by you.\n\n**The results of more domains**\n\nConsistent with prior work for few-shot generative domain adaptation (like CDC[1], RSSA[2], and DoRM[3]), our experimental data encompasses both global style ($\\textit{sketch}$ and $\\textit{baby}$) and local attributes ($\\textit{smile}$ and $\\textit{sunglasses}$). The combination of these domains demonstrates the effectiveness of our method since they encompass all types of generative domain adaptation.\n\nTo provide more comprehensive evidence of validity, we conduct additional experiments on $\\textit{Raphael}$ and $\\textit{Caricature}$ for both single and hybrid domain adaptation. As shown in Fig. 15 of Appendix, the results integrate the characteristics from multiple target domains and maintain robust consistency with source domain, which further demonstrates the effectiveness of our method. \n\n**The generalizability to other domains**\n\nTo verify the generalizability of our method to other domains, we conduct experiments on church domain following prior work. We adapt the pre-trained generator from $\\textit{LSUN Church}$ to $\\textit{Van Gogh's house paintings}$, $\\textit{haunted houses}$, and the combination of them. As shown in Fig. 16 of Appendix, the results acquire the corresponding style and showcase the preservation of good cross-domain consistency. This aligns with the results observed in the face domain.\n\n**The advantages and disadvantages of few-shot domain adaptation techniques with discrimination**\n\nFew-shot domain adaptation techniques with discrimination (like DoRM, DualStyleGAN and 3DAvatarGAN) excel in generating images closely resembling the style of the target domain as its discriminator tends to memorize training images. However, they suffer from the notorious issue of model collapse. Especially in few-shot scenarios, they easily overfits to the target images, compromising the diversity of generated images. Additionally, they are challenging to extend into an end-to-end hybrid domain adaptation approach. Their approach to hybrid domain often involves separately training multiple models and interpolating style codes, necessitating multiple model size and training time.\n\n**Why is hybrid domain adaptation necessary in these cases**\n\nImage editing indeed enables local adjustments resembling the original domain, like $\\textit{smile}$ and $\\textit{baby}$. However, in this scenario, our method holds several advantages. \n\nFirstly, our objective is to generate images with attributes of target domain while maintaining considerable diversity, which can be applicable in scenarios like data collection. Image editing, on the other hand, requires an original image as input, rendering it impractical for such applications. \n\nMoreover, hybrid domain adaptation has the capability to generate images from hybrid domain containing multiple target attributes like $\\textit{baby with the style of sketch}$ or $\\textit{smiling person with the style of sketch}$. Image editing, however, lacks the ability to perform such global stylistic modifications, limiting its broader applications.\n\n---\n\nThank you for your insightful comments about our work. We have added additional results with highlighting in the Appendix. Here we provide a point-by-point response to the issues raised by you.\n\n**Why we claim to introduce Hybrid generative DA**\n\nPrior techniques like style-mixing [1] can generate images from hybrid domain via interpolating latent codes. DoRM is just one of those methods. However, they lack a detailed investigation of HDA, which is merely mentioned as an additional feature. For example, they fail to provide systematic definition and proper evaluation metrics to distinguish good from bad.\n\nWith regarding to the method in achieving HDA, they encounter two primary issues:\n\n(1) DoRM is not an end-to-end pipeline to address HDA. They primarily focus on single domain adaptation, necessitating the separate training and interpolating multiple models to accomplish HDA. This may not be the right way to approach this task. \n\n(2) DoRM necessitates intricate tuning of hyperparameters that interpolate between the source and target domains. As demonstrated in the DoRM paper, they assign a weight of 0.005 to the $\\textit{baby}$ domain during single domain adaptation. Varying these hyperparameters significantly affects the outcomes. This complexity and sensitivity in parameter tuning make HDA's tuning more intricate and demanding.\n\n**How sensitive is our method to the domain coefficient $\\alpha$**\n\nIn our method for hybrid domain adaptation, this parameter controls the composition ratio of the attribute from each domain. As depicted in A.4 of Appendix, we use $\\alpha_i = 0.5$ for most experiments without the need for complex and intricate adjustments. To further explore the sensitivity, we conduct the study for simple traversal of $\\alpha_i$. As shown in the Fig. 14 of Appendix, the attributes of generated images transit smoothly between domains. Our method produces the similar attribute blending effect when $\\alpha_i \\in$ \\{0.4, 0.5, 0.6\\} .\n\n**Comparison to methods for diffusion model personalization**\n\nCurrent trend in customized text-to-image models like DreamBooth aim to mimic the appearance of subjects in a given reference set. Similar to ours, DreamBooth fine-tunes a pre-trained generator for the personalization. However, there are two key differences between personalization and HDA.\n\n(1) DreamBooth aims to retain the individuals from the training images, requiring similar individuals across the training set. Differently, our goal is to acquire the attributes of target domain like $\\textit{baby}$ and preserve consistency with the source generator, ensuring that images generated from the same noise exhibit similar individuals.\n\n(2) Dreambooth utilizes text-to-image generator which necessitates intricate and laborious adjustments of prompts to synthesize images with specific attributes. Additionally, certain attributes are challenging to accurately describe using text, such as artistic paintings. Conversely, our adapted model adeptly preserves the domain-specific attributes of reference images without the need for intricate prompt engineering.\n\n**Conceptual comparison to Domain Expansion**\n\nDomain Expansion aims to expand the pre-trained generator to have the capacity to generate images from multiple domains. To the end, it proposes to repurpose dormant directions in latent space for the new domains. While our approach involves training images from multiple target domains, our objective is to create a unseen composition of given target domains that integrates attributes from them. For example, given training images from $\\textit{baby}$ and $\\textit{sketch}$, Domain Expansion aims to expand the generator to have the capacity to generate images from $\\textit{baby}$ or $\\textit{sketch}$. Differently, we aim to adapt the generator to unseen hybrid domain $\\textit{baby with the style of sketch}$.\n\n**Details of the training time comparisons**\n\nIn Table 2, our comparison about training time is fair. Specifically, we measure all the time on a single NVIDIA TITAN GPU. Due to the absence of open-source code for DoRM, we implement it following their description in the paper, based on the official implementation of StyleGAN2-ADA. The Seq method is also developed in a similar manner. As for StyleNADA, we utilize their open-source code for training. For an apples-to-apples comparison, we set the same batch size as 4 and the same resolution as $256 \\times 256$.\n\n[1] Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8110–8119, 2020.\n\n---\n\nThank you for your insightful comments about our work. We have added additional results with highlighting in the Appendix. Here we provide a point-by-point response to the issues raised by you.\n\n**Why is hybrid domain adaptation a meaningful task**\n\nCurrent generative domain adaptation approaches typically employ the discriminator to discern whether generated images belong to the target domain. When we require to generate images with integrated attributes, they need to collect images from the hybrid target domain (e.g., a $\\textit{smiling}$ $\\textit{baby}$ with the style of $\\textit{sketch}$ in Fig. 1). However, these images tend to be more difficult to collect compared with single domain in real-world scenarios. In contrast, HDA only requires collecting data for several independent target domains rather than for each possible combination as mentioned by **Reviewer 2zhA**. Under such circumstances, HDA offers greater flexibility and versatility to adapt the generator to more composite and expansive domain.\n\n**Results using different numbers of shots**\n\nWe perform experiments on lower shots (5-shot and 1-shot) as suggested using various datasets. As shown in Fig. 11 of Appendix, the results of 5-shot are close to those of 10-shot, which integrates the attributes and maintains the consistency with source domain. Although multiple attributes have been learned, the results of 1-shot exhibit relatively lower cross-domain consistency compared with 10-shot and 5-shot.  This is because when there is only one reference image per domain, the subspace in our directional subspace loss degenerates into a single point (see Fig. 2). Then distinct generated images corresponding to different noise tend to converge towards the same image in CLIP's embedding space, which comprises cross-domain consistency as depicted in Section 3.3.\n\n**Results of conditional generation**\n\nAs shown in Fig. 12 of Appendix, we conduct the experiments on conditional generation. Specifically, we collect 10-shot images with $\\textit{red hair}$ and $\\textit{sunglasses}$. Then we use masks to separate these attributes and adapt the generator with masked images for $\\textit{red hair}$ and $\\textit{sunglasses}$ respectively. We can observe that the generated images possess the corresponding attribute for both single DA and hybrid DA. Simultaneously, these images also maintain consistency with source domain.\n    \n**Effect of different pre-trained image encoders**\n\nAs shown in Fig. 13 of Appendix, we conduct experiments on pre-trained Swin and Dinov2 to explore the impact of different image encoders on generated images. Their results all achieve HDA, indicating that our method is agnostic to different pre-trained image encoders. Although they exhibit slight stylistic differences, these are due to their different approaches to extract features into separated subspaces, as depicted in Fig. 8 of Appendix. To converge to the exact realization of the target domain, our method employs the ensemble technique that exploits both Swin and Dinov2. As shown in the figure, the results closely resembles the attributes of the target domain while maintaining the best consistency with source domain.\n\n**Compare with DALL-E and Imagen**\n\nDALL-E and Imagen are both text-to-image models known for their ability to generate images based on natural language descriptions. They exhibit the advantage of autonomously producing realistic images with specific attributes while maintaining considerable diversity. However, in comparison to our generator, they exhibit two primary drawbacks. \n\n(1) They necessitate intricate and laborious adjustments of prompts to generate images with specific attributes. Additionally, certain attributes are challenging to accurately describe using text, such as artistic paintings. Conversely, our generator can preserve the domain-specific attributes of reference images without the need for intricate prompt engineering.\n\n(2) It is hard to control the characters within images via prompts for them. In contrast, our adapted model maintains strong consistency with the source generator, preserving the same characters from the same noise.\n\nBesides, ours differs from the current trend in customized text-to-image models like DreamBooth. DreamBooth aims to retain the individuals from the training images, requiring similar individuals across the training set. Differently, our goal is to acquire the attributes of target domain like $\\textit{baby}$ and preserve consistency with the source generator, ensuring that images generated from the same noise exhibit similar individuals.", "author_response": "Thank you for your insightful comments about our work. We have added additional results with highlighting in the Appendix. Here we provide a point-by-point response to the issues raised by you.\n\n**Why is hybrid domain adaptation a meaningful task**\n\nCurrent generative domain adaptation approaches typically employ the discriminator to discern whether generated images belong to the target domain. When we require to generate images with integrated attributes, they need to collect images from the hybrid target domain (e.g., a $\\textit{smiling}$ $\\textit{baby}$ with the style of $\\textit{sketch}$ in Fig. 1). However, these images tend to be more difficult to collect compared with single domain in real-world scenarios. In contrast, HDA only requires collecting data for several independent target domains rather than for each possible combination as mentioned by **Reviewer 2zhA**. Under such circumstances, HDA offers greater flexibility and versatility to adapt the generator to more composite and expansive domain.\n\n**Results using different numbers of shots**\n\nWe perform experiments on lower shots (5-shot and 1-shot) as suggested using various datasets. As shown in Fig. 11 of Appendix, the results of 5-shot are close to those of 10-shot, which integrates the attributes and maintains the consistency with source domain. Although multiple attributes have been learned, the results of 1-shot exhibit relatively lower cross-domain consistency compared with 10-shot and 5-shot.  This is because when there is only one reference image per domain, the subspace in our directional subspace loss degenerates into a single point (see Fig. 2). Then distinct generated images corresponding to different noise tend to converge towards the same image in CLIP's embedding space, which comprises cross-domain consistency as depicted in Section 3.3.\n\n**Results of conditional generation**\n\nAs shown in Fig. 12 of Appendix, we conduct the experiments on conditional generation. Specifically, we collect 10-shot images with $\\textit{red hair}$ and $\\textit{sunglasses}$. Then we use masks to separate these attributes and adapt the generator with masked images for $\\textit{red hair}$ and $\\textit{sunglasses}$ respectively. We can observe that the generated images possess the corresponding attribute for both single DA and hybrid DA. Simultaneously, these images also maintain consistency with source domain.\n    \n**Effect of different pre-trained image encoders**\n\nAs shown in Fig. 13 of Appendix, we conduct experiments on pre-trained Swin and Dinov2 to explore the impact of different image encoders on generated images. Their results all achieve HDA, indicating that our method is agnostic to different pre-trained image encoders. Although they exhibit slight stylistic differences, these are due to their different approaches to extract features into separated subspaces, as depicted in Fig. 8 of Appendix. To converge to the exact realization of the target domain, our method employs the ensemble technique that exploits both Swin and Dinov2. As shown in the figure, the results closely resembles the attributes of the target domain while maintaining the best consistency with source domain.\n\n**Compare with DALL-E and Imagen**\n\nDALL-E and Imagen are both text-to-image models known for their ability to generate images based on natural language descriptions. They exhibit the advantage of autonomously producing realistic images with specific attributes while maintaining considerable diversity. However, in comparison to our generator, they exhibit two primary drawbacks. \n\n(1) They necessitate intricate and laborious adjustments of prompts to generate images with specific attributes. Additionally, certain attributes are challenging to accurately describe using text, such as artistic paintings. Conversely, our generator can preserve the domain-specific attributes of reference images without the need for intricate prompt engineering.\n\n(2) It is hard to control the characters within images via prompts for them. In contrast, our adapted model maintains strong consistency with the source generator, preserving the same characters from the same noise.\n\nBesides, ours differs from the current trend in customized text-to-image models like DreamBooth. DreamBooth aims to retain the individuals from the training images, requiring similar individuals across the training set. Differently, our goal is to acquire the attributes of target domain like $\\textit{baby}$ and preserve consistency with the source generator, ensuring that images generated from the same noise exhibit similar individuals."}
{"claim": "The paper lacks a study of the effect of using different pre-trained image encoders on the method's performance and robustness.", "claim_type": "experimental", "paper_id": "FE2e8664Sl", "paper_title": "Few-shot Hybrid Domain Adaptation of Image Generator", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "u2SOmGevjE", "reviewer": "Reviewer_fvXy", "review_text": "Summary: The paper introduces the few-shot generative Hybrid Domain Adaptation (HDA) task for image-based data. It is assumed that an image generator is accessed from a source domain and then from several target domains. Adaptation is performed on all target domains. To perform HDA, the paper presents a discriminator-free approach with a style GAN pre-trained on a source domain. It is adapted to individual target domains using a few shots from each domain and generates an image style showing composite attributes of each target domain separately.  The proposed approach relies on features extracted from standard transformer-based architectures. In addition, distance and direction losses are used to guide the generator to produce images with integrated features of the target domains while preserving the features of the source domain. Experiments are conducted using the FFHQ dataset and three evaluation metrics are used to measure performance and compare with different baselines.\n\nStrengths: - The paper is well written and easy to understand. The method is well described and the experiments are clearly presented. \n\n- The discriminator-free approach presented is interesting as it reduces the computational cost of fitting compared to existing approaches. \n\n- The approach shows promising results for all metrics compared to the baselines on the FFHQ dataset.\n\nWeaknesses: - (Major) The paper does not provide strong arguments as to why hybrid domain adaptation is a meaningful task. It is not clear why the current approaches and benchmarks, e.g. domain adaptation and domain generalisation, are not sufficient to address the problem presented. These points need further elaboration to motivate the paper. \n\n- (Major) results are reported using 10 shots, while it would be interesting to see results using different numbers of shots, e.g. 1-shot and 5-shot.\n\n- It is unclear whether the methodology is applicable to the generation of conditioned image styles. For example, generating an image that is conditioned on one or more styles of the target domain, rather than an image that spans all attributes of the target domains. It could discussed how the proposed methodology generalises to conditional generation. \n\n- A study of the effect of different pre-trained image encoders would add value to the paper.\n\nQuestions: - How would the proposed generator compare with well-known generator models such as DALLE, Imagen and etc.?", "labeling_timestamp": "2026-01-11T16:23:25.038965", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors explicitly report experiments using different pre-trained image encoders (Swin and Dinov2), state that results achieve HDA and that the method is agnostic to different encoders, and describe slight differences and an ensemble approach—directly addressing the reviewer's concern.", "evidence": "\"Effect of different pre-trained image encoders\"\n\n\"As shown in Fig. 13 of Appendix, we conduct experiments on pre-trained Swin and Dinov2 to explore the impact of different image encoders on generated images. Their results all achieve HDA, indicating that our method is agnostic to different pre-trained image encoders. Although they exhibit slight stylistic differences, these are due to their different approaches to extract features into separated subspaces, as depicted in Fig. 8 of Appendix. To converge to the exact realization of the target domain, our method employs the ensemble technique that exploits both Swin and Dinov2. As shown in the figure, the results closely resembles the attributes of the target domain while maintaining the best consistency with source domain.\"", "combined_author_response": "We hope these additional results and discussions can address your concerns. Please let us know if there are any further clarifications that we can offer. We would love to discuss more if any concern still remains.\n\n---\n\nWe hope these additional results and discussions can address your concerns. Please let us know if there are any further clarifications that we can offer. We would love to discuss more if any concern still remains.\n\n---\n\nWe are glad that our responses solve your concerns. Thank you again for your valuable feedback and suggestions!\n\n---\n\nDear Reviewers:\n\nWe sincerely thank you for your great efforts in reviewing this paper. We are pleased to read that our proposed problem is **interesting** and **well-motivated** [2zhA], our method is **intuitive** and **effective** [fvXy, 2zhA], and our results are **compelling** [fvXy, 2zhA, bxKn].\n\nWe have tried our best to address all the mentioned concerns and problems. **Our manuscript has been revised** to include the changes according to all the reviewers’ insightful comments, making our research more robust and accessible.\n\n**As the deadline for Author-Reviewer discussion is approaching**, we are eagerly looking forward to your responses. Please let us know if there are any additional clarifications or experiments that we could offer. We would love to discuss more if any concern still remains. Thanks again for your time!\n\nBest,\n\nAuthors\n\n---\n\n**The editability before and after the domain adaptation**\n\nWe conduct the editing on the images generated by original and adapted model for both single and hybrid domain adaptation. As shown in Fig. 17 of Appendix, the results indicate the adapted generator maintains similar editability like pose to the original generator. This verifies that the our method effectively preserves original generator's attributes. \n\n**Results in 3D GAN setting**\n\nWe conducted experiments using the popular 3D-aware image generation method, EG3D[4]. Specifically, we replace the discriminator as we did in Fig. 2 for both single and hybrid domain adaptation. As shown in Fig. 18 of Appendix, we adapt the pre-trained generator from FFHQ to $\\textit{sunglasses}$ and the hybrid of $\\textit{sunglasses}$ and $\\textit{smile}$ with 10-shot training images per domain. We can observe that the results effectively integrate the attributes and preserve the characters and poses of source domain.\n\n**Comparisons with $\\textit{Mind the GAP}$ and $\\textit{StyleCLIP}$**\n\nWhile Mind the GAP has a similar loss with direction loss, one term of our proposed directional subspace loss, their motivations differ. Mind the GAP is proposed for one-shot domain adaptation, thus representing the entire target domain using the embedding of a single image. In contrast, for the few-shot setting, we propose representing the entire domain using the subspace formed by the embeddings of multiple images. Furthermore, we utilize pre-trained encoders to obtain separated subspaces corresponding to different domains, enabling us to accomplish hybrid domain adaptation. \n\nThe direction loss in StyleCLIP is computed based on the cosine similarity between the generated images and textual prompts within the CLIP embedding space. It does not depend on the source image or domain. In practice, this loss leads to adversarial solutions and sees no benefit from maintaining diversity as depicted in Style-NADA[5]. A mode-collapsed generator producing only one image may be the best minimizer for the distance to a given textual prompt. Differently, our direction loss aims to preserve more characteristics from source domain and maintains its diversity.\n\n[1] Few-shot image generation via cross-domain correspondence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10743–10752, 2021.\n\n[2] Few shot generative model adaption via relaxed spatial structural alignment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11204–11213, 2022.\n\n[3] Domain re-modulation for few-shot generative domain adaptation. arXiv preprint arXiv:2302.02550, 2023.\n\n[4] Efficient geometry-aware 3d generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16123–16133, 2022.\n\n[5] Stylegan-nada: Clip-guided domain adaptation of image generators. arXiv preprint arXiv:2108.00946, 2021.\n\n---\n\nThank you for your insightful comments about our work. We have added additional results with highlighting in the Appendix. Here we provide a point-by-point response to the issues raised by you.\n\n**The results of more domains**\n\nConsistent with prior work for few-shot generative domain adaptation (like CDC[1], RSSA[2], and DoRM[3]), our experimental data encompasses both global style ($\\textit{sketch}$ and $\\textit{baby}$) and local attributes ($\\textit{smile}$ and $\\textit{sunglasses}$). The combination of these domains demonstrates the effectiveness of our method since they encompass all types of generative domain adaptation.\n\nTo provide more comprehensive evidence of validity, we conduct additional experiments on $\\textit{Raphael}$ and $\\textit{Caricature}$ for both single and hybrid domain adaptation. As shown in Fig. 15 of Appendix, the results integrate the characteristics from multiple target domains and maintain robust consistency with source domain, which further demonstrates the effectiveness of our method. \n\n**The generalizability to other domains**\n\nTo verify the generalizability of our method to other domains, we conduct experiments on church domain following prior work. We adapt the pre-trained generator from $\\textit{LSUN Church}$ to $\\textit{Van Gogh's house paintings}$, $\\textit{haunted houses}$, and the combination of them. As shown in Fig. 16 of Appendix, the results acquire the corresponding style and showcase the preservation of good cross-domain consistency. This aligns with the results observed in the face domain.\n\n**The advantages and disadvantages of few-shot domain adaptation techniques with discrimination**\n\nFew-shot domain adaptation techniques with discrimination (like DoRM, DualStyleGAN and 3DAvatarGAN) excel in generating images closely resembling the style of the target domain as its discriminator tends to memorize training images. However, they suffer from the notorious issue of model collapse. Especially in few-shot scenarios, they easily overfits to the target images, compromising the diversity of generated images. Additionally, they are challenging to extend into an end-to-end hybrid domain adaptation approach. Their approach to hybrid domain often involves separately training multiple models and interpolating style codes, necessitating multiple model size and training time.\n\n**Why is hybrid domain adaptation necessary in these cases**\n\nImage editing indeed enables local adjustments resembling the original domain, like $\\textit{smile}$ and $\\textit{baby}$. However, in this scenario, our method holds several advantages. \n\nFirstly, our objective is to generate images with attributes of target domain while maintaining considerable diversity, which can be applicable in scenarios like data collection. Image editing, on the other hand, requires an original image as input, rendering it impractical for such applications. \n\nMoreover, hybrid domain adaptation has the capability to generate images from hybrid domain containing multiple target attributes like $\\textit{baby with the style of sketch}$ or $\\textit{smiling person with the style of sketch}$. Image editing, however, lacks the ability to perform such global stylistic modifications, limiting its broader applications.\n\n---\n\nThank you for your insightful comments about our work. We have added additional results with highlighting in the Appendix. Here we provide a point-by-point response to the issues raised by you.\n\n**Why we claim to introduce Hybrid generative DA**\n\nPrior techniques like style-mixing [1] can generate images from hybrid domain via interpolating latent codes. DoRM is just one of those methods. However, they lack a detailed investigation of HDA, which is merely mentioned as an additional feature. For example, they fail to provide systematic definition and proper evaluation metrics to distinguish good from bad.\n\nWith regarding to the method in achieving HDA, they encounter two primary issues:\n\n(1) DoRM is not an end-to-end pipeline to address HDA. They primarily focus on single domain adaptation, necessitating the separate training and interpolating multiple models to accomplish HDA. This may not be the right way to approach this task. \n\n(2) DoRM necessitates intricate tuning of hyperparameters that interpolate between the source and target domains. As demonstrated in the DoRM paper, they assign a weight of 0.005 to the $\\textit{baby}$ domain during single domain adaptation. Varying these hyperparameters significantly affects the outcomes. This complexity and sensitivity in parameter tuning make HDA's tuning more intricate and demanding.\n\n**How sensitive is our method to the domain coefficient $\\alpha$**\n\nIn our method for hybrid domain adaptation, this parameter controls the composition ratio of the attribute from each domain. As depicted in A.4 of Appendix, we use $\\alpha_i = 0.5$ for most experiments without the need for complex and intricate adjustments. To further explore the sensitivity, we conduct the study for simple traversal of $\\alpha_i$. As shown in the Fig. 14 of Appendix, the attributes of generated images transit smoothly between domains. Our method produces the similar attribute blending effect when $\\alpha_i \\in$ \\{0.4, 0.5, 0.6\\} .\n\n**Comparison to methods for diffusion model personalization**\n\nCurrent trend in customized text-to-image models like DreamBooth aim to mimic the appearance of subjects in a given reference set. Similar to ours, DreamBooth fine-tunes a pre-trained generator for the personalization. However, there are two key differences between personalization and HDA.\n\n(1) DreamBooth aims to retain the individuals from the training images, requiring similar individuals across the training set. Differently, our goal is to acquire the attributes of target domain like $\\textit{baby}$ and preserve consistency with the source generator, ensuring that images generated from the same noise exhibit similar individuals.\n\n(2) Dreambooth utilizes text-to-image generator which necessitates intricate and laborious adjustments of prompts to synthesize images with specific attributes. Additionally, certain attributes are challenging to accurately describe using text, such as artistic paintings. Conversely, our adapted model adeptly preserves the domain-specific attributes of reference images without the need for intricate prompt engineering.\n\n**Conceptual comparison to Domain Expansion**\n\nDomain Expansion aims to expand the pre-trained generator to have the capacity to generate images from multiple domains. To the end, it proposes to repurpose dormant directions in latent space for the new domains. While our approach involves training images from multiple target domains, our objective is to create a unseen composition of given target domains that integrates attributes from them. For example, given training images from $\\textit{baby}$ and $\\textit{sketch}$, Domain Expansion aims to expand the generator to have the capacity to generate images from $\\textit{baby}$ or $\\textit{sketch}$. Differently, we aim to adapt the generator to unseen hybrid domain $\\textit{baby with the style of sketch}$.\n\n**Details of the training time comparisons**\n\nIn Table 2, our comparison about training time is fair. Specifically, we measure all the time on a single NVIDIA TITAN GPU. Due to the absence of open-source code for DoRM, we implement it following their description in the paper, based on the official implementation of StyleGAN2-ADA. The Seq method is also developed in a similar manner. As for StyleNADA, we utilize their open-source code for training. For an apples-to-apples comparison, we set the same batch size as 4 and the same resolution as $256 \\times 256$.\n\n[1] Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8110–8119, 2020.\n\n---\n\nThank you for your insightful comments about our work. We have added additional results with highlighting in the Appendix. Here we provide a point-by-point response to the issues raised by you.\n\n**Why is hybrid domain adaptation a meaningful task**\n\nCurrent generative domain adaptation approaches typically employ the discriminator to discern whether generated images belong to the target domain. When we require to generate images with integrated attributes, they need to collect images from the hybrid target domain (e.g., a $\\textit{smiling}$ $\\textit{baby}$ with the style of $\\textit{sketch}$ in Fig. 1). However, these images tend to be more difficult to collect compared with single domain in real-world scenarios. In contrast, HDA only requires collecting data for several independent target domains rather than for each possible combination as mentioned by **Reviewer 2zhA**. Under such circumstances, HDA offers greater flexibility and versatility to adapt the generator to more composite and expansive domain.\n\n**Results using different numbers of shots**\n\nWe perform experiments on lower shots (5-shot and 1-shot) as suggested using various datasets. As shown in Fig. 11 of Appendix, the results of 5-shot are close to those of 10-shot, which integrates the attributes and maintains the consistency with source domain. Although multiple attributes have been learned, the results of 1-shot exhibit relatively lower cross-domain consistency compared with 10-shot and 5-shot.  This is because when there is only one reference image per domain, the subspace in our directional subspace loss degenerates into a single point (see Fig. 2). Then distinct generated images corresponding to different noise tend to converge towards the same image in CLIP's embedding space, which comprises cross-domain consistency as depicted in Section 3.3.\n\n**Results of conditional generation**\n\nAs shown in Fig. 12 of Appendix, we conduct the experiments on conditional generation. Specifically, we collect 10-shot images with $\\textit{red hair}$ and $\\textit{sunglasses}$. Then we use masks to separate these attributes and adapt the generator with masked images for $\\textit{red hair}$ and $\\textit{sunglasses}$ respectively. We can observe that the generated images possess the corresponding attribute for both single DA and hybrid DA. Simultaneously, these images also maintain consistency with source domain.\n    \n**Effect of different pre-trained image encoders**\n\nAs shown in Fig. 13 of Appendix, we conduct experiments on pre-trained Swin and Dinov2 to explore the impact of different image encoders on generated images. Their results all achieve HDA, indicating that our method is agnostic to different pre-trained image encoders. Although they exhibit slight stylistic differences, these are due to their different approaches to extract features into separated subspaces, as depicted in Fig. 8 of Appendix. To converge to the exact realization of the target domain, our method employs the ensemble technique that exploits both Swin and Dinov2. As shown in the figure, the results closely resembles the attributes of the target domain while maintaining the best consistency with source domain.\n\n**Compare with DALL-E and Imagen**\n\nDALL-E and Imagen are both text-to-image models known for their ability to generate images based on natural language descriptions. They exhibit the advantage of autonomously producing realistic images with specific attributes while maintaining considerable diversity. However, in comparison to our generator, they exhibit two primary drawbacks. \n\n(1) They necessitate intricate and laborious adjustments of prompts to generate images with specific attributes. Additionally, certain attributes are challenging to accurately describe using text, such as artistic paintings. Conversely, our generator can preserve the domain-specific attributes of reference images without the need for intricate prompt engineering.\n\n(2) It is hard to control the characters within images via prompts for them. In contrast, our adapted model maintains strong consistency with the source generator, preserving the same characters from the same noise.\n\nBesides, ours differs from the current trend in customized text-to-image models like DreamBooth. DreamBooth aims to retain the individuals from the training images, requiring similar individuals across the training set. Differently, our goal is to acquire the attributes of target domain like $\\textit{baby}$ and preserve consistency with the source generator, ensuring that images generated from the same noise exhibit similar individuals.", "author_response": "Thank you for your insightful comments about our work. We have added additional results with highlighting in the Appendix. Here we provide a point-by-point response to the issues raised by you.\n\n**Why is hybrid domain adaptation a meaningful task**\n\nCurrent generative domain adaptation approaches typically employ the discriminator to discern whether generated images belong to the target domain. When we require to generate images with integrated attributes, they need to collect images from the hybrid target domain (e.g., a $\\textit{smiling}$ $\\textit{baby}$ with the style of $\\textit{sketch}$ in Fig. 1). However, these images tend to be more difficult to collect compared with single domain in real-world scenarios. In contrast, HDA only requires collecting data for several independent target domains rather than for each possible combination as mentioned by **Reviewer 2zhA**. Under such circumstances, HDA offers greater flexibility and versatility to adapt the generator to more composite and expansive domain.\n\n**Results using different numbers of shots**\n\nWe perform experiments on lower shots (5-shot and 1-shot) as suggested using various datasets. As shown in Fig. 11 of Appendix, the results of 5-shot are close to those of 10-shot, which integrates the attributes and maintains the consistency with source domain. Although multiple attributes have been learned, the results of 1-shot exhibit relatively lower cross-domain consistency compared with 10-shot and 5-shot.  This is because when there is only one reference image per domain, the subspace in our directional subspace loss degenerates into a single point (see Fig. 2). Then distinct generated images corresponding to different noise tend to converge towards the same image in CLIP's embedding space, which comprises cross-domain consistency as depicted in Section 3.3.\n\n**Results of conditional generation**\n\nAs shown in Fig. 12 of Appendix, we conduct the experiments on conditional generation. Specifically, we collect 10-shot images with $\\textit{red hair}$ and $\\textit{sunglasses}$. Then we use masks to separate these attributes and adapt the generator with masked images for $\\textit{red hair}$ and $\\textit{sunglasses}$ respectively. We can observe that the generated images possess the corresponding attribute for both single DA and hybrid DA. Simultaneously, these images also maintain consistency with source domain.\n    \n**Effect of different pre-trained image encoders**\n\nAs shown in Fig. 13 of Appendix, we conduct experiments on pre-trained Swin and Dinov2 to explore the impact of different image encoders on generated images. Their results all achieve HDA, indicating that our method is agnostic to different pre-trained image encoders. Although they exhibit slight stylistic differences, these are due to their different approaches to extract features into separated subspaces, as depicted in Fig. 8 of Appendix. To converge to the exact realization of the target domain, our method employs the ensemble technique that exploits both Swin and Dinov2. As shown in the figure, the results closely resembles the attributes of the target domain while maintaining the best consistency with source domain.\n\n**Compare with DALL-E and Imagen**\n\nDALL-E and Imagen are both text-to-image models known for their ability to generate images based on natural language descriptions. They exhibit the advantage of autonomously producing realistic images with specific attributes while maintaining considerable diversity. However, in comparison to our generator, they exhibit two primary drawbacks. \n\n(1) They necessitate intricate and laborious adjustments of prompts to generate images with specific attributes. Additionally, certain attributes are challenging to accurately describe using text, such as artistic paintings. Conversely, our generator can preserve the domain-specific attributes of reference images without the need for intricate prompt engineering.\n\n(2) It is hard to control the characters within images via prompts for them. In contrast, our adapted model maintains strong consistency with the source generator, preserving the same characters from the same noise.\n\nBesides, ours differs from the current trend in customized text-to-image models like DreamBooth. DreamBooth aims to retain the individuals from the training images, requiring similar individuals across the training set. Differently, our goal is to acquire the attributes of target domain like $\\textit{baby}$ and preserve consistency with the source generator, ensuring that images generated from the same noise exhibit similar individuals."}
{"claim": "The paper does not compare the proposed generator with widely known generative models such as DALL·E and Imagen to assess relative quality and capabilities.", "claim_type": "baseline", "paper_id": "FE2e8664Sl", "paper_title": "Few-shot Hybrid Domain Adaptation of Image Generator", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "u2SOmGevjE", "reviewer": "Reviewer_fvXy", "review_text": "Summary: The paper introduces the few-shot generative Hybrid Domain Adaptation (HDA) task for image-based data. It is assumed that an image generator is accessed from a source domain and then from several target domains. Adaptation is performed on all target domains. To perform HDA, the paper presents a discriminator-free approach with a style GAN pre-trained on a source domain. It is adapted to individual target domains using a few shots from each domain and generates an image style showing composite attributes of each target domain separately.  The proposed approach relies on features extracted from standard transformer-based architectures. In addition, distance and direction losses are used to guide the generator to produce images with integrated features of the target domains while preserving the features of the source domain. Experiments are conducted using the FFHQ dataset and three evaluation metrics are used to measure performance and compare with different baselines.\n\nStrengths: - The paper is well written and easy to understand. The method is well described and the experiments are clearly presented. \n\n- The discriminator-free approach presented is interesting as it reduces the computational cost of fitting compared to existing approaches. \n\n- The approach shows promising results for all metrics compared to the baselines on the FFHQ dataset.\n\nWeaknesses: - (Major) The paper does not provide strong arguments as to why hybrid domain adaptation is a meaningful task. It is not clear why the current approaches and benchmarks, e.g. domain adaptation and domain generalisation, are not sufficient to address the problem presented. These points need further elaboration to motivate the paper. \n\n- (Major) results are reported using 10 shots, while it would be interesting to see results using different numbers of shots, e.g. 1-shot and 5-shot.\n\n- It is unclear whether the methodology is applicable to the generation of conditioned image styles. For example, generating an image that is conditioned on one or more styles of the target domain, rather than an image that spans all attributes of the target domains. It could discussed how the proposed methodology generalises to conditional generation. \n\n- A study of the effect of different pre-trained image encoders would add value to the paper.\n\nQuestions: - How would the proposed generator compare with well-known generator models such as DALLE, Imagen and etc.?", "labeling_timestamp": "2026-01-11T16:23:26.619265", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "Authors respond by providing a conceptual comparison explaining differences and limitations of DALL·E/Imagen relative to their method (prompt engineering and character control), but they do not provide empirical or quantitative comparisons of relative quality or capabilities.", "evidence": "\"Compare with DALL-E and Imagen\n\nDALL-E and Imagen are both text-to-image models known for their ability to generate images based on natural language descriptions. They exhibit the advantage of autonomously producing realistic images with specific attributes while maintaining considerable diversity. However, in comparison to our generator, they exhibit two primary drawbacks. \n\n(1) They necessitate intricate and laborious adjustments of prompts to generate images with specific attributes. Additionally, certain attributes are challenging to accurately describe using text, such as artistic paintings. Conversely, our generator can preserve the domain-specific attributes of reference images without the need for intricate prompt engineering.\n\n(2) It is hard to control the characters within images via prompts for them. In contrast, our adapted model maintains strong consistency with the source generator, preserving the same characters from the same noise.\"", "combined_author_response": "We hope these additional results and discussions can address your concerns. Please let us know if there are any further clarifications that we can offer. We would love to discuss more if any concern still remains.\n\n---\n\nWe hope these additional results and discussions can address your concerns. Please let us know if there are any further clarifications that we can offer. We would love to discuss more if any concern still remains.\n\n---\n\nWe are glad that our responses solve your concerns. Thank you again for your valuable feedback and suggestions!\n\n---\n\nDear Reviewers:\n\nWe sincerely thank you for your great efforts in reviewing this paper. We are pleased to read that our proposed problem is **interesting** and **well-motivated** [2zhA], our method is **intuitive** and **effective** [fvXy, 2zhA], and our results are **compelling** [fvXy, 2zhA, bxKn].\n\nWe have tried our best to address all the mentioned concerns and problems. **Our manuscript has been revised** to include the changes according to all the reviewers’ insightful comments, making our research more robust and accessible.\n\n**As the deadline for Author-Reviewer discussion is approaching**, we are eagerly looking forward to your responses. Please let us know if there are any additional clarifications or experiments that we could offer. We would love to discuss more if any concern still remains. Thanks again for your time!\n\nBest,\n\nAuthors\n\n---\n\n**The editability before and after the domain adaptation**\n\nWe conduct the editing on the images generated by original and adapted model for both single and hybrid domain adaptation. As shown in Fig. 17 of Appendix, the results indicate the adapted generator maintains similar editability like pose to the original generator. This verifies that the our method effectively preserves original generator's attributes. \n\n**Results in 3D GAN setting**\n\nWe conducted experiments using the popular 3D-aware image generation method, EG3D[4]. Specifically, we replace the discriminator as we did in Fig. 2 for both single and hybrid domain adaptation. As shown in Fig. 18 of Appendix, we adapt the pre-trained generator from FFHQ to $\\textit{sunglasses}$ and the hybrid of $\\textit{sunglasses}$ and $\\textit{smile}$ with 10-shot training images per domain. We can observe that the results effectively integrate the attributes and preserve the characters and poses of source domain.\n\n**Comparisons with $\\textit{Mind the GAP}$ and $\\textit{StyleCLIP}$**\n\nWhile Mind the GAP has a similar loss with direction loss, one term of our proposed directional subspace loss, their motivations differ. Mind the GAP is proposed for one-shot domain adaptation, thus representing the entire target domain using the embedding of a single image. In contrast, for the few-shot setting, we propose representing the entire domain using the subspace formed by the embeddings of multiple images. Furthermore, we utilize pre-trained encoders to obtain separated subspaces corresponding to different domains, enabling us to accomplish hybrid domain adaptation. \n\nThe direction loss in StyleCLIP is computed based on the cosine similarity between the generated images and textual prompts within the CLIP embedding space. It does not depend on the source image or domain. In practice, this loss leads to adversarial solutions and sees no benefit from maintaining diversity as depicted in Style-NADA[5]. A mode-collapsed generator producing only one image may be the best minimizer for the distance to a given textual prompt. Differently, our direction loss aims to preserve more characteristics from source domain and maintains its diversity.\n\n[1] Few-shot image generation via cross-domain correspondence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10743–10752, 2021.\n\n[2] Few shot generative model adaption via relaxed spatial structural alignment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11204–11213, 2022.\n\n[3] Domain re-modulation for few-shot generative domain adaptation. arXiv preprint arXiv:2302.02550, 2023.\n\n[4] Efficient geometry-aware 3d generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16123–16133, 2022.\n\n[5] Stylegan-nada: Clip-guided domain adaptation of image generators. arXiv preprint arXiv:2108.00946, 2021.\n\n---\n\nThank you for your insightful comments about our work. We have added additional results with highlighting in the Appendix. Here we provide a point-by-point response to the issues raised by you.\n\n**The results of more domains**\n\nConsistent with prior work for few-shot generative domain adaptation (like CDC[1], RSSA[2], and DoRM[3]), our experimental data encompasses both global style ($\\textit{sketch}$ and $\\textit{baby}$) and local attributes ($\\textit{smile}$ and $\\textit{sunglasses}$). The combination of these domains demonstrates the effectiveness of our method since they encompass all types of generative domain adaptation.\n\nTo provide more comprehensive evidence of validity, we conduct additional experiments on $\\textit{Raphael}$ and $\\textit{Caricature}$ for both single and hybrid domain adaptation. As shown in Fig. 15 of Appendix, the results integrate the characteristics from multiple target domains and maintain robust consistency with source domain, which further demonstrates the effectiveness of our method. \n\n**The generalizability to other domains**\n\nTo verify the generalizability of our method to other domains, we conduct experiments on church domain following prior work. We adapt the pre-trained generator from $\\textit{LSUN Church}$ to $\\textit{Van Gogh's house paintings}$, $\\textit{haunted houses}$, and the combination of them. As shown in Fig. 16 of Appendix, the results acquire the corresponding style and showcase the preservation of good cross-domain consistency. This aligns with the results observed in the face domain.\n\n**The advantages and disadvantages of few-shot domain adaptation techniques with discrimination**\n\nFew-shot domain adaptation techniques with discrimination (like DoRM, DualStyleGAN and 3DAvatarGAN) excel in generating images closely resembling the style of the target domain as its discriminator tends to memorize training images. However, they suffer from the notorious issue of model collapse. Especially in few-shot scenarios, they easily overfits to the target images, compromising the diversity of generated images. Additionally, they are challenging to extend into an end-to-end hybrid domain adaptation approach. Their approach to hybrid domain often involves separately training multiple models and interpolating style codes, necessitating multiple model size and training time.\n\n**Why is hybrid domain adaptation necessary in these cases**\n\nImage editing indeed enables local adjustments resembling the original domain, like $\\textit{smile}$ and $\\textit{baby}$. However, in this scenario, our method holds several advantages. \n\nFirstly, our objective is to generate images with attributes of target domain while maintaining considerable diversity, which can be applicable in scenarios like data collection. Image editing, on the other hand, requires an original image as input, rendering it impractical for such applications. \n\nMoreover, hybrid domain adaptation has the capability to generate images from hybrid domain containing multiple target attributes like $\\textit{baby with the style of sketch}$ or $\\textit{smiling person with the style of sketch}$. Image editing, however, lacks the ability to perform such global stylistic modifications, limiting its broader applications.\n\n---\n\nThank you for your insightful comments about our work. We have added additional results with highlighting in the Appendix. Here we provide a point-by-point response to the issues raised by you.\n\n**Why we claim to introduce Hybrid generative DA**\n\nPrior techniques like style-mixing [1] can generate images from hybrid domain via interpolating latent codes. DoRM is just one of those methods. However, they lack a detailed investigation of HDA, which is merely mentioned as an additional feature. For example, they fail to provide systematic definition and proper evaluation metrics to distinguish good from bad.\n\nWith regarding to the method in achieving HDA, they encounter two primary issues:\n\n(1) DoRM is not an end-to-end pipeline to address HDA. They primarily focus on single domain adaptation, necessitating the separate training and interpolating multiple models to accomplish HDA. This may not be the right way to approach this task. \n\n(2) DoRM necessitates intricate tuning of hyperparameters that interpolate between the source and target domains. As demonstrated in the DoRM paper, they assign a weight of 0.005 to the $\\textit{baby}$ domain during single domain adaptation. Varying these hyperparameters significantly affects the outcomes. This complexity and sensitivity in parameter tuning make HDA's tuning more intricate and demanding.\n\n**How sensitive is our method to the domain coefficient $\\alpha$**\n\nIn our method for hybrid domain adaptation, this parameter controls the composition ratio of the attribute from each domain. As depicted in A.4 of Appendix, we use $\\alpha_i = 0.5$ for most experiments without the need for complex and intricate adjustments. To further explore the sensitivity, we conduct the study for simple traversal of $\\alpha_i$. As shown in the Fig. 14 of Appendix, the attributes of generated images transit smoothly between domains. Our method produces the similar attribute blending effect when $\\alpha_i \\in$ \\{0.4, 0.5, 0.6\\} .\n\n**Comparison to methods for diffusion model personalization**\n\nCurrent trend in customized text-to-image models like DreamBooth aim to mimic the appearance of subjects in a given reference set. Similar to ours, DreamBooth fine-tunes a pre-trained generator for the personalization. However, there are two key differences between personalization and HDA.\n\n(1) DreamBooth aims to retain the individuals from the training images, requiring similar individuals across the training set. Differently, our goal is to acquire the attributes of target domain like $\\textit{baby}$ and preserve consistency with the source generator, ensuring that images generated from the same noise exhibit similar individuals.\n\n(2) Dreambooth utilizes text-to-image generator which necessitates intricate and laborious adjustments of prompts to synthesize images with specific attributes. Additionally, certain attributes are challenging to accurately describe using text, such as artistic paintings. Conversely, our adapted model adeptly preserves the domain-specific attributes of reference images without the need for intricate prompt engineering.\n\n**Conceptual comparison to Domain Expansion**\n\nDomain Expansion aims to expand the pre-trained generator to have the capacity to generate images from multiple domains. To the end, it proposes to repurpose dormant directions in latent space for the new domains. While our approach involves training images from multiple target domains, our objective is to create a unseen composition of given target domains that integrates attributes from them. For example, given training images from $\\textit{baby}$ and $\\textit{sketch}$, Domain Expansion aims to expand the generator to have the capacity to generate images from $\\textit{baby}$ or $\\textit{sketch}$. Differently, we aim to adapt the generator to unseen hybrid domain $\\textit{baby with the style of sketch}$.\n\n**Details of the training time comparisons**\n\nIn Table 2, our comparison about training time is fair. Specifically, we measure all the time on a single NVIDIA TITAN GPU. Due to the absence of open-source code for DoRM, we implement it following their description in the paper, based on the official implementation of StyleGAN2-ADA. The Seq method is also developed in a similar manner. As for StyleNADA, we utilize their open-source code for training. For an apples-to-apples comparison, we set the same batch size as 4 and the same resolution as $256 \\times 256$.\n\n[1] Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8110–8119, 2020.\n\n---\n\nThank you for your insightful comments about our work. We have added additional results with highlighting in the Appendix. Here we provide a point-by-point response to the issues raised by you.\n\n**Why is hybrid domain adaptation a meaningful task**\n\nCurrent generative domain adaptation approaches typically employ the discriminator to discern whether generated images belong to the target domain. When we require to generate images with integrated attributes, they need to collect images from the hybrid target domain (e.g., a $\\textit{smiling}$ $\\textit{baby}$ with the style of $\\textit{sketch}$ in Fig. 1). However, these images tend to be more difficult to collect compared with single domain in real-world scenarios. In contrast, HDA only requires collecting data for several independent target domains rather than for each possible combination as mentioned by **Reviewer 2zhA**. Under such circumstances, HDA offers greater flexibility and versatility to adapt the generator to more composite and expansive domain.\n\n**Results using different numbers of shots**\n\nWe perform experiments on lower shots (5-shot and 1-shot) as suggested using various datasets. As shown in Fig. 11 of Appendix, the results of 5-shot are close to those of 10-shot, which integrates the attributes and maintains the consistency with source domain. Although multiple attributes have been learned, the results of 1-shot exhibit relatively lower cross-domain consistency compared with 10-shot and 5-shot.  This is because when there is only one reference image per domain, the subspace in our directional subspace loss degenerates into a single point (see Fig. 2). Then distinct generated images corresponding to different noise tend to converge towards the same image in CLIP's embedding space, which comprises cross-domain consistency as depicted in Section 3.3.\n\n**Results of conditional generation**\n\nAs shown in Fig. 12 of Appendix, we conduct the experiments on conditional generation. Specifically, we collect 10-shot images with $\\textit{red hair}$ and $\\textit{sunglasses}$. Then we use masks to separate these attributes and adapt the generator with masked images for $\\textit{red hair}$ and $\\textit{sunglasses}$ respectively. We can observe that the generated images possess the corresponding attribute for both single DA and hybrid DA. Simultaneously, these images also maintain consistency with source domain.\n    \n**Effect of different pre-trained image encoders**\n\nAs shown in Fig. 13 of Appendix, we conduct experiments on pre-trained Swin and Dinov2 to explore the impact of different image encoders on generated images. Their results all achieve HDA, indicating that our method is agnostic to different pre-trained image encoders. Although they exhibit slight stylistic differences, these are due to their different approaches to extract features into separated subspaces, as depicted in Fig. 8 of Appendix. To converge to the exact realization of the target domain, our method employs the ensemble technique that exploits both Swin and Dinov2. As shown in the figure, the results closely resembles the attributes of the target domain while maintaining the best consistency with source domain.\n\n**Compare with DALL-E and Imagen**\n\nDALL-E and Imagen are both text-to-image models known for their ability to generate images based on natural language descriptions. They exhibit the advantage of autonomously producing realistic images with specific attributes while maintaining considerable diversity. However, in comparison to our generator, they exhibit two primary drawbacks. \n\n(1) They necessitate intricate and laborious adjustments of prompts to generate images with specific attributes. Additionally, certain attributes are challenging to accurately describe using text, such as artistic paintings. Conversely, our generator can preserve the domain-specific attributes of reference images without the need for intricate prompt engineering.\n\n(2) It is hard to control the characters within images via prompts for them. In contrast, our adapted model maintains strong consistency with the source generator, preserving the same characters from the same noise.\n\nBesides, ours differs from the current trend in customized text-to-image models like DreamBooth. DreamBooth aims to retain the individuals from the training images, requiring similar individuals across the training set. Differently, our goal is to acquire the attributes of target domain like $\\textit{baby}$ and preserve consistency with the source generator, ensuring that images generated from the same noise exhibit similar individuals.", "author_response": "Thank you for your insightful comments about our work. We have added additional results with highlighting in the Appendix. Here we provide a point-by-point response to the issues raised by you.\n\n**Why is hybrid domain adaptation a meaningful task**\n\nCurrent generative domain adaptation approaches typically employ the discriminator to discern whether generated images belong to the target domain. When we require to generate images with integrated attributes, they need to collect images from the hybrid target domain (e.g., a $\\textit{smiling}$ $\\textit{baby}$ with the style of $\\textit{sketch}$ in Fig. 1). However, these images tend to be more difficult to collect compared with single domain in real-world scenarios. In contrast, HDA only requires collecting data for several independent target domains rather than for each possible combination as mentioned by **Reviewer 2zhA**. Under such circumstances, HDA offers greater flexibility and versatility to adapt the generator to more composite and expansive domain.\n\n**Results using different numbers of shots**\n\nWe perform experiments on lower shots (5-shot and 1-shot) as suggested using various datasets. As shown in Fig. 11 of Appendix, the results of 5-shot are close to those of 10-shot, which integrates the attributes and maintains the consistency with source domain. Although multiple attributes have been learned, the results of 1-shot exhibit relatively lower cross-domain consistency compared with 10-shot and 5-shot.  This is because when there is only one reference image per domain, the subspace in our directional subspace loss degenerates into a single point (see Fig. 2). Then distinct generated images corresponding to different noise tend to converge towards the same image in CLIP's embedding space, which comprises cross-domain consistency as depicted in Section 3.3.\n\n**Results of conditional generation**\n\nAs shown in Fig. 12 of Appendix, we conduct the experiments on conditional generation. Specifically, we collect 10-shot images with $\\textit{red hair}$ and $\\textit{sunglasses}$. Then we use masks to separate these attributes and adapt the generator with masked images for $\\textit{red hair}$ and $\\textit{sunglasses}$ respectively. We can observe that the generated images possess the corresponding attribute for both single DA and hybrid DA. Simultaneously, these images also maintain consistency with source domain.\n    \n**Effect of different pre-trained image encoders**\n\nAs shown in Fig. 13 of Appendix, we conduct experiments on pre-trained Swin and Dinov2 to explore the impact of different image encoders on generated images. Their results all achieve HDA, indicating that our method is agnostic to different pre-trained image encoders. Although they exhibit slight stylistic differences, these are due to their different approaches to extract features into separated subspaces, as depicted in Fig. 8 of Appendix. To converge to the exact realization of the target domain, our method employs the ensemble technique that exploits both Swin and Dinov2. As shown in the figure, the results closely resembles the attributes of the target domain while maintaining the best consistency with source domain.\n\n**Compare with DALL-E and Imagen**\n\nDALL-E and Imagen are both text-to-image models known for their ability to generate images based on natural language descriptions. They exhibit the advantage of autonomously producing realistic images with specific attributes while maintaining considerable diversity. However, in comparison to our generator, they exhibit two primary drawbacks. \n\n(1) They necessitate intricate and laborious adjustments of prompts to generate images with specific attributes. Additionally, certain attributes are challenging to accurately describe using text, such as artistic paintings. Conversely, our generator can preserve the domain-specific attributes of reference images without the need for intricate prompt engineering.\n\n(2) It is hard to control the characters within images via prompts for them. In contrast, our adapted model maintains strong consistency with the source generator, preserving the same characters from the same noise.\n\nBesides, ours differs from the current trend in customized text-to-image models like DreamBooth. DreamBooth aims to retain the individuals from the training images, requiring similar individuals across the training set. Differently, our goal is to acquire the attributes of target domain like $\\textit{baby}$ and preserve consistency with the source generator, ensuring that images generated from the same noise exhibit similar individuals."}
{"claim": "The authors did not release the code for their method, preventing reproducibility and independent verification of KV-cache experiments.", "claim_type": "methodology", "paper_id": "pNnvzQsS4P", "paper_title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vadn2W9weu", "reviewer": "Reviewer_KwWk", "review_text": "Summary: The authors have addressed the KV-cache compression problem by providing a finer quantization level. The KV-cache can pose a significant barrier to the inference of most autoregressive language models, a challenge that has been well studied in recent publications at ICML and NeurIPS. This paper introduces a novel approach by coupling multiple key/value channels together for quantization, exploiting their interdependence to encode the activations in a more information-efficient manner.\n\nStrengths: - The method is novel and demonstrates comparable accuracy to KVQuant, one of the pivotal approaches in this field.\n- It includes a substantial number of experiments to validate accuracy.\n- The quantization method introduced here is novel compared to other approaches. Additionally, the implementation in PyTorch represents a significant contribution.\n\nWeaknesses: - The code is not available. For research on KV cache, it is important to have the code available.\n\n- The section describing the random variables and entropy, specifically line 120, does not explicitly describe the random variables in mathematical notation. This should be revised for clarity. I would like to see this section more polished.\n\n- I believe the LLAMA3 model was available before the NeurIPS submission. Since that time, the authors may have extended their findings to these models. I would like to see the performance of your model in that setting.\n\n- I want to see how the runtime of your method compares to other methods. Recent works, like **QJL**, include good plots for token-wise generation time or end-to-end timing. Since you compare with KVQuant, it is also good to compare with **KIVI**, as it is one of the best methods. I recommend comparing with QJL and **KIVI**, and plotting the runtime alongside these methods.\n\n- I highly recommend the authors run their code on longer context datasets. LongBench could be a great example to evaluate its performance compared to other methods. I would suggest that perplexity is not the best metric for comparison.\n\nRelevant papers: [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache], [QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead]\n\n**If you address the concerns regarding the experiments and provide a broader comparison to the other methods, I would increase my score.**\n\nQuestions: - There is additional overhead regarding storing the centroid for each coupled key/value pair, making it difficult to track. I would like you to mention this overhead and explain how you set those values.", "labeling_timestamp": "2026-01-11T16:23:33.760239", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The authors' response discusses additional experiments, comparisons, and planned measurements but does not state that they have released (or will release) their code; the reviewer’s specific reproducibility concern about code release is not addressed.", "evidence": "\"We have conducted additional experiments using the official QJL codebase (https://github.com/amirzandieh/QJL) with an A100 40GB GPU.\"", "combined_author_response": "We sincerely thank the reviewer for their thoughtful comments and suggestions!\n\n---\n\nDear Reviewer KwWk,\n\nWe would like to express our sincere gratitude for your time and effort in reviewing our paper. Your feedback has been invaluable to us.\n\nAs the discussion period is drawing to a close, we kindly request that you review our previous responses to your review. If you have any additional questions or concerns, we would be happy to address them promptly.\n\nThank you again for your valuable contributions.\n\n---\n\nWe sincerely thank the reviewer for the response, and address your concerns as follows.\n\n- We presented the latency measurement with batch size 1, since Reviewer dwgV has specifically asked us for latency experiments in small batch sizes. We quote\n>From Figure 4, it seems all CQ methods perform worse than the fp16 version with small batch sizes. Can you explain why? Is it due to the overhead of (de)quantization?\n- Measurements with batch size 1 highlight the efficiency and latency aspects of our CUDA kernels.\n- We note that the latency comparison we presented between CQ and KIVI is a fair comparison with the same experimental settings: batch size of 1, equal token counts for prefill and decoding, and an identical sliding window size.\n- We agree with the reviewer that latency and throughput at different batch sizes are important for understanding the efficiency of our approach. Hence we will include additional latency and throughput measurements at different batch sizes and context lengths in the camera-ready version.\n\n---\n\nWe appreciate the reviewer for carefully reviewing our paper and offering thoughtful feedback. We have conducted additional experiments on passkey retrieval, following the setup in [1], with Llama-2-7b at its maximum context length of 4096. The passkey retrieval task is similar to the needle retrieval task in RULER. We still have trouble running RULER due to compatibility issues with Docker, and we will continue to work on it. As shown in the table below, CQ consistently outperforms KVQuant at various bit widths on passkey retrieval. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses.\n\n|  | Bit Width | Retrieval Success Rate |\n|---|---|---|\n| KVQuant-4b+1% sparse | 4.32 | **100%** |\n| KVQuant-4b | 4 | **100%** |\n| CQ-2c8b | 4 | **100%** |\n| KVQuant-2b+1% sparse | 2.32 | 94% |\n| CQ-4c9b | 2.26 | **98%** |\n| KVQuant-2b | 2 | 0% |\n| CQ-4c8b | 2 | **96%** |\n| KVQuant-1b+1% sparse | 1.32 | 2% |\n| CQ-8c10b | 1.27 | **78%** |\n| KVQuant-1b | 1 | 0% |\n| CQ-8c8b | 1 | **12%** |\n\n**References**\n\n[1] Zhu, Dawei, et al. \"Pose: Efficient context window extension of llms via positional skip-wise training.\" arXiv preprint arXiv:2309.10400 (2023).\n\n---\n\nWe thank the reviewer again for carefully reviewing our paper and providing constructive feedback. We have conducted additional experiments using the official QJL codebase (https://github.com/amirzandieh/QJL) with an A100 40GB GPU. We evaluate QJL and CQ with Llama-2-7b on LongBench. \nFor QJL, we used a sliding window of size 32 and a group size of 32  (`buffer_size=32,group_size=32`), and set other hyper-parameters following the codebase (`key_quantization_bits=256,key_quantization_bits_initial_layers=512,initial_layers_count=15,outlier_count_general=8,outlier_count_initial_layers=8,value_quantization_bits=2`). For CQ, we use the 4c8b (2-bit) configuration and a sliding window of size 32. The results are presented in the table below.\n\n|  | Bit Width | Qasper | TREC | SAMSum | TriviaQA |\n|---|---|---|---|---|---|\n| FP16 | 16 | 9.52 | 66.00 | 41.69 | 87.72 |\n| QJL | 3.00 | 5.98 | 15.00 | 14.84 | Error |\n| CQ-4c8b | 2.00 | **9.58** | **66.00** | **41.13** | **87.72** |\n\nWe also encountered some challenges when attempting to conduct additional experiments.\n\n1. For QJL on TriviaQA, we ran into the following error: `File \".../QJL/models/llama2_utils_qjl.py\", line 143, in _update_outliers\n    self.outlier_indices = torch.cat([self.outlier_indices, outlier_indices], dim=2).contiguous()\nTypeError: expected Tensor as element 0 in argument 0, but got NoneType`.\n\n2. We tried to directly compare CQ with QJL by following the experimental settings (longchat-7b-v1.5-32k on LongBench) in Table 1 of the QJL paper. However, we ran into out-of-memory issues with CQ-4c8b due to GPU memory constraints (Nvidia A100 40G).\n\nGiven the time constraint of the discussion period, we have made our best effort to provide a fair comparison between CQ and QJL. We are open to further investigation and would welcome specific suggestions from the reviewer on how to resolve the issues above. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses.", "author_response": "We thank the reviewer again for carefully reviewing our paper and providing constructive feedback. We have conducted additional experiments using the official QJL codebase (https://github.com/amirzandieh/QJL) with an A100 40GB GPU. We evaluate QJL and CQ with Llama-2-7b on LongBench. \nFor QJL, we used a sliding window of size 32 and a group size of 32  (`buffer_size=32,group_size=32`), and set other hyper-parameters following the codebase (`key_quantization_bits=256,key_quantization_bits_initial_layers=512,initial_layers_count=15,outlier_count_general=8,outlier_count_initial_layers=8,value_quantization_bits=2`). For CQ, we use the 4c8b (2-bit) configuration and a sliding window of size 32. The results are presented in the table below.\n\n|  | Bit Width | Qasper | TREC | SAMSum | TriviaQA |\n|---|---|---|---|---|---|\n| FP16 | 16 | 9.52 | 66.00 | 41.69 | 87.72 |\n| QJL | 3.00 | 5.98 | 15.00 | 14.84 | Error |\n| CQ-4c8b | 2.00 | **9.58** | **66.00** | **41.13** | **87.72** |\n\nWe also encountered some challenges when attempting to conduct additional experiments.\n\n1. For QJL on TriviaQA, we ran into the following error: `File \".../QJL/models/llama2_utils_qjl.py\", line 143, in _update_outliers\n    self.outlier_indices = torch.cat([self.outlier_indices, outlier_indices], dim=2).contiguous()\nTypeError: expected Tensor as element 0 in argument 0, but got NoneType`.\n\n2. We tried to directly compare CQ with QJL by following the experimental settings (longchat-7b-v1.5-32k on LongBench) in Table 1 of the QJL paper. However, we ran into out-of-memory issues with CQ-4c8b due to GPU memory constraints (Nvidia A100 40G).\n\nGiven the time constraint of the discussion period, we have made our best effort to provide a fair comparison between CQ and QJL. We are open to further investigation and would welcome specific suggestions from the reviewer on how to resolve the issues above. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses."}
{"claim": "The section on random variables and entropy (line 120) does not explicitly define the random variables in mathematical notation, reducing clarity and rigor.", "claim_type": "methodology", "paper_id": "pNnvzQsS4P", "paper_title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vadn2W9weu", "reviewer": "Reviewer_KwWk", "review_text": "Summary: The authors have addressed the KV-cache compression problem by providing a finer quantization level. The KV-cache can pose a significant barrier to the inference of most autoregressive language models, a challenge that has been well studied in recent publications at ICML and NeurIPS. This paper introduces a novel approach by coupling multiple key/value channels together for quantization, exploiting their interdependence to encode the activations in a more information-efficient manner.\n\nStrengths: - The method is novel and demonstrates comparable accuracy to KVQuant, one of the pivotal approaches in this field.\n- It includes a substantial number of experiments to validate accuracy.\n- The quantization method introduced here is novel compared to other approaches. Additionally, the implementation in PyTorch represents a significant contribution.\n\nWeaknesses: - The code is not available. For research on KV cache, it is important to have the code available.\n\n- The section describing the random variables and entropy, specifically line 120, does not explicitly describe the random variables in mathematical notation. This should be revised for clarity. I would like to see this section more polished.\n\n- I believe the LLAMA3 model was available before the NeurIPS submission. Since that time, the authors may have extended their findings to these models. I would like to see the performance of your model in that setting.\n\n- I want to see how the runtime of your method compares to other methods. Recent works, like **QJL**, include good plots for token-wise generation time or end-to-end timing. Since you compare with KVQuant, it is also good to compare with **KIVI**, as it is one of the best methods. I recommend comparing with QJL and **KIVI**, and plotting the runtime alongside these methods.\n\n- I highly recommend the authors run their code on longer context datasets. LongBench could be a great example to evaluate its performance compared to other methods. I would suggest that perplexity is not the best metric for comparison.\n\nRelevant papers: [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache], [QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead]\n\n**If you address the concerns regarding the experiments and provide a broader comparison to the other methods, I would increase my score.**\n\nQuestions: - There is additional overhead regarding storing the centroid for each coupled key/value pair, making it difficult to track. I would like you to mention this overhead and explain how you set those values.", "labeling_timestamp": "2026-01-11T16:23:29.035219", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The authors' response does not mention the section on random variables or entropy nor indicate they will add explicit mathematical notation; instead they address unrelated points (latency, experiments, comparisons).", "evidence": "\"- We presented the latency measurement with batch size 1, since Reviewer dwgV has specifically asked us for latency experiments in small batch sizes.\"", "combined_author_response": "We sincerely thank the reviewer for their thoughtful comments and suggestions!\n\n---\n\nDear Reviewer KwWk,\n\nWe would like to express our sincere gratitude for your time and effort in reviewing our paper. Your feedback has been invaluable to us.\n\nAs the discussion period is drawing to a close, we kindly request that you review our previous responses to your review. If you have any additional questions or concerns, we would be happy to address them promptly.\n\nThank you again for your valuable contributions.\n\n---\n\nWe sincerely thank the reviewer for the response, and address your concerns as follows.\n\n- We presented the latency measurement with batch size 1, since Reviewer dwgV has specifically asked us for latency experiments in small batch sizes. We quote\n>From Figure 4, it seems all CQ methods perform worse than the fp16 version with small batch sizes. Can you explain why? Is it due to the overhead of (de)quantization?\n- Measurements with batch size 1 highlight the efficiency and latency aspects of our CUDA kernels.\n- We note that the latency comparison we presented between CQ and KIVI is a fair comparison with the same experimental settings: batch size of 1, equal token counts for prefill and decoding, and an identical sliding window size.\n- We agree with the reviewer that latency and throughput at different batch sizes are important for understanding the efficiency of our approach. Hence we will include additional latency and throughput measurements at different batch sizes and context lengths in the camera-ready version.\n\n---\n\nWe appreciate the reviewer for carefully reviewing our paper and offering thoughtful feedback. We have conducted additional experiments on passkey retrieval, following the setup in [1], with Llama-2-7b at its maximum context length of 4096. The passkey retrieval task is similar to the needle retrieval task in RULER. We still have trouble running RULER due to compatibility issues with Docker, and we will continue to work on it. As shown in the table below, CQ consistently outperforms KVQuant at various bit widths on passkey retrieval. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses.\n\n|  | Bit Width | Retrieval Success Rate |\n|---|---|---|\n| KVQuant-4b+1% sparse | 4.32 | **100%** |\n| KVQuant-4b | 4 | **100%** |\n| CQ-2c8b | 4 | **100%** |\n| KVQuant-2b+1% sparse | 2.32 | 94% |\n| CQ-4c9b | 2.26 | **98%** |\n| KVQuant-2b | 2 | 0% |\n| CQ-4c8b | 2 | **96%** |\n| KVQuant-1b+1% sparse | 1.32 | 2% |\n| CQ-8c10b | 1.27 | **78%** |\n| KVQuant-1b | 1 | 0% |\n| CQ-8c8b | 1 | **12%** |\n\n**References**\n\n[1] Zhu, Dawei, et al. \"Pose: Efficient context window extension of llms via positional skip-wise training.\" arXiv preprint arXiv:2309.10400 (2023).\n\n---\n\nWe thank the reviewer again for carefully reviewing our paper and providing constructive feedback. We have conducted additional experiments using the official QJL codebase (https://github.com/amirzandieh/QJL) with an A100 40GB GPU. We evaluate QJL and CQ with Llama-2-7b on LongBench. \nFor QJL, we used a sliding window of size 32 and a group size of 32  (`buffer_size=32,group_size=32`), and set other hyper-parameters following the codebase (`key_quantization_bits=256,key_quantization_bits_initial_layers=512,initial_layers_count=15,outlier_count_general=8,outlier_count_initial_layers=8,value_quantization_bits=2`). For CQ, we use the 4c8b (2-bit) configuration and a sliding window of size 32. The results are presented in the table below.\n\n|  | Bit Width | Qasper | TREC | SAMSum | TriviaQA |\n|---|---|---|---|---|---|\n| FP16 | 16 | 9.52 | 66.00 | 41.69 | 87.72 |\n| QJL | 3.00 | 5.98 | 15.00 | 14.84 | Error |\n| CQ-4c8b | 2.00 | **9.58** | **66.00** | **41.13** | **87.72** |\n\nWe also encountered some challenges when attempting to conduct additional experiments.\n\n1. For QJL on TriviaQA, we ran into the following error: `File \".../QJL/models/llama2_utils_qjl.py\", line 143, in _update_outliers\n    self.outlier_indices = torch.cat([self.outlier_indices, outlier_indices], dim=2).contiguous()\nTypeError: expected Tensor as element 0 in argument 0, but got NoneType`.\n\n2. We tried to directly compare CQ with QJL by following the experimental settings (longchat-7b-v1.5-32k on LongBench) in Table 1 of the QJL paper. However, we ran into out-of-memory issues with CQ-4c8b due to GPU memory constraints (Nvidia A100 40G).\n\nGiven the time constraint of the discussion period, we have made our best effort to provide a fair comparison between CQ and QJL. We are open to further investigation and would welcome specific suggestions from the reviewer on how to resolve the issues above. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses.", "author_response": "We thank the reviewer again for carefully reviewing our paper and providing constructive feedback. We have conducted additional experiments using the official QJL codebase (https://github.com/amirzandieh/QJL) with an A100 40GB GPU. We evaluate QJL and CQ with Llama-2-7b on LongBench. \nFor QJL, we used a sliding window of size 32 and a group size of 32  (`buffer_size=32,group_size=32`), and set other hyper-parameters following the codebase (`key_quantization_bits=256,key_quantization_bits_initial_layers=512,initial_layers_count=15,outlier_count_general=8,outlier_count_initial_layers=8,value_quantization_bits=2`). For CQ, we use the 4c8b (2-bit) configuration and a sliding window of size 32. The results are presented in the table below.\n\n|  | Bit Width | Qasper | TREC | SAMSum | TriviaQA |\n|---|---|---|---|---|---|\n| FP16 | 16 | 9.52 | 66.00 | 41.69 | 87.72 |\n| QJL | 3.00 | 5.98 | 15.00 | 14.84 | Error |\n| CQ-4c8b | 2.00 | **9.58** | **66.00** | **41.13** | **87.72** |\n\nWe also encountered some challenges when attempting to conduct additional experiments.\n\n1. For QJL on TriviaQA, we ran into the following error: `File \".../QJL/models/llama2_utils_qjl.py\", line 143, in _update_outliers\n    self.outlier_indices = torch.cat([self.outlier_indices, outlier_indices], dim=2).contiguous()\nTypeError: expected Tensor as element 0 in argument 0, but got NoneType`.\n\n2. We tried to directly compare CQ with QJL by following the experimental settings (longchat-7b-v1.5-32k on LongBench) in Table 1 of the QJL paper. However, we ran into out-of-memory issues with CQ-4c8b due to GPU memory constraints (Nvidia A100 40G).\n\nGiven the time constraint of the discussion period, we have made our best effort to provide a fair comparison between CQ and QJL. We are open to further investigation and would welcome specific suggestions from the reviewer on how to resolve the issues above. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses."}
{"claim": "The paper does not report experiments evaluating the method on LLAMA3 models, despite LLAMA3 being available before submission.", "claim_type": "experimental", "paper_id": "pNnvzQsS4P", "paper_title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vadn2W9weu", "reviewer": "Reviewer_KwWk", "review_text": "Summary: The authors have addressed the KV-cache compression problem by providing a finer quantization level. The KV-cache can pose a significant barrier to the inference of most autoregressive language models, a challenge that has been well studied in recent publications at ICML and NeurIPS. This paper introduces a novel approach by coupling multiple key/value channels together for quantization, exploiting their interdependence to encode the activations in a more information-efficient manner.\n\nStrengths: - The method is novel and demonstrates comparable accuracy to KVQuant, one of the pivotal approaches in this field.\n- It includes a substantial number of experiments to validate accuracy.\n- The quantization method introduced here is novel compared to other approaches. Additionally, the implementation in PyTorch represents a significant contribution.\n\nWeaknesses: - The code is not available. For research on KV cache, it is important to have the code available.\n\n- The section describing the random variables and entropy, specifically line 120, does not explicitly describe the random variables in mathematical notation. This should be revised for clarity. I would like to see this section more polished.\n\n- I believe the LLAMA3 model was available before the NeurIPS submission. Since that time, the authors may have extended their findings to these models. I would like to see the performance of your model in that setting.\n\n- I want to see how the runtime of your method compares to other methods. Recent works, like **QJL**, include good plots for token-wise generation time or end-to-end timing. Since you compare with KVQuant, it is also good to compare with **KIVI**, as it is one of the best methods. I recommend comparing with QJL and **KIVI**, and plotting the runtime alongside these methods.\n\n- I highly recommend the authors run their code on longer context datasets. LongBench could be a great example to evaluate its performance compared to other methods. I would suggest that perplexity is not the best metric for comparison.\n\nRelevant papers: [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache], [QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead]\n\n**If you address the concerns regarding the experiments and provide a broader comparison to the other methods, I would increase my score.**\n\nQuestions: - There is additional overhead regarding storing the centroid for each coupled key/value pair, making it difficult to track. I would like you to mention this overhead and explain how you set those values.", "labeling_timestamp": "2026-01-11T16:23:37.866022", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The authors' response never addresses LLAMA3; all cited additional experiments refer to Llama-2-7b and other settings, so the reviewer’s specific concern about missing LLAMA3 evaluations is not answered.", "evidence": "\"We have conducted additional experiments on passkey retrieval, following the setup in [1], with Llama-2-7b at its maximum context length of 4096.\"; \"We have conducted additional experiments using the official QJL codebase ... We evaluate QJL and CQ with Llama-2-7b on LongBench.\"", "combined_author_response": "We sincerely thank the reviewer for their thoughtful comments and suggestions!\n\n---\n\nDear Reviewer KwWk,\n\nWe would like to express our sincere gratitude for your time and effort in reviewing our paper. Your feedback has been invaluable to us.\n\nAs the discussion period is drawing to a close, we kindly request that you review our previous responses to your review. If you have any additional questions or concerns, we would be happy to address them promptly.\n\nThank you again for your valuable contributions.\n\n---\n\nWe sincerely thank the reviewer for the response, and address your concerns as follows.\n\n- We presented the latency measurement with batch size 1, since Reviewer dwgV has specifically asked us for latency experiments in small batch sizes. We quote\n>From Figure 4, it seems all CQ methods perform worse than the fp16 version with small batch sizes. Can you explain why? Is it due to the overhead of (de)quantization?\n- Measurements with batch size 1 highlight the efficiency and latency aspects of our CUDA kernels.\n- We note that the latency comparison we presented between CQ and KIVI is a fair comparison with the same experimental settings: batch size of 1, equal token counts for prefill and decoding, and an identical sliding window size.\n- We agree with the reviewer that latency and throughput at different batch sizes are important for understanding the efficiency of our approach. Hence we will include additional latency and throughput measurements at different batch sizes and context lengths in the camera-ready version.\n\n---\n\nWe appreciate the reviewer for carefully reviewing our paper and offering thoughtful feedback. We have conducted additional experiments on passkey retrieval, following the setup in [1], with Llama-2-7b at its maximum context length of 4096. The passkey retrieval task is similar to the needle retrieval task in RULER. We still have trouble running RULER due to compatibility issues with Docker, and we will continue to work on it. As shown in the table below, CQ consistently outperforms KVQuant at various bit widths on passkey retrieval. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses.\n\n|  | Bit Width | Retrieval Success Rate |\n|---|---|---|\n| KVQuant-4b+1% sparse | 4.32 | **100%** |\n| KVQuant-4b | 4 | **100%** |\n| CQ-2c8b | 4 | **100%** |\n| KVQuant-2b+1% sparse | 2.32 | 94% |\n| CQ-4c9b | 2.26 | **98%** |\n| KVQuant-2b | 2 | 0% |\n| CQ-4c8b | 2 | **96%** |\n| KVQuant-1b+1% sparse | 1.32 | 2% |\n| CQ-8c10b | 1.27 | **78%** |\n| KVQuant-1b | 1 | 0% |\n| CQ-8c8b | 1 | **12%** |\n\n**References**\n\n[1] Zhu, Dawei, et al. \"Pose: Efficient context window extension of llms via positional skip-wise training.\" arXiv preprint arXiv:2309.10400 (2023).\n\n---\n\nWe thank the reviewer again for carefully reviewing our paper and providing constructive feedback. We have conducted additional experiments using the official QJL codebase (https://github.com/amirzandieh/QJL) with an A100 40GB GPU. We evaluate QJL and CQ with Llama-2-7b on LongBench. \nFor QJL, we used a sliding window of size 32 and a group size of 32  (`buffer_size=32,group_size=32`), and set other hyper-parameters following the codebase (`key_quantization_bits=256,key_quantization_bits_initial_layers=512,initial_layers_count=15,outlier_count_general=8,outlier_count_initial_layers=8,value_quantization_bits=2`). For CQ, we use the 4c8b (2-bit) configuration and a sliding window of size 32. The results are presented in the table below.\n\n|  | Bit Width | Qasper | TREC | SAMSum | TriviaQA |\n|---|---|---|---|---|---|\n| FP16 | 16 | 9.52 | 66.00 | 41.69 | 87.72 |\n| QJL | 3.00 | 5.98 | 15.00 | 14.84 | Error |\n| CQ-4c8b | 2.00 | **9.58** | **66.00** | **41.13** | **87.72** |\n\nWe also encountered some challenges when attempting to conduct additional experiments.\n\n1. For QJL on TriviaQA, we ran into the following error: `File \".../QJL/models/llama2_utils_qjl.py\", line 143, in _update_outliers\n    self.outlier_indices = torch.cat([self.outlier_indices, outlier_indices], dim=2).contiguous()\nTypeError: expected Tensor as element 0 in argument 0, but got NoneType`.\n\n2. We tried to directly compare CQ with QJL by following the experimental settings (longchat-7b-v1.5-32k on LongBench) in Table 1 of the QJL paper. However, we ran into out-of-memory issues with CQ-4c8b due to GPU memory constraints (Nvidia A100 40G).\n\nGiven the time constraint of the discussion period, we have made our best effort to provide a fair comparison between CQ and QJL. We are open to further investigation and would welcome specific suggestions from the reviewer on how to resolve the issues above. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses.", "author_response": "We thank the reviewer again for carefully reviewing our paper and providing constructive feedback. We have conducted additional experiments using the official QJL codebase (https://github.com/amirzandieh/QJL) with an A100 40GB GPU. We evaluate QJL and CQ with Llama-2-7b on LongBench. \nFor QJL, we used a sliding window of size 32 and a group size of 32  (`buffer_size=32,group_size=32`), and set other hyper-parameters following the codebase (`key_quantization_bits=256,key_quantization_bits_initial_layers=512,initial_layers_count=15,outlier_count_general=8,outlier_count_initial_layers=8,value_quantization_bits=2`). For CQ, we use the 4c8b (2-bit) configuration and a sliding window of size 32. The results are presented in the table below.\n\n|  | Bit Width | Qasper | TREC | SAMSum | TriviaQA |\n|---|---|---|---|---|---|\n| FP16 | 16 | 9.52 | 66.00 | 41.69 | 87.72 |\n| QJL | 3.00 | 5.98 | 15.00 | 14.84 | Error |\n| CQ-4c8b | 2.00 | **9.58** | **66.00** | **41.13** | **87.72** |\n\nWe also encountered some challenges when attempting to conduct additional experiments.\n\n1. For QJL on TriviaQA, we ran into the following error: `File \".../QJL/models/llama2_utils_qjl.py\", line 143, in _update_outliers\n    self.outlier_indices = torch.cat([self.outlier_indices, outlier_indices], dim=2).contiguous()\nTypeError: expected Tensor as element 0 in argument 0, but got NoneType`.\n\n2. We tried to directly compare CQ with QJL by following the experimental settings (longchat-7b-v1.5-32k on LongBench) in Table 1 of the QJL paper. However, we ran into out-of-memory issues with CQ-4c8b due to GPU memory constraints (Nvidia A100 40G).\n\nGiven the time constraint of the discussion period, we have made our best effort to provide a fair comparison between CQ and QJL. We are open to further investigation and would welcome specific suggestions from the reviewer on how to resolve the issues above. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses."}
{"claim": "The paper does not provide runtime comparisons against the recent QJL method, such as token-wise generation time or end-to-end timing plots.", "claim_type": "baseline", "paper_id": "pNnvzQsS4P", "paper_title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vadn2W9weu", "reviewer": "Reviewer_KwWk", "review_text": "Summary: The authors have addressed the KV-cache compression problem by providing a finer quantization level. The KV-cache can pose a significant barrier to the inference of most autoregressive language models, a challenge that has been well studied in recent publications at ICML and NeurIPS. This paper introduces a novel approach by coupling multiple key/value channels together for quantization, exploiting their interdependence to encode the activations in a more information-efficient manner.\n\nStrengths: - The method is novel and demonstrates comparable accuracy to KVQuant, one of the pivotal approaches in this field.\n- It includes a substantial number of experiments to validate accuracy.\n- The quantization method introduced here is novel compared to other approaches. Additionally, the implementation in PyTorch represents a significant contribution.\n\nWeaknesses: - The code is not available. For research on KV cache, it is important to have the code available.\n\n- The section describing the random variables and entropy, specifically line 120, does not explicitly describe the random variables in mathematical notation. This should be revised for clarity. I would like to see this section more polished.\n\n- I believe the LLAMA3 model was available before the NeurIPS submission. Since that time, the authors may have extended their findings to these models. I would like to see the performance of your model in that setting.\n\n- I want to see how the runtime of your method compares to other methods. Recent works, like **QJL**, include good plots for token-wise generation time or end-to-end timing. Since you compare with KVQuant, it is also good to compare with **KIVI**, as it is one of the best methods. I recommend comparing with QJL and **KIVI**, and plotting the runtime alongside these methods.\n\n- I highly recommend the authors run their code on longer context datasets. LongBench could be a great example to evaluate its performance compared to other methods. I would suggest that perplexity is not the best metric for comparison.\n\nRelevant papers: [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache], [QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead]\n\n**If you address the concerns regarding the experiments and provide a broader comparison to the other methods, I would increase my score.**\n\nQuestions: - There is additional overhead regarding storing the centroid for each coupled key/value pair, making it difficult to track. I would like you to mention this overhead and explain how you set those values.", "labeling_timestamp": "2026-01-11T16:23:50.630905", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors attempted to address the request by reporting latency for some settings, running QJL experiments (reporting accuracy issues), and explaining practical problems (errors and OOM). However they did not provide the specific runtime comparisons asked for (token-wise generation time or end-to-end timing plots vs QJL) in the response, only promising to add more latency/throughput measurements in the camera-ready version.", "evidence": "\"We presented the latency measurement with batch size 1, since Reviewer dwgV has specifically asked us for latency experiments in small batch sizes.\"; \"We have conducted additional experiments using the official QJL codebase... Given the time constraint of the discussion period, we have made our best effort to provide a fair comparison between CQ and QJL.\"; \"we ran into out-of-memory issues with CQ-4c8b due to GPU memory constraints (Nvidia A100 40G).\"; \"Hence we will include additional latency and throughput measurements at different batch sizes and context lengths in the camera-ready version.\"", "combined_author_response": "We sincerely thank the reviewer for their thoughtful comments and suggestions!\n\n---\n\nDear Reviewer KwWk,\n\nWe would like to express our sincere gratitude for your time and effort in reviewing our paper. Your feedback has been invaluable to us.\n\nAs the discussion period is drawing to a close, we kindly request that you review our previous responses to your review. If you have any additional questions or concerns, we would be happy to address them promptly.\n\nThank you again for your valuable contributions.\n\n---\n\nWe sincerely thank the reviewer for the response, and address your concerns as follows.\n\n- We presented the latency measurement with batch size 1, since Reviewer dwgV has specifically asked us for latency experiments in small batch sizes. We quote\n>From Figure 4, it seems all CQ methods perform worse than the fp16 version with small batch sizes. Can you explain why? Is it due to the overhead of (de)quantization?\n- Measurements with batch size 1 highlight the efficiency and latency aspects of our CUDA kernels.\n- We note that the latency comparison we presented between CQ and KIVI is a fair comparison with the same experimental settings: batch size of 1, equal token counts for prefill and decoding, and an identical sliding window size.\n- We agree with the reviewer that latency and throughput at different batch sizes are important for understanding the efficiency of our approach. Hence we will include additional latency and throughput measurements at different batch sizes and context lengths in the camera-ready version.\n\n---\n\nWe appreciate the reviewer for carefully reviewing our paper and offering thoughtful feedback. We have conducted additional experiments on passkey retrieval, following the setup in [1], with Llama-2-7b at its maximum context length of 4096. The passkey retrieval task is similar to the needle retrieval task in RULER. We still have trouble running RULER due to compatibility issues with Docker, and we will continue to work on it. As shown in the table below, CQ consistently outperforms KVQuant at various bit widths on passkey retrieval. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses.\n\n|  | Bit Width | Retrieval Success Rate |\n|---|---|---|\n| KVQuant-4b+1% sparse | 4.32 | **100%** |\n| KVQuant-4b | 4 | **100%** |\n| CQ-2c8b | 4 | **100%** |\n| KVQuant-2b+1% sparse | 2.32 | 94% |\n| CQ-4c9b | 2.26 | **98%** |\n| KVQuant-2b | 2 | 0% |\n| CQ-4c8b | 2 | **96%** |\n| KVQuant-1b+1% sparse | 1.32 | 2% |\n| CQ-8c10b | 1.27 | **78%** |\n| KVQuant-1b | 1 | 0% |\n| CQ-8c8b | 1 | **12%** |\n\n**References**\n\n[1] Zhu, Dawei, et al. \"Pose: Efficient context window extension of llms via positional skip-wise training.\" arXiv preprint arXiv:2309.10400 (2023).\n\n---\n\nWe thank the reviewer again for carefully reviewing our paper and providing constructive feedback. We have conducted additional experiments using the official QJL codebase (https://github.com/amirzandieh/QJL) with an A100 40GB GPU. We evaluate QJL and CQ with Llama-2-7b on LongBench. \nFor QJL, we used a sliding window of size 32 and a group size of 32  (`buffer_size=32,group_size=32`), and set other hyper-parameters following the codebase (`key_quantization_bits=256,key_quantization_bits_initial_layers=512,initial_layers_count=15,outlier_count_general=8,outlier_count_initial_layers=8,value_quantization_bits=2`). For CQ, we use the 4c8b (2-bit) configuration and a sliding window of size 32. The results are presented in the table below.\n\n|  | Bit Width | Qasper | TREC | SAMSum | TriviaQA |\n|---|---|---|---|---|---|\n| FP16 | 16 | 9.52 | 66.00 | 41.69 | 87.72 |\n| QJL | 3.00 | 5.98 | 15.00 | 14.84 | Error |\n| CQ-4c8b | 2.00 | **9.58** | **66.00** | **41.13** | **87.72** |\n\nWe also encountered some challenges when attempting to conduct additional experiments.\n\n1. For QJL on TriviaQA, we ran into the following error: `File \".../QJL/models/llama2_utils_qjl.py\", line 143, in _update_outliers\n    self.outlier_indices = torch.cat([self.outlier_indices, outlier_indices], dim=2).contiguous()\nTypeError: expected Tensor as element 0 in argument 0, but got NoneType`.\n\n2. We tried to directly compare CQ with QJL by following the experimental settings (longchat-7b-v1.5-32k on LongBench) in Table 1 of the QJL paper. However, we ran into out-of-memory issues with CQ-4c8b due to GPU memory constraints (Nvidia A100 40G).\n\nGiven the time constraint of the discussion period, we have made our best effort to provide a fair comparison between CQ and QJL. We are open to further investigation and would welcome specific suggestions from the reviewer on how to resolve the issues above. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses.", "author_response": "We thank the reviewer again for carefully reviewing our paper and providing constructive feedback. We have conducted additional experiments using the official QJL codebase (https://github.com/amirzandieh/QJL) with an A100 40GB GPU. We evaluate QJL and CQ with Llama-2-7b on LongBench. \nFor QJL, we used a sliding window of size 32 and a group size of 32  (`buffer_size=32,group_size=32`), and set other hyper-parameters following the codebase (`key_quantization_bits=256,key_quantization_bits_initial_layers=512,initial_layers_count=15,outlier_count_general=8,outlier_count_initial_layers=8,value_quantization_bits=2`). For CQ, we use the 4c8b (2-bit) configuration and a sliding window of size 32. The results are presented in the table below.\n\n|  | Bit Width | Qasper | TREC | SAMSum | TriviaQA |\n|---|---|---|---|---|---|\n| FP16 | 16 | 9.52 | 66.00 | 41.69 | 87.72 |\n| QJL | 3.00 | 5.98 | 15.00 | 14.84 | Error |\n| CQ-4c8b | 2.00 | **9.58** | **66.00** | **41.13** | **87.72** |\n\nWe also encountered some challenges when attempting to conduct additional experiments.\n\n1. For QJL on TriviaQA, we ran into the following error: `File \".../QJL/models/llama2_utils_qjl.py\", line 143, in _update_outliers\n    self.outlier_indices = torch.cat([self.outlier_indices, outlier_indices], dim=2).contiguous()\nTypeError: expected Tensor as element 0 in argument 0, but got NoneType`.\n\n2. We tried to directly compare CQ with QJL by following the experimental settings (longchat-7b-v1.5-32k on LongBench) in Table 1 of the QJL paper. However, we ran into out-of-memory issues with CQ-4c8b due to GPU memory constraints (Nvidia A100 40G).\n\nGiven the time constraint of the discussion period, we have made our best effort to provide a fair comparison between CQ and QJL. We are open to further investigation and would welcome specific suggestions from the reviewer on how to resolve the issues above. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses."}
{"claim": "The paper omits runtime and performance comparisons to KIVI, a leading KV-cache quantization method, despite comparing only with KVQuant.", "claim_type": "baseline", "paper_id": "pNnvzQsS4P", "paper_title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vadn2W9weu", "reviewer": "Reviewer_KwWk", "review_text": "Summary: The authors have addressed the KV-cache compression problem by providing a finer quantization level. The KV-cache can pose a significant barrier to the inference of most autoregressive language models, a challenge that has been well studied in recent publications at ICML and NeurIPS. This paper introduces a novel approach by coupling multiple key/value channels together for quantization, exploiting their interdependence to encode the activations in a more information-efficient manner.\n\nStrengths: - The method is novel and demonstrates comparable accuracy to KVQuant, one of the pivotal approaches in this field.\n- It includes a substantial number of experiments to validate accuracy.\n- The quantization method introduced here is novel compared to other approaches. Additionally, the implementation in PyTorch represents a significant contribution.\n\nWeaknesses: - The code is not available. For research on KV cache, it is important to have the code available.\n\n- The section describing the random variables and entropy, specifically line 120, does not explicitly describe the random variables in mathematical notation. This should be revised for clarity. I would like to see this section more polished.\n\n- I believe the LLAMA3 model was available before the NeurIPS submission. Since that time, the authors may have extended their findings to these models. I would like to see the performance of your model in that setting.\n\n- I want to see how the runtime of your method compares to other methods. Recent works, like **QJL**, include good plots for token-wise generation time or end-to-end timing. Since you compare with KVQuant, it is also good to compare with **KIVI**, as it is one of the best methods. I recommend comparing with QJL and **KIVI**, and plotting the runtime alongside these methods.\n\n- I highly recommend the authors run their code on longer context datasets. LongBench could be a great example to evaluate its performance compared to other methods. I would suggest that perplexity is not the best metric for comparison.\n\nRelevant papers: [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache], [QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead]\n\n**If you address the concerns regarding the experiments and provide a broader comparison to the other methods, I would increase my score.**\n\nQuestions: - There is additional overhead regarding storing the centroid for each coupled key/value pair, making it difficult to track. I would like you to mention this overhead and explain how you set those values.", "labeling_timestamp": "2026-01-11T16:24:20.549808", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The authors state they did include a latency comparison between CQ and KIVI (with batch size 1 and matched settings) and say they will add further latency/throughput measurements, directly opposing the reviewer's claim of omission.", "evidence": "\"We presented the latency measurement with batch size 1...\" and \"We note that the latency comparison we presented between CQ and KIVI is a fair comparison with the same experimental settings: batch size of 1, equal token counts for prefill and decoding, and an identical sliding window size.\" and \"Hence we will include additional latency and throughput measurements at different batch sizes and context lengths in the camera-ready version.\"", "combined_author_response": "We sincerely thank the reviewer for their thoughtful comments and suggestions!\n\n---\n\nDear Reviewer KwWk,\n\nWe would like to express our sincere gratitude for your time and effort in reviewing our paper. Your feedback has been invaluable to us.\n\nAs the discussion period is drawing to a close, we kindly request that you review our previous responses to your review. If you have any additional questions or concerns, we would be happy to address them promptly.\n\nThank you again for your valuable contributions.\n\n---\n\nWe sincerely thank the reviewer for the response, and address your concerns as follows.\n\n- We presented the latency measurement with batch size 1, since Reviewer dwgV has specifically asked us for latency experiments in small batch sizes. We quote\n>From Figure 4, it seems all CQ methods perform worse than the fp16 version with small batch sizes. Can you explain why? Is it due to the overhead of (de)quantization?\n- Measurements with batch size 1 highlight the efficiency and latency aspects of our CUDA kernels.\n- We note that the latency comparison we presented between CQ and KIVI is a fair comparison with the same experimental settings: batch size of 1, equal token counts for prefill and decoding, and an identical sliding window size.\n- We agree with the reviewer that latency and throughput at different batch sizes are important for understanding the efficiency of our approach. Hence we will include additional latency and throughput measurements at different batch sizes and context lengths in the camera-ready version.\n\n---\n\nWe appreciate the reviewer for carefully reviewing our paper and offering thoughtful feedback. We have conducted additional experiments on passkey retrieval, following the setup in [1], with Llama-2-7b at its maximum context length of 4096. The passkey retrieval task is similar to the needle retrieval task in RULER. We still have trouble running RULER due to compatibility issues with Docker, and we will continue to work on it. As shown in the table below, CQ consistently outperforms KVQuant at various bit widths on passkey retrieval. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses.\n\n|  | Bit Width | Retrieval Success Rate |\n|---|---|---|\n| KVQuant-4b+1% sparse | 4.32 | **100%** |\n| KVQuant-4b | 4 | **100%** |\n| CQ-2c8b | 4 | **100%** |\n| KVQuant-2b+1% sparse | 2.32 | 94% |\n| CQ-4c9b | 2.26 | **98%** |\n| KVQuant-2b | 2 | 0% |\n| CQ-4c8b | 2 | **96%** |\n| KVQuant-1b+1% sparse | 1.32 | 2% |\n| CQ-8c10b | 1.27 | **78%** |\n| KVQuant-1b | 1 | 0% |\n| CQ-8c8b | 1 | **12%** |\n\n**References**\n\n[1] Zhu, Dawei, et al. \"Pose: Efficient context window extension of llms via positional skip-wise training.\" arXiv preprint arXiv:2309.10400 (2023).\n\n---\n\nWe thank the reviewer again for carefully reviewing our paper and providing constructive feedback. We have conducted additional experiments using the official QJL codebase (https://github.com/amirzandieh/QJL) with an A100 40GB GPU. We evaluate QJL and CQ with Llama-2-7b on LongBench. \nFor QJL, we used a sliding window of size 32 and a group size of 32  (`buffer_size=32,group_size=32`), and set other hyper-parameters following the codebase (`key_quantization_bits=256,key_quantization_bits_initial_layers=512,initial_layers_count=15,outlier_count_general=8,outlier_count_initial_layers=8,value_quantization_bits=2`). For CQ, we use the 4c8b (2-bit) configuration and a sliding window of size 32. The results are presented in the table below.\n\n|  | Bit Width | Qasper | TREC | SAMSum | TriviaQA |\n|---|---|---|---|---|---|\n| FP16 | 16 | 9.52 | 66.00 | 41.69 | 87.72 |\n| QJL | 3.00 | 5.98 | 15.00 | 14.84 | Error |\n| CQ-4c8b | 2.00 | **9.58** | **66.00** | **41.13** | **87.72** |\n\nWe also encountered some challenges when attempting to conduct additional experiments.\n\n1. For QJL on TriviaQA, we ran into the following error: `File \".../QJL/models/llama2_utils_qjl.py\", line 143, in _update_outliers\n    self.outlier_indices = torch.cat([self.outlier_indices, outlier_indices], dim=2).contiguous()\nTypeError: expected Tensor as element 0 in argument 0, but got NoneType`.\n\n2. We tried to directly compare CQ with QJL by following the experimental settings (longchat-7b-v1.5-32k on LongBench) in Table 1 of the QJL paper. However, we ran into out-of-memory issues with CQ-4c8b due to GPU memory constraints (Nvidia A100 40G).\n\nGiven the time constraint of the discussion period, we have made our best effort to provide a fair comparison between CQ and QJL. We are open to further investigation and would welcome specific suggestions from the reviewer on how to resolve the issues above. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses.", "author_response": "We thank the reviewer again for carefully reviewing our paper and providing constructive feedback. We have conducted additional experiments using the official QJL codebase (https://github.com/amirzandieh/QJL) with an A100 40GB GPU. We evaluate QJL and CQ with Llama-2-7b on LongBench. \nFor QJL, we used a sliding window of size 32 and a group size of 32  (`buffer_size=32,group_size=32`), and set other hyper-parameters following the codebase (`key_quantization_bits=256,key_quantization_bits_initial_layers=512,initial_layers_count=15,outlier_count_general=8,outlier_count_initial_layers=8,value_quantization_bits=2`). For CQ, we use the 4c8b (2-bit) configuration and a sliding window of size 32. The results are presented in the table below.\n\n|  | Bit Width | Qasper | TREC | SAMSum | TriviaQA |\n|---|---|---|---|---|---|\n| FP16 | 16 | 9.52 | 66.00 | 41.69 | 87.72 |\n| QJL | 3.00 | 5.98 | 15.00 | 14.84 | Error |\n| CQ-4c8b | 2.00 | **9.58** | **66.00** | **41.13** | **87.72** |\n\nWe also encountered some challenges when attempting to conduct additional experiments.\n\n1. For QJL on TriviaQA, we ran into the following error: `File \".../QJL/models/llama2_utils_qjl.py\", line 143, in _update_outliers\n    self.outlier_indices = torch.cat([self.outlier_indices, outlier_indices], dim=2).contiguous()\nTypeError: expected Tensor as element 0 in argument 0, but got NoneType`.\n\n2. We tried to directly compare CQ with QJL by following the experimental settings (longchat-7b-v1.5-32k on LongBench) in Table 1 of the QJL paper. However, we ran into out-of-memory issues with CQ-4c8b due to GPU memory constraints (Nvidia A100 40G).\n\nGiven the time constraint of the discussion period, we have made our best effort to provide a fair comparison between CQ and QJL. We are open to further investigation and would welcome specific suggestions from the reviewer on how to resolve the issues above. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses."}
{"claim": "The authors do not include token-wise generation time or end-to-end timing plots to characterize inference latency across methods.", "claim_type": "experimental", "paper_id": "pNnvzQsS4P", "paper_title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vadn2W9weu", "reviewer": "Reviewer_KwWk", "review_text": "Summary: The authors have addressed the KV-cache compression problem by providing a finer quantization level. The KV-cache can pose a significant barrier to the inference of most autoregressive language models, a challenge that has been well studied in recent publications at ICML and NeurIPS. This paper introduces a novel approach by coupling multiple key/value channels together for quantization, exploiting their interdependence to encode the activations in a more information-efficient manner.\n\nStrengths: - The method is novel and demonstrates comparable accuracy to KVQuant, one of the pivotal approaches in this field.\n- It includes a substantial number of experiments to validate accuracy.\n- The quantization method introduced here is novel compared to other approaches. Additionally, the implementation in PyTorch represents a significant contribution.\n\nWeaknesses: - The code is not available. For research on KV cache, it is important to have the code available.\n\n- The section describing the random variables and entropy, specifically line 120, does not explicitly describe the random variables in mathematical notation. This should be revised for clarity. I would like to see this section more polished.\n\n- I believe the LLAMA3 model was available before the NeurIPS submission. Since that time, the authors may have extended their findings to these models. I would like to see the performance of your model in that setting.\n\n- I want to see how the runtime of your method compares to other methods. Recent works, like **QJL**, include good plots for token-wise generation time or end-to-end timing. Since you compare with KVQuant, it is also good to compare with **KIVI**, as it is one of the best methods. I recommend comparing with QJL and **KIVI**, and plotting the runtime alongside these methods.\n\n- I highly recommend the authors run their code on longer context datasets. LongBench could be a great example to evaluate its performance compared to other methods. I would suggest that perplexity is not the best metric for comparison.\n\nRelevant papers: [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache], [QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead]\n\n**If you address the concerns regarding the experiments and provide a broader comparison to the other methods, I would increase my score.**\n\nQuestions: - There is additional overhead regarding storing the centroid for each coupled key/value pair, making it difficult to track. I would like you to mention this overhead and explain how you set those values.", "labeling_timestamp": "2026-01-11T16:23:47.977793", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "Authors acknowledge latency measurement was limited (batch size 1), recognize the importance of broader latency/throughput measurements, and commit to adding additional latency and throughput results in the camera-ready version—thus partially addressing the reviewer's claim rather than fully providing token-wise or end-to-end timing plots now.", "evidence": "\"We presented the latency measurement with batch size 1... Measurements with batch size 1 highlight the efficiency and latency aspects of our CUDA kernels.\"; \"We agree with the reviewer that latency and throughput at different batch sizes are important for understanding the efficiency of our approach. Hence we will include additional latency and throughput measurements at different batch sizes and context lengths in the camera-ready version.\"", "combined_author_response": "We sincerely thank the reviewer for their thoughtful comments and suggestions!\n\n---\n\nDear Reviewer KwWk,\n\nWe would like to express our sincere gratitude for your time and effort in reviewing our paper. Your feedback has been invaluable to us.\n\nAs the discussion period is drawing to a close, we kindly request that you review our previous responses to your review. If you have any additional questions or concerns, we would be happy to address them promptly.\n\nThank you again for your valuable contributions.\n\n---\n\nWe sincerely thank the reviewer for the response, and address your concerns as follows.\n\n- We presented the latency measurement with batch size 1, since Reviewer dwgV has specifically asked us for latency experiments in small batch sizes. We quote\n>From Figure 4, it seems all CQ methods perform worse than the fp16 version with small batch sizes. Can you explain why? Is it due to the overhead of (de)quantization?\n- Measurements with batch size 1 highlight the efficiency and latency aspects of our CUDA kernels.\n- We note that the latency comparison we presented between CQ and KIVI is a fair comparison with the same experimental settings: batch size of 1, equal token counts for prefill and decoding, and an identical sliding window size.\n- We agree with the reviewer that latency and throughput at different batch sizes are important for understanding the efficiency of our approach. Hence we will include additional latency and throughput measurements at different batch sizes and context lengths in the camera-ready version.\n\n---\n\nWe appreciate the reviewer for carefully reviewing our paper and offering thoughtful feedback. We have conducted additional experiments on passkey retrieval, following the setup in [1], with Llama-2-7b at its maximum context length of 4096. The passkey retrieval task is similar to the needle retrieval task in RULER. We still have trouble running RULER due to compatibility issues with Docker, and we will continue to work on it. As shown in the table below, CQ consistently outperforms KVQuant at various bit widths on passkey retrieval. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses.\n\n|  | Bit Width | Retrieval Success Rate |\n|---|---|---|\n| KVQuant-4b+1% sparse | 4.32 | **100%** |\n| KVQuant-4b | 4 | **100%** |\n| CQ-2c8b | 4 | **100%** |\n| KVQuant-2b+1% sparse | 2.32 | 94% |\n| CQ-4c9b | 2.26 | **98%** |\n| KVQuant-2b | 2 | 0% |\n| CQ-4c8b | 2 | **96%** |\n| KVQuant-1b+1% sparse | 1.32 | 2% |\n| CQ-8c10b | 1.27 | **78%** |\n| KVQuant-1b | 1 | 0% |\n| CQ-8c8b | 1 | **12%** |\n\n**References**\n\n[1] Zhu, Dawei, et al. \"Pose: Efficient context window extension of llms via positional skip-wise training.\" arXiv preprint arXiv:2309.10400 (2023).\n\n---\n\nWe thank the reviewer again for carefully reviewing our paper and providing constructive feedback. We have conducted additional experiments using the official QJL codebase (https://github.com/amirzandieh/QJL) with an A100 40GB GPU. We evaluate QJL and CQ with Llama-2-7b on LongBench. \nFor QJL, we used a sliding window of size 32 and a group size of 32  (`buffer_size=32,group_size=32`), and set other hyper-parameters following the codebase (`key_quantization_bits=256,key_quantization_bits_initial_layers=512,initial_layers_count=15,outlier_count_general=8,outlier_count_initial_layers=8,value_quantization_bits=2`). For CQ, we use the 4c8b (2-bit) configuration and a sliding window of size 32. The results are presented in the table below.\n\n|  | Bit Width | Qasper | TREC | SAMSum | TriviaQA |\n|---|---|---|---|---|---|\n| FP16 | 16 | 9.52 | 66.00 | 41.69 | 87.72 |\n| QJL | 3.00 | 5.98 | 15.00 | 14.84 | Error |\n| CQ-4c8b | 2.00 | **9.58** | **66.00** | **41.13** | **87.72** |\n\nWe also encountered some challenges when attempting to conduct additional experiments.\n\n1. For QJL on TriviaQA, we ran into the following error: `File \".../QJL/models/llama2_utils_qjl.py\", line 143, in _update_outliers\n    self.outlier_indices = torch.cat([self.outlier_indices, outlier_indices], dim=2).contiguous()\nTypeError: expected Tensor as element 0 in argument 0, but got NoneType`.\n\n2. We tried to directly compare CQ with QJL by following the experimental settings (longchat-7b-v1.5-32k on LongBench) in Table 1 of the QJL paper. However, we ran into out-of-memory issues with CQ-4c8b due to GPU memory constraints (Nvidia A100 40G).\n\nGiven the time constraint of the discussion period, we have made our best effort to provide a fair comparison between CQ and QJL. We are open to further investigation and would welcome specific suggestions from the reviewer on how to resolve the issues above. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses.", "author_response": "We thank the reviewer again for carefully reviewing our paper and providing constructive feedback. We have conducted additional experiments using the official QJL codebase (https://github.com/amirzandieh/QJL) with an A100 40GB GPU. We evaluate QJL and CQ with Llama-2-7b on LongBench. \nFor QJL, we used a sliding window of size 32 and a group size of 32  (`buffer_size=32,group_size=32`), and set other hyper-parameters following the codebase (`key_quantization_bits=256,key_quantization_bits_initial_layers=512,initial_layers_count=15,outlier_count_general=8,outlier_count_initial_layers=8,value_quantization_bits=2`). For CQ, we use the 4c8b (2-bit) configuration and a sliding window of size 32. The results are presented in the table below.\n\n|  | Bit Width | Qasper | TREC | SAMSum | TriviaQA |\n|---|---|---|---|---|---|\n| FP16 | 16 | 9.52 | 66.00 | 41.69 | 87.72 |\n| QJL | 3.00 | 5.98 | 15.00 | 14.84 | Error |\n| CQ-4c8b | 2.00 | **9.58** | **66.00** | **41.13** | **87.72** |\n\nWe also encountered some challenges when attempting to conduct additional experiments.\n\n1. For QJL on TriviaQA, we ran into the following error: `File \".../QJL/models/llama2_utils_qjl.py\", line 143, in _update_outliers\n    self.outlier_indices = torch.cat([self.outlier_indices, outlier_indices], dim=2).contiguous()\nTypeError: expected Tensor as element 0 in argument 0, but got NoneType`.\n\n2. We tried to directly compare CQ with QJL by following the experimental settings (longchat-7b-v1.5-32k on LongBench) in Table 1 of the QJL paper. However, we ran into out-of-memory issues with CQ-4c8b due to GPU memory constraints (Nvidia A100 40G).\n\nGiven the time constraint of the discussion period, we have made our best effort to provide a fair comparison between CQ and QJL. We are open to further investigation and would welcome specific suggestions from the reviewer on how to resolve the issues above. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses."}
{"claim": "The evaluation lacks long-context benchmarks such as LongBench to assess performance on extended context lengths.", "claim_type": "experimental", "paper_id": "pNnvzQsS4P", "paper_title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vadn2W9weu", "reviewer": "Reviewer_KwWk", "review_text": "Summary: The authors have addressed the KV-cache compression problem by providing a finer quantization level. The KV-cache can pose a significant barrier to the inference of most autoregressive language models, a challenge that has been well studied in recent publications at ICML and NeurIPS. This paper introduces a novel approach by coupling multiple key/value channels together for quantization, exploiting their interdependence to encode the activations in a more information-efficient manner.\n\nStrengths: - The method is novel and demonstrates comparable accuracy to KVQuant, one of the pivotal approaches in this field.\n- It includes a substantial number of experiments to validate accuracy.\n- The quantization method introduced here is novel compared to other approaches. Additionally, the implementation in PyTorch represents a significant contribution.\n\nWeaknesses: - The code is not available. For research on KV cache, it is important to have the code available.\n\n- The section describing the random variables and entropy, specifically line 120, does not explicitly describe the random variables in mathematical notation. This should be revised for clarity. I would like to see this section more polished.\n\n- I believe the LLAMA3 model was available before the NeurIPS submission. Since that time, the authors may have extended their findings to these models. I would like to see the performance of your model in that setting.\n\n- I want to see how the runtime of your method compares to other methods. Recent works, like **QJL**, include good plots for token-wise generation time or end-to-end timing. Since you compare with KVQuant, it is also good to compare with **KIVI**, as it is one of the best methods. I recommend comparing with QJL and **KIVI**, and plotting the runtime alongside these methods.\n\n- I highly recommend the authors run their code on longer context datasets. LongBench could be a great example to evaluate its performance compared to other methods. I would suggest that perplexity is not the best metric for comparison.\n\nRelevant papers: [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache], [QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead]\n\n**If you address the concerns regarding the experiments and provide a broader comparison to the other methods, I would increase my score.**\n\nQuestions: - There is additional overhead regarding storing the centroid for each coupled key/value pair, making it difficult to track. I would like you to mention this overhead and explain how you set those values.", "labeling_timestamp": "2026-01-11T16:24:42.749072", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors responded by running additional long-context experiments (passkey retrieval at context=4096 and evaluations on LongBench) and provided results, but they also report errors and GPU OOM issues that limited a fully comprehensive LongBench comparison.", "evidence": "\"We have conducted additional experiments on passkey retrieval, following the setup in [1], with Llama-2-7b at its maximum context length of 4096.\" \"We evaluate QJL and CQ with Llama-2-7b on LongBench.\" \"However, we ran into out-of-memory issues with CQ-4c8b due to GPU memory constraints (Nvidia A100 40G).\"", "combined_author_response": "We sincerely thank the reviewer for their thoughtful comments and suggestions!\n\n---\n\nDear Reviewer KwWk,\n\nWe would like to express our sincere gratitude for your time and effort in reviewing our paper. Your feedback has been invaluable to us.\n\nAs the discussion period is drawing to a close, we kindly request that you review our previous responses to your review. If you have any additional questions or concerns, we would be happy to address them promptly.\n\nThank you again for your valuable contributions.\n\n---\n\nWe sincerely thank the reviewer for the response, and address your concerns as follows.\n\n- We presented the latency measurement with batch size 1, since Reviewer dwgV has specifically asked us for latency experiments in small batch sizes. We quote\n>From Figure 4, it seems all CQ methods perform worse than the fp16 version with small batch sizes. Can you explain why? Is it due to the overhead of (de)quantization?\n- Measurements with batch size 1 highlight the efficiency and latency aspects of our CUDA kernels.\n- We note that the latency comparison we presented between CQ and KIVI is a fair comparison with the same experimental settings: batch size of 1, equal token counts for prefill and decoding, and an identical sliding window size.\n- We agree with the reviewer that latency and throughput at different batch sizes are important for understanding the efficiency of our approach. Hence we will include additional latency and throughput measurements at different batch sizes and context lengths in the camera-ready version.\n\n---\n\nWe appreciate the reviewer for carefully reviewing our paper and offering thoughtful feedback. We have conducted additional experiments on passkey retrieval, following the setup in [1], with Llama-2-7b at its maximum context length of 4096. The passkey retrieval task is similar to the needle retrieval task in RULER. We still have trouble running RULER due to compatibility issues with Docker, and we will continue to work on it. As shown in the table below, CQ consistently outperforms KVQuant at various bit widths on passkey retrieval. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses.\n\n|  | Bit Width | Retrieval Success Rate |\n|---|---|---|\n| KVQuant-4b+1% sparse | 4.32 | **100%** |\n| KVQuant-4b | 4 | **100%** |\n| CQ-2c8b | 4 | **100%** |\n| KVQuant-2b+1% sparse | 2.32 | 94% |\n| CQ-4c9b | 2.26 | **98%** |\n| KVQuant-2b | 2 | 0% |\n| CQ-4c8b | 2 | **96%** |\n| KVQuant-1b+1% sparse | 1.32 | 2% |\n| CQ-8c10b | 1.27 | **78%** |\n| KVQuant-1b | 1 | 0% |\n| CQ-8c8b | 1 | **12%** |\n\n**References**\n\n[1] Zhu, Dawei, et al. \"Pose: Efficient context window extension of llms via positional skip-wise training.\" arXiv preprint arXiv:2309.10400 (2023).\n\n---\n\nWe thank the reviewer again for carefully reviewing our paper and providing constructive feedback. We have conducted additional experiments using the official QJL codebase (https://github.com/amirzandieh/QJL) with an A100 40GB GPU. We evaluate QJL and CQ with Llama-2-7b on LongBench. \nFor QJL, we used a sliding window of size 32 and a group size of 32  (`buffer_size=32,group_size=32`), and set other hyper-parameters following the codebase (`key_quantization_bits=256,key_quantization_bits_initial_layers=512,initial_layers_count=15,outlier_count_general=8,outlier_count_initial_layers=8,value_quantization_bits=2`). For CQ, we use the 4c8b (2-bit) configuration and a sliding window of size 32. The results are presented in the table below.\n\n|  | Bit Width | Qasper | TREC | SAMSum | TriviaQA |\n|---|---|---|---|---|---|\n| FP16 | 16 | 9.52 | 66.00 | 41.69 | 87.72 |\n| QJL | 3.00 | 5.98 | 15.00 | 14.84 | Error |\n| CQ-4c8b | 2.00 | **9.58** | **66.00** | **41.13** | **87.72** |\n\nWe also encountered some challenges when attempting to conduct additional experiments.\n\n1. For QJL on TriviaQA, we ran into the following error: `File \".../QJL/models/llama2_utils_qjl.py\", line 143, in _update_outliers\n    self.outlier_indices = torch.cat([self.outlier_indices, outlier_indices], dim=2).contiguous()\nTypeError: expected Tensor as element 0 in argument 0, but got NoneType`.\n\n2. We tried to directly compare CQ with QJL by following the experimental settings (longchat-7b-v1.5-32k on LongBench) in Table 1 of the QJL paper. However, we ran into out-of-memory issues with CQ-4c8b due to GPU memory constraints (Nvidia A100 40G).\n\nGiven the time constraint of the discussion period, we have made our best effort to provide a fair comparison between CQ and QJL. We are open to further investigation and would welcome specific suggestions from the reviewer on how to resolve the issues above. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses.", "author_response": "We thank the reviewer again for carefully reviewing our paper and providing constructive feedback. We have conducted additional experiments using the official QJL codebase (https://github.com/amirzandieh/QJL) with an A100 40GB GPU. We evaluate QJL and CQ with Llama-2-7b on LongBench. \nFor QJL, we used a sliding window of size 32 and a group size of 32  (`buffer_size=32,group_size=32`), and set other hyper-parameters following the codebase (`key_quantization_bits=256,key_quantization_bits_initial_layers=512,initial_layers_count=15,outlier_count_general=8,outlier_count_initial_layers=8,value_quantization_bits=2`). For CQ, we use the 4c8b (2-bit) configuration and a sliding window of size 32. The results are presented in the table below.\n\n|  | Bit Width | Qasper | TREC | SAMSum | TriviaQA |\n|---|---|---|---|---|---|\n| FP16 | 16 | 9.52 | 66.00 | 41.69 | 87.72 |\n| QJL | 3.00 | 5.98 | 15.00 | 14.84 | Error |\n| CQ-4c8b | 2.00 | **9.58** | **66.00** | **41.13** | **87.72** |\n\nWe also encountered some challenges when attempting to conduct additional experiments.\n\n1. For QJL on TriviaQA, we ran into the following error: `File \".../QJL/models/llama2_utils_qjl.py\", line 143, in _update_outliers\n    self.outlier_indices = torch.cat([self.outlier_indices, outlier_indices], dim=2).contiguous()\nTypeError: expected Tensor as element 0 in argument 0, but got NoneType`.\n\n2. We tried to directly compare CQ with QJL by following the experimental settings (longchat-7b-v1.5-32k on LongBench) in Table 1 of the QJL paper. However, we ran into out-of-memory issues with CQ-4c8b due to GPU memory constraints (Nvidia A100 40G).\n\nGiven the time constraint of the discussion period, we have made our best effort to provide a fair comparison between CQ and QJL. We are open to further investigation and would welcome specific suggestions from the reviewer on how to resolve the issues above. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses."}
{"claim": "The paper relies primarily on perplexity as the evaluation metric, which the reviewer argues is not the most appropriate metric for KV-cache comparisons.", "claim_type": "experimental", "paper_id": "pNnvzQsS4P", "paper_title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vadn2W9weu", "reviewer": "Reviewer_KwWk", "review_text": "Summary: The authors have addressed the KV-cache compression problem by providing a finer quantization level. The KV-cache can pose a significant barrier to the inference of most autoregressive language models, a challenge that has been well studied in recent publications at ICML and NeurIPS. This paper introduces a novel approach by coupling multiple key/value channels together for quantization, exploiting their interdependence to encode the activations in a more information-efficient manner.\n\nStrengths: - The method is novel and demonstrates comparable accuracy to KVQuant, one of the pivotal approaches in this field.\n- It includes a substantial number of experiments to validate accuracy.\n- The quantization method introduced here is novel compared to other approaches. Additionally, the implementation in PyTorch represents a significant contribution.\n\nWeaknesses: - The code is not available. For research on KV cache, it is important to have the code available.\n\n- The section describing the random variables and entropy, specifically line 120, does not explicitly describe the random variables in mathematical notation. This should be revised for clarity. I would like to see this section more polished.\n\n- I believe the LLAMA3 model was available before the NeurIPS submission. Since that time, the authors may have extended their findings to these models. I would like to see the performance of your model in that setting.\n\n- I want to see how the runtime of your method compares to other methods. Recent works, like **QJL**, include good plots for token-wise generation time or end-to-end timing. Since you compare with KVQuant, it is also good to compare with **KIVI**, as it is one of the best methods. I recommend comparing with QJL and **KIVI**, and plotting the runtime alongside these methods.\n\n- I highly recommend the authors run their code on longer context datasets. LongBench could be a great example to evaluate its performance compared to other methods. I would suggest that perplexity is not the best metric for comparison.\n\nRelevant papers: [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache], [QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead]\n\n**If you address the concerns regarding the experiments and provide a broader comparison to the other methods, I would increase my score.**\n\nQuestions: - There is additional overhead regarding storing the centroid for each coupled key/value pair, making it difficult to track. I would like you to mention this overhead and explain how you set those values.", "labeling_timestamp": "2026-01-11T16:24:06.179376", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors do not explicitly concede that relying on perplexity was inappropriate, but they partially address the concern by adding non-perplexity evaluations (latency measurements, passkey/needle retrieval success rates, and LongBench downstream-task results) and agreeing to include more latency/throughput experiments at varied batch sizes.", "evidence": "We agree with the reviewer that latency and throughput at different batch sizes are important for understanding the efficiency of our approach. Hence we will include additional latency and throughput measurements at different batch sizes and context lengths in the camera-ready version.\n\nWe have conducted additional experiments on passkey retrieval, following the setup in [1], with Llama-2-7b at its maximum context length of 4096. ... As shown in the table below, CQ consistently outperforms KVQuant at various bit widths on passkey retrieval.", "combined_author_response": "We sincerely thank the reviewer for their thoughtful comments and suggestions!\n\n---\n\nDear Reviewer KwWk,\n\nWe would like to express our sincere gratitude for your time and effort in reviewing our paper. Your feedback has been invaluable to us.\n\nAs the discussion period is drawing to a close, we kindly request that you review our previous responses to your review. If you have any additional questions or concerns, we would be happy to address them promptly.\n\nThank you again for your valuable contributions.\n\n---\n\nWe sincerely thank the reviewer for the response, and address your concerns as follows.\n\n- We presented the latency measurement with batch size 1, since Reviewer dwgV has specifically asked us for latency experiments in small batch sizes. We quote\n>From Figure 4, it seems all CQ methods perform worse than the fp16 version with small batch sizes. Can you explain why? Is it due to the overhead of (de)quantization?\n- Measurements with batch size 1 highlight the efficiency and latency aspects of our CUDA kernels.\n- We note that the latency comparison we presented between CQ and KIVI is a fair comparison with the same experimental settings: batch size of 1, equal token counts for prefill and decoding, and an identical sliding window size.\n- We agree with the reviewer that latency and throughput at different batch sizes are important for understanding the efficiency of our approach. Hence we will include additional latency and throughput measurements at different batch sizes and context lengths in the camera-ready version.\n\n---\n\nWe appreciate the reviewer for carefully reviewing our paper and offering thoughtful feedback. We have conducted additional experiments on passkey retrieval, following the setup in [1], with Llama-2-7b at its maximum context length of 4096. The passkey retrieval task is similar to the needle retrieval task in RULER. We still have trouble running RULER due to compatibility issues with Docker, and we will continue to work on it. As shown in the table below, CQ consistently outperforms KVQuant at various bit widths on passkey retrieval. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses.\n\n|  | Bit Width | Retrieval Success Rate |\n|---|---|---|\n| KVQuant-4b+1% sparse | 4.32 | **100%** |\n| KVQuant-4b | 4 | **100%** |\n| CQ-2c8b | 4 | **100%** |\n| KVQuant-2b+1% sparse | 2.32 | 94% |\n| CQ-4c9b | 2.26 | **98%** |\n| KVQuant-2b | 2 | 0% |\n| CQ-4c8b | 2 | **96%** |\n| KVQuant-1b+1% sparse | 1.32 | 2% |\n| CQ-8c10b | 1.27 | **78%** |\n| KVQuant-1b | 1 | 0% |\n| CQ-8c8b | 1 | **12%** |\n\n**References**\n\n[1] Zhu, Dawei, et al. \"Pose: Efficient context window extension of llms via positional skip-wise training.\" arXiv preprint arXiv:2309.10400 (2023).\n\n---\n\nWe thank the reviewer again for carefully reviewing our paper and providing constructive feedback. We have conducted additional experiments using the official QJL codebase (https://github.com/amirzandieh/QJL) with an A100 40GB GPU. We evaluate QJL and CQ with Llama-2-7b on LongBench. \nFor QJL, we used a sliding window of size 32 and a group size of 32  (`buffer_size=32,group_size=32`), and set other hyper-parameters following the codebase (`key_quantization_bits=256,key_quantization_bits_initial_layers=512,initial_layers_count=15,outlier_count_general=8,outlier_count_initial_layers=8,value_quantization_bits=2`). For CQ, we use the 4c8b (2-bit) configuration and a sliding window of size 32. The results are presented in the table below.\n\n|  | Bit Width | Qasper | TREC | SAMSum | TriviaQA |\n|---|---|---|---|---|---|\n| FP16 | 16 | 9.52 | 66.00 | 41.69 | 87.72 |\n| QJL | 3.00 | 5.98 | 15.00 | 14.84 | Error |\n| CQ-4c8b | 2.00 | **9.58** | **66.00** | **41.13** | **87.72** |\n\nWe also encountered some challenges when attempting to conduct additional experiments.\n\n1. For QJL on TriviaQA, we ran into the following error: `File \".../QJL/models/llama2_utils_qjl.py\", line 143, in _update_outliers\n    self.outlier_indices = torch.cat([self.outlier_indices, outlier_indices], dim=2).contiguous()\nTypeError: expected Tensor as element 0 in argument 0, but got NoneType`.\n\n2. We tried to directly compare CQ with QJL by following the experimental settings (longchat-7b-v1.5-32k on LongBench) in Table 1 of the QJL paper. However, we ran into out-of-memory issues with CQ-4c8b due to GPU memory constraints (Nvidia A100 40G).\n\nGiven the time constraint of the discussion period, we have made our best effort to provide a fair comparison between CQ and QJL. We are open to further investigation and would welcome specific suggestions from the reviewer on how to resolve the issues above. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses.", "author_response": "We thank the reviewer again for carefully reviewing our paper and providing constructive feedback. We have conducted additional experiments using the official QJL codebase (https://github.com/amirzandieh/QJL) with an A100 40GB GPU. We evaluate QJL and CQ with Llama-2-7b on LongBench. \nFor QJL, we used a sliding window of size 32 and a group size of 32  (`buffer_size=32,group_size=32`), and set other hyper-parameters following the codebase (`key_quantization_bits=256,key_quantization_bits_initial_layers=512,initial_layers_count=15,outlier_count_general=8,outlier_count_initial_layers=8,value_quantization_bits=2`). For CQ, we use the 4c8b (2-bit) configuration and a sliding window of size 32. The results are presented in the table below.\n\n|  | Bit Width | Qasper | TREC | SAMSum | TriviaQA |\n|---|---|---|---|---|---|\n| FP16 | 16 | 9.52 | 66.00 | 41.69 | 87.72 |\n| QJL | 3.00 | 5.98 | 15.00 | 14.84 | Error |\n| CQ-4c8b | 2.00 | **9.58** | **66.00** | **41.13** | **87.72** |\n\nWe also encountered some challenges when attempting to conduct additional experiments.\n\n1. For QJL on TriviaQA, we ran into the following error: `File \".../QJL/models/llama2_utils_qjl.py\", line 143, in _update_outliers\n    self.outlier_indices = torch.cat([self.outlier_indices, outlier_indices], dim=2).contiguous()\nTypeError: expected Tensor as element 0 in argument 0, but got NoneType`.\n\n2. We tried to directly compare CQ with QJL by following the experimental settings (longchat-7b-v1.5-32k on LongBench) in Table 1 of the QJL paper. However, we ran into out-of-memory issues with CQ-4c8b due to GPU memory constraints (Nvidia A100 40G).\n\nGiven the time constraint of the discussion period, we have made our best effort to provide a fair comparison between CQ and QJL. We are open to further investigation and would welcome specific suggestions from the reviewer on how to resolve the issues above. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses."}
{"claim": "The authors fail to quantify or discuss the additional storage overhead incurred by storing a centroid for each coupled key/value pair.", "claim_type": "quantitative", "paper_id": "pNnvzQsS4P", "paper_title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vadn2W9weu", "reviewer": "Reviewer_KwWk", "review_text": "Summary: The authors have addressed the KV-cache compression problem by providing a finer quantization level. The KV-cache can pose a significant barrier to the inference of most autoregressive language models, a challenge that has been well studied in recent publications at ICML and NeurIPS. This paper introduces a novel approach by coupling multiple key/value channels together for quantization, exploiting their interdependence to encode the activations in a more information-efficient manner.\n\nStrengths: - The method is novel and demonstrates comparable accuracy to KVQuant, one of the pivotal approaches in this field.\n- It includes a substantial number of experiments to validate accuracy.\n- The quantization method introduced here is novel compared to other approaches. Additionally, the implementation in PyTorch represents a significant contribution.\n\nWeaknesses: - The code is not available. For research on KV cache, it is important to have the code available.\n\n- The section describing the random variables and entropy, specifically line 120, does not explicitly describe the random variables in mathematical notation. This should be revised for clarity. I would like to see this section more polished.\n\n- I believe the LLAMA3 model was available before the NeurIPS submission. Since that time, the authors may have extended their findings to these models. I would like to see the performance of your model in that setting.\n\n- I want to see how the runtime of your method compares to other methods. Recent works, like **QJL**, include good plots for token-wise generation time or end-to-end timing. Since you compare with KVQuant, it is also good to compare with **KIVI**, as it is one of the best methods. I recommend comparing with QJL and **KIVI**, and plotting the runtime alongside these methods.\n\n- I highly recommend the authors run their code on longer context datasets. LongBench could be a great example to evaluate its performance compared to other methods. I would suggest that perplexity is not the best metric for comparison.\n\nRelevant papers: [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache], [QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead]\n\n**If you address the concerns regarding the experiments and provide a broader comparison to the other methods, I would increase my score.**\n\nQuestions: - There is additional overhead regarding storing the centroid for each coupled key/value pair, making it difficult to track. I would like you to mention this overhead and explain how you set those values.", "labeling_timestamp": "2026-01-11T16:24:16.486606", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The authors' response addresses latency, retrieval accuracy, and comparisons to other methods but does not quantify or discuss the additional storage overhead of storing a centroid per coupled key/value pair, nor provide numbers or analysis of that overhead.", "evidence": "\"We presented the latency measurement with batch size 1, since Reviewer dwgV has specifically asked us for latency experiments in small batch sizes.\"; \"we ran into out-of-memory issues with CQ-4c8b due to GPU memory constraints (Nvidia A100 40G).\"", "combined_author_response": "We sincerely thank the reviewer for their thoughtful comments and suggestions!\n\n---\n\nDear Reviewer KwWk,\n\nWe would like to express our sincere gratitude for your time and effort in reviewing our paper. Your feedback has been invaluable to us.\n\nAs the discussion period is drawing to a close, we kindly request that you review our previous responses to your review. If you have any additional questions or concerns, we would be happy to address them promptly.\n\nThank you again for your valuable contributions.\n\n---\n\nWe sincerely thank the reviewer for the response, and address your concerns as follows.\n\n- We presented the latency measurement with batch size 1, since Reviewer dwgV has specifically asked us for latency experiments in small batch sizes. We quote\n>From Figure 4, it seems all CQ methods perform worse than the fp16 version with small batch sizes. Can you explain why? Is it due to the overhead of (de)quantization?\n- Measurements with batch size 1 highlight the efficiency and latency aspects of our CUDA kernels.\n- We note that the latency comparison we presented between CQ and KIVI is a fair comparison with the same experimental settings: batch size of 1, equal token counts for prefill and decoding, and an identical sliding window size.\n- We agree with the reviewer that latency and throughput at different batch sizes are important for understanding the efficiency of our approach. Hence we will include additional latency and throughput measurements at different batch sizes and context lengths in the camera-ready version.\n\n---\n\nWe appreciate the reviewer for carefully reviewing our paper and offering thoughtful feedback. We have conducted additional experiments on passkey retrieval, following the setup in [1], with Llama-2-7b at its maximum context length of 4096. The passkey retrieval task is similar to the needle retrieval task in RULER. We still have trouble running RULER due to compatibility issues with Docker, and we will continue to work on it. As shown in the table below, CQ consistently outperforms KVQuant at various bit widths on passkey retrieval. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses.\n\n|  | Bit Width | Retrieval Success Rate |\n|---|---|---|\n| KVQuant-4b+1% sparse | 4.32 | **100%** |\n| KVQuant-4b | 4 | **100%** |\n| CQ-2c8b | 4 | **100%** |\n| KVQuant-2b+1% sparse | 2.32 | 94% |\n| CQ-4c9b | 2.26 | **98%** |\n| KVQuant-2b | 2 | 0% |\n| CQ-4c8b | 2 | **96%** |\n| KVQuant-1b+1% sparse | 1.32 | 2% |\n| CQ-8c10b | 1.27 | **78%** |\n| KVQuant-1b | 1 | 0% |\n| CQ-8c8b | 1 | **12%** |\n\n**References**\n\n[1] Zhu, Dawei, et al. \"Pose: Efficient context window extension of llms via positional skip-wise training.\" arXiv preprint arXiv:2309.10400 (2023).\n\n---\n\nWe thank the reviewer again for carefully reviewing our paper and providing constructive feedback. We have conducted additional experiments using the official QJL codebase (https://github.com/amirzandieh/QJL) with an A100 40GB GPU. We evaluate QJL and CQ with Llama-2-7b on LongBench. \nFor QJL, we used a sliding window of size 32 and a group size of 32  (`buffer_size=32,group_size=32`), and set other hyper-parameters following the codebase (`key_quantization_bits=256,key_quantization_bits_initial_layers=512,initial_layers_count=15,outlier_count_general=8,outlier_count_initial_layers=8,value_quantization_bits=2`). For CQ, we use the 4c8b (2-bit) configuration and a sliding window of size 32. The results are presented in the table below.\n\n|  | Bit Width | Qasper | TREC | SAMSum | TriviaQA |\n|---|---|---|---|---|---|\n| FP16 | 16 | 9.52 | 66.00 | 41.69 | 87.72 |\n| QJL | 3.00 | 5.98 | 15.00 | 14.84 | Error |\n| CQ-4c8b | 2.00 | **9.58** | **66.00** | **41.13** | **87.72** |\n\nWe also encountered some challenges when attempting to conduct additional experiments.\n\n1. For QJL on TriviaQA, we ran into the following error: `File \".../QJL/models/llama2_utils_qjl.py\", line 143, in _update_outliers\n    self.outlier_indices = torch.cat([self.outlier_indices, outlier_indices], dim=2).contiguous()\nTypeError: expected Tensor as element 0 in argument 0, but got NoneType`.\n\n2. We tried to directly compare CQ with QJL by following the experimental settings (longchat-7b-v1.5-32k on LongBench) in Table 1 of the QJL paper. However, we ran into out-of-memory issues with CQ-4c8b due to GPU memory constraints (Nvidia A100 40G).\n\nGiven the time constraint of the discussion period, we have made our best effort to provide a fair comparison between CQ and QJL. We are open to further investigation and would welcome specific suggestions from the reviewer on how to resolve the issues above. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses.", "author_response": "We thank the reviewer again for carefully reviewing our paper and providing constructive feedback. We have conducted additional experiments using the official QJL codebase (https://github.com/amirzandieh/QJL) with an A100 40GB GPU. We evaluate QJL and CQ with Llama-2-7b on LongBench. \nFor QJL, we used a sliding window of size 32 and a group size of 32  (`buffer_size=32,group_size=32`), and set other hyper-parameters following the codebase (`key_quantization_bits=256,key_quantization_bits_initial_layers=512,initial_layers_count=15,outlier_count_general=8,outlier_count_initial_layers=8,value_quantization_bits=2`). For CQ, we use the 4c8b (2-bit) configuration and a sliding window of size 32. The results are presented in the table below.\n\n|  | Bit Width | Qasper | TREC | SAMSum | TriviaQA |\n|---|---|---|---|---|---|\n| FP16 | 16 | 9.52 | 66.00 | 41.69 | 87.72 |\n| QJL | 3.00 | 5.98 | 15.00 | 14.84 | Error |\n| CQ-4c8b | 2.00 | **9.58** | **66.00** | **41.13** | **87.72** |\n\nWe also encountered some challenges when attempting to conduct additional experiments.\n\n1. For QJL on TriviaQA, we ran into the following error: `File \".../QJL/models/llama2_utils_qjl.py\", line 143, in _update_outliers\n    self.outlier_indices = torch.cat([self.outlier_indices, outlier_indices], dim=2).contiguous()\nTypeError: expected Tensor as element 0 in argument 0, but got NoneType`.\n\n2. We tried to directly compare CQ with QJL by following the experimental settings (longchat-7b-v1.5-32k on LongBench) in Table 1 of the QJL paper. However, we ran into out-of-memory issues with CQ-4c8b due to GPU memory constraints (Nvidia A100 40G).\n\nGiven the time constraint of the discussion period, we have made our best effort to provide a fair comparison between CQ and QJL. We are open to further investigation and would welcome specific suggestions from the reviewer on how to resolve the issues above. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses."}
{"claim": "The paper does not explain how centroid values for each coupled key/value pair are chosen, configured, or fitted.", "claim_type": "methodology", "paper_id": "pNnvzQsS4P", "paper_title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vadn2W9weu", "reviewer": "Reviewer_KwWk", "review_text": "Summary: The authors have addressed the KV-cache compression problem by providing a finer quantization level. The KV-cache can pose a significant barrier to the inference of most autoregressive language models, a challenge that has been well studied in recent publications at ICML and NeurIPS. This paper introduces a novel approach by coupling multiple key/value channels together for quantization, exploiting their interdependence to encode the activations in a more information-efficient manner.\n\nStrengths: - The method is novel and demonstrates comparable accuracy to KVQuant, one of the pivotal approaches in this field.\n- It includes a substantial number of experiments to validate accuracy.\n- The quantization method introduced here is novel compared to other approaches. Additionally, the implementation in PyTorch represents a significant contribution.\n\nWeaknesses: - The code is not available. For research on KV cache, it is important to have the code available.\n\n- The section describing the random variables and entropy, specifically line 120, does not explicitly describe the random variables in mathematical notation. This should be revised for clarity. I would like to see this section more polished.\n\n- I believe the LLAMA3 model was available before the NeurIPS submission. Since that time, the authors may have extended their findings to these models. I would like to see the performance of your model in that setting.\n\n- I want to see how the runtime of your method compares to other methods. Recent works, like **QJL**, include good plots for token-wise generation time or end-to-end timing. Since you compare with KVQuant, it is also good to compare with **KIVI**, as it is one of the best methods. I recommend comparing with QJL and **KIVI**, and plotting the runtime alongside these methods.\n\n- I highly recommend the authors run their code on longer context datasets. LongBench could be a great example to evaluate its performance compared to other methods. I would suggest that perplexity is not the best metric for comparison.\n\nRelevant papers: [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache], [QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead]\n\n**If you address the concerns regarding the experiments and provide a broader comparison to the other methods, I would increase my score.**\n\nQuestions: - There is additional overhead regarding storing the centroid for each coupled key/value pair, making it difficult to track. I would like you to mention this overhead and explain how you set those values.", "labeling_timestamp": "2026-01-11T16:24:17.708067", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The authors' response discusses latency measurements, various retrieval and comparison experiments, and implementation issues, but does not address how centroid values for coupled key/value pairs are chosen, configured, or fitted.", "evidence": "- We presented the latency measurement with batch size 1, since Reviewer dwgV has specifically asked us for latency experiments in small batch sizes.", "combined_author_response": "We sincerely thank the reviewer for their thoughtful comments and suggestions!\n\n---\n\nDear Reviewer KwWk,\n\nWe would like to express our sincere gratitude for your time and effort in reviewing our paper. Your feedback has been invaluable to us.\n\nAs the discussion period is drawing to a close, we kindly request that you review our previous responses to your review. If you have any additional questions or concerns, we would be happy to address them promptly.\n\nThank you again for your valuable contributions.\n\n---\n\nWe sincerely thank the reviewer for the response, and address your concerns as follows.\n\n- We presented the latency measurement with batch size 1, since Reviewer dwgV has specifically asked us for latency experiments in small batch sizes. We quote\n>From Figure 4, it seems all CQ methods perform worse than the fp16 version with small batch sizes. Can you explain why? Is it due to the overhead of (de)quantization?\n- Measurements with batch size 1 highlight the efficiency and latency aspects of our CUDA kernels.\n- We note that the latency comparison we presented between CQ and KIVI is a fair comparison with the same experimental settings: batch size of 1, equal token counts for prefill and decoding, and an identical sliding window size.\n- We agree with the reviewer that latency and throughput at different batch sizes are important for understanding the efficiency of our approach. Hence we will include additional latency and throughput measurements at different batch sizes and context lengths in the camera-ready version.\n\n---\n\nWe appreciate the reviewer for carefully reviewing our paper and offering thoughtful feedback. We have conducted additional experiments on passkey retrieval, following the setup in [1], with Llama-2-7b at its maximum context length of 4096. The passkey retrieval task is similar to the needle retrieval task in RULER. We still have trouble running RULER due to compatibility issues with Docker, and we will continue to work on it. As shown in the table below, CQ consistently outperforms KVQuant at various bit widths on passkey retrieval. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses.\n\n|  | Bit Width | Retrieval Success Rate |\n|---|---|---|\n| KVQuant-4b+1% sparse | 4.32 | **100%** |\n| KVQuant-4b | 4 | **100%** |\n| CQ-2c8b | 4 | **100%** |\n| KVQuant-2b+1% sparse | 2.32 | 94% |\n| CQ-4c9b | 2.26 | **98%** |\n| KVQuant-2b | 2 | 0% |\n| CQ-4c8b | 2 | **96%** |\n| KVQuant-1b+1% sparse | 1.32 | 2% |\n| CQ-8c10b | 1.27 | **78%** |\n| KVQuant-1b | 1 | 0% |\n| CQ-8c8b | 1 | **12%** |\n\n**References**\n\n[1] Zhu, Dawei, et al. \"Pose: Efficient context window extension of llms via positional skip-wise training.\" arXiv preprint arXiv:2309.10400 (2023).\n\n---\n\nWe thank the reviewer again for carefully reviewing our paper and providing constructive feedback. We have conducted additional experiments using the official QJL codebase (https://github.com/amirzandieh/QJL) with an A100 40GB GPU. We evaluate QJL and CQ with Llama-2-7b on LongBench. \nFor QJL, we used a sliding window of size 32 and a group size of 32  (`buffer_size=32,group_size=32`), and set other hyper-parameters following the codebase (`key_quantization_bits=256,key_quantization_bits_initial_layers=512,initial_layers_count=15,outlier_count_general=8,outlier_count_initial_layers=8,value_quantization_bits=2`). For CQ, we use the 4c8b (2-bit) configuration and a sliding window of size 32. The results are presented in the table below.\n\n|  | Bit Width | Qasper | TREC | SAMSum | TriviaQA |\n|---|---|---|---|---|---|\n| FP16 | 16 | 9.52 | 66.00 | 41.69 | 87.72 |\n| QJL | 3.00 | 5.98 | 15.00 | 14.84 | Error |\n| CQ-4c8b | 2.00 | **9.58** | **66.00** | **41.13** | **87.72** |\n\nWe also encountered some challenges when attempting to conduct additional experiments.\n\n1. For QJL on TriviaQA, we ran into the following error: `File \".../QJL/models/llama2_utils_qjl.py\", line 143, in _update_outliers\n    self.outlier_indices = torch.cat([self.outlier_indices, outlier_indices], dim=2).contiguous()\nTypeError: expected Tensor as element 0 in argument 0, but got NoneType`.\n\n2. We tried to directly compare CQ with QJL by following the experimental settings (longchat-7b-v1.5-32k on LongBench) in Table 1 of the QJL paper. However, we ran into out-of-memory issues with CQ-4c8b due to GPU memory constraints (Nvidia A100 40G).\n\nGiven the time constraint of the discussion period, we have made our best effort to provide a fair comparison between CQ and QJL. We are open to further investigation and would welcome specific suggestions from the reviewer on how to resolve the issues above. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses.", "author_response": "We thank the reviewer again for carefully reviewing our paper and providing constructive feedback. We have conducted additional experiments using the official QJL codebase (https://github.com/amirzandieh/QJL) with an A100 40GB GPU. We evaluate QJL and CQ with Llama-2-7b on LongBench. \nFor QJL, we used a sliding window of size 32 and a group size of 32  (`buffer_size=32,group_size=32`), and set other hyper-parameters following the codebase (`key_quantization_bits=256,key_quantization_bits_initial_layers=512,initial_layers_count=15,outlier_count_general=8,outlier_count_initial_layers=8,value_quantization_bits=2`). For CQ, we use the 4c8b (2-bit) configuration and a sliding window of size 32. The results are presented in the table below.\n\n|  | Bit Width | Qasper | TREC | SAMSum | TriviaQA |\n|---|---|---|---|---|---|\n| FP16 | 16 | 9.52 | 66.00 | 41.69 | 87.72 |\n| QJL | 3.00 | 5.98 | 15.00 | 14.84 | Error |\n| CQ-4c8b | 2.00 | **9.58** | **66.00** | **41.13** | **87.72** |\n\nWe also encountered some challenges when attempting to conduct additional experiments.\n\n1. For QJL on TriviaQA, we ran into the following error: `File \".../QJL/models/llama2_utils_qjl.py\", line 143, in _update_outliers\n    self.outlier_indices = torch.cat([self.outlier_indices, outlier_indices], dim=2).contiguous()\nTypeError: expected Tensor as element 0 in argument 0, but got NoneType`.\n\n2. We tried to directly compare CQ with QJL by following the experimental settings (longchat-7b-v1.5-32k on LongBench) in Table 1 of the QJL paper. However, we ran into out-of-memory issues with CQ-4c8b due to GPU memory constraints (Nvidia A100 40G).\n\nGiven the time constraint of the discussion period, we have made our best effort to provide a fair comparison between CQ and QJL. We are open to further investigation and would welcome specific suggestions from the reviewer on how to resolve the issues above. We are also happy to provide additional clarification on any follow-up questions. We respectfully request that the reviewer reconsider our paper in light of these responses."}
{"claim": "The paper's pocket encoder architecture is directly borrowed from the Uni-Mol model, reducing the paper's claimed methodological originality.", "claim_type": "methodology", "paper_id": "uMAujpVi9m", "paper_title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "9ap4s1SZlZ", "reviewer": "Reviewer_yfM3", "review_text": "Summary: This paper primarily aims to enhance the pocket pretraining method, as existing approaches only consider pockets during pretraining. There are two main contributions in this paper: (1) The authors introduce a novel method, ProFSA, for pocket pretraining, which extracts additional information from corresponding ligands. However, the number of pocket-ligand complex structures is quite limited in existing datasets. (2) To address this issue, the authors generate over 5 million complexes by segmenting fragments and their corresponding pockets in protein structures. By aligning features of fragments and pockets, the pocket encoder learns the interaction between fragments and pockets. The authors design downstream tasks such as pocket druggability prediction, pocket matching, and ligand binding affinity prediction to demonstrate the effectiveness of ProFSA.\n\nStrengths: The authors propose a new perspective of pretraining pockets and construct a large-scale dataset, which data distribution is also considered, to make the efficient pre-training possible.\n\nThe results are competitive, especially for zero-shot settings.\n\nAbundant experiments and ablation study support the argument and result of the authors.\n\nWeaknesses: 1. The technical novelty is limited.\n  - The pocket encoder is borrowed from Uni-Mol.\n  - The contrastive loss is the vanilla form of classical contrastive learning.\n\n2. The bound of Theorem 3.1 is trivial. The authors claim that the bound naturally exists for these representations extracted by pretrained molecule models. However, it's a bit counterintuitive, because many models not pretrained on molecule datasets also fulfill this prior. So, can these models be used for this task? **I strongly suggest removing this part from the paper**.\n\n3. Some issues about dataset creation:\n - 3.1. The authors consider the distribution of ligand size and pocket size when designing the dataset. However, molecules possess more properties that can also lead to imbalance. It would be better to, at least, add some discussion about this issue.\n - 3.2. In the second stage of the data construction process, the approach to defining pockets needs further explanation or an ablation study.\n\n4. Experiments: It would be better to add some biological justification or visualization of the results.\n\nFor this paper, one fact is that the technical novelty is below the bar of ICLR. However, I admire the simple but effective model for the right question. It's a struggle for me to make a decision. I will maintain a neutral attitude and make my final decision after the discussion.\n\nQuestions: See weakness.", "labeling_timestamp": "2026-01-11T16:24:29.667836", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors explicitly acknowledge using the Uni-Mol pocket encoder architecture for fair comparison (admitting the borrowing) but dispute the implication that this undermines novelty by emphasizing their data-centric contributions, different training (no Uni-Mol pretrained weights) and other methodological differences.", "evidence": "\"we intentionally borrowed a pocket encoder from Uni-Mol to make a fair comparison with it ... Notably, we didn't load the pretrained weights of Uni-Mol pocket encoder. We only use the same backbone architecture and the pretraining was completely done on our processed data with our training strategy.\"", "combined_author_response": "Dear AC and reviewers,\n\nWe sincerely appreciate your time and efforts in reviewing our work. Based on your suggestions, we have revised our paper. We would like to use this section to reiterate the explanation for some common concerns and summarize the contributions of our paper.\n\nIn our revised paper, we provide a detailed explanation of the motivation and theory behind our method: **the mechanisms of pocket-peptide interactions and pocket-ligand interactions are very similar**. Our proposed **data-centric** pretraining pipeline is unique in its use of peptides to mimic small molecules, a method that is, to our knowledge, **the first of its kind**. We expanded our discussion about its biological insights in **Section 3**. We add a **new Figure 2 to illustrate different types of non-covalent interactions** that are common for both ligand-protein data and intra-protein data. Following this theory, we have designed a pipeline to generate **over 5 million** synthetic ligand-protein pairs, which greatly eases the **data scarcity** problem in the field of protein-ligand interaction learning. We also prove our data quite effective in various downstream tasks, including pocket property prediction, pocket matching, ligand binding affinity prediction, ligand efficiency prediction, and protein-protein interaction tasks. The **last two tasks are newly added** to further support our method (**Appendix C5 and C6**). We are confident that our **ready-to-release synthetic dataset** will significantly benefit the AI for drug discovery community. Its impact extends beyond enhancing pocket representations in downstream analyses, as it also equips researchers with a valuable resource for a range of applications, such as protein-ligand docking, structure-based drug design, and virtual screening. Importantly, these areas often face challenges due to **a lack of training data necessary for learning effective binding patterns between protein pockets and ligands**. \n\nIn our revised paper, we recognize the differences between real ligands and pseudo ligands (peptides). We address these discrepancies and present a distribution plot of various properties in **Appendix G**. Our approach primarily leverages the **analogous nature of ligand-pocket and peptide-pocket interactions**. Therefore, despite these differences, our model can **still extract valuable interaction information from the pretraining dataset**, which enhances its performance in downstream tasks. This is illustrated in **Figure 4(a)**, showing the benefits of interaction-aware pretraining. We have also undertaken measures like distribution alignment and freezing the molecular encoder to mitigate the impact of these discrepancies. The positive results of these efforts, as evidenced in **Table 5**, confirm their effectiveness and necessity. Notably, our pretrained model can be **further finetuned with real labeled ligand-pocket pairwise data**, as we show in the Ligand Binding Affinity task in **section 4.3**.\n\nAs for other questions, we have added more ablation studies on our design choices, including the impact of a fixed molecular encoder (Table 5), and different threshold values to define pockets(Appendix C.4). \n\nWe are immensely grateful for the invaluable feedback from all the reviewers, which has guided us in refining and clarifying our work. We hope that the revised version of our paper, coupled with the discussion period, will more clearly highlight the novelty, effectiveness, and contributions of our approach to the field.\n\n---\n\nThank you for your invaluable feedback on our paper. In response to your insightful suggestions, we have made several modifications in our revised paper. \n\nWe have cited additional papers in Section 2.2 following your advice. The description of loss terms in Section 3.2 has been corrected.  COSP was not included as a baseline for the Ligand Binding Affinity (LBA)  task because it is not open-sourced, and its performance on LBA was not evaluated in their published work. We used it as a baseline in the pocket-matching task.\n\nIn Section 3, we've expanded our discussion to answer your question on **ProFSA's effectiveness without distributional alignment**. This section now offers a detailed explanation of the foundational concepts and justifications for our method. We draw attention to the parallels between ligand-protein and intra-protein non-covalent interactions, as shown in **Figure 2**. This comparison supports our strategy of using peptides as stand-ins for actual ligands to mimic pocket-ligand interactions. Consequently, **even in the absence of distributional alignment, the types of interactions remain comparably relevant**, which explains why ProFSA continues to function, albeit with reduced effectiveness, as demonstrated in Table 5. We also want to clarify that distributional alignment is only applied to fragment-pocket complex as a whole, and **it would not change the definition of the pocket with a given ligand**. Without this alignment, these fragments are chosen uniformly, with lengths varying from 1 to 7 residues, and pockets are consistently identified based on residues within a 6Å radius of these peptide fragments.\n\nWe are grateful for the time and effort you have dedicated to reviewing our work. Your thorough and constructive feedback has significantly contributed to the refinement of our research. Please let us know if there are any other aspects of our paper that you would like us to address or clarify. We look forward to your further feedback.\n\n---\n\nThank you for your insightful feedback. In response to your concerns regarding the scope of our tasks, we have expanded our research in the revised paper. We now include additional tasks such as **ligand efficiency prediction** and **protein-protein affinity prediction**, detailed in **Appendix C5 and C6**. These tasks were incorporated to enhance the robustness of our method. We acknowledge that they are currently in the appendix due to space constraints, but we are open to relocating them to the main text if it would be beneficial.\n\nRegarding your concern about the lack of recent baselines, we provided a detailed explanation in our previous response. We want to reiterate that our benchmarks do include the latest baselines, with some as recent as 2023.\n\nWe greatly appreciate the time and effort you have invested in reviewing our work. Your feedback has been invaluable in refining our research. Please let us know if there are any other aspects of our paper that you would like us to address or clarify. We look forward to your further feedback.\n\n---\n\nIn our revised paper, we have incorporated changes following feedback from another reviewer, which we believe also addresses your concerns and queries.\n\nRegarding your question on the evidence supporting our approach, particularly the use of peptides to simulate small molecules, we've enriched **Section 3.1**. A new paragraph there now provides an in-depth explanation of non-covalent interactions. To aid understanding, **Figure 2** has been updated with visualizations that include both real and pseudo receptor-ligand pairs, illustrating three types of interactions. Furthermore, we delve into the details of **how intra-protein interactions mirror protein-ligand interactions**, involving specific types of amino acids.\n\nTo better evaluate our design choices, the ablation studies have been extended. Alongside the existing studies in **Table 4** (different pretrained molecular encoders), **Table 5** (distribution alignment), and **Figure 5** (data scale). we've added a study on the impact of using a fixed molecular encoder in **Table 5** in the main text to address your question on the fixed molecular encoder. We've also included an ablation study on varying **cutoff values for pocket definition**, placed in the **Appendix C.4** due to its more focused scope. We can move it to the main part of the paper in the future if needed.\n\nWe acknowledge your concern about the **differences between true and pseudo ligands**, so we've expanded our discussion on the distinct properties of our curated dataset versus the PDBBind dataset in **Figure 9** and **Section 3.2**. The ablation study in **Table 5** demonstrates our methods and their effectiveness in minimizing these discrepancies. Notably, the performance significantly drops without our distribution alignment or fixed molecular encoder, highlighting our efforts to mitigate these issues. The updated **Figure 2** further supports this, showing that while the properties of true and pseudo ligands may differ, **the interaction types are consistent.** We hope these results can ease your concerns about our synthetic dataset, and they also respond to your query about **why we kept the molecular encoder constant during pocket pretraining.**\n\nWe appreciate the constructive critiques and thoughtful engagement from you. Your feedback has undeniably contributed to the refinement and strengthening of our paper. We sincerely value the time and expertise invested in the review process, and we look forward to any further suggestions you may have.\n\n---\n\nWe sincerely appreciate the thoughtful feedback provided on our paper and have diligently incorporated your suggestions into this revised version. In response to your guidance, we have made three significant modifications to the core content of our paper, all of which aim to enhance clarity and address the concerns raised.\n\nTaking into account your suggestion, we have removed **Theorem 3.1** from the main body of our paper. Additionally, we've improved the explanation and included an ablation study on our method of defining pockets. This ablation study, focusing on the cutoff values and primarily a hyperparameter issue, is placed in the Appendix due to its more limited scope of insight. We can move it to the main body of the paper in the future if needed. We have also expanded our discussion on the differences between our dataset and the PDBBind dataset, elucidating how fixed molecule encoders contribute to mitigating this discrepancy.\n\nIn **Section 3.1**, we have introduced a new paragraph dedicated to providing readers with a deeper understanding of non-covalent interactions. To facilitate comprehension, we have included visualizations featuring three types of interactions, incorporating both real and pseudo receptor-ligand pairs in the new **Figure 2**. Furthermore, we delve into the details of how intra-protein interactions mirror protein-ligand interactions, involving specific types of amino acids.\n\nThe biological justifications for the efficacy of interaction-aware pretraining are now more extensively explored in our revised paper. Specifically, we present a compelling showcase featuring estradiol-binding proteins, offering insights into the geometric disparities between the compared proteins and highlighting the similarities in their binding interfaces in the new **Figure 4a**. The existing visualization of BioLip pocket representations in the new **Figure 4b** can make our claims more convincing. Together, these visual aids illustrate how our interaction-aware pretraining empowers models to focus on crucial interface residues, discern subtle distinctions, and disregard irrelevant geometric dissimilarities.\n\nWe have also included an in-depth discussion of pretrained molecule encoders in the newly added **Section 2.3**, further enriching the theoretical foundation of our work.\nAll changes can be found in the renewed paper, which is updated lively.\n\nWe genuinely value the constructive input you provided during the review process, as it has undoubtedly contributed to the refinement of our paper. Your insights have proven invaluable in elevating the clarity and depth of our contributions to the field. We express our gratitude for the time and expertise invested in reviewing our work, and we look forward to any additional feedback or suggestions you may have.\n\n---\n\n# Response 2/2\n\n## Response to the question on N Terminal and C Terminal\n\nWhen a peptide bond is formed (green), one amine group and one carboxy group are left untouched. Therefore, in a linear polypeptide, there would always be a free amine group and a free carboxy group, and they are called the N terminal(blue) and the C terminal(red) respectively in biochemistry. You can find the visualization in **Appendix H**.\n\n## Response to fixing the molecule encoder in contrastive learning\n\nFrom an **effectiveness** standpoint, as you mentioned before, there are unavoidable discrepancies between pseudo and true ligands. If we didn't fix our molecule encoder during pretraining, then the conditions of Theorem 3.1 might not be met. The ablation results presented in the previous response clearly indicate a performance decline when the molecular encoder is not fixed during contrastive learning, particularly in the context of pocket matching. This supports our assertion that the pocket encoder struggles to acquire chemical knowledge without the stability provided by a fixed molecular encoder.\n\nFrom an **efficiency** standpoint, a fixed molecule encoder also offers significant advantages. It reduces memory usage and accelerates training speed because all molecule embeddings can be pre-computed before training. This eliminates the need for feed-forward and back-propagation operations of the molecule encoder during training, leading to efficiency gains. Additionally, adapting to different molecule encoders becomes more straightforward when they are not co-trained during the pretraining process. This approach allows for more flexibility and ease in integrating various encoding methodologies **even if the molecule encoder is not accessible for training**. That is to say, our approach can even be used with undifferentiable molecular representations.\n\n## Response to extending to other tasks like protein-protein interaction prediction\n\nThank you for pointing out the potential application of our framework to protein-protein interaction (PPI) tasks. We have to point out that the interactions between proteins and proteins are far different from proteins and molecules. Specifically, the interaction areas are usually much larger, but shallower, and involved functional groups are less diverse due to limited types of natural amino acids. Therefore, several changes should be made to extend our ProFSA method to PPI tasks:\n\n1. As for data creation, unlike the localized and specific nature of pocket-ligand interactions, protein-protein interactions are **more global and involve broader protein features**. This requires a shift in the data creation process to split entire protein structures into domains instead of short fragments and to capture potential interactions between them. \n\n2. The interaction interface between protein and protein is much larger. We might need to change from atomic-level to residue-level representations to efficiently capture interaction information.\n\n3. For contrastive learning training, ProFSA uses a fixed pretrained molecular encoder to cope with various small molecules and their atomic representation. However, for protein-protein interaction, it is not necessary as we could only model 20 types of amino acids at the residue level.\n\nDespite several changes that need to be done for PPI tasks, we find that we could divide protein complexes into local interactions. When the complex structures and monomer structures are provided, we have achieved a reasonable result on a flexible PPI affinity prediction benchmark with our current model in a zero-shot way. \n\n| Method            | Spearman $\\uparrow$  |\n|-------------------|----------------------|\n| SchNet            | 0.072 ± 0.021        |\n| DimeNet++         | 0.171 ± 0.054        |\n| EGNN              | 0.080 ± 0.038        |\n| TorchMD           | 0.117 ± 0.008        |\n| GET               | **0.363 ± 0.017**    |\n| ProFSA(zero-shot) | _0.248_              |\n\nSurprisingly, as a **zero-shot** method, ProFSA is able to outperform other **supervised learning** models except for GET, a newly proposed unified model. This demonstrates our approach is also able to capture protein-protein interaction information despite it being designed for protein-ligand interaction. \n\nA detailed explanation of this experiment is shown in **Appendix C6**.\n\n---\n\n# Response 1/2\n\n## Response to evidence for the construction of the pseudo-ligand and using peptides to replace small molecules \n\nAs we discussed in the paper, pseudo-ligands share similar sizes as real ligands, and they also make similar non-covalent interactions with the pocket. To further support our proposal, we would like to cite another paper **\"A defined structural unit enables de novo design of small-molecule–binding proteins\"**[1], published in Science 2020, which supports that **intra-protein interactions are similar to protein-ligand interactions.** That is the reason we use peptides to **represent** small molecules to let the pocket encoder learn the interaction information. \n\nAlso, we'd like to clarify that our intention is not to **\"replace\"** small molecules with peptides. It's a pretraining framework that leverages abundant protein-only data and uses peptides to **simulate** pocket-small molecule interactions for enhanced pocket representations. Following this pretraining phase, **the model can be finetuned with datasets that include real small molecules, ensuring its effectiveness in practical applications**.\n\nTo demonstrate the validity of constructing pseudo-ligands, we provide visualizations in **Appendix F** showing that interactions between pockets and our pseudo-ligands are similar to those with true ligands. These figures reveal shared interaction types like **hydrogen bonding**, **$\\pi-\\pi$ stacking**, and **salt bridge**, depicted with color-coded dashed lines for each interaction type. This should help clarify the rationale and evidence supporting our approach.\n\n[1] Polizzi, Nicholas F., and William F. DeGrado. \"A defined structural unit enables de novo design of small-molecule–binding proteins.\" Science 369, no. 6508 (2020)\n\n## Response to ablation studies evaluating the impact of critical design choices\n\nThanks for your advice on adding more ablation studies on design choices. We did ablation studies on the effectiveness of distributional alignment, where the major difference is that fragment sizes are modified. We found that aligned fragment sizes with real ligands could provide the best performance. The result is shown in **Table 5**.\n\nAs for the distance thresholds, we define the pocket following the UniMol setup. We also found our model can also adapt to an 8Å setup in the ToughM1 experiment. Following your advice, we did an ablation study on the distance thresholds. We tested our method with three different thresholds: 4Å, 6Å, and 8Å. The result is shown below:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| 4Å | 0.7062 | 0.7549 | 0.1240 | 0.1095 | 28.29 | 13.07 |\n| 6Å | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| 8Å | 0.8322 | 0.8292 | 0.1256 | 0.1125 | 34.83 | 12.92 |\n\nSince the 8Å threshold corresponds with the pocket definition in the Kahraman and Tough M1 datasets, it leads to optimal results. Our decision to use a 6Å threshold was made to align with the methodology of pretraining data creation by Uni-Mol, facilitating a fair comparison. Notably, even with the 6Å threshold, we achieved strong results in the pocket-matching task, which serves as a testament to the effectiveness of our approach.\n\n## Response to substantial discrepancies between pseudo and real ligands\n\nThank you for pointing out the discrepancies between pseudo and real ligands. We understand and acknowledge your concern. To mitigate the impact of this discrepancy, we have made several efforts. First, we performed **distribution alignment** to make the data distribution of pseudo-ligands more similar to that of true ligands in PDBbind. Additionally, we **fixed the molecule encoder during pretraining** to prevent it from being misled by the discrepancy. Our ablation studies have shown that these strategies are effective:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nThe table above shows that lacking distribution alignment and not fixing the molecular encoder during pretraining both lead to lower performance, underscoring the effectiveness of our methods in reducing discrepancies.\n\nTheorem 3.1 theoretically supports our approach's efficacy, even with discrepancies between pseudo and real ligands. Empirically, our method outperforms other pretraining techniques, confirming its effectiveness despite these discrepancies.\n\n---\n\n## Response to limited tasks and potential in protein-ligand binding pose prediction\n\nIn our paper, the primary objective is to assess the effectiveness of our pretraining method and the quality of the trained pocket representations. To achieve this, we focused on two types of downstream tasks: pocket-only tasks (pocket property prediction and pocket matching) and pocket-ligand interaction tasks (ligand binding affinity prediction).\n\nWe appreciate the suggestion to apply our method to the protein-ligand binding pose prediction task. However, it's important to note that our current evaluation framework is designed to specifically assess **pocket representations**. For the tasks we chose, the architecture is straightforward: either a **zero-shot** evaluation or a **simple MLP** for mapping embeddings to predictions. In contrast, protein-ligand binding pose prediction often involves **complex methodologies**. For instance, state-of-the-art methods like DiffDock[1] require training a diffusion generative model and a separate confidence model, while other approaches like EDM-Dock[2], rely on reconstructing ligand conformations from predicted distance maps. These methods are not end-to-end and do not directly align with our objective of evaluating pretrained pocket representations.\n\nNevertheless, we recognize the potential of our method in enhancing existing binding pose prediction techniques. To integrate our approach, we would need to modify our framework. Using the same data creation strategy, we could train a binding pose prediction model with our preprocessed data, which could then be further fine-tuned using real pocket-ligand pair data from sources like PDBbind. Additionally, our data could be used to train a side-chain packing model, allowing for side-chain flexibility during docking. Thank you again for your advice, and we will leave protein-ligand binding pose prediction as a separate future work for our method.\n\nIn response to the need for evaluating our method on a broader range of tasks, we have extended our analysis to include two additional downstream tasks: LEP (Ligand Efficacy Prediction), and PPA (Protein-Protein Affinity Prediction).\n\nresult for LEP:\n\n| Method      | AUROC $\\uparrow$ | AUPRC $\\uparrow$ |\n|-------------|--------------|--------------|\n| ATOM3D-GNN  | 0.681        | 0.598        |\n| GeoSSL      | 0.776±0.03   | 0.694±0.06   |\n| Uni-Mol     | 0.782±0.02   | 0.695±0.07   |\n| ProFSA      | 0.840±0.04   | 0.806±0.04   |\n\nGeoSSL and Uni-Mol, both pretraining methods, yield comparable results. However, ProFSA outperforms these methods, demonstrating the advantage of our pocket pretraining approach.\n\nresult for PPA:\n\n| Method            | Spearman $\\uparrow$  |\n|-------------------|----------------------|\n| SchNet            | 0.072 ± 0.021        |\n| DimeNet++         | 0.171 ± 0.054        |\n| EGNN              | 0.080 ± 0.038        |\n| TorchMD           | 0.117 ± 0.008        |\n| GET               | **0.363 ± 0.017**    |\n| ProFSA(zero shot) | _0.248_              |\n\nAs a **zero-shot** method, ProFSA is able to outperform other **supervised learning** models except for GET, a newly proposed unified model. This demonstrates our approach is able to capture protein-protein interaction information despite it being designed for protein-ligand interaction modeling.\n\nYou can find detailed experiment settings and results in **Appendix C5 and C6**.\n\n[1] Corso et al., \"Diffdock: Diffusion steps, twists, and turns for molecular docking.\", ICLR 2023.\n[2] Masters et al., \"Deep learning model for efficient protein–ligand docking with implicit side-chain flexibility.\" Journal of Chemical Information and Modeling 63, no. 6 (2023).\n\n## Response to the issue of lack of latest baselines\n\nFor the pocket matching task, CoSP is a newly proposed baseline which is published in ECML PKDD 2023. Alongside CoSP and Uni-Mol, we selected some of the most effective baseline results presented in the CoSP paper. We also tried to evaluate recent methods like PocketAnchor (Li et al., Cell Systems 2022) on our own since they are not tested on the pocket matching task. However, since the result didn't outperform other baseline machine learning methods(Uni-Mol, CoSP, and DeeplyTough), we decided not to include it in our final baseline comparison. We can include it in the camera-ready version if needed.\n\nFor the ligand binding affinity task, a lot of newly proposed baselines are included, i.e. ProNet (Wang et al., 2022b); as well as pretraining methods such as GeoSSL (Liu et al., 2023), EGNN-PLM (Wu et al., 2022), DeepAffinity (Karimi et al., 2019) and Uni-Mol (Zhou et al., 2023).\n\n---\n\n## Response to lack of related works\n\nThanks for providing us with more insightful related works that could support our arguments. We actually have cited these three papers in the section on ligand binding affinity experiments, but we are also happy to add these in Related Works section 2.2. You can find the change in pdfdiff.\n\n## Response to lack of COSP as a baseline in Table 3\n\nBecause best to our knowledge, they did not release their code and they did not test their method on the ligand binding affinity dataset. \n\n## Response to the question on the loss terms\n\nThank you for your advice. We apologize that we accidentally mentioned loss1 and loss2 in reverse, and we are sorry for the confusing statement. You are correct that the first loss is to identify the true protein pocket when given a pseudo-ligand. We revise the original statement to: \"The primary purpose of the first loss is to identify the true protein pocket from a batch of samples when given a pseudo-ligand. Similarly, the second loss seeks to identify the corresponding ligand fragment for a given pocket.\" You can also find the change in pdfdiff.\n\n## Response to the question on the length of the pocket representation without alignment\n\nWith or without distributional alignment, pockets are always defined by the given protein fragment with a fixed distance cutoff (6Å in our works, following the UniMol setup). The distributional alignment process merely samples these pairs to match the sizes of real ligands and pockets. Without distributional alignment, fragments are uniformly sampled from 1 to 7 residues. Pockets are similarly determined as with residues within the range of 6Å around peptide fragments.\n\n---\n\n# Question about the confidence score\n\nThanks for your review. We found your understanding and judgment of our paper comprehensive and precise. Also, you provided detailed and thoughtful advice for our paper, which helped a lot. We sincerely appreciate your review, but we also wonder if there is any misunderstanding here, as you only give a confidence score of 1.\n\n---\n\n# Response 3/3\n\n## Response to biological justification and visualization of the results\n\nOur method's **biological justification** for achieving good experimental results is based on the properties and interactions of pseudo-ligands. As discussed in our paper, these pseudo-ligands are similar in size to real ligands and engage in comparable non-covalent interactions with protein pockets. This fundamental similarity is crucial for the effectiveness of our approach.\n\nMoreover, our methodology is further supported by the findings in the 2020 Science paper titled \"A defined structural unit enables de novo design of small-molecule–binding proteins.\"[1] This research underscores that **intra-protein interactions are analogous to protein-ligand interactions**, which validates our use of peptides as proxies for small molecules. By employing peptides as pseudo-ligands in this manner, our pocket encoder is able to learn and replicate the interaction dynamics typically observed in real ligand scenarios. This understanding is pivotal to the success of our method in downstream applications.\n\nTo better justify our methods and results, following your advice, we also provide some visualizations in **Appendix F**. **Figure 6** illustrates that the pseudo pairs we created share various interaction types commonly found in real pocket-ligand pairs, such as **hydrogen bonding, $\\pi-\\pi$ stacking, and salt bridge**. In the figures, each type of interaction is represented by dashed lines, color-coded to correspond with the specific interaction type.\n\nAnother visualization of our pocket-matching result is shown in **Figure 7**. We showed an example of two non-homology ATP binding pockets to explain why pocket-matching can benefit from interaction-aware pretraining so that we have achieved superior results in the Kahraman dataset and the BioLip t-SNE visualization. The PEP carboxykinase(PDB:1AYL) and tRNA synthetase(PDB:1B8A) are two ATP-binding proteins that share zero sequence similarity (verified with the BLAST) extracted from the Kahraman dataset. However, as they are both fueled by ATP, their binding site shares similar binding patterns. The cation-$\\pi$ interactions(blue dash lines) and salt bridges (magenta dash lines) are important to the ATP binding, which can be viewed as convergent evolutions at the molecule level. Even though, these two pockets are very distinct in terms of shapes and sizes because they bind ATPs in different conformations. Therefore, biochemical interactions are the key to accomplishing the pocket-matching task, which is ignored in previous self-supervised learning methods like Uni-Mol.\n\n[1] Polizzi, Nicholas F., and William F. DeGrado. \"A defined structural unit enables de novo design of small-molecule–binding proteins.\" Science 369, no. 6508 (2020)\n\n---\n\n# Response 2/3\n\n## Response to the bound of Theorem 3.1 \n\nWe are not very clear about the point of your question. We have two understandings and they are replied as follows.\n\nIf you mean $ ||t − t ^{(0)}|| < 1/2 l_T$ is trivial, please note that it is only a condition of the theorem. As you said, many other encoders can also satisfy this condition. This shows that our method is applicable to many different molecular encoders with or without pretraining as we proved in **Table 4**. Moreover, the condition is only a necessary condition. We also need the pretraining contrastive loss to be sufficiently small to guarantee a small contrastive loss between pockets and real ligands. In fact, in the process of our early exploration, we noticed that some encoders cannot achieve a low pretraining loss, indicating the entire conditions of the theorem are nontrivial.\n\nIf you mean that the conclusion of theorem 3.1 is trivial, we want to emphasize that our conclusion is $\\lim_{L_i(t, s) =0}L_i(t^{(0)}, s) =0$. (We write it in a $\\epsilon$-$\\delta$ language in paper. They are equivalent forms.) Our conclusion shows the loss containing real ligands is consistent with the loss containing pseudo ligands that we optimized in pre-training. \n\nIn contrast,  a trivial result from $||t − t ^{(0)}|| < 1/ 2l_T$ is that $\\lim_{L_i(t, s) =0}L_i(t^{(0)}, s) <B$, B is a bound related to l_t and the representations. It does not guarantee a consistent loss and is substantially different from our result.\nTherefore, our theorem is nontrivial and reveals the transfer ability of our contrastive pre-training from the pseudo-ligand domain to the real ligand domain.\n\nAs for this theoretical result, we are open to further discussion.\n\n## Response to molecules possess more properties that can also lead to imbalance\n\nIt is correct that molecules have more properties than their size, like logP, Hbond donor and acceptor number, and rotatory bond number. As small molecule drugs are mostly designed to penetrate barriers like gut or cell membranes, they usually have much larger logP values, which means more hydrophobic. For the same reason, they usually have fewer hydrogen bond donors and acceptors, and only minimal essential ones are kept for specified interactions. Also, to minimize the entropy effect upon binding and to increase binding affinity, rotatory bonds are also unfavored. However, it is impossible to mimic such features with peptides, as the backbone of peptides is intrinsically hydrophilic and flexible. As we showed in a new figure, our pseudo-ligands are less similar to real ligands in those properties even with size alignment. You can find the figure in the new pdf in **Appendix G**.\n\nWe believe that the aforementioned divergence is the main limitation of poor zero-shot performance in predicting hydrophobicity scores (**Table 1**). However, as demonstrated in previous publications like CoSP, we could leverage real ligand-pocket pairs from PDBBind or BioLip database to further finetune our network, as an extension of the pipeline. In this way, we could handle property mismatches but still enjoy the power of our large-scale pretraining.\n\n## Response to the approach to defining pockets \n\nWe define the pocket following the UniMol setup. We found our model can also adapt to an 8Å setup in the toughM1 experiment \n\nIn particular, we define the pocket for each protein-ligand pair as residues of the protein that have at least one atom within the range of 6Å from a heavy atom in the ligand. To further explain our design, we have done an ablation study on different choices of thresholds of 4Å, 6Å, and 8Å:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| 4Å | 0.7062 | 0.7549 | 0.1240 | 0.1095 | 28.29 | 13.07 |\n| 6Å | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| 8Å | 0.8322 | 0.8292 | 0.1256 | 0.1125 | 34.83 | 12.92 |\n\nSince the 8Å threshold corresponds with the pocket definition in the Kahraman and Tough M1 datasets, it leads to optimal results. Our decision to use a 6Å threshold was made to align with the methodology of pretraining data creation by Uni-Mol, facilitating a fair comparison. Notably, even with the 6Å threshold, we achieved strong results in the pocket-matching task, which serves as a testament to the effectiveness of our approach.\n\n---\n\n# Response 1/3\nWe appreciate the time and effort you have dedicated to reviewing our paper, and we are grateful for your constructive feedback and thoughtful evaluation of our work. We would like to address your comments about the technical novelty of our work and provide additional clarification on certain aspects of our paper.\n\n## Response to technical novelty\n\nFirstly, we acknowledge your observation that we didn't use any fancy models compared with typical **model-centric** works. While we respect your assessment, we would like to highlight that our primary focus in this paper is a **data-centric** pretraining method to introduce groundbreaking improvements in the field of protein pocket pretraining. Our work aims to address a critical challenge of data scarcity in the field of protein pocket representation by constructing large-scale synthetic data that facilitates the pretraining of models, ultimately enhancing the accuracy and robustness of protein pocket representations. We believe that the research community can conduct fast following-ups on our **released dataset** with more sophisticated models, and even extend our pipelines to other tasks like docking or drug design. We also would like to point out that we have provided a novel method to **distill knowledge from well-trained molecule models** to protein models. Though employing the same contrastive loss, our approach is different from existing contrastive learning models since it uses a fixed molecular encoder. The motivation here is to use a well-trained molecular encoder on a relatively larger dataset to **guide** the training of the protein encoder. For example, some quantum-chemistry properties that are difficult to compute for large systems like proteins could be distilled from molecules, by using some quantum-chemistry-aware molecular encoders such as Frad. Though our framework seems to be simple, it is non-trivial to make it work. We have made several efforts to solve the unavoidable discrepancies between true ligands and pseudo ligands. The ablation study shows that our efforts are effective and necessary:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nRegarding your positive acknowledgment of our \"simple but effective model for the right question,\" we are pleased to hear that our approach resonates with the objective we set out to achieve. We designed our model with **simplicity** in mind, prioritizing effectiveness and practical utility for the specific problem domain. This deliberate choice aligns with the notion that sometimes the most impactful solutions are elegantly straightforward. Also, we intentionally borrowed a pocket encoder from Uni-Mol to make a **fair comparison** with it, which strongly supports the power of our dataset. Similar to the molecule encoder, our pocket encoder can be changed to any other model due to our flexible framework. Notably, **we didn't load the pretrained weights of Uni-Mol pocket encoder**. We only use the same backbone architecture and the pretraining was completely done on our processed data with our training strategy. \n\nNotably, many simple but effective approaches have been recognized by top-tier conferences and journals. A prime example of this is in the field of protein language models, like the ESM series and ProtTrans. These studies adapted the Transformer architecture and masked language modeling techniques from natural language processing to protein sequences. While they didn't introduce groundbreaking techniques, their substantial contributions to protein modeling are evident, with publications in prestigious conferences and journals like NeurIPS, ICML, TPAMI, PNAS, and Science. Another example is the widely acclaimed CLIP paper presented at ICML 2021. CLIP, while not employing novel techniques, stands as a hallmark of data-centric deep learning. Its use of contrastive learning enabled training on expansive web-sourced text-image datasets, moving beyond the constraints of meticulously curated databases like MS-COCO. This aligns with our approach to addressing **data scarcity challenges** by facilitating training on large-scale datasets, mirroring our strategy for overcoming similar hurdles in the protein pocket modeling domain.\n\nWe appreciate your efforts to remain neutral and understand the challenges in decision-making. We believe further discussion will highlight our paper's contributions. Your feedback and insights in the upcoming discussion will be invaluable for refining our work.", "author_response": "# Response 1/3\nWe appreciate the time and effort you have dedicated to reviewing our paper, and we are grateful for your constructive feedback and thoughtful evaluation of our work. We would like to address your comments about the technical novelty of our work and provide additional clarification on certain aspects of our paper.\n\n## Response to technical novelty\n\nFirstly, we acknowledge your observation that we didn't use any fancy models compared with typical **model-centric** works. While we respect your assessment, we would like to highlight that our primary focus in this paper is a **data-centric** pretraining method to introduce groundbreaking improvements in the field of protein pocket pretraining. Our work aims to address a critical challenge of data scarcity in the field of protein pocket representation by constructing large-scale synthetic data that facilitates the pretraining of models, ultimately enhancing the accuracy and robustness of protein pocket representations. We believe that the research community can conduct fast following-ups on our **released dataset** with more sophisticated models, and even extend our pipelines to other tasks like docking or drug design. We also would like to point out that we have provided a novel method to **distill knowledge from well-trained molecule models** to protein models. Though employing the same contrastive loss, our approach is different from existing contrastive learning models since it uses a fixed molecular encoder. The motivation here is to use a well-trained molecular encoder on a relatively larger dataset to **guide** the training of the protein encoder. For example, some quantum-chemistry properties that are difficult to compute for large systems like proteins could be distilled from molecules, by using some quantum-chemistry-aware molecular encoders such as Frad. Though our framework seems to be simple, it is non-trivial to make it work. We have made several efforts to solve the unavoidable discrepancies between true ligands and pseudo ligands. The ablation study shows that our efforts are effective and necessary:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nRegarding your positive acknowledgment of our \"simple but effective model for the right question,\" we are pleased to hear that our approach resonates with the objective we set out to achieve. We designed our model with **simplicity** in mind, prioritizing effectiveness and practical utility for the specific problem domain. This deliberate choice aligns with the notion that sometimes the most impactful solutions are elegantly straightforward. Also, we intentionally borrowed a pocket encoder from Uni-Mol to make a **fair comparison** with it, which strongly supports the power of our dataset. Similar to the molecule encoder, our pocket encoder can be changed to any other model due to our flexible framework. Notably, **we didn't load the pretrained weights of Uni-Mol pocket encoder**. We only use the same backbone architecture and the pretraining was completely done on our processed data with our training strategy. \n\nNotably, many simple but effective approaches have been recognized by top-tier conferences and journals. A prime example of this is in the field of protein language models, like the ESM series and ProtTrans. These studies adapted the Transformer architecture and masked language modeling techniques from natural language processing to protein sequences. While they didn't introduce groundbreaking techniques, their substantial contributions to protein modeling are evident, with publications in prestigious conferences and journals like NeurIPS, ICML, TPAMI, PNAS, and Science. Another example is the widely acclaimed CLIP paper presented at ICML 2021. CLIP, while not employing novel techniques, stands as a hallmark of data-centric deep learning. Its use of contrastive learning enabled training on expansive web-sourced text-image datasets, moving beyond the constraints of meticulously curated databases like MS-COCO. This aligns with our approach to addressing **data scarcity challenges** by facilitating training on large-scale datasets, mirroring our strategy for overcoming similar hurdles in the protein pocket modeling domain.\n\nWe appreciate your efforts to remain neutral and understand the challenges in decision-making. We believe further discussion will highlight our paper's contributions. Your feedback and insights in the upcoming discussion will be invaluable for refining our work."}
{"claim": "The contrastive loss used by the authors is the vanilla form of classical contrastive learning without apparent innovation.", "claim_type": "methodology", "paper_id": "uMAujpVi9m", "paper_title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "9ap4s1SZlZ", "reviewer": "Reviewer_yfM3", "review_text": "Summary: This paper primarily aims to enhance the pocket pretraining method, as existing approaches only consider pockets during pretraining. There are two main contributions in this paper: (1) The authors introduce a novel method, ProFSA, for pocket pretraining, which extracts additional information from corresponding ligands. However, the number of pocket-ligand complex structures is quite limited in existing datasets. (2) To address this issue, the authors generate over 5 million complexes by segmenting fragments and their corresponding pockets in protein structures. By aligning features of fragments and pockets, the pocket encoder learns the interaction between fragments and pockets. The authors design downstream tasks such as pocket druggability prediction, pocket matching, and ligand binding affinity prediction to demonstrate the effectiveness of ProFSA.\n\nStrengths: The authors propose a new perspective of pretraining pockets and construct a large-scale dataset, which data distribution is also considered, to make the efficient pre-training possible.\n\nThe results are competitive, especially for zero-shot settings.\n\nAbundant experiments and ablation study support the argument and result of the authors.\n\nWeaknesses: 1. The technical novelty is limited.\n  - The pocket encoder is borrowed from Uni-Mol.\n  - The contrastive loss is the vanilla form of classical contrastive learning.\n\n2. The bound of Theorem 3.1 is trivial. The authors claim that the bound naturally exists for these representations extracted by pretrained molecule models. However, it's a bit counterintuitive, because many models not pretrained on molecule datasets also fulfill this prior. So, can these models be used for this task? **I strongly suggest removing this part from the paper**.\n\n3. Some issues about dataset creation:\n - 3.1. The authors consider the distribution of ligand size and pocket size when designing the dataset. However, molecules possess more properties that can also lead to imbalance. It would be better to, at least, add some discussion about this issue.\n - 3.2. In the second stage of the data construction process, the approach to defining pockets needs further explanation or an ablation study.\n\n4. Experiments: It would be better to add some biological justification or visualization of the results.\n\nFor this paper, one fact is that the technical novelty is below the bar of ICLR. However, I admire the simple but effective model for the right question. It's a struggle for me to make a decision. I will maintain a neutral attitude and make my final decision after the discussion.\n\nQuestions: See weakness.", "labeling_timestamp": "2026-01-11T16:24:46.863949", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The authors explicitly reject the claim that there is no innovation: they acknowledge the contrastive loss form is standard but argue their novelty lies in the data-centric pipeline (peptides as pseudo-ligands), the use of a fixed molecular encoder to distill molecule-model knowledge into the pocket encoder, distributional alignment, theoretical justification, and ablations showing these design choices are necessary—therefore they refute the reviewer’s assertion of ‘no apparent innovation’.", "evidence": "\"we have provided a novel method to **distill knowledge from well-trained molecule models** to protein models. Though employing the same contrastive loss, our approach is different from existing contrastive learning models since it uses a fixed molecular encoder.\"; \"Firstly, we acknowledge your observation that we didn't use any fancy models compared with typical **model-centric** works. While we respect your assessment, we would like to highlight that our primary focus in this paper is a **data-centric** pretraining method... constructing large-scale synthetic data that facilitates the pretraining of models...\"", "combined_author_response": "Dear AC and reviewers,\n\nWe sincerely appreciate your time and efforts in reviewing our work. Based on your suggestions, we have revised our paper. We would like to use this section to reiterate the explanation for some common concerns and summarize the contributions of our paper.\n\nIn our revised paper, we provide a detailed explanation of the motivation and theory behind our method: **the mechanisms of pocket-peptide interactions and pocket-ligand interactions are very similar**. Our proposed **data-centric** pretraining pipeline is unique in its use of peptides to mimic small molecules, a method that is, to our knowledge, **the first of its kind**. We expanded our discussion about its biological insights in **Section 3**. We add a **new Figure 2 to illustrate different types of non-covalent interactions** that are common for both ligand-protein data and intra-protein data. Following this theory, we have designed a pipeline to generate **over 5 million** synthetic ligand-protein pairs, which greatly eases the **data scarcity** problem in the field of protein-ligand interaction learning. We also prove our data quite effective in various downstream tasks, including pocket property prediction, pocket matching, ligand binding affinity prediction, ligand efficiency prediction, and protein-protein interaction tasks. The **last two tasks are newly added** to further support our method (**Appendix C5 and C6**). We are confident that our **ready-to-release synthetic dataset** will significantly benefit the AI for drug discovery community. Its impact extends beyond enhancing pocket representations in downstream analyses, as it also equips researchers with a valuable resource for a range of applications, such as protein-ligand docking, structure-based drug design, and virtual screening. Importantly, these areas often face challenges due to **a lack of training data necessary for learning effective binding patterns between protein pockets and ligands**. \n\nIn our revised paper, we recognize the differences between real ligands and pseudo ligands (peptides). We address these discrepancies and present a distribution plot of various properties in **Appendix G**. Our approach primarily leverages the **analogous nature of ligand-pocket and peptide-pocket interactions**. Therefore, despite these differences, our model can **still extract valuable interaction information from the pretraining dataset**, which enhances its performance in downstream tasks. This is illustrated in **Figure 4(a)**, showing the benefits of interaction-aware pretraining. We have also undertaken measures like distribution alignment and freezing the molecular encoder to mitigate the impact of these discrepancies. The positive results of these efforts, as evidenced in **Table 5**, confirm their effectiveness and necessity. Notably, our pretrained model can be **further finetuned with real labeled ligand-pocket pairwise data**, as we show in the Ligand Binding Affinity task in **section 4.3**.\n\nAs for other questions, we have added more ablation studies on our design choices, including the impact of a fixed molecular encoder (Table 5), and different threshold values to define pockets(Appendix C.4). \n\nWe are immensely grateful for the invaluable feedback from all the reviewers, which has guided us in refining and clarifying our work. We hope that the revised version of our paper, coupled with the discussion period, will more clearly highlight the novelty, effectiveness, and contributions of our approach to the field.\n\n---\n\nThank you for your invaluable feedback on our paper. In response to your insightful suggestions, we have made several modifications in our revised paper. \n\nWe have cited additional papers in Section 2.2 following your advice. The description of loss terms in Section 3.2 has been corrected.  COSP was not included as a baseline for the Ligand Binding Affinity (LBA)  task because it is not open-sourced, and its performance on LBA was not evaluated in their published work. We used it as a baseline in the pocket-matching task.\n\nIn Section 3, we've expanded our discussion to answer your question on **ProFSA's effectiveness without distributional alignment**. This section now offers a detailed explanation of the foundational concepts and justifications for our method. We draw attention to the parallels between ligand-protein and intra-protein non-covalent interactions, as shown in **Figure 2**. This comparison supports our strategy of using peptides as stand-ins for actual ligands to mimic pocket-ligand interactions. Consequently, **even in the absence of distributional alignment, the types of interactions remain comparably relevant**, which explains why ProFSA continues to function, albeit with reduced effectiveness, as demonstrated in Table 5. We also want to clarify that distributional alignment is only applied to fragment-pocket complex as a whole, and **it would not change the definition of the pocket with a given ligand**. Without this alignment, these fragments are chosen uniformly, with lengths varying from 1 to 7 residues, and pockets are consistently identified based on residues within a 6Å radius of these peptide fragments.\n\nWe are grateful for the time and effort you have dedicated to reviewing our work. Your thorough and constructive feedback has significantly contributed to the refinement of our research. Please let us know if there are any other aspects of our paper that you would like us to address or clarify. We look forward to your further feedback.\n\n---\n\nThank you for your insightful feedback. In response to your concerns regarding the scope of our tasks, we have expanded our research in the revised paper. We now include additional tasks such as **ligand efficiency prediction** and **protein-protein affinity prediction**, detailed in **Appendix C5 and C6**. These tasks were incorporated to enhance the robustness of our method. We acknowledge that they are currently in the appendix due to space constraints, but we are open to relocating them to the main text if it would be beneficial.\n\nRegarding your concern about the lack of recent baselines, we provided a detailed explanation in our previous response. We want to reiterate that our benchmarks do include the latest baselines, with some as recent as 2023.\n\nWe greatly appreciate the time and effort you have invested in reviewing our work. Your feedback has been invaluable in refining our research. Please let us know if there are any other aspects of our paper that you would like us to address or clarify. We look forward to your further feedback.\n\n---\n\nIn our revised paper, we have incorporated changes following feedback from another reviewer, which we believe also addresses your concerns and queries.\n\nRegarding your question on the evidence supporting our approach, particularly the use of peptides to simulate small molecules, we've enriched **Section 3.1**. A new paragraph there now provides an in-depth explanation of non-covalent interactions. To aid understanding, **Figure 2** has been updated with visualizations that include both real and pseudo receptor-ligand pairs, illustrating three types of interactions. Furthermore, we delve into the details of **how intra-protein interactions mirror protein-ligand interactions**, involving specific types of amino acids.\n\nTo better evaluate our design choices, the ablation studies have been extended. Alongside the existing studies in **Table 4** (different pretrained molecular encoders), **Table 5** (distribution alignment), and **Figure 5** (data scale). we've added a study on the impact of using a fixed molecular encoder in **Table 5** in the main text to address your question on the fixed molecular encoder. We've also included an ablation study on varying **cutoff values for pocket definition**, placed in the **Appendix C.4** due to its more focused scope. We can move it to the main part of the paper in the future if needed.\n\nWe acknowledge your concern about the **differences between true and pseudo ligands**, so we've expanded our discussion on the distinct properties of our curated dataset versus the PDBBind dataset in **Figure 9** and **Section 3.2**. The ablation study in **Table 5** demonstrates our methods and their effectiveness in minimizing these discrepancies. Notably, the performance significantly drops without our distribution alignment or fixed molecular encoder, highlighting our efforts to mitigate these issues. The updated **Figure 2** further supports this, showing that while the properties of true and pseudo ligands may differ, **the interaction types are consistent.** We hope these results can ease your concerns about our synthetic dataset, and they also respond to your query about **why we kept the molecular encoder constant during pocket pretraining.**\n\nWe appreciate the constructive critiques and thoughtful engagement from you. Your feedback has undeniably contributed to the refinement and strengthening of our paper. We sincerely value the time and expertise invested in the review process, and we look forward to any further suggestions you may have.\n\n---\n\nWe sincerely appreciate the thoughtful feedback provided on our paper and have diligently incorporated your suggestions into this revised version. In response to your guidance, we have made three significant modifications to the core content of our paper, all of which aim to enhance clarity and address the concerns raised.\n\nTaking into account your suggestion, we have removed **Theorem 3.1** from the main body of our paper. Additionally, we've improved the explanation and included an ablation study on our method of defining pockets. This ablation study, focusing on the cutoff values and primarily a hyperparameter issue, is placed in the Appendix due to its more limited scope of insight. We can move it to the main body of the paper in the future if needed. We have also expanded our discussion on the differences between our dataset and the PDBBind dataset, elucidating how fixed molecule encoders contribute to mitigating this discrepancy.\n\nIn **Section 3.1**, we have introduced a new paragraph dedicated to providing readers with a deeper understanding of non-covalent interactions. To facilitate comprehension, we have included visualizations featuring three types of interactions, incorporating both real and pseudo receptor-ligand pairs in the new **Figure 2**. Furthermore, we delve into the details of how intra-protein interactions mirror protein-ligand interactions, involving specific types of amino acids.\n\nThe biological justifications for the efficacy of interaction-aware pretraining are now more extensively explored in our revised paper. Specifically, we present a compelling showcase featuring estradiol-binding proteins, offering insights into the geometric disparities between the compared proteins and highlighting the similarities in their binding interfaces in the new **Figure 4a**. The existing visualization of BioLip pocket representations in the new **Figure 4b** can make our claims more convincing. Together, these visual aids illustrate how our interaction-aware pretraining empowers models to focus on crucial interface residues, discern subtle distinctions, and disregard irrelevant geometric dissimilarities.\n\nWe have also included an in-depth discussion of pretrained molecule encoders in the newly added **Section 2.3**, further enriching the theoretical foundation of our work.\nAll changes can be found in the renewed paper, which is updated lively.\n\nWe genuinely value the constructive input you provided during the review process, as it has undoubtedly contributed to the refinement of our paper. Your insights have proven invaluable in elevating the clarity and depth of our contributions to the field. We express our gratitude for the time and expertise invested in reviewing our work, and we look forward to any additional feedback or suggestions you may have.\n\n---\n\n# Response 2/2\n\n## Response to the question on N Terminal and C Terminal\n\nWhen a peptide bond is formed (green), one amine group and one carboxy group are left untouched. Therefore, in a linear polypeptide, there would always be a free amine group and a free carboxy group, and they are called the N terminal(blue) and the C terminal(red) respectively in biochemistry. You can find the visualization in **Appendix H**.\n\n## Response to fixing the molecule encoder in contrastive learning\n\nFrom an **effectiveness** standpoint, as you mentioned before, there are unavoidable discrepancies between pseudo and true ligands. If we didn't fix our molecule encoder during pretraining, then the conditions of Theorem 3.1 might not be met. The ablation results presented in the previous response clearly indicate a performance decline when the molecular encoder is not fixed during contrastive learning, particularly in the context of pocket matching. This supports our assertion that the pocket encoder struggles to acquire chemical knowledge without the stability provided by a fixed molecular encoder.\n\nFrom an **efficiency** standpoint, a fixed molecule encoder also offers significant advantages. It reduces memory usage and accelerates training speed because all molecule embeddings can be pre-computed before training. This eliminates the need for feed-forward and back-propagation operations of the molecule encoder during training, leading to efficiency gains. Additionally, adapting to different molecule encoders becomes more straightforward when they are not co-trained during the pretraining process. This approach allows for more flexibility and ease in integrating various encoding methodologies **even if the molecule encoder is not accessible for training**. That is to say, our approach can even be used with undifferentiable molecular representations.\n\n## Response to extending to other tasks like protein-protein interaction prediction\n\nThank you for pointing out the potential application of our framework to protein-protein interaction (PPI) tasks. We have to point out that the interactions between proteins and proteins are far different from proteins and molecules. Specifically, the interaction areas are usually much larger, but shallower, and involved functional groups are less diverse due to limited types of natural amino acids. Therefore, several changes should be made to extend our ProFSA method to PPI tasks:\n\n1. As for data creation, unlike the localized and specific nature of pocket-ligand interactions, protein-protein interactions are **more global and involve broader protein features**. This requires a shift in the data creation process to split entire protein structures into domains instead of short fragments and to capture potential interactions between them. \n\n2. The interaction interface between protein and protein is much larger. We might need to change from atomic-level to residue-level representations to efficiently capture interaction information.\n\n3. For contrastive learning training, ProFSA uses a fixed pretrained molecular encoder to cope with various small molecules and their atomic representation. However, for protein-protein interaction, it is not necessary as we could only model 20 types of amino acids at the residue level.\n\nDespite several changes that need to be done for PPI tasks, we find that we could divide protein complexes into local interactions. When the complex structures and monomer structures are provided, we have achieved a reasonable result on a flexible PPI affinity prediction benchmark with our current model in a zero-shot way. \n\n| Method            | Spearman $\\uparrow$  |\n|-------------------|----------------------|\n| SchNet            | 0.072 ± 0.021        |\n| DimeNet++         | 0.171 ± 0.054        |\n| EGNN              | 0.080 ± 0.038        |\n| TorchMD           | 0.117 ± 0.008        |\n| GET               | **0.363 ± 0.017**    |\n| ProFSA(zero-shot) | _0.248_              |\n\nSurprisingly, as a **zero-shot** method, ProFSA is able to outperform other **supervised learning** models except for GET, a newly proposed unified model. This demonstrates our approach is also able to capture protein-protein interaction information despite it being designed for protein-ligand interaction. \n\nA detailed explanation of this experiment is shown in **Appendix C6**.\n\n---\n\n# Response 1/2\n\n## Response to evidence for the construction of the pseudo-ligand and using peptides to replace small molecules \n\nAs we discussed in the paper, pseudo-ligands share similar sizes as real ligands, and they also make similar non-covalent interactions with the pocket. To further support our proposal, we would like to cite another paper **\"A defined structural unit enables de novo design of small-molecule–binding proteins\"**[1], published in Science 2020, which supports that **intra-protein interactions are similar to protein-ligand interactions.** That is the reason we use peptides to **represent** small molecules to let the pocket encoder learn the interaction information. \n\nAlso, we'd like to clarify that our intention is not to **\"replace\"** small molecules with peptides. It's a pretraining framework that leverages abundant protein-only data and uses peptides to **simulate** pocket-small molecule interactions for enhanced pocket representations. Following this pretraining phase, **the model can be finetuned with datasets that include real small molecules, ensuring its effectiveness in practical applications**.\n\nTo demonstrate the validity of constructing pseudo-ligands, we provide visualizations in **Appendix F** showing that interactions between pockets and our pseudo-ligands are similar to those with true ligands. These figures reveal shared interaction types like **hydrogen bonding**, **$\\pi-\\pi$ stacking**, and **salt bridge**, depicted with color-coded dashed lines for each interaction type. This should help clarify the rationale and evidence supporting our approach.\n\n[1] Polizzi, Nicholas F., and William F. DeGrado. \"A defined structural unit enables de novo design of small-molecule–binding proteins.\" Science 369, no. 6508 (2020)\n\n## Response to ablation studies evaluating the impact of critical design choices\n\nThanks for your advice on adding more ablation studies on design choices. We did ablation studies on the effectiveness of distributional alignment, where the major difference is that fragment sizes are modified. We found that aligned fragment sizes with real ligands could provide the best performance. The result is shown in **Table 5**.\n\nAs for the distance thresholds, we define the pocket following the UniMol setup. We also found our model can also adapt to an 8Å setup in the ToughM1 experiment. Following your advice, we did an ablation study on the distance thresholds. We tested our method with three different thresholds: 4Å, 6Å, and 8Å. The result is shown below:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| 4Å | 0.7062 | 0.7549 | 0.1240 | 0.1095 | 28.29 | 13.07 |\n| 6Å | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| 8Å | 0.8322 | 0.8292 | 0.1256 | 0.1125 | 34.83 | 12.92 |\n\nSince the 8Å threshold corresponds with the pocket definition in the Kahraman and Tough M1 datasets, it leads to optimal results. Our decision to use a 6Å threshold was made to align with the methodology of pretraining data creation by Uni-Mol, facilitating a fair comparison. Notably, even with the 6Å threshold, we achieved strong results in the pocket-matching task, which serves as a testament to the effectiveness of our approach.\n\n## Response to substantial discrepancies between pseudo and real ligands\n\nThank you for pointing out the discrepancies between pseudo and real ligands. We understand and acknowledge your concern. To mitigate the impact of this discrepancy, we have made several efforts. First, we performed **distribution alignment** to make the data distribution of pseudo-ligands more similar to that of true ligands in PDBbind. Additionally, we **fixed the molecule encoder during pretraining** to prevent it from being misled by the discrepancy. Our ablation studies have shown that these strategies are effective:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nThe table above shows that lacking distribution alignment and not fixing the molecular encoder during pretraining both lead to lower performance, underscoring the effectiveness of our methods in reducing discrepancies.\n\nTheorem 3.1 theoretically supports our approach's efficacy, even with discrepancies between pseudo and real ligands. Empirically, our method outperforms other pretraining techniques, confirming its effectiveness despite these discrepancies.\n\n---\n\n## Response to limited tasks and potential in protein-ligand binding pose prediction\n\nIn our paper, the primary objective is to assess the effectiveness of our pretraining method and the quality of the trained pocket representations. To achieve this, we focused on two types of downstream tasks: pocket-only tasks (pocket property prediction and pocket matching) and pocket-ligand interaction tasks (ligand binding affinity prediction).\n\nWe appreciate the suggestion to apply our method to the protein-ligand binding pose prediction task. However, it's important to note that our current evaluation framework is designed to specifically assess **pocket representations**. For the tasks we chose, the architecture is straightforward: either a **zero-shot** evaluation or a **simple MLP** for mapping embeddings to predictions. In contrast, protein-ligand binding pose prediction often involves **complex methodologies**. For instance, state-of-the-art methods like DiffDock[1] require training a diffusion generative model and a separate confidence model, while other approaches like EDM-Dock[2], rely on reconstructing ligand conformations from predicted distance maps. These methods are not end-to-end and do not directly align with our objective of evaluating pretrained pocket representations.\n\nNevertheless, we recognize the potential of our method in enhancing existing binding pose prediction techniques. To integrate our approach, we would need to modify our framework. Using the same data creation strategy, we could train a binding pose prediction model with our preprocessed data, which could then be further fine-tuned using real pocket-ligand pair data from sources like PDBbind. Additionally, our data could be used to train a side-chain packing model, allowing for side-chain flexibility during docking. Thank you again for your advice, and we will leave protein-ligand binding pose prediction as a separate future work for our method.\n\nIn response to the need for evaluating our method on a broader range of tasks, we have extended our analysis to include two additional downstream tasks: LEP (Ligand Efficacy Prediction), and PPA (Protein-Protein Affinity Prediction).\n\nresult for LEP:\n\n| Method      | AUROC $\\uparrow$ | AUPRC $\\uparrow$ |\n|-------------|--------------|--------------|\n| ATOM3D-GNN  | 0.681        | 0.598        |\n| GeoSSL      | 0.776±0.03   | 0.694±0.06   |\n| Uni-Mol     | 0.782±0.02   | 0.695±0.07   |\n| ProFSA      | 0.840±0.04   | 0.806±0.04   |\n\nGeoSSL and Uni-Mol, both pretraining methods, yield comparable results. However, ProFSA outperforms these methods, demonstrating the advantage of our pocket pretraining approach.\n\nresult for PPA:\n\n| Method            | Spearman $\\uparrow$  |\n|-------------------|----------------------|\n| SchNet            | 0.072 ± 0.021        |\n| DimeNet++         | 0.171 ± 0.054        |\n| EGNN              | 0.080 ± 0.038        |\n| TorchMD           | 0.117 ± 0.008        |\n| GET               | **0.363 ± 0.017**    |\n| ProFSA(zero shot) | _0.248_              |\n\nAs a **zero-shot** method, ProFSA is able to outperform other **supervised learning** models except for GET, a newly proposed unified model. This demonstrates our approach is able to capture protein-protein interaction information despite it being designed for protein-ligand interaction modeling.\n\nYou can find detailed experiment settings and results in **Appendix C5 and C6**.\n\n[1] Corso et al., \"Diffdock: Diffusion steps, twists, and turns for molecular docking.\", ICLR 2023.\n[2] Masters et al., \"Deep learning model for efficient protein–ligand docking with implicit side-chain flexibility.\" Journal of Chemical Information and Modeling 63, no. 6 (2023).\n\n## Response to the issue of lack of latest baselines\n\nFor the pocket matching task, CoSP is a newly proposed baseline which is published in ECML PKDD 2023. Alongside CoSP and Uni-Mol, we selected some of the most effective baseline results presented in the CoSP paper. We also tried to evaluate recent methods like PocketAnchor (Li et al., Cell Systems 2022) on our own since they are not tested on the pocket matching task. However, since the result didn't outperform other baseline machine learning methods(Uni-Mol, CoSP, and DeeplyTough), we decided not to include it in our final baseline comparison. We can include it in the camera-ready version if needed.\n\nFor the ligand binding affinity task, a lot of newly proposed baselines are included, i.e. ProNet (Wang et al., 2022b); as well as pretraining methods such as GeoSSL (Liu et al., 2023), EGNN-PLM (Wu et al., 2022), DeepAffinity (Karimi et al., 2019) and Uni-Mol (Zhou et al., 2023).\n\n---\n\n## Response to lack of related works\n\nThanks for providing us with more insightful related works that could support our arguments. We actually have cited these three papers in the section on ligand binding affinity experiments, but we are also happy to add these in Related Works section 2.2. You can find the change in pdfdiff.\n\n## Response to lack of COSP as a baseline in Table 3\n\nBecause best to our knowledge, they did not release their code and they did not test their method on the ligand binding affinity dataset. \n\n## Response to the question on the loss terms\n\nThank you for your advice. We apologize that we accidentally mentioned loss1 and loss2 in reverse, and we are sorry for the confusing statement. You are correct that the first loss is to identify the true protein pocket when given a pseudo-ligand. We revise the original statement to: \"The primary purpose of the first loss is to identify the true protein pocket from a batch of samples when given a pseudo-ligand. Similarly, the second loss seeks to identify the corresponding ligand fragment for a given pocket.\" You can also find the change in pdfdiff.\n\n## Response to the question on the length of the pocket representation without alignment\n\nWith or without distributional alignment, pockets are always defined by the given protein fragment with a fixed distance cutoff (6Å in our works, following the UniMol setup). The distributional alignment process merely samples these pairs to match the sizes of real ligands and pockets. Without distributional alignment, fragments are uniformly sampled from 1 to 7 residues. Pockets are similarly determined as with residues within the range of 6Å around peptide fragments.\n\n---\n\n# Question about the confidence score\n\nThanks for your review. We found your understanding and judgment of our paper comprehensive and precise. Also, you provided detailed and thoughtful advice for our paper, which helped a lot. We sincerely appreciate your review, but we also wonder if there is any misunderstanding here, as you only give a confidence score of 1.\n\n---\n\n# Response 3/3\n\n## Response to biological justification and visualization of the results\n\nOur method's **biological justification** for achieving good experimental results is based on the properties and interactions of pseudo-ligands. As discussed in our paper, these pseudo-ligands are similar in size to real ligands and engage in comparable non-covalent interactions with protein pockets. This fundamental similarity is crucial for the effectiveness of our approach.\n\nMoreover, our methodology is further supported by the findings in the 2020 Science paper titled \"A defined structural unit enables de novo design of small-molecule–binding proteins.\"[1] This research underscores that **intra-protein interactions are analogous to protein-ligand interactions**, which validates our use of peptides as proxies for small molecules. By employing peptides as pseudo-ligands in this manner, our pocket encoder is able to learn and replicate the interaction dynamics typically observed in real ligand scenarios. This understanding is pivotal to the success of our method in downstream applications.\n\nTo better justify our methods and results, following your advice, we also provide some visualizations in **Appendix F**. **Figure 6** illustrates that the pseudo pairs we created share various interaction types commonly found in real pocket-ligand pairs, such as **hydrogen bonding, $\\pi-\\pi$ stacking, and salt bridge**. In the figures, each type of interaction is represented by dashed lines, color-coded to correspond with the specific interaction type.\n\nAnother visualization of our pocket-matching result is shown in **Figure 7**. We showed an example of two non-homology ATP binding pockets to explain why pocket-matching can benefit from interaction-aware pretraining so that we have achieved superior results in the Kahraman dataset and the BioLip t-SNE visualization. The PEP carboxykinase(PDB:1AYL) and tRNA synthetase(PDB:1B8A) are two ATP-binding proteins that share zero sequence similarity (verified with the BLAST) extracted from the Kahraman dataset. However, as they are both fueled by ATP, their binding site shares similar binding patterns. The cation-$\\pi$ interactions(blue dash lines) and salt bridges (magenta dash lines) are important to the ATP binding, which can be viewed as convergent evolutions at the molecule level. Even though, these two pockets are very distinct in terms of shapes and sizes because they bind ATPs in different conformations. Therefore, biochemical interactions are the key to accomplishing the pocket-matching task, which is ignored in previous self-supervised learning methods like Uni-Mol.\n\n[1] Polizzi, Nicholas F., and William F. DeGrado. \"A defined structural unit enables de novo design of small-molecule–binding proteins.\" Science 369, no. 6508 (2020)\n\n---\n\n# Response 2/3\n\n## Response to the bound of Theorem 3.1 \n\nWe are not very clear about the point of your question. We have two understandings and they are replied as follows.\n\nIf you mean $ ||t − t ^{(0)}|| < 1/2 l_T$ is trivial, please note that it is only a condition of the theorem. As you said, many other encoders can also satisfy this condition. This shows that our method is applicable to many different molecular encoders with or without pretraining as we proved in **Table 4**. Moreover, the condition is only a necessary condition. We also need the pretraining contrastive loss to be sufficiently small to guarantee a small contrastive loss between pockets and real ligands. In fact, in the process of our early exploration, we noticed that some encoders cannot achieve a low pretraining loss, indicating the entire conditions of the theorem are nontrivial.\n\nIf you mean that the conclusion of theorem 3.1 is trivial, we want to emphasize that our conclusion is $\\lim_{L_i(t, s) =0}L_i(t^{(0)}, s) =0$. (We write it in a $\\epsilon$-$\\delta$ language in paper. They are equivalent forms.) Our conclusion shows the loss containing real ligands is consistent with the loss containing pseudo ligands that we optimized in pre-training. \n\nIn contrast,  a trivial result from $||t − t ^{(0)}|| < 1/ 2l_T$ is that $\\lim_{L_i(t, s) =0}L_i(t^{(0)}, s) <B$, B is a bound related to l_t and the representations. It does not guarantee a consistent loss and is substantially different from our result.\nTherefore, our theorem is nontrivial and reveals the transfer ability of our contrastive pre-training from the pseudo-ligand domain to the real ligand domain.\n\nAs for this theoretical result, we are open to further discussion.\n\n## Response to molecules possess more properties that can also lead to imbalance\n\nIt is correct that molecules have more properties than their size, like logP, Hbond donor and acceptor number, and rotatory bond number. As small molecule drugs are mostly designed to penetrate barriers like gut or cell membranes, they usually have much larger logP values, which means more hydrophobic. For the same reason, they usually have fewer hydrogen bond donors and acceptors, and only minimal essential ones are kept for specified interactions. Also, to minimize the entropy effect upon binding and to increase binding affinity, rotatory bonds are also unfavored. However, it is impossible to mimic such features with peptides, as the backbone of peptides is intrinsically hydrophilic and flexible. As we showed in a new figure, our pseudo-ligands are less similar to real ligands in those properties even with size alignment. You can find the figure in the new pdf in **Appendix G**.\n\nWe believe that the aforementioned divergence is the main limitation of poor zero-shot performance in predicting hydrophobicity scores (**Table 1**). However, as demonstrated in previous publications like CoSP, we could leverage real ligand-pocket pairs from PDBBind or BioLip database to further finetune our network, as an extension of the pipeline. In this way, we could handle property mismatches but still enjoy the power of our large-scale pretraining.\n\n## Response to the approach to defining pockets \n\nWe define the pocket following the UniMol setup. We found our model can also adapt to an 8Å setup in the toughM1 experiment \n\nIn particular, we define the pocket for each protein-ligand pair as residues of the protein that have at least one atom within the range of 6Å from a heavy atom in the ligand. To further explain our design, we have done an ablation study on different choices of thresholds of 4Å, 6Å, and 8Å:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| 4Å | 0.7062 | 0.7549 | 0.1240 | 0.1095 | 28.29 | 13.07 |\n| 6Å | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| 8Å | 0.8322 | 0.8292 | 0.1256 | 0.1125 | 34.83 | 12.92 |\n\nSince the 8Å threshold corresponds with the pocket definition in the Kahraman and Tough M1 datasets, it leads to optimal results. Our decision to use a 6Å threshold was made to align with the methodology of pretraining data creation by Uni-Mol, facilitating a fair comparison. Notably, even with the 6Å threshold, we achieved strong results in the pocket-matching task, which serves as a testament to the effectiveness of our approach.\n\n---\n\n# Response 1/3\nWe appreciate the time and effort you have dedicated to reviewing our paper, and we are grateful for your constructive feedback and thoughtful evaluation of our work. We would like to address your comments about the technical novelty of our work and provide additional clarification on certain aspects of our paper.\n\n## Response to technical novelty\n\nFirstly, we acknowledge your observation that we didn't use any fancy models compared with typical **model-centric** works. While we respect your assessment, we would like to highlight that our primary focus in this paper is a **data-centric** pretraining method to introduce groundbreaking improvements in the field of protein pocket pretraining. Our work aims to address a critical challenge of data scarcity in the field of protein pocket representation by constructing large-scale synthetic data that facilitates the pretraining of models, ultimately enhancing the accuracy and robustness of protein pocket representations. We believe that the research community can conduct fast following-ups on our **released dataset** with more sophisticated models, and even extend our pipelines to other tasks like docking or drug design. We also would like to point out that we have provided a novel method to **distill knowledge from well-trained molecule models** to protein models. Though employing the same contrastive loss, our approach is different from existing contrastive learning models since it uses a fixed molecular encoder. The motivation here is to use a well-trained molecular encoder on a relatively larger dataset to **guide** the training of the protein encoder. For example, some quantum-chemistry properties that are difficult to compute for large systems like proteins could be distilled from molecules, by using some quantum-chemistry-aware molecular encoders such as Frad. Though our framework seems to be simple, it is non-trivial to make it work. We have made several efforts to solve the unavoidable discrepancies between true ligands and pseudo ligands. The ablation study shows that our efforts are effective and necessary:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nRegarding your positive acknowledgment of our \"simple but effective model for the right question,\" we are pleased to hear that our approach resonates with the objective we set out to achieve. We designed our model with **simplicity** in mind, prioritizing effectiveness and practical utility for the specific problem domain. This deliberate choice aligns with the notion that sometimes the most impactful solutions are elegantly straightforward. Also, we intentionally borrowed a pocket encoder from Uni-Mol to make a **fair comparison** with it, which strongly supports the power of our dataset. Similar to the molecule encoder, our pocket encoder can be changed to any other model due to our flexible framework. Notably, **we didn't load the pretrained weights of Uni-Mol pocket encoder**. We only use the same backbone architecture and the pretraining was completely done on our processed data with our training strategy. \n\nNotably, many simple but effective approaches have been recognized by top-tier conferences and journals. A prime example of this is in the field of protein language models, like the ESM series and ProtTrans. These studies adapted the Transformer architecture and masked language modeling techniques from natural language processing to protein sequences. While they didn't introduce groundbreaking techniques, their substantial contributions to protein modeling are evident, with publications in prestigious conferences and journals like NeurIPS, ICML, TPAMI, PNAS, and Science. Another example is the widely acclaimed CLIP paper presented at ICML 2021. CLIP, while not employing novel techniques, stands as a hallmark of data-centric deep learning. Its use of contrastive learning enabled training on expansive web-sourced text-image datasets, moving beyond the constraints of meticulously curated databases like MS-COCO. This aligns with our approach to addressing **data scarcity challenges** by facilitating training on large-scale datasets, mirroring our strategy for overcoming similar hurdles in the protein pocket modeling domain.\n\nWe appreciate your efforts to remain neutral and understand the challenges in decision-making. We believe further discussion will highlight our paper's contributions. Your feedback and insights in the upcoming discussion will be invaluable for refining our work.", "author_response": "# Response 1/3\nWe appreciate the time and effort you have dedicated to reviewing our paper, and we are grateful for your constructive feedback and thoughtful evaluation of our work. We would like to address your comments about the technical novelty of our work and provide additional clarification on certain aspects of our paper.\n\n## Response to technical novelty\n\nFirstly, we acknowledge your observation that we didn't use any fancy models compared with typical **model-centric** works. While we respect your assessment, we would like to highlight that our primary focus in this paper is a **data-centric** pretraining method to introduce groundbreaking improvements in the field of protein pocket pretraining. Our work aims to address a critical challenge of data scarcity in the field of protein pocket representation by constructing large-scale synthetic data that facilitates the pretraining of models, ultimately enhancing the accuracy and robustness of protein pocket representations. We believe that the research community can conduct fast following-ups on our **released dataset** with more sophisticated models, and even extend our pipelines to other tasks like docking or drug design. We also would like to point out that we have provided a novel method to **distill knowledge from well-trained molecule models** to protein models. Though employing the same contrastive loss, our approach is different from existing contrastive learning models since it uses a fixed molecular encoder. The motivation here is to use a well-trained molecular encoder on a relatively larger dataset to **guide** the training of the protein encoder. For example, some quantum-chemistry properties that are difficult to compute for large systems like proteins could be distilled from molecules, by using some quantum-chemistry-aware molecular encoders such as Frad. Though our framework seems to be simple, it is non-trivial to make it work. We have made several efforts to solve the unavoidable discrepancies between true ligands and pseudo ligands. The ablation study shows that our efforts are effective and necessary:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nRegarding your positive acknowledgment of our \"simple but effective model for the right question,\" we are pleased to hear that our approach resonates with the objective we set out to achieve. We designed our model with **simplicity** in mind, prioritizing effectiveness and practical utility for the specific problem domain. This deliberate choice aligns with the notion that sometimes the most impactful solutions are elegantly straightforward. Also, we intentionally borrowed a pocket encoder from Uni-Mol to make a **fair comparison** with it, which strongly supports the power of our dataset. Similar to the molecule encoder, our pocket encoder can be changed to any other model due to our flexible framework. Notably, **we didn't load the pretrained weights of Uni-Mol pocket encoder**. We only use the same backbone architecture and the pretraining was completely done on our processed data with our training strategy. \n\nNotably, many simple but effective approaches have been recognized by top-tier conferences and journals. A prime example of this is in the field of protein language models, like the ESM series and ProtTrans. These studies adapted the Transformer architecture and masked language modeling techniques from natural language processing to protein sequences. While they didn't introduce groundbreaking techniques, their substantial contributions to protein modeling are evident, with publications in prestigious conferences and journals like NeurIPS, ICML, TPAMI, PNAS, and Science. Another example is the widely acclaimed CLIP paper presented at ICML 2021. CLIP, while not employing novel techniques, stands as a hallmark of data-centric deep learning. Its use of contrastive learning enabled training on expansive web-sourced text-image datasets, moving beyond the constraints of meticulously curated databases like MS-COCO. This aligns with our approach to addressing **data scarcity challenges** by facilitating training on large-scale datasets, mirroring our strategy for overcoming similar hurdles in the protein pocket modeling domain.\n\nWe appreciate your efforts to remain neutral and understand the challenges in decision-making. We believe further discussion will highlight our paper's contributions. Your feedback and insights in the upcoming discussion will be invaluable for refining our work."}
{"claim": "The paper's technical novelty is judged by the reviewer to be below the acceptance standard expected at ICLR.", "claim_type": "novelty", "paper_id": "uMAujpVi9m", "paper_title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "9ap4s1SZlZ", "reviewer": "Reviewer_yfM3", "review_text": "Summary: This paper primarily aims to enhance the pocket pretraining method, as existing approaches only consider pockets during pretraining. There are two main contributions in this paper: (1) The authors introduce a novel method, ProFSA, for pocket pretraining, which extracts additional information from corresponding ligands. However, the number of pocket-ligand complex structures is quite limited in existing datasets. (2) To address this issue, the authors generate over 5 million complexes by segmenting fragments and their corresponding pockets in protein structures. By aligning features of fragments and pockets, the pocket encoder learns the interaction between fragments and pockets. The authors design downstream tasks such as pocket druggability prediction, pocket matching, and ligand binding affinity prediction to demonstrate the effectiveness of ProFSA.\n\nStrengths: The authors propose a new perspective of pretraining pockets and construct a large-scale dataset, which data distribution is also considered, to make the efficient pre-training possible.\n\nThe results are competitive, especially for zero-shot settings.\n\nAbundant experiments and ablation study support the argument and result of the authors.\n\nWeaknesses: 1. The technical novelty is limited.\n  - The pocket encoder is borrowed from Uni-Mol.\n  - The contrastive loss is the vanilla form of classical contrastive learning.\n\n2. The bound of Theorem 3.1 is trivial. The authors claim that the bound naturally exists for these representations extracted by pretrained molecule models. However, it's a bit counterintuitive, because many models not pretrained on molecule datasets also fulfill this prior. So, can these models be used for this task? **I strongly suggest removing this part from the paper**.\n\n3. Some issues about dataset creation:\n - 3.1. The authors consider the distribution of ligand size and pocket size when designing the dataset. However, molecules possess more properties that can also lead to imbalance. It would be better to, at least, add some discussion about this issue.\n - 3.2. In the second stage of the data construction process, the approach to defining pockets needs further explanation or an ablation study.\n\n4. Experiments: It would be better to add some biological justification or visualization of the results.\n\nFor this paper, one fact is that the technical novelty is below the bar of ICLR. However, I admire the simple but effective model for the right question. It's a struggle for me to make a decision. I will maintain a neutral attitude and make my final decision after the discussion.\n\nQuestions: See weakness.", "labeling_timestamp": "2026-01-11T16:24:43.005263", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The authors explicitly disagree with the reviewer's claim, arguing that their contribution is novel (data-centric pretraining, large synthetic dataset, and a distinct contrastive training using a fixed molecular encoder) and provide empirical/theoretical/ablation support.", "evidence": "\"Firstly, we acknowledge your observation that we didn't use any fancy models compared with typical **model-centric** works. ... we would like to highlight that our primary focus in this paper is a **data-centric** pretraining method to introduce groundbreaking improvements in the field of protein pocket pretraining. Our work aims to address a critical challenge of data scarcity in the field of protein pocket representation by constructing large-scale synthetic data that facilitates the pretraining of models... We also would like to point out that we have provided a novel method to **distill knowledge from well-trained molecule models** to protein models. Though employing the same contrastive loss, our approach is different from existing contrastive learning models since it uses a fixed molecular encoder.\"", "combined_author_response": "Dear AC and reviewers,\n\nWe sincerely appreciate your time and efforts in reviewing our work. Based on your suggestions, we have revised our paper. We would like to use this section to reiterate the explanation for some common concerns and summarize the contributions of our paper.\n\nIn our revised paper, we provide a detailed explanation of the motivation and theory behind our method: **the mechanisms of pocket-peptide interactions and pocket-ligand interactions are very similar**. Our proposed **data-centric** pretraining pipeline is unique in its use of peptides to mimic small molecules, a method that is, to our knowledge, **the first of its kind**. We expanded our discussion about its biological insights in **Section 3**. We add a **new Figure 2 to illustrate different types of non-covalent interactions** that are common for both ligand-protein data and intra-protein data. Following this theory, we have designed a pipeline to generate **over 5 million** synthetic ligand-protein pairs, which greatly eases the **data scarcity** problem in the field of protein-ligand interaction learning. We also prove our data quite effective in various downstream tasks, including pocket property prediction, pocket matching, ligand binding affinity prediction, ligand efficiency prediction, and protein-protein interaction tasks. The **last two tasks are newly added** to further support our method (**Appendix C5 and C6**). We are confident that our **ready-to-release synthetic dataset** will significantly benefit the AI for drug discovery community. Its impact extends beyond enhancing pocket representations in downstream analyses, as it also equips researchers with a valuable resource for a range of applications, such as protein-ligand docking, structure-based drug design, and virtual screening. Importantly, these areas often face challenges due to **a lack of training data necessary for learning effective binding patterns between protein pockets and ligands**. \n\nIn our revised paper, we recognize the differences between real ligands and pseudo ligands (peptides). We address these discrepancies and present a distribution plot of various properties in **Appendix G**. Our approach primarily leverages the **analogous nature of ligand-pocket and peptide-pocket interactions**. Therefore, despite these differences, our model can **still extract valuable interaction information from the pretraining dataset**, which enhances its performance in downstream tasks. This is illustrated in **Figure 4(a)**, showing the benefits of interaction-aware pretraining. We have also undertaken measures like distribution alignment and freezing the molecular encoder to mitigate the impact of these discrepancies. The positive results of these efforts, as evidenced in **Table 5**, confirm their effectiveness and necessity. Notably, our pretrained model can be **further finetuned with real labeled ligand-pocket pairwise data**, as we show in the Ligand Binding Affinity task in **section 4.3**.\n\nAs for other questions, we have added more ablation studies on our design choices, including the impact of a fixed molecular encoder (Table 5), and different threshold values to define pockets(Appendix C.4). \n\nWe are immensely grateful for the invaluable feedback from all the reviewers, which has guided us in refining and clarifying our work. We hope that the revised version of our paper, coupled with the discussion period, will more clearly highlight the novelty, effectiveness, and contributions of our approach to the field.\n\n---\n\nThank you for your invaluable feedback on our paper. In response to your insightful suggestions, we have made several modifications in our revised paper. \n\nWe have cited additional papers in Section 2.2 following your advice. The description of loss terms in Section 3.2 has been corrected.  COSP was not included as a baseline for the Ligand Binding Affinity (LBA)  task because it is not open-sourced, and its performance on LBA was not evaluated in their published work. We used it as a baseline in the pocket-matching task.\n\nIn Section 3, we've expanded our discussion to answer your question on **ProFSA's effectiveness without distributional alignment**. This section now offers a detailed explanation of the foundational concepts and justifications for our method. We draw attention to the parallels between ligand-protein and intra-protein non-covalent interactions, as shown in **Figure 2**. This comparison supports our strategy of using peptides as stand-ins for actual ligands to mimic pocket-ligand interactions. Consequently, **even in the absence of distributional alignment, the types of interactions remain comparably relevant**, which explains why ProFSA continues to function, albeit with reduced effectiveness, as demonstrated in Table 5. We also want to clarify that distributional alignment is only applied to fragment-pocket complex as a whole, and **it would not change the definition of the pocket with a given ligand**. Without this alignment, these fragments are chosen uniformly, with lengths varying from 1 to 7 residues, and pockets are consistently identified based on residues within a 6Å radius of these peptide fragments.\n\nWe are grateful for the time and effort you have dedicated to reviewing our work. Your thorough and constructive feedback has significantly contributed to the refinement of our research. Please let us know if there are any other aspects of our paper that you would like us to address or clarify. We look forward to your further feedback.\n\n---\n\nThank you for your insightful feedback. In response to your concerns regarding the scope of our tasks, we have expanded our research in the revised paper. We now include additional tasks such as **ligand efficiency prediction** and **protein-protein affinity prediction**, detailed in **Appendix C5 and C6**. These tasks were incorporated to enhance the robustness of our method. We acknowledge that they are currently in the appendix due to space constraints, but we are open to relocating them to the main text if it would be beneficial.\n\nRegarding your concern about the lack of recent baselines, we provided a detailed explanation in our previous response. We want to reiterate that our benchmarks do include the latest baselines, with some as recent as 2023.\n\nWe greatly appreciate the time and effort you have invested in reviewing our work. Your feedback has been invaluable in refining our research. Please let us know if there are any other aspects of our paper that you would like us to address or clarify. We look forward to your further feedback.\n\n---\n\nIn our revised paper, we have incorporated changes following feedback from another reviewer, which we believe also addresses your concerns and queries.\n\nRegarding your question on the evidence supporting our approach, particularly the use of peptides to simulate small molecules, we've enriched **Section 3.1**. A new paragraph there now provides an in-depth explanation of non-covalent interactions. To aid understanding, **Figure 2** has been updated with visualizations that include both real and pseudo receptor-ligand pairs, illustrating three types of interactions. Furthermore, we delve into the details of **how intra-protein interactions mirror protein-ligand interactions**, involving specific types of amino acids.\n\nTo better evaluate our design choices, the ablation studies have been extended. Alongside the existing studies in **Table 4** (different pretrained molecular encoders), **Table 5** (distribution alignment), and **Figure 5** (data scale). we've added a study on the impact of using a fixed molecular encoder in **Table 5** in the main text to address your question on the fixed molecular encoder. We've also included an ablation study on varying **cutoff values for pocket definition**, placed in the **Appendix C.4** due to its more focused scope. We can move it to the main part of the paper in the future if needed.\n\nWe acknowledge your concern about the **differences between true and pseudo ligands**, so we've expanded our discussion on the distinct properties of our curated dataset versus the PDBBind dataset in **Figure 9** and **Section 3.2**. The ablation study in **Table 5** demonstrates our methods and their effectiveness in minimizing these discrepancies. Notably, the performance significantly drops without our distribution alignment or fixed molecular encoder, highlighting our efforts to mitigate these issues. The updated **Figure 2** further supports this, showing that while the properties of true and pseudo ligands may differ, **the interaction types are consistent.** We hope these results can ease your concerns about our synthetic dataset, and they also respond to your query about **why we kept the molecular encoder constant during pocket pretraining.**\n\nWe appreciate the constructive critiques and thoughtful engagement from you. Your feedback has undeniably contributed to the refinement and strengthening of our paper. We sincerely value the time and expertise invested in the review process, and we look forward to any further suggestions you may have.\n\n---\n\nWe sincerely appreciate the thoughtful feedback provided on our paper and have diligently incorporated your suggestions into this revised version. In response to your guidance, we have made three significant modifications to the core content of our paper, all of which aim to enhance clarity and address the concerns raised.\n\nTaking into account your suggestion, we have removed **Theorem 3.1** from the main body of our paper. Additionally, we've improved the explanation and included an ablation study on our method of defining pockets. This ablation study, focusing on the cutoff values and primarily a hyperparameter issue, is placed in the Appendix due to its more limited scope of insight. We can move it to the main body of the paper in the future if needed. We have also expanded our discussion on the differences between our dataset and the PDBBind dataset, elucidating how fixed molecule encoders contribute to mitigating this discrepancy.\n\nIn **Section 3.1**, we have introduced a new paragraph dedicated to providing readers with a deeper understanding of non-covalent interactions. To facilitate comprehension, we have included visualizations featuring three types of interactions, incorporating both real and pseudo receptor-ligand pairs in the new **Figure 2**. Furthermore, we delve into the details of how intra-protein interactions mirror protein-ligand interactions, involving specific types of amino acids.\n\nThe biological justifications for the efficacy of interaction-aware pretraining are now more extensively explored in our revised paper. Specifically, we present a compelling showcase featuring estradiol-binding proteins, offering insights into the geometric disparities between the compared proteins and highlighting the similarities in their binding interfaces in the new **Figure 4a**. The existing visualization of BioLip pocket representations in the new **Figure 4b** can make our claims more convincing. Together, these visual aids illustrate how our interaction-aware pretraining empowers models to focus on crucial interface residues, discern subtle distinctions, and disregard irrelevant geometric dissimilarities.\n\nWe have also included an in-depth discussion of pretrained molecule encoders in the newly added **Section 2.3**, further enriching the theoretical foundation of our work.\nAll changes can be found in the renewed paper, which is updated lively.\n\nWe genuinely value the constructive input you provided during the review process, as it has undoubtedly contributed to the refinement of our paper. Your insights have proven invaluable in elevating the clarity and depth of our contributions to the field. We express our gratitude for the time and expertise invested in reviewing our work, and we look forward to any additional feedback or suggestions you may have.\n\n---\n\n# Response 2/2\n\n## Response to the question on N Terminal and C Terminal\n\nWhen a peptide bond is formed (green), one amine group and one carboxy group are left untouched. Therefore, in a linear polypeptide, there would always be a free amine group and a free carboxy group, and they are called the N terminal(blue) and the C terminal(red) respectively in biochemistry. You can find the visualization in **Appendix H**.\n\n## Response to fixing the molecule encoder in contrastive learning\n\nFrom an **effectiveness** standpoint, as you mentioned before, there are unavoidable discrepancies between pseudo and true ligands. If we didn't fix our molecule encoder during pretraining, then the conditions of Theorem 3.1 might not be met. The ablation results presented in the previous response clearly indicate a performance decline when the molecular encoder is not fixed during contrastive learning, particularly in the context of pocket matching. This supports our assertion that the pocket encoder struggles to acquire chemical knowledge without the stability provided by a fixed molecular encoder.\n\nFrom an **efficiency** standpoint, a fixed molecule encoder also offers significant advantages. It reduces memory usage and accelerates training speed because all molecule embeddings can be pre-computed before training. This eliminates the need for feed-forward and back-propagation operations of the molecule encoder during training, leading to efficiency gains. Additionally, adapting to different molecule encoders becomes more straightforward when they are not co-trained during the pretraining process. This approach allows for more flexibility and ease in integrating various encoding methodologies **even if the molecule encoder is not accessible for training**. That is to say, our approach can even be used with undifferentiable molecular representations.\n\n## Response to extending to other tasks like protein-protein interaction prediction\n\nThank you for pointing out the potential application of our framework to protein-protein interaction (PPI) tasks. We have to point out that the interactions between proteins and proteins are far different from proteins and molecules. Specifically, the interaction areas are usually much larger, but shallower, and involved functional groups are less diverse due to limited types of natural amino acids. Therefore, several changes should be made to extend our ProFSA method to PPI tasks:\n\n1. As for data creation, unlike the localized and specific nature of pocket-ligand interactions, protein-protein interactions are **more global and involve broader protein features**. This requires a shift in the data creation process to split entire protein structures into domains instead of short fragments and to capture potential interactions between them. \n\n2. The interaction interface between protein and protein is much larger. We might need to change from atomic-level to residue-level representations to efficiently capture interaction information.\n\n3. For contrastive learning training, ProFSA uses a fixed pretrained molecular encoder to cope with various small molecules and their atomic representation. However, for protein-protein interaction, it is not necessary as we could only model 20 types of amino acids at the residue level.\n\nDespite several changes that need to be done for PPI tasks, we find that we could divide protein complexes into local interactions. When the complex structures and monomer structures are provided, we have achieved a reasonable result on a flexible PPI affinity prediction benchmark with our current model in a zero-shot way. \n\n| Method            | Spearman $\\uparrow$  |\n|-------------------|----------------------|\n| SchNet            | 0.072 ± 0.021        |\n| DimeNet++         | 0.171 ± 0.054        |\n| EGNN              | 0.080 ± 0.038        |\n| TorchMD           | 0.117 ± 0.008        |\n| GET               | **0.363 ± 0.017**    |\n| ProFSA(zero-shot) | _0.248_              |\n\nSurprisingly, as a **zero-shot** method, ProFSA is able to outperform other **supervised learning** models except for GET, a newly proposed unified model. This demonstrates our approach is also able to capture protein-protein interaction information despite it being designed for protein-ligand interaction. \n\nA detailed explanation of this experiment is shown in **Appendix C6**.\n\n---\n\n# Response 1/2\n\n## Response to evidence for the construction of the pseudo-ligand and using peptides to replace small molecules \n\nAs we discussed in the paper, pseudo-ligands share similar sizes as real ligands, and they also make similar non-covalent interactions with the pocket. To further support our proposal, we would like to cite another paper **\"A defined structural unit enables de novo design of small-molecule–binding proteins\"**[1], published in Science 2020, which supports that **intra-protein interactions are similar to protein-ligand interactions.** That is the reason we use peptides to **represent** small molecules to let the pocket encoder learn the interaction information. \n\nAlso, we'd like to clarify that our intention is not to **\"replace\"** small molecules with peptides. It's a pretraining framework that leverages abundant protein-only data and uses peptides to **simulate** pocket-small molecule interactions for enhanced pocket representations. Following this pretraining phase, **the model can be finetuned with datasets that include real small molecules, ensuring its effectiveness in practical applications**.\n\nTo demonstrate the validity of constructing pseudo-ligands, we provide visualizations in **Appendix F** showing that interactions between pockets and our pseudo-ligands are similar to those with true ligands. These figures reveal shared interaction types like **hydrogen bonding**, **$\\pi-\\pi$ stacking**, and **salt bridge**, depicted with color-coded dashed lines for each interaction type. This should help clarify the rationale and evidence supporting our approach.\n\n[1] Polizzi, Nicholas F., and William F. DeGrado. \"A defined structural unit enables de novo design of small-molecule–binding proteins.\" Science 369, no. 6508 (2020)\n\n## Response to ablation studies evaluating the impact of critical design choices\n\nThanks for your advice on adding more ablation studies on design choices. We did ablation studies on the effectiveness of distributional alignment, where the major difference is that fragment sizes are modified. We found that aligned fragment sizes with real ligands could provide the best performance. The result is shown in **Table 5**.\n\nAs for the distance thresholds, we define the pocket following the UniMol setup. We also found our model can also adapt to an 8Å setup in the ToughM1 experiment. Following your advice, we did an ablation study on the distance thresholds. We tested our method with three different thresholds: 4Å, 6Å, and 8Å. The result is shown below:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| 4Å | 0.7062 | 0.7549 | 0.1240 | 0.1095 | 28.29 | 13.07 |\n| 6Å | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| 8Å | 0.8322 | 0.8292 | 0.1256 | 0.1125 | 34.83 | 12.92 |\n\nSince the 8Å threshold corresponds with the pocket definition in the Kahraman and Tough M1 datasets, it leads to optimal results. Our decision to use a 6Å threshold was made to align with the methodology of pretraining data creation by Uni-Mol, facilitating a fair comparison. Notably, even with the 6Å threshold, we achieved strong results in the pocket-matching task, which serves as a testament to the effectiveness of our approach.\n\n## Response to substantial discrepancies between pseudo and real ligands\n\nThank you for pointing out the discrepancies between pseudo and real ligands. We understand and acknowledge your concern. To mitigate the impact of this discrepancy, we have made several efforts. First, we performed **distribution alignment** to make the data distribution of pseudo-ligands more similar to that of true ligands in PDBbind. Additionally, we **fixed the molecule encoder during pretraining** to prevent it from being misled by the discrepancy. Our ablation studies have shown that these strategies are effective:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nThe table above shows that lacking distribution alignment and not fixing the molecular encoder during pretraining both lead to lower performance, underscoring the effectiveness of our methods in reducing discrepancies.\n\nTheorem 3.1 theoretically supports our approach's efficacy, even with discrepancies between pseudo and real ligands. Empirically, our method outperforms other pretraining techniques, confirming its effectiveness despite these discrepancies.\n\n---\n\n## Response to limited tasks and potential in protein-ligand binding pose prediction\n\nIn our paper, the primary objective is to assess the effectiveness of our pretraining method and the quality of the trained pocket representations. To achieve this, we focused on two types of downstream tasks: pocket-only tasks (pocket property prediction and pocket matching) and pocket-ligand interaction tasks (ligand binding affinity prediction).\n\nWe appreciate the suggestion to apply our method to the protein-ligand binding pose prediction task. However, it's important to note that our current evaluation framework is designed to specifically assess **pocket representations**. For the tasks we chose, the architecture is straightforward: either a **zero-shot** evaluation or a **simple MLP** for mapping embeddings to predictions. In contrast, protein-ligand binding pose prediction often involves **complex methodologies**. For instance, state-of-the-art methods like DiffDock[1] require training a diffusion generative model and a separate confidence model, while other approaches like EDM-Dock[2], rely on reconstructing ligand conformations from predicted distance maps. These methods are not end-to-end and do not directly align with our objective of evaluating pretrained pocket representations.\n\nNevertheless, we recognize the potential of our method in enhancing existing binding pose prediction techniques. To integrate our approach, we would need to modify our framework. Using the same data creation strategy, we could train a binding pose prediction model with our preprocessed data, which could then be further fine-tuned using real pocket-ligand pair data from sources like PDBbind. Additionally, our data could be used to train a side-chain packing model, allowing for side-chain flexibility during docking. Thank you again for your advice, and we will leave protein-ligand binding pose prediction as a separate future work for our method.\n\nIn response to the need for evaluating our method on a broader range of tasks, we have extended our analysis to include two additional downstream tasks: LEP (Ligand Efficacy Prediction), and PPA (Protein-Protein Affinity Prediction).\n\nresult for LEP:\n\n| Method      | AUROC $\\uparrow$ | AUPRC $\\uparrow$ |\n|-------------|--------------|--------------|\n| ATOM3D-GNN  | 0.681        | 0.598        |\n| GeoSSL      | 0.776±0.03   | 0.694±0.06   |\n| Uni-Mol     | 0.782±0.02   | 0.695±0.07   |\n| ProFSA      | 0.840±0.04   | 0.806±0.04   |\n\nGeoSSL and Uni-Mol, both pretraining methods, yield comparable results. However, ProFSA outperforms these methods, demonstrating the advantage of our pocket pretraining approach.\n\nresult for PPA:\n\n| Method            | Spearman $\\uparrow$  |\n|-------------------|----------------------|\n| SchNet            | 0.072 ± 0.021        |\n| DimeNet++         | 0.171 ± 0.054        |\n| EGNN              | 0.080 ± 0.038        |\n| TorchMD           | 0.117 ± 0.008        |\n| GET               | **0.363 ± 0.017**    |\n| ProFSA(zero shot) | _0.248_              |\n\nAs a **zero-shot** method, ProFSA is able to outperform other **supervised learning** models except for GET, a newly proposed unified model. This demonstrates our approach is able to capture protein-protein interaction information despite it being designed for protein-ligand interaction modeling.\n\nYou can find detailed experiment settings and results in **Appendix C5 and C6**.\n\n[1] Corso et al., \"Diffdock: Diffusion steps, twists, and turns for molecular docking.\", ICLR 2023.\n[2] Masters et al., \"Deep learning model for efficient protein–ligand docking with implicit side-chain flexibility.\" Journal of Chemical Information and Modeling 63, no. 6 (2023).\n\n## Response to the issue of lack of latest baselines\n\nFor the pocket matching task, CoSP is a newly proposed baseline which is published in ECML PKDD 2023. Alongside CoSP and Uni-Mol, we selected some of the most effective baseline results presented in the CoSP paper. We also tried to evaluate recent methods like PocketAnchor (Li et al., Cell Systems 2022) on our own since they are not tested on the pocket matching task. However, since the result didn't outperform other baseline machine learning methods(Uni-Mol, CoSP, and DeeplyTough), we decided not to include it in our final baseline comparison. We can include it in the camera-ready version if needed.\n\nFor the ligand binding affinity task, a lot of newly proposed baselines are included, i.e. ProNet (Wang et al., 2022b); as well as pretraining methods such as GeoSSL (Liu et al., 2023), EGNN-PLM (Wu et al., 2022), DeepAffinity (Karimi et al., 2019) and Uni-Mol (Zhou et al., 2023).\n\n---\n\n## Response to lack of related works\n\nThanks for providing us with more insightful related works that could support our arguments. We actually have cited these three papers in the section on ligand binding affinity experiments, but we are also happy to add these in Related Works section 2.2. You can find the change in pdfdiff.\n\n## Response to lack of COSP as a baseline in Table 3\n\nBecause best to our knowledge, they did not release their code and they did not test their method on the ligand binding affinity dataset. \n\n## Response to the question on the loss terms\n\nThank you for your advice. We apologize that we accidentally mentioned loss1 and loss2 in reverse, and we are sorry for the confusing statement. You are correct that the first loss is to identify the true protein pocket when given a pseudo-ligand. We revise the original statement to: \"The primary purpose of the first loss is to identify the true protein pocket from a batch of samples when given a pseudo-ligand. Similarly, the second loss seeks to identify the corresponding ligand fragment for a given pocket.\" You can also find the change in pdfdiff.\n\n## Response to the question on the length of the pocket representation without alignment\n\nWith or without distributional alignment, pockets are always defined by the given protein fragment with a fixed distance cutoff (6Å in our works, following the UniMol setup). The distributional alignment process merely samples these pairs to match the sizes of real ligands and pockets. Without distributional alignment, fragments are uniformly sampled from 1 to 7 residues. Pockets are similarly determined as with residues within the range of 6Å around peptide fragments.\n\n---\n\n# Question about the confidence score\n\nThanks for your review. We found your understanding and judgment of our paper comprehensive and precise. Also, you provided detailed and thoughtful advice for our paper, which helped a lot. We sincerely appreciate your review, but we also wonder if there is any misunderstanding here, as you only give a confidence score of 1.\n\n---\n\n# Response 3/3\n\n## Response to biological justification and visualization of the results\n\nOur method's **biological justification** for achieving good experimental results is based on the properties and interactions of pseudo-ligands. As discussed in our paper, these pseudo-ligands are similar in size to real ligands and engage in comparable non-covalent interactions with protein pockets. This fundamental similarity is crucial for the effectiveness of our approach.\n\nMoreover, our methodology is further supported by the findings in the 2020 Science paper titled \"A defined structural unit enables de novo design of small-molecule–binding proteins.\"[1] This research underscores that **intra-protein interactions are analogous to protein-ligand interactions**, which validates our use of peptides as proxies for small molecules. By employing peptides as pseudo-ligands in this manner, our pocket encoder is able to learn and replicate the interaction dynamics typically observed in real ligand scenarios. This understanding is pivotal to the success of our method in downstream applications.\n\nTo better justify our methods and results, following your advice, we also provide some visualizations in **Appendix F**. **Figure 6** illustrates that the pseudo pairs we created share various interaction types commonly found in real pocket-ligand pairs, such as **hydrogen bonding, $\\pi-\\pi$ stacking, and salt bridge**. In the figures, each type of interaction is represented by dashed lines, color-coded to correspond with the specific interaction type.\n\nAnother visualization of our pocket-matching result is shown in **Figure 7**. We showed an example of two non-homology ATP binding pockets to explain why pocket-matching can benefit from interaction-aware pretraining so that we have achieved superior results in the Kahraman dataset and the BioLip t-SNE visualization. The PEP carboxykinase(PDB:1AYL) and tRNA synthetase(PDB:1B8A) are two ATP-binding proteins that share zero sequence similarity (verified with the BLAST) extracted from the Kahraman dataset. However, as they are both fueled by ATP, their binding site shares similar binding patterns. The cation-$\\pi$ interactions(blue dash lines) and salt bridges (magenta dash lines) are important to the ATP binding, which can be viewed as convergent evolutions at the molecule level. Even though, these two pockets are very distinct in terms of shapes and sizes because they bind ATPs in different conformations. Therefore, biochemical interactions are the key to accomplishing the pocket-matching task, which is ignored in previous self-supervised learning methods like Uni-Mol.\n\n[1] Polizzi, Nicholas F., and William F. DeGrado. \"A defined structural unit enables de novo design of small-molecule–binding proteins.\" Science 369, no. 6508 (2020)\n\n---\n\n# Response 2/3\n\n## Response to the bound of Theorem 3.1 \n\nWe are not very clear about the point of your question. We have two understandings and they are replied as follows.\n\nIf you mean $ ||t − t ^{(0)}|| < 1/2 l_T$ is trivial, please note that it is only a condition of the theorem. As you said, many other encoders can also satisfy this condition. This shows that our method is applicable to many different molecular encoders with or without pretraining as we proved in **Table 4**. Moreover, the condition is only a necessary condition. We also need the pretraining contrastive loss to be sufficiently small to guarantee a small contrastive loss between pockets and real ligands. In fact, in the process of our early exploration, we noticed that some encoders cannot achieve a low pretraining loss, indicating the entire conditions of the theorem are nontrivial.\n\nIf you mean that the conclusion of theorem 3.1 is trivial, we want to emphasize that our conclusion is $\\lim_{L_i(t, s) =0}L_i(t^{(0)}, s) =0$. (We write it in a $\\epsilon$-$\\delta$ language in paper. They are equivalent forms.) Our conclusion shows the loss containing real ligands is consistent with the loss containing pseudo ligands that we optimized in pre-training. \n\nIn contrast,  a trivial result from $||t − t ^{(0)}|| < 1/ 2l_T$ is that $\\lim_{L_i(t, s) =0}L_i(t^{(0)}, s) <B$, B is a bound related to l_t and the representations. It does not guarantee a consistent loss and is substantially different from our result.\nTherefore, our theorem is nontrivial and reveals the transfer ability of our contrastive pre-training from the pseudo-ligand domain to the real ligand domain.\n\nAs for this theoretical result, we are open to further discussion.\n\n## Response to molecules possess more properties that can also lead to imbalance\n\nIt is correct that molecules have more properties than their size, like logP, Hbond donor and acceptor number, and rotatory bond number. As small molecule drugs are mostly designed to penetrate barriers like gut or cell membranes, they usually have much larger logP values, which means more hydrophobic. For the same reason, they usually have fewer hydrogen bond donors and acceptors, and only minimal essential ones are kept for specified interactions. Also, to minimize the entropy effect upon binding and to increase binding affinity, rotatory bonds are also unfavored. However, it is impossible to mimic such features with peptides, as the backbone of peptides is intrinsically hydrophilic and flexible. As we showed in a new figure, our pseudo-ligands are less similar to real ligands in those properties even with size alignment. You can find the figure in the new pdf in **Appendix G**.\n\nWe believe that the aforementioned divergence is the main limitation of poor zero-shot performance in predicting hydrophobicity scores (**Table 1**). However, as demonstrated in previous publications like CoSP, we could leverage real ligand-pocket pairs from PDBBind or BioLip database to further finetune our network, as an extension of the pipeline. In this way, we could handle property mismatches but still enjoy the power of our large-scale pretraining.\n\n## Response to the approach to defining pockets \n\nWe define the pocket following the UniMol setup. We found our model can also adapt to an 8Å setup in the toughM1 experiment \n\nIn particular, we define the pocket for each protein-ligand pair as residues of the protein that have at least one atom within the range of 6Å from a heavy atom in the ligand. To further explain our design, we have done an ablation study on different choices of thresholds of 4Å, 6Å, and 8Å:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| 4Å | 0.7062 | 0.7549 | 0.1240 | 0.1095 | 28.29 | 13.07 |\n| 6Å | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| 8Å | 0.8322 | 0.8292 | 0.1256 | 0.1125 | 34.83 | 12.92 |\n\nSince the 8Å threshold corresponds with the pocket definition in the Kahraman and Tough M1 datasets, it leads to optimal results. Our decision to use a 6Å threshold was made to align with the methodology of pretraining data creation by Uni-Mol, facilitating a fair comparison. Notably, even with the 6Å threshold, we achieved strong results in the pocket-matching task, which serves as a testament to the effectiveness of our approach.\n\n---\n\n# Response 1/3\nWe appreciate the time and effort you have dedicated to reviewing our paper, and we are grateful for your constructive feedback and thoughtful evaluation of our work. We would like to address your comments about the technical novelty of our work and provide additional clarification on certain aspects of our paper.\n\n## Response to technical novelty\n\nFirstly, we acknowledge your observation that we didn't use any fancy models compared with typical **model-centric** works. While we respect your assessment, we would like to highlight that our primary focus in this paper is a **data-centric** pretraining method to introduce groundbreaking improvements in the field of protein pocket pretraining. Our work aims to address a critical challenge of data scarcity in the field of protein pocket representation by constructing large-scale synthetic data that facilitates the pretraining of models, ultimately enhancing the accuracy and robustness of protein pocket representations. We believe that the research community can conduct fast following-ups on our **released dataset** with more sophisticated models, and even extend our pipelines to other tasks like docking or drug design. We also would like to point out that we have provided a novel method to **distill knowledge from well-trained molecule models** to protein models. Though employing the same contrastive loss, our approach is different from existing contrastive learning models since it uses a fixed molecular encoder. The motivation here is to use a well-trained molecular encoder on a relatively larger dataset to **guide** the training of the protein encoder. For example, some quantum-chemistry properties that are difficult to compute for large systems like proteins could be distilled from molecules, by using some quantum-chemistry-aware molecular encoders such as Frad. Though our framework seems to be simple, it is non-trivial to make it work. We have made several efforts to solve the unavoidable discrepancies between true ligands and pseudo ligands. The ablation study shows that our efforts are effective and necessary:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nRegarding your positive acknowledgment of our \"simple but effective model for the right question,\" we are pleased to hear that our approach resonates with the objective we set out to achieve. We designed our model with **simplicity** in mind, prioritizing effectiveness and practical utility for the specific problem domain. This deliberate choice aligns with the notion that sometimes the most impactful solutions are elegantly straightforward. Also, we intentionally borrowed a pocket encoder from Uni-Mol to make a **fair comparison** with it, which strongly supports the power of our dataset. Similar to the molecule encoder, our pocket encoder can be changed to any other model due to our flexible framework. Notably, **we didn't load the pretrained weights of Uni-Mol pocket encoder**. We only use the same backbone architecture and the pretraining was completely done on our processed data with our training strategy. \n\nNotably, many simple but effective approaches have been recognized by top-tier conferences and journals. A prime example of this is in the field of protein language models, like the ESM series and ProtTrans. These studies adapted the Transformer architecture and masked language modeling techniques from natural language processing to protein sequences. While they didn't introduce groundbreaking techniques, their substantial contributions to protein modeling are evident, with publications in prestigious conferences and journals like NeurIPS, ICML, TPAMI, PNAS, and Science. Another example is the widely acclaimed CLIP paper presented at ICML 2021. CLIP, while not employing novel techniques, stands as a hallmark of data-centric deep learning. Its use of contrastive learning enabled training on expansive web-sourced text-image datasets, moving beyond the constraints of meticulously curated databases like MS-COCO. This aligns with our approach to addressing **data scarcity challenges** by facilitating training on large-scale datasets, mirroring our strategy for overcoming similar hurdles in the protein pocket modeling domain.\n\nWe appreciate your efforts to remain neutral and understand the challenges in decision-making. We believe further discussion will highlight our paper's contributions. Your feedback and insights in the upcoming discussion will be invaluable for refining our work.", "author_response": "# Response 1/3\nWe appreciate the time and effort you have dedicated to reviewing our paper, and we are grateful for your constructive feedback and thoughtful evaluation of our work. We would like to address your comments about the technical novelty of our work and provide additional clarification on certain aspects of our paper.\n\n## Response to technical novelty\n\nFirstly, we acknowledge your observation that we didn't use any fancy models compared with typical **model-centric** works. While we respect your assessment, we would like to highlight that our primary focus in this paper is a **data-centric** pretraining method to introduce groundbreaking improvements in the field of protein pocket pretraining. Our work aims to address a critical challenge of data scarcity in the field of protein pocket representation by constructing large-scale synthetic data that facilitates the pretraining of models, ultimately enhancing the accuracy and robustness of protein pocket representations. We believe that the research community can conduct fast following-ups on our **released dataset** with more sophisticated models, and even extend our pipelines to other tasks like docking or drug design. We also would like to point out that we have provided a novel method to **distill knowledge from well-trained molecule models** to protein models. Though employing the same contrastive loss, our approach is different from existing contrastive learning models since it uses a fixed molecular encoder. The motivation here is to use a well-trained molecular encoder on a relatively larger dataset to **guide** the training of the protein encoder. For example, some quantum-chemistry properties that are difficult to compute for large systems like proteins could be distilled from molecules, by using some quantum-chemistry-aware molecular encoders such as Frad. Though our framework seems to be simple, it is non-trivial to make it work. We have made several efforts to solve the unavoidable discrepancies between true ligands and pseudo ligands. The ablation study shows that our efforts are effective and necessary:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nRegarding your positive acknowledgment of our \"simple but effective model for the right question,\" we are pleased to hear that our approach resonates with the objective we set out to achieve. We designed our model with **simplicity** in mind, prioritizing effectiveness and practical utility for the specific problem domain. This deliberate choice aligns with the notion that sometimes the most impactful solutions are elegantly straightforward. Also, we intentionally borrowed a pocket encoder from Uni-Mol to make a **fair comparison** with it, which strongly supports the power of our dataset. Similar to the molecule encoder, our pocket encoder can be changed to any other model due to our flexible framework. Notably, **we didn't load the pretrained weights of Uni-Mol pocket encoder**. We only use the same backbone architecture and the pretraining was completely done on our processed data with our training strategy. \n\nNotably, many simple but effective approaches have been recognized by top-tier conferences and journals. A prime example of this is in the field of protein language models, like the ESM series and ProtTrans. These studies adapted the Transformer architecture and masked language modeling techniques from natural language processing to protein sequences. While they didn't introduce groundbreaking techniques, their substantial contributions to protein modeling are evident, with publications in prestigious conferences and journals like NeurIPS, ICML, TPAMI, PNAS, and Science. Another example is the widely acclaimed CLIP paper presented at ICML 2021. CLIP, while not employing novel techniques, stands as a hallmark of data-centric deep learning. Its use of contrastive learning enabled training on expansive web-sourced text-image datasets, moving beyond the constraints of meticulously curated databases like MS-COCO. This aligns with our approach to addressing **data scarcity challenges** by facilitating training on large-scale datasets, mirroring our strategy for overcoming similar hurdles in the protein pocket modeling domain.\n\nWe appreciate your efforts to remain neutral and understand the challenges in decision-making. We believe further discussion will highlight our paper's contributions. Your feedback and insights in the upcoming discussion will be invaluable for refining our work."}
{"claim": "The theoretical bound presented in Theorem 3.1 is trivial and does not provide meaningful theoretical insight for this work.", "claim_type": "novelty", "paper_id": "uMAujpVi9m", "paper_title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "9ap4s1SZlZ", "reviewer": "Reviewer_yfM3", "review_text": "Summary: This paper primarily aims to enhance the pocket pretraining method, as existing approaches only consider pockets during pretraining. There are two main contributions in this paper: (1) The authors introduce a novel method, ProFSA, for pocket pretraining, which extracts additional information from corresponding ligands. However, the number of pocket-ligand complex structures is quite limited in existing datasets. (2) To address this issue, the authors generate over 5 million complexes by segmenting fragments and their corresponding pockets in protein structures. By aligning features of fragments and pockets, the pocket encoder learns the interaction between fragments and pockets. The authors design downstream tasks such as pocket druggability prediction, pocket matching, and ligand binding affinity prediction to demonstrate the effectiveness of ProFSA.\n\nStrengths: The authors propose a new perspective of pretraining pockets and construct a large-scale dataset, which data distribution is also considered, to make the efficient pre-training possible.\n\nThe results are competitive, especially for zero-shot settings.\n\nAbundant experiments and ablation study support the argument and result of the authors.\n\nWeaknesses: 1. The technical novelty is limited.\n  - The pocket encoder is borrowed from Uni-Mol.\n  - The contrastive loss is the vanilla form of classical contrastive learning.\n\n2. The bound of Theorem 3.1 is trivial. The authors claim that the bound naturally exists for these representations extracted by pretrained molecule models. However, it's a bit counterintuitive, because many models not pretrained on molecule datasets also fulfill this prior. So, can these models be used for this task? **I strongly suggest removing this part from the paper**.\n\n3. Some issues about dataset creation:\n - 3.1. The authors consider the distribution of ligand size and pocket size when designing the dataset. However, molecules possess more properties that can also lead to imbalance. It would be better to, at least, add some discussion about this issue.\n - 3.2. In the second stage of the data construction process, the approach to defining pockets needs further explanation or an ablation study.\n\n4. Experiments: It would be better to add some biological justification or visualization of the results.\n\nFor this paper, one fact is that the technical novelty is below the bar of ICLR. However, I admire the simple but effective model for the right question. It's a struggle for me to make a decision. I will maintain a neutral attitude and make my final decision after the discussion.\n\nQuestions: See weakness.", "labeling_timestamp": "2026-01-11T16:24:49.727202", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The authors explicitly reject the claim that Theorem 3.1 is trivial: they argue the theorem's conditions are nontrivial, clarify the nontrivial nature of its conclusion (showing loss consistency and transfer ability from pseudo- to real-ligand domains), and offer technical reasoning to support this. Although they state they removed the theorem from the main body, they still defend its theoretical value.", "evidence": "“If you mean ... the conclusion of theorem 3.1 is trivial, we want to emphasize that our conclusion is \\lim_{L_i(t, s) =0}L_i(t^{(0)}, s) =0. (We write it in a \\epsilon-\\delta language in paper. They are equivalent forms.) Our conclusion shows the loss containing real ligands is consistent with the loss containing pseudo ligands that we optimized in pre-training. ... Therefore, our theorem is nontrivial and reveals the transfer ability of our contrastive pre-training from the pseudo-ligand domain to the real ligand domain.”; additionally, “We have removed Theorem 3.1 from the main body of our paper.”", "combined_author_response": "Dear AC and reviewers,\n\nWe sincerely appreciate your time and efforts in reviewing our work. Based on your suggestions, we have revised our paper. We would like to use this section to reiterate the explanation for some common concerns and summarize the contributions of our paper.\n\nIn our revised paper, we provide a detailed explanation of the motivation and theory behind our method: **the mechanisms of pocket-peptide interactions and pocket-ligand interactions are very similar**. Our proposed **data-centric** pretraining pipeline is unique in its use of peptides to mimic small molecules, a method that is, to our knowledge, **the first of its kind**. We expanded our discussion about its biological insights in **Section 3**. We add a **new Figure 2 to illustrate different types of non-covalent interactions** that are common for both ligand-protein data and intra-protein data. Following this theory, we have designed a pipeline to generate **over 5 million** synthetic ligand-protein pairs, which greatly eases the **data scarcity** problem in the field of protein-ligand interaction learning. We also prove our data quite effective in various downstream tasks, including pocket property prediction, pocket matching, ligand binding affinity prediction, ligand efficiency prediction, and protein-protein interaction tasks. The **last two tasks are newly added** to further support our method (**Appendix C5 and C6**). We are confident that our **ready-to-release synthetic dataset** will significantly benefit the AI for drug discovery community. Its impact extends beyond enhancing pocket representations in downstream analyses, as it also equips researchers with a valuable resource for a range of applications, such as protein-ligand docking, structure-based drug design, and virtual screening. Importantly, these areas often face challenges due to **a lack of training data necessary for learning effective binding patterns between protein pockets and ligands**. \n\nIn our revised paper, we recognize the differences between real ligands and pseudo ligands (peptides). We address these discrepancies and present a distribution plot of various properties in **Appendix G**. Our approach primarily leverages the **analogous nature of ligand-pocket and peptide-pocket interactions**. Therefore, despite these differences, our model can **still extract valuable interaction information from the pretraining dataset**, which enhances its performance in downstream tasks. This is illustrated in **Figure 4(a)**, showing the benefits of interaction-aware pretraining. We have also undertaken measures like distribution alignment and freezing the molecular encoder to mitigate the impact of these discrepancies. The positive results of these efforts, as evidenced in **Table 5**, confirm their effectiveness and necessity. Notably, our pretrained model can be **further finetuned with real labeled ligand-pocket pairwise data**, as we show in the Ligand Binding Affinity task in **section 4.3**.\n\nAs for other questions, we have added more ablation studies on our design choices, including the impact of a fixed molecular encoder (Table 5), and different threshold values to define pockets(Appendix C.4). \n\nWe are immensely grateful for the invaluable feedback from all the reviewers, which has guided us in refining and clarifying our work. We hope that the revised version of our paper, coupled with the discussion period, will more clearly highlight the novelty, effectiveness, and contributions of our approach to the field.\n\n---\n\nThank you for your invaluable feedback on our paper. In response to your insightful suggestions, we have made several modifications in our revised paper. \n\nWe have cited additional papers in Section 2.2 following your advice. The description of loss terms in Section 3.2 has been corrected.  COSP was not included as a baseline for the Ligand Binding Affinity (LBA)  task because it is not open-sourced, and its performance on LBA was not evaluated in their published work. We used it as a baseline in the pocket-matching task.\n\nIn Section 3, we've expanded our discussion to answer your question on **ProFSA's effectiveness without distributional alignment**. This section now offers a detailed explanation of the foundational concepts and justifications for our method. We draw attention to the parallels between ligand-protein and intra-protein non-covalent interactions, as shown in **Figure 2**. This comparison supports our strategy of using peptides as stand-ins for actual ligands to mimic pocket-ligand interactions. Consequently, **even in the absence of distributional alignment, the types of interactions remain comparably relevant**, which explains why ProFSA continues to function, albeit with reduced effectiveness, as demonstrated in Table 5. We also want to clarify that distributional alignment is only applied to fragment-pocket complex as a whole, and **it would not change the definition of the pocket with a given ligand**. Without this alignment, these fragments are chosen uniformly, with lengths varying from 1 to 7 residues, and pockets are consistently identified based on residues within a 6Å radius of these peptide fragments.\n\nWe are grateful for the time and effort you have dedicated to reviewing our work. Your thorough and constructive feedback has significantly contributed to the refinement of our research. Please let us know if there are any other aspects of our paper that you would like us to address or clarify. We look forward to your further feedback.\n\n---\n\nThank you for your insightful feedback. In response to your concerns regarding the scope of our tasks, we have expanded our research in the revised paper. We now include additional tasks such as **ligand efficiency prediction** and **protein-protein affinity prediction**, detailed in **Appendix C5 and C6**. These tasks were incorporated to enhance the robustness of our method. We acknowledge that they are currently in the appendix due to space constraints, but we are open to relocating them to the main text if it would be beneficial.\n\nRegarding your concern about the lack of recent baselines, we provided a detailed explanation in our previous response. We want to reiterate that our benchmarks do include the latest baselines, with some as recent as 2023.\n\nWe greatly appreciate the time and effort you have invested in reviewing our work. Your feedback has been invaluable in refining our research. Please let us know if there are any other aspects of our paper that you would like us to address or clarify. We look forward to your further feedback.\n\n---\n\nIn our revised paper, we have incorporated changes following feedback from another reviewer, which we believe also addresses your concerns and queries.\n\nRegarding your question on the evidence supporting our approach, particularly the use of peptides to simulate small molecules, we've enriched **Section 3.1**. A new paragraph there now provides an in-depth explanation of non-covalent interactions. To aid understanding, **Figure 2** has been updated with visualizations that include both real and pseudo receptor-ligand pairs, illustrating three types of interactions. Furthermore, we delve into the details of **how intra-protein interactions mirror protein-ligand interactions**, involving specific types of amino acids.\n\nTo better evaluate our design choices, the ablation studies have been extended. Alongside the existing studies in **Table 4** (different pretrained molecular encoders), **Table 5** (distribution alignment), and **Figure 5** (data scale). we've added a study on the impact of using a fixed molecular encoder in **Table 5** in the main text to address your question on the fixed molecular encoder. We've also included an ablation study on varying **cutoff values for pocket definition**, placed in the **Appendix C.4** due to its more focused scope. We can move it to the main part of the paper in the future if needed.\n\nWe acknowledge your concern about the **differences between true and pseudo ligands**, so we've expanded our discussion on the distinct properties of our curated dataset versus the PDBBind dataset in **Figure 9** and **Section 3.2**. The ablation study in **Table 5** demonstrates our methods and their effectiveness in minimizing these discrepancies. Notably, the performance significantly drops without our distribution alignment or fixed molecular encoder, highlighting our efforts to mitigate these issues. The updated **Figure 2** further supports this, showing that while the properties of true and pseudo ligands may differ, **the interaction types are consistent.** We hope these results can ease your concerns about our synthetic dataset, and they also respond to your query about **why we kept the molecular encoder constant during pocket pretraining.**\n\nWe appreciate the constructive critiques and thoughtful engagement from you. Your feedback has undeniably contributed to the refinement and strengthening of our paper. We sincerely value the time and expertise invested in the review process, and we look forward to any further suggestions you may have.\n\n---\n\nWe sincerely appreciate the thoughtful feedback provided on our paper and have diligently incorporated your suggestions into this revised version. In response to your guidance, we have made three significant modifications to the core content of our paper, all of which aim to enhance clarity and address the concerns raised.\n\nTaking into account your suggestion, we have removed **Theorem 3.1** from the main body of our paper. Additionally, we've improved the explanation and included an ablation study on our method of defining pockets. This ablation study, focusing on the cutoff values and primarily a hyperparameter issue, is placed in the Appendix due to its more limited scope of insight. We can move it to the main body of the paper in the future if needed. We have also expanded our discussion on the differences between our dataset and the PDBBind dataset, elucidating how fixed molecule encoders contribute to mitigating this discrepancy.\n\nIn **Section 3.1**, we have introduced a new paragraph dedicated to providing readers with a deeper understanding of non-covalent interactions. To facilitate comprehension, we have included visualizations featuring three types of interactions, incorporating both real and pseudo receptor-ligand pairs in the new **Figure 2**. Furthermore, we delve into the details of how intra-protein interactions mirror protein-ligand interactions, involving specific types of amino acids.\n\nThe biological justifications for the efficacy of interaction-aware pretraining are now more extensively explored in our revised paper. Specifically, we present a compelling showcase featuring estradiol-binding proteins, offering insights into the geometric disparities between the compared proteins and highlighting the similarities in their binding interfaces in the new **Figure 4a**. The existing visualization of BioLip pocket representations in the new **Figure 4b** can make our claims more convincing. Together, these visual aids illustrate how our interaction-aware pretraining empowers models to focus on crucial interface residues, discern subtle distinctions, and disregard irrelevant geometric dissimilarities.\n\nWe have also included an in-depth discussion of pretrained molecule encoders in the newly added **Section 2.3**, further enriching the theoretical foundation of our work.\nAll changes can be found in the renewed paper, which is updated lively.\n\nWe genuinely value the constructive input you provided during the review process, as it has undoubtedly contributed to the refinement of our paper. Your insights have proven invaluable in elevating the clarity and depth of our contributions to the field. We express our gratitude for the time and expertise invested in reviewing our work, and we look forward to any additional feedback or suggestions you may have.\n\n---\n\n# Response 2/2\n\n## Response to the question on N Terminal and C Terminal\n\nWhen a peptide bond is formed (green), one amine group and one carboxy group are left untouched. Therefore, in a linear polypeptide, there would always be a free amine group and a free carboxy group, and they are called the N terminal(blue) and the C terminal(red) respectively in biochemistry. You can find the visualization in **Appendix H**.\n\n## Response to fixing the molecule encoder in contrastive learning\n\nFrom an **effectiveness** standpoint, as you mentioned before, there are unavoidable discrepancies between pseudo and true ligands. If we didn't fix our molecule encoder during pretraining, then the conditions of Theorem 3.1 might not be met. The ablation results presented in the previous response clearly indicate a performance decline when the molecular encoder is not fixed during contrastive learning, particularly in the context of pocket matching. This supports our assertion that the pocket encoder struggles to acquire chemical knowledge without the stability provided by a fixed molecular encoder.\n\nFrom an **efficiency** standpoint, a fixed molecule encoder also offers significant advantages. It reduces memory usage and accelerates training speed because all molecule embeddings can be pre-computed before training. This eliminates the need for feed-forward and back-propagation operations of the molecule encoder during training, leading to efficiency gains. Additionally, adapting to different molecule encoders becomes more straightforward when they are not co-trained during the pretraining process. This approach allows for more flexibility and ease in integrating various encoding methodologies **even if the molecule encoder is not accessible for training**. That is to say, our approach can even be used with undifferentiable molecular representations.\n\n## Response to extending to other tasks like protein-protein interaction prediction\n\nThank you for pointing out the potential application of our framework to protein-protein interaction (PPI) tasks. We have to point out that the interactions between proteins and proteins are far different from proteins and molecules. Specifically, the interaction areas are usually much larger, but shallower, and involved functional groups are less diverse due to limited types of natural amino acids. Therefore, several changes should be made to extend our ProFSA method to PPI tasks:\n\n1. As for data creation, unlike the localized and specific nature of pocket-ligand interactions, protein-protein interactions are **more global and involve broader protein features**. This requires a shift in the data creation process to split entire protein structures into domains instead of short fragments and to capture potential interactions between them. \n\n2. The interaction interface between protein and protein is much larger. We might need to change from atomic-level to residue-level representations to efficiently capture interaction information.\n\n3. For contrastive learning training, ProFSA uses a fixed pretrained molecular encoder to cope with various small molecules and their atomic representation. However, for protein-protein interaction, it is not necessary as we could only model 20 types of amino acids at the residue level.\n\nDespite several changes that need to be done for PPI tasks, we find that we could divide protein complexes into local interactions. When the complex structures and monomer structures are provided, we have achieved a reasonable result on a flexible PPI affinity prediction benchmark with our current model in a zero-shot way. \n\n| Method            | Spearman $\\uparrow$  |\n|-------------------|----------------------|\n| SchNet            | 0.072 ± 0.021        |\n| DimeNet++         | 0.171 ± 0.054        |\n| EGNN              | 0.080 ± 0.038        |\n| TorchMD           | 0.117 ± 0.008        |\n| GET               | **0.363 ± 0.017**    |\n| ProFSA(zero-shot) | _0.248_              |\n\nSurprisingly, as a **zero-shot** method, ProFSA is able to outperform other **supervised learning** models except for GET, a newly proposed unified model. This demonstrates our approach is also able to capture protein-protein interaction information despite it being designed for protein-ligand interaction. \n\nA detailed explanation of this experiment is shown in **Appendix C6**.\n\n---\n\n# Response 1/2\n\n## Response to evidence for the construction of the pseudo-ligand and using peptides to replace small molecules \n\nAs we discussed in the paper, pseudo-ligands share similar sizes as real ligands, and they also make similar non-covalent interactions with the pocket. To further support our proposal, we would like to cite another paper **\"A defined structural unit enables de novo design of small-molecule–binding proteins\"**[1], published in Science 2020, which supports that **intra-protein interactions are similar to protein-ligand interactions.** That is the reason we use peptides to **represent** small molecules to let the pocket encoder learn the interaction information. \n\nAlso, we'd like to clarify that our intention is not to **\"replace\"** small molecules with peptides. It's a pretraining framework that leverages abundant protein-only data and uses peptides to **simulate** pocket-small molecule interactions for enhanced pocket representations. Following this pretraining phase, **the model can be finetuned with datasets that include real small molecules, ensuring its effectiveness in practical applications**.\n\nTo demonstrate the validity of constructing pseudo-ligands, we provide visualizations in **Appendix F** showing that interactions between pockets and our pseudo-ligands are similar to those with true ligands. These figures reveal shared interaction types like **hydrogen bonding**, **$\\pi-\\pi$ stacking**, and **salt bridge**, depicted with color-coded dashed lines for each interaction type. This should help clarify the rationale and evidence supporting our approach.\n\n[1] Polizzi, Nicholas F., and William F. DeGrado. \"A defined structural unit enables de novo design of small-molecule–binding proteins.\" Science 369, no. 6508 (2020)\n\n## Response to ablation studies evaluating the impact of critical design choices\n\nThanks for your advice on adding more ablation studies on design choices. We did ablation studies on the effectiveness of distributional alignment, where the major difference is that fragment sizes are modified. We found that aligned fragment sizes with real ligands could provide the best performance. The result is shown in **Table 5**.\n\nAs for the distance thresholds, we define the pocket following the UniMol setup. We also found our model can also adapt to an 8Å setup in the ToughM1 experiment. Following your advice, we did an ablation study on the distance thresholds. We tested our method with three different thresholds: 4Å, 6Å, and 8Å. The result is shown below:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| 4Å | 0.7062 | 0.7549 | 0.1240 | 0.1095 | 28.29 | 13.07 |\n| 6Å | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| 8Å | 0.8322 | 0.8292 | 0.1256 | 0.1125 | 34.83 | 12.92 |\n\nSince the 8Å threshold corresponds with the pocket definition in the Kahraman and Tough M1 datasets, it leads to optimal results. Our decision to use a 6Å threshold was made to align with the methodology of pretraining data creation by Uni-Mol, facilitating a fair comparison. Notably, even with the 6Å threshold, we achieved strong results in the pocket-matching task, which serves as a testament to the effectiveness of our approach.\n\n## Response to substantial discrepancies between pseudo and real ligands\n\nThank you for pointing out the discrepancies between pseudo and real ligands. We understand and acknowledge your concern. To mitigate the impact of this discrepancy, we have made several efforts. First, we performed **distribution alignment** to make the data distribution of pseudo-ligands more similar to that of true ligands in PDBbind. Additionally, we **fixed the molecule encoder during pretraining** to prevent it from being misled by the discrepancy. Our ablation studies have shown that these strategies are effective:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nThe table above shows that lacking distribution alignment and not fixing the molecular encoder during pretraining both lead to lower performance, underscoring the effectiveness of our methods in reducing discrepancies.\n\nTheorem 3.1 theoretically supports our approach's efficacy, even with discrepancies between pseudo and real ligands. Empirically, our method outperforms other pretraining techniques, confirming its effectiveness despite these discrepancies.\n\n---\n\n## Response to limited tasks and potential in protein-ligand binding pose prediction\n\nIn our paper, the primary objective is to assess the effectiveness of our pretraining method and the quality of the trained pocket representations. To achieve this, we focused on two types of downstream tasks: pocket-only tasks (pocket property prediction and pocket matching) and pocket-ligand interaction tasks (ligand binding affinity prediction).\n\nWe appreciate the suggestion to apply our method to the protein-ligand binding pose prediction task. However, it's important to note that our current evaluation framework is designed to specifically assess **pocket representations**. For the tasks we chose, the architecture is straightforward: either a **zero-shot** evaluation or a **simple MLP** for mapping embeddings to predictions. In contrast, protein-ligand binding pose prediction often involves **complex methodologies**. For instance, state-of-the-art methods like DiffDock[1] require training a diffusion generative model and a separate confidence model, while other approaches like EDM-Dock[2], rely on reconstructing ligand conformations from predicted distance maps. These methods are not end-to-end and do not directly align with our objective of evaluating pretrained pocket representations.\n\nNevertheless, we recognize the potential of our method in enhancing existing binding pose prediction techniques. To integrate our approach, we would need to modify our framework. Using the same data creation strategy, we could train a binding pose prediction model with our preprocessed data, which could then be further fine-tuned using real pocket-ligand pair data from sources like PDBbind. Additionally, our data could be used to train a side-chain packing model, allowing for side-chain flexibility during docking. Thank you again for your advice, and we will leave protein-ligand binding pose prediction as a separate future work for our method.\n\nIn response to the need for evaluating our method on a broader range of tasks, we have extended our analysis to include two additional downstream tasks: LEP (Ligand Efficacy Prediction), and PPA (Protein-Protein Affinity Prediction).\n\nresult for LEP:\n\n| Method      | AUROC $\\uparrow$ | AUPRC $\\uparrow$ |\n|-------------|--------------|--------------|\n| ATOM3D-GNN  | 0.681        | 0.598        |\n| GeoSSL      | 0.776±0.03   | 0.694±0.06   |\n| Uni-Mol     | 0.782±0.02   | 0.695±0.07   |\n| ProFSA      | 0.840±0.04   | 0.806±0.04   |\n\nGeoSSL and Uni-Mol, both pretraining methods, yield comparable results. However, ProFSA outperforms these methods, demonstrating the advantage of our pocket pretraining approach.\n\nresult for PPA:\n\n| Method            | Spearman $\\uparrow$  |\n|-------------------|----------------------|\n| SchNet            | 0.072 ± 0.021        |\n| DimeNet++         | 0.171 ± 0.054        |\n| EGNN              | 0.080 ± 0.038        |\n| TorchMD           | 0.117 ± 0.008        |\n| GET               | **0.363 ± 0.017**    |\n| ProFSA(zero shot) | _0.248_              |\n\nAs a **zero-shot** method, ProFSA is able to outperform other **supervised learning** models except for GET, a newly proposed unified model. This demonstrates our approach is able to capture protein-protein interaction information despite it being designed for protein-ligand interaction modeling.\n\nYou can find detailed experiment settings and results in **Appendix C5 and C6**.\n\n[1] Corso et al., \"Diffdock: Diffusion steps, twists, and turns for molecular docking.\", ICLR 2023.\n[2] Masters et al., \"Deep learning model for efficient protein–ligand docking with implicit side-chain flexibility.\" Journal of Chemical Information and Modeling 63, no. 6 (2023).\n\n## Response to the issue of lack of latest baselines\n\nFor the pocket matching task, CoSP is a newly proposed baseline which is published in ECML PKDD 2023. Alongside CoSP and Uni-Mol, we selected some of the most effective baseline results presented in the CoSP paper. We also tried to evaluate recent methods like PocketAnchor (Li et al., Cell Systems 2022) on our own since they are not tested on the pocket matching task. However, since the result didn't outperform other baseline machine learning methods(Uni-Mol, CoSP, and DeeplyTough), we decided not to include it in our final baseline comparison. We can include it in the camera-ready version if needed.\n\nFor the ligand binding affinity task, a lot of newly proposed baselines are included, i.e. ProNet (Wang et al., 2022b); as well as pretraining methods such as GeoSSL (Liu et al., 2023), EGNN-PLM (Wu et al., 2022), DeepAffinity (Karimi et al., 2019) and Uni-Mol (Zhou et al., 2023).\n\n---\n\n## Response to lack of related works\n\nThanks for providing us with more insightful related works that could support our arguments. We actually have cited these three papers in the section on ligand binding affinity experiments, but we are also happy to add these in Related Works section 2.2. You can find the change in pdfdiff.\n\n## Response to lack of COSP as a baseline in Table 3\n\nBecause best to our knowledge, they did not release their code and they did not test their method on the ligand binding affinity dataset. \n\n## Response to the question on the loss terms\n\nThank you for your advice. We apologize that we accidentally mentioned loss1 and loss2 in reverse, and we are sorry for the confusing statement. You are correct that the first loss is to identify the true protein pocket when given a pseudo-ligand. We revise the original statement to: \"The primary purpose of the first loss is to identify the true protein pocket from a batch of samples when given a pseudo-ligand. Similarly, the second loss seeks to identify the corresponding ligand fragment for a given pocket.\" You can also find the change in pdfdiff.\n\n## Response to the question on the length of the pocket representation without alignment\n\nWith or without distributional alignment, pockets are always defined by the given protein fragment with a fixed distance cutoff (6Å in our works, following the UniMol setup). The distributional alignment process merely samples these pairs to match the sizes of real ligands and pockets. Without distributional alignment, fragments are uniformly sampled from 1 to 7 residues. Pockets are similarly determined as with residues within the range of 6Å around peptide fragments.\n\n---\n\n# Question about the confidence score\n\nThanks for your review. We found your understanding and judgment of our paper comprehensive and precise. Also, you provided detailed and thoughtful advice for our paper, which helped a lot. We sincerely appreciate your review, but we also wonder if there is any misunderstanding here, as you only give a confidence score of 1.\n\n---\n\n# Response 3/3\n\n## Response to biological justification and visualization of the results\n\nOur method's **biological justification** for achieving good experimental results is based on the properties and interactions of pseudo-ligands. As discussed in our paper, these pseudo-ligands are similar in size to real ligands and engage in comparable non-covalent interactions with protein pockets. This fundamental similarity is crucial for the effectiveness of our approach.\n\nMoreover, our methodology is further supported by the findings in the 2020 Science paper titled \"A defined structural unit enables de novo design of small-molecule–binding proteins.\"[1] This research underscores that **intra-protein interactions are analogous to protein-ligand interactions**, which validates our use of peptides as proxies for small molecules. By employing peptides as pseudo-ligands in this manner, our pocket encoder is able to learn and replicate the interaction dynamics typically observed in real ligand scenarios. This understanding is pivotal to the success of our method in downstream applications.\n\nTo better justify our methods and results, following your advice, we also provide some visualizations in **Appendix F**. **Figure 6** illustrates that the pseudo pairs we created share various interaction types commonly found in real pocket-ligand pairs, such as **hydrogen bonding, $\\pi-\\pi$ stacking, and salt bridge**. In the figures, each type of interaction is represented by dashed lines, color-coded to correspond with the specific interaction type.\n\nAnother visualization of our pocket-matching result is shown in **Figure 7**. We showed an example of two non-homology ATP binding pockets to explain why pocket-matching can benefit from interaction-aware pretraining so that we have achieved superior results in the Kahraman dataset and the BioLip t-SNE visualization. The PEP carboxykinase(PDB:1AYL) and tRNA synthetase(PDB:1B8A) are two ATP-binding proteins that share zero sequence similarity (verified with the BLAST) extracted from the Kahraman dataset. However, as they are both fueled by ATP, their binding site shares similar binding patterns. The cation-$\\pi$ interactions(blue dash lines) and salt bridges (magenta dash lines) are important to the ATP binding, which can be viewed as convergent evolutions at the molecule level. Even though, these two pockets are very distinct in terms of shapes and sizes because they bind ATPs in different conformations. Therefore, biochemical interactions are the key to accomplishing the pocket-matching task, which is ignored in previous self-supervised learning methods like Uni-Mol.\n\n[1] Polizzi, Nicholas F., and William F. DeGrado. \"A defined structural unit enables de novo design of small-molecule–binding proteins.\" Science 369, no. 6508 (2020)\n\n---\n\n# Response 2/3\n\n## Response to the bound of Theorem 3.1 \n\nWe are not very clear about the point of your question. We have two understandings and they are replied as follows.\n\nIf you mean $ ||t − t ^{(0)}|| < 1/2 l_T$ is trivial, please note that it is only a condition of the theorem. As you said, many other encoders can also satisfy this condition. This shows that our method is applicable to many different molecular encoders with or without pretraining as we proved in **Table 4**. Moreover, the condition is only a necessary condition. We also need the pretraining contrastive loss to be sufficiently small to guarantee a small contrastive loss between pockets and real ligands. In fact, in the process of our early exploration, we noticed that some encoders cannot achieve a low pretraining loss, indicating the entire conditions of the theorem are nontrivial.\n\nIf you mean that the conclusion of theorem 3.1 is trivial, we want to emphasize that our conclusion is $\\lim_{L_i(t, s) =0}L_i(t^{(0)}, s) =0$. (We write it in a $\\epsilon$-$\\delta$ language in paper. They are equivalent forms.) Our conclusion shows the loss containing real ligands is consistent with the loss containing pseudo ligands that we optimized in pre-training. \n\nIn contrast,  a trivial result from $||t − t ^{(0)}|| < 1/ 2l_T$ is that $\\lim_{L_i(t, s) =0}L_i(t^{(0)}, s) <B$, B is a bound related to l_t and the representations. It does not guarantee a consistent loss and is substantially different from our result.\nTherefore, our theorem is nontrivial and reveals the transfer ability of our contrastive pre-training from the pseudo-ligand domain to the real ligand domain.\n\nAs for this theoretical result, we are open to further discussion.\n\n## Response to molecules possess more properties that can also lead to imbalance\n\nIt is correct that molecules have more properties than their size, like logP, Hbond donor and acceptor number, and rotatory bond number. As small molecule drugs are mostly designed to penetrate barriers like gut or cell membranes, they usually have much larger logP values, which means more hydrophobic. For the same reason, they usually have fewer hydrogen bond donors and acceptors, and only minimal essential ones are kept for specified interactions. Also, to minimize the entropy effect upon binding and to increase binding affinity, rotatory bonds are also unfavored. However, it is impossible to mimic such features with peptides, as the backbone of peptides is intrinsically hydrophilic and flexible. As we showed in a new figure, our pseudo-ligands are less similar to real ligands in those properties even with size alignment. You can find the figure in the new pdf in **Appendix G**.\n\nWe believe that the aforementioned divergence is the main limitation of poor zero-shot performance in predicting hydrophobicity scores (**Table 1**). However, as demonstrated in previous publications like CoSP, we could leverage real ligand-pocket pairs from PDBBind or BioLip database to further finetune our network, as an extension of the pipeline. In this way, we could handle property mismatches but still enjoy the power of our large-scale pretraining.\n\n## Response to the approach to defining pockets \n\nWe define the pocket following the UniMol setup. We found our model can also adapt to an 8Å setup in the toughM1 experiment \n\nIn particular, we define the pocket for each protein-ligand pair as residues of the protein that have at least one atom within the range of 6Å from a heavy atom in the ligand. To further explain our design, we have done an ablation study on different choices of thresholds of 4Å, 6Å, and 8Å:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| 4Å | 0.7062 | 0.7549 | 0.1240 | 0.1095 | 28.29 | 13.07 |\n| 6Å | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| 8Å | 0.8322 | 0.8292 | 0.1256 | 0.1125 | 34.83 | 12.92 |\n\nSince the 8Å threshold corresponds with the pocket definition in the Kahraman and Tough M1 datasets, it leads to optimal results. Our decision to use a 6Å threshold was made to align with the methodology of pretraining data creation by Uni-Mol, facilitating a fair comparison. Notably, even with the 6Å threshold, we achieved strong results in the pocket-matching task, which serves as a testament to the effectiveness of our approach.\n\n---\n\n# Response 1/3\nWe appreciate the time and effort you have dedicated to reviewing our paper, and we are grateful for your constructive feedback and thoughtful evaluation of our work. We would like to address your comments about the technical novelty of our work and provide additional clarification on certain aspects of our paper.\n\n## Response to technical novelty\n\nFirstly, we acknowledge your observation that we didn't use any fancy models compared with typical **model-centric** works. While we respect your assessment, we would like to highlight that our primary focus in this paper is a **data-centric** pretraining method to introduce groundbreaking improvements in the field of protein pocket pretraining. Our work aims to address a critical challenge of data scarcity in the field of protein pocket representation by constructing large-scale synthetic data that facilitates the pretraining of models, ultimately enhancing the accuracy and robustness of protein pocket representations. We believe that the research community can conduct fast following-ups on our **released dataset** with more sophisticated models, and even extend our pipelines to other tasks like docking or drug design. We also would like to point out that we have provided a novel method to **distill knowledge from well-trained molecule models** to protein models. Though employing the same contrastive loss, our approach is different from existing contrastive learning models since it uses a fixed molecular encoder. The motivation here is to use a well-trained molecular encoder on a relatively larger dataset to **guide** the training of the protein encoder. For example, some quantum-chemistry properties that are difficult to compute for large systems like proteins could be distilled from molecules, by using some quantum-chemistry-aware molecular encoders such as Frad. Though our framework seems to be simple, it is non-trivial to make it work. We have made several efforts to solve the unavoidable discrepancies between true ligands and pseudo ligands. The ablation study shows that our efforts are effective and necessary:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nRegarding your positive acknowledgment of our \"simple but effective model for the right question,\" we are pleased to hear that our approach resonates with the objective we set out to achieve. We designed our model with **simplicity** in mind, prioritizing effectiveness and practical utility for the specific problem domain. This deliberate choice aligns with the notion that sometimes the most impactful solutions are elegantly straightforward. Also, we intentionally borrowed a pocket encoder from Uni-Mol to make a **fair comparison** with it, which strongly supports the power of our dataset. Similar to the molecule encoder, our pocket encoder can be changed to any other model due to our flexible framework. Notably, **we didn't load the pretrained weights of Uni-Mol pocket encoder**. We only use the same backbone architecture and the pretraining was completely done on our processed data with our training strategy. \n\nNotably, many simple but effective approaches have been recognized by top-tier conferences and journals. A prime example of this is in the field of protein language models, like the ESM series and ProtTrans. These studies adapted the Transformer architecture and masked language modeling techniques from natural language processing to protein sequences. While they didn't introduce groundbreaking techniques, their substantial contributions to protein modeling are evident, with publications in prestigious conferences and journals like NeurIPS, ICML, TPAMI, PNAS, and Science. Another example is the widely acclaimed CLIP paper presented at ICML 2021. CLIP, while not employing novel techniques, stands as a hallmark of data-centric deep learning. Its use of contrastive learning enabled training on expansive web-sourced text-image datasets, moving beyond the constraints of meticulously curated databases like MS-COCO. This aligns with our approach to addressing **data scarcity challenges** by facilitating training on large-scale datasets, mirroring our strategy for overcoming similar hurdles in the protein pocket modeling domain.\n\nWe appreciate your efforts to remain neutral and understand the challenges in decision-making. We believe further discussion will highlight our paper's contributions. Your feedback and insights in the upcoming discussion will be invaluable for refining our work.", "author_response": "# Response 1/3\nWe appreciate the time and effort you have dedicated to reviewing our paper, and we are grateful for your constructive feedback and thoughtful evaluation of our work. We would like to address your comments about the technical novelty of our work and provide additional clarification on certain aspects of our paper.\n\n## Response to technical novelty\n\nFirstly, we acknowledge your observation that we didn't use any fancy models compared with typical **model-centric** works. While we respect your assessment, we would like to highlight that our primary focus in this paper is a **data-centric** pretraining method to introduce groundbreaking improvements in the field of protein pocket pretraining. Our work aims to address a critical challenge of data scarcity in the field of protein pocket representation by constructing large-scale synthetic data that facilitates the pretraining of models, ultimately enhancing the accuracy and robustness of protein pocket representations. We believe that the research community can conduct fast following-ups on our **released dataset** with more sophisticated models, and even extend our pipelines to other tasks like docking or drug design. We also would like to point out that we have provided a novel method to **distill knowledge from well-trained molecule models** to protein models. Though employing the same contrastive loss, our approach is different from existing contrastive learning models since it uses a fixed molecular encoder. The motivation here is to use a well-trained molecular encoder on a relatively larger dataset to **guide** the training of the protein encoder. For example, some quantum-chemistry properties that are difficult to compute for large systems like proteins could be distilled from molecules, by using some quantum-chemistry-aware molecular encoders such as Frad. Though our framework seems to be simple, it is non-trivial to make it work. We have made several efforts to solve the unavoidable discrepancies between true ligands and pseudo ligands. The ablation study shows that our efforts are effective and necessary:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nRegarding your positive acknowledgment of our \"simple but effective model for the right question,\" we are pleased to hear that our approach resonates with the objective we set out to achieve. We designed our model with **simplicity** in mind, prioritizing effectiveness and practical utility for the specific problem domain. This deliberate choice aligns with the notion that sometimes the most impactful solutions are elegantly straightforward. Also, we intentionally borrowed a pocket encoder from Uni-Mol to make a **fair comparison** with it, which strongly supports the power of our dataset. Similar to the molecule encoder, our pocket encoder can be changed to any other model due to our flexible framework. Notably, **we didn't load the pretrained weights of Uni-Mol pocket encoder**. We only use the same backbone architecture and the pretraining was completely done on our processed data with our training strategy. \n\nNotably, many simple but effective approaches have been recognized by top-tier conferences and journals. A prime example of this is in the field of protein language models, like the ESM series and ProtTrans. These studies adapted the Transformer architecture and masked language modeling techniques from natural language processing to protein sequences. While they didn't introduce groundbreaking techniques, their substantial contributions to protein modeling are evident, with publications in prestigious conferences and journals like NeurIPS, ICML, TPAMI, PNAS, and Science. Another example is the widely acclaimed CLIP paper presented at ICML 2021. CLIP, while not employing novel techniques, stands as a hallmark of data-centric deep learning. Its use of contrastive learning enabled training on expansive web-sourced text-image datasets, moving beyond the constraints of meticulously curated databases like MS-COCO. This aligns with our approach to addressing **data scarcity challenges** by facilitating training on large-scale datasets, mirroring our strategy for overcoming similar hurdles in the protein pocket modeling domain.\n\nWe appreciate your efforts to remain neutral and understand the challenges in decision-making. We believe further discussion will highlight our paper's contributions. Your feedback and insights in the upcoming discussion will be invaluable for refining our work."}
{"claim": "The authors claim the bound naturally exists for representations from pretrained molecule models, yet many non-molecule-pretrained models also satisfy this prior.", "claim_type": "experimental", "paper_id": "uMAujpVi9m", "paper_title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "9ap4s1SZlZ", "reviewer": "Reviewer_yfM3", "review_text": "Summary: This paper primarily aims to enhance the pocket pretraining method, as existing approaches only consider pockets during pretraining. There are two main contributions in this paper: (1) The authors introduce a novel method, ProFSA, for pocket pretraining, which extracts additional information from corresponding ligands. However, the number of pocket-ligand complex structures is quite limited in existing datasets. (2) To address this issue, the authors generate over 5 million complexes by segmenting fragments and their corresponding pockets in protein structures. By aligning features of fragments and pockets, the pocket encoder learns the interaction between fragments and pockets. The authors design downstream tasks such as pocket druggability prediction, pocket matching, and ligand binding affinity prediction to demonstrate the effectiveness of ProFSA.\n\nStrengths: The authors propose a new perspective of pretraining pockets and construct a large-scale dataset, which data distribution is also considered, to make the efficient pre-training possible.\n\nThe results are competitive, especially for zero-shot settings.\n\nAbundant experiments and ablation study support the argument and result of the authors.\n\nWeaknesses: 1. The technical novelty is limited.\n  - The pocket encoder is borrowed from Uni-Mol.\n  - The contrastive loss is the vanilla form of classical contrastive learning.\n\n2. The bound of Theorem 3.1 is trivial. The authors claim that the bound naturally exists for these representations extracted by pretrained molecule models. However, it's a bit counterintuitive, because many models not pretrained on molecule datasets also fulfill this prior. So, can these models be used for this task? **I strongly suggest removing this part from the paper**.\n\n3. Some issues about dataset creation:\n - 3.1. The authors consider the distribution of ligand size and pocket size when designing the dataset. However, molecules possess more properties that can also lead to imbalance. It would be better to, at least, add some discussion about this issue.\n - 3.2. In the second stage of the data construction process, the approach to defining pockets needs further explanation or an ablation study.\n\n4. Experiments: It would be better to add some biological justification or visualization of the results.\n\nFor this paper, one fact is that the technical novelty is below the bar of ICLR. However, I admire the simple but effective model for the right question. It's a struggle for me to make a decision. I will maintain a neutral attitude and make my final decision after the discussion.\n\nQuestions: See weakness.", "labeling_timestamp": "2026-01-11T16:25:05.690940", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors acknowledge that the theorem's condition can be satisfied by many encoders (including those without molecule pretraining) but argue this does not make the result trivial — additional conditions (e.g. achieving low pretraining loss) are required and some encoders fail to meet them, so the authors partially accept the reviewer's point while defending the nontriviality of their bound.", "evidence": "\"As you said, many other encoders can also satisfy this condition. This shows that our method is applicable to many different molecular encoders with or without pretraining as we proved in Table 4.\" ... \"In fact, in the process of our early exploration, we noticed that some encoders cannot achieve a low pretraining loss, indicating the entire conditions of the theorem are nontrivial.\" ... \"Therefore, our theorem is nontrivial and reveals the transfer ability of our contrastive pre-training from the pseudo-ligand domain to the real ligand domain.\"", "combined_author_response": "Dear AC and reviewers,\n\nWe sincerely appreciate your time and efforts in reviewing our work. Based on your suggestions, we have revised our paper. We would like to use this section to reiterate the explanation for some common concerns and summarize the contributions of our paper.\n\nIn our revised paper, we provide a detailed explanation of the motivation and theory behind our method: **the mechanisms of pocket-peptide interactions and pocket-ligand interactions are very similar**. Our proposed **data-centric** pretraining pipeline is unique in its use of peptides to mimic small molecules, a method that is, to our knowledge, **the first of its kind**. We expanded our discussion about its biological insights in **Section 3**. We add a **new Figure 2 to illustrate different types of non-covalent interactions** that are common for both ligand-protein data and intra-protein data. Following this theory, we have designed a pipeline to generate **over 5 million** synthetic ligand-protein pairs, which greatly eases the **data scarcity** problem in the field of protein-ligand interaction learning. We also prove our data quite effective in various downstream tasks, including pocket property prediction, pocket matching, ligand binding affinity prediction, ligand efficiency prediction, and protein-protein interaction tasks. The **last two tasks are newly added** to further support our method (**Appendix C5 and C6**). We are confident that our **ready-to-release synthetic dataset** will significantly benefit the AI for drug discovery community. Its impact extends beyond enhancing pocket representations in downstream analyses, as it also equips researchers with a valuable resource for a range of applications, such as protein-ligand docking, structure-based drug design, and virtual screening. Importantly, these areas often face challenges due to **a lack of training data necessary for learning effective binding patterns between protein pockets and ligands**. \n\nIn our revised paper, we recognize the differences between real ligands and pseudo ligands (peptides). We address these discrepancies and present a distribution plot of various properties in **Appendix G**. Our approach primarily leverages the **analogous nature of ligand-pocket and peptide-pocket interactions**. Therefore, despite these differences, our model can **still extract valuable interaction information from the pretraining dataset**, which enhances its performance in downstream tasks. This is illustrated in **Figure 4(a)**, showing the benefits of interaction-aware pretraining. We have also undertaken measures like distribution alignment and freezing the molecular encoder to mitigate the impact of these discrepancies. The positive results of these efforts, as evidenced in **Table 5**, confirm their effectiveness and necessity. Notably, our pretrained model can be **further finetuned with real labeled ligand-pocket pairwise data**, as we show in the Ligand Binding Affinity task in **section 4.3**.\n\nAs for other questions, we have added more ablation studies on our design choices, including the impact of a fixed molecular encoder (Table 5), and different threshold values to define pockets(Appendix C.4). \n\nWe are immensely grateful for the invaluable feedback from all the reviewers, which has guided us in refining and clarifying our work. We hope that the revised version of our paper, coupled with the discussion period, will more clearly highlight the novelty, effectiveness, and contributions of our approach to the field.\n\n---\n\nThank you for your invaluable feedback on our paper. In response to your insightful suggestions, we have made several modifications in our revised paper. \n\nWe have cited additional papers in Section 2.2 following your advice. The description of loss terms in Section 3.2 has been corrected.  COSP was not included as a baseline for the Ligand Binding Affinity (LBA)  task because it is not open-sourced, and its performance on LBA was not evaluated in their published work. We used it as a baseline in the pocket-matching task.\n\nIn Section 3, we've expanded our discussion to answer your question on **ProFSA's effectiveness without distributional alignment**. This section now offers a detailed explanation of the foundational concepts and justifications for our method. We draw attention to the parallels between ligand-protein and intra-protein non-covalent interactions, as shown in **Figure 2**. This comparison supports our strategy of using peptides as stand-ins for actual ligands to mimic pocket-ligand interactions. Consequently, **even in the absence of distributional alignment, the types of interactions remain comparably relevant**, which explains why ProFSA continues to function, albeit with reduced effectiveness, as demonstrated in Table 5. We also want to clarify that distributional alignment is only applied to fragment-pocket complex as a whole, and **it would not change the definition of the pocket with a given ligand**. Without this alignment, these fragments are chosen uniformly, with lengths varying from 1 to 7 residues, and pockets are consistently identified based on residues within a 6Å radius of these peptide fragments.\n\nWe are grateful for the time and effort you have dedicated to reviewing our work. Your thorough and constructive feedback has significantly contributed to the refinement of our research. Please let us know if there are any other aspects of our paper that you would like us to address or clarify. We look forward to your further feedback.\n\n---\n\nThank you for your insightful feedback. In response to your concerns regarding the scope of our tasks, we have expanded our research in the revised paper. We now include additional tasks such as **ligand efficiency prediction** and **protein-protein affinity prediction**, detailed in **Appendix C5 and C6**. These tasks were incorporated to enhance the robustness of our method. We acknowledge that they are currently in the appendix due to space constraints, but we are open to relocating them to the main text if it would be beneficial.\n\nRegarding your concern about the lack of recent baselines, we provided a detailed explanation in our previous response. We want to reiterate that our benchmarks do include the latest baselines, with some as recent as 2023.\n\nWe greatly appreciate the time and effort you have invested in reviewing our work. Your feedback has been invaluable in refining our research. Please let us know if there are any other aspects of our paper that you would like us to address or clarify. We look forward to your further feedback.\n\n---\n\nIn our revised paper, we have incorporated changes following feedback from another reviewer, which we believe also addresses your concerns and queries.\n\nRegarding your question on the evidence supporting our approach, particularly the use of peptides to simulate small molecules, we've enriched **Section 3.1**. A new paragraph there now provides an in-depth explanation of non-covalent interactions. To aid understanding, **Figure 2** has been updated with visualizations that include both real and pseudo receptor-ligand pairs, illustrating three types of interactions. Furthermore, we delve into the details of **how intra-protein interactions mirror protein-ligand interactions**, involving specific types of amino acids.\n\nTo better evaluate our design choices, the ablation studies have been extended. Alongside the existing studies in **Table 4** (different pretrained molecular encoders), **Table 5** (distribution alignment), and **Figure 5** (data scale). we've added a study on the impact of using a fixed molecular encoder in **Table 5** in the main text to address your question on the fixed molecular encoder. We've also included an ablation study on varying **cutoff values for pocket definition**, placed in the **Appendix C.4** due to its more focused scope. We can move it to the main part of the paper in the future if needed.\n\nWe acknowledge your concern about the **differences between true and pseudo ligands**, so we've expanded our discussion on the distinct properties of our curated dataset versus the PDBBind dataset in **Figure 9** and **Section 3.2**. The ablation study in **Table 5** demonstrates our methods and their effectiveness in minimizing these discrepancies. Notably, the performance significantly drops without our distribution alignment or fixed molecular encoder, highlighting our efforts to mitigate these issues. The updated **Figure 2** further supports this, showing that while the properties of true and pseudo ligands may differ, **the interaction types are consistent.** We hope these results can ease your concerns about our synthetic dataset, and they also respond to your query about **why we kept the molecular encoder constant during pocket pretraining.**\n\nWe appreciate the constructive critiques and thoughtful engagement from you. Your feedback has undeniably contributed to the refinement and strengthening of our paper. We sincerely value the time and expertise invested in the review process, and we look forward to any further suggestions you may have.\n\n---\n\nWe sincerely appreciate the thoughtful feedback provided on our paper and have diligently incorporated your suggestions into this revised version. In response to your guidance, we have made three significant modifications to the core content of our paper, all of which aim to enhance clarity and address the concerns raised.\n\nTaking into account your suggestion, we have removed **Theorem 3.1** from the main body of our paper. Additionally, we've improved the explanation and included an ablation study on our method of defining pockets. This ablation study, focusing on the cutoff values and primarily a hyperparameter issue, is placed in the Appendix due to its more limited scope of insight. We can move it to the main body of the paper in the future if needed. We have also expanded our discussion on the differences between our dataset and the PDBBind dataset, elucidating how fixed molecule encoders contribute to mitigating this discrepancy.\n\nIn **Section 3.1**, we have introduced a new paragraph dedicated to providing readers with a deeper understanding of non-covalent interactions. To facilitate comprehension, we have included visualizations featuring three types of interactions, incorporating both real and pseudo receptor-ligand pairs in the new **Figure 2**. Furthermore, we delve into the details of how intra-protein interactions mirror protein-ligand interactions, involving specific types of amino acids.\n\nThe biological justifications for the efficacy of interaction-aware pretraining are now more extensively explored in our revised paper. Specifically, we present a compelling showcase featuring estradiol-binding proteins, offering insights into the geometric disparities between the compared proteins and highlighting the similarities in their binding interfaces in the new **Figure 4a**. The existing visualization of BioLip pocket representations in the new **Figure 4b** can make our claims more convincing. Together, these visual aids illustrate how our interaction-aware pretraining empowers models to focus on crucial interface residues, discern subtle distinctions, and disregard irrelevant geometric dissimilarities.\n\nWe have also included an in-depth discussion of pretrained molecule encoders in the newly added **Section 2.3**, further enriching the theoretical foundation of our work.\nAll changes can be found in the renewed paper, which is updated lively.\n\nWe genuinely value the constructive input you provided during the review process, as it has undoubtedly contributed to the refinement of our paper. Your insights have proven invaluable in elevating the clarity and depth of our contributions to the field. We express our gratitude for the time and expertise invested in reviewing our work, and we look forward to any additional feedback or suggestions you may have.\n\n---\n\n# Response 2/2\n\n## Response to the question on N Terminal and C Terminal\n\nWhen a peptide bond is formed (green), one amine group and one carboxy group are left untouched. Therefore, in a linear polypeptide, there would always be a free amine group and a free carboxy group, and they are called the N terminal(blue) and the C terminal(red) respectively in biochemistry. You can find the visualization in **Appendix H**.\n\n## Response to fixing the molecule encoder in contrastive learning\n\nFrom an **effectiveness** standpoint, as you mentioned before, there are unavoidable discrepancies between pseudo and true ligands. If we didn't fix our molecule encoder during pretraining, then the conditions of Theorem 3.1 might not be met. The ablation results presented in the previous response clearly indicate a performance decline when the molecular encoder is not fixed during contrastive learning, particularly in the context of pocket matching. This supports our assertion that the pocket encoder struggles to acquire chemical knowledge without the stability provided by a fixed molecular encoder.\n\nFrom an **efficiency** standpoint, a fixed molecule encoder also offers significant advantages. It reduces memory usage and accelerates training speed because all molecule embeddings can be pre-computed before training. This eliminates the need for feed-forward and back-propagation operations of the molecule encoder during training, leading to efficiency gains. Additionally, adapting to different molecule encoders becomes more straightforward when they are not co-trained during the pretraining process. This approach allows for more flexibility and ease in integrating various encoding methodologies **even if the molecule encoder is not accessible for training**. That is to say, our approach can even be used with undifferentiable molecular representations.\n\n## Response to extending to other tasks like protein-protein interaction prediction\n\nThank you for pointing out the potential application of our framework to protein-protein interaction (PPI) tasks. We have to point out that the interactions between proteins and proteins are far different from proteins and molecules. Specifically, the interaction areas are usually much larger, but shallower, and involved functional groups are less diverse due to limited types of natural amino acids. Therefore, several changes should be made to extend our ProFSA method to PPI tasks:\n\n1. As for data creation, unlike the localized and specific nature of pocket-ligand interactions, protein-protein interactions are **more global and involve broader protein features**. This requires a shift in the data creation process to split entire protein structures into domains instead of short fragments and to capture potential interactions between them. \n\n2. The interaction interface between protein and protein is much larger. We might need to change from atomic-level to residue-level representations to efficiently capture interaction information.\n\n3. For contrastive learning training, ProFSA uses a fixed pretrained molecular encoder to cope with various small molecules and their atomic representation. However, for protein-protein interaction, it is not necessary as we could only model 20 types of amino acids at the residue level.\n\nDespite several changes that need to be done for PPI tasks, we find that we could divide protein complexes into local interactions. When the complex structures and monomer structures are provided, we have achieved a reasonable result on a flexible PPI affinity prediction benchmark with our current model in a zero-shot way. \n\n| Method            | Spearman $\\uparrow$  |\n|-------------------|----------------------|\n| SchNet            | 0.072 ± 0.021        |\n| DimeNet++         | 0.171 ± 0.054        |\n| EGNN              | 0.080 ± 0.038        |\n| TorchMD           | 0.117 ± 0.008        |\n| GET               | **0.363 ± 0.017**    |\n| ProFSA(zero-shot) | _0.248_              |\n\nSurprisingly, as a **zero-shot** method, ProFSA is able to outperform other **supervised learning** models except for GET, a newly proposed unified model. This demonstrates our approach is also able to capture protein-protein interaction information despite it being designed for protein-ligand interaction. \n\nA detailed explanation of this experiment is shown in **Appendix C6**.\n\n---\n\n# Response 1/2\n\n## Response to evidence for the construction of the pseudo-ligand and using peptides to replace small molecules \n\nAs we discussed in the paper, pseudo-ligands share similar sizes as real ligands, and they also make similar non-covalent interactions with the pocket. To further support our proposal, we would like to cite another paper **\"A defined structural unit enables de novo design of small-molecule–binding proteins\"**[1], published in Science 2020, which supports that **intra-protein interactions are similar to protein-ligand interactions.** That is the reason we use peptides to **represent** small molecules to let the pocket encoder learn the interaction information. \n\nAlso, we'd like to clarify that our intention is not to **\"replace\"** small molecules with peptides. It's a pretraining framework that leverages abundant protein-only data and uses peptides to **simulate** pocket-small molecule interactions for enhanced pocket representations. Following this pretraining phase, **the model can be finetuned with datasets that include real small molecules, ensuring its effectiveness in practical applications**.\n\nTo demonstrate the validity of constructing pseudo-ligands, we provide visualizations in **Appendix F** showing that interactions between pockets and our pseudo-ligands are similar to those with true ligands. These figures reveal shared interaction types like **hydrogen bonding**, **$\\pi-\\pi$ stacking**, and **salt bridge**, depicted with color-coded dashed lines for each interaction type. This should help clarify the rationale and evidence supporting our approach.\n\n[1] Polizzi, Nicholas F., and William F. DeGrado. \"A defined structural unit enables de novo design of small-molecule–binding proteins.\" Science 369, no. 6508 (2020)\n\n## Response to ablation studies evaluating the impact of critical design choices\n\nThanks for your advice on adding more ablation studies on design choices. We did ablation studies on the effectiveness of distributional alignment, where the major difference is that fragment sizes are modified. We found that aligned fragment sizes with real ligands could provide the best performance. The result is shown in **Table 5**.\n\nAs for the distance thresholds, we define the pocket following the UniMol setup. We also found our model can also adapt to an 8Å setup in the ToughM1 experiment. Following your advice, we did an ablation study on the distance thresholds. We tested our method with three different thresholds: 4Å, 6Å, and 8Å. The result is shown below:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| 4Å | 0.7062 | 0.7549 | 0.1240 | 0.1095 | 28.29 | 13.07 |\n| 6Å | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| 8Å | 0.8322 | 0.8292 | 0.1256 | 0.1125 | 34.83 | 12.92 |\n\nSince the 8Å threshold corresponds with the pocket definition in the Kahraman and Tough M1 datasets, it leads to optimal results. Our decision to use a 6Å threshold was made to align with the methodology of pretraining data creation by Uni-Mol, facilitating a fair comparison. Notably, even with the 6Å threshold, we achieved strong results in the pocket-matching task, which serves as a testament to the effectiveness of our approach.\n\n## Response to substantial discrepancies between pseudo and real ligands\n\nThank you for pointing out the discrepancies between pseudo and real ligands. We understand and acknowledge your concern. To mitigate the impact of this discrepancy, we have made several efforts. First, we performed **distribution alignment** to make the data distribution of pseudo-ligands more similar to that of true ligands in PDBbind. Additionally, we **fixed the molecule encoder during pretraining** to prevent it from being misled by the discrepancy. Our ablation studies have shown that these strategies are effective:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nThe table above shows that lacking distribution alignment and not fixing the molecular encoder during pretraining both lead to lower performance, underscoring the effectiveness of our methods in reducing discrepancies.\n\nTheorem 3.1 theoretically supports our approach's efficacy, even with discrepancies between pseudo and real ligands. Empirically, our method outperforms other pretraining techniques, confirming its effectiveness despite these discrepancies.\n\n---\n\n## Response to limited tasks and potential in protein-ligand binding pose prediction\n\nIn our paper, the primary objective is to assess the effectiveness of our pretraining method and the quality of the trained pocket representations. To achieve this, we focused on two types of downstream tasks: pocket-only tasks (pocket property prediction and pocket matching) and pocket-ligand interaction tasks (ligand binding affinity prediction).\n\nWe appreciate the suggestion to apply our method to the protein-ligand binding pose prediction task. However, it's important to note that our current evaluation framework is designed to specifically assess **pocket representations**. For the tasks we chose, the architecture is straightforward: either a **zero-shot** evaluation or a **simple MLP** for mapping embeddings to predictions. In contrast, protein-ligand binding pose prediction often involves **complex methodologies**. For instance, state-of-the-art methods like DiffDock[1] require training a diffusion generative model and a separate confidence model, while other approaches like EDM-Dock[2], rely on reconstructing ligand conformations from predicted distance maps. These methods are not end-to-end and do not directly align with our objective of evaluating pretrained pocket representations.\n\nNevertheless, we recognize the potential of our method in enhancing existing binding pose prediction techniques. To integrate our approach, we would need to modify our framework. Using the same data creation strategy, we could train a binding pose prediction model with our preprocessed data, which could then be further fine-tuned using real pocket-ligand pair data from sources like PDBbind. Additionally, our data could be used to train a side-chain packing model, allowing for side-chain flexibility during docking. Thank you again for your advice, and we will leave protein-ligand binding pose prediction as a separate future work for our method.\n\nIn response to the need for evaluating our method on a broader range of tasks, we have extended our analysis to include two additional downstream tasks: LEP (Ligand Efficacy Prediction), and PPA (Protein-Protein Affinity Prediction).\n\nresult for LEP:\n\n| Method      | AUROC $\\uparrow$ | AUPRC $\\uparrow$ |\n|-------------|--------------|--------------|\n| ATOM3D-GNN  | 0.681        | 0.598        |\n| GeoSSL      | 0.776±0.03   | 0.694±0.06   |\n| Uni-Mol     | 0.782±0.02   | 0.695±0.07   |\n| ProFSA      | 0.840±0.04   | 0.806±0.04   |\n\nGeoSSL and Uni-Mol, both pretraining methods, yield comparable results. However, ProFSA outperforms these methods, demonstrating the advantage of our pocket pretraining approach.\n\nresult for PPA:\n\n| Method            | Spearman $\\uparrow$  |\n|-------------------|----------------------|\n| SchNet            | 0.072 ± 0.021        |\n| DimeNet++         | 0.171 ± 0.054        |\n| EGNN              | 0.080 ± 0.038        |\n| TorchMD           | 0.117 ± 0.008        |\n| GET               | **0.363 ± 0.017**    |\n| ProFSA(zero shot) | _0.248_              |\n\nAs a **zero-shot** method, ProFSA is able to outperform other **supervised learning** models except for GET, a newly proposed unified model. This demonstrates our approach is able to capture protein-protein interaction information despite it being designed for protein-ligand interaction modeling.\n\nYou can find detailed experiment settings and results in **Appendix C5 and C6**.\n\n[1] Corso et al., \"Diffdock: Diffusion steps, twists, and turns for molecular docking.\", ICLR 2023.\n[2] Masters et al., \"Deep learning model for efficient protein–ligand docking with implicit side-chain flexibility.\" Journal of Chemical Information and Modeling 63, no. 6 (2023).\n\n## Response to the issue of lack of latest baselines\n\nFor the pocket matching task, CoSP is a newly proposed baseline which is published in ECML PKDD 2023. Alongside CoSP and Uni-Mol, we selected some of the most effective baseline results presented in the CoSP paper. We also tried to evaluate recent methods like PocketAnchor (Li et al., Cell Systems 2022) on our own since they are not tested on the pocket matching task. However, since the result didn't outperform other baseline machine learning methods(Uni-Mol, CoSP, and DeeplyTough), we decided not to include it in our final baseline comparison. We can include it in the camera-ready version if needed.\n\nFor the ligand binding affinity task, a lot of newly proposed baselines are included, i.e. ProNet (Wang et al., 2022b); as well as pretraining methods such as GeoSSL (Liu et al., 2023), EGNN-PLM (Wu et al., 2022), DeepAffinity (Karimi et al., 2019) and Uni-Mol (Zhou et al., 2023).\n\n---\n\n## Response to lack of related works\n\nThanks for providing us with more insightful related works that could support our arguments. We actually have cited these three papers in the section on ligand binding affinity experiments, but we are also happy to add these in Related Works section 2.2. You can find the change in pdfdiff.\n\n## Response to lack of COSP as a baseline in Table 3\n\nBecause best to our knowledge, they did not release their code and they did not test their method on the ligand binding affinity dataset. \n\n## Response to the question on the loss terms\n\nThank you for your advice. We apologize that we accidentally mentioned loss1 and loss2 in reverse, and we are sorry for the confusing statement. You are correct that the first loss is to identify the true protein pocket when given a pseudo-ligand. We revise the original statement to: \"The primary purpose of the first loss is to identify the true protein pocket from a batch of samples when given a pseudo-ligand. Similarly, the second loss seeks to identify the corresponding ligand fragment for a given pocket.\" You can also find the change in pdfdiff.\n\n## Response to the question on the length of the pocket representation without alignment\n\nWith or without distributional alignment, pockets are always defined by the given protein fragment with a fixed distance cutoff (6Å in our works, following the UniMol setup). The distributional alignment process merely samples these pairs to match the sizes of real ligands and pockets. Without distributional alignment, fragments are uniformly sampled from 1 to 7 residues. Pockets are similarly determined as with residues within the range of 6Å around peptide fragments.\n\n---\n\n# Question about the confidence score\n\nThanks for your review. We found your understanding and judgment of our paper comprehensive and precise. Also, you provided detailed and thoughtful advice for our paper, which helped a lot. We sincerely appreciate your review, but we also wonder if there is any misunderstanding here, as you only give a confidence score of 1.\n\n---\n\n# Response 3/3\n\n## Response to biological justification and visualization of the results\n\nOur method's **biological justification** for achieving good experimental results is based on the properties and interactions of pseudo-ligands. As discussed in our paper, these pseudo-ligands are similar in size to real ligands and engage in comparable non-covalent interactions with protein pockets. This fundamental similarity is crucial for the effectiveness of our approach.\n\nMoreover, our methodology is further supported by the findings in the 2020 Science paper titled \"A defined structural unit enables de novo design of small-molecule–binding proteins.\"[1] This research underscores that **intra-protein interactions are analogous to protein-ligand interactions**, which validates our use of peptides as proxies for small molecules. By employing peptides as pseudo-ligands in this manner, our pocket encoder is able to learn and replicate the interaction dynamics typically observed in real ligand scenarios. This understanding is pivotal to the success of our method in downstream applications.\n\nTo better justify our methods and results, following your advice, we also provide some visualizations in **Appendix F**. **Figure 6** illustrates that the pseudo pairs we created share various interaction types commonly found in real pocket-ligand pairs, such as **hydrogen bonding, $\\pi-\\pi$ stacking, and salt bridge**. In the figures, each type of interaction is represented by dashed lines, color-coded to correspond with the specific interaction type.\n\nAnother visualization of our pocket-matching result is shown in **Figure 7**. We showed an example of two non-homology ATP binding pockets to explain why pocket-matching can benefit from interaction-aware pretraining so that we have achieved superior results in the Kahraman dataset and the BioLip t-SNE visualization. The PEP carboxykinase(PDB:1AYL) and tRNA synthetase(PDB:1B8A) are two ATP-binding proteins that share zero sequence similarity (verified with the BLAST) extracted from the Kahraman dataset. However, as they are both fueled by ATP, their binding site shares similar binding patterns. The cation-$\\pi$ interactions(blue dash lines) and salt bridges (magenta dash lines) are important to the ATP binding, which can be viewed as convergent evolutions at the molecule level. Even though, these two pockets are very distinct in terms of shapes and sizes because they bind ATPs in different conformations. Therefore, biochemical interactions are the key to accomplishing the pocket-matching task, which is ignored in previous self-supervised learning methods like Uni-Mol.\n\n[1] Polizzi, Nicholas F., and William F. DeGrado. \"A defined structural unit enables de novo design of small-molecule–binding proteins.\" Science 369, no. 6508 (2020)\n\n---\n\n# Response 2/3\n\n## Response to the bound of Theorem 3.1 \n\nWe are not very clear about the point of your question. We have two understandings and they are replied as follows.\n\nIf you mean $ ||t − t ^{(0)}|| < 1/2 l_T$ is trivial, please note that it is only a condition of the theorem. As you said, many other encoders can also satisfy this condition. This shows that our method is applicable to many different molecular encoders with or without pretraining as we proved in **Table 4**. Moreover, the condition is only a necessary condition. We also need the pretraining contrastive loss to be sufficiently small to guarantee a small contrastive loss between pockets and real ligands. In fact, in the process of our early exploration, we noticed that some encoders cannot achieve a low pretraining loss, indicating the entire conditions of the theorem are nontrivial.\n\nIf you mean that the conclusion of theorem 3.1 is trivial, we want to emphasize that our conclusion is $\\lim_{L_i(t, s) =0}L_i(t^{(0)}, s) =0$. (We write it in a $\\epsilon$-$\\delta$ language in paper. They are equivalent forms.) Our conclusion shows the loss containing real ligands is consistent with the loss containing pseudo ligands that we optimized in pre-training. \n\nIn contrast,  a trivial result from $||t − t ^{(0)}|| < 1/ 2l_T$ is that $\\lim_{L_i(t, s) =0}L_i(t^{(0)}, s) <B$, B is a bound related to l_t and the representations. It does not guarantee a consistent loss and is substantially different from our result.\nTherefore, our theorem is nontrivial and reveals the transfer ability of our contrastive pre-training from the pseudo-ligand domain to the real ligand domain.\n\nAs for this theoretical result, we are open to further discussion.\n\n## Response to molecules possess more properties that can also lead to imbalance\n\nIt is correct that molecules have more properties than their size, like logP, Hbond donor and acceptor number, and rotatory bond number. As small molecule drugs are mostly designed to penetrate barriers like gut or cell membranes, they usually have much larger logP values, which means more hydrophobic. For the same reason, they usually have fewer hydrogen bond donors and acceptors, and only minimal essential ones are kept for specified interactions. Also, to minimize the entropy effect upon binding and to increase binding affinity, rotatory bonds are also unfavored. However, it is impossible to mimic such features with peptides, as the backbone of peptides is intrinsically hydrophilic and flexible. As we showed in a new figure, our pseudo-ligands are less similar to real ligands in those properties even with size alignment. You can find the figure in the new pdf in **Appendix G**.\n\nWe believe that the aforementioned divergence is the main limitation of poor zero-shot performance in predicting hydrophobicity scores (**Table 1**). However, as demonstrated in previous publications like CoSP, we could leverage real ligand-pocket pairs from PDBBind or BioLip database to further finetune our network, as an extension of the pipeline. In this way, we could handle property mismatches but still enjoy the power of our large-scale pretraining.\n\n## Response to the approach to defining pockets \n\nWe define the pocket following the UniMol setup. We found our model can also adapt to an 8Å setup in the toughM1 experiment \n\nIn particular, we define the pocket for each protein-ligand pair as residues of the protein that have at least one atom within the range of 6Å from a heavy atom in the ligand. To further explain our design, we have done an ablation study on different choices of thresholds of 4Å, 6Å, and 8Å:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| 4Å | 0.7062 | 0.7549 | 0.1240 | 0.1095 | 28.29 | 13.07 |\n| 6Å | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| 8Å | 0.8322 | 0.8292 | 0.1256 | 0.1125 | 34.83 | 12.92 |\n\nSince the 8Å threshold corresponds with the pocket definition in the Kahraman and Tough M1 datasets, it leads to optimal results. Our decision to use a 6Å threshold was made to align with the methodology of pretraining data creation by Uni-Mol, facilitating a fair comparison. Notably, even with the 6Å threshold, we achieved strong results in the pocket-matching task, which serves as a testament to the effectiveness of our approach.\n\n---\n\n# Response 1/3\nWe appreciate the time and effort you have dedicated to reviewing our paper, and we are grateful for your constructive feedback and thoughtful evaluation of our work. We would like to address your comments about the technical novelty of our work and provide additional clarification on certain aspects of our paper.\n\n## Response to technical novelty\n\nFirstly, we acknowledge your observation that we didn't use any fancy models compared with typical **model-centric** works. While we respect your assessment, we would like to highlight that our primary focus in this paper is a **data-centric** pretraining method to introduce groundbreaking improvements in the field of protein pocket pretraining. Our work aims to address a critical challenge of data scarcity in the field of protein pocket representation by constructing large-scale synthetic data that facilitates the pretraining of models, ultimately enhancing the accuracy and robustness of protein pocket representations. We believe that the research community can conduct fast following-ups on our **released dataset** with more sophisticated models, and even extend our pipelines to other tasks like docking or drug design. We also would like to point out that we have provided a novel method to **distill knowledge from well-trained molecule models** to protein models. Though employing the same contrastive loss, our approach is different from existing contrastive learning models since it uses a fixed molecular encoder. The motivation here is to use a well-trained molecular encoder on a relatively larger dataset to **guide** the training of the protein encoder. For example, some quantum-chemistry properties that are difficult to compute for large systems like proteins could be distilled from molecules, by using some quantum-chemistry-aware molecular encoders such as Frad. Though our framework seems to be simple, it is non-trivial to make it work. We have made several efforts to solve the unavoidable discrepancies between true ligands and pseudo ligands. The ablation study shows that our efforts are effective and necessary:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nRegarding your positive acknowledgment of our \"simple but effective model for the right question,\" we are pleased to hear that our approach resonates with the objective we set out to achieve. We designed our model with **simplicity** in mind, prioritizing effectiveness and practical utility for the specific problem domain. This deliberate choice aligns with the notion that sometimes the most impactful solutions are elegantly straightforward. Also, we intentionally borrowed a pocket encoder from Uni-Mol to make a **fair comparison** with it, which strongly supports the power of our dataset. Similar to the molecule encoder, our pocket encoder can be changed to any other model due to our flexible framework. Notably, **we didn't load the pretrained weights of Uni-Mol pocket encoder**. We only use the same backbone architecture and the pretraining was completely done on our processed data with our training strategy. \n\nNotably, many simple but effective approaches have been recognized by top-tier conferences and journals. A prime example of this is in the field of protein language models, like the ESM series and ProtTrans. These studies adapted the Transformer architecture and masked language modeling techniques from natural language processing to protein sequences. While they didn't introduce groundbreaking techniques, their substantial contributions to protein modeling are evident, with publications in prestigious conferences and journals like NeurIPS, ICML, TPAMI, PNAS, and Science. Another example is the widely acclaimed CLIP paper presented at ICML 2021. CLIP, while not employing novel techniques, stands as a hallmark of data-centric deep learning. Its use of contrastive learning enabled training on expansive web-sourced text-image datasets, moving beyond the constraints of meticulously curated databases like MS-COCO. This aligns with our approach to addressing **data scarcity challenges** by facilitating training on large-scale datasets, mirroring our strategy for overcoming similar hurdles in the protein pocket modeling domain.\n\nWe appreciate your efforts to remain neutral and understand the challenges in decision-making. We believe further discussion will highlight our paper's contributions. Your feedback and insights in the upcoming discussion will be invaluable for refining our work.", "author_response": "# Response 1/3\nWe appreciate the time and effort you have dedicated to reviewing our paper, and we are grateful for your constructive feedback and thoughtful evaluation of our work. We would like to address your comments about the technical novelty of our work and provide additional clarification on certain aspects of our paper.\n\n## Response to technical novelty\n\nFirstly, we acknowledge your observation that we didn't use any fancy models compared with typical **model-centric** works. While we respect your assessment, we would like to highlight that our primary focus in this paper is a **data-centric** pretraining method to introduce groundbreaking improvements in the field of protein pocket pretraining. Our work aims to address a critical challenge of data scarcity in the field of protein pocket representation by constructing large-scale synthetic data that facilitates the pretraining of models, ultimately enhancing the accuracy and robustness of protein pocket representations. We believe that the research community can conduct fast following-ups on our **released dataset** with more sophisticated models, and even extend our pipelines to other tasks like docking or drug design. We also would like to point out that we have provided a novel method to **distill knowledge from well-trained molecule models** to protein models. Though employing the same contrastive loss, our approach is different from existing contrastive learning models since it uses a fixed molecular encoder. The motivation here is to use a well-trained molecular encoder on a relatively larger dataset to **guide** the training of the protein encoder. For example, some quantum-chemistry properties that are difficult to compute for large systems like proteins could be distilled from molecules, by using some quantum-chemistry-aware molecular encoders such as Frad. Though our framework seems to be simple, it is non-trivial to make it work. We have made several efforts to solve the unavoidable discrepancies between true ligands and pseudo ligands. The ablation study shows that our efforts are effective and necessary:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nRegarding your positive acknowledgment of our \"simple but effective model for the right question,\" we are pleased to hear that our approach resonates with the objective we set out to achieve. We designed our model with **simplicity** in mind, prioritizing effectiveness and practical utility for the specific problem domain. This deliberate choice aligns with the notion that sometimes the most impactful solutions are elegantly straightforward. Also, we intentionally borrowed a pocket encoder from Uni-Mol to make a **fair comparison** with it, which strongly supports the power of our dataset. Similar to the molecule encoder, our pocket encoder can be changed to any other model due to our flexible framework. Notably, **we didn't load the pretrained weights of Uni-Mol pocket encoder**. We only use the same backbone architecture and the pretraining was completely done on our processed data with our training strategy. \n\nNotably, many simple but effective approaches have been recognized by top-tier conferences and journals. A prime example of this is in the field of protein language models, like the ESM series and ProtTrans. These studies adapted the Transformer architecture and masked language modeling techniques from natural language processing to protein sequences. While they didn't introduce groundbreaking techniques, their substantial contributions to protein modeling are evident, with publications in prestigious conferences and journals like NeurIPS, ICML, TPAMI, PNAS, and Science. Another example is the widely acclaimed CLIP paper presented at ICML 2021. CLIP, while not employing novel techniques, stands as a hallmark of data-centric deep learning. Its use of contrastive learning enabled training on expansive web-sourced text-image datasets, moving beyond the constraints of meticulously curated databases like MS-COCO. This aligns with our approach to addressing **data scarcity challenges** by facilitating training on large-scale datasets, mirroring our strategy for overcoming similar hurdles in the protein pocket modeling domain.\n\nWe appreciate your efforts to remain neutral and understand the challenges in decision-making. We believe further discussion will highlight our paper's contributions. Your feedback and insights in the upcoming discussion will be invaluable for refining our work."}
{"claim": "The reviewer strongly suggests removing Theorem 3.1 because its bound is trivial and not informative for the paper's contributions.", "claim_type": "novelty", "paper_id": "uMAujpVi9m", "paper_title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "9ap4s1SZlZ", "reviewer": "Reviewer_yfM3", "review_text": "Summary: This paper primarily aims to enhance the pocket pretraining method, as existing approaches only consider pockets during pretraining. There are two main contributions in this paper: (1) The authors introduce a novel method, ProFSA, for pocket pretraining, which extracts additional information from corresponding ligands. However, the number of pocket-ligand complex structures is quite limited in existing datasets. (2) To address this issue, the authors generate over 5 million complexes by segmenting fragments and their corresponding pockets in protein structures. By aligning features of fragments and pockets, the pocket encoder learns the interaction between fragments and pockets. The authors design downstream tasks such as pocket druggability prediction, pocket matching, and ligand binding affinity prediction to demonstrate the effectiveness of ProFSA.\n\nStrengths: The authors propose a new perspective of pretraining pockets and construct a large-scale dataset, which data distribution is also considered, to make the efficient pre-training possible.\n\nThe results are competitive, especially for zero-shot settings.\n\nAbundant experiments and ablation study support the argument and result of the authors.\n\nWeaknesses: 1. The technical novelty is limited.\n  - The pocket encoder is borrowed from Uni-Mol.\n  - The contrastive loss is the vanilla form of classical contrastive learning.\n\n2. The bound of Theorem 3.1 is trivial. The authors claim that the bound naturally exists for these representations extracted by pretrained molecule models. However, it's a bit counterintuitive, because many models not pretrained on molecule datasets also fulfill this prior. So, can these models be used for this task? **I strongly suggest removing this part from the paper**.\n\n3. Some issues about dataset creation:\n - 3.1. The authors consider the distribution of ligand size and pocket size when designing the dataset. However, molecules possess more properties that can also lead to imbalance. It would be better to, at least, add some discussion about this issue.\n - 3.2. In the second stage of the data construction process, the approach to defining pockets needs further explanation or an ablation study.\n\n4. Experiments: It would be better to add some biological justification or visualization of the results.\n\nFor this paper, one fact is that the technical novelty is below the bar of ICLR. However, I admire the simple but effective model for the right question. It's a struggle for me to make a decision. I will maintain a neutral attitude and make my final decision after the discussion.\n\nQuestions: See weakness.", "labeling_timestamp": "2026-01-11T16:25:05.711841", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors explicitly state they removed Theorem 3.1 from the main body in response to the reviewer’s suggestion, indicating agreement with the reviewer's request.", "evidence": "Taking into account your suggestion, we have removed **Theorem 3.1** from the main body of our paper.", "combined_author_response": "Dear AC and reviewers,\n\nWe sincerely appreciate your time and efforts in reviewing our work. Based on your suggestions, we have revised our paper. We would like to use this section to reiterate the explanation for some common concerns and summarize the contributions of our paper.\n\nIn our revised paper, we provide a detailed explanation of the motivation and theory behind our method: **the mechanisms of pocket-peptide interactions and pocket-ligand interactions are very similar**. Our proposed **data-centric** pretraining pipeline is unique in its use of peptides to mimic small molecules, a method that is, to our knowledge, **the first of its kind**. We expanded our discussion about its biological insights in **Section 3**. We add a **new Figure 2 to illustrate different types of non-covalent interactions** that are common for both ligand-protein data and intra-protein data. Following this theory, we have designed a pipeline to generate **over 5 million** synthetic ligand-protein pairs, which greatly eases the **data scarcity** problem in the field of protein-ligand interaction learning. We also prove our data quite effective in various downstream tasks, including pocket property prediction, pocket matching, ligand binding affinity prediction, ligand efficiency prediction, and protein-protein interaction tasks. The **last two tasks are newly added** to further support our method (**Appendix C5 and C6**). We are confident that our **ready-to-release synthetic dataset** will significantly benefit the AI for drug discovery community. Its impact extends beyond enhancing pocket representations in downstream analyses, as it also equips researchers with a valuable resource for a range of applications, such as protein-ligand docking, structure-based drug design, and virtual screening. Importantly, these areas often face challenges due to **a lack of training data necessary for learning effective binding patterns between protein pockets and ligands**. \n\nIn our revised paper, we recognize the differences between real ligands and pseudo ligands (peptides). We address these discrepancies and present a distribution plot of various properties in **Appendix G**. Our approach primarily leverages the **analogous nature of ligand-pocket and peptide-pocket interactions**. Therefore, despite these differences, our model can **still extract valuable interaction information from the pretraining dataset**, which enhances its performance in downstream tasks. This is illustrated in **Figure 4(a)**, showing the benefits of interaction-aware pretraining. We have also undertaken measures like distribution alignment and freezing the molecular encoder to mitigate the impact of these discrepancies. The positive results of these efforts, as evidenced in **Table 5**, confirm their effectiveness and necessity. Notably, our pretrained model can be **further finetuned with real labeled ligand-pocket pairwise data**, as we show in the Ligand Binding Affinity task in **section 4.3**.\n\nAs for other questions, we have added more ablation studies on our design choices, including the impact of a fixed molecular encoder (Table 5), and different threshold values to define pockets(Appendix C.4). \n\nWe are immensely grateful for the invaluable feedback from all the reviewers, which has guided us in refining and clarifying our work. We hope that the revised version of our paper, coupled with the discussion period, will more clearly highlight the novelty, effectiveness, and contributions of our approach to the field.\n\n---\n\nThank you for your invaluable feedback on our paper. In response to your insightful suggestions, we have made several modifications in our revised paper. \n\nWe have cited additional papers in Section 2.2 following your advice. The description of loss terms in Section 3.2 has been corrected.  COSP was not included as a baseline for the Ligand Binding Affinity (LBA)  task because it is not open-sourced, and its performance on LBA was not evaluated in their published work. We used it as a baseline in the pocket-matching task.\n\nIn Section 3, we've expanded our discussion to answer your question on **ProFSA's effectiveness without distributional alignment**. This section now offers a detailed explanation of the foundational concepts and justifications for our method. We draw attention to the parallels between ligand-protein and intra-protein non-covalent interactions, as shown in **Figure 2**. This comparison supports our strategy of using peptides as stand-ins for actual ligands to mimic pocket-ligand interactions. Consequently, **even in the absence of distributional alignment, the types of interactions remain comparably relevant**, which explains why ProFSA continues to function, albeit with reduced effectiveness, as demonstrated in Table 5. We also want to clarify that distributional alignment is only applied to fragment-pocket complex as a whole, and **it would not change the definition of the pocket with a given ligand**. Without this alignment, these fragments are chosen uniformly, with lengths varying from 1 to 7 residues, and pockets are consistently identified based on residues within a 6Å radius of these peptide fragments.\n\nWe are grateful for the time and effort you have dedicated to reviewing our work. Your thorough and constructive feedback has significantly contributed to the refinement of our research. Please let us know if there are any other aspects of our paper that you would like us to address or clarify. We look forward to your further feedback.\n\n---\n\nThank you for your insightful feedback. In response to your concerns regarding the scope of our tasks, we have expanded our research in the revised paper. We now include additional tasks such as **ligand efficiency prediction** and **protein-protein affinity prediction**, detailed in **Appendix C5 and C6**. These tasks were incorporated to enhance the robustness of our method. We acknowledge that they are currently in the appendix due to space constraints, but we are open to relocating them to the main text if it would be beneficial.\n\nRegarding your concern about the lack of recent baselines, we provided a detailed explanation in our previous response. We want to reiterate that our benchmarks do include the latest baselines, with some as recent as 2023.\n\nWe greatly appreciate the time and effort you have invested in reviewing our work. Your feedback has been invaluable in refining our research. Please let us know if there are any other aspects of our paper that you would like us to address or clarify. We look forward to your further feedback.\n\n---\n\nIn our revised paper, we have incorporated changes following feedback from another reviewer, which we believe also addresses your concerns and queries.\n\nRegarding your question on the evidence supporting our approach, particularly the use of peptides to simulate small molecules, we've enriched **Section 3.1**. A new paragraph there now provides an in-depth explanation of non-covalent interactions. To aid understanding, **Figure 2** has been updated with visualizations that include both real and pseudo receptor-ligand pairs, illustrating three types of interactions. Furthermore, we delve into the details of **how intra-protein interactions mirror protein-ligand interactions**, involving specific types of amino acids.\n\nTo better evaluate our design choices, the ablation studies have been extended. Alongside the existing studies in **Table 4** (different pretrained molecular encoders), **Table 5** (distribution alignment), and **Figure 5** (data scale). we've added a study on the impact of using a fixed molecular encoder in **Table 5** in the main text to address your question on the fixed molecular encoder. We've also included an ablation study on varying **cutoff values for pocket definition**, placed in the **Appendix C.4** due to its more focused scope. We can move it to the main part of the paper in the future if needed.\n\nWe acknowledge your concern about the **differences between true and pseudo ligands**, so we've expanded our discussion on the distinct properties of our curated dataset versus the PDBBind dataset in **Figure 9** and **Section 3.2**. The ablation study in **Table 5** demonstrates our methods and their effectiveness in minimizing these discrepancies. Notably, the performance significantly drops without our distribution alignment or fixed molecular encoder, highlighting our efforts to mitigate these issues. The updated **Figure 2** further supports this, showing that while the properties of true and pseudo ligands may differ, **the interaction types are consistent.** We hope these results can ease your concerns about our synthetic dataset, and they also respond to your query about **why we kept the molecular encoder constant during pocket pretraining.**\n\nWe appreciate the constructive critiques and thoughtful engagement from you. Your feedback has undeniably contributed to the refinement and strengthening of our paper. We sincerely value the time and expertise invested in the review process, and we look forward to any further suggestions you may have.\n\n---\n\nWe sincerely appreciate the thoughtful feedback provided on our paper and have diligently incorporated your suggestions into this revised version. In response to your guidance, we have made three significant modifications to the core content of our paper, all of which aim to enhance clarity and address the concerns raised.\n\nTaking into account your suggestion, we have removed **Theorem 3.1** from the main body of our paper. Additionally, we've improved the explanation and included an ablation study on our method of defining pockets. This ablation study, focusing on the cutoff values and primarily a hyperparameter issue, is placed in the Appendix due to its more limited scope of insight. We can move it to the main body of the paper in the future if needed. We have also expanded our discussion on the differences between our dataset and the PDBBind dataset, elucidating how fixed molecule encoders contribute to mitigating this discrepancy.\n\nIn **Section 3.1**, we have introduced a new paragraph dedicated to providing readers with a deeper understanding of non-covalent interactions. To facilitate comprehension, we have included visualizations featuring three types of interactions, incorporating both real and pseudo receptor-ligand pairs in the new **Figure 2**. Furthermore, we delve into the details of how intra-protein interactions mirror protein-ligand interactions, involving specific types of amino acids.\n\nThe biological justifications for the efficacy of interaction-aware pretraining are now more extensively explored in our revised paper. Specifically, we present a compelling showcase featuring estradiol-binding proteins, offering insights into the geometric disparities between the compared proteins and highlighting the similarities in their binding interfaces in the new **Figure 4a**. The existing visualization of BioLip pocket representations in the new **Figure 4b** can make our claims more convincing. Together, these visual aids illustrate how our interaction-aware pretraining empowers models to focus on crucial interface residues, discern subtle distinctions, and disregard irrelevant geometric dissimilarities.\n\nWe have also included an in-depth discussion of pretrained molecule encoders in the newly added **Section 2.3**, further enriching the theoretical foundation of our work.\nAll changes can be found in the renewed paper, which is updated lively.\n\nWe genuinely value the constructive input you provided during the review process, as it has undoubtedly contributed to the refinement of our paper. Your insights have proven invaluable in elevating the clarity and depth of our contributions to the field. We express our gratitude for the time and expertise invested in reviewing our work, and we look forward to any additional feedback or suggestions you may have.\n\n---\n\n# Response 2/2\n\n## Response to the question on N Terminal and C Terminal\n\nWhen a peptide bond is formed (green), one amine group and one carboxy group are left untouched. Therefore, in a linear polypeptide, there would always be a free amine group and a free carboxy group, and they are called the N terminal(blue) and the C terminal(red) respectively in biochemistry. You can find the visualization in **Appendix H**.\n\n## Response to fixing the molecule encoder in contrastive learning\n\nFrom an **effectiveness** standpoint, as you mentioned before, there are unavoidable discrepancies between pseudo and true ligands. If we didn't fix our molecule encoder during pretraining, then the conditions of Theorem 3.1 might not be met. The ablation results presented in the previous response clearly indicate a performance decline when the molecular encoder is not fixed during contrastive learning, particularly in the context of pocket matching. This supports our assertion that the pocket encoder struggles to acquire chemical knowledge without the stability provided by a fixed molecular encoder.\n\nFrom an **efficiency** standpoint, a fixed molecule encoder also offers significant advantages. It reduces memory usage and accelerates training speed because all molecule embeddings can be pre-computed before training. This eliminates the need for feed-forward and back-propagation operations of the molecule encoder during training, leading to efficiency gains. Additionally, adapting to different molecule encoders becomes more straightforward when they are not co-trained during the pretraining process. This approach allows for more flexibility and ease in integrating various encoding methodologies **even if the molecule encoder is not accessible for training**. That is to say, our approach can even be used with undifferentiable molecular representations.\n\n## Response to extending to other tasks like protein-protein interaction prediction\n\nThank you for pointing out the potential application of our framework to protein-protein interaction (PPI) tasks. We have to point out that the interactions between proteins and proteins are far different from proteins and molecules. Specifically, the interaction areas are usually much larger, but shallower, and involved functional groups are less diverse due to limited types of natural amino acids. Therefore, several changes should be made to extend our ProFSA method to PPI tasks:\n\n1. As for data creation, unlike the localized and specific nature of pocket-ligand interactions, protein-protein interactions are **more global and involve broader protein features**. This requires a shift in the data creation process to split entire protein structures into domains instead of short fragments and to capture potential interactions between them. \n\n2. The interaction interface between protein and protein is much larger. We might need to change from atomic-level to residue-level representations to efficiently capture interaction information.\n\n3. For contrastive learning training, ProFSA uses a fixed pretrained molecular encoder to cope with various small molecules and their atomic representation. However, for protein-protein interaction, it is not necessary as we could only model 20 types of amino acids at the residue level.\n\nDespite several changes that need to be done for PPI tasks, we find that we could divide protein complexes into local interactions. When the complex structures and monomer structures are provided, we have achieved a reasonable result on a flexible PPI affinity prediction benchmark with our current model in a zero-shot way. \n\n| Method            | Spearman $\\uparrow$  |\n|-------------------|----------------------|\n| SchNet            | 0.072 ± 0.021        |\n| DimeNet++         | 0.171 ± 0.054        |\n| EGNN              | 0.080 ± 0.038        |\n| TorchMD           | 0.117 ± 0.008        |\n| GET               | **0.363 ± 0.017**    |\n| ProFSA(zero-shot) | _0.248_              |\n\nSurprisingly, as a **zero-shot** method, ProFSA is able to outperform other **supervised learning** models except for GET, a newly proposed unified model. This demonstrates our approach is also able to capture protein-protein interaction information despite it being designed for protein-ligand interaction. \n\nA detailed explanation of this experiment is shown in **Appendix C6**.\n\n---\n\n# Response 1/2\n\n## Response to evidence for the construction of the pseudo-ligand and using peptides to replace small molecules \n\nAs we discussed in the paper, pseudo-ligands share similar sizes as real ligands, and they also make similar non-covalent interactions with the pocket. To further support our proposal, we would like to cite another paper **\"A defined structural unit enables de novo design of small-molecule–binding proteins\"**[1], published in Science 2020, which supports that **intra-protein interactions are similar to protein-ligand interactions.** That is the reason we use peptides to **represent** small molecules to let the pocket encoder learn the interaction information. \n\nAlso, we'd like to clarify that our intention is not to **\"replace\"** small molecules with peptides. It's a pretraining framework that leverages abundant protein-only data and uses peptides to **simulate** pocket-small molecule interactions for enhanced pocket representations. Following this pretraining phase, **the model can be finetuned with datasets that include real small molecules, ensuring its effectiveness in practical applications**.\n\nTo demonstrate the validity of constructing pseudo-ligands, we provide visualizations in **Appendix F** showing that interactions between pockets and our pseudo-ligands are similar to those with true ligands. These figures reveal shared interaction types like **hydrogen bonding**, **$\\pi-\\pi$ stacking**, and **salt bridge**, depicted with color-coded dashed lines for each interaction type. This should help clarify the rationale and evidence supporting our approach.\n\n[1] Polizzi, Nicholas F., and William F. DeGrado. \"A defined structural unit enables de novo design of small-molecule–binding proteins.\" Science 369, no. 6508 (2020)\n\n## Response to ablation studies evaluating the impact of critical design choices\n\nThanks for your advice on adding more ablation studies on design choices. We did ablation studies on the effectiveness of distributional alignment, where the major difference is that fragment sizes are modified. We found that aligned fragment sizes with real ligands could provide the best performance. The result is shown in **Table 5**.\n\nAs for the distance thresholds, we define the pocket following the UniMol setup. We also found our model can also adapt to an 8Å setup in the ToughM1 experiment. Following your advice, we did an ablation study on the distance thresholds. We tested our method with three different thresholds: 4Å, 6Å, and 8Å. The result is shown below:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| 4Å | 0.7062 | 0.7549 | 0.1240 | 0.1095 | 28.29 | 13.07 |\n| 6Å | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| 8Å | 0.8322 | 0.8292 | 0.1256 | 0.1125 | 34.83 | 12.92 |\n\nSince the 8Å threshold corresponds with the pocket definition in the Kahraman and Tough M1 datasets, it leads to optimal results. Our decision to use a 6Å threshold was made to align with the methodology of pretraining data creation by Uni-Mol, facilitating a fair comparison. Notably, even with the 6Å threshold, we achieved strong results in the pocket-matching task, which serves as a testament to the effectiveness of our approach.\n\n## Response to substantial discrepancies between pseudo and real ligands\n\nThank you for pointing out the discrepancies between pseudo and real ligands. We understand and acknowledge your concern. To mitigate the impact of this discrepancy, we have made several efforts. First, we performed **distribution alignment** to make the data distribution of pseudo-ligands more similar to that of true ligands in PDBbind. Additionally, we **fixed the molecule encoder during pretraining** to prevent it from being misled by the discrepancy. Our ablation studies have shown that these strategies are effective:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nThe table above shows that lacking distribution alignment and not fixing the molecular encoder during pretraining both lead to lower performance, underscoring the effectiveness of our methods in reducing discrepancies.\n\nTheorem 3.1 theoretically supports our approach's efficacy, even with discrepancies between pseudo and real ligands. Empirically, our method outperforms other pretraining techniques, confirming its effectiveness despite these discrepancies.\n\n---\n\n## Response to limited tasks and potential in protein-ligand binding pose prediction\n\nIn our paper, the primary objective is to assess the effectiveness of our pretraining method and the quality of the trained pocket representations. To achieve this, we focused on two types of downstream tasks: pocket-only tasks (pocket property prediction and pocket matching) and pocket-ligand interaction tasks (ligand binding affinity prediction).\n\nWe appreciate the suggestion to apply our method to the protein-ligand binding pose prediction task. However, it's important to note that our current evaluation framework is designed to specifically assess **pocket representations**. For the tasks we chose, the architecture is straightforward: either a **zero-shot** evaluation or a **simple MLP** for mapping embeddings to predictions. In contrast, protein-ligand binding pose prediction often involves **complex methodologies**. For instance, state-of-the-art methods like DiffDock[1] require training a diffusion generative model and a separate confidence model, while other approaches like EDM-Dock[2], rely on reconstructing ligand conformations from predicted distance maps. These methods are not end-to-end and do not directly align with our objective of evaluating pretrained pocket representations.\n\nNevertheless, we recognize the potential of our method in enhancing existing binding pose prediction techniques. To integrate our approach, we would need to modify our framework. Using the same data creation strategy, we could train a binding pose prediction model with our preprocessed data, which could then be further fine-tuned using real pocket-ligand pair data from sources like PDBbind. Additionally, our data could be used to train a side-chain packing model, allowing for side-chain flexibility during docking. Thank you again for your advice, and we will leave protein-ligand binding pose prediction as a separate future work for our method.\n\nIn response to the need for evaluating our method on a broader range of tasks, we have extended our analysis to include two additional downstream tasks: LEP (Ligand Efficacy Prediction), and PPA (Protein-Protein Affinity Prediction).\n\nresult for LEP:\n\n| Method      | AUROC $\\uparrow$ | AUPRC $\\uparrow$ |\n|-------------|--------------|--------------|\n| ATOM3D-GNN  | 0.681        | 0.598        |\n| GeoSSL      | 0.776±0.03   | 0.694±0.06   |\n| Uni-Mol     | 0.782±0.02   | 0.695±0.07   |\n| ProFSA      | 0.840±0.04   | 0.806±0.04   |\n\nGeoSSL and Uni-Mol, both pretraining methods, yield comparable results. However, ProFSA outperforms these methods, demonstrating the advantage of our pocket pretraining approach.\n\nresult for PPA:\n\n| Method            | Spearman $\\uparrow$  |\n|-------------------|----------------------|\n| SchNet            | 0.072 ± 0.021        |\n| DimeNet++         | 0.171 ± 0.054        |\n| EGNN              | 0.080 ± 0.038        |\n| TorchMD           | 0.117 ± 0.008        |\n| GET               | **0.363 ± 0.017**    |\n| ProFSA(zero shot) | _0.248_              |\n\nAs a **zero-shot** method, ProFSA is able to outperform other **supervised learning** models except for GET, a newly proposed unified model. This demonstrates our approach is able to capture protein-protein interaction information despite it being designed for protein-ligand interaction modeling.\n\nYou can find detailed experiment settings and results in **Appendix C5 and C6**.\n\n[1] Corso et al., \"Diffdock: Diffusion steps, twists, and turns for molecular docking.\", ICLR 2023.\n[2] Masters et al., \"Deep learning model for efficient protein–ligand docking with implicit side-chain flexibility.\" Journal of Chemical Information and Modeling 63, no. 6 (2023).\n\n## Response to the issue of lack of latest baselines\n\nFor the pocket matching task, CoSP is a newly proposed baseline which is published in ECML PKDD 2023. Alongside CoSP and Uni-Mol, we selected some of the most effective baseline results presented in the CoSP paper. We also tried to evaluate recent methods like PocketAnchor (Li et al., Cell Systems 2022) on our own since they are not tested on the pocket matching task. However, since the result didn't outperform other baseline machine learning methods(Uni-Mol, CoSP, and DeeplyTough), we decided not to include it in our final baseline comparison. We can include it in the camera-ready version if needed.\n\nFor the ligand binding affinity task, a lot of newly proposed baselines are included, i.e. ProNet (Wang et al., 2022b); as well as pretraining methods such as GeoSSL (Liu et al., 2023), EGNN-PLM (Wu et al., 2022), DeepAffinity (Karimi et al., 2019) and Uni-Mol (Zhou et al., 2023).\n\n---\n\n## Response to lack of related works\n\nThanks for providing us with more insightful related works that could support our arguments. We actually have cited these three papers in the section on ligand binding affinity experiments, but we are also happy to add these in Related Works section 2.2. You can find the change in pdfdiff.\n\n## Response to lack of COSP as a baseline in Table 3\n\nBecause best to our knowledge, they did not release their code and they did not test their method on the ligand binding affinity dataset. \n\n## Response to the question on the loss terms\n\nThank you for your advice. We apologize that we accidentally mentioned loss1 and loss2 in reverse, and we are sorry for the confusing statement. You are correct that the first loss is to identify the true protein pocket when given a pseudo-ligand. We revise the original statement to: \"The primary purpose of the first loss is to identify the true protein pocket from a batch of samples when given a pseudo-ligand. Similarly, the second loss seeks to identify the corresponding ligand fragment for a given pocket.\" You can also find the change in pdfdiff.\n\n## Response to the question on the length of the pocket representation without alignment\n\nWith or without distributional alignment, pockets are always defined by the given protein fragment with a fixed distance cutoff (6Å in our works, following the UniMol setup). The distributional alignment process merely samples these pairs to match the sizes of real ligands and pockets. Without distributional alignment, fragments are uniformly sampled from 1 to 7 residues. Pockets are similarly determined as with residues within the range of 6Å around peptide fragments.\n\n---\n\n# Question about the confidence score\n\nThanks for your review. We found your understanding and judgment of our paper comprehensive and precise. Also, you provided detailed and thoughtful advice for our paper, which helped a lot. We sincerely appreciate your review, but we also wonder if there is any misunderstanding here, as you only give a confidence score of 1.\n\n---\n\n# Response 3/3\n\n## Response to biological justification and visualization of the results\n\nOur method's **biological justification** for achieving good experimental results is based on the properties and interactions of pseudo-ligands. As discussed in our paper, these pseudo-ligands are similar in size to real ligands and engage in comparable non-covalent interactions with protein pockets. This fundamental similarity is crucial for the effectiveness of our approach.\n\nMoreover, our methodology is further supported by the findings in the 2020 Science paper titled \"A defined structural unit enables de novo design of small-molecule–binding proteins.\"[1] This research underscores that **intra-protein interactions are analogous to protein-ligand interactions**, which validates our use of peptides as proxies for small molecules. By employing peptides as pseudo-ligands in this manner, our pocket encoder is able to learn and replicate the interaction dynamics typically observed in real ligand scenarios. This understanding is pivotal to the success of our method in downstream applications.\n\nTo better justify our methods and results, following your advice, we also provide some visualizations in **Appendix F**. **Figure 6** illustrates that the pseudo pairs we created share various interaction types commonly found in real pocket-ligand pairs, such as **hydrogen bonding, $\\pi-\\pi$ stacking, and salt bridge**. In the figures, each type of interaction is represented by dashed lines, color-coded to correspond with the specific interaction type.\n\nAnother visualization of our pocket-matching result is shown in **Figure 7**. We showed an example of two non-homology ATP binding pockets to explain why pocket-matching can benefit from interaction-aware pretraining so that we have achieved superior results in the Kahraman dataset and the BioLip t-SNE visualization. The PEP carboxykinase(PDB:1AYL) and tRNA synthetase(PDB:1B8A) are two ATP-binding proteins that share zero sequence similarity (verified with the BLAST) extracted from the Kahraman dataset. However, as they are both fueled by ATP, their binding site shares similar binding patterns. The cation-$\\pi$ interactions(blue dash lines) and salt bridges (magenta dash lines) are important to the ATP binding, which can be viewed as convergent evolutions at the molecule level. Even though, these two pockets are very distinct in terms of shapes and sizes because they bind ATPs in different conformations. Therefore, biochemical interactions are the key to accomplishing the pocket-matching task, which is ignored in previous self-supervised learning methods like Uni-Mol.\n\n[1] Polizzi, Nicholas F., and William F. DeGrado. \"A defined structural unit enables de novo design of small-molecule–binding proteins.\" Science 369, no. 6508 (2020)\n\n---\n\n# Response 2/3\n\n## Response to the bound of Theorem 3.1 \n\nWe are not very clear about the point of your question. We have two understandings and they are replied as follows.\n\nIf you mean $ ||t − t ^{(0)}|| < 1/2 l_T$ is trivial, please note that it is only a condition of the theorem. As you said, many other encoders can also satisfy this condition. This shows that our method is applicable to many different molecular encoders with or without pretraining as we proved in **Table 4**. Moreover, the condition is only a necessary condition. We also need the pretraining contrastive loss to be sufficiently small to guarantee a small contrastive loss between pockets and real ligands. In fact, in the process of our early exploration, we noticed that some encoders cannot achieve a low pretraining loss, indicating the entire conditions of the theorem are nontrivial.\n\nIf you mean that the conclusion of theorem 3.1 is trivial, we want to emphasize that our conclusion is $\\lim_{L_i(t, s) =0}L_i(t^{(0)}, s) =0$. (We write it in a $\\epsilon$-$\\delta$ language in paper. They are equivalent forms.) Our conclusion shows the loss containing real ligands is consistent with the loss containing pseudo ligands that we optimized in pre-training. \n\nIn contrast,  a trivial result from $||t − t ^{(0)}|| < 1/ 2l_T$ is that $\\lim_{L_i(t, s) =0}L_i(t^{(0)}, s) <B$, B is a bound related to l_t and the representations. It does not guarantee a consistent loss and is substantially different from our result.\nTherefore, our theorem is nontrivial and reveals the transfer ability of our contrastive pre-training from the pseudo-ligand domain to the real ligand domain.\n\nAs for this theoretical result, we are open to further discussion.\n\n## Response to molecules possess more properties that can also lead to imbalance\n\nIt is correct that molecules have more properties than their size, like logP, Hbond donor and acceptor number, and rotatory bond number. As small molecule drugs are mostly designed to penetrate barriers like gut or cell membranes, they usually have much larger logP values, which means more hydrophobic. For the same reason, they usually have fewer hydrogen bond donors and acceptors, and only minimal essential ones are kept for specified interactions. Also, to minimize the entropy effect upon binding and to increase binding affinity, rotatory bonds are also unfavored. However, it is impossible to mimic such features with peptides, as the backbone of peptides is intrinsically hydrophilic and flexible. As we showed in a new figure, our pseudo-ligands are less similar to real ligands in those properties even with size alignment. You can find the figure in the new pdf in **Appendix G**.\n\nWe believe that the aforementioned divergence is the main limitation of poor zero-shot performance in predicting hydrophobicity scores (**Table 1**). However, as demonstrated in previous publications like CoSP, we could leverage real ligand-pocket pairs from PDBBind or BioLip database to further finetune our network, as an extension of the pipeline. In this way, we could handle property mismatches but still enjoy the power of our large-scale pretraining.\n\n## Response to the approach to defining pockets \n\nWe define the pocket following the UniMol setup. We found our model can also adapt to an 8Å setup in the toughM1 experiment \n\nIn particular, we define the pocket for each protein-ligand pair as residues of the protein that have at least one atom within the range of 6Å from a heavy atom in the ligand. To further explain our design, we have done an ablation study on different choices of thresholds of 4Å, 6Å, and 8Å:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| 4Å | 0.7062 | 0.7549 | 0.1240 | 0.1095 | 28.29 | 13.07 |\n| 6Å | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| 8Å | 0.8322 | 0.8292 | 0.1256 | 0.1125 | 34.83 | 12.92 |\n\nSince the 8Å threshold corresponds with the pocket definition in the Kahraman and Tough M1 datasets, it leads to optimal results. Our decision to use a 6Å threshold was made to align with the methodology of pretraining data creation by Uni-Mol, facilitating a fair comparison. Notably, even with the 6Å threshold, we achieved strong results in the pocket-matching task, which serves as a testament to the effectiveness of our approach.\n\n---\n\n# Response 1/3\nWe appreciate the time and effort you have dedicated to reviewing our paper, and we are grateful for your constructive feedback and thoughtful evaluation of our work. We would like to address your comments about the technical novelty of our work and provide additional clarification on certain aspects of our paper.\n\n## Response to technical novelty\n\nFirstly, we acknowledge your observation that we didn't use any fancy models compared with typical **model-centric** works. While we respect your assessment, we would like to highlight that our primary focus in this paper is a **data-centric** pretraining method to introduce groundbreaking improvements in the field of protein pocket pretraining. Our work aims to address a critical challenge of data scarcity in the field of protein pocket representation by constructing large-scale synthetic data that facilitates the pretraining of models, ultimately enhancing the accuracy and robustness of protein pocket representations. We believe that the research community can conduct fast following-ups on our **released dataset** with more sophisticated models, and even extend our pipelines to other tasks like docking or drug design. We also would like to point out that we have provided a novel method to **distill knowledge from well-trained molecule models** to protein models. Though employing the same contrastive loss, our approach is different from existing contrastive learning models since it uses a fixed molecular encoder. The motivation here is to use a well-trained molecular encoder on a relatively larger dataset to **guide** the training of the protein encoder. For example, some quantum-chemistry properties that are difficult to compute for large systems like proteins could be distilled from molecules, by using some quantum-chemistry-aware molecular encoders such as Frad. Though our framework seems to be simple, it is non-trivial to make it work. We have made several efforts to solve the unavoidable discrepancies between true ligands and pseudo ligands. The ablation study shows that our efforts are effective and necessary:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nRegarding your positive acknowledgment of our \"simple but effective model for the right question,\" we are pleased to hear that our approach resonates with the objective we set out to achieve. We designed our model with **simplicity** in mind, prioritizing effectiveness and practical utility for the specific problem domain. This deliberate choice aligns with the notion that sometimes the most impactful solutions are elegantly straightforward. Also, we intentionally borrowed a pocket encoder from Uni-Mol to make a **fair comparison** with it, which strongly supports the power of our dataset. Similar to the molecule encoder, our pocket encoder can be changed to any other model due to our flexible framework. Notably, **we didn't load the pretrained weights of Uni-Mol pocket encoder**. We only use the same backbone architecture and the pretraining was completely done on our processed data with our training strategy. \n\nNotably, many simple but effective approaches have been recognized by top-tier conferences and journals. A prime example of this is in the field of protein language models, like the ESM series and ProtTrans. These studies adapted the Transformer architecture and masked language modeling techniques from natural language processing to protein sequences. While they didn't introduce groundbreaking techniques, their substantial contributions to protein modeling are evident, with publications in prestigious conferences and journals like NeurIPS, ICML, TPAMI, PNAS, and Science. Another example is the widely acclaimed CLIP paper presented at ICML 2021. CLIP, while not employing novel techniques, stands as a hallmark of data-centric deep learning. Its use of contrastive learning enabled training on expansive web-sourced text-image datasets, moving beyond the constraints of meticulously curated databases like MS-COCO. This aligns with our approach to addressing **data scarcity challenges** by facilitating training on large-scale datasets, mirroring our strategy for overcoming similar hurdles in the protein pocket modeling domain.\n\nWe appreciate your efforts to remain neutral and understand the challenges in decision-making. We believe further discussion will highlight our paper's contributions. Your feedback and insights in the upcoming discussion will be invaluable for refining our work.", "author_response": "# Response 1/3\nWe appreciate the time and effort you have dedicated to reviewing our paper, and we are grateful for your constructive feedback and thoughtful evaluation of our work. We would like to address your comments about the technical novelty of our work and provide additional clarification on certain aspects of our paper.\n\n## Response to technical novelty\n\nFirstly, we acknowledge your observation that we didn't use any fancy models compared with typical **model-centric** works. While we respect your assessment, we would like to highlight that our primary focus in this paper is a **data-centric** pretraining method to introduce groundbreaking improvements in the field of protein pocket pretraining. Our work aims to address a critical challenge of data scarcity in the field of protein pocket representation by constructing large-scale synthetic data that facilitates the pretraining of models, ultimately enhancing the accuracy and robustness of protein pocket representations. We believe that the research community can conduct fast following-ups on our **released dataset** with more sophisticated models, and even extend our pipelines to other tasks like docking or drug design. We also would like to point out that we have provided a novel method to **distill knowledge from well-trained molecule models** to protein models. Though employing the same contrastive loss, our approach is different from existing contrastive learning models since it uses a fixed molecular encoder. The motivation here is to use a well-trained molecular encoder on a relatively larger dataset to **guide** the training of the protein encoder. For example, some quantum-chemistry properties that are difficult to compute for large systems like proteins could be distilled from molecules, by using some quantum-chemistry-aware molecular encoders such as Frad. Though our framework seems to be simple, it is non-trivial to make it work. We have made several efforts to solve the unavoidable discrepancies between true ligands and pseudo ligands. The ablation study shows that our efforts are effective and necessary:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nRegarding your positive acknowledgment of our \"simple but effective model for the right question,\" we are pleased to hear that our approach resonates with the objective we set out to achieve. We designed our model with **simplicity** in mind, prioritizing effectiveness and practical utility for the specific problem domain. This deliberate choice aligns with the notion that sometimes the most impactful solutions are elegantly straightforward. Also, we intentionally borrowed a pocket encoder from Uni-Mol to make a **fair comparison** with it, which strongly supports the power of our dataset. Similar to the molecule encoder, our pocket encoder can be changed to any other model due to our flexible framework. Notably, **we didn't load the pretrained weights of Uni-Mol pocket encoder**. We only use the same backbone architecture and the pretraining was completely done on our processed data with our training strategy. \n\nNotably, many simple but effective approaches have been recognized by top-tier conferences and journals. A prime example of this is in the field of protein language models, like the ESM series and ProtTrans. These studies adapted the Transformer architecture and masked language modeling techniques from natural language processing to protein sequences. While they didn't introduce groundbreaking techniques, their substantial contributions to protein modeling are evident, with publications in prestigious conferences and journals like NeurIPS, ICML, TPAMI, PNAS, and Science. Another example is the widely acclaimed CLIP paper presented at ICML 2021. CLIP, while not employing novel techniques, stands as a hallmark of data-centric deep learning. Its use of contrastive learning enabled training on expansive web-sourced text-image datasets, moving beyond the constraints of meticulously curated databases like MS-COCO. This aligns with our approach to addressing **data scarcity challenges** by facilitating training on large-scale datasets, mirroring our strategy for overcoming similar hurdles in the protein pocket modeling domain.\n\nWe appreciate your efforts to remain neutral and understand the challenges in decision-making. We believe further discussion will highlight our paper's contributions. Your feedback and insights in the upcoming discussion will be invaluable for refining our work."}
{"claim": "Existing public datasets contain a limited number of pocket-ligand complex structures, limiting direct pocket-ligand pretraining opportunities.", "claim_type": "quantitative", "paper_id": "uMAujpVi9m", "paper_title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "9ap4s1SZlZ", "reviewer": "Reviewer_yfM3", "review_text": "Summary: This paper primarily aims to enhance the pocket pretraining method, as existing approaches only consider pockets during pretraining. There are two main contributions in this paper: (1) The authors introduce a novel method, ProFSA, for pocket pretraining, which extracts additional information from corresponding ligands. However, the number of pocket-ligand complex structures is quite limited in existing datasets. (2) To address this issue, the authors generate over 5 million complexes by segmenting fragments and their corresponding pockets in protein structures. By aligning features of fragments and pockets, the pocket encoder learns the interaction between fragments and pockets. The authors design downstream tasks such as pocket druggability prediction, pocket matching, and ligand binding affinity prediction to demonstrate the effectiveness of ProFSA.\n\nStrengths: The authors propose a new perspective of pretraining pockets and construct a large-scale dataset, which data distribution is also considered, to make the efficient pre-training possible.\n\nThe results are competitive, especially for zero-shot settings.\n\nAbundant experiments and ablation study support the argument and result of the authors.\n\nWeaknesses: 1. The technical novelty is limited.\n  - The pocket encoder is borrowed from Uni-Mol.\n  - The contrastive loss is the vanilla form of classical contrastive learning.\n\n2. The bound of Theorem 3.1 is trivial. The authors claim that the bound naturally exists for these representations extracted by pretrained molecule models. However, it's a bit counterintuitive, because many models not pretrained on molecule datasets also fulfill this prior. So, can these models be used for this task? **I strongly suggest removing this part from the paper**.\n\n3. Some issues about dataset creation:\n - 3.1. The authors consider the distribution of ligand size and pocket size when designing the dataset. However, molecules possess more properties that can also lead to imbalance. It would be better to, at least, add some discussion about this issue.\n - 3.2. In the second stage of the data construction process, the approach to defining pockets needs further explanation or an ablation study.\n\n4. Experiments: It would be better to add some biological justification or visualization of the results.\n\nFor this paper, one fact is that the technical novelty is below the bar of ICLR. However, I admire the simple but effective model for the right question. It's a struggle for me to make a decision. I will maintain a neutral attitude and make my final decision after the discussion.\n\nQuestions: See weakness.", "labeling_timestamp": "2026-01-11T16:25:01.173041", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors acknowledge the data scarcity and directly address it by creating a large synthetic dataset and a peptide-based pretraining pipeline to compensate for limited real pocket-ligand complexes.", "evidence": "\"we have designed a pipeline to generate **over 5 million** synthetic ligand-protein pairs, which greatly eases the **data scarcity** problem in the field of protein-ligand interaction learning.\"", "combined_author_response": "Dear AC and reviewers,\n\nWe sincerely appreciate your time and efforts in reviewing our work. Based on your suggestions, we have revised our paper. We would like to use this section to reiterate the explanation for some common concerns and summarize the contributions of our paper.\n\nIn our revised paper, we provide a detailed explanation of the motivation and theory behind our method: **the mechanisms of pocket-peptide interactions and pocket-ligand interactions are very similar**. Our proposed **data-centric** pretraining pipeline is unique in its use of peptides to mimic small molecules, a method that is, to our knowledge, **the first of its kind**. We expanded our discussion about its biological insights in **Section 3**. We add a **new Figure 2 to illustrate different types of non-covalent interactions** that are common for both ligand-protein data and intra-protein data. Following this theory, we have designed a pipeline to generate **over 5 million** synthetic ligand-protein pairs, which greatly eases the **data scarcity** problem in the field of protein-ligand interaction learning. We also prove our data quite effective in various downstream tasks, including pocket property prediction, pocket matching, ligand binding affinity prediction, ligand efficiency prediction, and protein-protein interaction tasks. The **last two tasks are newly added** to further support our method (**Appendix C5 and C6**). We are confident that our **ready-to-release synthetic dataset** will significantly benefit the AI for drug discovery community. Its impact extends beyond enhancing pocket representations in downstream analyses, as it also equips researchers with a valuable resource for a range of applications, such as protein-ligand docking, structure-based drug design, and virtual screening. Importantly, these areas often face challenges due to **a lack of training data necessary for learning effective binding patterns between protein pockets and ligands**. \n\nIn our revised paper, we recognize the differences between real ligands and pseudo ligands (peptides). We address these discrepancies and present a distribution plot of various properties in **Appendix G**. Our approach primarily leverages the **analogous nature of ligand-pocket and peptide-pocket interactions**. Therefore, despite these differences, our model can **still extract valuable interaction information from the pretraining dataset**, which enhances its performance in downstream tasks. This is illustrated in **Figure 4(a)**, showing the benefits of interaction-aware pretraining. We have also undertaken measures like distribution alignment and freezing the molecular encoder to mitigate the impact of these discrepancies. The positive results of these efforts, as evidenced in **Table 5**, confirm their effectiveness and necessity. Notably, our pretrained model can be **further finetuned with real labeled ligand-pocket pairwise data**, as we show in the Ligand Binding Affinity task in **section 4.3**.\n\nAs for other questions, we have added more ablation studies on our design choices, including the impact of a fixed molecular encoder (Table 5), and different threshold values to define pockets(Appendix C.4). \n\nWe are immensely grateful for the invaluable feedback from all the reviewers, which has guided us in refining and clarifying our work. We hope that the revised version of our paper, coupled with the discussion period, will more clearly highlight the novelty, effectiveness, and contributions of our approach to the field.\n\n---\n\nThank you for your invaluable feedback on our paper. In response to your insightful suggestions, we have made several modifications in our revised paper. \n\nWe have cited additional papers in Section 2.2 following your advice. The description of loss terms in Section 3.2 has been corrected.  COSP was not included as a baseline for the Ligand Binding Affinity (LBA)  task because it is not open-sourced, and its performance on LBA was not evaluated in their published work. We used it as a baseline in the pocket-matching task.\n\nIn Section 3, we've expanded our discussion to answer your question on **ProFSA's effectiveness without distributional alignment**. This section now offers a detailed explanation of the foundational concepts and justifications for our method. We draw attention to the parallels between ligand-protein and intra-protein non-covalent interactions, as shown in **Figure 2**. This comparison supports our strategy of using peptides as stand-ins for actual ligands to mimic pocket-ligand interactions. Consequently, **even in the absence of distributional alignment, the types of interactions remain comparably relevant**, which explains why ProFSA continues to function, albeit with reduced effectiveness, as demonstrated in Table 5. We also want to clarify that distributional alignment is only applied to fragment-pocket complex as a whole, and **it would not change the definition of the pocket with a given ligand**. Without this alignment, these fragments are chosen uniformly, with lengths varying from 1 to 7 residues, and pockets are consistently identified based on residues within a 6Å radius of these peptide fragments.\n\nWe are grateful for the time and effort you have dedicated to reviewing our work. Your thorough and constructive feedback has significantly contributed to the refinement of our research. Please let us know if there are any other aspects of our paper that you would like us to address or clarify. We look forward to your further feedback.\n\n---\n\nThank you for your insightful feedback. In response to your concerns regarding the scope of our tasks, we have expanded our research in the revised paper. We now include additional tasks such as **ligand efficiency prediction** and **protein-protein affinity prediction**, detailed in **Appendix C5 and C6**. These tasks were incorporated to enhance the robustness of our method. We acknowledge that they are currently in the appendix due to space constraints, but we are open to relocating them to the main text if it would be beneficial.\n\nRegarding your concern about the lack of recent baselines, we provided a detailed explanation in our previous response. We want to reiterate that our benchmarks do include the latest baselines, with some as recent as 2023.\n\nWe greatly appreciate the time and effort you have invested in reviewing our work. Your feedback has been invaluable in refining our research. Please let us know if there are any other aspects of our paper that you would like us to address or clarify. We look forward to your further feedback.\n\n---\n\nIn our revised paper, we have incorporated changes following feedback from another reviewer, which we believe also addresses your concerns and queries.\n\nRegarding your question on the evidence supporting our approach, particularly the use of peptides to simulate small molecules, we've enriched **Section 3.1**. A new paragraph there now provides an in-depth explanation of non-covalent interactions. To aid understanding, **Figure 2** has been updated with visualizations that include both real and pseudo receptor-ligand pairs, illustrating three types of interactions. Furthermore, we delve into the details of **how intra-protein interactions mirror protein-ligand interactions**, involving specific types of amino acids.\n\nTo better evaluate our design choices, the ablation studies have been extended. Alongside the existing studies in **Table 4** (different pretrained molecular encoders), **Table 5** (distribution alignment), and **Figure 5** (data scale). we've added a study on the impact of using a fixed molecular encoder in **Table 5** in the main text to address your question on the fixed molecular encoder. We've also included an ablation study on varying **cutoff values for pocket definition**, placed in the **Appendix C.4** due to its more focused scope. We can move it to the main part of the paper in the future if needed.\n\nWe acknowledge your concern about the **differences between true and pseudo ligands**, so we've expanded our discussion on the distinct properties of our curated dataset versus the PDBBind dataset in **Figure 9** and **Section 3.2**. The ablation study in **Table 5** demonstrates our methods and their effectiveness in minimizing these discrepancies. Notably, the performance significantly drops without our distribution alignment or fixed molecular encoder, highlighting our efforts to mitigate these issues. The updated **Figure 2** further supports this, showing that while the properties of true and pseudo ligands may differ, **the interaction types are consistent.** We hope these results can ease your concerns about our synthetic dataset, and they also respond to your query about **why we kept the molecular encoder constant during pocket pretraining.**\n\nWe appreciate the constructive critiques and thoughtful engagement from you. Your feedback has undeniably contributed to the refinement and strengthening of our paper. We sincerely value the time and expertise invested in the review process, and we look forward to any further suggestions you may have.\n\n---\n\nWe sincerely appreciate the thoughtful feedback provided on our paper and have diligently incorporated your suggestions into this revised version. In response to your guidance, we have made three significant modifications to the core content of our paper, all of which aim to enhance clarity and address the concerns raised.\n\nTaking into account your suggestion, we have removed **Theorem 3.1** from the main body of our paper. Additionally, we've improved the explanation and included an ablation study on our method of defining pockets. This ablation study, focusing on the cutoff values and primarily a hyperparameter issue, is placed in the Appendix due to its more limited scope of insight. We can move it to the main body of the paper in the future if needed. We have also expanded our discussion on the differences between our dataset and the PDBBind dataset, elucidating how fixed molecule encoders contribute to mitigating this discrepancy.\n\nIn **Section 3.1**, we have introduced a new paragraph dedicated to providing readers with a deeper understanding of non-covalent interactions. To facilitate comprehension, we have included visualizations featuring three types of interactions, incorporating both real and pseudo receptor-ligand pairs in the new **Figure 2**. Furthermore, we delve into the details of how intra-protein interactions mirror protein-ligand interactions, involving specific types of amino acids.\n\nThe biological justifications for the efficacy of interaction-aware pretraining are now more extensively explored in our revised paper. Specifically, we present a compelling showcase featuring estradiol-binding proteins, offering insights into the geometric disparities between the compared proteins and highlighting the similarities in their binding interfaces in the new **Figure 4a**. The existing visualization of BioLip pocket representations in the new **Figure 4b** can make our claims more convincing. Together, these visual aids illustrate how our interaction-aware pretraining empowers models to focus on crucial interface residues, discern subtle distinctions, and disregard irrelevant geometric dissimilarities.\n\nWe have also included an in-depth discussion of pretrained molecule encoders in the newly added **Section 2.3**, further enriching the theoretical foundation of our work.\nAll changes can be found in the renewed paper, which is updated lively.\n\nWe genuinely value the constructive input you provided during the review process, as it has undoubtedly contributed to the refinement of our paper. Your insights have proven invaluable in elevating the clarity and depth of our contributions to the field. We express our gratitude for the time and expertise invested in reviewing our work, and we look forward to any additional feedback or suggestions you may have.\n\n---\n\n# Response 2/2\n\n## Response to the question on N Terminal and C Terminal\n\nWhen a peptide bond is formed (green), one amine group and one carboxy group are left untouched. Therefore, in a linear polypeptide, there would always be a free amine group and a free carboxy group, and they are called the N terminal(blue) and the C terminal(red) respectively in biochemistry. You can find the visualization in **Appendix H**.\n\n## Response to fixing the molecule encoder in contrastive learning\n\nFrom an **effectiveness** standpoint, as you mentioned before, there are unavoidable discrepancies between pseudo and true ligands. If we didn't fix our molecule encoder during pretraining, then the conditions of Theorem 3.1 might not be met. The ablation results presented in the previous response clearly indicate a performance decline when the molecular encoder is not fixed during contrastive learning, particularly in the context of pocket matching. This supports our assertion that the pocket encoder struggles to acquire chemical knowledge without the stability provided by a fixed molecular encoder.\n\nFrom an **efficiency** standpoint, a fixed molecule encoder also offers significant advantages. It reduces memory usage and accelerates training speed because all molecule embeddings can be pre-computed before training. This eliminates the need for feed-forward and back-propagation operations of the molecule encoder during training, leading to efficiency gains. Additionally, adapting to different molecule encoders becomes more straightforward when they are not co-trained during the pretraining process. This approach allows for more flexibility and ease in integrating various encoding methodologies **even if the molecule encoder is not accessible for training**. That is to say, our approach can even be used with undifferentiable molecular representations.\n\n## Response to extending to other tasks like protein-protein interaction prediction\n\nThank you for pointing out the potential application of our framework to protein-protein interaction (PPI) tasks. We have to point out that the interactions between proteins and proteins are far different from proteins and molecules. Specifically, the interaction areas are usually much larger, but shallower, and involved functional groups are less diverse due to limited types of natural amino acids. Therefore, several changes should be made to extend our ProFSA method to PPI tasks:\n\n1. As for data creation, unlike the localized and specific nature of pocket-ligand interactions, protein-protein interactions are **more global and involve broader protein features**. This requires a shift in the data creation process to split entire protein structures into domains instead of short fragments and to capture potential interactions between them. \n\n2. The interaction interface between protein and protein is much larger. We might need to change from atomic-level to residue-level representations to efficiently capture interaction information.\n\n3. For contrastive learning training, ProFSA uses a fixed pretrained molecular encoder to cope with various small molecules and their atomic representation. However, for protein-protein interaction, it is not necessary as we could only model 20 types of amino acids at the residue level.\n\nDespite several changes that need to be done for PPI tasks, we find that we could divide protein complexes into local interactions. When the complex structures and monomer structures are provided, we have achieved a reasonable result on a flexible PPI affinity prediction benchmark with our current model in a zero-shot way. \n\n| Method            | Spearman $\\uparrow$  |\n|-------------------|----------------------|\n| SchNet            | 0.072 ± 0.021        |\n| DimeNet++         | 0.171 ± 0.054        |\n| EGNN              | 0.080 ± 0.038        |\n| TorchMD           | 0.117 ± 0.008        |\n| GET               | **0.363 ± 0.017**    |\n| ProFSA(zero-shot) | _0.248_              |\n\nSurprisingly, as a **zero-shot** method, ProFSA is able to outperform other **supervised learning** models except for GET, a newly proposed unified model. This demonstrates our approach is also able to capture protein-protein interaction information despite it being designed for protein-ligand interaction. \n\nA detailed explanation of this experiment is shown in **Appendix C6**.\n\n---\n\n# Response 1/2\n\n## Response to evidence for the construction of the pseudo-ligand and using peptides to replace small molecules \n\nAs we discussed in the paper, pseudo-ligands share similar sizes as real ligands, and they also make similar non-covalent interactions with the pocket. To further support our proposal, we would like to cite another paper **\"A defined structural unit enables de novo design of small-molecule–binding proteins\"**[1], published in Science 2020, which supports that **intra-protein interactions are similar to protein-ligand interactions.** That is the reason we use peptides to **represent** small molecules to let the pocket encoder learn the interaction information. \n\nAlso, we'd like to clarify that our intention is not to **\"replace\"** small molecules with peptides. It's a pretraining framework that leverages abundant protein-only data and uses peptides to **simulate** pocket-small molecule interactions for enhanced pocket representations. Following this pretraining phase, **the model can be finetuned with datasets that include real small molecules, ensuring its effectiveness in practical applications**.\n\nTo demonstrate the validity of constructing pseudo-ligands, we provide visualizations in **Appendix F** showing that interactions between pockets and our pseudo-ligands are similar to those with true ligands. These figures reveal shared interaction types like **hydrogen bonding**, **$\\pi-\\pi$ stacking**, and **salt bridge**, depicted with color-coded dashed lines for each interaction type. This should help clarify the rationale and evidence supporting our approach.\n\n[1] Polizzi, Nicholas F., and William F. DeGrado. \"A defined structural unit enables de novo design of small-molecule–binding proteins.\" Science 369, no. 6508 (2020)\n\n## Response to ablation studies evaluating the impact of critical design choices\n\nThanks for your advice on adding more ablation studies on design choices. We did ablation studies on the effectiveness of distributional alignment, where the major difference is that fragment sizes are modified. We found that aligned fragment sizes with real ligands could provide the best performance. The result is shown in **Table 5**.\n\nAs for the distance thresholds, we define the pocket following the UniMol setup. We also found our model can also adapt to an 8Å setup in the ToughM1 experiment. Following your advice, we did an ablation study on the distance thresholds. We tested our method with three different thresholds: 4Å, 6Å, and 8Å. The result is shown below:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| 4Å | 0.7062 | 0.7549 | 0.1240 | 0.1095 | 28.29 | 13.07 |\n| 6Å | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| 8Å | 0.8322 | 0.8292 | 0.1256 | 0.1125 | 34.83 | 12.92 |\n\nSince the 8Å threshold corresponds with the pocket definition in the Kahraman and Tough M1 datasets, it leads to optimal results. Our decision to use a 6Å threshold was made to align with the methodology of pretraining data creation by Uni-Mol, facilitating a fair comparison. Notably, even with the 6Å threshold, we achieved strong results in the pocket-matching task, which serves as a testament to the effectiveness of our approach.\n\n## Response to substantial discrepancies between pseudo and real ligands\n\nThank you for pointing out the discrepancies between pseudo and real ligands. We understand and acknowledge your concern. To mitigate the impact of this discrepancy, we have made several efforts. First, we performed **distribution alignment** to make the data distribution of pseudo-ligands more similar to that of true ligands in PDBbind. Additionally, we **fixed the molecule encoder during pretraining** to prevent it from being misled by the discrepancy. Our ablation studies have shown that these strategies are effective:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nThe table above shows that lacking distribution alignment and not fixing the molecular encoder during pretraining both lead to lower performance, underscoring the effectiveness of our methods in reducing discrepancies.\n\nTheorem 3.1 theoretically supports our approach's efficacy, even with discrepancies between pseudo and real ligands. Empirically, our method outperforms other pretraining techniques, confirming its effectiveness despite these discrepancies.\n\n---\n\n## Response to limited tasks and potential in protein-ligand binding pose prediction\n\nIn our paper, the primary objective is to assess the effectiveness of our pretraining method and the quality of the trained pocket representations. To achieve this, we focused on two types of downstream tasks: pocket-only tasks (pocket property prediction and pocket matching) and pocket-ligand interaction tasks (ligand binding affinity prediction).\n\nWe appreciate the suggestion to apply our method to the protein-ligand binding pose prediction task. However, it's important to note that our current evaluation framework is designed to specifically assess **pocket representations**. For the tasks we chose, the architecture is straightforward: either a **zero-shot** evaluation or a **simple MLP** for mapping embeddings to predictions. In contrast, protein-ligand binding pose prediction often involves **complex methodologies**. For instance, state-of-the-art methods like DiffDock[1] require training a diffusion generative model and a separate confidence model, while other approaches like EDM-Dock[2], rely on reconstructing ligand conformations from predicted distance maps. These methods are not end-to-end and do not directly align with our objective of evaluating pretrained pocket representations.\n\nNevertheless, we recognize the potential of our method in enhancing existing binding pose prediction techniques. To integrate our approach, we would need to modify our framework. Using the same data creation strategy, we could train a binding pose prediction model with our preprocessed data, which could then be further fine-tuned using real pocket-ligand pair data from sources like PDBbind. Additionally, our data could be used to train a side-chain packing model, allowing for side-chain flexibility during docking. Thank you again for your advice, and we will leave protein-ligand binding pose prediction as a separate future work for our method.\n\nIn response to the need for evaluating our method on a broader range of tasks, we have extended our analysis to include two additional downstream tasks: LEP (Ligand Efficacy Prediction), and PPA (Protein-Protein Affinity Prediction).\n\nresult for LEP:\n\n| Method      | AUROC $\\uparrow$ | AUPRC $\\uparrow$ |\n|-------------|--------------|--------------|\n| ATOM3D-GNN  | 0.681        | 0.598        |\n| GeoSSL      | 0.776±0.03   | 0.694±0.06   |\n| Uni-Mol     | 0.782±0.02   | 0.695±0.07   |\n| ProFSA      | 0.840±0.04   | 0.806±0.04   |\n\nGeoSSL and Uni-Mol, both pretraining methods, yield comparable results. However, ProFSA outperforms these methods, demonstrating the advantage of our pocket pretraining approach.\n\nresult for PPA:\n\n| Method            | Spearman $\\uparrow$  |\n|-------------------|----------------------|\n| SchNet            | 0.072 ± 0.021        |\n| DimeNet++         | 0.171 ± 0.054        |\n| EGNN              | 0.080 ± 0.038        |\n| TorchMD           | 0.117 ± 0.008        |\n| GET               | **0.363 ± 0.017**    |\n| ProFSA(zero shot) | _0.248_              |\n\nAs a **zero-shot** method, ProFSA is able to outperform other **supervised learning** models except for GET, a newly proposed unified model. This demonstrates our approach is able to capture protein-protein interaction information despite it being designed for protein-ligand interaction modeling.\n\nYou can find detailed experiment settings and results in **Appendix C5 and C6**.\n\n[1] Corso et al., \"Diffdock: Diffusion steps, twists, and turns for molecular docking.\", ICLR 2023.\n[2] Masters et al., \"Deep learning model for efficient protein–ligand docking with implicit side-chain flexibility.\" Journal of Chemical Information and Modeling 63, no. 6 (2023).\n\n## Response to the issue of lack of latest baselines\n\nFor the pocket matching task, CoSP is a newly proposed baseline which is published in ECML PKDD 2023. Alongside CoSP and Uni-Mol, we selected some of the most effective baseline results presented in the CoSP paper. We also tried to evaluate recent methods like PocketAnchor (Li et al., Cell Systems 2022) on our own since they are not tested on the pocket matching task. However, since the result didn't outperform other baseline machine learning methods(Uni-Mol, CoSP, and DeeplyTough), we decided not to include it in our final baseline comparison. We can include it in the camera-ready version if needed.\n\nFor the ligand binding affinity task, a lot of newly proposed baselines are included, i.e. ProNet (Wang et al., 2022b); as well as pretraining methods such as GeoSSL (Liu et al., 2023), EGNN-PLM (Wu et al., 2022), DeepAffinity (Karimi et al., 2019) and Uni-Mol (Zhou et al., 2023).\n\n---\n\n## Response to lack of related works\n\nThanks for providing us with more insightful related works that could support our arguments. We actually have cited these three papers in the section on ligand binding affinity experiments, but we are also happy to add these in Related Works section 2.2. You can find the change in pdfdiff.\n\n## Response to lack of COSP as a baseline in Table 3\n\nBecause best to our knowledge, they did not release their code and they did not test their method on the ligand binding affinity dataset. \n\n## Response to the question on the loss terms\n\nThank you for your advice. We apologize that we accidentally mentioned loss1 and loss2 in reverse, and we are sorry for the confusing statement. You are correct that the first loss is to identify the true protein pocket when given a pseudo-ligand. We revise the original statement to: \"The primary purpose of the first loss is to identify the true protein pocket from a batch of samples when given a pseudo-ligand. Similarly, the second loss seeks to identify the corresponding ligand fragment for a given pocket.\" You can also find the change in pdfdiff.\n\n## Response to the question on the length of the pocket representation without alignment\n\nWith or without distributional alignment, pockets are always defined by the given protein fragment with a fixed distance cutoff (6Å in our works, following the UniMol setup). The distributional alignment process merely samples these pairs to match the sizes of real ligands and pockets. Without distributional alignment, fragments are uniformly sampled from 1 to 7 residues. Pockets are similarly determined as with residues within the range of 6Å around peptide fragments.\n\n---\n\n# Question about the confidence score\n\nThanks for your review. We found your understanding and judgment of our paper comprehensive and precise. Also, you provided detailed and thoughtful advice for our paper, which helped a lot. We sincerely appreciate your review, but we also wonder if there is any misunderstanding here, as you only give a confidence score of 1.\n\n---\n\n# Response 3/3\n\n## Response to biological justification and visualization of the results\n\nOur method's **biological justification** for achieving good experimental results is based on the properties and interactions of pseudo-ligands. As discussed in our paper, these pseudo-ligands are similar in size to real ligands and engage in comparable non-covalent interactions with protein pockets. This fundamental similarity is crucial for the effectiveness of our approach.\n\nMoreover, our methodology is further supported by the findings in the 2020 Science paper titled \"A defined structural unit enables de novo design of small-molecule–binding proteins.\"[1] This research underscores that **intra-protein interactions are analogous to protein-ligand interactions**, which validates our use of peptides as proxies for small molecules. By employing peptides as pseudo-ligands in this manner, our pocket encoder is able to learn and replicate the interaction dynamics typically observed in real ligand scenarios. This understanding is pivotal to the success of our method in downstream applications.\n\nTo better justify our methods and results, following your advice, we also provide some visualizations in **Appendix F**. **Figure 6** illustrates that the pseudo pairs we created share various interaction types commonly found in real pocket-ligand pairs, such as **hydrogen bonding, $\\pi-\\pi$ stacking, and salt bridge**. In the figures, each type of interaction is represented by dashed lines, color-coded to correspond with the specific interaction type.\n\nAnother visualization of our pocket-matching result is shown in **Figure 7**. We showed an example of two non-homology ATP binding pockets to explain why pocket-matching can benefit from interaction-aware pretraining so that we have achieved superior results in the Kahraman dataset and the BioLip t-SNE visualization. The PEP carboxykinase(PDB:1AYL) and tRNA synthetase(PDB:1B8A) are two ATP-binding proteins that share zero sequence similarity (verified with the BLAST) extracted from the Kahraman dataset. However, as they are both fueled by ATP, their binding site shares similar binding patterns. The cation-$\\pi$ interactions(blue dash lines) and salt bridges (magenta dash lines) are important to the ATP binding, which can be viewed as convergent evolutions at the molecule level. Even though, these two pockets are very distinct in terms of shapes and sizes because they bind ATPs in different conformations. Therefore, biochemical interactions are the key to accomplishing the pocket-matching task, which is ignored in previous self-supervised learning methods like Uni-Mol.\n\n[1] Polizzi, Nicholas F., and William F. DeGrado. \"A defined structural unit enables de novo design of small-molecule–binding proteins.\" Science 369, no. 6508 (2020)\n\n---\n\n# Response 2/3\n\n## Response to the bound of Theorem 3.1 \n\nWe are not very clear about the point of your question. We have two understandings and they are replied as follows.\n\nIf you mean $ ||t − t ^{(0)}|| < 1/2 l_T$ is trivial, please note that it is only a condition of the theorem. As you said, many other encoders can also satisfy this condition. This shows that our method is applicable to many different molecular encoders with or without pretraining as we proved in **Table 4**. Moreover, the condition is only a necessary condition. We also need the pretraining contrastive loss to be sufficiently small to guarantee a small contrastive loss between pockets and real ligands. In fact, in the process of our early exploration, we noticed that some encoders cannot achieve a low pretraining loss, indicating the entire conditions of the theorem are nontrivial.\n\nIf you mean that the conclusion of theorem 3.1 is trivial, we want to emphasize that our conclusion is $\\lim_{L_i(t, s) =0}L_i(t^{(0)}, s) =0$. (We write it in a $\\epsilon$-$\\delta$ language in paper. They are equivalent forms.) Our conclusion shows the loss containing real ligands is consistent with the loss containing pseudo ligands that we optimized in pre-training. \n\nIn contrast,  a trivial result from $||t − t ^{(0)}|| < 1/ 2l_T$ is that $\\lim_{L_i(t, s) =0}L_i(t^{(0)}, s) <B$, B is a bound related to l_t and the representations. It does not guarantee a consistent loss and is substantially different from our result.\nTherefore, our theorem is nontrivial and reveals the transfer ability of our contrastive pre-training from the pseudo-ligand domain to the real ligand domain.\n\nAs for this theoretical result, we are open to further discussion.\n\n## Response to molecules possess more properties that can also lead to imbalance\n\nIt is correct that molecules have more properties than their size, like logP, Hbond donor and acceptor number, and rotatory bond number. As small molecule drugs are mostly designed to penetrate barriers like gut or cell membranes, they usually have much larger logP values, which means more hydrophobic. For the same reason, they usually have fewer hydrogen bond donors and acceptors, and only minimal essential ones are kept for specified interactions. Also, to minimize the entropy effect upon binding and to increase binding affinity, rotatory bonds are also unfavored. However, it is impossible to mimic such features with peptides, as the backbone of peptides is intrinsically hydrophilic and flexible. As we showed in a new figure, our pseudo-ligands are less similar to real ligands in those properties even with size alignment. You can find the figure in the new pdf in **Appendix G**.\n\nWe believe that the aforementioned divergence is the main limitation of poor zero-shot performance in predicting hydrophobicity scores (**Table 1**). However, as demonstrated in previous publications like CoSP, we could leverage real ligand-pocket pairs from PDBBind or BioLip database to further finetune our network, as an extension of the pipeline. In this way, we could handle property mismatches but still enjoy the power of our large-scale pretraining.\n\n## Response to the approach to defining pockets \n\nWe define the pocket following the UniMol setup. We found our model can also adapt to an 8Å setup in the toughM1 experiment \n\nIn particular, we define the pocket for each protein-ligand pair as residues of the protein that have at least one atom within the range of 6Å from a heavy atom in the ligand. To further explain our design, we have done an ablation study on different choices of thresholds of 4Å, 6Å, and 8Å:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| 4Å | 0.7062 | 0.7549 | 0.1240 | 0.1095 | 28.29 | 13.07 |\n| 6Å | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| 8Å | 0.8322 | 0.8292 | 0.1256 | 0.1125 | 34.83 | 12.92 |\n\nSince the 8Å threshold corresponds with the pocket definition in the Kahraman and Tough M1 datasets, it leads to optimal results. Our decision to use a 6Å threshold was made to align with the methodology of pretraining data creation by Uni-Mol, facilitating a fair comparison. Notably, even with the 6Å threshold, we achieved strong results in the pocket-matching task, which serves as a testament to the effectiveness of our approach.\n\n---\n\n# Response 1/3\nWe appreciate the time and effort you have dedicated to reviewing our paper, and we are grateful for your constructive feedback and thoughtful evaluation of our work. We would like to address your comments about the technical novelty of our work and provide additional clarification on certain aspects of our paper.\n\n## Response to technical novelty\n\nFirstly, we acknowledge your observation that we didn't use any fancy models compared with typical **model-centric** works. While we respect your assessment, we would like to highlight that our primary focus in this paper is a **data-centric** pretraining method to introduce groundbreaking improvements in the field of protein pocket pretraining. Our work aims to address a critical challenge of data scarcity in the field of protein pocket representation by constructing large-scale synthetic data that facilitates the pretraining of models, ultimately enhancing the accuracy and robustness of protein pocket representations. We believe that the research community can conduct fast following-ups on our **released dataset** with more sophisticated models, and even extend our pipelines to other tasks like docking or drug design. We also would like to point out that we have provided a novel method to **distill knowledge from well-trained molecule models** to protein models. Though employing the same contrastive loss, our approach is different from existing contrastive learning models since it uses a fixed molecular encoder. The motivation here is to use a well-trained molecular encoder on a relatively larger dataset to **guide** the training of the protein encoder. For example, some quantum-chemistry properties that are difficult to compute for large systems like proteins could be distilled from molecules, by using some quantum-chemistry-aware molecular encoders such as Frad. Though our framework seems to be simple, it is non-trivial to make it work. We have made several efforts to solve the unavoidable discrepancies between true ligands and pseudo ligands. The ablation study shows that our efforts are effective and necessary:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nRegarding your positive acknowledgment of our \"simple but effective model for the right question,\" we are pleased to hear that our approach resonates with the objective we set out to achieve. We designed our model with **simplicity** in mind, prioritizing effectiveness and practical utility for the specific problem domain. This deliberate choice aligns with the notion that sometimes the most impactful solutions are elegantly straightforward. Also, we intentionally borrowed a pocket encoder from Uni-Mol to make a **fair comparison** with it, which strongly supports the power of our dataset. Similar to the molecule encoder, our pocket encoder can be changed to any other model due to our flexible framework. Notably, **we didn't load the pretrained weights of Uni-Mol pocket encoder**. We only use the same backbone architecture and the pretraining was completely done on our processed data with our training strategy. \n\nNotably, many simple but effective approaches have been recognized by top-tier conferences and journals. A prime example of this is in the field of protein language models, like the ESM series and ProtTrans. These studies adapted the Transformer architecture and masked language modeling techniques from natural language processing to protein sequences. While they didn't introduce groundbreaking techniques, their substantial contributions to protein modeling are evident, with publications in prestigious conferences and journals like NeurIPS, ICML, TPAMI, PNAS, and Science. Another example is the widely acclaimed CLIP paper presented at ICML 2021. CLIP, while not employing novel techniques, stands as a hallmark of data-centric deep learning. Its use of contrastive learning enabled training on expansive web-sourced text-image datasets, moving beyond the constraints of meticulously curated databases like MS-COCO. This aligns with our approach to addressing **data scarcity challenges** by facilitating training on large-scale datasets, mirroring our strategy for overcoming similar hurdles in the protein pocket modeling domain.\n\nWe appreciate your efforts to remain neutral and understand the challenges in decision-making. We believe further discussion will highlight our paper's contributions. Your feedback and insights in the upcoming discussion will be invaluable for refining our work.", "author_response": "# Response 1/3\nWe appreciate the time and effort you have dedicated to reviewing our paper, and we are grateful for your constructive feedback and thoughtful evaluation of our work. We would like to address your comments about the technical novelty of our work and provide additional clarification on certain aspects of our paper.\n\n## Response to technical novelty\n\nFirstly, we acknowledge your observation that we didn't use any fancy models compared with typical **model-centric** works. While we respect your assessment, we would like to highlight that our primary focus in this paper is a **data-centric** pretraining method to introduce groundbreaking improvements in the field of protein pocket pretraining. Our work aims to address a critical challenge of data scarcity in the field of protein pocket representation by constructing large-scale synthetic data that facilitates the pretraining of models, ultimately enhancing the accuracy and robustness of protein pocket representations. We believe that the research community can conduct fast following-ups on our **released dataset** with more sophisticated models, and even extend our pipelines to other tasks like docking or drug design. We also would like to point out that we have provided a novel method to **distill knowledge from well-trained molecule models** to protein models. Though employing the same contrastive loss, our approach is different from existing contrastive learning models since it uses a fixed molecular encoder. The motivation here is to use a well-trained molecular encoder on a relatively larger dataset to **guide** the training of the protein encoder. For example, some quantum-chemistry properties that are difficult to compute for large systems like proteins could be distilled from molecules, by using some quantum-chemistry-aware molecular encoders such as Frad. Though our framework seems to be simple, it is non-trivial to make it work. We have made several efforts to solve the unavoidable discrepancies between true ligands and pseudo ligands. The ablation study shows that our efforts are effective and necessary:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nRegarding your positive acknowledgment of our \"simple but effective model for the right question,\" we are pleased to hear that our approach resonates with the objective we set out to achieve. We designed our model with **simplicity** in mind, prioritizing effectiveness and practical utility for the specific problem domain. This deliberate choice aligns with the notion that sometimes the most impactful solutions are elegantly straightforward. Also, we intentionally borrowed a pocket encoder from Uni-Mol to make a **fair comparison** with it, which strongly supports the power of our dataset. Similar to the molecule encoder, our pocket encoder can be changed to any other model due to our flexible framework. Notably, **we didn't load the pretrained weights of Uni-Mol pocket encoder**. We only use the same backbone architecture and the pretraining was completely done on our processed data with our training strategy. \n\nNotably, many simple but effective approaches have been recognized by top-tier conferences and journals. A prime example of this is in the field of protein language models, like the ESM series and ProtTrans. These studies adapted the Transformer architecture and masked language modeling techniques from natural language processing to protein sequences. While they didn't introduce groundbreaking techniques, their substantial contributions to protein modeling are evident, with publications in prestigious conferences and journals like NeurIPS, ICML, TPAMI, PNAS, and Science. Another example is the widely acclaimed CLIP paper presented at ICML 2021. CLIP, while not employing novel techniques, stands as a hallmark of data-centric deep learning. Its use of contrastive learning enabled training on expansive web-sourced text-image datasets, moving beyond the constraints of meticulously curated databases like MS-COCO. This aligns with our approach to addressing **data scarcity challenges** by facilitating training on large-scale datasets, mirroring our strategy for overcoming similar hurdles in the protein pocket modeling domain.\n\nWe appreciate your efforts to remain neutral and understand the challenges in decision-making. We believe further discussion will highlight our paper's contributions. Your feedback and insights in the upcoming discussion will be invaluable for refining our work."}
{"claim": "The authors considered only ligand size and pocket size distributions when designing the dataset, omitting other potentially relevant molecular properties.", "claim_type": "methodology", "paper_id": "uMAujpVi9m", "paper_title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "9ap4s1SZlZ", "reviewer": "Reviewer_yfM3", "review_text": "Summary: This paper primarily aims to enhance the pocket pretraining method, as existing approaches only consider pockets during pretraining. There are two main contributions in this paper: (1) The authors introduce a novel method, ProFSA, for pocket pretraining, which extracts additional information from corresponding ligands. However, the number of pocket-ligand complex structures is quite limited in existing datasets. (2) To address this issue, the authors generate over 5 million complexes by segmenting fragments and their corresponding pockets in protein structures. By aligning features of fragments and pockets, the pocket encoder learns the interaction between fragments and pockets. The authors design downstream tasks such as pocket druggability prediction, pocket matching, and ligand binding affinity prediction to demonstrate the effectiveness of ProFSA.\n\nStrengths: The authors propose a new perspective of pretraining pockets and construct a large-scale dataset, which data distribution is also considered, to make the efficient pre-training possible.\n\nThe results are competitive, especially for zero-shot settings.\n\nAbundant experiments and ablation study support the argument and result of the authors.\n\nWeaknesses: 1. The technical novelty is limited.\n  - The pocket encoder is borrowed from Uni-Mol.\n  - The contrastive loss is the vanilla form of classical contrastive learning.\n\n2. The bound of Theorem 3.1 is trivial. The authors claim that the bound naturally exists for these representations extracted by pretrained molecule models. However, it's a bit counterintuitive, because many models not pretrained on molecule datasets also fulfill this prior. So, can these models be used for this task? **I strongly suggest removing this part from the paper**.\n\n3. Some issues about dataset creation:\n - 3.1. The authors consider the distribution of ligand size and pocket size when designing the dataset. However, molecules possess more properties that can also lead to imbalance. It would be better to, at least, add some discussion about this issue.\n - 3.2. In the second stage of the data construction process, the approach to defining pockets needs further explanation or an ablation study.\n\n4. Experiments: It would be better to add some biological justification or visualization of the results.\n\nFor this paper, one fact is that the technical novelty is below the bar of ICLR. However, I admire the simple but effective model for the right question. It's a struggle for me to make a decision. I will maintain a neutral attitude and make my final decision after the discussion.\n\nQuestions: See weakness.", "labeling_timestamp": "2026-01-11T16:25:37.618977", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors explicitly acknowledge that they primarily aligned size distributions and admit other molecular properties (e.g., logP, H-bond donors/acceptors, rotatable bonds, hydrophobicity) were not mimicked with peptides; they describe this as a limitation and report mitigation (distribution alignment, fixed encoder, finetuning) and provide plots in an appendix.", "evidence": "\"It is correct that molecules have more properties than their size, like logP, Hbond donor and acceptor number, and rotatory bond number... However, it is impossible to mimic such features with peptides, as the backbone of peptides is intrinsically hydrophilic and flexible. As we showed in a new figure, our pseudo-ligands are less similar to real ligands in those properties even with size alignment. You can find the figure in the new pdf in Appendix G.\" and \"To mitigate the impact of this discrepancy, we have made several efforts. First, we performed distribution alignment to make the data distribution of pseudo-ligands more similar to that of true ligands in PDBbind. Additionally, we fixed the molecule encoder during pretraining to prevent it from being misled by the discrepancy.\"", "combined_author_response": "Dear AC and reviewers,\n\nWe sincerely appreciate your time and efforts in reviewing our work. Based on your suggestions, we have revised our paper. We would like to use this section to reiterate the explanation for some common concerns and summarize the contributions of our paper.\n\nIn our revised paper, we provide a detailed explanation of the motivation and theory behind our method: **the mechanisms of pocket-peptide interactions and pocket-ligand interactions are very similar**. Our proposed **data-centric** pretraining pipeline is unique in its use of peptides to mimic small molecules, a method that is, to our knowledge, **the first of its kind**. We expanded our discussion about its biological insights in **Section 3**. We add a **new Figure 2 to illustrate different types of non-covalent interactions** that are common for both ligand-protein data and intra-protein data. Following this theory, we have designed a pipeline to generate **over 5 million** synthetic ligand-protein pairs, which greatly eases the **data scarcity** problem in the field of protein-ligand interaction learning. We also prove our data quite effective in various downstream tasks, including pocket property prediction, pocket matching, ligand binding affinity prediction, ligand efficiency prediction, and protein-protein interaction tasks. The **last two tasks are newly added** to further support our method (**Appendix C5 and C6**). We are confident that our **ready-to-release synthetic dataset** will significantly benefit the AI for drug discovery community. Its impact extends beyond enhancing pocket representations in downstream analyses, as it also equips researchers with a valuable resource for a range of applications, such as protein-ligand docking, structure-based drug design, and virtual screening. Importantly, these areas often face challenges due to **a lack of training data necessary for learning effective binding patterns between protein pockets and ligands**. \n\nIn our revised paper, we recognize the differences between real ligands and pseudo ligands (peptides). We address these discrepancies and present a distribution plot of various properties in **Appendix G**. Our approach primarily leverages the **analogous nature of ligand-pocket and peptide-pocket interactions**. Therefore, despite these differences, our model can **still extract valuable interaction information from the pretraining dataset**, which enhances its performance in downstream tasks. This is illustrated in **Figure 4(a)**, showing the benefits of interaction-aware pretraining. We have also undertaken measures like distribution alignment and freezing the molecular encoder to mitigate the impact of these discrepancies. The positive results of these efforts, as evidenced in **Table 5**, confirm their effectiveness and necessity. Notably, our pretrained model can be **further finetuned with real labeled ligand-pocket pairwise data**, as we show in the Ligand Binding Affinity task in **section 4.3**.\n\nAs for other questions, we have added more ablation studies on our design choices, including the impact of a fixed molecular encoder (Table 5), and different threshold values to define pockets(Appendix C.4). \n\nWe are immensely grateful for the invaluable feedback from all the reviewers, which has guided us in refining and clarifying our work. We hope that the revised version of our paper, coupled with the discussion period, will more clearly highlight the novelty, effectiveness, and contributions of our approach to the field.\n\n---\n\nThank you for your invaluable feedback on our paper. In response to your insightful suggestions, we have made several modifications in our revised paper. \n\nWe have cited additional papers in Section 2.2 following your advice. The description of loss terms in Section 3.2 has been corrected.  COSP was not included as a baseline for the Ligand Binding Affinity (LBA)  task because it is not open-sourced, and its performance on LBA was not evaluated in their published work. We used it as a baseline in the pocket-matching task.\n\nIn Section 3, we've expanded our discussion to answer your question on **ProFSA's effectiveness without distributional alignment**. This section now offers a detailed explanation of the foundational concepts and justifications for our method. We draw attention to the parallels between ligand-protein and intra-protein non-covalent interactions, as shown in **Figure 2**. This comparison supports our strategy of using peptides as stand-ins for actual ligands to mimic pocket-ligand interactions. Consequently, **even in the absence of distributional alignment, the types of interactions remain comparably relevant**, which explains why ProFSA continues to function, albeit with reduced effectiveness, as demonstrated in Table 5. We also want to clarify that distributional alignment is only applied to fragment-pocket complex as a whole, and **it would not change the definition of the pocket with a given ligand**. Without this alignment, these fragments are chosen uniformly, with lengths varying from 1 to 7 residues, and pockets are consistently identified based on residues within a 6Å radius of these peptide fragments.\n\nWe are grateful for the time and effort you have dedicated to reviewing our work. Your thorough and constructive feedback has significantly contributed to the refinement of our research. Please let us know if there are any other aspects of our paper that you would like us to address or clarify. We look forward to your further feedback.\n\n---\n\nThank you for your insightful feedback. In response to your concerns regarding the scope of our tasks, we have expanded our research in the revised paper. We now include additional tasks such as **ligand efficiency prediction** and **protein-protein affinity prediction**, detailed in **Appendix C5 and C6**. These tasks were incorporated to enhance the robustness of our method. We acknowledge that they are currently in the appendix due to space constraints, but we are open to relocating them to the main text if it would be beneficial.\n\nRegarding your concern about the lack of recent baselines, we provided a detailed explanation in our previous response. We want to reiterate that our benchmarks do include the latest baselines, with some as recent as 2023.\n\nWe greatly appreciate the time and effort you have invested in reviewing our work. Your feedback has been invaluable in refining our research. Please let us know if there are any other aspects of our paper that you would like us to address or clarify. We look forward to your further feedback.\n\n---\n\nIn our revised paper, we have incorporated changes following feedback from another reviewer, which we believe also addresses your concerns and queries.\n\nRegarding your question on the evidence supporting our approach, particularly the use of peptides to simulate small molecules, we've enriched **Section 3.1**. A new paragraph there now provides an in-depth explanation of non-covalent interactions. To aid understanding, **Figure 2** has been updated with visualizations that include both real and pseudo receptor-ligand pairs, illustrating three types of interactions. Furthermore, we delve into the details of **how intra-protein interactions mirror protein-ligand interactions**, involving specific types of amino acids.\n\nTo better evaluate our design choices, the ablation studies have been extended. Alongside the existing studies in **Table 4** (different pretrained molecular encoders), **Table 5** (distribution alignment), and **Figure 5** (data scale). we've added a study on the impact of using a fixed molecular encoder in **Table 5** in the main text to address your question on the fixed molecular encoder. We've also included an ablation study on varying **cutoff values for pocket definition**, placed in the **Appendix C.4** due to its more focused scope. We can move it to the main part of the paper in the future if needed.\n\nWe acknowledge your concern about the **differences between true and pseudo ligands**, so we've expanded our discussion on the distinct properties of our curated dataset versus the PDBBind dataset in **Figure 9** and **Section 3.2**. The ablation study in **Table 5** demonstrates our methods and their effectiveness in minimizing these discrepancies. Notably, the performance significantly drops without our distribution alignment or fixed molecular encoder, highlighting our efforts to mitigate these issues. The updated **Figure 2** further supports this, showing that while the properties of true and pseudo ligands may differ, **the interaction types are consistent.** We hope these results can ease your concerns about our synthetic dataset, and they also respond to your query about **why we kept the molecular encoder constant during pocket pretraining.**\n\nWe appreciate the constructive critiques and thoughtful engagement from you. Your feedback has undeniably contributed to the refinement and strengthening of our paper. We sincerely value the time and expertise invested in the review process, and we look forward to any further suggestions you may have.\n\n---\n\nWe sincerely appreciate the thoughtful feedback provided on our paper and have diligently incorporated your suggestions into this revised version. In response to your guidance, we have made three significant modifications to the core content of our paper, all of which aim to enhance clarity and address the concerns raised.\n\nTaking into account your suggestion, we have removed **Theorem 3.1** from the main body of our paper. Additionally, we've improved the explanation and included an ablation study on our method of defining pockets. This ablation study, focusing on the cutoff values and primarily a hyperparameter issue, is placed in the Appendix due to its more limited scope of insight. We can move it to the main body of the paper in the future if needed. We have also expanded our discussion on the differences between our dataset and the PDBBind dataset, elucidating how fixed molecule encoders contribute to mitigating this discrepancy.\n\nIn **Section 3.1**, we have introduced a new paragraph dedicated to providing readers with a deeper understanding of non-covalent interactions. To facilitate comprehension, we have included visualizations featuring three types of interactions, incorporating both real and pseudo receptor-ligand pairs in the new **Figure 2**. Furthermore, we delve into the details of how intra-protein interactions mirror protein-ligand interactions, involving specific types of amino acids.\n\nThe biological justifications for the efficacy of interaction-aware pretraining are now more extensively explored in our revised paper. Specifically, we present a compelling showcase featuring estradiol-binding proteins, offering insights into the geometric disparities between the compared proteins and highlighting the similarities in their binding interfaces in the new **Figure 4a**. The existing visualization of BioLip pocket representations in the new **Figure 4b** can make our claims more convincing. Together, these visual aids illustrate how our interaction-aware pretraining empowers models to focus on crucial interface residues, discern subtle distinctions, and disregard irrelevant geometric dissimilarities.\n\nWe have also included an in-depth discussion of pretrained molecule encoders in the newly added **Section 2.3**, further enriching the theoretical foundation of our work.\nAll changes can be found in the renewed paper, which is updated lively.\n\nWe genuinely value the constructive input you provided during the review process, as it has undoubtedly contributed to the refinement of our paper. Your insights have proven invaluable in elevating the clarity and depth of our contributions to the field. We express our gratitude for the time and expertise invested in reviewing our work, and we look forward to any additional feedback or suggestions you may have.\n\n---\n\n# Response 2/2\n\n## Response to the question on N Terminal and C Terminal\n\nWhen a peptide bond is formed (green), one amine group and one carboxy group are left untouched. Therefore, in a linear polypeptide, there would always be a free amine group and a free carboxy group, and they are called the N terminal(blue) and the C terminal(red) respectively in biochemistry. You can find the visualization in **Appendix H**.\n\n## Response to fixing the molecule encoder in contrastive learning\n\nFrom an **effectiveness** standpoint, as you mentioned before, there are unavoidable discrepancies between pseudo and true ligands. If we didn't fix our molecule encoder during pretraining, then the conditions of Theorem 3.1 might not be met. The ablation results presented in the previous response clearly indicate a performance decline when the molecular encoder is not fixed during contrastive learning, particularly in the context of pocket matching. This supports our assertion that the pocket encoder struggles to acquire chemical knowledge without the stability provided by a fixed molecular encoder.\n\nFrom an **efficiency** standpoint, a fixed molecule encoder also offers significant advantages. It reduces memory usage and accelerates training speed because all molecule embeddings can be pre-computed before training. This eliminates the need for feed-forward and back-propagation operations of the molecule encoder during training, leading to efficiency gains. Additionally, adapting to different molecule encoders becomes more straightforward when they are not co-trained during the pretraining process. This approach allows for more flexibility and ease in integrating various encoding methodologies **even if the molecule encoder is not accessible for training**. That is to say, our approach can even be used with undifferentiable molecular representations.\n\n## Response to extending to other tasks like protein-protein interaction prediction\n\nThank you for pointing out the potential application of our framework to protein-protein interaction (PPI) tasks. We have to point out that the interactions between proteins and proteins are far different from proteins and molecules. Specifically, the interaction areas are usually much larger, but shallower, and involved functional groups are less diverse due to limited types of natural amino acids. Therefore, several changes should be made to extend our ProFSA method to PPI tasks:\n\n1. As for data creation, unlike the localized and specific nature of pocket-ligand interactions, protein-protein interactions are **more global and involve broader protein features**. This requires a shift in the data creation process to split entire protein structures into domains instead of short fragments and to capture potential interactions between them. \n\n2. The interaction interface between protein and protein is much larger. We might need to change from atomic-level to residue-level representations to efficiently capture interaction information.\n\n3. For contrastive learning training, ProFSA uses a fixed pretrained molecular encoder to cope with various small molecules and their atomic representation. However, for protein-protein interaction, it is not necessary as we could only model 20 types of amino acids at the residue level.\n\nDespite several changes that need to be done for PPI tasks, we find that we could divide protein complexes into local interactions. When the complex structures and monomer structures are provided, we have achieved a reasonable result on a flexible PPI affinity prediction benchmark with our current model in a zero-shot way. \n\n| Method            | Spearman $\\uparrow$  |\n|-------------------|----------------------|\n| SchNet            | 0.072 ± 0.021        |\n| DimeNet++         | 0.171 ± 0.054        |\n| EGNN              | 0.080 ± 0.038        |\n| TorchMD           | 0.117 ± 0.008        |\n| GET               | **0.363 ± 0.017**    |\n| ProFSA(zero-shot) | _0.248_              |\n\nSurprisingly, as a **zero-shot** method, ProFSA is able to outperform other **supervised learning** models except for GET, a newly proposed unified model. This demonstrates our approach is also able to capture protein-protein interaction information despite it being designed for protein-ligand interaction. \n\nA detailed explanation of this experiment is shown in **Appendix C6**.\n\n---\n\n# Response 1/2\n\n## Response to evidence for the construction of the pseudo-ligand and using peptides to replace small molecules \n\nAs we discussed in the paper, pseudo-ligands share similar sizes as real ligands, and they also make similar non-covalent interactions with the pocket. To further support our proposal, we would like to cite another paper **\"A defined structural unit enables de novo design of small-molecule–binding proteins\"**[1], published in Science 2020, which supports that **intra-protein interactions are similar to protein-ligand interactions.** That is the reason we use peptides to **represent** small molecules to let the pocket encoder learn the interaction information. \n\nAlso, we'd like to clarify that our intention is not to **\"replace\"** small molecules with peptides. It's a pretraining framework that leverages abundant protein-only data and uses peptides to **simulate** pocket-small molecule interactions for enhanced pocket representations. Following this pretraining phase, **the model can be finetuned with datasets that include real small molecules, ensuring its effectiveness in practical applications**.\n\nTo demonstrate the validity of constructing pseudo-ligands, we provide visualizations in **Appendix F** showing that interactions between pockets and our pseudo-ligands are similar to those with true ligands. These figures reveal shared interaction types like **hydrogen bonding**, **$\\pi-\\pi$ stacking**, and **salt bridge**, depicted with color-coded dashed lines for each interaction type. This should help clarify the rationale and evidence supporting our approach.\n\n[1] Polizzi, Nicholas F., and William F. DeGrado. \"A defined structural unit enables de novo design of small-molecule–binding proteins.\" Science 369, no. 6508 (2020)\n\n## Response to ablation studies evaluating the impact of critical design choices\n\nThanks for your advice on adding more ablation studies on design choices. We did ablation studies on the effectiveness of distributional alignment, where the major difference is that fragment sizes are modified. We found that aligned fragment sizes with real ligands could provide the best performance. The result is shown in **Table 5**.\n\nAs for the distance thresholds, we define the pocket following the UniMol setup. We also found our model can also adapt to an 8Å setup in the ToughM1 experiment. Following your advice, we did an ablation study on the distance thresholds. We tested our method with three different thresholds: 4Å, 6Å, and 8Å. The result is shown below:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| 4Å | 0.7062 | 0.7549 | 0.1240 | 0.1095 | 28.29 | 13.07 |\n| 6Å | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| 8Å | 0.8322 | 0.8292 | 0.1256 | 0.1125 | 34.83 | 12.92 |\n\nSince the 8Å threshold corresponds with the pocket definition in the Kahraman and Tough M1 datasets, it leads to optimal results. Our decision to use a 6Å threshold was made to align with the methodology of pretraining data creation by Uni-Mol, facilitating a fair comparison. Notably, even with the 6Å threshold, we achieved strong results in the pocket-matching task, which serves as a testament to the effectiveness of our approach.\n\n## Response to substantial discrepancies between pseudo and real ligands\n\nThank you for pointing out the discrepancies between pseudo and real ligands. We understand and acknowledge your concern. To mitigate the impact of this discrepancy, we have made several efforts. First, we performed **distribution alignment** to make the data distribution of pseudo-ligands more similar to that of true ligands in PDBbind. Additionally, we **fixed the molecule encoder during pretraining** to prevent it from being misled by the discrepancy. Our ablation studies have shown that these strategies are effective:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nThe table above shows that lacking distribution alignment and not fixing the molecular encoder during pretraining both lead to lower performance, underscoring the effectiveness of our methods in reducing discrepancies.\n\nTheorem 3.1 theoretically supports our approach's efficacy, even with discrepancies between pseudo and real ligands. Empirically, our method outperforms other pretraining techniques, confirming its effectiveness despite these discrepancies.\n\n---\n\n## Response to limited tasks and potential in protein-ligand binding pose prediction\n\nIn our paper, the primary objective is to assess the effectiveness of our pretraining method and the quality of the trained pocket representations. To achieve this, we focused on two types of downstream tasks: pocket-only tasks (pocket property prediction and pocket matching) and pocket-ligand interaction tasks (ligand binding affinity prediction).\n\nWe appreciate the suggestion to apply our method to the protein-ligand binding pose prediction task. However, it's important to note that our current evaluation framework is designed to specifically assess **pocket representations**. For the tasks we chose, the architecture is straightforward: either a **zero-shot** evaluation or a **simple MLP** for mapping embeddings to predictions. In contrast, protein-ligand binding pose prediction often involves **complex methodologies**. For instance, state-of-the-art methods like DiffDock[1] require training a diffusion generative model and a separate confidence model, while other approaches like EDM-Dock[2], rely on reconstructing ligand conformations from predicted distance maps. These methods are not end-to-end and do not directly align with our objective of evaluating pretrained pocket representations.\n\nNevertheless, we recognize the potential of our method in enhancing existing binding pose prediction techniques. To integrate our approach, we would need to modify our framework. Using the same data creation strategy, we could train a binding pose prediction model with our preprocessed data, which could then be further fine-tuned using real pocket-ligand pair data from sources like PDBbind. Additionally, our data could be used to train a side-chain packing model, allowing for side-chain flexibility during docking. Thank you again for your advice, and we will leave protein-ligand binding pose prediction as a separate future work for our method.\n\nIn response to the need for evaluating our method on a broader range of tasks, we have extended our analysis to include two additional downstream tasks: LEP (Ligand Efficacy Prediction), and PPA (Protein-Protein Affinity Prediction).\n\nresult for LEP:\n\n| Method      | AUROC $\\uparrow$ | AUPRC $\\uparrow$ |\n|-------------|--------------|--------------|\n| ATOM3D-GNN  | 0.681        | 0.598        |\n| GeoSSL      | 0.776±0.03   | 0.694±0.06   |\n| Uni-Mol     | 0.782±0.02   | 0.695±0.07   |\n| ProFSA      | 0.840±0.04   | 0.806±0.04   |\n\nGeoSSL and Uni-Mol, both pretraining methods, yield comparable results. However, ProFSA outperforms these methods, demonstrating the advantage of our pocket pretraining approach.\n\nresult for PPA:\n\n| Method            | Spearman $\\uparrow$  |\n|-------------------|----------------------|\n| SchNet            | 0.072 ± 0.021        |\n| DimeNet++         | 0.171 ± 0.054        |\n| EGNN              | 0.080 ± 0.038        |\n| TorchMD           | 0.117 ± 0.008        |\n| GET               | **0.363 ± 0.017**    |\n| ProFSA(zero shot) | _0.248_              |\n\nAs a **zero-shot** method, ProFSA is able to outperform other **supervised learning** models except for GET, a newly proposed unified model. This demonstrates our approach is able to capture protein-protein interaction information despite it being designed for protein-ligand interaction modeling.\n\nYou can find detailed experiment settings and results in **Appendix C5 and C6**.\n\n[1] Corso et al., \"Diffdock: Diffusion steps, twists, and turns for molecular docking.\", ICLR 2023.\n[2] Masters et al., \"Deep learning model for efficient protein–ligand docking with implicit side-chain flexibility.\" Journal of Chemical Information and Modeling 63, no. 6 (2023).\n\n## Response to the issue of lack of latest baselines\n\nFor the pocket matching task, CoSP is a newly proposed baseline which is published in ECML PKDD 2023. Alongside CoSP and Uni-Mol, we selected some of the most effective baseline results presented in the CoSP paper. We also tried to evaluate recent methods like PocketAnchor (Li et al., Cell Systems 2022) on our own since they are not tested on the pocket matching task. However, since the result didn't outperform other baseline machine learning methods(Uni-Mol, CoSP, and DeeplyTough), we decided not to include it in our final baseline comparison. We can include it in the camera-ready version if needed.\n\nFor the ligand binding affinity task, a lot of newly proposed baselines are included, i.e. ProNet (Wang et al., 2022b); as well as pretraining methods such as GeoSSL (Liu et al., 2023), EGNN-PLM (Wu et al., 2022), DeepAffinity (Karimi et al., 2019) and Uni-Mol (Zhou et al., 2023).\n\n---\n\n## Response to lack of related works\n\nThanks for providing us with more insightful related works that could support our arguments. We actually have cited these three papers in the section on ligand binding affinity experiments, but we are also happy to add these in Related Works section 2.2. You can find the change in pdfdiff.\n\n## Response to lack of COSP as a baseline in Table 3\n\nBecause best to our knowledge, they did not release their code and they did not test their method on the ligand binding affinity dataset. \n\n## Response to the question on the loss terms\n\nThank you for your advice. We apologize that we accidentally mentioned loss1 and loss2 in reverse, and we are sorry for the confusing statement. You are correct that the first loss is to identify the true protein pocket when given a pseudo-ligand. We revise the original statement to: \"The primary purpose of the first loss is to identify the true protein pocket from a batch of samples when given a pseudo-ligand. Similarly, the second loss seeks to identify the corresponding ligand fragment for a given pocket.\" You can also find the change in pdfdiff.\n\n## Response to the question on the length of the pocket representation without alignment\n\nWith or without distributional alignment, pockets are always defined by the given protein fragment with a fixed distance cutoff (6Å in our works, following the UniMol setup). The distributional alignment process merely samples these pairs to match the sizes of real ligands and pockets. Without distributional alignment, fragments are uniformly sampled from 1 to 7 residues. Pockets are similarly determined as with residues within the range of 6Å around peptide fragments.\n\n---\n\n# Question about the confidence score\n\nThanks for your review. We found your understanding and judgment of our paper comprehensive and precise. Also, you provided detailed and thoughtful advice for our paper, which helped a lot. We sincerely appreciate your review, but we also wonder if there is any misunderstanding here, as you only give a confidence score of 1.\n\n---\n\n# Response 3/3\n\n## Response to biological justification and visualization of the results\n\nOur method's **biological justification** for achieving good experimental results is based on the properties and interactions of pseudo-ligands. As discussed in our paper, these pseudo-ligands are similar in size to real ligands and engage in comparable non-covalent interactions with protein pockets. This fundamental similarity is crucial for the effectiveness of our approach.\n\nMoreover, our methodology is further supported by the findings in the 2020 Science paper titled \"A defined structural unit enables de novo design of small-molecule–binding proteins.\"[1] This research underscores that **intra-protein interactions are analogous to protein-ligand interactions**, which validates our use of peptides as proxies for small molecules. By employing peptides as pseudo-ligands in this manner, our pocket encoder is able to learn and replicate the interaction dynamics typically observed in real ligand scenarios. This understanding is pivotal to the success of our method in downstream applications.\n\nTo better justify our methods and results, following your advice, we also provide some visualizations in **Appendix F**. **Figure 6** illustrates that the pseudo pairs we created share various interaction types commonly found in real pocket-ligand pairs, such as **hydrogen bonding, $\\pi-\\pi$ stacking, and salt bridge**. In the figures, each type of interaction is represented by dashed lines, color-coded to correspond with the specific interaction type.\n\nAnother visualization of our pocket-matching result is shown in **Figure 7**. We showed an example of two non-homology ATP binding pockets to explain why pocket-matching can benefit from interaction-aware pretraining so that we have achieved superior results in the Kahraman dataset and the BioLip t-SNE visualization. The PEP carboxykinase(PDB:1AYL) and tRNA synthetase(PDB:1B8A) are two ATP-binding proteins that share zero sequence similarity (verified with the BLAST) extracted from the Kahraman dataset. However, as they are both fueled by ATP, their binding site shares similar binding patterns. The cation-$\\pi$ interactions(blue dash lines) and salt bridges (magenta dash lines) are important to the ATP binding, which can be viewed as convergent evolutions at the molecule level. Even though, these two pockets are very distinct in terms of shapes and sizes because they bind ATPs in different conformations. Therefore, biochemical interactions are the key to accomplishing the pocket-matching task, which is ignored in previous self-supervised learning methods like Uni-Mol.\n\n[1] Polizzi, Nicholas F., and William F. DeGrado. \"A defined structural unit enables de novo design of small-molecule–binding proteins.\" Science 369, no. 6508 (2020)\n\n---\n\n# Response 2/3\n\n## Response to the bound of Theorem 3.1 \n\nWe are not very clear about the point of your question. We have two understandings and they are replied as follows.\n\nIf you mean $ ||t − t ^{(0)}|| < 1/2 l_T$ is trivial, please note that it is only a condition of the theorem. As you said, many other encoders can also satisfy this condition. This shows that our method is applicable to many different molecular encoders with or without pretraining as we proved in **Table 4**. Moreover, the condition is only a necessary condition. We also need the pretraining contrastive loss to be sufficiently small to guarantee a small contrastive loss between pockets and real ligands. In fact, in the process of our early exploration, we noticed that some encoders cannot achieve a low pretraining loss, indicating the entire conditions of the theorem are nontrivial.\n\nIf you mean that the conclusion of theorem 3.1 is trivial, we want to emphasize that our conclusion is $\\lim_{L_i(t, s) =0}L_i(t^{(0)}, s) =0$. (We write it in a $\\epsilon$-$\\delta$ language in paper. They are equivalent forms.) Our conclusion shows the loss containing real ligands is consistent with the loss containing pseudo ligands that we optimized in pre-training. \n\nIn contrast,  a trivial result from $||t − t ^{(0)}|| < 1/ 2l_T$ is that $\\lim_{L_i(t, s) =0}L_i(t^{(0)}, s) <B$, B is a bound related to l_t and the representations. It does not guarantee a consistent loss and is substantially different from our result.\nTherefore, our theorem is nontrivial and reveals the transfer ability of our contrastive pre-training from the pseudo-ligand domain to the real ligand domain.\n\nAs for this theoretical result, we are open to further discussion.\n\n## Response to molecules possess more properties that can also lead to imbalance\n\nIt is correct that molecules have more properties than their size, like logP, Hbond donor and acceptor number, and rotatory bond number. As small molecule drugs are mostly designed to penetrate barriers like gut or cell membranes, they usually have much larger logP values, which means more hydrophobic. For the same reason, they usually have fewer hydrogen bond donors and acceptors, and only minimal essential ones are kept for specified interactions. Also, to minimize the entropy effect upon binding and to increase binding affinity, rotatory bonds are also unfavored. However, it is impossible to mimic such features with peptides, as the backbone of peptides is intrinsically hydrophilic and flexible. As we showed in a new figure, our pseudo-ligands are less similar to real ligands in those properties even with size alignment. You can find the figure in the new pdf in **Appendix G**.\n\nWe believe that the aforementioned divergence is the main limitation of poor zero-shot performance in predicting hydrophobicity scores (**Table 1**). However, as demonstrated in previous publications like CoSP, we could leverage real ligand-pocket pairs from PDBBind or BioLip database to further finetune our network, as an extension of the pipeline. In this way, we could handle property mismatches but still enjoy the power of our large-scale pretraining.\n\n## Response to the approach to defining pockets \n\nWe define the pocket following the UniMol setup. We found our model can also adapt to an 8Å setup in the toughM1 experiment \n\nIn particular, we define the pocket for each protein-ligand pair as residues of the protein that have at least one atom within the range of 6Å from a heavy atom in the ligand. To further explain our design, we have done an ablation study on different choices of thresholds of 4Å, 6Å, and 8Å:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| 4Å | 0.7062 | 0.7549 | 0.1240 | 0.1095 | 28.29 | 13.07 |\n| 6Å | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| 8Å | 0.8322 | 0.8292 | 0.1256 | 0.1125 | 34.83 | 12.92 |\n\nSince the 8Å threshold corresponds with the pocket definition in the Kahraman and Tough M1 datasets, it leads to optimal results. Our decision to use a 6Å threshold was made to align with the methodology of pretraining data creation by Uni-Mol, facilitating a fair comparison. Notably, even with the 6Å threshold, we achieved strong results in the pocket-matching task, which serves as a testament to the effectiveness of our approach.\n\n---\n\n# Response 1/3\nWe appreciate the time and effort you have dedicated to reviewing our paper, and we are grateful for your constructive feedback and thoughtful evaluation of our work. We would like to address your comments about the technical novelty of our work and provide additional clarification on certain aspects of our paper.\n\n## Response to technical novelty\n\nFirstly, we acknowledge your observation that we didn't use any fancy models compared with typical **model-centric** works. While we respect your assessment, we would like to highlight that our primary focus in this paper is a **data-centric** pretraining method to introduce groundbreaking improvements in the field of protein pocket pretraining. Our work aims to address a critical challenge of data scarcity in the field of protein pocket representation by constructing large-scale synthetic data that facilitates the pretraining of models, ultimately enhancing the accuracy and robustness of protein pocket representations. We believe that the research community can conduct fast following-ups on our **released dataset** with more sophisticated models, and even extend our pipelines to other tasks like docking or drug design. We also would like to point out that we have provided a novel method to **distill knowledge from well-trained molecule models** to protein models. Though employing the same contrastive loss, our approach is different from existing contrastive learning models since it uses a fixed molecular encoder. The motivation here is to use a well-trained molecular encoder on a relatively larger dataset to **guide** the training of the protein encoder. For example, some quantum-chemistry properties that are difficult to compute for large systems like proteins could be distilled from molecules, by using some quantum-chemistry-aware molecular encoders such as Frad. Though our framework seems to be simple, it is non-trivial to make it work. We have made several efforts to solve the unavoidable discrepancies between true ligands and pseudo ligands. The ablation study shows that our efforts are effective and necessary:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nRegarding your positive acknowledgment of our \"simple but effective model for the right question,\" we are pleased to hear that our approach resonates with the objective we set out to achieve. We designed our model with **simplicity** in mind, prioritizing effectiveness and practical utility for the specific problem domain. This deliberate choice aligns with the notion that sometimes the most impactful solutions are elegantly straightforward. Also, we intentionally borrowed a pocket encoder from Uni-Mol to make a **fair comparison** with it, which strongly supports the power of our dataset. Similar to the molecule encoder, our pocket encoder can be changed to any other model due to our flexible framework. Notably, **we didn't load the pretrained weights of Uni-Mol pocket encoder**. We only use the same backbone architecture and the pretraining was completely done on our processed data with our training strategy. \n\nNotably, many simple but effective approaches have been recognized by top-tier conferences and journals. A prime example of this is in the field of protein language models, like the ESM series and ProtTrans. These studies adapted the Transformer architecture and masked language modeling techniques from natural language processing to protein sequences. While they didn't introduce groundbreaking techniques, their substantial contributions to protein modeling are evident, with publications in prestigious conferences and journals like NeurIPS, ICML, TPAMI, PNAS, and Science. Another example is the widely acclaimed CLIP paper presented at ICML 2021. CLIP, while not employing novel techniques, stands as a hallmark of data-centric deep learning. Its use of contrastive learning enabled training on expansive web-sourced text-image datasets, moving beyond the constraints of meticulously curated databases like MS-COCO. This aligns with our approach to addressing **data scarcity challenges** by facilitating training on large-scale datasets, mirroring our strategy for overcoming similar hurdles in the protein pocket modeling domain.\n\nWe appreciate your efforts to remain neutral and understand the challenges in decision-making. We believe further discussion will highlight our paper's contributions. Your feedback and insights in the upcoming discussion will be invaluable for refining our work.", "author_response": "# Response 1/3\nWe appreciate the time and effort you have dedicated to reviewing our paper, and we are grateful for your constructive feedback and thoughtful evaluation of our work. We would like to address your comments about the technical novelty of our work and provide additional clarification on certain aspects of our paper.\n\n## Response to technical novelty\n\nFirstly, we acknowledge your observation that we didn't use any fancy models compared with typical **model-centric** works. While we respect your assessment, we would like to highlight that our primary focus in this paper is a **data-centric** pretraining method to introduce groundbreaking improvements in the field of protein pocket pretraining. Our work aims to address a critical challenge of data scarcity in the field of protein pocket representation by constructing large-scale synthetic data that facilitates the pretraining of models, ultimately enhancing the accuracy and robustness of protein pocket representations. We believe that the research community can conduct fast following-ups on our **released dataset** with more sophisticated models, and even extend our pipelines to other tasks like docking or drug design. We also would like to point out that we have provided a novel method to **distill knowledge from well-trained molecule models** to protein models. Though employing the same contrastive loss, our approach is different from existing contrastive learning models since it uses a fixed molecular encoder. The motivation here is to use a well-trained molecular encoder on a relatively larger dataset to **guide** the training of the protein encoder. For example, some quantum-chemistry properties that are difficult to compute for large systems like proteins could be distilled from molecules, by using some quantum-chemistry-aware molecular encoders such as Frad. Though our framework seems to be simple, it is non-trivial to make it work. We have made several efforts to solve the unavoidable discrepancies between true ligands and pseudo ligands. The ablation study shows that our efforts are effective and necessary:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nRegarding your positive acknowledgment of our \"simple but effective model for the right question,\" we are pleased to hear that our approach resonates with the objective we set out to achieve. We designed our model with **simplicity** in mind, prioritizing effectiveness and practical utility for the specific problem domain. This deliberate choice aligns with the notion that sometimes the most impactful solutions are elegantly straightforward. Also, we intentionally borrowed a pocket encoder from Uni-Mol to make a **fair comparison** with it, which strongly supports the power of our dataset. Similar to the molecule encoder, our pocket encoder can be changed to any other model due to our flexible framework. Notably, **we didn't load the pretrained weights of Uni-Mol pocket encoder**. We only use the same backbone architecture and the pretraining was completely done on our processed data with our training strategy. \n\nNotably, many simple but effective approaches have been recognized by top-tier conferences and journals. A prime example of this is in the field of protein language models, like the ESM series and ProtTrans. These studies adapted the Transformer architecture and masked language modeling techniques from natural language processing to protein sequences. While they didn't introduce groundbreaking techniques, their substantial contributions to protein modeling are evident, with publications in prestigious conferences and journals like NeurIPS, ICML, TPAMI, PNAS, and Science. Another example is the widely acclaimed CLIP paper presented at ICML 2021. CLIP, while not employing novel techniques, stands as a hallmark of data-centric deep learning. Its use of contrastive learning enabled training on expansive web-sourced text-image datasets, moving beyond the constraints of meticulously curated databases like MS-COCO. This aligns with our approach to addressing **data scarcity challenges** by facilitating training on large-scale datasets, mirroring our strategy for overcoming similar hurdles in the protein pocket modeling domain.\n\nWe appreciate your efforts to remain neutral and understand the challenges in decision-making. We believe further discussion will highlight our paper's contributions. Your feedback and insights in the upcoming discussion will be invaluable for refining our work."}
{"claim": "The constructed dataset neglects other molecular properties that can cause imbalance and the paper lacks discussion about these potential imbalances.", "claim_type": "experimental", "paper_id": "uMAujpVi9m", "paper_title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "9ap4s1SZlZ", "reviewer": "Reviewer_yfM3", "review_text": "Summary: This paper primarily aims to enhance the pocket pretraining method, as existing approaches only consider pockets during pretraining. There are two main contributions in this paper: (1) The authors introduce a novel method, ProFSA, for pocket pretraining, which extracts additional information from corresponding ligands. However, the number of pocket-ligand complex structures is quite limited in existing datasets. (2) To address this issue, the authors generate over 5 million complexes by segmenting fragments and their corresponding pockets in protein structures. By aligning features of fragments and pockets, the pocket encoder learns the interaction between fragments and pockets. The authors design downstream tasks such as pocket druggability prediction, pocket matching, and ligand binding affinity prediction to demonstrate the effectiveness of ProFSA.\n\nStrengths: The authors propose a new perspective of pretraining pockets and construct a large-scale dataset, which data distribution is also considered, to make the efficient pre-training possible.\n\nThe results are competitive, especially for zero-shot settings.\n\nAbundant experiments and ablation study support the argument and result of the authors.\n\nWeaknesses: 1. The technical novelty is limited.\n  - The pocket encoder is borrowed from Uni-Mol.\n  - The contrastive loss is the vanilla form of classical contrastive learning.\n\n2. The bound of Theorem 3.1 is trivial. The authors claim that the bound naturally exists for these representations extracted by pretrained molecule models. However, it's a bit counterintuitive, because many models not pretrained on molecule datasets also fulfill this prior. So, can these models be used for this task? **I strongly suggest removing this part from the paper**.\n\n3. Some issues about dataset creation:\n - 3.1. The authors consider the distribution of ligand size and pocket size when designing the dataset. However, molecules possess more properties that can also lead to imbalance. It would be better to, at least, add some discussion about this issue.\n - 3.2. In the second stage of the data construction process, the approach to defining pockets needs further explanation or an ablation study.\n\n4. Experiments: It would be better to add some biological justification or visualization of the results.\n\nFor this paper, one fact is that the technical novelty is below the bar of ICLR. However, I admire the simple but effective model for the right question. It's a struggle for me to make a decision. I will maintain a neutral attitude and make my final decision after the discussion.\n\nQuestions: See weakness.", "labeling_timestamp": "2026-01-11T16:25:23.687250", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors explicitly acknowledge that molecules have additional properties (logP, H-bond donors/acceptors, rotatable bonds) that peptides cannot mimic, state this causes divergence and limits zero-shot performance, and describe added discussion, figures (Appendix G), and mitigation strategies (distributional alignment, fixed encoder, and finetuning with real ligand data).", "evidence": "\"It is correct that molecules have more properties than their size, like logP, Hbond donor and acceptor number, and rotatory bond number.\" \"However, it is impossible to mimic such features with peptides, as the backbone of peptides is intrinsically hydrophilic and flexible. As we showed in a new figure, our pseudo-ligands are less similar to real ligands in those properties even with size alignment. You can find the figure in the new pdf in Appendix G.\" \"We believe that the aforementioned divergence is the main limitation of poor zero-shot performance in predicting hydrophobicity scores (Table 1).\"", "combined_author_response": "Dear AC and reviewers,\n\nWe sincerely appreciate your time and efforts in reviewing our work. Based on your suggestions, we have revised our paper. We would like to use this section to reiterate the explanation for some common concerns and summarize the contributions of our paper.\n\nIn our revised paper, we provide a detailed explanation of the motivation and theory behind our method: **the mechanisms of pocket-peptide interactions and pocket-ligand interactions are very similar**. Our proposed **data-centric** pretraining pipeline is unique in its use of peptides to mimic small molecules, a method that is, to our knowledge, **the first of its kind**. We expanded our discussion about its biological insights in **Section 3**. We add a **new Figure 2 to illustrate different types of non-covalent interactions** that are common for both ligand-protein data and intra-protein data. Following this theory, we have designed a pipeline to generate **over 5 million** synthetic ligand-protein pairs, which greatly eases the **data scarcity** problem in the field of protein-ligand interaction learning. We also prove our data quite effective in various downstream tasks, including pocket property prediction, pocket matching, ligand binding affinity prediction, ligand efficiency prediction, and protein-protein interaction tasks. The **last two tasks are newly added** to further support our method (**Appendix C5 and C6**). We are confident that our **ready-to-release synthetic dataset** will significantly benefit the AI for drug discovery community. Its impact extends beyond enhancing pocket representations in downstream analyses, as it also equips researchers with a valuable resource for a range of applications, such as protein-ligand docking, structure-based drug design, and virtual screening. Importantly, these areas often face challenges due to **a lack of training data necessary for learning effective binding patterns between protein pockets and ligands**. \n\nIn our revised paper, we recognize the differences between real ligands and pseudo ligands (peptides). We address these discrepancies and present a distribution plot of various properties in **Appendix G**. Our approach primarily leverages the **analogous nature of ligand-pocket and peptide-pocket interactions**. Therefore, despite these differences, our model can **still extract valuable interaction information from the pretraining dataset**, which enhances its performance in downstream tasks. This is illustrated in **Figure 4(a)**, showing the benefits of interaction-aware pretraining. We have also undertaken measures like distribution alignment and freezing the molecular encoder to mitigate the impact of these discrepancies. The positive results of these efforts, as evidenced in **Table 5**, confirm their effectiveness and necessity. Notably, our pretrained model can be **further finetuned with real labeled ligand-pocket pairwise data**, as we show in the Ligand Binding Affinity task in **section 4.3**.\n\nAs for other questions, we have added more ablation studies on our design choices, including the impact of a fixed molecular encoder (Table 5), and different threshold values to define pockets(Appendix C.4). \n\nWe are immensely grateful for the invaluable feedback from all the reviewers, which has guided us in refining and clarifying our work. We hope that the revised version of our paper, coupled with the discussion period, will more clearly highlight the novelty, effectiveness, and contributions of our approach to the field.\n\n---\n\nThank you for your invaluable feedback on our paper. In response to your insightful suggestions, we have made several modifications in our revised paper. \n\nWe have cited additional papers in Section 2.2 following your advice. The description of loss terms in Section 3.2 has been corrected.  COSP was not included as a baseline for the Ligand Binding Affinity (LBA)  task because it is not open-sourced, and its performance on LBA was not evaluated in their published work. We used it as a baseline in the pocket-matching task.\n\nIn Section 3, we've expanded our discussion to answer your question on **ProFSA's effectiveness without distributional alignment**. This section now offers a detailed explanation of the foundational concepts and justifications for our method. We draw attention to the parallels between ligand-protein and intra-protein non-covalent interactions, as shown in **Figure 2**. This comparison supports our strategy of using peptides as stand-ins for actual ligands to mimic pocket-ligand interactions. Consequently, **even in the absence of distributional alignment, the types of interactions remain comparably relevant**, which explains why ProFSA continues to function, albeit with reduced effectiveness, as demonstrated in Table 5. We also want to clarify that distributional alignment is only applied to fragment-pocket complex as a whole, and **it would not change the definition of the pocket with a given ligand**. Without this alignment, these fragments are chosen uniformly, with lengths varying from 1 to 7 residues, and pockets are consistently identified based on residues within a 6Å radius of these peptide fragments.\n\nWe are grateful for the time and effort you have dedicated to reviewing our work. Your thorough and constructive feedback has significantly contributed to the refinement of our research. Please let us know if there are any other aspects of our paper that you would like us to address or clarify. We look forward to your further feedback.\n\n---\n\nThank you for your insightful feedback. In response to your concerns regarding the scope of our tasks, we have expanded our research in the revised paper. We now include additional tasks such as **ligand efficiency prediction** and **protein-protein affinity prediction**, detailed in **Appendix C5 and C6**. These tasks were incorporated to enhance the robustness of our method. We acknowledge that they are currently in the appendix due to space constraints, but we are open to relocating them to the main text if it would be beneficial.\n\nRegarding your concern about the lack of recent baselines, we provided a detailed explanation in our previous response. We want to reiterate that our benchmarks do include the latest baselines, with some as recent as 2023.\n\nWe greatly appreciate the time and effort you have invested in reviewing our work. Your feedback has been invaluable in refining our research. Please let us know if there are any other aspects of our paper that you would like us to address or clarify. We look forward to your further feedback.\n\n---\n\nIn our revised paper, we have incorporated changes following feedback from another reviewer, which we believe also addresses your concerns and queries.\n\nRegarding your question on the evidence supporting our approach, particularly the use of peptides to simulate small molecules, we've enriched **Section 3.1**. A new paragraph there now provides an in-depth explanation of non-covalent interactions. To aid understanding, **Figure 2** has been updated with visualizations that include both real and pseudo receptor-ligand pairs, illustrating three types of interactions. Furthermore, we delve into the details of **how intra-protein interactions mirror protein-ligand interactions**, involving specific types of amino acids.\n\nTo better evaluate our design choices, the ablation studies have been extended. Alongside the existing studies in **Table 4** (different pretrained molecular encoders), **Table 5** (distribution alignment), and **Figure 5** (data scale). we've added a study on the impact of using a fixed molecular encoder in **Table 5** in the main text to address your question on the fixed molecular encoder. We've also included an ablation study on varying **cutoff values for pocket definition**, placed in the **Appendix C.4** due to its more focused scope. We can move it to the main part of the paper in the future if needed.\n\nWe acknowledge your concern about the **differences between true and pseudo ligands**, so we've expanded our discussion on the distinct properties of our curated dataset versus the PDBBind dataset in **Figure 9** and **Section 3.2**. The ablation study in **Table 5** demonstrates our methods and their effectiveness in minimizing these discrepancies. Notably, the performance significantly drops without our distribution alignment or fixed molecular encoder, highlighting our efforts to mitigate these issues. The updated **Figure 2** further supports this, showing that while the properties of true and pseudo ligands may differ, **the interaction types are consistent.** We hope these results can ease your concerns about our synthetic dataset, and they also respond to your query about **why we kept the molecular encoder constant during pocket pretraining.**\n\nWe appreciate the constructive critiques and thoughtful engagement from you. Your feedback has undeniably contributed to the refinement and strengthening of our paper. We sincerely value the time and expertise invested in the review process, and we look forward to any further suggestions you may have.\n\n---\n\nWe sincerely appreciate the thoughtful feedback provided on our paper and have diligently incorporated your suggestions into this revised version. In response to your guidance, we have made three significant modifications to the core content of our paper, all of which aim to enhance clarity and address the concerns raised.\n\nTaking into account your suggestion, we have removed **Theorem 3.1** from the main body of our paper. Additionally, we've improved the explanation and included an ablation study on our method of defining pockets. This ablation study, focusing on the cutoff values and primarily a hyperparameter issue, is placed in the Appendix due to its more limited scope of insight. We can move it to the main body of the paper in the future if needed. We have also expanded our discussion on the differences between our dataset and the PDBBind dataset, elucidating how fixed molecule encoders contribute to mitigating this discrepancy.\n\nIn **Section 3.1**, we have introduced a new paragraph dedicated to providing readers with a deeper understanding of non-covalent interactions. To facilitate comprehension, we have included visualizations featuring three types of interactions, incorporating both real and pseudo receptor-ligand pairs in the new **Figure 2**. Furthermore, we delve into the details of how intra-protein interactions mirror protein-ligand interactions, involving specific types of amino acids.\n\nThe biological justifications for the efficacy of interaction-aware pretraining are now more extensively explored in our revised paper. Specifically, we present a compelling showcase featuring estradiol-binding proteins, offering insights into the geometric disparities between the compared proteins and highlighting the similarities in their binding interfaces in the new **Figure 4a**. The existing visualization of BioLip pocket representations in the new **Figure 4b** can make our claims more convincing. Together, these visual aids illustrate how our interaction-aware pretraining empowers models to focus on crucial interface residues, discern subtle distinctions, and disregard irrelevant geometric dissimilarities.\n\nWe have also included an in-depth discussion of pretrained molecule encoders in the newly added **Section 2.3**, further enriching the theoretical foundation of our work.\nAll changes can be found in the renewed paper, which is updated lively.\n\nWe genuinely value the constructive input you provided during the review process, as it has undoubtedly contributed to the refinement of our paper. Your insights have proven invaluable in elevating the clarity and depth of our contributions to the field. We express our gratitude for the time and expertise invested in reviewing our work, and we look forward to any additional feedback or suggestions you may have.\n\n---\n\n# Response 2/2\n\n## Response to the question on N Terminal and C Terminal\n\nWhen a peptide bond is formed (green), one amine group and one carboxy group are left untouched. Therefore, in a linear polypeptide, there would always be a free amine group and a free carboxy group, and they are called the N terminal(blue) and the C terminal(red) respectively in biochemistry. You can find the visualization in **Appendix H**.\n\n## Response to fixing the molecule encoder in contrastive learning\n\nFrom an **effectiveness** standpoint, as you mentioned before, there are unavoidable discrepancies between pseudo and true ligands. If we didn't fix our molecule encoder during pretraining, then the conditions of Theorem 3.1 might not be met. The ablation results presented in the previous response clearly indicate a performance decline when the molecular encoder is not fixed during contrastive learning, particularly in the context of pocket matching. This supports our assertion that the pocket encoder struggles to acquire chemical knowledge without the stability provided by a fixed molecular encoder.\n\nFrom an **efficiency** standpoint, a fixed molecule encoder also offers significant advantages. It reduces memory usage and accelerates training speed because all molecule embeddings can be pre-computed before training. This eliminates the need for feed-forward and back-propagation operations of the molecule encoder during training, leading to efficiency gains. Additionally, adapting to different molecule encoders becomes more straightforward when they are not co-trained during the pretraining process. This approach allows for more flexibility and ease in integrating various encoding methodologies **even if the molecule encoder is not accessible for training**. That is to say, our approach can even be used with undifferentiable molecular representations.\n\n## Response to extending to other tasks like protein-protein interaction prediction\n\nThank you for pointing out the potential application of our framework to protein-protein interaction (PPI) tasks. We have to point out that the interactions between proteins and proteins are far different from proteins and molecules. Specifically, the interaction areas are usually much larger, but shallower, and involved functional groups are less diverse due to limited types of natural amino acids. Therefore, several changes should be made to extend our ProFSA method to PPI tasks:\n\n1. As for data creation, unlike the localized and specific nature of pocket-ligand interactions, protein-protein interactions are **more global and involve broader protein features**. This requires a shift in the data creation process to split entire protein structures into domains instead of short fragments and to capture potential interactions between them. \n\n2. The interaction interface between protein and protein is much larger. We might need to change from atomic-level to residue-level representations to efficiently capture interaction information.\n\n3. For contrastive learning training, ProFSA uses a fixed pretrained molecular encoder to cope with various small molecules and their atomic representation. However, for protein-protein interaction, it is not necessary as we could only model 20 types of amino acids at the residue level.\n\nDespite several changes that need to be done for PPI tasks, we find that we could divide protein complexes into local interactions. When the complex structures and monomer structures are provided, we have achieved a reasonable result on a flexible PPI affinity prediction benchmark with our current model in a zero-shot way. \n\n| Method            | Spearman $\\uparrow$  |\n|-------------------|----------------------|\n| SchNet            | 0.072 ± 0.021        |\n| DimeNet++         | 0.171 ± 0.054        |\n| EGNN              | 0.080 ± 0.038        |\n| TorchMD           | 0.117 ± 0.008        |\n| GET               | **0.363 ± 0.017**    |\n| ProFSA(zero-shot) | _0.248_              |\n\nSurprisingly, as a **zero-shot** method, ProFSA is able to outperform other **supervised learning** models except for GET, a newly proposed unified model. This demonstrates our approach is also able to capture protein-protein interaction information despite it being designed for protein-ligand interaction. \n\nA detailed explanation of this experiment is shown in **Appendix C6**.\n\n---\n\n# Response 1/2\n\n## Response to evidence for the construction of the pseudo-ligand and using peptides to replace small molecules \n\nAs we discussed in the paper, pseudo-ligands share similar sizes as real ligands, and they also make similar non-covalent interactions with the pocket. To further support our proposal, we would like to cite another paper **\"A defined structural unit enables de novo design of small-molecule–binding proteins\"**[1], published in Science 2020, which supports that **intra-protein interactions are similar to protein-ligand interactions.** That is the reason we use peptides to **represent** small molecules to let the pocket encoder learn the interaction information. \n\nAlso, we'd like to clarify that our intention is not to **\"replace\"** small molecules with peptides. It's a pretraining framework that leverages abundant protein-only data and uses peptides to **simulate** pocket-small molecule interactions for enhanced pocket representations. Following this pretraining phase, **the model can be finetuned with datasets that include real small molecules, ensuring its effectiveness in practical applications**.\n\nTo demonstrate the validity of constructing pseudo-ligands, we provide visualizations in **Appendix F** showing that interactions between pockets and our pseudo-ligands are similar to those with true ligands. These figures reveal shared interaction types like **hydrogen bonding**, **$\\pi-\\pi$ stacking**, and **salt bridge**, depicted with color-coded dashed lines for each interaction type. This should help clarify the rationale and evidence supporting our approach.\n\n[1] Polizzi, Nicholas F., and William F. DeGrado. \"A defined structural unit enables de novo design of small-molecule–binding proteins.\" Science 369, no. 6508 (2020)\n\n## Response to ablation studies evaluating the impact of critical design choices\n\nThanks for your advice on adding more ablation studies on design choices. We did ablation studies on the effectiveness of distributional alignment, where the major difference is that fragment sizes are modified. We found that aligned fragment sizes with real ligands could provide the best performance. The result is shown in **Table 5**.\n\nAs for the distance thresholds, we define the pocket following the UniMol setup. We also found our model can also adapt to an 8Å setup in the ToughM1 experiment. Following your advice, we did an ablation study on the distance thresholds. We tested our method with three different thresholds: 4Å, 6Å, and 8Å. The result is shown below:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| 4Å | 0.7062 | 0.7549 | 0.1240 | 0.1095 | 28.29 | 13.07 |\n| 6Å | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| 8Å | 0.8322 | 0.8292 | 0.1256 | 0.1125 | 34.83 | 12.92 |\n\nSince the 8Å threshold corresponds with the pocket definition in the Kahraman and Tough M1 datasets, it leads to optimal results. Our decision to use a 6Å threshold was made to align with the methodology of pretraining data creation by Uni-Mol, facilitating a fair comparison. Notably, even with the 6Å threshold, we achieved strong results in the pocket-matching task, which serves as a testament to the effectiveness of our approach.\n\n## Response to substantial discrepancies between pseudo and real ligands\n\nThank you for pointing out the discrepancies between pseudo and real ligands. We understand and acknowledge your concern. To mitigate the impact of this discrepancy, we have made several efforts. First, we performed **distribution alignment** to make the data distribution of pseudo-ligands more similar to that of true ligands in PDBbind. Additionally, we **fixed the molecule encoder during pretraining** to prevent it from being misled by the discrepancy. Our ablation studies have shown that these strategies are effective:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nThe table above shows that lacking distribution alignment and not fixing the molecular encoder during pretraining both lead to lower performance, underscoring the effectiveness of our methods in reducing discrepancies.\n\nTheorem 3.1 theoretically supports our approach's efficacy, even with discrepancies between pseudo and real ligands. Empirically, our method outperforms other pretraining techniques, confirming its effectiveness despite these discrepancies.\n\n---\n\n## Response to limited tasks and potential in protein-ligand binding pose prediction\n\nIn our paper, the primary objective is to assess the effectiveness of our pretraining method and the quality of the trained pocket representations. To achieve this, we focused on two types of downstream tasks: pocket-only tasks (pocket property prediction and pocket matching) and pocket-ligand interaction tasks (ligand binding affinity prediction).\n\nWe appreciate the suggestion to apply our method to the protein-ligand binding pose prediction task. However, it's important to note that our current evaluation framework is designed to specifically assess **pocket representations**. For the tasks we chose, the architecture is straightforward: either a **zero-shot** evaluation or a **simple MLP** for mapping embeddings to predictions. In contrast, protein-ligand binding pose prediction often involves **complex methodologies**. For instance, state-of-the-art methods like DiffDock[1] require training a diffusion generative model and a separate confidence model, while other approaches like EDM-Dock[2], rely on reconstructing ligand conformations from predicted distance maps. These methods are not end-to-end and do not directly align with our objective of evaluating pretrained pocket representations.\n\nNevertheless, we recognize the potential of our method in enhancing existing binding pose prediction techniques. To integrate our approach, we would need to modify our framework. Using the same data creation strategy, we could train a binding pose prediction model with our preprocessed data, which could then be further fine-tuned using real pocket-ligand pair data from sources like PDBbind. Additionally, our data could be used to train a side-chain packing model, allowing for side-chain flexibility during docking. Thank you again for your advice, and we will leave protein-ligand binding pose prediction as a separate future work for our method.\n\nIn response to the need for evaluating our method on a broader range of tasks, we have extended our analysis to include two additional downstream tasks: LEP (Ligand Efficacy Prediction), and PPA (Protein-Protein Affinity Prediction).\n\nresult for LEP:\n\n| Method      | AUROC $\\uparrow$ | AUPRC $\\uparrow$ |\n|-------------|--------------|--------------|\n| ATOM3D-GNN  | 0.681        | 0.598        |\n| GeoSSL      | 0.776±0.03   | 0.694±0.06   |\n| Uni-Mol     | 0.782±0.02   | 0.695±0.07   |\n| ProFSA      | 0.840±0.04   | 0.806±0.04   |\n\nGeoSSL and Uni-Mol, both pretraining methods, yield comparable results. However, ProFSA outperforms these methods, demonstrating the advantage of our pocket pretraining approach.\n\nresult for PPA:\n\n| Method            | Spearman $\\uparrow$  |\n|-------------------|----------------------|\n| SchNet            | 0.072 ± 0.021        |\n| DimeNet++         | 0.171 ± 0.054        |\n| EGNN              | 0.080 ± 0.038        |\n| TorchMD           | 0.117 ± 0.008        |\n| GET               | **0.363 ± 0.017**    |\n| ProFSA(zero shot) | _0.248_              |\n\nAs a **zero-shot** method, ProFSA is able to outperform other **supervised learning** models except for GET, a newly proposed unified model. This demonstrates our approach is able to capture protein-protein interaction information despite it being designed for protein-ligand interaction modeling.\n\nYou can find detailed experiment settings and results in **Appendix C5 and C6**.\n\n[1] Corso et al., \"Diffdock: Diffusion steps, twists, and turns for molecular docking.\", ICLR 2023.\n[2] Masters et al., \"Deep learning model for efficient protein–ligand docking with implicit side-chain flexibility.\" Journal of Chemical Information and Modeling 63, no. 6 (2023).\n\n## Response to the issue of lack of latest baselines\n\nFor the pocket matching task, CoSP is a newly proposed baseline which is published in ECML PKDD 2023. Alongside CoSP and Uni-Mol, we selected some of the most effective baseline results presented in the CoSP paper. We also tried to evaluate recent methods like PocketAnchor (Li et al., Cell Systems 2022) on our own since they are not tested on the pocket matching task. However, since the result didn't outperform other baseline machine learning methods(Uni-Mol, CoSP, and DeeplyTough), we decided not to include it in our final baseline comparison. We can include it in the camera-ready version if needed.\n\nFor the ligand binding affinity task, a lot of newly proposed baselines are included, i.e. ProNet (Wang et al., 2022b); as well as pretraining methods such as GeoSSL (Liu et al., 2023), EGNN-PLM (Wu et al., 2022), DeepAffinity (Karimi et al., 2019) and Uni-Mol (Zhou et al., 2023).\n\n---\n\n## Response to lack of related works\n\nThanks for providing us with more insightful related works that could support our arguments. We actually have cited these three papers in the section on ligand binding affinity experiments, but we are also happy to add these in Related Works section 2.2. You can find the change in pdfdiff.\n\n## Response to lack of COSP as a baseline in Table 3\n\nBecause best to our knowledge, they did not release their code and they did not test their method on the ligand binding affinity dataset. \n\n## Response to the question on the loss terms\n\nThank you for your advice. We apologize that we accidentally mentioned loss1 and loss2 in reverse, and we are sorry for the confusing statement. You are correct that the first loss is to identify the true protein pocket when given a pseudo-ligand. We revise the original statement to: \"The primary purpose of the first loss is to identify the true protein pocket from a batch of samples when given a pseudo-ligand. Similarly, the second loss seeks to identify the corresponding ligand fragment for a given pocket.\" You can also find the change in pdfdiff.\n\n## Response to the question on the length of the pocket representation without alignment\n\nWith or without distributional alignment, pockets are always defined by the given protein fragment with a fixed distance cutoff (6Å in our works, following the UniMol setup). The distributional alignment process merely samples these pairs to match the sizes of real ligands and pockets. Without distributional alignment, fragments are uniformly sampled from 1 to 7 residues. Pockets are similarly determined as with residues within the range of 6Å around peptide fragments.\n\n---\n\n# Question about the confidence score\n\nThanks for your review. We found your understanding and judgment of our paper comprehensive and precise. Also, you provided detailed and thoughtful advice for our paper, which helped a lot. We sincerely appreciate your review, but we also wonder if there is any misunderstanding here, as you only give a confidence score of 1.\n\n---\n\n# Response 3/3\n\n## Response to biological justification and visualization of the results\n\nOur method's **biological justification** for achieving good experimental results is based on the properties and interactions of pseudo-ligands. As discussed in our paper, these pseudo-ligands are similar in size to real ligands and engage in comparable non-covalent interactions with protein pockets. This fundamental similarity is crucial for the effectiveness of our approach.\n\nMoreover, our methodology is further supported by the findings in the 2020 Science paper titled \"A defined structural unit enables de novo design of small-molecule–binding proteins.\"[1] This research underscores that **intra-protein interactions are analogous to protein-ligand interactions**, which validates our use of peptides as proxies for small molecules. By employing peptides as pseudo-ligands in this manner, our pocket encoder is able to learn and replicate the interaction dynamics typically observed in real ligand scenarios. This understanding is pivotal to the success of our method in downstream applications.\n\nTo better justify our methods and results, following your advice, we also provide some visualizations in **Appendix F**. **Figure 6** illustrates that the pseudo pairs we created share various interaction types commonly found in real pocket-ligand pairs, such as **hydrogen bonding, $\\pi-\\pi$ stacking, and salt bridge**. In the figures, each type of interaction is represented by dashed lines, color-coded to correspond with the specific interaction type.\n\nAnother visualization of our pocket-matching result is shown in **Figure 7**. We showed an example of two non-homology ATP binding pockets to explain why pocket-matching can benefit from interaction-aware pretraining so that we have achieved superior results in the Kahraman dataset and the BioLip t-SNE visualization. The PEP carboxykinase(PDB:1AYL) and tRNA synthetase(PDB:1B8A) are two ATP-binding proteins that share zero sequence similarity (verified with the BLAST) extracted from the Kahraman dataset. However, as they are both fueled by ATP, their binding site shares similar binding patterns. The cation-$\\pi$ interactions(blue dash lines) and salt bridges (magenta dash lines) are important to the ATP binding, which can be viewed as convergent evolutions at the molecule level. Even though, these two pockets are very distinct in terms of shapes and sizes because they bind ATPs in different conformations. Therefore, biochemical interactions are the key to accomplishing the pocket-matching task, which is ignored in previous self-supervised learning methods like Uni-Mol.\n\n[1] Polizzi, Nicholas F., and William F. DeGrado. \"A defined structural unit enables de novo design of small-molecule–binding proteins.\" Science 369, no. 6508 (2020)\n\n---\n\n# Response 2/3\n\n## Response to the bound of Theorem 3.1 \n\nWe are not very clear about the point of your question. We have two understandings and they are replied as follows.\n\nIf you mean $ ||t − t ^{(0)}|| < 1/2 l_T$ is trivial, please note that it is only a condition of the theorem. As you said, many other encoders can also satisfy this condition. This shows that our method is applicable to many different molecular encoders with or without pretraining as we proved in **Table 4**. Moreover, the condition is only a necessary condition. We also need the pretraining contrastive loss to be sufficiently small to guarantee a small contrastive loss between pockets and real ligands. In fact, in the process of our early exploration, we noticed that some encoders cannot achieve a low pretraining loss, indicating the entire conditions of the theorem are nontrivial.\n\nIf you mean that the conclusion of theorem 3.1 is trivial, we want to emphasize that our conclusion is $\\lim_{L_i(t, s) =0}L_i(t^{(0)}, s) =0$. (We write it in a $\\epsilon$-$\\delta$ language in paper. They are equivalent forms.) Our conclusion shows the loss containing real ligands is consistent with the loss containing pseudo ligands that we optimized in pre-training. \n\nIn contrast,  a trivial result from $||t − t ^{(0)}|| < 1/ 2l_T$ is that $\\lim_{L_i(t, s) =0}L_i(t^{(0)}, s) <B$, B is a bound related to l_t and the representations. It does not guarantee a consistent loss and is substantially different from our result.\nTherefore, our theorem is nontrivial and reveals the transfer ability of our contrastive pre-training from the pseudo-ligand domain to the real ligand domain.\n\nAs for this theoretical result, we are open to further discussion.\n\n## Response to molecules possess more properties that can also lead to imbalance\n\nIt is correct that molecules have more properties than their size, like logP, Hbond donor and acceptor number, and rotatory bond number. As small molecule drugs are mostly designed to penetrate barriers like gut or cell membranes, they usually have much larger logP values, which means more hydrophobic. For the same reason, they usually have fewer hydrogen bond donors and acceptors, and only minimal essential ones are kept for specified interactions. Also, to minimize the entropy effect upon binding and to increase binding affinity, rotatory bonds are also unfavored. However, it is impossible to mimic such features with peptides, as the backbone of peptides is intrinsically hydrophilic and flexible. As we showed in a new figure, our pseudo-ligands are less similar to real ligands in those properties even with size alignment. You can find the figure in the new pdf in **Appendix G**.\n\nWe believe that the aforementioned divergence is the main limitation of poor zero-shot performance in predicting hydrophobicity scores (**Table 1**). However, as demonstrated in previous publications like CoSP, we could leverage real ligand-pocket pairs from PDBBind or BioLip database to further finetune our network, as an extension of the pipeline. In this way, we could handle property mismatches but still enjoy the power of our large-scale pretraining.\n\n## Response to the approach to defining pockets \n\nWe define the pocket following the UniMol setup. We found our model can also adapt to an 8Å setup in the toughM1 experiment \n\nIn particular, we define the pocket for each protein-ligand pair as residues of the protein that have at least one atom within the range of 6Å from a heavy atom in the ligand. To further explain our design, we have done an ablation study on different choices of thresholds of 4Å, 6Å, and 8Å:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| 4Å | 0.7062 | 0.7549 | 0.1240 | 0.1095 | 28.29 | 13.07 |\n| 6Å | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| 8Å | 0.8322 | 0.8292 | 0.1256 | 0.1125 | 34.83 | 12.92 |\n\nSince the 8Å threshold corresponds with the pocket definition in the Kahraman and Tough M1 datasets, it leads to optimal results. Our decision to use a 6Å threshold was made to align with the methodology of pretraining data creation by Uni-Mol, facilitating a fair comparison. Notably, even with the 6Å threshold, we achieved strong results in the pocket-matching task, which serves as a testament to the effectiveness of our approach.\n\n---\n\n# Response 1/3\nWe appreciate the time and effort you have dedicated to reviewing our paper, and we are grateful for your constructive feedback and thoughtful evaluation of our work. We would like to address your comments about the technical novelty of our work and provide additional clarification on certain aspects of our paper.\n\n## Response to technical novelty\n\nFirstly, we acknowledge your observation that we didn't use any fancy models compared with typical **model-centric** works. While we respect your assessment, we would like to highlight that our primary focus in this paper is a **data-centric** pretraining method to introduce groundbreaking improvements in the field of protein pocket pretraining. Our work aims to address a critical challenge of data scarcity in the field of protein pocket representation by constructing large-scale synthetic data that facilitates the pretraining of models, ultimately enhancing the accuracy and robustness of protein pocket representations. We believe that the research community can conduct fast following-ups on our **released dataset** with more sophisticated models, and even extend our pipelines to other tasks like docking or drug design. We also would like to point out that we have provided a novel method to **distill knowledge from well-trained molecule models** to protein models. Though employing the same contrastive loss, our approach is different from existing contrastive learning models since it uses a fixed molecular encoder. The motivation here is to use a well-trained molecular encoder on a relatively larger dataset to **guide** the training of the protein encoder. For example, some quantum-chemistry properties that are difficult to compute for large systems like proteins could be distilled from molecules, by using some quantum-chemistry-aware molecular encoders such as Frad. Though our framework seems to be simple, it is non-trivial to make it work. We have made several efforts to solve the unavoidable discrepancies between true ligands and pseudo ligands. The ablation study shows that our efforts are effective and necessary:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nRegarding your positive acknowledgment of our \"simple but effective model for the right question,\" we are pleased to hear that our approach resonates with the objective we set out to achieve. We designed our model with **simplicity** in mind, prioritizing effectiveness and practical utility for the specific problem domain. This deliberate choice aligns with the notion that sometimes the most impactful solutions are elegantly straightforward. Also, we intentionally borrowed a pocket encoder from Uni-Mol to make a **fair comparison** with it, which strongly supports the power of our dataset. Similar to the molecule encoder, our pocket encoder can be changed to any other model due to our flexible framework. Notably, **we didn't load the pretrained weights of Uni-Mol pocket encoder**. We only use the same backbone architecture and the pretraining was completely done on our processed data with our training strategy. \n\nNotably, many simple but effective approaches have been recognized by top-tier conferences and journals. A prime example of this is in the field of protein language models, like the ESM series and ProtTrans. These studies adapted the Transformer architecture and masked language modeling techniques from natural language processing to protein sequences. While they didn't introduce groundbreaking techniques, their substantial contributions to protein modeling are evident, with publications in prestigious conferences and journals like NeurIPS, ICML, TPAMI, PNAS, and Science. Another example is the widely acclaimed CLIP paper presented at ICML 2021. CLIP, while not employing novel techniques, stands as a hallmark of data-centric deep learning. Its use of contrastive learning enabled training on expansive web-sourced text-image datasets, moving beyond the constraints of meticulously curated databases like MS-COCO. This aligns with our approach to addressing **data scarcity challenges** by facilitating training on large-scale datasets, mirroring our strategy for overcoming similar hurdles in the protein pocket modeling domain.\n\nWe appreciate your efforts to remain neutral and understand the challenges in decision-making. We believe further discussion will highlight our paper's contributions. Your feedback and insights in the upcoming discussion will be invaluable for refining our work.", "author_response": "# Response 1/3\nWe appreciate the time and effort you have dedicated to reviewing our paper, and we are grateful for your constructive feedback and thoughtful evaluation of our work. We would like to address your comments about the technical novelty of our work and provide additional clarification on certain aspects of our paper.\n\n## Response to technical novelty\n\nFirstly, we acknowledge your observation that we didn't use any fancy models compared with typical **model-centric** works. While we respect your assessment, we would like to highlight that our primary focus in this paper is a **data-centric** pretraining method to introduce groundbreaking improvements in the field of protein pocket pretraining. Our work aims to address a critical challenge of data scarcity in the field of protein pocket representation by constructing large-scale synthetic data that facilitates the pretraining of models, ultimately enhancing the accuracy and robustness of protein pocket representations. We believe that the research community can conduct fast following-ups on our **released dataset** with more sophisticated models, and even extend our pipelines to other tasks like docking or drug design. We also would like to point out that we have provided a novel method to **distill knowledge from well-trained molecule models** to protein models. Though employing the same contrastive loss, our approach is different from existing contrastive learning models since it uses a fixed molecular encoder. The motivation here is to use a well-trained molecular encoder on a relatively larger dataset to **guide** the training of the protein encoder. For example, some quantum-chemistry properties that are difficult to compute for large systems like proteins could be distilled from molecules, by using some quantum-chemistry-aware molecular encoders such as Frad. Though our framework seems to be simple, it is non-trivial to make it work. We have made several efforts to solve the unavoidable discrepancies between true ligands and pseudo ligands. The ablation study shows that our efforts are effective and necessary:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nRegarding your positive acknowledgment of our \"simple but effective model for the right question,\" we are pleased to hear that our approach resonates with the objective we set out to achieve. We designed our model with **simplicity** in mind, prioritizing effectiveness and practical utility for the specific problem domain. This deliberate choice aligns with the notion that sometimes the most impactful solutions are elegantly straightforward. Also, we intentionally borrowed a pocket encoder from Uni-Mol to make a **fair comparison** with it, which strongly supports the power of our dataset. Similar to the molecule encoder, our pocket encoder can be changed to any other model due to our flexible framework. Notably, **we didn't load the pretrained weights of Uni-Mol pocket encoder**. We only use the same backbone architecture and the pretraining was completely done on our processed data with our training strategy. \n\nNotably, many simple but effective approaches have been recognized by top-tier conferences and journals. A prime example of this is in the field of protein language models, like the ESM series and ProtTrans. These studies adapted the Transformer architecture and masked language modeling techniques from natural language processing to protein sequences. While they didn't introduce groundbreaking techniques, their substantial contributions to protein modeling are evident, with publications in prestigious conferences and journals like NeurIPS, ICML, TPAMI, PNAS, and Science. Another example is the widely acclaimed CLIP paper presented at ICML 2021. CLIP, while not employing novel techniques, stands as a hallmark of data-centric deep learning. Its use of contrastive learning enabled training on expansive web-sourced text-image datasets, moving beyond the constraints of meticulously curated databases like MS-COCO. This aligns with our approach to addressing **data scarcity challenges** by facilitating training on large-scale datasets, mirroring our strategy for overcoming similar hurdles in the protein pocket modeling domain.\n\nWe appreciate your efforts to remain neutral and understand the challenges in decision-making. We believe further discussion will highlight our paper's contributions. Your feedback and insights in the upcoming discussion will be invaluable for refining our work."}
{"claim": "In the second stage of data construction, the approach to defining pockets lacks sufficient explanation and requires an ablation study or clearer justification.", "claim_type": "methodology", "paper_id": "uMAujpVi9m", "paper_title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "9ap4s1SZlZ", "reviewer": "Reviewer_yfM3", "review_text": "Summary: This paper primarily aims to enhance the pocket pretraining method, as existing approaches only consider pockets during pretraining. There are two main contributions in this paper: (1) The authors introduce a novel method, ProFSA, for pocket pretraining, which extracts additional information from corresponding ligands. However, the number of pocket-ligand complex structures is quite limited in existing datasets. (2) To address this issue, the authors generate over 5 million complexes by segmenting fragments and their corresponding pockets in protein structures. By aligning features of fragments and pockets, the pocket encoder learns the interaction between fragments and pockets. The authors design downstream tasks such as pocket druggability prediction, pocket matching, and ligand binding affinity prediction to demonstrate the effectiveness of ProFSA.\n\nStrengths: The authors propose a new perspective of pretraining pockets and construct a large-scale dataset, which data distribution is also considered, to make the efficient pre-training possible.\n\nThe results are competitive, especially for zero-shot settings.\n\nAbundant experiments and ablation study support the argument and result of the authors.\n\nWeaknesses: 1. The technical novelty is limited.\n  - The pocket encoder is borrowed from Uni-Mol.\n  - The contrastive loss is the vanilla form of classical contrastive learning.\n\n2. The bound of Theorem 3.1 is trivial. The authors claim that the bound naturally exists for these representations extracted by pretrained molecule models. However, it's a bit counterintuitive, because many models not pretrained on molecule datasets also fulfill this prior. So, can these models be used for this task? **I strongly suggest removing this part from the paper**.\n\n3. Some issues about dataset creation:\n - 3.1. The authors consider the distribution of ligand size and pocket size when designing the dataset. However, molecules possess more properties that can also lead to imbalance. It would be better to, at least, add some discussion about this issue.\n - 3.2. In the second stage of the data construction process, the approach to defining pockets needs further explanation or an ablation study.\n\n4. Experiments: It would be better to add some biological justification or visualization of the results.\n\nFor this paper, one fact is that the technical novelty is below the bar of ICLR. However, I admire the simple but effective model for the right question. It's a struggle for me to make a decision. I will maintain a neutral attitude and make my final decision after the discussion.\n\nQuestions: See weakness.", "labeling_timestamp": "2026-01-11T16:25:27.908704", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors explicitly added explanations and an ablation study on pocket-definition thresholds (4Å, 6Å, 8Å), and state these additions are in the revised paper (Appendix C.4), directly addressing the reviewer's request.", "evidence": "We have also included an ablation study on varying cutoff values for pocket definition, placed in the Appendix C.4 due to its more focused scope. ... In particular, we define the pocket for each protein-ligand pair as residues of the protein that have at least one atom within the range of 6Å from a heavy atom in the ligand. To further explain our design, we have done an ablation study on different choices of thresholds of 4Å, 6Å, and 8Å:", "combined_author_response": "Dear AC and reviewers,\n\nWe sincerely appreciate your time and efforts in reviewing our work. Based on your suggestions, we have revised our paper. We would like to use this section to reiterate the explanation for some common concerns and summarize the contributions of our paper.\n\nIn our revised paper, we provide a detailed explanation of the motivation and theory behind our method: **the mechanisms of pocket-peptide interactions and pocket-ligand interactions are very similar**. Our proposed **data-centric** pretraining pipeline is unique in its use of peptides to mimic small molecules, a method that is, to our knowledge, **the first of its kind**. We expanded our discussion about its biological insights in **Section 3**. We add a **new Figure 2 to illustrate different types of non-covalent interactions** that are common for both ligand-protein data and intra-protein data. Following this theory, we have designed a pipeline to generate **over 5 million** synthetic ligand-protein pairs, which greatly eases the **data scarcity** problem in the field of protein-ligand interaction learning. We also prove our data quite effective in various downstream tasks, including pocket property prediction, pocket matching, ligand binding affinity prediction, ligand efficiency prediction, and protein-protein interaction tasks. The **last two tasks are newly added** to further support our method (**Appendix C5 and C6**). We are confident that our **ready-to-release synthetic dataset** will significantly benefit the AI for drug discovery community. Its impact extends beyond enhancing pocket representations in downstream analyses, as it also equips researchers with a valuable resource for a range of applications, such as protein-ligand docking, structure-based drug design, and virtual screening. Importantly, these areas often face challenges due to **a lack of training data necessary for learning effective binding patterns between protein pockets and ligands**. \n\nIn our revised paper, we recognize the differences between real ligands and pseudo ligands (peptides). We address these discrepancies and present a distribution plot of various properties in **Appendix G**. Our approach primarily leverages the **analogous nature of ligand-pocket and peptide-pocket interactions**. Therefore, despite these differences, our model can **still extract valuable interaction information from the pretraining dataset**, which enhances its performance in downstream tasks. This is illustrated in **Figure 4(a)**, showing the benefits of interaction-aware pretraining. We have also undertaken measures like distribution alignment and freezing the molecular encoder to mitigate the impact of these discrepancies. The positive results of these efforts, as evidenced in **Table 5**, confirm their effectiveness and necessity. Notably, our pretrained model can be **further finetuned with real labeled ligand-pocket pairwise data**, as we show in the Ligand Binding Affinity task in **section 4.3**.\n\nAs for other questions, we have added more ablation studies on our design choices, including the impact of a fixed molecular encoder (Table 5), and different threshold values to define pockets(Appendix C.4). \n\nWe are immensely grateful for the invaluable feedback from all the reviewers, which has guided us in refining and clarifying our work. We hope that the revised version of our paper, coupled with the discussion period, will more clearly highlight the novelty, effectiveness, and contributions of our approach to the field.\n\n---\n\nThank you for your invaluable feedback on our paper. In response to your insightful suggestions, we have made several modifications in our revised paper. \n\nWe have cited additional papers in Section 2.2 following your advice. The description of loss terms in Section 3.2 has been corrected.  COSP was not included as a baseline for the Ligand Binding Affinity (LBA)  task because it is not open-sourced, and its performance on LBA was not evaluated in their published work. We used it as a baseline in the pocket-matching task.\n\nIn Section 3, we've expanded our discussion to answer your question on **ProFSA's effectiveness without distributional alignment**. This section now offers a detailed explanation of the foundational concepts and justifications for our method. We draw attention to the parallels between ligand-protein and intra-protein non-covalent interactions, as shown in **Figure 2**. This comparison supports our strategy of using peptides as stand-ins for actual ligands to mimic pocket-ligand interactions. Consequently, **even in the absence of distributional alignment, the types of interactions remain comparably relevant**, which explains why ProFSA continues to function, albeit with reduced effectiveness, as demonstrated in Table 5. We also want to clarify that distributional alignment is only applied to fragment-pocket complex as a whole, and **it would not change the definition of the pocket with a given ligand**. Without this alignment, these fragments are chosen uniformly, with lengths varying from 1 to 7 residues, and pockets are consistently identified based on residues within a 6Å radius of these peptide fragments.\n\nWe are grateful for the time and effort you have dedicated to reviewing our work. Your thorough and constructive feedback has significantly contributed to the refinement of our research. Please let us know if there are any other aspects of our paper that you would like us to address or clarify. We look forward to your further feedback.\n\n---\n\nThank you for your insightful feedback. In response to your concerns regarding the scope of our tasks, we have expanded our research in the revised paper. We now include additional tasks such as **ligand efficiency prediction** and **protein-protein affinity prediction**, detailed in **Appendix C5 and C6**. These tasks were incorporated to enhance the robustness of our method. We acknowledge that they are currently in the appendix due to space constraints, but we are open to relocating them to the main text if it would be beneficial.\n\nRegarding your concern about the lack of recent baselines, we provided a detailed explanation in our previous response. We want to reiterate that our benchmarks do include the latest baselines, with some as recent as 2023.\n\nWe greatly appreciate the time and effort you have invested in reviewing our work. Your feedback has been invaluable in refining our research. Please let us know if there are any other aspects of our paper that you would like us to address or clarify. We look forward to your further feedback.\n\n---\n\nIn our revised paper, we have incorporated changes following feedback from another reviewer, which we believe also addresses your concerns and queries.\n\nRegarding your question on the evidence supporting our approach, particularly the use of peptides to simulate small molecules, we've enriched **Section 3.1**. A new paragraph there now provides an in-depth explanation of non-covalent interactions. To aid understanding, **Figure 2** has been updated with visualizations that include both real and pseudo receptor-ligand pairs, illustrating three types of interactions. Furthermore, we delve into the details of **how intra-protein interactions mirror protein-ligand interactions**, involving specific types of amino acids.\n\nTo better evaluate our design choices, the ablation studies have been extended. Alongside the existing studies in **Table 4** (different pretrained molecular encoders), **Table 5** (distribution alignment), and **Figure 5** (data scale). we've added a study on the impact of using a fixed molecular encoder in **Table 5** in the main text to address your question on the fixed molecular encoder. We've also included an ablation study on varying **cutoff values for pocket definition**, placed in the **Appendix C.4** due to its more focused scope. We can move it to the main part of the paper in the future if needed.\n\nWe acknowledge your concern about the **differences between true and pseudo ligands**, so we've expanded our discussion on the distinct properties of our curated dataset versus the PDBBind dataset in **Figure 9** and **Section 3.2**. The ablation study in **Table 5** demonstrates our methods and their effectiveness in minimizing these discrepancies. Notably, the performance significantly drops without our distribution alignment or fixed molecular encoder, highlighting our efforts to mitigate these issues. The updated **Figure 2** further supports this, showing that while the properties of true and pseudo ligands may differ, **the interaction types are consistent.** We hope these results can ease your concerns about our synthetic dataset, and they also respond to your query about **why we kept the molecular encoder constant during pocket pretraining.**\n\nWe appreciate the constructive critiques and thoughtful engagement from you. Your feedback has undeniably contributed to the refinement and strengthening of our paper. We sincerely value the time and expertise invested in the review process, and we look forward to any further suggestions you may have.\n\n---\n\nWe sincerely appreciate the thoughtful feedback provided on our paper and have diligently incorporated your suggestions into this revised version. In response to your guidance, we have made three significant modifications to the core content of our paper, all of which aim to enhance clarity and address the concerns raised.\n\nTaking into account your suggestion, we have removed **Theorem 3.1** from the main body of our paper. Additionally, we've improved the explanation and included an ablation study on our method of defining pockets. This ablation study, focusing on the cutoff values and primarily a hyperparameter issue, is placed in the Appendix due to its more limited scope of insight. We can move it to the main body of the paper in the future if needed. We have also expanded our discussion on the differences between our dataset and the PDBBind dataset, elucidating how fixed molecule encoders contribute to mitigating this discrepancy.\n\nIn **Section 3.1**, we have introduced a new paragraph dedicated to providing readers with a deeper understanding of non-covalent interactions. To facilitate comprehension, we have included visualizations featuring three types of interactions, incorporating both real and pseudo receptor-ligand pairs in the new **Figure 2**. Furthermore, we delve into the details of how intra-protein interactions mirror protein-ligand interactions, involving specific types of amino acids.\n\nThe biological justifications for the efficacy of interaction-aware pretraining are now more extensively explored in our revised paper. Specifically, we present a compelling showcase featuring estradiol-binding proteins, offering insights into the geometric disparities between the compared proteins and highlighting the similarities in their binding interfaces in the new **Figure 4a**. The existing visualization of BioLip pocket representations in the new **Figure 4b** can make our claims more convincing. Together, these visual aids illustrate how our interaction-aware pretraining empowers models to focus on crucial interface residues, discern subtle distinctions, and disregard irrelevant geometric dissimilarities.\n\nWe have also included an in-depth discussion of pretrained molecule encoders in the newly added **Section 2.3**, further enriching the theoretical foundation of our work.\nAll changes can be found in the renewed paper, which is updated lively.\n\nWe genuinely value the constructive input you provided during the review process, as it has undoubtedly contributed to the refinement of our paper. Your insights have proven invaluable in elevating the clarity and depth of our contributions to the field. We express our gratitude for the time and expertise invested in reviewing our work, and we look forward to any additional feedback or suggestions you may have.\n\n---\n\n# Response 2/2\n\n## Response to the question on N Terminal and C Terminal\n\nWhen a peptide bond is formed (green), one amine group and one carboxy group are left untouched. Therefore, in a linear polypeptide, there would always be a free amine group and a free carboxy group, and they are called the N terminal(blue) and the C terminal(red) respectively in biochemistry. You can find the visualization in **Appendix H**.\n\n## Response to fixing the molecule encoder in contrastive learning\n\nFrom an **effectiveness** standpoint, as you mentioned before, there are unavoidable discrepancies between pseudo and true ligands. If we didn't fix our molecule encoder during pretraining, then the conditions of Theorem 3.1 might not be met. The ablation results presented in the previous response clearly indicate a performance decline when the molecular encoder is not fixed during contrastive learning, particularly in the context of pocket matching. This supports our assertion that the pocket encoder struggles to acquire chemical knowledge without the stability provided by a fixed molecular encoder.\n\nFrom an **efficiency** standpoint, a fixed molecule encoder also offers significant advantages. It reduces memory usage and accelerates training speed because all molecule embeddings can be pre-computed before training. This eliminates the need for feed-forward and back-propagation operations of the molecule encoder during training, leading to efficiency gains. Additionally, adapting to different molecule encoders becomes more straightforward when they are not co-trained during the pretraining process. This approach allows for more flexibility and ease in integrating various encoding methodologies **even if the molecule encoder is not accessible for training**. That is to say, our approach can even be used with undifferentiable molecular representations.\n\n## Response to extending to other tasks like protein-protein interaction prediction\n\nThank you for pointing out the potential application of our framework to protein-protein interaction (PPI) tasks. We have to point out that the interactions between proteins and proteins are far different from proteins and molecules. Specifically, the interaction areas are usually much larger, but shallower, and involved functional groups are less diverse due to limited types of natural amino acids. Therefore, several changes should be made to extend our ProFSA method to PPI tasks:\n\n1. As for data creation, unlike the localized and specific nature of pocket-ligand interactions, protein-protein interactions are **more global and involve broader protein features**. This requires a shift in the data creation process to split entire protein structures into domains instead of short fragments and to capture potential interactions between them. \n\n2. The interaction interface between protein and protein is much larger. We might need to change from atomic-level to residue-level representations to efficiently capture interaction information.\n\n3. For contrastive learning training, ProFSA uses a fixed pretrained molecular encoder to cope with various small molecules and their atomic representation. However, for protein-protein interaction, it is not necessary as we could only model 20 types of amino acids at the residue level.\n\nDespite several changes that need to be done for PPI tasks, we find that we could divide protein complexes into local interactions. When the complex structures and monomer structures are provided, we have achieved a reasonable result on a flexible PPI affinity prediction benchmark with our current model in a zero-shot way. \n\n| Method            | Spearman $\\uparrow$  |\n|-------------------|----------------------|\n| SchNet            | 0.072 ± 0.021        |\n| DimeNet++         | 0.171 ± 0.054        |\n| EGNN              | 0.080 ± 0.038        |\n| TorchMD           | 0.117 ± 0.008        |\n| GET               | **0.363 ± 0.017**    |\n| ProFSA(zero-shot) | _0.248_              |\n\nSurprisingly, as a **zero-shot** method, ProFSA is able to outperform other **supervised learning** models except for GET, a newly proposed unified model. This demonstrates our approach is also able to capture protein-protein interaction information despite it being designed for protein-ligand interaction. \n\nA detailed explanation of this experiment is shown in **Appendix C6**.\n\n---\n\n# Response 1/2\n\n## Response to evidence for the construction of the pseudo-ligand and using peptides to replace small molecules \n\nAs we discussed in the paper, pseudo-ligands share similar sizes as real ligands, and they also make similar non-covalent interactions with the pocket. To further support our proposal, we would like to cite another paper **\"A defined structural unit enables de novo design of small-molecule–binding proteins\"**[1], published in Science 2020, which supports that **intra-protein interactions are similar to protein-ligand interactions.** That is the reason we use peptides to **represent** small molecules to let the pocket encoder learn the interaction information. \n\nAlso, we'd like to clarify that our intention is not to **\"replace\"** small molecules with peptides. It's a pretraining framework that leverages abundant protein-only data and uses peptides to **simulate** pocket-small molecule interactions for enhanced pocket representations. Following this pretraining phase, **the model can be finetuned with datasets that include real small molecules, ensuring its effectiveness in practical applications**.\n\nTo demonstrate the validity of constructing pseudo-ligands, we provide visualizations in **Appendix F** showing that interactions between pockets and our pseudo-ligands are similar to those with true ligands. These figures reveal shared interaction types like **hydrogen bonding**, **$\\pi-\\pi$ stacking**, and **salt bridge**, depicted with color-coded dashed lines for each interaction type. This should help clarify the rationale and evidence supporting our approach.\n\n[1] Polizzi, Nicholas F., and William F. DeGrado. \"A defined structural unit enables de novo design of small-molecule–binding proteins.\" Science 369, no. 6508 (2020)\n\n## Response to ablation studies evaluating the impact of critical design choices\n\nThanks for your advice on adding more ablation studies on design choices. We did ablation studies on the effectiveness of distributional alignment, where the major difference is that fragment sizes are modified. We found that aligned fragment sizes with real ligands could provide the best performance. The result is shown in **Table 5**.\n\nAs for the distance thresholds, we define the pocket following the UniMol setup. We also found our model can also adapt to an 8Å setup in the ToughM1 experiment. Following your advice, we did an ablation study on the distance thresholds. We tested our method with three different thresholds: 4Å, 6Å, and 8Å. The result is shown below:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| 4Å | 0.7062 | 0.7549 | 0.1240 | 0.1095 | 28.29 | 13.07 |\n| 6Å | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| 8Å | 0.8322 | 0.8292 | 0.1256 | 0.1125 | 34.83 | 12.92 |\n\nSince the 8Å threshold corresponds with the pocket definition in the Kahraman and Tough M1 datasets, it leads to optimal results. Our decision to use a 6Å threshold was made to align with the methodology of pretraining data creation by Uni-Mol, facilitating a fair comparison. Notably, even with the 6Å threshold, we achieved strong results in the pocket-matching task, which serves as a testament to the effectiveness of our approach.\n\n## Response to substantial discrepancies between pseudo and real ligands\n\nThank you for pointing out the discrepancies between pseudo and real ligands. We understand and acknowledge your concern. To mitigate the impact of this discrepancy, we have made several efforts. First, we performed **distribution alignment** to make the data distribution of pseudo-ligands more similar to that of true ligands in PDBbind. Additionally, we **fixed the molecule encoder during pretraining** to prevent it from being misled by the discrepancy. Our ablation studies have shown that these strategies are effective:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nThe table above shows that lacking distribution alignment and not fixing the molecular encoder during pretraining both lead to lower performance, underscoring the effectiveness of our methods in reducing discrepancies.\n\nTheorem 3.1 theoretically supports our approach's efficacy, even with discrepancies between pseudo and real ligands. Empirically, our method outperforms other pretraining techniques, confirming its effectiveness despite these discrepancies.\n\n---\n\n## Response to limited tasks and potential in protein-ligand binding pose prediction\n\nIn our paper, the primary objective is to assess the effectiveness of our pretraining method and the quality of the trained pocket representations. To achieve this, we focused on two types of downstream tasks: pocket-only tasks (pocket property prediction and pocket matching) and pocket-ligand interaction tasks (ligand binding affinity prediction).\n\nWe appreciate the suggestion to apply our method to the protein-ligand binding pose prediction task. However, it's important to note that our current evaluation framework is designed to specifically assess **pocket representations**. For the tasks we chose, the architecture is straightforward: either a **zero-shot** evaluation or a **simple MLP** for mapping embeddings to predictions. In contrast, protein-ligand binding pose prediction often involves **complex methodologies**. For instance, state-of-the-art methods like DiffDock[1] require training a diffusion generative model and a separate confidence model, while other approaches like EDM-Dock[2], rely on reconstructing ligand conformations from predicted distance maps. These methods are not end-to-end and do not directly align with our objective of evaluating pretrained pocket representations.\n\nNevertheless, we recognize the potential of our method in enhancing existing binding pose prediction techniques. To integrate our approach, we would need to modify our framework. Using the same data creation strategy, we could train a binding pose prediction model with our preprocessed data, which could then be further fine-tuned using real pocket-ligand pair data from sources like PDBbind. Additionally, our data could be used to train a side-chain packing model, allowing for side-chain flexibility during docking. Thank you again for your advice, and we will leave protein-ligand binding pose prediction as a separate future work for our method.\n\nIn response to the need for evaluating our method on a broader range of tasks, we have extended our analysis to include two additional downstream tasks: LEP (Ligand Efficacy Prediction), and PPA (Protein-Protein Affinity Prediction).\n\nresult for LEP:\n\n| Method      | AUROC $\\uparrow$ | AUPRC $\\uparrow$ |\n|-------------|--------------|--------------|\n| ATOM3D-GNN  | 0.681        | 0.598        |\n| GeoSSL      | 0.776±0.03   | 0.694±0.06   |\n| Uni-Mol     | 0.782±0.02   | 0.695±0.07   |\n| ProFSA      | 0.840±0.04   | 0.806±0.04   |\n\nGeoSSL and Uni-Mol, both pretraining methods, yield comparable results. However, ProFSA outperforms these methods, demonstrating the advantage of our pocket pretraining approach.\n\nresult for PPA:\n\n| Method            | Spearman $\\uparrow$  |\n|-------------------|----------------------|\n| SchNet            | 0.072 ± 0.021        |\n| DimeNet++         | 0.171 ± 0.054        |\n| EGNN              | 0.080 ± 0.038        |\n| TorchMD           | 0.117 ± 0.008        |\n| GET               | **0.363 ± 0.017**    |\n| ProFSA(zero shot) | _0.248_              |\n\nAs a **zero-shot** method, ProFSA is able to outperform other **supervised learning** models except for GET, a newly proposed unified model. This demonstrates our approach is able to capture protein-protein interaction information despite it being designed for protein-ligand interaction modeling.\n\nYou can find detailed experiment settings and results in **Appendix C5 and C6**.\n\n[1] Corso et al., \"Diffdock: Diffusion steps, twists, and turns for molecular docking.\", ICLR 2023.\n[2] Masters et al., \"Deep learning model for efficient protein–ligand docking with implicit side-chain flexibility.\" Journal of Chemical Information and Modeling 63, no. 6 (2023).\n\n## Response to the issue of lack of latest baselines\n\nFor the pocket matching task, CoSP is a newly proposed baseline which is published in ECML PKDD 2023. Alongside CoSP and Uni-Mol, we selected some of the most effective baseline results presented in the CoSP paper. We also tried to evaluate recent methods like PocketAnchor (Li et al., Cell Systems 2022) on our own since they are not tested on the pocket matching task. However, since the result didn't outperform other baseline machine learning methods(Uni-Mol, CoSP, and DeeplyTough), we decided not to include it in our final baseline comparison. We can include it in the camera-ready version if needed.\n\nFor the ligand binding affinity task, a lot of newly proposed baselines are included, i.e. ProNet (Wang et al., 2022b); as well as pretraining methods such as GeoSSL (Liu et al., 2023), EGNN-PLM (Wu et al., 2022), DeepAffinity (Karimi et al., 2019) and Uni-Mol (Zhou et al., 2023).\n\n---\n\n## Response to lack of related works\n\nThanks for providing us with more insightful related works that could support our arguments. We actually have cited these three papers in the section on ligand binding affinity experiments, but we are also happy to add these in Related Works section 2.2. You can find the change in pdfdiff.\n\n## Response to lack of COSP as a baseline in Table 3\n\nBecause best to our knowledge, they did not release their code and they did not test their method on the ligand binding affinity dataset. \n\n## Response to the question on the loss terms\n\nThank you for your advice. We apologize that we accidentally mentioned loss1 and loss2 in reverse, and we are sorry for the confusing statement. You are correct that the first loss is to identify the true protein pocket when given a pseudo-ligand. We revise the original statement to: \"The primary purpose of the first loss is to identify the true protein pocket from a batch of samples when given a pseudo-ligand. Similarly, the second loss seeks to identify the corresponding ligand fragment for a given pocket.\" You can also find the change in pdfdiff.\n\n## Response to the question on the length of the pocket representation without alignment\n\nWith or without distributional alignment, pockets are always defined by the given protein fragment with a fixed distance cutoff (6Å in our works, following the UniMol setup). The distributional alignment process merely samples these pairs to match the sizes of real ligands and pockets. Without distributional alignment, fragments are uniformly sampled from 1 to 7 residues. Pockets are similarly determined as with residues within the range of 6Å around peptide fragments.\n\n---\n\n# Question about the confidence score\n\nThanks for your review. We found your understanding and judgment of our paper comprehensive and precise. Also, you provided detailed and thoughtful advice for our paper, which helped a lot. We sincerely appreciate your review, but we also wonder if there is any misunderstanding here, as you only give a confidence score of 1.\n\n---\n\n# Response 3/3\n\n## Response to biological justification and visualization of the results\n\nOur method's **biological justification** for achieving good experimental results is based on the properties and interactions of pseudo-ligands. As discussed in our paper, these pseudo-ligands are similar in size to real ligands and engage in comparable non-covalent interactions with protein pockets. This fundamental similarity is crucial for the effectiveness of our approach.\n\nMoreover, our methodology is further supported by the findings in the 2020 Science paper titled \"A defined structural unit enables de novo design of small-molecule–binding proteins.\"[1] This research underscores that **intra-protein interactions are analogous to protein-ligand interactions**, which validates our use of peptides as proxies for small molecules. By employing peptides as pseudo-ligands in this manner, our pocket encoder is able to learn and replicate the interaction dynamics typically observed in real ligand scenarios. This understanding is pivotal to the success of our method in downstream applications.\n\nTo better justify our methods and results, following your advice, we also provide some visualizations in **Appendix F**. **Figure 6** illustrates that the pseudo pairs we created share various interaction types commonly found in real pocket-ligand pairs, such as **hydrogen bonding, $\\pi-\\pi$ stacking, and salt bridge**. In the figures, each type of interaction is represented by dashed lines, color-coded to correspond with the specific interaction type.\n\nAnother visualization of our pocket-matching result is shown in **Figure 7**. We showed an example of two non-homology ATP binding pockets to explain why pocket-matching can benefit from interaction-aware pretraining so that we have achieved superior results in the Kahraman dataset and the BioLip t-SNE visualization. The PEP carboxykinase(PDB:1AYL) and tRNA synthetase(PDB:1B8A) are two ATP-binding proteins that share zero sequence similarity (verified with the BLAST) extracted from the Kahraman dataset. However, as they are both fueled by ATP, their binding site shares similar binding patterns. The cation-$\\pi$ interactions(blue dash lines) and salt bridges (magenta dash lines) are important to the ATP binding, which can be viewed as convergent evolutions at the molecule level. Even though, these two pockets are very distinct in terms of shapes and sizes because they bind ATPs in different conformations. Therefore, biochemical interactions are the key to accomplishing the pocket-matching task, which is ignored in previous self-supervised learning methods like Uni-Mol.\n\n[1] Polizzi, Nicholas F., and William F. DeGrado. \"A defined structural unit enables de novo design of small-molecule–binding proteins.\" Science 369, no. 6508 (2020)\n\n---\n\n# Response 2/3\n\n## Response to the bound of Theorem 3.1 \n\nWe are not very clear about the point of your question. We have two understandings and they are replied as follows.\n\nIf you mean $ ||t − t ^{(0)}|| < 1/2 l_T$ is trivial, please note that it is only a condition of the theorem. As you said, many other encoders can also satisfy this condition. This shows that our method is applicable to many different molecular encoders with or without pretraining as we proved in **Table 4**. Moreover, the condition is only a necessary condition. We also need the pretraining contrastive loss to be sufficiently small to guarantee a small contrastive loss between pockets and real ligands. In fact, in the process of our early exploration, we noticed that some encoders cannot achieve a low pretraining loss, indicating the entire conditions of the theorem are nontrivial.\n\nIf you mean that the conclusion of theorem 3.1 is trivial, we want to emphasize that our conclusion is $\\lim_{L_i(t, s) =0}L_i(t^{(0)}, s) =0$. (We write it in a $\\epsilon$-$\\delta$ language in paper. They are equivalent forms.) Our conclusion shows the loss containing real ligands is consistent with the loss containing pseudo ligands that we optimized in pre-training. \n\nIn contrast,  a trivial result from $||t − t ^{(0)}|| < 1/ 2l_T$ is that $\\lim_{L_i(t, s) =0}L_i(t^{(0)}, s) <B$, B is a bound related to l_t and the representations. It does not guarantee a consistent loss and is substantially different from our result.\nTherefore, our theorem is nontrivial and reveals the transfer ability of our contrastive pre-training from the pseudo-ligand domain to the real ligand domain.\n\nAs for this theoretical result, we are open to further discussion.\n\n## Response to molecules possess more properties that can also lead to imbalance\n\nIt is correct that molecules have more properties than their size, like logP, Hbond donor and acceptor number, and rotatory bond number. As small molecule drugs are mostly designed to penetrate barriers like gut or cell membranes, they usually have much larger logP values, which means more hydrophobic. For the same reason, they usually have fewer hydrogen bond donors and acceptors, and only minimal essential ones are kept for specified interactions. Also, to minimize the entropy effect upon binding and to increase binding affinity, rotatory bonds are also unfavored. However, it is impossible to mimic such features with peptides, as the backbone of peptides is intrinsically hydrophilic and flexible. As we showed in a new figure, our pseudo-ligands are less similar to real ligands in those properties even with size alignment. You can find the figure in the new pdf in **Appendix G**.\n\nWe believe that the aforementioned divergence is the main limitation of poor zero-shot performance in predicting hydrophobicity scores (**Table 1**). However, as demonstrated in previous publications like CoSP, we could leverage real ligand-pocket pairs from PDBBind or BioLip database to further finetune our network, as an extension of the pipeline. In this way, we could handle property mismatches but still enjoy the power of our large-scale pretraining.\n\n## Response to the approach to defining pockets \n\nWe define the pocket following the UniMol setup. We found our model can also adapt to an 8Å setup in the toughM1 experiment \n\nIn particular, we define the pocket for each protein-ligand pair as residues of the protein that have at least one atom within the range of 6Å from a heavy atom in the ligand. To further explain our design, we have done an ablation study on different choices of thresholds of 4Å, 6Å, and 8Å:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| 4Å | 0.7062 | 0.7549 | 0.1240 | 0.1095 | 28.29 | 13.07 |\n| 6Å | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| 8Å | 0.8322 | 0.8292 | 0.1256 | 0.1125 | 34.83 | 12.92 |\n\nSince the 8Å threshold corresponds with the pocket definition in the Kahraman and Tough M1 datasets, it leads to optimal results. Our decision to use a 6Å threshold was made to align with the methodology of pretraining data creation by Uni-Mol, facilitating a fair comparison. Notably, even with the 6Å threshold, we achieved strong results in the pocket-matching task, which serves as a testament to the effectiveness of our approach.\n\n---\n\n# Response 1/3\nWe appreciate the time and effort you have dedicated to reviewing our paper, and we are grateful for your constructive feedback and thoughtful evaluation of our work. We would like to address your comments about the technical novelty of our work and provide additional clarification on certain aspects of our paper.\n\n## Response to technical novelty\n\nFirstly, we acknowledge your observation that we didn't use any fancy models compared with typical **model-centric** works. While we respect your assessment, we would like to highlight that our primary focus in this paper is a **data-centric** pretraining method to introduce groundbreaking improvements in the field of protein pocket pretraining. Our work aims to address a critical challenge of data scarcity in the field of protein pocket representation by constructing large-scale synthetic data that facilitates the pretraining of models, ultimately enhancing the accuracy and robustness of protein pocket representations. We believe that the research community can conduct fast following-ups on our **released dataset** with more sophisticated models, and even extend our pipelines to other tasks like docking or drug design. We also would like to point out that we have provided a novel method to **distill knowledge from well-trained molecule models** to protein models. Though employing the same contrastive loss, our approach is different from existing contrastive learning models since it uses a fixed molecular encoder. The motivation here is to use a well-trained molecular encoder on a relatively larger dataset to **guide** the training of the protein encoder. For example, some quantum-chemistry properties that are difficult to compute for large systems like proteins could be distilled from molecules, by using some quantum-chemistry-aware molecular encoders such as Frad. Though our framework seems to be simple, it is non-trivial to make it work. We have made several efforts to solve the unavoidable discrepancies between true ligands and pseudo ligands. The ablation study shows that our efforts are effective and necessary:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nRegarding your positive acknowledgment of our \"simple but effective model for the right question,\" we are pleased to hear that our approach resonates with the objective we set out to achieve. We designed our model with **simplicity** in mind, prioritizing effectiveness and practical utility for the specific problem domain. This deliberate choice aligns with the notion that sometimes the most impactful solutions are elegantly straightforward. Also, we intentionally borrowed a pocket encoder from Uni-Mol to make a **fair comparison** with it, which strongly supports the power of our dataset. Similar to the molecule encoder, our pocket encoder can be changed to any other model due to our flexible framework. Notably, **we didn't load the pretrained weights of Uni-Mol pocket encoder**. We only use the same backbone architecture and the pretraining was completely done on our processed data with our training strategy. \n\nNotably, many simple but effective approaches have been recognized by top-tier conferences and journals. A prime example of this is in the field of protein language models, like the ESM series and ProtTrans. These studies adapted the Transformer architecture and masked language modeling techniques from natural language processing to protein sequences. While they didn't introduce groundbreaking techniques, their substantial contributions to protein modeling are evident, with publications in prestigious conferences and journals like NeurIPS, ICML, TPAMI, PNAS, and Science. Another example is the widely acclaimed CLIP paper presented at ICML 2021. CLIP, while not employing novel techniques, stands as a hallmark of data-centric deep learning. Its use of contrastive learning enabled training on expansive web-sourced text-image datasets, moving beyond the constraints of meticulously curated databases like MS-COCO. This aligns with our approach to addressing **data scarcity challenges** by facilitating training on large-scale datasets, mirroring our strategy for overcoming similar hurdles in the protein pocket modeling domain.\n\nWe appreciate your efforts to remain neutral and understand the challenges in decision-making. We believe further discussion will highlight our paper's contributions. Your feedback and insights in the upcoming discussion will be invaluable for refining our work.", "author_response": "# Response 1/3\nWe appreciate the time and effort you have dedicated to reviewing our paper, and we are grateful for your constructive feedback and thoughtful evaluation of our work. We would like to address your comments about the technical novelty of our work and provide additional clarification on certain aspects of our paper.\n\n## Response to technical novelty\n\nFirstly, we acknowledge your observation that we didn't use any fancy models compared with typical **model-centric** works. While we respect your assessment, we would like to highlight that our primary focus in this paper is a **data-centric** pretraining method to introduce groundbreaking improvements in the field of protein pocket pretraining. Our work aims to address a critical challenge of data scarcity in the field of protein pocket representation by constructing large-scale synthetic data that facilitates the pretraining of models, ultimately enhancing the accuracy and robustness of protein pocket representations. We believe that the research community can conduct fast following-ups on our **released dataset** with more sophisticated models, and even extend our pipelines to other tasks like docking or drug design. We also would like to point out that we have provided a novel method to **distill knowledge from well-trained molecule models** to protein models. Though employing the same contrastive loss, our approach is different from existing contrastive learning models since it uses a fixed molecular encoder. The motivation here is to use a well-trained molecular encoder on a relatively larger dataset to **guide** the training of the protein encoder. For example, some quantum-chemistry properties that are difficult to compute for large systems like proteins could be distilled from molecules, by using some quantum-chemistry-aware molecular encoders such as Frad. Though our framework seems to be simple, it is non-trivial to make it work. We have made several efforts to solve the unavoidable discrepancies between true ligands and pseudo ligands. The ablation study shows that our efforts are effective and necessary:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nRegarding your positive acknowledgment of our \"simple but effective model for the right question,\" we are pleased to hear that our approach resonates with the objective we set out to achieve. We designed our model with **simplicity** in mind, prioritizing effectiveness and practical utility for the specific problem domain. This deliberate choice aligns with the notion that sometimes the most impactful solutions are elegantly straightforward. Also, we intentionally borrowed a pocket encoder from Uni-Mol to make a **fair comparison** with it, which strongly supports the power of our dataset. Similar to the molecule encoder, our pocket encoder can be changed to any other model due to our flexible framework. Notably, **we didn't load the pretrained weights of Uni-Mol pocket encoder**. We only use the same backbone architecture and the pretraining was completely done on our processed data with our training strategy. \n\nNotably, many simple but effective approaches have been recognized by top-tier conferences and journals. A prime example of this is in the field of protein language models, like the ESM series and ProtTrans. These studies adapted the Transformer architecture and masked language modeling techniques from natural language processing to protein sequences. While they didn't introduce groundbreaking techniques, their substantial contributions to protein modeling are evident, with publications in prestigious conferences and journals like NeurIPS, ICML, TPAMI, PNAS, and Science. Another example is the widely acclaimed CLIP paper presented at ICML 2021. CLIP, while not employing novel techniques, stands as a hallmark of data-centric deep learning. Its use of contrastive learning enabled training on expansive web-sourced text-image datasets, moving beyond the constraints of meticulously curated databases like MS-COCO. This aligns with our approach to addressing **data scarcity challenges** by facilitating training on large-scale datasets, mirroring our strategy for overcoming similar hurdles in the protein pocket modeling domain.\n\nWe appreciate your efforts to remain neutral and understand the challenges in decision-making. We believe further discussion will highlight our paper's contributions. Your feedback and insights in the upcoming discussion will be invaluable for refining our work."}
{"claim": "The experimental results lack biological justification and do not include visualizations to support the claimed structural or biological relevance.", "claim_type": "experimental", "paper_id": "uMAujpVi9m", "paper_title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "9ap4s1SZlZ", "reviewer": "Reviewer_yfM3", "review_text": "Summary: This paper primarily aims to enhance the pocket pretraining method, as existing approaches only consider pockets during pretraining. There are two main contributions in this paper: (1) The authors introduce a novel method, ProFSA, for pocket pretraining, which extracts additional information from corresponding ligands. However, the number of pocket-ligand complex structures is quite limited in existing datasets. (2) To address this issue, the authors generate over 5 million complexes by segmenting fragments and their corresponding pockets in protein structures. By aligning features of fragments and pockets, the pocket encoder learns the interaction between fragments and pockets. The authors design downstream tasks such as pocket druggability prediction, pocket matching, and ligand binding affinity prediction to demonstrate the effectiveness of ProFSA.\n\nStrengths: The authors propose a new perspective of pretraining pockets and construct a large-scale dataset, which data distribution is also considered, to make the efficient pre-training possible.\n\nThe results are competitive, especially for zero-shot settings.\n\nAbundant experiments and ablation study support the argument and result of the authors.\n\nWeaknesses: 1. The technical novelty is limited.\n  - The pocket encoder is borrowed from Uni-Mol.\n  - The contrastive loss is the vanilla form of classical contrastive learning.\n\n2. The bound of Theorem 3.1 is trivial. The authors claim that the bound naturally exists for these representations extracted by pretrained molecule models. However, it's a bit counterintuitive, because many models not pretrained on molecule datasets also fulfill this prior. So, can these models be used for this task? **I strongly suggest removing this part from the paper**.\n\n3. Some issues about dataset creation:\n - 3.1. The authors consider the distribution of ligand size and pocket size when designing the dataset. However, molecules possess more properties that can also lead to imbalance. It would be better to, at least, add some discussion about this issue.\n - 3.2. In the second stage of the data construction process, the approach to defining pockets needs further explanation or an ablation study.\n\n4. Experiments: It would be better to add some biological justification or visualization of the results.\n\nFor this paper, one fact is that the technical novelty is below the bar of ICLR. However, I admire the simple but effective model for the right question. It's a struggle for me to make a decision. I will maintain a neutral attitude and make my final decision after the discussion.\n\nQuestions: See weakness.", "labeling_timestamp": "2026-01-11T16:25:27.445440", "model": "gpt-5-mini", "label": "Supported", "justification": "Authors explicitly acknowledge the concern, add biological explanation and cite literature, and state they have added multiple visualizations (Figure 2, Figure 4, Appendix F/G) to demonstrate interaction similarity and biological rationale.", "evidence": "\"We add a new Figure 2 to illustrate different types of non-covalent interactions...\"; \"we provide visualizations in Appendix F showing that interactions between pockets and our pseudo-ligands are similar to those with true ligands.\"", "combined_author_response": "Dear AC and reviewers,\n\nWe sincerely appreciate your time and efforts in reviewing our work. Based on your suggestions, we have revised our paper. We would like to use this section to reiterate the explanation for some common concerns and summarize the contributions of our paper.\n\nIn our revised paper, we provide a detailed explanation of the motivation and theory behind our method: **the mechanisms of pocket-peptide interactions and pocket-ligand interactions are very similar**. Our proposed **data-centric** pretraining pipeline is unique in its use of peptides to mimic small molecules, a method that is, to our knowledge, **the first of its kind**. We expanded our discussion about its biological insights in **Section 3**. We add a **new Figure 2 to illustrate different types of non-covalent interactions** that are common for both ligand-protein data and intra-protein data. Following this theory, we have designed a pipeline to generate **over 5 million** synthetic ligand-protein pairs, which greatly eases the **data scarcity** problem in the field of protein-ligand interaction learning. We also prove our data quite effective in various downstream tasks, including pocket property prediction, pocket matching, ligand binding affinity prediction, ligand efficiency prediction, and protein-protein interaction tasks. The **last two tasks are newly added** to further support our method (**Appendix C5 and C6**). We are confident that our **ready-to-release synthetic dataset** will significantly benefit the AI for drug discovery community. Its impact extends beyond enhancing pocket representations in downstream analyses, as it also equips researchers with a valuable resource for a range of applications, such as protein-ligand docking, structure-based drug design, and virtual screening. Importantly, these areas often face challenges due to **a lack of training data necessary for learning effective binding patterns between protein pockets and ligands**. \n\nIn our revised paper, we recognize the differences between real ligands and pseudo ligands (peptides). We address these discrepancies and present a distribution plot of various properties in **Appendix G**. Our approach primarily leverages the **analogous nature of ligand-pocket and peptide-pocket interactions**. Therefore, despite these differences, our model can **still extract valuable interaction information from the pretraining dataset**, which enhances its performance in downstream tasks. This is illustrated in **Figure 4(a)**, showing the benefits of interaction-aware pretraining. We have also undertaken measures like distribution alignment and freezing the molecular encoder to mitigate the impact of these discrepancies. The positive results of these efforts, as evidenced in **Table 5**, confirm their effectiveness and necessity. Notably, our pretrained model can be **further finetuned with real labeled ligand-pocket pairwise data**, as we show in the Ligand Binding Affinity task in **section 4.3**.\n\nAs for other questions, we have added more ablation studies on our design choices, including the impact of a fixed molecular encoder (Table 5), and different threshold values to define pockets(Appendix C.4). \n\nWe are immensely grateful for the invaluable feedback from all the reviewers, which has guided us in refining and clarifying our work. We hope that the revised version of our paper, coupled with the discussion period, will more clearly highlight the novelty, effectiveness, and contributions of our approach to the field.\n\n---\n\nThank you for your invaluable feedback on our paper. In response to your insightful suggestions, we have made several modifications in our revised paper. \n\nWe have cited additional papers in Section 2.2 following your advice. The description of loss terms in Section 3.2 has been corrected.  COSP was not included as a baseline for the Ligand Binding Affinity (LBA)  task because it is not open-sourced, and its performance on LBA was not evaluated in their published work. We used it as a baseline in the pocket-matching task.\n\nIn Section 3, we've expanded our discussion to answer your question on **ProFSA's effectiveness without distributional alignment**. This section now offers a detailed explanation of the foundational concepts and justifications for our method. We draw attention to the parallels between ligand-protein and intra-protein non-covalent interactions, as shown in **Figure 2**. This comparison supports our strategy of using peptides as stand-ins for actual ligands to mimic pocket-ligand interactions. Consequently, **even in the absence of distributional alignment, the types of interactions remain comparably relevant**, which explains why ProFSA continues to function, albeit with reduced effectiveness, as demonstrated in Table 5. We also want to clarify that distributional alignment is only applied to fragment-pocket complex as a whole, and **it would not change the definition of the pocket with a given ligand**. Without this alignment, these fragments are chosen uniformly, with lengths varying from 1 to 7 residues, and pockets are consistently identified based on residues within a 6Å radius of these peptide fragments.\n\nWe are grateful for the time and effort you have dedicated to reviewing our work. Your thorough and constructive feedback has significantly contributed to the refinement of our research. Please let us know if there are any other aspects of our paper that you would like us to address or clarify. We look forward to your further feedback.\n\n---\n\nThank you for your insightful feedback. In response to your concerns regarding the scope of our tasks, we have expanded our research in the revised paper. We now include additional tasks such as **ligand efficiency prediction** and **protein-protein affinity prediction**, detailed in **Appendix C5 and C6**. These tasks were incorporated to enhance the robustness of our method. We acknowledge that they are currently in the appendix due to space constraints, but we are open to relocating them to the main text if it would be beneficial.\n\nRegarding your concern about the lack of recent baselines, we provided a detailed explanation in our previous response. We want to reiterate that our benchmarks do include the latest baselines, with some as recent as 2023.\n\nWe greatly appreciate the time and effort you have invested in reviewing our work. Your feedback has been invaluable in refining our research. Please let us know if there are any other aspects of our paper that you would like us to address or clarify. We look forward to your further feedback.\n\n---\n\nIn our revised paper, we have incorporated changes following feedback from another reviewer, which we believe also addresses your concerns and queries.\n\nRegarding your question on the evidence supporting our approach, particularly the use of peptides to simulate small molecules, we've enriched **Section 3.1**. A new paragraph there now provides an in-depth explanation of non-covalent interactions. To aid understanding, **Figure 2** has been updated with visualizations that include both real and pseudo receptor-ligand pairs, illustrating three types of interactions. Furthermore, we delve into the details of **how intra-protein interactions mirror protein-ligand interactions**, involving specific types of amino acids.\n\nTo better evaluate our design choices, the ablation studies have been extended. Alongside the existing studies in **Table 4** (different pretrained molecular encoders), **Table 5** (distribution alignment), and **Figure 5** (data scale). we've added a study on the impact of using a fixed molecular encoder in **Table 5** in the main text to address your question on the fixed molecular encoder. We've also included an ablation study on varying **cutoff values for pocket definition**, placed in the **Appendix C.4** due to its more focused scope. We can move it to the main part of the paper in the future if needed.\n\nWe acknowledge your concern about the **differences between true and pseudo ligands**, so we've expanded our discussion on the distinct properties of our curated dataset versus the PDBBind dataset in **Figure 9** and **Section 3.2**. The ablation study in **Table 5** demonstrates our methods and their effectiveness in minimizing these discrepancies. Notably, the performance significantly drops without our distribution alignment or fixed molecular encoder, highlighting our efforts to mitigate these issues. The updated **Figure 2** further supports this, showing that while the properties of true and pseudo ligands may differ, **the interaction types are consistent.** We hope these results can ease your concerns about our synthetic dataset, and they also respond to your query about **why we kept the molecular encoder constant during pocket pretraining.**\n\nWe appreciate the constructive critiques and thoughtful engagement from you. Your feedback has undeniably contributed to the refinement and strengthening of our paper. We sincerely value the time and expertise invested in the review process, and we look forward to any further suggestions you may have.\n\n---\n\nWe sincerely appreciate the thoughtful feedback provided on our paper and have diligently incorporated your suggestions into this revised version. In response to your guidance, we have made three significant modifications to the core content of our paper, all of which aim to enhance clarity and address the concerns raised.\n\nTaking into account your suggestion, we have removed **Theorem 3.1** from the main body of our paper. Additionally, we've improved the explanation and included an ablation study on our method of defining pockets. This ablation study, focusing on the cutoff values and primarily a hyperparameter issue, is placed in the Appendix due to its more limited scope of insight. We can move it to the main body of the paper in the future if needed. We have also expanded our discussion on the differences between our dataset and the PDBBind dataset, elucidating how fixed molecule encoders contribute to mitigating this discrepancy.\n\nIn **Section 3.1**, we have introduced a new paragraph dedicated to providing readers with a deeper understanding of non-covalent interactions. To facilitate comprehension, we have included visualizations featuring three types of interactions, incorporating both real and pseudo receptor-ligand pairs in the new **Figure 2**. Furthermore, we delve into the details of how intra-protein interactions mirror protein-ligand interactions, involving specific types of amino acids.\n\nThe biological justifications for the efficacy of interaction-aware pretraining are now more extensively explored in our revised paper. Specifically, we present a compelling showcase featuring estradiol-binding proteins, offering insights into the geometric disparities between the compared proteins and highlighting the similarities in their binding interfaces in the new **Figure 4a**. The existing visualization of BioLip pocket representations in the new **Figure 4b** can make our claims more convincing. Together, these visual aids illustrate how our interaction-aware pretraining empowers models to focus on crucial interface residues, discern subtle distinctions, and disregard irrelevant geometric dissimilarities.\n\nWe have also included an in-depth discussion of pretrained molecule encoders in the newly added **Section 2.3**, further enriching the theoretical foundation of our work.\nAll changes can be found in the renewed paper, which is updated lively.\n\nWe genuinely value the constructive input you provided during the review process, as it has undoubtedly contributed to the refinement of our paper. Your insights have proven invaluable in elevating the clarity and depth of our contributions to the field. We express our gratitude for the time and expertise invested in reviewing our work, and we look forward to any additional feedback or suggestions you may have.\n\n---\n\n# Response 2/2\n\n## Response to the question on N Terminal and C Terminal\n\nWhen a peptide bond is formed (green), one amine group and one carboxy group are left untouched. Therefore, in a linear polypeptide, there would always be a free amine group and a free carboxy group, and they are called the N terminal(blue) and the C terminal(red) respectively in biochemistry. You can find the visualization in **Appendix H**.\n\n## Response to fixing the molecule encoder in contrastive learning\n\nFrom an **effectiveness** standpoint, as you mentioned before, there are unavoidable discrepancies between pseudo and true ligands. If we didn't fix our molecule encoder during pretraining, then the conditions of Theorem 3.1 might not be met. The ablation results presented in the previous response clearly indicate a performance decline when the molecular encoder is not fixed during contrastive learning, particularly in the context of pocket matching. This supports our assertion that the pocket encoder struggles to acquire chemical knowledge without the stability provided by a fixed molecular encoder.\n\nFrom an **efficiency** standpoint, a fixed molecule encoder also offers significant advantages. It reduces memory usage and accelerates training speed because all molecule embeddings can be pre-computed before training. This eliminates the need for feed-forward and back-propagation operations of the molecule encoder during training, leading to efficiency gains. Additionally, adapting to different molecule encoders becomes more straightforward when they are not co-trained during the pretraining process. This approach allows for more flexibility and ease in integrating various encoding methodologies **even if the molecule encoder is not accessible for training**. That is to say, our approach can even be used with undifferentiable molecular representations.\n\n## Response to extending to other tasks like protein-protein interaction prediction\n\nThank you for pointing out the potential application of our framework to protein-protein interaction (PPI) tasks. We have to point out that the interactions between proteins and proteins are far different from proteins and molecules. Specifically, the interaction areas are usually much larger, but shallower, and involved functional groups are less diverse due to limited types of natural amino acids. Therefore, several changes should be made to extend our ProFSA method to PPI tasks:\n\n1. As for data creation, unlike the localized and specific nature of pocket-ligand interactions, protein-protein interactions are **more global and involve broader protein features**. This requires a shift in the data creation process to split entire protein structures into domains instead of short fragments and to capture potential interactions between them. \n\n2. The interaction interface between protein and protein is much larger. We might need to change from atomic-level to residue-level representations to efficiently capture interaction information.\n\n3. For contrastive learning training, ProFSA uses a fixed pretrained molecular encoder to cope with various small molecules and their atomic representation. However, for protein-protein interaction, it is not necessary as we could only model 20 types of amino acids at the residue level.\n\nDespite several changes that need to be done for PPI tasks, we find that we could divide protein complexes into local interactions. When the complex structures and monomer structures are provided, we have achieved a reasonable result on a flexible PPI affinity prediction benchmark with our current model in a zero-shot way. \n\n| Method            | Spearman $\\uparrow$  |\n|-------------------|----------------------|\n| SchNet            | 0.072 ± 0.021        |\n| DimeNet++         | 0.171 ± 0.054        |\n| EGNN              | 0.080 ± 0.038        |\n| TorchMD           | 0.117 ± 0.008        |\n| GET               | **0.363 ± 0.017**    |\n| ProFSA(zero-shot) | _0.248_              |\n\nSurprisingly, as a **zero-shot** method, ProFSA is able to outperform other **supervised learning** models except for GET, a newly proposed unified model. This demonstrates our approach is also able to capture protein-protein interaction information despite it being designed for protein-ligand interaction. \n\nA detailed explanation of this experiment is shown in **Appendix C6**.\n\n---\n\n# Response 1/2\n\n## Response to evidence for the construction of the pseudo-ligand and using peptides to replace small molecules \n\nAs we discussed in the paper, pseudo-ligands share similar sizes as real ligands, and they also make similar non-covalent interactions with the pocket. To further support our proposal, we would like to cite another paper **\"A defined structural unit enables de novo design of small-molecule–binding proteins\"**[1], published in Science 2020, which supports that **intra-protein interactions are similar to protein-ligand interactions.** That is the reason we use peptides to **represent** small molecules to let the pocket encoder learn the interaction information. \n\nAlso, we'd like to clarify that our intention is not to **\"replace\"** small molecules with peptides. It's a pretraining framework that leverages abundant protein-only data and uses peptides to **simulate** pocket-small molecule interactions for enhanced pocket representations. Following this pretraining phase, **the model can be finetuned with datasets that include real small molecules, ensuring its effectiveness in practical applications**.\n\nTo demonstrate the validity of constructing pseudo-ligands, we provide visualizations in **Appendix F** showing that interactions between pockets and our pseudo-ligands are similar to those with true ligands. These figures reveal shared interaction types like **hydrogen bonding**, **$\\pi-\\pi$ stacking**, and **salt bridge**, depicted with color-coded dashed lines for each interaction type. This should help clarify the rationale and evidence supporting our approach.\n\n[1] Polizzi, Nicholas F., and William F. DeGrado. \"A defined structural unit enables de novo design of small-molecule–binding proteins.\" Science 369, no. 6508 (2020)\n\n## Response to ablation studies evaluating the impact of critical design choices\n\nThanks for your advice on adding more ablation studies on design choices. We did ablation studies on the effectiveness of distributional alignment, where the major difference is that fragment sizes are modified. We found that aligned fragment sizes with real ligands could provide the best performance. The result is shown in **Table 5**.\n\nAs for the distance thresholds, we define the pocket following the UniMol setup. We also found our model can also adapt to an 8Å setup in the ToughM1 experiment. Following your advice, we did an ablation study on the distance thresholds. We tested our method with three different thresholds: 4Å, 6Å, and 8Å. The result is shown below:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| 4Å | 0.7062 | 0.7549 | 0.1240 | 0.1095 | 28.29 | 13.07 |\n| 6Å | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| 8Å | 0.8322 | 0.8292 | 0.1256 | 0.1125 | 34.83 | 12.92 |\n\nSince the 8Å threshold corresponds with the pocket definition in the Kahraman and Tough M1 datasets, it leads to optimal results. Our decision to use a 6Å threshold was made to align with the methodology of pretraining data creation by Uni-Mol, facilitating a fair comparison. Notably, even with the 6Å threshold, we achieved strong results in the pocket-matching task, which serves as a testament to the effectiveness of our approach.\n\n## Response to substantial discrepancies between pseudo and real ligands\n\nThank you for pointing out the discrepancies between pseudo and real ligands. We understand and acknowledge your concern. To mitigate the impact of this discrepancy, we have made several efforts. First, we performed **distribution alignment** to make the data distribution of pseudo-ligands more similar to that of true ligands in PDBbind. Additionally, we **fixed the molecule encoder during pretraining** to prevent it from being misled by the discrepancy. Our ablation studies have shown that these strategies are effective:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nThe table above shows that lacking distribution alignment and not fixing the molecular encoder during pretraining both lead to lower performance, underscoring the effectiveness of our methods in reducing discrepancies.\n\nTheorem 3.1 theoretically supports our approach's efficacy, even with discrepancies between pseudo and real ligands. Empirically, our method outperforms other pretraining techniques, confirming its effectiveness despite these discrepancies.\n\n---\n\n## Response to limited tasks and potential in protein-ligand binding pose prediction\n\nIn our paper, the primary objective is to assess the effectiveness of our pretraining method and the quality of the trained pocket representations. To achieve this, we focused on two types of downstream tasks: pocket-only tasks (pocket property prediction and pocket matching) and pocket-ligand interaction tasks (ligand binding affinity prediction).\n\nWe appreciate the suggestion to apply our method to the protein-ligand binding pose prediction task. However, it's important to note that our current evaluation framework is designed to specifically assess **pocket representations**. For the tasks we chose, the architecture is straightforward: either a **zero-shot** evaluation or a **simple MLP** for mapping embeddings to predictions. In contrast, protein-ligand binding pose prediction often involves **complex methodologies**. For instance, state-of-the-art methods like DiffDock[1] require training a diffusion generative model and a separate confidence model, while other approaches like EDM-Dock[2], rely on reconstructing ligand conformations from predicted distance maps. These methods are not end-to-end and do not directly align with our objective of evaluating pretrained pocket representations.\n\nNevertheless, we recognize the potential of our method in enhancing existing binding pose prediction techniques. To integrate our approach, we would need to modify our framework. Using the same data creation strategy, we could train a binding pose prediction model with our preprocessed data, which could then be further fine-tuned using real pocket-ligand pair data from sources like PDBbind. Additionally, our data could be used to train a side-chain packing model, allowing for side-chain flexibility during docking. Thank you again for your advice, and we will leave protein-ligand binding pose prediction as a separate future work for our method.\n\nIn response to the need for evaluating our method on a broader range of tasks, we have extended our analysis to include two additional downstream tasks: LEP (Ligand Efficacy Prediction), and PPA (Protein-Protein Affinity Prediction).\n\nresult for LEP:\n\n| Method      | AUROC $\\uparrow$ | AUPRC $\\uparrow$ |\n|-------------|--------------|--------------|\n| ATOM3D-GNN  | 0.681        | 0.598        |\n| GeoSSL      | 0.776±0.03   | 0.694±0.06   |\n| Uni-Mol     | 0.782±0.02   | 0.695±0.07   |\n| ProFSA      | 0.840±0.04   | 0.806±0.04   |\n\nGeoSSL and Uni-Mol, both pretraining methods, yield comparable results. However, ProFSA outperforms these methods, demonstrating the advantage of our pocket pretraining approach.\n\nresult for PPA:\n\n| Method            | Spearman $\\uparrow$  |\n|-------------------|----------------------|\n| SchNet            | 0.072 ± 0.021        |\n| DimeNet++         | 0.171 ± 0.054        |\n| EGNN              | 0.080 ± 0.038        |\n| TorchMD           | 0.117 ± 0.008        |\n| GET               | **0.363 ± 0.017**    |\n| ProFSA(zero shot) | _0.248_              |\n\nAs a **zero-shot** method, ProFSA is able to outperform other **supervised learning** models except for GET, a newly proposed unified model. This demonstrates our approach is able to capture protein-protein interaction information despite it being designed for protein-ligand interaction modeling.\n\nYou can find detailed experiment settings and results in **Appendix C5 and C6**.\n\n[1] Corso et al., \"Diffdock: Diffusion steps, twists, and turns for molecular docking.\", ICLR 2023.\n[2] Masters et al., \"Deep learning model for efficient protein–ligand docking with implicit side-chain flexibility.\" Journal of Chemical Information and Modeling 63, no. 6 (2023).\n\n## Response to the issue of lack of latest baselines\n\nFor the pocket matching task, CoSP is a newly proposed baseline which is published in ECML PKDD 2023. Alongside CoSP and Uni-Mol, we selected some of the most effective baseline results presented in the CoSP paper. We also tried to evaluate recent methods like PocketAnchor (Li et al., Cell Systems 2022) on our own since they are not tested on the pocket matching task. However, since the result didn't outperform other baseline machine learning methods(Uni-Mol, CoSP, and DeeplyTough), we decided not to include it in our final baseline comparison. We can include it in the camera-ready version if needed.\n\nFor the ligand binding affinity task, a lot of newly proposed baselines are included, i.e. ProNet (Wang et al., 2022b); as well as pretraining methods such as GeoSSL (Liu et al., 2023), EGNN-PLM (Wu et al., 2022), DeepAffinity (Karimi et al., 2019) and Uni-Mol (Zhou et al., 2023).\n\n---\n\n## Response to lack of related works\n\nThanks for providing us with more insightful related works that could support our arguments. We actually have cited these three papers in the section on ligand binding affinity experiments, but we are also happy to add these in Related Works section 2.2. You can find the change in pdfdiff.\n\n## Response to lack of COSP as a baseline in Table 3\n\nBecause best to our knowledge, they did not release their code and they did not test their method on the ligand binding affinity dataset. \n\n## Response to the question on the loss terms\n\nThank you for your advice. We apologize that we accidentally mentioned loss1 and loss2 in reverse, and we are sorry for the confusing statement. You are correct that the first loss is to identify the true protein pocket when given a pseudo-ligand. We revise the original statement to: \"The primary purpose of the first loss is to identify the true protein pocket from a batch of samples when given a pseudo-ligand. Similarly, the second loss seeks to identify the corresponding ligand fragment for a given pocket.\" You can also find the change in pdfdiff.\n\n## Response to the question on the length of the pocket representation without alignment\n\nWith or without distributional alignment, pockets are always defined by the given protein fragment with a fixed distance cutoff (6Å in our works, following the UniMol setup). The distributional alignment process merely samples these pairs to match the sizes of real ligands and pockets. Without distributional alignment, fragments are uniformly sampled from 1 to 7 residues. Pockets are similarly determined as with residues within the range of 6Å around peptide fragments.\n\n---\n\n# Question about the confidence score\n\nThanks for your review. We found your understanding and judgment of our paper comprehensive and precise. Also, you provided detailed and thoughtful advice for our paper, which helped a lot. We sincerely appreciate your review, but we also wonder if there is any misunderstanding here, as you only give a confidence score of 1.\n\n---\n\n# Response 3/3\n\n## Response to biological justification and visualization of the results\n\nOur method's **biological justification** for achieving good experimental results is based on the properties and interactions of pseudo-ligands. As discussed in our paper, these pseudo-ligands are similar in size to real ligands and engage in comparable non-covalent interactions with protein pockets. This fundamental similarity is crucial for the effectiveness of our approach.\n\nMoreover, our methodology is further supported by the findings in the 2020 Science paper titled \"A defined structural unit enables de novo design of small-molecule–binding proteins.\"[1] This research underscores that **intra-protein interactions are analogous to protein-ligand interactions**, which validates our use of peptides as proxies for small molecules. By employing peptides as pseudo-ligands in this manner, our pocket encoder is able to learn and replicate the interaction dynamics typically observed in real ligand scenarios. This understanding is pivotal to the success of our method in downstream applications.\n\nTo better justify our methods and results, following your advice, we also provide some visualizations in **Appendix F**. **Figure 6** illustrates that the pseudo pairs we created share various interaction types commonly found in real pocket-ligand pairs, such as **hydrogen bonding, $\\pi-\\pi$ stacking, and salt bridge**. In the figures, each type of interaction is represented by dashed lines, color-coded to correspond with the specific interaction type.\n\nAnother visualization of our pocket-matching result is shown in **Figure 7**. We showed an example of two non-homology ATP binding pockets to explain why pocket-matching can benefit from interaction-aware pretraining so that we have achieved superior results in the Kahraman dataset and the BioLip t-SNE visualization. The PEP carboxykinase(PDB:1AYL) and tRNA synthetase(PDB:1B8A) are two ATP-binding proteins that share zero sequence similarity (verified with the BLAST) extracted from the Kahraman dataset. However, as they are both fueled by ATP, their binding site shares similar binding patterns. The cation-$\\pi$ interactions(blue dash lines) and salt bridges (magenta dash lines) are important to the ATP binding, which can be viewed as convergent evolutions at the molecule level. Even though, these two pockets are very distinct in terms of shapes and sizes because they bind ATPs in different conformations. Therefore, biochemical interactions are the key to accomplishing the pocket-matching task, which is ignored in previous self-supervised learning methods like Uni-Mol.\n\n[1] Polizzi, Nicholas F., and William F. DeGrado. \"A defined structural unit enables de novo design of small-molecule–binding proteins.\" Science 369, no. 6508 (2020)\n\n---\n\n# Response 2/3\n\n## Response to the bound of Theorem 3.1 \n\nWe are not very clear about the point of your question. We have two understandings and they are replied as follows.\n\nIf you mean $ ||t − t ^{(0)}|| < 1/2 l_T$ is trivial, please note that it is only a condition of the theorem. As you said, many other encoders can also satisfy this condition. This shows that our method is applicable to many different molecular encoders with or without pretraining as we proved in **Table 4**. Moreover, the condition is only a necessary condition. We also need the pretraining contrastive loss to be sufficiently small to guarantee a small contrastive loss between pockets and real ligands. In fact, in the process of our early exploration, we noticed that some encoders cannot achieve a low pretraining loss, indicating the entire conditions of the theorem are nontrivial.\n\nIf you mean that the conclusion of theorem 3.1 is trivial, we want to emphasize that our conclusion is $\\lim_{L_i(t, s) =0}L_i(t^{(0)}, s) =0$. (We write it in a $\\epsilon$-$\\delta$ language in paper. They are equivalent forms.) Our conclusion shows the loss containing real ligands is consistent with the loss containing pseudo ligands that we optimized in pre-training. \n\nIn contrast,  a trivial result from $||t − t ^{(0)}|| < 1/ 2l_T$ is that $\\lim_{L_i(t, s) =0}L_i(t^{(0)}, s) <B$, B is a bound related to l_t and the representations. It does not guarantee a consistent loss and is substantially different from our result.\nTherefore, our theorem is nontrivial and reveals the transfer ability of our contrastive pre-training from the pseudo-ligand domain to the real ligand domain.\n\nAs for this theoretical result, we are open to further discussion.\n\n## Response to molecules possess more properties that can also lead to imbalance\n\nIt is correct that molecules have more properties than their size, like logP, Hbond donor and acceptor number, and rotatory bond number. As small molecule drugs are mostly designed to penetrate barriers like gut or cell membranes, they usually have much larger logP values, which means more hydrophobic. For the same reason, they usually have fewer hydrogen bond donors and acceptors, and only minimal essential ones are kept for specified interactions. Also, to minimize the entropy effect upon binding and to increase binding affinity, rotatory bonds are also unfavored. However, it is impossible to mimic such features with peptides, as the backbone of peptides is intrinsically hydrophilic and flexible. As we showed in a new figure, our pseudo-ligands are less similar to real ligands in those properties even with size alignment. You can find the figure in the new pdf in **Appendix G**.\n\nWe believe that the aforementioned divergence is the main limitation of poor zero-shot performance in predicting hydrophobicity scores (**Table 1**). However, as demonstrated in previous publications like CoSP, we could leverage real ligand-pocket pairs from PDBBind or BioLip database to further finetune our network, as an extension of the pipeline. In this way, we could handle property mismatches but still enjoy the power of our large-scale pretraining.\n\n## Response to the approach to defining pockets \n\nWe define the pocket following the UniMol setup. We found our model can also adapt to an 8Å setup in the toughM1 experiment \n\nIn particular, we define the pocket for each protein-ligand pair as residues of the protein that have at least one atom within the range of 6Å from a heavy atom in the ligand. To further explain our design, we have done an ablation study on different choices of thresholds of 4Å, 6Å, and 8Å:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| 4Å | 0.7062 | 0.7549 | 0.1240 | 0.1095 | 28.29 | 13.07 |\n| 6Å | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| 8Å | 0.8322 | 0.8292 | 0.1256 | 0.1125 | 34.83 | 12.92 |\n\nSince the 8Å threshold corresponds with the pocket definition in the Kahraman and Tough M1 datasets, it leads to optimal results. Our decision to use a 6Å threshold was made to align with the methodology of pretraining data creation by Uni-Mol, facilitating a fair comparison. Notably, even with the 6Å threshold, we achieved strong results in the pocket-matching task, which serves as a testament to the effectiveness of our approach.\n\n---\n\n# Response 1/3\nWe appreciate the time and effort you have dedicated to reviewing our paper, and we are grateful for your constructive feedback and thoughtful evaluation of our work. We would like to address your comments about the technical novelty of our work and provide additional clarification on certain aspects of our paper.\n\n## Response to technical novelty\n\nFirstly, we acknowledge your observation that we didn't use any fancy models compared with typical **model-centric** works. While we respect your assessment, we would like to highlight that our primary focus in this paper is a **data-centric** pretraining method to introduce groundbreaking improvements in the field of protein pocket pretraining. Our work aims to address a critical challenge of data scarcity in the field of protein pocket representation by constructing large-scale synthetic data that facilitates the pretraining of models, ultimately enhancing the accuracy and robustness of protein pocket representations. We believe that the research community can conduct fast following-ups on our **released dataset** with more sophisticated models, and even extend our pipelines to other tasks like docking or drug design. We also would like to point out that we have provided a novel method to **distill knowledge from well-trained molecule models** to protein models. Though employing the same contrastive loss, our approach is different from existing contrastive learning models since it uses a fixed molecular encoder. The motivation here is to use a well-trained molecular encoder on a relatively larger dataset to **guide** the training of the protein encoder. For example, some quantum-chemistry properties that are difficult to compute for large systems like proteins could be distilled from molecules, by using some quantum-chemistry-aware molecular encoders such as Frad. Though our framework seems to be simple, it is non-trivial to make it work. We have made several efforts to solve the unavoidable discrepancies between true ligands and pseudo ligands. The ablation study shows that our efforts are effective and necessary:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nRegarding your positive acknowledgment of our \"simple but effective model for the right question,\" we are pleased to hear that our approach resonates with the objective we set out to achieve. We designed our model with **simplicity** in mind, prioritizing effectiveness and practical utility for the specific problem domain. This deliberate choice aligns with the notion that sometimes the most impactful solutions are elegantly straightforward. Also, we intentionally borrowed a pocket encoder from Uni-Mol to make a **fair comparison** with it, which strongly supports the power of our dataset. Similar to the molecule encoder, our pocket encoder can be changed to any other model due to our flexible framework. Notably, **we didn't load the pretrained weights of Uni-Mol pocket encoder**. We only use the same backbone architecture and the pretraining was completely done on our processed data with our training strategy. \n\nNotably, many simple but effective approaches have been recognized by top-tier conferences and journals. A prime example of this is in the field of protein language models, like the ESM series and ProtTrans. These studies adapted the Transformer architecture and masked language modeling techniques from natural language processing to protein sequences. While they didn't introduce groundbreaking techniques, their substantial contributions to protein modeling are evident, with publications in prestigious conferences and journals like NeurIPS, ICML, TPAMI, PNAS, and Science. Another example is the widely acclaimed CLIP paper presented at ICML 2021. CLIP, while not employing novel techniques, stands as a hallmark of data-centric deep learning. Its use of contrastive learning enabled training on expansive web-sourced text-image datasets, moving beyond the constraints of meticulously curated databases like MS-COCO. This aligns with our approach to addressing **data scarcity challenges** by facilitating training on large-scale datasets, mirroring our strategy for overcoming similar hurdles in the protein pocket modeling domain.\n\nWe appreciate your efforts to remain neutral and understand the challenges in decision-making. We believe further discussion will highlight our paper's contributions. Your feedback and insights in the upcoming discussion will be invaluable for refining our work.", "author_response": "# Response 1/3\nWe appreciate the time and effort you have dedicated to reviewing our paper, and we are grateful for your constructive feedback and thoughtful evaluation of our work. We would like to address your comments about the technical novelty of our work and provide additional clarification on certain aspects of our paper.\n\n## Response to technical novelty\n\nFirstly, we acknowledge your observation that we didn't use any fancy models compared with typical **model-centric** works. While we respect your assessment, we would like to highlight that our primary focus in this paper is a **data-centric** pretraining method to introduce groundbreaking improvements in the field of protein pocket pretraining. Our work aims to address a critical challenge of data scarcity in the field of protein pocket representation by constructing large-scale synthetic data that facilitates the pretraining of models, ultimately enhancing the accuracy and robustness of protein pocket representations. We believe that the research community can conduct fast following-ups on our **released dataset** with more sophisticated models, and even extend our pipelines to other tasks like docking or drug design. We also would like to point out that we have provided a novel method to **distill knowledge from well-trained molecule models** to protein models. Though employing the same contrastive loss, our approach is different from existing contrastive learning models since it uses a fixed molecular encoder. The motivation here is to use a well-trained molecular encoder on a relatively larger dataset to **guide** the training of the protein encoder. For example, some quantum-chemistry properties that are difficult to compute for large systems like proteins could be distilled from molecules, by using some quantum-chemistry-aware molecular encoders such as Frad. Though our framework seems to be simple, it is non-trivial to make it work. We have made several efforts to solve the unavoidable discrepancies between true ligands and pseudo ligands. The ablation study shows that our efforts are effective and necessary:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nRegarding your positive acknowledgment of our \"simple but effective model for the right question,\" we are pleased to hear that our approach resonates with the objective we set out to achieve. We designed our model with **simplicity** in mind, prioritizing effectiveness and practical utility for the specific problem domain. This deliberate choice aligns with the notion that sometimes the most impactful solutions are elegantly straightforward. Also, we intentionally borrowed a pocket encoder from Uni-Mol to make a **fair comparison** with it, which strongly supports the power of our dataset. Similar to the molecule encoder, our pocket encoder can be changed to any other model due to our flexible framework. Notably, **we didn't load the pretrained weights of Uni-Mol pocket encoder**. We only use the same backbone architecture and the pretraining was completely done on our processed data with our training strategy. \n\nNotably, many simple but effective approaches have been recognized by top-tier conferences and journals. A prime example of this is in the field of protein language models, like the ESM series and ProtTrans. These studies adapted the Transformer architecture and masked language modeling techniques from natural language processing to protein sequences. While they didn't introduce groundbreaking techniques, their substantial contributions to protein modeling are evident, with publications in prestigious conferences and journals like NeurIPS, ICML, TPAMI, PNAS, and Science. Another example is the widely acclaimed CLIP paper presented at ICML 2021. CLIP, while not employing novel techniques, stands as a hallmark of data-centric deep learning. Its use of contrastive learning enabled training on expansive web-sourced text-image datasets, moving beyond the constraints of meticulously curated databases like MS-COCO. This aligns with our approach to addressing **data scarcity challenges** by facilitating training on large-scale datasets, mirroring our strategy for overcoming similar hurdles in the protein pocket modeling domain.\n\nWe appreciate your efforts to remain neutral and understand the challenges in decision-making. We believe further discussion will highlight our paper's contributions. Your feedback and insights in the upcoming discussion will be invaluable for refining our work."}
{"claim": "It is unclear whether models not pretrained on molecule datasets can be effectively used for the proposed pocket pretraining task.", "claim_type": "experimental", "paper_id": "uMAujpVi9m", "paper_title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "9ap4s1SZlZ", "reviewer": "Reviewer_yfM3", "review_text": "Summary: This paper primarily aims to enhance the pocket pretraining method, as existing approaches only consider pockets during pretraining. There are two main contributions in this paper: (1) The authors introduce a novel method, ProFSA, for pocket pretraining, which extracts additional information from corresponding ligands. However, the number of pocket-ligand complex structures is quite limited in existing datasets. (2) To address this issue, the authors generate over 5 million complexes by segmenting fragments and their corresponding pockets in protein structures. By aligning features of fragments and pockets, the pocket encoder learns the interaction between fragments and pockets. The authors design downstream tasks such as pocket druggability prediction, pocket matching, and ligand binding affinity prediction to demonstrate the effectiveness of ProFSA.\n\nStrengths: The authors propose a new perspective of pretraining pockets and construct a large-scale dataset, which data distribution is also considered, to make the efficient pre-training possible.\n\nThe results are competitive, especially for zero-shot settings.\n\nAbundant experiments and ablation study support the argument and result of the authors.\n\nWeaknesses: 1. The technical novelty is limited.\n  - The pocket encoder is borrowed from Uni-Mol.\n  - The contrastive loss is the vanilla form of classical contrastive learning.\n\n2. The bound of Theorem 3.1 is trivial. The authors claim that the bound naturally exists for these representations extracted by pretrained molecule models. However, it's a bit counterintuitive, because many models not pretrained on molecule datasets also fulfill this prior. So, can these models be used for this task? **I strongly suggest removing this part from the paper**.\n\n3. Some issues about dataset creation:\n - 3.1. The authors consider the distribution of ligand size and pocket size when designing the dataset. However, molecules possess more properties that can also lead to imbalance. It would be better to, at least, add some discussion about this issue.\n - 3.2. In the second stage of the data construction process, the approach to defining pockets needs further explanation or an ablation study.\n\n4. Experiments: It would be better to add some biological justification or visualization of the results.\n\nFor this paper, one fact is that the technical novelty is below the bar of ICLR. However, I admire the simple but effective model for the right question. It's a struggle for me to make a decision. I will maintain a neutral attitude and make my final decision after the discussion.\n\nQuestions: See weakness.", "labeling_timestamp": "2026-01-11T16:25:32.227371", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "Authors explain that a pretrained, fixed molecular encoder is important (and show ablations where not fixing it harms performance), but they also claim the method can work with encoders \"with or without pretraining\" (Table 4). Thus they partially resolve the concern: they emphasize pretrained/fixed encoders are preferable but assert applicability to non-pretrained encoders without fully demonstrating equal effectiveness.", "evidence": "\"This shows that our method is applicable to many different molecular encoders with or without pretraining as we proved in Table 4.\"; \"The ablation results presented in the previous response clearly indicate a performance decline when the molecular encoder is not fixed during contrastive learning, particularly in the context of pocket matching.\"", "combined_author_response": "Dear AC and reviewers,\n\nWe sincerely appreciate your time and efforts in reviewing our work. Based on your suggestions, we have revised our paper. We would like to use this section to reiterate the explanation for some common concerns and summarize the contributions of our paper.\n\nIn our revised paper, we provide a detailed explanation of the motivation and theory behind our method: **the mechanisms of pocket-peptide interactions and pocket-ligand interactions are very similar**. Our proposed **data-centric** pretraining pipeline is unique in its use of peptides to mimic small molecules, a method that is, to our knowledge, **the first of its kind**. We expanded our discussion about its biological insights in **Section 3**. We add a **new Figure 2 to illustrate different types of non-covalent interactions** that are common for both ligand-protein data and intra-protein data. Following this theory, we have designed a pipeline to generate **over 5 million** synthetic ligand-protein pairs, which greatly eases the **data scarcity** problem in the field of protein-ligand interaction learning. We also prove our data quite effective in various downstream tasks, including pocket property prediction, pocket matching, ligand binding affinity prediction, ligand efficiency prediction, and protein-protein interaction tasks. The **last two tasks are newly added** to further support our method (**Appendix C5 and C6**). We are confident that our **ready-to-release synthetic dataset** will significantly benefit the AI for drug discovery community. Its impact extends beyond enhancing pocket representations in downstream analyses, as it also equips researchers with a valuable resource for a range of applications, such as protein-ligand docking, structure-based drug design, and virtual screening. Importantly, these areas often face challenges due to **a lack of training data necessary for learning effective binding patterns between protein pockets and ligands**. \n\nIn our revised paper, we recognize the differences between real ligands and pseudo ligands (peptides). We address these discrepancies and present a distribution plot of various properties in **Appendix G**. Our approach primarily leverages the **analogous nature of ligand-pocket and peptide-pocket interactions**. Therefore, despite these differences, our model can **still extract valuable interaction information from the pretraining dataset**, which enhances its performance in downstream tasks. This is illustrated in **Figure 4(a)**, showing the benefits of interaction-aware pretraining. We have also undertaken measures like distribution alignment and freezing the molecular encoder to mitigate the impact of these discrepancies. The positive results of these efforts, as evidenced in **Table 5**, confirm their effectiveness and necessity. Notably, our pretrained model can be **further finetuned with real labeled ligand-pocket pairwise data**, as we show in the Ligand Binding Affinity task in **section 4.3**.\n\nAs for other questions, we have added more ablation studies on our design choices, including the impact of a fixed molecular encoder (Table 5), and different threshold values to define pockets(Appendix C.4). \n\nWe are immensely grateful for the invaluable feedback from all the reviewers, which has guided us in refining and clarifying our work. We hope that the revised version of our paper, coupled with the discussion period, will more clearly highlight the novelty, effectiveness, and contributions of our approach to the field.\n\n---\n\nThank you for your invaluable feedback on our paper. In response to your insightful suggestions, we have made several modifications in our revised paper. \n\nWe have cited additional papers in Section 2.2 following your advice. The description of loss terms in Section 3.2 has been corrected.  COSP was not included as a baseline for the Ligand Binding Affinity (LBA)  task because it is not open-sourced, and its performance on LBA was not evaluated in their published work. We used it as a baseline in the pocket-matching task.\n\nIn Section 3, we've expanded our discussion to answer your question on **ProFSA's effectiveness without distributional alignment**. This section now offers a detailed explanation of the foundational concepts and justifications for our method. We draw attention to the parallels between ligand-protein and intra-protein non-covalent interactions, as shown in **Figure 2**. This comparison supports our strategy of using peptides as stand-ins for actual ligands to mimic pocket-ligand interactions. Consequently, **even in the absence of distributional alignment, the types of interactions remain comparably relevant**, which explains why ProFSA continues to function, albeit with reduced effectiveness, as demonstrated in Table 5. We also want to clarify that distributional alignment is only applied to fragment-pocket complex as a whole, and **it would not change the definition of the pocket with a given ligand**. Without this alignment, these fragments are chosen uniformly, with lengths varying from 1 to 7 residues, and pockets are consistently identified based on residues within a 6Å radius of these peptide fragments.\n\nWe are grateful for the time and effort you have dedicated to reviewing our work. Your thorough and constructive feedback has significantly contributed to the refinement of our research. Please let us know if there are any other aspects of our paper that you would like us to address or clarify. We look forward to your further feedback.\n\n---\n\nThank you for your insightful feedback. In response to your concerns regarding the scope of our tasks, we have expanded our research in the revised paper. We now include additional tasks such as **ligand efficiency prediction** and **protein-protein affinity prediction**, detailed in **Appendix C5 and C6**. These tasks were incorporated to enhance the robustness of our method. We acknowledge that they are currently in the appendix due to space constraints, but we are open to relocating them to the main text if it would be beneficial.\n\nRegarding your concern about the lack of recent baselines, we provided a detailed explanation in our previous response. We want to reiterate that our benchmarks do include the latest baselines, with some as recent as 2023.\n\nWe greatly appreciate the time and effort you have invested in reviewing our work. Your feedback has been invaluable in refining our research. Please let us know if there are any other aspects of our paper that you would like us to address or clarify. We look forward to your further feedback.\n\n---\n\nIn our revised paper, we have incorporated changes following feedback from another reviewer, which we believe also addresses your concerns and queries.\n\nRegarding your question on the evidence supporting our approach, particularly the use of peptides to simulate small molecules, we've enriched **Section 3.1**. A new paragraph there now provides an in-depth explanation of non-covalent interactions. To aid understanding, **Figure 2** has been updated with visualizations that include both real and pseudo receptor-ligand pairs, illustrating three types of interactions. Furthermore, we delve into the details of **how intra-protein interactions mirror protein-ligand interactions**, involving specific types of amino acids.\n\nTo better evaluate our design choices, the ablation studies have been extended. Alongside the existing studies in **Table 4** (different pretrained molecular encoders), **Table 5** (distribution alignment), and **Figure 5** (data scale). we've added a study on the impact of using a fixed molecular encoder in **Table 5** in the main text to address your question on the fixed molecular encoder. We've also included an ablation study on varying **cutoff values for pocket definition**, placed in the **Appendix C.4** due to its more focused scope. We can move it to the main part of the paper in the future if needed.\n\nWe acknowledge your concern about the **differences between true and pseudo ligands**, so we've expanded our discussion on the distinct properties of our curated dataset versus the PDBBind dataset in **Figure 9** and **Section 3.2**. The ablation study in **Table 5** demonstrates our methods and their effectiveness in minimizing these discrepancies. Notably, the performance significantly drops without our distribution alignment or fixed molecular encoder, highlighting our efforts to mitigate these issues. The updated **Figure 2** further supports this, showing that while the properties of true and pseudo ligands may differ, **the interaction types are consistent.** We hope these results can ease your concerns about our synthetic dataset, and they also respond to your query about **why we kept the molecular encoder constant during pocket pretraining.**\n\nWe appreciate the constructive critiques and thoughtful engagement from you. Your feedback has undeniably contributed to the refinement and strengthening of our paper. We sincerely value the time and expertise invested in the review process, and we look forward to any further suggestions you may have.\n\n---\n\nWe sincerely appreciate the thoughtful feedback provided on our paper and have diligently incorporated your suggestions into this revised version. In response to your guidance, we have made three significant modifications to the core content of our paper, all of which aim to enhance clarity and address the concerns raised.\n\nTaking into account your suggestion, we have removed **Theorem 3.1** from the main body of our paper. Additionally, we've improved the explanation and included an ablation study on our method of defining pockets. This ablation study, focusing on the cutoff values and primarily a hyperparameter issue, is placed in the Appendix due to its more limited scope of insight. We can move it to the main body of the paper in the future if needed. We have also expanded our discussion on the differences between our dataset and the PDBBind dataset, elucidating how fixed molecule encoders contribute to mitigating this discrepancy.\n\nIn **Section 3.1**, we have introduced a new paragraph dedicated to providing readers with a deeper understanding of non-covalent interactions. To facilitate comprehension, we have included visualizations featuring three types of interactions, incorporating both real and pseudo receptor-ligand pairs in the new **Figure 2**. Furthermore, we delve into the details of how intra-protein interactions mirror protein-ligand interactions, involving specific types of amino acids.\n\nThe biological justifications for the efficacy of interaction-aware pretraining are now more extensively explored in our revised paper. Specifically, we present a compelling showcase featuring estradiol-binding proteins, offering insights into the geometric disparities between the compared proteins and highlighting the similarities in their binding interfaces in the new **Figure 4a**. The existing visualization of BioLip pocket representations in the new **Figure 4b** can make our claims more convincing. Together, these visual aids illustrate how our interaction-aware pretraining empowers models to focus on crucial interface residues, discern subtle distinctions, and disregard irrelevant geometric dissimilarities.\n\nWe have also included an in-depth discussion of pretrained molecule encoders in the newly added **Section 2.3**, further enriching the theoretical foundation of our work.\nAll changes can be found in the renewed paper, which is updated lively.\n\nWe genuinely value the constructive input you provided during the review process, as it has undoubtedly contributed to the refinement of our paper. Your insights have proven invaluable in elevating the clarity and depth of our contributions to the field. We express our gratitude for the time and expertise invested in reviewing our work, and we look forward to any additional feedback or suggestions you may have.\n\n---\n\n# Response 2/2\n\n## Response to the question on N Terminal and C Terminal\n\nWhen a peptide bond is formed (green), one amine group and one carboxy group are left untouched. Therefore, in a linear polypeptide, there would always be a free amine group and a free carboxy group, and they are called the N terminal(blue) and the C terminal(red) respectively in biochemistry. You can find the visualization in **Appendix H**.\n\n## Response to fixing the molecule encoder in contrastive learning\n\nFrom an **effectiveness** standpoint, as you mentioned before, there are unavoidable discrepancies between pseudo and true ligands. If we didn't fix our molecule encoder during pretraining, then the conditions of Theorem 3.1 might not be met. The ablation results presented in the previous response clearly indicate a performance decline when the molecular encoder is not fixed during contrastive learning, particularly in the context of pocket matching. This supports our assertion that the pocket encoder struggles to acquire chemical knowledge without the stability provided by a fixed molecular encoder.\n\nFrom an **efficiency** standpoint, a fixed molecule encoder also offers significant advantages. It reduces memory usage and accelerates training speed because all molecule embeddings can be pre-computed before training. This eliminates the need for feed-forward and back-propagation operations of the molecule encoder during training, leading to efficiency gains. Additionally, adapting to different molecule encoders becomes more straightforward when they are not co-trained during the pretraining process. This approach allows for more flexibility and ease in integrating various encoding methodologies **even if the molecule encoder is not accessible for training**. That is to say, our approach can even be used with undifferentiable molecular representations.\n\n## Response to extending to other tasks like protein-protein interaction prediction\n\nThank you for pointing out the potential application of our framework to protein-protein interaction (PPI) tasks. We have to point out that the interactions between proteins and proteins are far different from proteins and molecules. Specifically, the interaction areas are usually much larger, but shallower, and involved functional groups are less diverse due to limited types of natural amino acids. Therefore, several changes should be made to extend our ProFSA method to PPI tasks:\n\n1. As for data creation, unlike the localized and specific nature of pocket-ligand interactions, protein-protein interactions are **more global and involve broader protein features**. This requires a shift in the data creation process to split entire protein structures into domains instead of short fragments and to capture potential interactions between them. \n\n2. The interaction interface between protein and protein is much larger. We might need to change from atomic-level to residue-level representations to efficiently capture interaction information.\n\n3. For contrastive learning training, ProFSA uses a fixed pretrained molecular encoder to cope with various small molecules and their atomic representation. However, for protein-protein interaction, it is not necessary as we could only model 20 types of amino acids at the residue level.\n\nDespite several changes that need to be done for PPI tasks, we find that we could divide protein complexes into local interactions. When the complex structures and monomer structures are provided, we have achieved a reasonable result on a flexible PPI affinity prediction benchmark with our current model in a zero-shot way. \n\n| Method            | Spearman $\\uparrow$  |\n|-------------------|----------------------|\n| SchNet            | 0.072 ± 0.021        |\n| DimeNet++         | 0.171 ± 0.054        |\n| EGNN              | 0.080 ± 0.038        |\n| TorchMD           | 0.117 ± 0.008        |\n| GET               | **0.363 ± 0.017**    |\n| ProFSA(zero-shot) | _0.248_              |\n\nSurprisingly, as a **zero-shot** method, ProFSA is able to outperform other **supervised learning** models except for GET, a newly proposed unified model. This demonstrates our approach is also able to capture protein-protein interaction information despite it being designed for protein-ligand interaction. \n\nA detailed explanation of this experiment is shown in **Appendix C6**.\n\n---\n\n# Response 1/2\n\n## Response to evidence for the construction of the pseudo-ligand and using peptides to replace small molecules \n\nAs we discussed in the paper, pseudo-ligands share similar sizes as real ligands, and they also make similar non-covalent interactions with the pocket. To further support our proposal, we would like to cite another paper **\"A defined structural unit enables de novo design of small-molecule–binding proteins\"**[1], published in Science 2020, which supports that **intra-protein interactions are similar to protein-ligand interactions.** That is the reason we use peptides to **represent** small molecules to let the pocket encoder learn the interaction information. \n\nAlso, we'd like to clarify that our intention is not to **\"replace\"** small molecules with peptides. It's a pretraining framework that leverages abundant protein-only data and uses peptides to **simulate** pocket-small molecule interactions for enhanced pocket representations. Following this pretraining phase, **the model can be finetuned with datasets that include real small molecules, ensuring its effectiveness in practical applications**.\n\nTo demonstrate the validity of constructing pseudo-ligands, we provide visualizations in **Appendix F** showing that interactions between pockets and our pseudo-ligands are similar to those with true ligands. These figures reveal shared interaction types like **hydrogen bonding**, **$\\pi-\\pi$ stacking**, and **salt bridge**, depicted with color-coded dashed lines for each interaction type. This should help clarify the rationale and evidence supporting our approach.\n\n[1] Polizzi, Nicholas F., and William F. DeGrado. \"A defined structural unit enables de novo design of small-molecule–binding proteins.\" Science 369, no. 6508 (2020)\n\n## Response to ablation studies evaluating the impact of critical design choices\n\nThanks for your advice on adding more ablation studies on design choices. We did ablation studies on the effectiveness of distributional alignment, where the major difference is that fragment sizes are modified. We found that aligned fragment sizes with real ligands could provide the best performance. The result is shown in **Table 5**.\n\nAs for the distance thresholds, we define the pocket following the UniMol setup. We also found our model can also adapt to an 8Å setup in the ToughM1 experiment. Following your advice, we did an ablation study on the distance thresholds. We tested our method with three different thresholds: 4Å, 6Å, and 8Å. The result is shown below:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| 4Å | 0.7062 | 0.7549 | 0.1240 | 0.1095 | 28.29 | 13.07 |\n| 6Å | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| 8Å | 0.8322 | 0.8292 | 0.1256 | 0.1125 | 34.83 | 12.92 |\n\nSince the 8Å threshold corresponds with the pocket definition in the Kahraman and Tough M1 datasets, it leads to optimal results. Our decision to use a 6Å threshold was made to align with the methodology of pretraining data creation by Uni-Mol, facilitating a fair comparison. Notably, even with the 6Å threshold, we achieved strong results in the pocket-matching task, which serves as a testament to the effectiveness of our approach.\n\n## Response to substantial discrepancies between pseudo and real ligands\n\nThank you for pointing out the discrepancies between pseudo and real ligands. We understand and acknowledge your concern. To mitigate the impact of this discrepancy, we have made several efforts. First, we performed **distribution alignment** to make the data distribution of pseudo-ligands more similar to that of true ligands in PDBbind. Additionally, we **fixed the molecule encoder during pretraining** to prevent it from being misled by the discrepancy. Our ablation studies have shown that these strategies are effective:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nThe table above shows that lacking distribution alignment and not fixing the molecular encoder during pretraining both lead to lower performance, underscoring the effectiveness of our methods in reducing discrepancies.\n\nTheorem 3.1 theoretically supports our approach's efficacy, even with discrepancies between pseudo and real ligands. Empirically, our method outperforms other pretraining techniques, confirming its effectiveness despite these discrepancies.\n\n---\n\n## Response to limited tasks and potential in protein-ligand binding pose prediction\n\nIn our paper, the primary objective is to assess the effectiveness of our pretraining method and the quality of the trained pocket representations. To achieve this, we focused on two types of downstream tasks: pocket-only tasks (pocket property prediction and pocket matching) and pocket-ligand interaction tasks (ligand binding affinity prediction).\n\nWe appreciate the suggestion to apply our method to the protein-ligand binding pose prediction task. However, it's important to note that our current evaluation framework is designed to specifically assess **pocket representations**. For the tasks we chose, the architecture is straightforward: either a **zero-shot** evaluation or a **simple MLP** for mapping embeddings to predictions. In contrast, protein-ligand binding pose prediction often involves **complex methodologies**. For instance, state-of-the-art methods like DiffDock[1] require training a diffusion generative model and a separate confidence model, while other approaches like EDM-Dock[2], rely on reconstructing ligand conformations from predicted distance maps. These methods are not end-to-end and do not directly align with our objective of evaluating pretrained pocket representations.\n\nNevertheless, we recognize the potential of our method in enhancing existing binding pose prediction techniques. To integrate our approach, we would need to modify our framework. Using the same data creation strategy, we could train a binding pose prediction model with our preprocessed data, which could then be further fine-tuned using real pocket-ligand pair data from sources like PDBbind. Additionally, our data could be used to train a side-chain packing model, allowing for side-chain flexibility during docking. Thank you again for your advice, and we will leave protein-ligand binding pose prediction as a separate future work for our method.\n\nIn response to the need for evaluating our method on a broader range of tasks, we have extended our analysis to include two additional downstream tasks: LEP (Ligand Efficacy Prediction), and PPA (Protein-Protein Affinity Prediction).\n\nresult for LEP:\n\n| Method      | AUROC $\\uparrow$ | AUPRC $\\uparrow$ |\n|-------------|--------------|--------------|\n| ATOM3D-GNN  | 0.681        | 0.598        |\n| GeoSSL      | 0.776±0.03   | 0.694±0.06   |\n| Uni-Mol     | 0.782±0.02   | 0.695±0.07   |\n| ProFSA      | 0.840±0.04   | 0.806±0.04   |\n\nGeoSSL and Uni-Mol, both pretraining methods, yield comparable results. However, ProFSA outperforms these methods, demonstrating the advantage of our pocket pretraining approach.\n\nresult for PPA:\n\n| Method            | Spearman $\\uparrow$  |\n|-------------------|----------------------|\n| SchNet            | 0.072 ± 0.021        |\n| DimeNet++         | 0.171 ± 0.054        |\n| EGNN              | 0.080 ± 0.038        |\n| TorchMD           | 0.117 ± 0.008        |\n| GET               | **0.363 ± 0.017**    |\n| ProFSA(zero shot) | _0.248_              |\n\nAs a **zero-shot** method, ProFSA is able to outperform other **supervised learning** models except for GET, a newly proposed unified model. This demonstrates our approach is able to capture protein-protein interaction information despite it being designed for protein-ligand interaction modeling.\n\nYou can find detailed experiment settings and results in **Appendix C5 and C6**.\n\n[1] Corso et al., \"Diffdock: Diffusion steps, twists, and turns for molecular docking.\", ICLR 2023.\n[2] Masters et al., \"Deep learning model for efficient protein–ligand docking with implicit side-chain flexibility.\" Journal of Chemical Information and Modeling 63, no. 6 (2023).\n\n## Response to the issue of lack of latest baselines\n\nFor the pocket matching task, CoSP is a newly proposed baseline which is published in ECML PKDD 2023. Alongside CoSP and Uni-Mol, we selected some of the most effective baseline results presented in the CoSP paper. We also tried to evaluate recent methods like PocketAnchor (Li et al., Cell Systems 2022) on our own since they are not tested on the pocket matching task. However, since the result didn't outperform other baseline machine learning methods(Uni-Mol, CoSP, and DeeplyTough), we decided not to include it in our final baseline comparison. We can include it in the camera-ready version if needed.\n\nFor the ligand binding affinity task, a lot of newly proposed baselines are included, i.e. ProNet (Wang et al., 2022b); as well as pretraining methods such as GeoSSL (Liu et al., 2023), EGNN-PLM (Wu et al., 2022), DeepAffinity (Karimi et al., 2019) and Uni-Mol (Zhou et al., 2023).\n\n---\n\n## Response to lack of related works\n\nThanks for providing us with more insightful related works that could support our arguments. We actually have cited these three papers in the section on ligand binding affinity experiments, but we are also happy to add these in Related Works section 2.2. You can find the change in pdfdiff.\n\n## Response to lack of COSP as a baseline in Table 3\n\nBecause best to our knowledge, they did not release their code and they did not test their method on the ligand binding affinity dataset. \n\n## Response to the question on the loss terms\n\nThank you for your advice. We apologize that we accidentally mentioned loss1 and loss2 in reverse, and we are sorry for the confusing statement. You are correct that the first loss is to identify the true protein pocket when given a pseudo-ligand. We revise the original statement to: \"The primary purpose of the first loss is to identify the true protein pocket from a batch of samples when given a pseudo-ligand. Similarly, the second loss seeks to identify the corresponding ligand fragment for a given pocket.\" You can also find the change in pdfdiff.\n\n## Response to the question on the length of the pocket representation without alignment\n\nWith or without distributional alignment, pockets are always defined by the given protein fragment with a fixed distance cutoff (6Å in our works, following the UniMol setup). The distributional alignment process merely samples these pairs to match the sizes of real ligands and pockets. Without distributional alignment, fragments are uniformly sampled from 1 to 7 residues. Pockets are similarly determined as with residues within the range of 6Å around peptide fragments.\n\n---\n\n# Question about the confidence score\n\nThanks for your review. We found your understanding and judgment of our paper comprehensive and precise. Also, you provided detailed and thoughtful advice for our paper, which helped a lot. We sincerely appreciate your review, but we also wonder if there is any misunderstanding here, as you only give a confidence score of 1.\n\n---\n\n# Response 3/3\n\n## Response to biological justification and visualization of the results\n\nOur method's **biological justification** for achieving good experimental results is based on the properties and interactions of pseudo-ligands. As discussed in our paper, these pseudo-ligands are similar in size to real ligands and engage in comparable non-covalent interactions with protein pockets. This fundamental similarity is crucial for the effectiveness of our approach.\n\nMoreover, our methodology is further supported by the findings in the 2020 Science paper titled \"A defined structural unit enables de novo design of small-molecule–binding proteins.\"[1] This research underscores that **intra-protein interactions are analogous to protein-ligand interactions**, which validates our use of peptides as proxies for small molecules. By employing peptides as pseudo-ligands in this manner, our pocket encoder is able to learn and replicate the interaction dynamics typically observed in real ligand scenarios. This understanding is pivotal to the success of our method in downstream applications.\n\nTo better justify our methods and results, following your advice, we also provide some visualizations in **Appendix F**. **Figure 6** illustrates that the pseudo pairs we created share various interaction types commonly found in real pocket-ligand pairs, such as **hydrogen bonding, $\\pi-\\pi$ stacking, and salt bridge**. In the figures, each type of interaction is represented by dashed lines, color-coded to correspond with the specific interaction type.\n\nAnother visualization of our pocket-matching result is shown in **Figure 7**. We showed an example of two non-homology ATP binding pockets to explain why pocket-matching can benefit from interaction-aware pretraining so that we have achieved superior results in the Kahraman dataset and the BioLip t-SNE visualization. The PEP carboxykinase(PDB:1AYL) and tRNA synthetase(PDB:1B8A) are two ATP-binding proteins that share zero sequence similarity (verified with the BLAST) extracted from the Kahraman dataset. However, as they are both fueled by ATP, their binding site shares similar binding patterns. The cation-$\\pi$ interactions(blue dash lines) and salt bridges (magenta dash lines) are important to the ATP binding, which can be viewed as convergent evolutions at the molecule level. Even though, these two pockets are very distinct in terms of shapes and sizes because they bind ATPs in different conformations. Therefore, biochemical interactions are the key to accomplishing the pocket-matching task, which is ignored in previous self-supervised learning methods like Uni-Mol.\n\n[1] Polizzi, Nicholas F., and William F. DeGrado. \"A defined structural unit enables de novo design of small-molecule–binding proteins.\" Science 369, no. 6508 (2020)\n\n---\n\n# Response 2/3\n\n## Response to the bound of Theorem 3.1 \n\nWe are not very clear about the point of your question. We have two understandings and they are replied as follows.\n\nIf you mean $ ||t − t ^{(0)}|| < 1/2 l_T$ is trivial, please note that it is only a condition of the theorem. As you said, many other encoders can also satisfy this condition. This shows that our method is applicable to many different molecular encoders with or without pretraining as we proved in **Table 4**. Moreover, the condition is only a necessary condition. We also need the pretraining contrastive loss to be sufficiently small to guarantee a small contrastive loss between pockets and real ligands. In fact, in the process of our early exploration, we noticed that some encoders cannot achieve a low pretraining loss, indicating the entire conditions of the theorem are nontrivial.\n\nIf you mean that the conclusion of theorem 3.1 is trivial, we want to emphasize that our conclusion is $\\lim_{L_i(t, s) =0}L_i(t^{(0)}, s) =0$. (We write it in a $\\epsilon$-$\\delta$ language in paper. They are equivalent forms.) Our conclusion shows the loss containing real ligands is consistent with the loss containing pseudo ligands that we optimized in pre-training. \n\nIn contrast,  a trivial result from $||t − t ^{(0)}|| < 1/ 2l_T$ is that $\\lim_{L_i(t, s) =0}L_i(t^{(0)}, s) <B$, B is a bound related to l_t and the representations. It does not guarantee a consistent loss and is substantially different from our result.\nTherefore, our theorem is nontrivial and reveals the transfer ability of our contrastive pre-training from the pseudo-ligand domain to the real ligand domain.\n\nAs for this theoretical result, we are open to further discussion.\n\n## Response to molecules possess more properties that can also lead to imbalance\n\nIt is correct that molecules have more properties than their size, like logP, Hbond donor and acceptor number, and rotatory bond number. As small molecule drugs are mostly designed to penetrate barriers like gut or cell membranes, they usually have much larger logP values, which means more hydrophobic. For the same reason, they usually have fewer hydrogen bond donors and acceptors, and only minimal essential ones are kept for specified interactions. Also, to minimize the entropy effect upon binding and to increase binding affinity, rotatory bonds are also unfavored. However, it is impossible to mimic such features with peptides, as the backbone of peptides is intrinsically hydrophilic and flexible. As we showed in a new figure, our pseudo-ligands are less similar to real ligands in those properties even with size alignment. You can find the figure in the new pdf in **Appendix G**.\n\nWe believe that the aforementioned divergence is the main limitation of poor zero-shot performance in predicting hydrophobicity scores (**Table 1**). However, as demonstrated in previous publications like CoSP, we could leverage real ligand-pocket pairs from PDBBind or BioLip database to further finetune our network, as an extension of the pipeline. In this way, we could handle property mismatches but still enjoy the power of our large-scale pretraining.\n\n## Response to the approach to defining pockets \n\nWe define the pocket following the UniMol setup. We found our model can also adapt to an 8Å setup in the toughM1 experiment \n\nIn particular, we define the pocket for each protein-ligand pair as residues of the protein that have at least one atom within the range of 6Å from a heavy atom in the ligand. To further explain our design, we have done an ablation study on different choices of thresholds of 4Å, 6Å, and 8Å:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| 4Å | 0.7062 | 0.7549 | 0.1240 | 0.1095 | 28.29 | 13.07 |\n| 6Å | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| 8Å | 0.8322 | 0.8292 | 0.1256 | 0.1125 | 34.83 | 12.92 |\n\nSince the 8Å threshold corresponds with the pocket definition in the Kahraman and Tough M1 datasets, it leads to optimal results. Our decision to use a 6Å threshold was made to align with the methodology of pretraining data creation by Uni-Mol, facilitating a fair comparison. Notably, even with the 6Å threshold, we achieved strong results in the pocket-matching task, which serves as a testament to the effectiveness of our approach.\n\n---\n\n# Response 1/3\nWe appreciate the time and effort you have dedicated to reviewing our paper, and we are grateful for your constructive feedback and thoughtful evaluation of our work. We would like to address your comments about the technical novelty of our work and provide additional clarification on certain aspects of our paper.\n\n## Response to technical novelty\n\nFirstly, we acknowledge your observation that we didn't use any fancy models compared with typical **model-centric** works. While we respect your assessment, we would like to highlight that our primary focus in this paper is a **data-centric** pretraining method to introduce groundbreaking improvements in the field of protein pocket pretraining. Our work aims to address a critical challenge of data scarcity in the field of protein pocket representation by constructing large-scale synthetic data that facilitates the pretraining of models, ultimately enhancing the accuracy and robustness of protein pocket representations. We believe that the research community can conduct fast following-ups on our **released dataset** with more sophisticated models, and even extend our pipelines to other tasks like docking or drug design. We also would like to point out that we have provided a novel method to **distill knowledge from well-trained molecule models** to protein models. Though employing the same contrastive loss, our approach is different from existing contrastive learning models since it uses a fixed molecular encoder. The motivation here is to use a well-trained molecular encoder on a relatively larger dataset to **guide** the training of the protein encoder. For example, some quantum-chemistry properties that are difficult to compute for large systems like proteins could be distilled from molecules, by using some quantum-chemistry-aware molecular encoders such as Frad. Though our framework seems to be simple, it is non-trivial to make it work. We have made several efforts to solve the unavoidable discrepancies between true ligands and pseudo ligands. The ablation study shows that our efforts are effective and necessary:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nRegarding your positive acknowledgment of our \"simple but effective model for the right question,\" we are pleased to hear that our approach resonates with the objective we set out to achieve. We designed our model with **simplicity** in mind, prioritizing effectiveness and practical utility for the specific problem domain. This deliberate choice aligns with the notion that sometimes the most impactful solutions are elegantly straightforward. Also, we intentionally borrowed a pocket encoder from Uni-Mol to make a **fair comparison** with it, which strongly supports the power of our dataset. Similar to the molecule encoder, our pocket encoder can be changed to any other model due to our flexible framework. Notably, **we didn't load the pretrained weights of Uni-Mol pocket encoder**. We only use the same backbone architecture and the pretraining was completely done on our processed data with our training strategy. \n\nNotably, many simple but effective approaches have been recognized by top-tier conferences and journals. A prime example of this is in the field of protein language models, like the ESM series and ProtTrans. These studies adapted the Transformer architecture and masked language modeling techniques from natural language processing to protein sequences. While they didn't introduce groundbreaking techniques, their substantial contributions to protein modeling are evident, with publications in prestigious conferences and journals like NeurIPS, ICML, TPAMI, PNAS, and Science. Another example is the widely acclaimed CLIP paper presented at ICML 2021. CLIP, while not employing novel techniques, stands as a hallmark of data-centric deep learning. Its use of contrastive learning enabled training on expansive web-sourced text-image datasets, moving beyond the constraints of meticulously curated databases like MS-COCO. This aligns with our approach to addressing **data scarcity challenges** by facilitating training on large-scale datasets, mirroring our strategy for overcoming similar hurdles in the protein pocket modeling domain.\n\nWe appreciate your efforts to remain neutral and understand the challenges in decision-making. We believe further discussion will highlight our paper's contributions. Your feedback and insights in the upcoming discussion will be invaluable for refining our work.", "author_response": "# Response 1/3\nWe appreciate the time and effort you have dedicated to reviewing our paper, and we are grateful for your constructive feedback and thoughtful evaluation of our work. We would like to address your comments about the technical novelty of our work and provide additional clarification on certain aspects of our paper.\n\n## Response to technical novelty\n\nFirstly, we acknowledge your observation that we didn't use any fancy models compared with typical **model-centric** works. While we respect your assessment, we would like to highlight that our primary focus in this paper is a **data-centric** pretraining method to introduce groundbreaking improvements in the field of protein pocket pretraining. Our work aims to address a critical challenge of data scarcity in the field of protein pocket representation by constructing large-scale synthetic data that facilitates the pretraining of models, ultimately enhancing the accuracy and robustness of protein pocket representations. We believe that the research community can conduct fast following-ups on our **released dataset** with more sophisticated models, and even extend our pipelines to other tasks like docking or drug design. We also would like to point out that we have provided a novel method to **distill knowledge from well-trained molecule models** to protein models. Though employing the same contrastive loss, our approach is different from existing contrastive learning models since it uses a fixed molecular encoder. The motivation here is to use a well-trained molecular encoder on a relatively larger dataset to **guide** the training of the protein encoder. For example, some quantum-chemistry properties that are difficult to compute for large systems like proteins could be distilled from molecules, by using some quantum-chemistry-aware molecular encoders such as Frad. Though our framework seems to be simple, it is non-trivial to make it work. We have made several efforts to solve the unavoidable discrepancies between true ligands and pseudo ligands. The ablation study shows that our efforts are effective and necessary:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nRegarding your positive acknowledgment of our \"simple but effective model for the right question,\" we are pleased to hear that our approach resonates with the objective we set out to achieve. We designed our model with **simplicity** in mind, prioritizing effectiveness and practical utility for the specific problem domain. This deliberate choice aligns with the notion that sometimes the most impactful solutions are elegantly straightforward. Also, we intentionally borrowed a pocket encoder from Uni-Mol to make a **fair comparison** with it, which strongly supports the power of our dataset. Similar to the molecule encoder, our pocket encoder can be changed to any other model due to our flexible framework. Notably, **we didn't load the pretrained weights of Uni-Mol pocket encoder**. We only use the same backbone architecture and the pretraining was completely done on our processed data with our training strategy. \n\nNotably, many simple but effective approaches have been recognized by top-tier conferences and journals. A prime example of this is in the field of protein language models, like the ESM series and ProtTrans. These studies adapted the Transformer architecture and masked language modeling techniques from natural language processing to protein sequences. While they didn't introduce groundbreaking techniques, their substantial contributions to protein modeling are evident, with publications in prestigious conferences and journals like NeurIPS, ICML, TPAMI, PNAS, and Science. Another example is the widely acclaimed CLIP paper presented at ICML 2021. CLIP, while not employing novel techniques, stands as a hallmark of data-centric deep learning. Its use of contrastive learning enabled training on expansive web-sourced text-image datasets, moving beyond the constraints of meticulously curated databases like MS-COCO. This aligns with our approach to addressing **data scarcity challenges** by facilitating training on large-scale datasets, mirroring our strategy for overcoming similar hurdles in the protein pocket modeling domain.\n\nWe appreciate your efforts to remain neutral and understand the challenges in decision-making. We believe further discussion will highlight our paper's contributions. Your feedback and insights in the upcoming discussion will be invaluable for refining our work."}
{"claim": "The paper's results are about computability rather than algorithmics or neural processing because it does not establish an upper bound on time to generate the next token.", "claim_type": "methodology", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:25:52.858900", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The authors do not directly address the reviewer's specific claim about lacking an upper bound on time to generate the next token. They discuss proof details, scope, and assumptions (e.g. infinite languages) but neither concede nor refute that the results are only about computability rather than algorithmic/time bounds.", "evidence": "\"On your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction.\"", "combined_author_response": "Thanks for your reply. We understand that we disagree on several of these points, but we appreciate all the thought you put into the proofs and the details of the paper, and your suggestions about different approaches to the main result.\n\n---\n\nThank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite.\n\n---\n\nThanks very much for your reply; we appreciate that you were able to look through the other reviews and rebuttals, and are glad to see your confirmation of the presence of the error in the solution proposed by Reviewer v4Eq.", "author_response": "Thank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite."}
{"claim": "The contribution is weak from an algorithmic point of view.", "claim_type": "novelty", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:25:43.813359", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The authors explicitly disagree with the reviewer's criticisms and defend the technical and algorithmic substance of their work, arguing the proofs are subtle and nontrivial and that the reviewer’s alternative proofs are incorrect.", "evidence": "\"We understand that we disagree on several of these points...\"; \"we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors.\"; \"we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5.\"", "combined_author_response": "Thanks for your reply. We understand that we disagree on several of these points, but we appreciate all the thought you put into the proofs and the details of the paper, and your suggestions about different approaches to the main result.\n\n---\n\nThank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite.\n\n---\n\nThanks very much for your reply; we appreciate that you were able to look through the other reviews and rebuttals, and are glad to see your confirmation of the presence of the error in the solution proposed by Reviewer v4Eq.", "author_response": "Thank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite."}
{"claim": "The work is more appropriate for a computational learning theory venue (for example COLT) than for NeurIPS.", "claim_type": "subjective", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:25:47.724151", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The authors explicitly disagree that the paper belongs only at a computational learning theory venue and argue it is within NeurIPS scope, citing theoretical tracks and connections to language modeling.", "evidence": "On your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference.", "combined_author_response": "Thanks for your reply. We understand that we disagree on several of these points, but we appreciate all the thought you put into the proofs and the details of the paper, and your suggestions about different approaches to the main result.\n\n---\n\nThank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite.\n\n---\n\nThanks very much for your reply; we appreciate that you were able to look through the other reviews and rebuttals, and are glad to see your confirmation of the presence of the error in the solution proposed by Reviewer v4Eq.", "author_response": "Thank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite."}
{"claim": "The paper's results only hold under the assumption that all languages in the list are infinite.", "claim_type": "methodology", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:25:56.586195", "model": "gpt-5-mini", "label": "Supported", "justification": "Authors acknowledge that allowing finite languages makes the question technically more complicated, confirm they assume languages are infinite, and justify retaining that assumption while offering to discuss the issue in a revision.", "evidence": "\"On the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. ... This is exactly the reason to assume that the candidate languages $L_i$ are infinite.\"", "combined_author_response": "Thanks for your reply. We understand that we disagree on several of these points, but we appreciate all the thought you put into the proofs and the details of the paper, and your suggestions about different approaches to the main result.\n\n---\n\nThank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite.\n\n---\n\nThanks very much for your reply; we appreciate that you were able to look through the other reviews and rebuttals, and are glad to see your confirmation of the presence of the error in the solution proposed by Reviewer v4Eq.", "author_response": "Thank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite."}
{"claim": "Assuming all languages are infinite is an unnatural and unjustified restriction seemingly introduced to make the proof work.", "claim_type": "subjective", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:25:59.679914", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The authors explicitly disagree with the reviewer's characterization and defend the infiniteness assumption as reasonable and motivated; they acknowledge extra technical complications if languages can be finite but argue those complications detract from the motivation rather than justifying rejecting the assumption.", "evidence": "\"On the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. ... In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite.\"", "combined_author_response": "Thanks for your reply. We understand that we disagree on several of these points, but we appreciate all the thought you put into the proofs and the details of the paper, and your suggestions about different approaches to the main result.\n\n---\n\nThank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite.\n\n---\n\nThanks very much for your reply; we appreciate that you were able to look through the other reviews and rebuttals, and are glad to see your confirmation of the presence of the error in the solution proposed by Reviewer v4Eq.", "author_response": "Thank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite."}
{"claim": "The proof of the main theorem breaks if the infinite-language assumption is removed.", "claim_type": "methodology", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:26:00.615676", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors acknowledge that allowing finite languages makes the situation \"technically more complicated\" and say they can discuss this in a revision, but they do not concede that the proof outright \"breaks\"; instead they defend keeping the infinite-language assumption as reasonable.", "evidence": "\"On the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision.\"; \"This is exactly the reason to assume that the candidate languages $L_i$ are infinite.\"", "combined_author_response": "Thanks for your reply. We understand that we disagree on several of these points, but we appreciate all the thought you put into the proofs and the details of the paper, and your suggestions about different approaches to the main result.\n\n---\n\nThank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite.\n\n---\n\nThanks very much for your reply; we appreciate that you were able to look through the other reviews and rebuttals, and are glad to see your confirmation of the presence of the error in the solution proposed by Reviewer v4Eq.", "author_response": "Thank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite."}
{"claim": "The authors' explanation in line 126 for requiring all languages to be infinite is unconvincing.", "claim_type": "subjective", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:26:06.539396", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The authors reject the reviewer's characterization as unconvincing: they acknowledge added technical complications if languages can be finite but argue those complications detract from the motivation and reaffirm that assuming infinite languages is appropriate for real language generation, thus disagreeing with the claim.", "evidence": "\"On the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. ... This is exactly the reason to assume that the candidate languages $L_i$ are infinite.\"", "combined_author_response": "Thanks for your reply. We understand that we disagree on several of these points, but we appreciate all the thought you put into the proofs and the details of the paper, and your suggestions about different approaches to the main result.\n\n---\n\nThank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite.\n\n---\n\nThanks very much for your reply; we appreciate that you were able to look through the other reviews and rebuttals, and are glad to see your confirmation of the presence of the error in the solution proposed by Reviewer v4Eq.", "author_response": "Thank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite."}
{"claim": "Specifying that an algorithm must output unseen strings forever is impossible for finite languages, making that requirement problematic.", "claim_type": "methodology", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:26:13.776772", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "Authors acknowledge that allowing finite languages makes the question technically more complicated and say they can discuss it in a revision, but they do not accept that this invalidates their approach — they defend assuming infinite languages as the intended setting.", "evidence": "On the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. ... we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. ... This is exactly the reason to assume that the candidate languages $L_i$ are infinite.", "combined_author_response": "Thanks for your reply. We understand that we disagree on several of these points, but we appreciate all the thought you put into the proofs and the details of the paper, and your suggestions about different approaches to the main result.\n\n---\n\nThank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite.\n\n---\n\nThanks very much for your reply; we appreciate that you were able to look through the other reviews and rebuttals, and are glad to see your confirmation of the presence of the error in the solution proposed by Reviewer v4Eq.", "author_response": "Thank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite."}
{"claim": "The infinite-language assumption prevents instantiating the result with any computational model where language finiteness is undecidable.", "claim_type": "methodology", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:26:19.932715", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors acknowledge that allowing finite languages makes the question technically more complicated and offer to address it in a revision, but they defend the infinite-language assumption as motivated and do not concede that undecidable finiteness prevents instantiation of their result.", "evidence": "\"On the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision.\"; \"This is exactly the reason to assume that the candidate languages $L_i$ are infinite.\"", "combined_author_response": "Thanks for your reply. We understand that we disagree on several of these points, but we appreciate all the thought you put into the proofs and the details of the paper, and your suggestions about different approaches to the main result.\n\n---\n\nThank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite.\n\n---\n\nThanks very much for your reply; we appreciate that you were able to look through the other reviews and rebuttals, and are glad to see your confirmation of the presence of the error in the solution proposed by Reviewer v4Eq.", "author_response": "Thank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite."}
{"claim": "Only very restricted classes of languages have decidable finiteness, so the finiteness assumption severely limits applicability.", "claim_type": "other", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:26:19.921342", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "Authors acknowledge that allowing finite languages makes the problem technically more complicated (agreeing with part of the reviewer's concern) but do not concede that this 'severely limits applicability' — instead they defend the infinite-language assumption as justified for the paper's motivation and say they can discuss finiteness in a revision.", "evidence": "1) \"On the point about the languages L_i being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision.\" 2) \"we think that these added complications arising from finiteness detract from the underlying motivation...the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances...This is exactly the reason to assume that the candidate languages L_i are infinite.\"", "combined_author_response": "Thanks for your reply. We understand that we disagree on several of these points, but we appreciate all the thought you put into the proofs and the details of the paper, and your suggestions about different approaches to the main result.\n\n---\n\nThank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite.\n\n---\n\nThanks very much for your reply; we appreciate that you were able to look through the other reviews and rebuttals, and are glad to see your confirmation of the presence of the error in the solution proposed by Reviewer v4Eq.", "author_response": "Thank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite."}
{"claim": "Going beyond context-free languages typically renders finiteness or emptiness decision problems undecidable.", "claim_type": "other", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:26:20.815749", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The authors do not respond to the reviewer's claim about decidability (undecidability of finiteness or emptiness when going beyond context-free languages). They discuss proof issues and the assumption that candidate languages are infinite, but they do not address decidability/undecidability or the context-free boundary explicitly.", "evidence": "\"On the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision.\"", "combined_author_response": "Thanks for your reply. We understand that we disagree on several of these points, but we appreciate all the thought you put into the proofs and the details of the paper, and your suggestions about different approaches to the main result.\n\n---\n\nThank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite.\n\n---\n\nThanks very much for your reply; we appreciate that you were able to look through the other reviews and rebuttals, and are glad to see your confirmation of the presence of the error in the solution proposed by Reviewer v4Eq.", "author_response": "Thank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite."}
{"claim": "Undecidability of finiteness rules out enumerating languages by enumerating the machines that represent those languages.", "claim_type": "other", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:26:33.209506", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "Authors acknowledge that allowing finite languages makes the problem technically more complicated (supporting part of the reviewer's concern), but they do not accept this as ruling out enumeration; instead they assume candidate languages are infinite and treat finiteness as a non-core complication.", "evidence": "\"On the point about the languages L_i being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision.\"; \"we think that these added complications arising from finiteness detract from the underlying motivation... This is exactly the reason to assume that the candidate languages L_i are infinite.\"", "combined_author_response": "Thanks for your reply. We understand that we disagree on several of these points, but we appreciate all the thought you put into the proofs and the details of the paper, and your suggestions about different approaches to the main result.\n\n---\n\nThank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite.\n\n---\n\nThanks very much for your reply; we appreciate that you were able to look through the other reviews and rebuttals, and are glad to see your confirmation of the presence of the error in the solution proposed by Reviewer v4Eq.", "author_response": "Thank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite."}
{"claim": "The paper does not explicitly acknowledge that assuming all languages are infinite is a drawback.", "claim_type": "methodology", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:26:38.061081", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "Authors acknowledge that allowing finite languages complicates the picture and offer to discuss this in a revision (partial agreement), but they defend the infinite-language assumption and argue it is not a drawback.", "evidence": "On the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite.", "combined_author_response": "Thanks for your reply. We understand that we disagree on several of these points, but we appreciate all the thought you put into the proofs and the details of the paper, and your suggestions about different approaches to the main result.\n\n---\n\nThank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite.\n\n---\n\nThanks very much for your reply; we appreciate that you were able to look through the other reviews and rebuttals, and are glad to see your confirmation of the presence of the error in the solution proposed by Reviewer v4Eq.", "author_response": "Thank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite."}
{"claim": "The presentation and clarity of the paper can be significantly improved.", "claim_type": "presentation", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:26:46.708337", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The authors explicitly disagree with the reviewer's suggestion that presentation/clarity should be shortened or significantly changed; they defend the current level of detail as necessary and appropriate, noting the proofs are subtle and the full explanations are important. They offer only a limited concession on discussing a specific technical variant (finite languages) in a revision.", "evidence": "We understand that we disagree on several of these points, but we appreciate all the thought you put into the proofs and the details of the paper, and your suggestions about different approaches to the main result. You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5.", "combined_author_response": "Thanks for your reply. We understand that we disagree on several of these points, but we appreciate all the thought you put into the proofs and the details of the paper, and your suggestions about different approaches to the main result.\n\n---\n\nThank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite.\n\n---\n\nThanks very much for your reply; we appreciate that you were able to look through the other reviews and rebuttals, and are glad to see your confirmation of the presence of the error in the solution proposed by Reviewer v4Eq.", "author_response": "Thank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite."}
{"claim": "The reviewer asks why the authors did not explicitly state in the paper that the finiteness assumption is a drawback.", "claim_type": "presentation", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:26:43.046686", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors acknowledge the issue (agreeing that allowing finite languages makes the question more complicated and that they can discuss it in a revision) but do not concede that the finiteness assumption is a drawback; instead they defend assuming infinite languages as more relevant and say the complications detract from the motivation.", "evidence": "On the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. ... This is exactly the reason to assume that the candidate languages $L_i$ are infinite.", "combined_author_response": "Thanks for your reply. We understand that we disagree on several of these points, but we appreciate all the thought you put into the proofs and the details of the paper, and your suggestions about different approaches to the main result.\n\n---\n\nThank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite.\n\n---\n\nThanks very much for your reply; we appreciate that you were able to look through the other reviews and rebuttals, and are glad to see your confirmation of the presence of the error in the solution proposed by Reviewer v4Eq.", "author_response": "Thank you for your reply, and for suggesting a fix to your earlier incorrect argument. We'd like to start by pointing out that your new proposed proof is also incorrect. The problem is in the step where you claim that \"Since $L_z$ is consistent with $S_t$ for every $t$, we have that for any $t\\geq z$, there is at least one simply-critical language.\" Here is an example that shows there might be no simply-critical language at certain steps where $t \\geq z$, contradicting this claim. For the example, let the languages be subsets of the natural numbers, let $L_1$ consist of all multiples of 6, $L_2$ consist of all multiples of 10, and $L_3$ consist of all multiples of 15. (It will not be important for this example what $L_4, L_5, ...$ are.) Let $L_3$ be the true language; i.e. $z = 3$. Suppose that the adversary's first three examples are 60, 120, and 180, so at $t = 3$, the set $S_t$ is {60,120,180}. \n\nFor this value $t = 3$, which satisfies $t \\geq z$, each of $L_1$, $L_2$, and $L_3$ is consistent with $S_t$, but none is a subset of any of the others, so there is no simply-critical language for $t = 3$. (Note that in comparison, there is a language in this example that is critical under our definition, since as noted in the paper, the first consistent language is always critical.) \n\nSince the author response period is closing today and we may not get a chance to respond further, we would like to make a few further points based on the above. \n\n(i) First, it would be possible to modify your proof to get to a correct proof, but the ways we see to do it would involve incorporating the remaining ideas from the proof in our paper. In particular, defining criticality has to be done carefully, as the problem with your incorrect argument introducing simple-criticality makes clear. Moreover, and crucially, even with our notion of criticality, the true language $L_z$ does not necessarily become critical as soon as $t \\geq z$ (as you were attempting to achieve with simple-criticality). Rather, we may have to wait until a potentially later step in the enumeration; our paper accomplishes this in (4.3) via the analysis of the step $t^+$. If you make all these changes, then you would fix the problems with your current proposed proof, but you would also be gradually arriving at all the steps in our current proof. \n\n(ii) You argue that our explanations have unnecessary length. Given that full proof in our paper is only a few pages, we do not think it is particularly long in an absolute sense, even with complete explanations included. Moreover, given that your reviews have now contained two incorrect attempts at a proof, we would suggest that this indicates how getting the details of the proof right is fairly subtle, and it is easy to inadvertently set things up in a way that leads to errors. That is exactly the kind of situation that we typically think of as calling for complete arguments and explanations rather than abbreviated ones. For example, your later suggested description that \"The inclusion queries in Item 1 can be replaced by membership tests (as done by the authors) simply by considering slices of the languages up to $L_t$ containing only strings up to a certain length\" is indeed correct at a high level, but it is essentially equivalent to Section 5 of our paper, just with all the details suppressed. Given the subtlety of these arguments, and the ease with which errors can arise, we think it is important for these details to be present; and if you were to fill in these details, you would get back to something essentially equivalent to Section 5. \n\nOn your remaining points, we believe that the feasibility of language generation is a question of fundamental interest, and given that NeurIPS has tracks for theoretical work on the inherent limits to learning, we also believe it is clearly in scope for the conference. The paper discusses on pages 2-3 and again on page 9 some of the potential connections to current issues in language modeling; we agree there are many open questions that can be considered here, and we find the presence of these open questions a benefit of the current direction. \n\nOn the point about the languages $L_i$ being infinite, as noted earlier, we agree that the question becomes technically more complicated when some languages can be finite, and we can indeed discuss this point in a revision. As we also discussed earlier, we think that these added complications arising from finiteness detract from the underlying motivation rather than adding to it. In particular, we'd reiterate the point from our earlier response that the challenge in real language generation problems is not the concern that the training data might have exhausted all possible valid utterances; it is generally understood, both intuitively and on more technical grounds, that there will always be further valid utterances that have not yet been seen. This is exactly the reason to assume that the candidate languages $L_i$ are infinite."}
{"claim": "The paper does not report the computational latency or runtime cost of the iterative initialization algorithm used for decomposing pretrained matrices.", "claim_type": "experimental", "paper_id": "xw29VvOMmU", "paper_title": "LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Bm88GSraZd", "reviewer": "Reviewer_CSBT", "review_text": "Summary: This paper proposes LQ-LoRA, a memory-efficient LLM adaptation method that decomposes each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. The algorithm is adapted from QLoRA and applied modification to solve the problem that zero initialization of the low-rank matrix may not be optimal when the fixed matrix is quantized. The method decomposes the matrix by an iterative algorithm and updates only the low-rank matrix weights during fine-tuning. Results showed that the proposed method outperforms QLoRA and LREC with similar bit compression rates.\n\nStrengths: -\tThe proposed method decomposes the pretrained matrix into a quantizable fixed matrix and low-rank matrix that is already optimized before fine-tuning starts, which contributes to improved accuracy.\n-\tThe paper shows that LQ-LoRA can be used as a mixed quantization strategy, and also proposes a data-aware version of the algorithm, which enables users to flexibly set a target memory budget.\n-\tResults show that the proposed method can be generalized to different model families by showing outperforming results with RoBERTa and LLaMA.\n\nWeaknesses: - The authors have introduced a method that employs an iterative algorithm for initialization. Can they provide insights regarding the computational latency associated with their approach?\n\n- The authors assert the efficiency of LQ-LoRA based on empirical evidence, yet lack theoretical backing. To strengthen the credibility of the algorithm, a comparison might be beneficial, especially with methods that initialize the Q(W) + L1L2 matrix in a manner that closely mirrors the original pretrained matrix W. Consider, for instance, the use of GPTQ as a compensatory mechanism.\n\n- It appears that this paper serves as an expanded or refined rendition of the Q-LoRA paper. As such, it seemingly inherits the same limitation, notably the inference overhead, given that this approach must fail to integrate the LoRA layer into an existing linear layer. \n\n- Similarly, I would like to raise a query about the paper's novelty. While this method undeniably enhances the current approach (Q-LoRA), from a PEFT perspective, there could be superior methods, particularly concerning inference challenges. On the topic of novelty, I await the insights of fellow reviewers.\n\nQuestions: Included in the weakness.", "labeling_timestamp": "2026-01-11T16:26:34.513690", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors directly address the runtime question: they describe the iterative algorithm as a one-time, small overhead and provide measured timings (seconds per matrix) for processing LLaMA-2 7B matrices with rank=64 and NF3 quantization.", "evidence": "Latency of The Iterative Algorithm\n\nThis is a one-time cost at the initialization and takes a fraction of the total training time. Each step of the iterative algorithm consists of SVD followed by quantization. Note that we use randomized SVD (rSVD) for speed consideration, and we run the algorithm for up to 100 steps (and we stop before this if the error starts to increase).\n\nTo be more concrete, we measured the runtime to process three of the LLaMA-2 7B's matrices with `rank=64` and NF3 quantization. Please see the table below.\n\n| Data-Aware (i.e., Section 3.3) | Time / Matrix |\n| ----- | ----- |\n| No  | 1.3 to 2.1 seconds |\n| Yes | 1.3 to 2.3 seconds |", "combined_author_response": "Thank you for the discussion! Please see our responses below.\n\n_1. The accuracy of LREC-4bit on MMLU almost matches that of LQ-LoRA_\n\nFor both LLaMA-2 7B and 70B models, LQ-LoRA (Fisher) with 3.25 bits can match the performance of LREC/GPTQ-LoRA 4-bits (i.e., 4.156-bits). Similarly, LQ-LoRA (Fisher) with 2.5/2.75 bits can match the performance of LREC/GPTQ-LoRA 3-bits (i.e., 3.148-bits). Please see the table below. \n\n| Method | Bits per param | MMLU (70B) | MMLU (7B) |\n| ----- | ----- | ----- | ----- |\n| GPTQ-LoRA 3-bit |  3.148 | 0.67$^\\dagger$ | 0.39 |\n| LQ-LoRA (Fisher) | 2.50 | 0.67 | 0.39 |\n| LQ-LoRA (Fisher) | 2.75 | 0.67 | 0.43 |\n\n| Method | Bits per param | MMLU (70B) | MMLU (7B) |\n| ----- | ----- | ----- | ----- |\n| LREC-4 bit | 4.156 | 0.69 | 0.45 |\n| LQ-LoRA (Fisher) | 3.25 | 0.69 | 0.46 |\n\n$^\\dagger$Note that we additionally filled in the MMLU 70B performance for GPTQ-LoRA 3-bit; this number was absent from the initial submission. Based on the above, we conclude that LQ-LoRA can meaningfully improve upon GPTQ-LoRA and QLoRA.\n\n_2. The authors compare their method with GPTQ-LoRA only_\n\nBesides GPTQ-LoRA, we also compared our method QLoRA and different variants of LQ-LoRA. In addition, our latest PTQ experiments included comparisons with RTN, GPTQ, AWQ, and OmniQuant. For details, please see \"Additional Post Training Quantization Experiments\" in our earlier response.\n\n_3. BRECQ and FlexRound_\n\nThanks for the suggestions! We think applying other quantization techniques is certainly interesting, but we would like to raise three points.\n\na) For quantizing large language models with 10B+ parameters, our understanding is that  NF quantization (from QLoRA) and GPTQ --- methods we compared with --- are near state-of-the-art quantization methods. (While we show numbers from other methods, such as Omniquant, in the PTQ comparison table above, these are, as far as we know, preprints).\n\nb) Applying fancier quantization techniques such as BRECQ/Adaround to large models is challenging. For example, for BRECQ,  the OmniQuant paper [1] notes,\n> ... BRECQ ... cannot be applied in models with billions of parameters because they are hard to optimize due to the huge solution space.\n\nSimilarly, the SpQR paper [2] stated that,\n\n> ... BRECQ ... were designed for vision models or small-scale language models, with less than 100M parameters.\n\n[1] https://arxiv.org/abs/2308.13137\n\n[2] https://arxiv.org/abs/2306.03078\n\nFlexRound is an interesting suggestion, but we were unable to find an open-source implementation of this. Hence, we focused our comparison on widely-used LLM quantization methods with open-source code, i.e., QLoRA and GPTQ-LoRA.\n\nc) Finally, (and most importantly), we note that LQ-LoRA can work with generic quantization approaches! For example, during each step of the iterative algorithm, instead of using NF-quantization, we can use other quantization methods to minimize $\\Vert \\mathbf{X}\\mathbf{W} - \\mathbf{X}(\\operatorname{quantize}(\\mathbf{W}) + \\mathbf{L}_1 \\mathbf{L}_2) \\Vert$. We focused on NF quantization because the quantization function is extremely quick (on the order of seconds), and it performs on par with GPTQ despite being data-agnostic. But we think it's worth highlighting this aspect of LQ-LoRA more, and we will discuss this further.\n\n---\n\nHi there! Please let us know if the above response answered some of your questions in the original review, and please let us know if you have follow-up questions!\n\nIn particular, we want to highlight a few key takeaways from our response:\n1. We included a baseline that uses OPTQ/GPTQ to quantize first before learning low-rank updates. We called this \"GPTQ-LoRA\" baseline \"LREC\" in the paper.\n2. We performed additional experiments using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. (Please see the above response for more details.)\n3. The techniques in this paper are not specific to NF. We chose NF because of its superior quality. To be more concrete about this, we conducted additional experiments. (Please see the above response for more details, too.)\n\n---\n\nHi there! Please let us know if the above response answered some of your questions in the original review, and please let us know if you have follow-up questions!\n\nIn particular, we want to highlight a few key takeaways from our response:\n1. The iterative algorithm adds a small extra computation head --- a fraction of the total training time.\n2. We included GPTQ-LoRA comparisons but referred to them as \"LREC\" in the paper.\n3. Inference with quantized + low-rank is not necessarily slower, especially in batch = 1 setting (per the latest releases from QLoRA; kudos to them).\n4. Practitioners could use the \"quantized + low-rank\" models without merging. Such models could be just as fast as merged dense models and more memory-efficient. Based on this potential, we performed additional experiments by using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset.\n\n---\n\nThanks for the comments --- please see our responses below.\n\n**Comparison with methods that use the data $X$ to quantize**\n\nThanks for the question! We actually did implement and compare against a baseline that uses OPTQ/GPTQ to quantize first before learning low-rank updates. In particular, we use GPTQ to first quantize the model and then learn low-rank updates. We called this \"GPTQ-LoRA\" baseline \"LREC\" in the paper following this paper (https://arxiv.org/pdf/2306.08162.pdf). We find that GPTQ-LoRA underperforms both QLoRA and LQ-LoRA (Table 1).\n\n(We now realize that this naming caused some confusion. Thanks for pointing that out --- we will clarify this in the paper.)\n\n\n**Additional Post Training Quantization Experiments**\n\nIn addition to the GPTQ-LoRA experiments already in the paper, we performed additional experiments using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. Specifically, we train the LQ-LoRA (Fisher, 2.75-bit, 64-rank) for one epoch on two C4 partitions and WikiText-2. We then further quantize the LoRA components themselves to 8-bit (NF8). Please see the Table below for the results, where the numbers other than LQ-LoRA are from OmniQuant [1]. Note that we use the term \"effective bits\" to denote bits per parameter that treat the LoRA components as \"overheads\" for comparing with methods without LoRA; LoRA components (with NF-8 quantization) amount to about 0.1-0.2 extra bits per parameter.\n\n\n\n| Base Model | Method                           | Effective Bits (7B, 70B) | C4 (7B) | C4 (70B) | WikiText2 (7B) | WikiText2 (70B) |\n| -------- | ---------------------------------- | ----------- | ---- | ---- | ---- | ---- |\n| LLaMA-2 | Dense                               | 16          | 6.97 | 5.52 | 5.47 | 3.31 |\n| LLaMA-2 | RTN                                 | 3.15        | 8.40 | 6.02 | 6.66 | 3.97 |\n| LLaMA-2 | GPTQ            | 3.15        | 7.89 | 5.85 | 6.29 | 3.85 |\n| LLaMA-2 | AWQ               | 3.15        | 7.84 | -    | 6.24 | -    |\n| LLaMA-2 | OmniQuant  | 3.15        | 7.75 | 5.85 | 6.03 | 3.78 |\n| LLaMA-2 | LQ-LoRA (Fisher)   | 2.95, 2.85 | 7.60 | 5.88 | 5.67 | 3.65 |\n\n\n\n\n[1] https://arxiv.org/abs/2308.13137\n\nThese results indicate that LQ-LoRA can also be used as a state-of-the-art PTQ method. Given the promise of LQ-LoRA as a potential alternative to PTQ, we will include this comparison against PTQ results in the next iteration of the paper.\n\n\n\n**NF-based vs INT-based Quantization**\n\nYou are correct in that the techniques in this paper are not specific to NF. QLoRA paper demonstrated the superior quality of such a scheme compared to classical integer quantization. Our preliminary experiments confirmed this, and hence, we chose NF.\n\nWe conducted the following experiment to be more concrete about NF vs INT. We ran LQ (i.e., Section 3.1) on LLaMA-2 7B's dense parameters with `rank=64` and NF3/INT3/NF4/INT4 quantization configuration (i.e., `b0=3/4, b1=8, b2=fp32, B0=64, B1=256`), and measure the reconstruction errors (i.e., $||\\mathbf{W} - (\\mathbf{Q} + \\mathbf{L}_1 \\mathbf{L}_2) ||_2^2$). The table below shows that NF indeed performed better.\n\n| Method      | Reconstruction error (unit: $10^4$) |\n| ----------- | ----------- |\n| LQ with NF3      | 7.12       |\n| LQ with INT3     | 11.6        |\n| LQ with NF4      | 1.42       |\n| LQ with INT4     | 2.02        |\n\n---\n\nThanks to the reviewer for the comments! Please see our responses below.\n\n**Latency of The Iterative Algorithm**\n\nThis is a one-time cost at the initialization and takes a fraction of the total training time. Each step of the iterative algorithm consists of SVD followed by quantization. Note that we use randomized SVD (rSVD) for speed consideration, and we run the algorithm for up to 100 steps (and we stop before this if the error starts to increase).\n\nTo be more concrete, we measured the runtime to process three of the LLaMA-2 7B's matrices with `rank=64` and NF3 quantization. Please see the table below.\n\n| Data-Aware (i.e., Section 3.3) | Time / Matrix |\n| ----- | ----- |\n| No  | 1.3 to 2.1 seconds |\n| Yes | 1.3 to 2.3 seconds |\n\n**Comparison with GPTQ-based Methods**\n\nThanks for the question! We actually did implement and compare against GPTQ-LoRA. We called this \"LREC\" in the paper following this paper (https://arxiv.org/pdf/2306.08162.pdf). We find that GPTQ-LoRA underperforms both QLoRA and LQ-LoRA (Table 1).\n\n(We now realize that this naming caused some confusion. Thanks for pointing that out --- we will clarify this in the paper.)\n\n**Limitations from QLoRA**\n\nThis is a very good point! But we want to point out that inference with quantized + low-rank is not necessarily slower, especially in batch = 1 setting. Smaller quantized matrices allow us to reduce data movement between SRAM and HBM of the GPU. For example, in the latest releases of `bitsandbytes` (the low-level engine behind QLoRA) [1], they claimed their matrix-vector multiplication between quantized matrix and dense vector (the one needed during inference) could be competitive and even outperformed dense matrix-vector multiplication. \n\n[1] https://github.com/TimDettmers/bitsandbytes/releases\n\n*Integrating LoRA Layers Into Linear Layers*\n\nGiven the potential inference speed-ups, we think that practitioners could use the \"quantized + low-rank\" models without merging. Such models could be just as fast (as discussed above) as merged dense models and more memory-efficient. In that regard, you could (loosely speaking) consider this as an alternative PTQ method.\n\nBased on this potential of LQ-LoRA as an alternative to PTQ, we performed additional experiments by using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. Specifically, we train the LQ-LoRA (Fisher, 2.75-bit, 64-rank) for one epoch on two C4 partitions and WikiText-2. We then further quantize the LoRA components themselves to 8-bit (NF8). Please see the Table below for the results, where the numbers other than LQ-LoRA are from OmniQuant [1]. Note that we use the term \"effective bits\" to denote bits per parameter that treat the LoRA components as \"overheads\" for comparing with methods without LoRA; LoRA components (with NF-8 quantization) amount to about 0.1-0.2 extra bits per parameter.\n\n| Base Model | Method                           | Effective Bits (7B, 70B) | C4 (7B) | C4 (70B) | WikiText2 (7B) | WikiText2 (70B) |\n| -------- | ---------------------------------- | ----------- | ---- | ---- | ---- | ---- |\n| LLaMA-2 | Dense                               | 16          | 6.97 | 5.52 | 5.47 | 3.31 |\n| LLaMA-2 | RTN                                 | 3.15        | 8.40 | 6.02 | 6.66 | 3.97 |\n| LLaMA-2 | GPTQ            | 3.15        | 7.89 | 5.85 | 6.29 | 3.85 |\n| LLaMA-2 | AWQ               | 3.15        | 7.84 | -    | 6.24 | -    |\n| LLaMA-2 | OmniQuant  | 3.15        | 7.75 | 5.85 | 6.03 | 3.78 |\n| LLaMA-2 | LQ-LoRA (Fisher)   | 2.95, 2.85 | 7.60 | 5.88 | 5.67 | 3.65 |\n\n[1] https://arxiv.org/abs/2308.13137\n\nThese results indicate that LQ-LoRA can also be used as a state-of-the-art PTQ method. Given the promise of LQ-LoRA as a potential alternative to PTQ, we will include this comparison against PTQ results in the next iteration of the paper.\n\n\n**Novelty**\n\n*Similarly, I would like to raise a query about the paper's novelty. While this method undeniably enhances the current approach (Q-LoRA), from a PEFT perspective, there could be superior methods, particularly concerning inference challenges. On the topic of novelty, I await the insights of fellow reviewers.*\n\nWe want to reiterate three main contributions on top of QLoRA:\n1. We adopt a matrix decomposition view of initializing the quantized and low-rank components of LoRA adaptation. (Note that QLoRA does not perform explicit matrix decomposition)\n2. Our ILP formulation enables us to search for the mixed-configuration quantization that fits specific resource requirements (e.g., 2.75 bits per parameter).\n3. We extended the two methods above to a data-aware setting by incorporating the sensitivities of parameters.\n\n---\n\nThanks for the helpful review! Please take a look at our responses below.\n\n**Novelty with respect to QLoRA**\n\nWe want to note that QLoRA does not perform explicit matrix decomposition. It performs quantization of the original weight matrix and learns additive low-rank updates. In contrast, we perform explicit matrix decomposition of the original matrix into low-rank and quantized components via a matrix reconstruction objective.\n\n**Ablations**\n\n*How important is the ILP to LQ-LoRA? Can you show the performance of LQ-LoRA without the ILP?*\n\nOne of the main usages of ILP is to flexibly quantize the model to meet specific memory constraints (e.g., 2.75 bits/parameter). That being said, LQ-LoRA is effective even without the ILP. For example, Table 4 shows LQ-LoRA without ILP. Here, we use NF3 quantization configuration --- NF3 reuses the configuration of the QLoRA's NF4 quantization with 3-bit first-level quantization. \n\n*Can you show the performance of the regular LoRA method (no quantization), and also quantization (at different bit-rates) without LoRA, in Table 2?*\n\nWe want to note that the QLoRA paper already demonstrated little performance loss between regular LoRA and QLoRA (4bit). Hence, we can think of QLoRA (4bit) performance as a proxy for regular LoRA.\n\nHowever, based on your suggestion/question, we performed additional experiments using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. Specifically, we train the LQ-LoRA (Fisher, 2.75-bit, 64-rank) for one epoch on two C4 partitions and WikiText-2. We then further quantize the LoRA components themselves to 8-bit (NF8). Please see the Table below for the results, where the numbers other than LQ-LoRA are from OmniQuant [1]. Note that we use the term \"effective bits\" to denote bits per parameter that treat the LoRA components as \"overheads\" for comparing with methods without LoRA; LoRA components (with NF-8 quantization) amount to about 0.1-0.2 extra bits per parameter.\n\n| Base Model | Method                           | Effective Bits (7B, 70B) | C4 (7B) | C4 (70B) | WikiText2 (7B) | WikiText2 (70B) |\n| -------- | ---------------------------------- | ----------- | ---- | ---- | ---- | ---- |\n| LLaMA-2 | Dense                               | 16          | 6.97 | 5.52 | 5.47 | 3.31 |\n| LLaMA-2 | RTN                                 | 3.15        | 8.40 | 6.02 | 6.66 | 3.97 |\n| LLaMA-2 | GPTQ            | 3.15        | 7.89 | 5.85 | 6.29 | 3.85 |\n| LLaMA-2 | AWQ               | 3.15        | 7.84 | -    | 6.24 | -    |\n| LLaMA-2 | OmniQuant  | 3.15        | 7.75 | 5.85 | 6.03 | 3.78 |\n| LLaMA-2 | LQ-LoRA (Fisher)   | 2.95, 2.85 | 7.60 | 5.88 | 5.67 | 3.65 |\n\n[1] https://arxiv.org/abs/2308.13137\n\nThese results indicate that LQ-LoRA can also be used as a state-of-the-art PTQ method. Given the promise of LQ-LoRA as a potential alternative to PTQ, we will include this comparison against PTQ results in the next iteration of the paper.\n\n**ILP for Rank Selection**\n\nThis is a very good/interesting suggestion! We have looked into this by ILP-searching the bits and ranks together. One practical challenge is measuring the \"storages/errors\"  of the ranks of low-rank components. This is important to trade off bits of the quantized matrices and ranks of LoRA. Notice that LoRA components will be fine-tuned; searching the ranks using errors at initialization will unnecessarily favor more bits at the cost of lower ranks. We might be able to formulate this as a bi-level optimization problem (first searching the ranks and then searching the bits), but we will leave this as future work.\n\n**Questions**\n\n*Is the only difference between QLoRA+ILP, and LQ-LoRA, the initialization?*\n\nYep! \n\n*Does the ILP budget, as well as the \"bits per param\" column, also consider the low-rank components?*\n\nThis table does not take into account low-rank components since we use rank = 64 for our QLoRA and LREC (GPTQ + LoRA) baselines. However, the PTQ table above **does** take into account the contribution from the low-rank components to ensure fair comparison against other PTQ methods. We find that the LoRA components (if quantized) add about ~0.2 bits. We will clarify this further!\n\n**Suggestions**\n\nThanks for the suggestions regarding the figures and tables! We will take these into account for the final version of the paper.\n\n---\n\nWe thank the reviewer for their comments/questions. Please find our responses below.\n\n**Mixed Precision & Hardware**\n\nWe note that our work is in the **weight only** quantization regime (as are QLoRA, GPTQ, etc.), where only the weights (and not the activations) are quantized to lower bits. The actual matmul is done in full precision after dequantization. \n\nConcretely, we use just-in-time de-quantize the (quantized) matrix $\\mathbf{Q}$, execute matrix operations (e.g., `matmul(X, Q) ==> matmul(X, dequantize(Q))`), and throw away the de-quantized matrix. This supports arbitrary quantization, and the main technical difficulty is how to do de-quantization quickly. We outlined more details in the appendix.\n\nAlthough weight-only quantization cannot make use of (faster and more energy-efficient) lower-precision matmuls, weight-only quantization still has two practical benefits. First, at a macro level, having a smaller model (due to quantization) allows us to increase the batch size. This reduces the need for expensive data/model parallelism, which requires cross-device or even cross-node communication. Second, at a micro level, smaller quantized matrices allow us to reduce data movement between SRAM and HBM of the GPU. As an example, QLoRA's matrix multiplication implementation could be faster than dense matrix multiplication [1].\n\n[1] https://github.com/TimDettmers/bitsandbytes/releases\n\n**Related paper on Joint Low-rank and Quantized Matrix Decomposition**\n\nThanks for the suggestion; this is an interesting paper! We will discuss this paper in the related works section.\n\nWe primarily considered the cases in which $\\mathbf{L}_1$ and $\\mathbf{L}_2$ are floating-point parameters. This is necessary because we want to fine-tune these parameters. That being said, the techniques introduced in that paper paper might be helpful for other (future) use cases. For example, we could post-training merge $\\mathbf{Q}, \\mathbf{L}_1, \\mathbf{L}_2$, and re-decompose them into three separate quantized matrices for inference.\n\n**Question 1**\n\nYou are correct -- we will clarify this; thanks!\n\n**Question 2**\n\nYou are correct in that the techniques in this paper are not specific to NF. QLoRA paper demonstrated the superior quality of such a scheme compared to classical integer quantization. Our preliminary experiments confirmed this, and hence, we chose NF.\n\nWe conducted the following experiment to be more concrete about NF vs INT. We ran LQ (i.e., Section 3.1) on LLaMA-2 7B's dense parameters with `rank=64` and NF3/INT3/NF4/INT4 quantization configuration (i.e., `b0=3/4, b1=8, b2=fp32, B0=64, B1=256`), and measure the reconstruction errors (i.e., $||\\mathbf{W} - (\\mathbf{Q} + \\mathbf{L}_1 \\mathbf{L}_2) ||_2^2$). The table below shows that NF indeed performed better.\n\n| Method      | Reconstruction error (unit: $10^4$) |\n| ----------- | ----------- |\n| LQ with NF3      | 7.12       |\n| LQ with INT3     | 11.6        |\n| LQ with NF4      | 1.42       |\n| LQ with INT4     | 2.02        |\n\n**Question 3**\n\nThis is the same $\\delta$ in QLoRA, and we treated it as a constant for all settings. We will clarify further in the next iteration of the paper.\n\n**Question 4**\n\nWe _defined_ $\\mathbb{Q}$ as the set of matrices that can be quantized (Equation 1 is a minimization problem, not an exact decomposition problem). Thanks for pointing out the confusion; we will clarify this.\n\n**Question 5**\n\nThe middle figure represents the setting of many (Q)LoRA methods, in which the Low-Rank components are initialized to be zero. In that sense, $\\operatorname{quantize}(\\mathbf{W}) + \\mathbf{L}_1 \\mathbf{L}_2 = \\operatorname{quantize}(\\mathbf{W})$ for those methods.\n\n**Question 6**\n\nGood catch -- it's meant for \"budget\" but shared similar symbols with $B_0$ and $B_1$.\n\n**Question 7**\n\nWe say a matrix $\\mathbf{F}$ has homogenous rows or columns when either rows or columns of the matrix have identical values. We will clarify this.\n\n**Question 8**\n\nTable 4 used the quantization configuration of NF3/NF4. Specifically, they use the block sizes and second-level quantization as the original NF4, but with the first-level quantization set to 3/4-bit. Table 2, however, used ILP to search for the quantization configuration, hence the difference.\n\nWe made such decisions for Table 4 to remove the possible confounding variable of using ILP to search for the quantization configuration (i.e., we just wanted to see the effect of higher rank LoRA components). We will clarify!", "author_response": "Thanks to the reviewer for the comments! Please see our responses below.\n\n**Latency of The Iterative Algorithm**\n\nThis is a one-time cost at the initialization and takes a fraction of the total training time. Each step of the iterative algorithm consists of SVD followed by quantization. Note that we use randomized SVD (rSVD) for speed consideration, and we run the algorithm for up to 100 steps (and we stop before this if the error starts to increase).\n\nTo be more concrete, we measured the runtime to process three of the LLaMA-2 7B's matrices with `rank=64` and NF3 quantization. Please see the table below.\n\n| Data-Aware (i.e., Section 3.3) | Time / Matrix |\n| ----- | ----- |\n| No  | 1.3 to 2.1 seconds |\n| Yes | 1.3 to 2.3 seconds |\n\n**Comparison with GPTQ-based Methods**\n\nThanks for the question! We actually did implement and compare against GPTQ-LoRA. We called this \"LREC\" in the paper following this paper (https://arxiv.org/pdf/2306.08162.pdf). We find that GPTQ-LoRA underperforms both QLoRA and LQ-LoRA (Table 1).\n\n(We now realize that this naming caused some confusion. Thanks for pointing that out --- we will clarify this in the paper.)\n\n**Limitations from QLoRA**\n\nThis is a very good point! But we want to point out that inference with quantized + low-rank is not necessarily slower, especially in batch = 1 setting. Smaller quantized matrices allow us to reduce data movement between SRAM and HBM of the GPU. For example, in the latest releases of `bitsandbytes` (the low-level engine behind QLoRA) [1], they claimed their matrix-vector multiplication between quantized matrix and dense vector (the one needed during inference) could be competitive and even outperformed dense matrix-vector multiplication. \n\n[1] https://github.com/TimDettmers/bitsandbytes/releases\n\n*Integrating LoRA Layers Into Linear Layers*\n\nGiven the potential inference speed-ups, we think that practitioners could use the \"quantized + low-rank\" models without merging. Such models could be just as fast (as discussed above) as merged dense models and more memory-efficient. In that regard, you could (loosely speaking) consider this as an alternative PTQ method.\n\nBased on this potential of LQ-LoRA as an alternative to PTQ, we performed additional experiments by using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. Specifically, we train the LQ-LoRA (Fisher, 2.75-bit, 64-rank) for one epoch on two C4 partitions and WikiText-2. We then further quantize the LoRA components themselves to 8-bit (NF8). Please see the Table below for the results, where the numbers other than LQ-LoRA are from OmniQuant [1]. Note that we use the term \"effective bits\" to denote bits per parameter that treat the LoRA components as \"overheads\" for comparing with methods without LoRA; LoRA components (with NF-8 quantization) amount to about 0.1-0.2 extra bits per parameter.\n\n| Base Model | Method                           | Effective Bits (7B, 70B) | C4 (7B) | C4 (70B) | WikiText2 (7B) | WikiText2 (70B) |\n| -------- | ---------------------------------- | ----------- | ---- | ---- | ---- | ---- |\n| LLaMA-2 | Dense                               | 16          | 6.97 | 5.52 | 5.47 | 3.31 |\n| LLaMA-2 | RTN                                 | 3.15        | 8.40 | 6.02 | 6.66 | 3.97 |\n| LLaMA-2 | GPTQ            | 3.15        | 7.89 | 5.85 | 6.29 | 3.85 |\n| LLaMA-2 | AWQ               | 3.15        | 7.84 | -    | 6.24 | -    |\n| LLaMA-2 | OmniQuant  | 3.15        | 7.75 | 5.85 | 6.03 | 3.78 |\n| LLaMA-2 | LQ-LoRA (Fisher)   | 2.95, 2.85 | 7.60 | 5.88 | 5.67 | 3.65 |\n\n[1] https://arxiv.org/abs/2308.13137\n\nThese results indicate that LQ-LoRA can also be used as a state-of-the-art PTQ method. Given the promise of LQ-LoRA as a potential alternative to PTQ, we will include this comparison against PTQ results in the next iteration of the paper.\n\n\n**Novelty**\n\n*Similarly, I would like to raise a query about the paper's novelty. While this method undeniably enhances the current approach (Q-LoRA), from a PEFT perspective, there could be superior methods, particularly concerning inference challenges. On the topic of novelty, I await the insights of fellow reviewers.*\n\nWe want to reiterate three main contributions on top of QLoRA:\n1. We adopt a matrix decomposition view of initializing the quantized and low-rank components of LoRA adaptation. (Note that QLoRA does not perform explicit matrix decomposition)\n2. Our ILP formulation enables us to search for the mixed-configuration quantization that fits specific resource requirements (e.g., 2.75 bits per parameter).\n3. We extended the two methods above to a data-aware setting by incorporating the sensitivities of parameters."}
{"claim": "The authors assert empirical efficiency of LQ-LoRA but provide no theoretical analysis or proofs to justify those efficiency claims.", "claim_type": "methodology", "paper_id": "xw29VvOMmU", "paper_title": "LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Bm88GSraZd", "reviewer": "Reviewer_CSBT", "review_text": "Summary: This paper proposes LQ-LoRA, a memory-efficient LLM adaptation method that decomposes each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. The algorithm is adapted from QLoRA and applied modification to solve the problem that zero initialization of the low-rank matrix may not be optimal when the fixed matrix is quantized. The method decomposes the matrix by an iterative algorithm and updates only the low-rank matrix weights during fine-tuning. Results showed that the proposed method outperforms QLoRA and LREC with similar bit compression rates.\n\nStrengths: -\tThe proposed method decomposes the pretrained matrix into a quantizable fixed matrix and low-rank matrix that is already optimized before fine-tuning starts, which contributes to improved accuracy.\n-\tThe paper shows that LQ-LoRA can be used as a mixed quantization strategy, and also proposes a data-aware version of the algorithm, which enables users to flexibly set a target memory budget.\n-\tResults show that the proposed method can be generalized to different model families by showing outperforming results with RoBERTa and LLaMA.\n\nWeaknesses: - The authors have introduced a method that employs an iterative algorithm for initialization. Can they provide insights regarding the computational latency associated with their approach?\n\n- The authors assert the efficiency of LQ-LoRA based on empirical evidence, yet lack theoretical backing. To strengthen the credibility of the algorithm, a comparison might be beneficial, especially with methods that initialize the Q(W) + L1L2 matrix in a manner that closely mirrors the original pretrained matrix W. Consider, for instance, the use of GPTQ as a compensatory mechanism.\n\n- It appears that this paper serves as an expanded or refined rendition of the Q-LoRA paper. As such, it seemingly inherits the same limitation, notably the inference overhead, given that this approach must fail to integrate the LoRA layer into an existing linear layer. \n\n- Similarly, I would like to raise a query about the paper's novelty. While this method undeniably enhances the current approach (Q-LoRA), from a PEFT perspective, there could be superior methods, particularly concerning inference challenges. On the topic of novelty, I await the insights of fellow reviewers.\n\nQuestions: Included in the weakness.", "labeling_timestamp": "2026-01-11T16:26:57.675876", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors do not provide theoretical analysis or proofs addressing the reviewer’s request; instead they respond with extensive empirical results, runtime measurements, and practical arguments showing efficiency in practice. Thus they partially address the concern (with empirical justification) but do not supply the requested theoretical justification.", "evidence": "“This is a one-time cost at the initialization and takes a fraction of the total training time... To be more concrete, we measured the runtime to process three of the LLaMA-2 7B's matrices with `rank=64` and NF3 quantization. ... Time / Matrix: No: 1.3 to 2.1 seconds, Yes: 1.3 to 2.3 seconds.”; “These results indicate that LQ-LoRA can also be used as a state-of-the-art PTQ method.”; “We conducted the following experiment to be more concrete about NF vs INT. ... The table below shows that NF indeed performed better.”", "combined_author_response": "Thank you for the discussion! Please see our responses below.\n\n_1. The accuracy of LREC-4bit on MMLU almost matches that of LQ-LoRA_\n\nFor both LLaMA-2 7B and 70B models, LQ-LoRA (Fisher) with 3.25 bits can match the performance of LREC/GPTQ-LoRA 4-bits (i.e., 4.156-bits). Similarly, LQ-LoRA (Fisher) with 2.5/2.75 bits can match the performance of LREC/GPTQ-LoRA 3-bits (i.e., 3.148-bits). Please see the table below. \n\n| Method | Bits per param | MMLU (70B) | MMLU (7B) |\n| ----- | ----- | ----- | ----- |\n| GPTQ-LoRA 3-bit |  3.148 | 0.67$^\\dagger$ | 0.39 |\n| LQ-LoRA (Fisher) | 2.50 | 0.67 | 0.39 |\n| LQ-LoRA (Fisher) | 2.75 | 0.67 | 0.43 |\n\n| Method | Bits per param | MMLU (70B) | MMLU (7B) |\n| ----- | ----- | ----- | ----- |\n| LREC-4 bit | 4.156 | 0.69 | 0.45 |\n| LQ-LoRA (Fisher) | 3.25 | 0.69 | 0.46 |\n\n$^\\dagger$Note that we additionally filled in the MMLU 70B performance for GPTQ-LoRA 3-bit; this number was absent from the initial submission. Based on the above, we conclude that LQ-LoRA can meaningfully improve upon GPTQ-LoRA and QLoRA.\n\n_2. The authors compare their method with GPTQ-LoRA only_\n\nBesides GPTQ-LoRA, we also compared our method QLoRA and different variants of LQ-LoRA. In addition, our latest PTQ experiments included comparisons with RTN, GPTQ, AWQ, and OmniQuant. For details, please see \"Additional Post Training Quantization Experiments\" in our earlier response.\n\n_3. BRECQ and FlexRound_\n\nThanks for the suggestions! We think applying other quantization techniques is certainly interesting, but we would like to raise three points.\n\na) For quantizing large language models with 10B+ parameters, our understanding is that  NF quantization (from QLoRA) and GPTQ --- methods we compared with --- are near state-of-the-art quantization methods. (While we show numbers from other methods, such as Omniquant, in the PTQ comparison table above, these are, as far as we know, preprints).\n\nb) Applying fancier quantization techniques such as BRECQ/Adaround to large models is challenging. For example, for BRECQ,  the OmniQuant paper [1] notes,\n> ... BRECQ ... cannot be applied in models with billions of parameters because they are hard to optimize due to the huge solution space.\n\nSimilarly, the SpQR paper [2] stated that,\n\n> ... BRECQ ... were designed for vision models or small-scale language models, with less than 100M parameters.\n\n[1] https://arxiv.org/abs/2308.13137\n\n[2] https://arxiv.org/abs/2306.03078\n\nFlexRound is an interesting suggestion, but we were unable to find an open-source implementation of this. Hence, we focused our comparison on widely-used LLM quantization methods with open-source code, i.e., QLoRA and GPTQ-LoRA.\n\nc) Finally, (and most importantly), we note that LQ-LoRA can work with generic quantization approaches! For example, during each step of the iterative algorithm, instead of using NF-quantization, we can use other quantization methods to minimize $\\Vert \\mathbf{X}\\mathbf{W} - \\mathbf{X}(\\operatorname{quantize}(\\mathbf{W}) + \\mathbf{L}_1 \\mathbf{L}_2) \\Vert$. We focused on NF quantization because the quantization function is extremely quick (on the order of seconds), and it performs on par with GPTQ despite being data-agnostic. But we think it's worth highlighting this aspect of LQ-LoRA more, and we will discuss this further.\n\n---\n\nHi there! Please let us know if the above response answered some of your questions in the original review, and please let us know if you have follow-up questions!\n\nIn particular, we want to highlight a few key takeaways from our response:\n1. We included a baseline that uses OPTQ/GPTQ to quantize first before learning low-rank updates. We called this \"GPTQ-LoRA\" baseline \"LREC\" in the paper.\n2. We performed additional experiments using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. (Please see the above response for more details.)\n3. The techniques in this paper are not specific to NF. We chose NF because of its superior quality. To be more concrete about this, we conducted additional experiments. (Please see the above response for more details, too.)\n\n---\n\nHi there! Please let us know if the above response answered some of your questions in the original review, and please let us know if you have follow-up questions!\n\nIn particular, we want to highlight a few key takeaways from our response:\n1. The iterative algorithm adds a small extra computation head --- a fraction of the total training time.\n2. We included GPTQ-LoRA comparisons but referred to them as \"LREC\" in the paper.\n3. Inference with quantized + low-rank is not necessarily slower, especially in batch = 1 setting (per the latest releases from QLoRA; kudos to them).\n4. Practitioners could use the \"quantized + low-rank\" models without merging. Such models could be just as fast as merged dense models and more memory-efficient. Based on this potential, we performed additional experiments by using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset.\n\n---\n\nThanks for the comments --- please see our responses below.\n\n**Comparison with methods that use the data $X$ to quantize**\n\nThanks for the question! We actually did implement and compare against a baseline that uses OPTQ/GPTQ to quantize first before learning low-rank updates. In particular, we use GPTQ to first quantize the model and then learn low-rank updates. We called this \"GPTQ-LoRA\" baseline \"LREC\" in the paper following this paper (https://arxiv.org/pdf/2306.08162.pdf). We find that GPTQ-LoRA underperforms both QLoRA and LQ-LoRA (Table 1).\n\n(We now realize that this naming caused some confusion. Thanks for pointing that out --- we will clarify this in the paper.)\n\n\n**Additional Post Training Quantization Experiments**\n\nIn addition to the GPTQ-LoRA experiments already in the paper, we performed additional experiments using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. Specifically, we train the LQ-LoRA (Fisher, 2.75-bit, 64-rank) for one epoch on two C4 partitions and WikiText-2. We then further quantize the LoRA components themselves to 8-bit (NF8). Please see the Table below for the results, where the numbers other than LQ-LoRA are from OmniQuant [1]. Note that we use the term \"effective bits\" to denote bits per parameter that treat the LoRA components as \"overheads\" for comparing with methods without LoRA; LoRA components (with NF-8 quantization) amount to about 0.1-0.2 extra bits per parameter.\n\n\n\n| Base Model | Method                           | Effective Bits (7B, 70B) | C4 (7B) | C4 (70B) | WikiText2 (7B) | WikiText2 (70B) |\n| -------- | ---------------------------------- | ----------- | ---- | ---- | ---- | ---- |\n| LLaMA-2 | Dense                               | 16          | 6.97 | 5.52 | 5.47 | 3.31 |\n| LLaMA-2 | RTN                                 | 3.15        | 8.40 | 6.02 | 6.66 | 3.97 |\n| LLaMA-2 | GPTQ            | 3.15        | 7.89 | 5.85 | 6.29 | 3.85 |\n| LLaMA-2 | AWQ               | 3.15        | 7.84 | -    | 6.24 | -    |\n| LLaMA-2 | OmniQuant  | 3.15        | 7.75 | 5.85 | 6.03 | 3.78 |\n| LLaMA-2 | LQ-LoRA (Fisher)   | 2.95, 2.85 | 7.60 | 5.88 | 5.67 | 3.65 |\n\n\n\n\n[1] https://arxiv.org/abs/2308.13137\n\nThese results indicate that LQ-LoRA can also be used as a state-of-the-art PTQ method. Given the promise of LQ-LoRA as a potential alternative to PTQ, we will include this comparison against PTQ results in the next iteration of the paper.\n\n\n\n**NF-based vs INT-based Quantization**\n\nYou are correct in that the techniques in this paper are not specific to NF. QLoRA paper demonstrated the superior quality of such a scheme compared to classical integer quantization. Our preliminary experiments confirmed this, and hence, we chose NF.\n\nWe conducted the following experiment to be more concrete about NF vs INT. We ran LQ (i.e., Section 3.1) on LLaMA-2 7B's dense parameters with `rank=64` and NF3/INT3/NF4/INT4 quantization configuration (i.e., `b0=3/4, b1=8, b2=fp32, B0=64, B1=256`), and measure the reconstruction errors (i.e., $||\\mathbf{W} - (\\mathbf{Q} + \\mathbf{L}_1 \\mathbf{L}_2) ||_2^2$). The table below shows that NF indeed performed better.\n\n| Method      | Reconstruction error (unit: $10^4$) |\n| ----------- | ----------- |\n| LQ with NF3      | 7.12       |\n| LQ with INT3     | 11.6        |\n| LQ with NF4      | 1.42       |\n| LQ with INT4     | 2.02        |\n\n---\n\nThanks to the reviewer for the comments! Please see our responses below.\n\n**Latency of The Iterative Algorithm**\n\nThis is a one-time cost at the initialization and takes a fraction of the total training time. Each step of the iterative algorithm consists of SVD followed by quantization. Note that we use randomized SVD (rSVD) for speed consideration, and we run the algorithm for up to 100 steps (and we stop before this if the error starts to increase).\n\nTo be more concrete, we measured the runtime to process three of the LLaMA-2 7B's matrices with `rank=64` and NF3 quantization. Please see the table below.\n\n| Data-Aware (i.e., Section 3.3) | Time / Matrix |\n| ----- | ----- |\n| No  | 1.3 to 2.1 seconds |\n| Yes | 1.3 to 2.3 seconds |\n\n**Comparison with GPTQ-based Methods**\n\nThanks for the question! We actually did implement and compare against GPTQ-LoRA. We called this \"LREC\" in the paper following this paper (https://arxiv.org/pdf/2306.08162.pdf). We find that GPTQ-LoRA underperforms both QLoRA and LQ-LoRA (Table 1).\n\n(We now realize that this naming caused some confusion. Thanks for pointing that out --- we will clarify this in the paper.)\n\n**Limitations from QLoRA**\n\nThis is a very good point! But we want to point out that inference with quantized + low-rank is not necessarily slower, especially in batch = 1 setting. Smaller quantized matrices allow us to reduce data movement between SRAM and HBM of the GPU. For example, in the latest releases of `bitsandbytes` (the low-level engine behind QLoRA) [1], they claimed their matrix-vector multiplication between quantized matrix and dense vector (the one needed during inference) could be competitive and even outperformed dense matrix-vector multiplication. \n\n[1] https://github.com/TimDettmers/bitsandbytes/releases\n\n*Integrating LoRA Layers Into Linear Layers*\n\nGiven the potential inference speed-ups, we think that practitioners could use the \"quantized + low-rank\" models without merging. Such models could be just as fast (as discussed above) as merged dense models and more memory-efficient. In that regard, you could (loosely speaking) consider this as an alternative PTQ method.\n\nBased on this potential of LQ-LoRA as an alternative to PTQ, we performed additional experiments by using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. Specifically, we train the LQ-LoRA (Fisher, 2.75-bit, 64-rank) for one epoch on two C4 partitions and WikiText-2. We then further quantize the LoRA components themselves to 8-bit (NF8). Please see the Table below for the results, where the numbers other than LQ-LoRA are from OmniQuant [1]. Note that we use the term \"effective bits\" to denote bits per parameter that treat the LoRA components as \"overheads\" for comparing with methods without LoRA; LoRA components (with NF-8 quantization) amount to about 0.1-0.2 extra bits per parameter.\n\n| Base Model | Method                           | Effective Bits (7B, 70B) | C4 (7B) | C4 (70B) | WikiText2 (7B) | WikiText2 (70B) |\n| -------- | ---------------------------------- | ----------- | ---- | ---- | ---- | ---- |\n| LLaMA-2 | Dense                               | 16          | 6.97 | 5.52 | 5.47 | 3.31 |\n| LLaMA-2 | RTN                                 | 3.15        | 8.40 | 6.02 | 6.66 | 3.97 |\n| LLaMA-2 | GPTQ            | 3.15        | 7.89 | 5.85 | 6.29 | 3.85 |\n| LLaMA-2 | AWQ               | 3.15        | 7.84 | -    | 6.24 | -    |\n| LLaMA-2 | OmniQuant  | 3.15        | 7.75 | 5.85 | 6.03 | 3.78 |\n| LLaMA-2 | LQ-LoRA (Fisher)   | 2.95, 2.85 | 7.60 | 5.88 | 5.67 | 3.65 |\n\n[1] https://arxiv.org/abs/2308.13137\n\nThese results indicate that LQ-LoRA can also be used as a state-of-the-art PTQ method. Given the promise of LQ-LoRA as a potential alternative to PTQ, we will include this comparison against PTQ results in the next iteration of the paper.\n\n\n**Novelty**\n\n*Similarly, I would like to raise a query about the paper's novelty. While this method undeniably enhances the current approach (Q-LoRA), from a PEFT perspective, there could be superior methods, particularly concerning inference challenges. On the topic of novelty, I await the insights of fellow reviewers.*\n\nWe want to reiterate three main contributions on top of QLoRA:\n1. We adopt a matrix decomposition view of initializing the quantized and low-rank components of LoRA adaptation. (Note that QLoRA does not perform explicit matrix decomposition)\n2. Our ILP formulation enables us to search for the mixed-configuration quantization that fits specific resource requirements (e.g., 2.75 bits per parameter).\n3. We extended the two methods above to a data-aware setting by incorporating the sensitivities of parameters.\n\n---\n\nThanks for the helpful review! Please take a look at our responses below.\n\n**Novelty with respect to QLoRA**\n\nWe want to note that QLoRA does not perform explicit matrix decomposition. It performs quantization of the original weight matrix and learns additive low-rank updates. In contrast, we perform explicit matrix decomposition of the original matrix into low-rank and quantized components via a matrix reconstruction objective.\n\n**Ablations**\n\n*How important is the ILP to LQ-LoRA? Can you show the performance of LQ-LoRA without the ILP?*\n\nOne of the main usages of ILP is to flexibly quantize the model to meet specific memory constraints (e.g., 2.75 bits/parameter). That being said, LQ-LoRA is effective even without the ILP. For example, Table 4 shows LQ-LoRA without ILP. Here, we use NF3 quantization configuration --- NF3 reuses the configuration of the QLoRA's NF4 quantization with 3-bit first-level quantization. \n\n*Can you show the performance of the regular LoRA method (no quantization), and also quantization (at different bit-rates) without LoRA, in Table 2?*\n\nWe want to note that the QLoRA paper already demonstrated little performance loss between regular LoRA and QLoRA (4bit). Hence, we can think of QLoRA (4bit) performance as a proxy for regular LoRA.\n\nHowever, based on your suggestion/question, we performed additional experiments using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. Specifically, we train the LQ-LoRA (Fisher, 2.75-bit, 64-rank) for one epoch on two C4 partitions and WikiText-2. We then further quantize the LoRA components themselves to 8-bit (NF8). Please see the Table below for the results, where the numbers other than LQ-LoRA are from OmniQuant [1]. Note that we use the term \"effective bits\" to denote bits per parameter that treat the LoRA components as \"overheads\" for comparing with methods without LoRA; LoRA components (with NF-8 quantization) amount to about 0.1-0.2 extra bits per parameter.\n\n| Base Model | Method                           | Effective Bits (7B, 70B) | C4 (7B) | C4 (70B) | WikiText2 (7B) | WikiText2 (70B) |\n| -------- | ---------------------------------- | ----------- | ---- | ---- | ---- | ---- |\n| LLaMA-2 | Dense                               | 16          | 6.97 | 5.52 | 5.47 | 3.31 |\n| LLaMA-2 | RTN                                 | 3.15        | 8.40 | 6.02 | 6.66 | 3.97 |\n| LLaMA-2 | GPTQ            | 3.15        | 7.89 | 5.85 | 6.29 | 3.85 |\n| LLaMA-2 | AWQ               | 3.15        | 7.84 | -    | 6.24 | -    |\n| LLaMA-2 | OmniQuant  | 3.15        | 7.75 | 5.85 | 6.03 | 3.78 |\n| LLaMA-2 | LQ-LoRA (Fisher)   | 2.95, 2.85 | 7.60 | 5.88 | 5.67 | 3.65 |\n\n[1] https://arxiv.org/abs/2308.13137\n\nThese results indicate that LQ-LoRA can also be used as a state-of-the-art PTQ method. Given the promise of LQ-LoRA as a potential alternative to PTQ, we will include this comparison against PTQ results in the next iteration of the paper.\n\n**ILP for Rank Selection**\n\nThis is a very good/interesting suggestion! We have looked into this by ILP-searching the bits and ranks together. One practical challenge is measuring the \"storages/errors\"  of the ranks of low-rank components. This is important to trade off bits of the quantized matrices and ranks of LoRA. Notice that LoRA components will be fine-tuned; searching the ranks using errors at initialization will unnecessarily favor more bits at the cost of lower ranks. We might be able to formulate this as a bi-level optimization problem (first searching the ranks and then searching the bits), but we will leave this as future work.\n\n**Questions**\n\n*Is the only difference between QLoRA+ILP, and LQ-LoRA, the initialization?*\n\nYep! \n\n*Does the ILP budget, as well as the \"bits per param\" column, also consider the low-rank components?*\n\nThis table does not take into account low-rank components since we use rank = 64 for our QLoRA and LREC (GPTQ + LoRA) baselines. However, the PTQ table above **does** take into account the contribution from the low-rank components to ensure fair comparison against other PTQ methods. We find that the LoRA components (if quantized) add about ~0.2 bits. We will clarify this further!\n\n**Suggestions**\n\nThanks for the suggestions regarding the figures and tables! We will take these into account for the final version of the paper.\n\n---\n\nWe thank the reviewer for their comments/questions. Please find our responses below.\n\n**Mixed Precision & Hardware**\n\nWe note that our work is in the **weight only** quantization regime (as are QLoRA, GPTQ, etc.), where only the weights (and not the activations) are quantized to lower bits. The actual matmul is done in full precision after dequantization. \n\nConcretely, we use just-in-time de-quantize the (quantized) matrix $\\mathbf{Q}$, execute matrix operations (e.g., `matmul(X, Q) ==> matmul(X, dequantize(Q))`), and throw away the de-quantized matrix. This supports arbitrary quantization, and the main technical difficulty is how to do de-quantization quickly. We outlined more details in the appendix.\n\nAlthough weight-only quantization cannot make use of (faster and more energy-efficient) lower-precision matmuls, weight-only quantization still has two practical benefits. First, at a macro level, having a smaller model (due to quantization) allows us to increase the batch size. This reduces the need for expensive data/model parallelism, which requires cross-device or even cross-node communication. Second, at a micro level, smaller quantized matrices allow us to reduce data movement between SRAM and HBM of the GPU. As an example, QLoRA's matrix multiplication implementation could be faster than dense matrix multiplication [1].\n\n[1] https://github.com/TimDettmers/bitsandbytes/releases\n\n**Related paper on Joint Low-rank and Quantized Matrix Decomposition**\n\nThanks for the suggestion; this is an interesting paper! We will discuss this paper in the related works section.\n\nWe primarily considered the cases in which $\\mathbf{L}_1$ and $\\mathbf{L}_2$ are floating-point parameters. This is necessary because we want to fine-tune these parameters. That being said, the techniques introduced in that paper paper might be helpful for other (future) use cases. For example, we could post-training merge $\\mathbf{Q}, \\mathbf{L}_1, \\mathbf{L}_2$, and re-decompose them into three separate quantized matrices for inference.\n\n**Question 1**\n\nYou are correct -- we will clarify this; thanks!\n\n**Question 2**\n\nYou are correct in that the techniques in this paper are not specific to NF. QLoRA paper demonstrated the superior quality of such a scheme compared to classical integer quantization. Our preliminary experiments confirmed this, and hence, we chose NF.\n\nWe conducted the following experiment to be more concrete about NF vs INT. We ran LQ (i.e., Section 3.1) on LLaMA-2 7B's dense parameters with `rank=64` and NF3/INT3/NF4/INT4 quantization configuration (i.e., `b0=3/4, b1=8, b2=fp32, B0=64, B1=256`), and measure the reconstruction errors (i.e., $||\\mathbf{W} - (\\mathbf{Q} + \\mathbf{L}_1 \\mathbf{L}_2) ||_2^2$). The table below shows that NF indeed performed better.\n\n| Method      | Reconstruction error (unit: $10^4$) |\n| ----------- | ----------- |\n| LQ with NF3      | 7.12       |\n| LQ with INT3     | 11.6        |\n| LQ with NF4      | 1.42       |\n| LQ with INT4     | 2.02        |\n\n**Question 3**\n\nThis is the same $\\delta$ in QLoRA, and we treated it as a constant for all settings. We will clarify further in the next iteration of the paper.\n\n**Question 4**\n\nWe _defined_ $\\mathbb{Q}$ as the set of matrices that can be quantized (Equation 1 is a minimization problem, not an exact decomposition problem). Thanks for pointing out the confusion; we will clarify this.\n\n**Question 5**\n\nThe middle figure represents the setting of many (Q)LoRA methods, in which the Low-Rank components are initialized to be zero. In that sense, $\\operatorname{quantize}(\\mathbf{W}) + \\mathbf{L}_1 \\mathbf{L}_2 = \\operatorname{quantize}(\\mathbf{W})$ for those methods.\n\n**Question 6**\n\nGood catch -- it's meant for \"budget\" but shared similar symbols with $B_0$ and $B_1$.\n\n**Question 7**\n\nWe say a matrix $\\mathbf{F}$ has homogenous rows or columns when either rows or columns of the matrix have identical values. We will clarify this.\n\n**Question 8**\n\nTable 4 used the quantization configuration of NF3/NF4. Specifically, they use the block sizes and second-level quantization as the original NF4, but with the first-level quantization set to 3/4-bit. Table 2, however, used ILP to search for the quantization configuration, hence the difference.\n\nWe made such decisions for Table 4 to remove the possible confounding variable of using ILP to search for the quantization configuration (i.e., we just wanted to see the effect of higher rank LoRA components). We will clarify!", "author_response": "Thanks to the reviewer for the comments! Please see our responses below.\n\n**Latency of The Iterative Algorithm**\n\nThis is a one-time cost at the initialization and takes a fraction of the total training time. Each step of the iterative algorithm consists of SVD followed by quantization. Note that we use randomized SVD (rSVD) for speed consideration, and we run the algorithm for up to 100 steps (and we stop before this if the error starts to increase).\n\nTo be more concrete, we measured the runtime to process three of the LLaMA-2 7B's matrices with `rank=64` and NF3 quantization. Please see the table below.\n\n| Data-Aware (i.e., Section 3.3) | Time / Matrix |\n| ----- | ----- |\n| No  | 1.3 to 2.1 seconds |\n| Yes | 1.3 to 2.3 seconds |\n\n**Comparison with GPTQ-based Methods**\n\nThanks for the question! We actually did implement and compare against GPTQ-LoRA. We called this \"LREC\" in the paper following this paper (https://arxiv.org/pdf/2306.08162.pdf). We find that GPTQ-LoRA underperforms both QLoRA and LQ-LoRA (Table 1).\n\n(We now realize that this naming caused some confusion. Thanks for pointing that out --- we will clarify this in the paper.)\n\n**Limitations from QLoRA**\n\nThis is a very good point! But we want to point out that inference with quantized + low-rank is not necessarily slower, especially in batch = 1 setting. Smaller quantized matrices allow us to reduce data movement between SRAM and HBM of the GPU. For example, in the latest releases of `bitsandbytes` (the low-level engine behind QLoRA) [1], they claimed their matrix-vector multiplication between quantized matrix and dense vector (the one needed during inference) could be competitive and even outperformed dense matrix-vector multiplication. \n\n[1] https://github.com/TimDettmers/bitsandbytes/releases\n\n*Integrating LoRA Layers Into Linear Layers*\n\nGiven the potential inference speed-ups, we think that practitioners could use the \"quantized + low-rank\" models without merging. Such models could be just as fast (as discussed above) as merged dense models and more memory-efficient. In that regard, you could (loosely speaking) consider this as an alternative PTQ method.\n\nBased on this potential of LQ-LoRA as an alternative to PTQ, we performed additional experiments by using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. Specifically, we train the LQ-LoRA (Fisher, 2.75-bit, 64-rank) for one epoch on two C4 partitions and WikiText-2. We then further quantize the LoRA components themselves to 8-bit (NF8). Please see the Table below for the results, where the numbers other than LQ-LoRA are from OmniQuant [1]. Note that we use the term \"effective bits\" to denote bits per parameter that treat the LoRA components as \"overheads\" for comparing with methods without LoRA; LoRA components (with NF-8 quantization) amount to about 0.1-0.2 extra bits per parameter.\n\n| Base Model | Method                           | Effective Bits (7B, 70B) | C4 (7B) | C4 (70B) | WikiText2 (7B) | WikiText2 (70B) |\n| -------- | ---------------------------------- | ----------- | ---- | ---- | ---- | ---- |\n| LLaMA-2 | Dense                               | 16          | 6.97 | 5.52 | 5.47 | 3.31 |\n| LLaMA-2 | RTN                                 | 3.15        | 8.40 | 6.02 | 6.66 | 3.97 |\n| LLaMA-2 | GPTQ            | 3.15        | 7.89 | 5.85 | 6.29 | 3.85 |\n| LLaMA-2 | AWQ               | 3.15        | 7.84 | -    | 6.24 | -    |\n| LLaMA-2 | OmniQuant  | 3.15        | 7.75 | 5.85 | 6.03 | 3.78 |\n| LLaMA-2 | LQ-LoRA (Fisher)   | 2.95, 2.85 | 7.60 | 5.88 | 5.67 | 3.65 |\n\n[1] https://arxiv.org/abs/2308.13137\n\nThese results indicate that LQ-LoRA can also be used as a state-of-the-art PTQ method. Given the promise of LQ-LoRA as a potential alternative to PTQ, we will include this comparison against PTQ results in the next iteration of the paper.\n\n\n**Novelty**\n\n*Similarly, I would like to raise a query about the paper's novelty. While this method undeniably enhances the current approach (Q-LoRA), from a PEFT perspective, there could be superior methods, particularly concerning inference challenges. On the topic of novelty, I await the insights of fellow reviewers.*\n\nWe want to reiterate three main contributions on top of QLoRA:\n1. We adopt a matrix decomposition view of initializing the quantized and low-rank components of LoRA adaptation. (Note that QLoRA does not perform explicit matrix decomposition)\n2. Our ILP formulation enables us to search for the mixed-configuration quantization that fits specific resource requirements (e.g., 2.75 bits per parameter).\n3. We extended the two methods above to a data-aware setting by incorporating the sensitivities of parameters."}
{"claim": "The paper lacks comparisons to alternative initialization methods that set Q(W)+L1L2 to closely approximate the original pretrained matrix W.", "claim_type": "baseline", "paper_id": "xw29VvOMmU", "paper_title": "LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Bm88GSraZd", "reviewer": "Reviewer_CSBT", "review_text": "Summary: This paper proposes LQ-LoRA, a memory-efficient LLM adaptation method that decomposes each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. The algorithm is adapted from QLoRA and applied modification to solve the problem that zero initialization of the low-rank matrix may not be optimal when the fixed matrix is quantized. The method decomposes the matrix by an iterative algorithm and updates only the low-rank matrix weights during fine-tuning. Results showed that the proposed method outperforms QLoRA and LREC with similar bit compression rates.\n\nStrengths: -\tThe proposed method decomposes the pretrained matrix into a quantizable fixed matrix and low-rank matrix that is already optimized before fine-tuning starts, which contributes to improved accuracy.\n-\tThe paper shows that LQ-LoRA can be used as a mixed quantization strategy, and also proposes a data-aware version of the algorithm, which enables users to flexibly set a target memory budget.\n-\tResults show that the proposed method can be generalized to different model families by showing outperforming results with RoBERTa and LLaMA.\n\nWeaknesses: - The authors have introduced a method that employs an iterative algorithm for initialization. Can they provide insights regarding the computational latency associated with their approach?\n\n- The authors assert the efficiency of LQ-LoRA based on empirical evidence, yet lack theoretical backing. To strengthen the credibility of the algorithm, a comparison might be beneficial, especially with methods that initialize the Q(W) + L1L2 matrix in a manner that closely mirrors the original pretrained matrix W. Consider, for instance, the use of GPTQ as a compensatory mechanism.\n\n- It appears that this paper serves as an expanded or refined rendition of the Q-LoRA paper. As such, it seemingly inherits the same limitation, notably the inference overhead, given that this approach must fail to integrate the LoRA layer into an existing linear layer. \n\n- Similarly, I would like to raise a query about the paper's novelty. While this method undeniably enhances the current approach (Q-LoRA), from a PEFT perspective, there could be superior methods, particularly concerning inference challenges. On the topic of novelty, I await the insights of fellow reviewers.\n\nQuestions: Included in the weakness.", "labeling_timestamp": "2026-01-11T16:27:11.191190", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The authors state they implemented and evaluated a quantize-then-LoRA baseline (GPTQ/OPTQ → LoRA, called GPTQ-LoRA/LREC) and report additional PTQ comparisons (RTN, GPTQ, AWQ, OmniQuant), with tables showing results—directly countering the claim that such comparisons were missing. They also explain compatibility with other quantization schemes and why some specialized methods (BRECQ/FlexRound) were not included.", "evidence": "\"We actually did implement and compare against a baseline that uses OPTQ/GPTQ to quantize first before learning low-rank updates. In particular, we use GPTQ to first quantize the model and then learn low-rank updates. We called this 'GPTQ-LoRA' baseline 'LREC' in the paper... We find that GPTQ-LoRA underperforms both QLoRA and LQ-LoRA (Table 1).\"; \"In addition, our latest PTQ experiments included comparisons with RTN, GPTQ, AWQ, and OmniQuant. For details, please see 'Additional Post Training Quantization Experiments' in our earlier response.\"", "combined_author_response": "Thank you for the discussion! Please see our responses below.\n\n_1. The accuracy of LREC-4bit on MMLU almost matches that of LQ-LoRA_\n\nFor both LLaMA-2 7B and 70B models, LQ-LoRA (Fisher) with 3.25 bits can match the performance of LREC/GPTQ-LoRA 4-bits (i.e., 4.156-bits). Similarly, LQ-LoRA (Fisher) with 2.5/2.75 bits can match the performance of LREC/GPTQ-LoRA 3-bits (i.e., 3.148-bits). Please see the table below. \n\n| Method | Bits per param | MMLU (70B) | MMLU (7B) |\n| ----- | ----- | ----- | ----- |\n| GPTQ-LoRA 3-bit |  3.148 | 0.67$^\\dagger$ | 0.39 |\n| LQ-LoRA (Fisher) | 2.50 | 0.67 | 0.39 |\n| LQ-LoRA (Fisher) | 2.75 | 0.67 | 0.43 |\n\n| Method | Bits per param | MMLU (70B) | MMLU (7B) |\n| ----- | ----- | ----- | ----- |\n| LREC-4 bit | 4.156 | 0.69 | 0.45 |\n| LQ-LoRA (Fisher) | 3.25 | 0.69 | 0.46 |\n\n$^\\dagger$Note that we additionally filled in the MMLU 70B performance for GPTQ-LoRA 3-bit; this number was absent from the initial submission. Based on the above, we conclude that LQ-LoRA can meaningfully improve upon GPTQ-LoRA and QLoRA.\n\n_2. The authors compare their method with GPTQ-LoRA only_\n\nBesides GPTQ-LoRA, we also compared our method QLoRA and different variants of LQ-LoRA. In addition, our latest PTQ experiments included comparisons with RTN, GPTQ, AWQ, and OmniQuant. For details, please see \"Additional Post Training Quantization Experiments\" in our earlier response.\n\n_3. BRECQ and FlexRound_\n\nThanks for the suggestions! We think applying other quantization techniques is certainly interesting, but we would like to raise three points.\n\na) For quantizing large language models with 10B+ parameters, our understanding is that  NF quantization (from QLoRA) and GPTQ --- methods we compared with --- are near state-of-the-art quantization methods. (While we show numbers from other methods, such as Omniquant, in the PTQ comparison table above, these are, as far as we know, preprints).\n\nb) Applying fancier quantization techniques such as BRECQ/Adaround to large models is challenging. For example, for BRECQ,  the OmniQuant paper [1] notes,\n> ... BRECQ ... cannot be applied in models with billions of parameters because they are hard to optimize due to the huge solution space.\n\nSimilarly, the SpQR paper [2] stated that,\n\n> ... BRECQ ... were designed for vision models or small-scale language models, with less than 100M parameters.\n\n[1] https://arxiv.org/abs/2308.13137\n\n[2] https://arxiv.org/abs/2306.03078\n\nFlexRound is an interesting suggestion, but we were unable to find an open-source implementation of this. Hence, we focused our comparison on widely-used LLM quantization methods with open-source code, i.e., QLoRA and GPTQ-LoRA.\n\nc) Finally, (and most importantly), we note that LQ-LoRA can work with generic quantization approaches! For example, during each step of the iterative algorithm, instead of using NF-quantization, we can use other quantization methods to minimize $\\Vert \\mathbf{X}\\mathbf{W} - \\mathbf{X}(\\operatorname{quantize}(\\mathbf{W}) + \\mathbf{L}_1 \\mathbf{L}_2) \\Vert$. We focused on NF quantization because the quantization function is extremely quick (on the order of seconds), and it performs on par with GPTQ despite being data-agnostic. But we think it's worth highlighting this aspect of LQ-LoRA more, and we will discuss this further.\n\n---\n\nHi there! Please let us know if the above response answered some of your questions in the original review, and please let us know if you have follow-up questions!\n\nIn particular, we want to highlight a few key takeaways from our response:\n1. We included a baseline that uses OPTQ/GPTQ to quantize first before learning low-rank updates. We called this \"GPTQ-LoRA\" baseline \"LREC\" in the paper.\n2. We performed additional experiments using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. (Please see the above response for more details.)\n3. The techniques in this paper are not specific to NF. We chose NF because of its superior quality. To be more concrete about this, we conducted additional experiments. (Please see the above response for more details, too.)\n\n---\n\nHi there! Please let us know if the above response answered some of your questions in the original review, and please let us know if you have follow-up questions!\n\nIn particular, we want to highlight a few key takeaways from our response:\n1. The iterative algorithm adds a small extra computation head --- a fraction of the total training time.\n2. We included GPTQ-LoRA comparisons but referred to them as \"LREC\" in the paper.\n3. Inference with quantized + low-rank is not necessarily slower, especially in batch = 1 setting (per the latest releases from QLoRA; kudos to them).\n4. Practitioners could use the \"quantized + low-rank\" models without merging. Such models could be just as fast as merged dense models and more memory-efficient. Based on this potential, we performed additional experiments by using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset.\n\n---\n\nThanks for the comments --- please see our responses below.\n\n**Comparison with methods that use the data $X$ to quantize**\n\nThanks for the question! We actually did implement and compare against a baseline that uses OPTQ/GPTQ to quantize first before learning low-rank updates. In particular, we use GPTQ to first quantize the model and then learn low-rank updates. We called this \"GPTQ-LoRA\" baseline \"LREC\" in the paper following this paper (https://arxiv.org/pdf/2306.08162.pdf). We find that GPTQ-LoRA underperforms both QLoRA and LQ-LoRA (Table 1).\n\n(We now realize that this naming caused some confusion. Thanks for pointing that out --- we will clarify this in the paper.)\n\n\n**Additional Post Training Quantization Experiments**\n\nIn addition to the GPTQ-LoRA experiments already in the paper, we performed additional experiments using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. Specifically, we train the LQ-LoRA (Fisher, 2.75-bit, 64-rank) for one epoch on two C4 partitions and WikiText-2. We then further quantize the LoRA components themselves to 8-bit (NF8). Please see the Table below for the results, where the numbers other than LQ-LoRA are from OmniQuant [1]. Note that we use the term \"effective bits\" to denote bits per parameter that treat the LoRA components as \"overheads\" for comparing with methods without LoRA; LoRA components (with NF-8 quantization) amount to about 0.1-0.2 extra bits per parameter.\n\n\n\n| Base Model | Method                           | Effective Bits (7B, 70B) | C4 (7B) | C4 (70B) | WikiText2 (7B) | WikiText2 (70B) |\n| -------- | ---------------------------------- | ----------- | ---- | ---- | ---- | ---- |\n| LLaMA-2 | Dense                               | 16          | 6.97 | 5.52 | 5.47 | 3.31 |\n| LLaMA-2 | RTN                                 | 3.15        | 8.40 | 6.02 | 6.66 | 3.97 |\n| LLaMA-2 | GPTQ            | 3.15        | 7.89 | 5.85 | 6.29 | 3.85 |\n| LLaMA-2 | AWQ               | 3.15        | 7.84 | -    | 6.24 | -    |\n| LLaMA-2 | OmniQuant  | 3.15        | 7.75 | 5.85 | 6.03 | 3.78 |\n| LLaMA-2 | LQ-LoRA (Fisher)   | 2.95, 2.85 | 7.60 | 5.88 | 5.67 | 3.65 |\n\n\n\n\n[1] https://arxiv.org/abs/2308.13137\n\nThese results indicate that LQ-LoRA can also be used as a state-of-the-art PTQ method. Given the promise of LQ-LoRA as a potential alternative to PTQ, we will include this comparison against PTQ results in the next iteration of the paper.\n\n\n\n**NF-based vs INT-based Quantization**\n\nYou are correct in that the techniques in this paper are not specific to NF. QLoRA paper demonstrated the superior quality of such a scheme compared to classical integer quantization. Our preliminary experiments confirmed this, and hence, we chose NF.\n\nWe conducted the following experiment to be more concrete about NF vs INT. We ran LQ (i.e., Section 3.1) on LLaMA-2 7B's dense parameters with `rank=64` and NF3/INT3/NF4/INT4 quantization configuration (i.e., `b0=3/4, b1=8, b2=fp32, B0=64, B1=256`), and measure the reconstruction errors (i.e., $||\\mathbf{W} - (\\mathbf{Q} + \\mathbf{L}_1 \\mathbf{L}_2) ||_2^2$). The table below shows that NF indeed performed better.\n\n| Method      | Reconstruction error (unit: $10^4$) |\n| ----------- | ----------- |\n| LQ with NF3      | 7.12       |\n| LQ with INT3     | 11.6        |\n| LQ with NF4      | 1.42       |\n| LQ with INT4     | 2.02        |\n\n---\n\nThanks to the reviewer for the comments! Please see our responses below.\n\n**Latency of The Iterative Algorithm**\n\nThis is a one-time cost at the initialization and takes a fraction of the total training time. Each step of the iterative algorithm consists of SVD followed by quantization. Note that we use randomized SVD (rSVD) for speed consideration, and we run the algorithm for up to 100 steps (and we stop before this if the error starts to increase).\n\nTo be more concrete, we measured the runtime to process three of the LLaMA-2 7B's matrices with `rank=64` and NF3 quantization. Please see the table below.\n\n| Data-Aware (i.e., Section 3.3) | Time / Matrix |\n| ----- | ----- |\n| No  | 1.3 to 2.1 seconds |\n| Yes | 1.3 to 2.3 seconds |\n\n**Comparison with GPTQ-based Methods**\n\nThanks for the question! We actually did implement and compare against GPTQ-LoRA. We called this \"LREC\" in the paper following this paper (https://arxiv.org/pdf/2306.08162.pdf). We find that GPTQ-LoRA underperforms both QLoRA and LQ-LoRA (Table 1).\n\n(We now realize that this naming caused some confusion. Thanks for pointing that out --- we will clarify this in the paper.)\n\n**Limitations from QLoRA**\n\nThis is a very good point! But we want to point out that inference with quantized + low-rank is not necessarily slower, especially in batch = 1 setting. Smaller quantized matrices allow us to reduce data movement between SRAM and HBM of the GPU. For example, in the latest releases of `bitsandbytes` (the low-level engine behind QLoRA) [1], they claimed their matrix-vector multiplication between quantized matrix and dense vector (the one needed during inference) could be competitive and even outperformed dense matrix-vector multiplication. \n\n[1] https://github.com/TimDettmers/bitsandbytes/releases\n\n*Integrating LoRA Layers Into Linear Layers*\n\nGiven the potential inference speed-ups, we think that practitioners could use the \"quantized + low-rank\" models without merging. Such models could be just as fast (as discussed above) as merged dense models and more memory-efficient. In that regard, you could (loosely speaking) consider this as an alternative PTQ method.\n\nBased on this potential of LQ-LoRA as an alternative to PTQ, we performed additional experiments by using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. Specifically, we train the LQ-LoRA (Fisher, 2.75-bit, 64-rank) for one epoch on two C4 partitions and WikiText-2. We then further quantize the LoRA components themselves to 8-bit (NF8). Please see the Table below for the results, where the numbers other than LQ-LoRA are from OmniQuant [1]. Note that we use the term \"effective bits\" to denote bits per parameter that treat the LoRA components as \"overheads\" for comparing with methods without LoRA; LoRA components (with NF-8 quantization) amount to about 0.1-0.2 extra bits per parameter.\n\n| Base Model | Method                           | Effective Bits (7B, 70B) | C4 (7B) | C4 (70B) | WikiText2 (7B) | WikiText2 (70B) |\n| -------- | ---------------------------------- | ----------- | ---- | ---- | ---- | ---- |\n| LLaMA-2 | Dense                               | 16          | 6.97 | 5.52 | 5.47 | 3.31 |\n| LLaMA-2 | RTN                                 | 3.15        | 8.40 | 6.02 | 6.66 | 3.97 |\n| LLaMA-2 | GPTQ            | 3.15        | 7.89 | 5.85 | 6.29 | 3.85 |\n| LLaMA-2 | AWQ               | 3.15        | 7.84 | -    | 6.24 | -    |\n| LLaMA-2 | OmniQuant  | 3.15        | 7.75 | 5.85 | 6.03 | 3.78 |\n| LLaMA-2 | LQ-LoRA (Fisher)   | 2.95, 2.85 | 7.60 | 5.88 | 5.67 | 3.65 |\n\n[1] https://arxiv.org/abs/2308.13137\n\nThese results indicate that LQ-LoRA can also be used as a state-of-the-art PTQ method. Given the promise of LQ-LoRA as a potential alternative to PTQ, we will include this comparison against PTQ results in the next iteration of the paper.\n\n\n**Novelty**\n\n*Similarly, I would like to raise a query about the paper's novelty. While this method undeniably enhances the current approach (Q-LoRA), from a PEFT perspective, there could be superior methods, particularly concerning inference challenges. On the topic of novelty, I await the insights of fellow reviewers.*\n\nWe want to reiterate three main contributions on top of QLoRA:\n1. We adopt a matrix decomposition view of initializing the quantized and low-rank components of LoRA adaptation. (Note that QLoRA does not perform explicit matrix decomposition)\n2. Our ILP formulation enables us to search for the mixed-configuration quantization that fits specific resource requirements (e.g., 2.75 bits per parameter).\n3. We extended the two methods above to a data-aware setting by incorporating the sensitivities of parameters.\n\n---\n\nThanks for the helpful review! Please take a look at our responses below.\n\n**Novelty with respect to QLoRA**\n\nWe want to note that QLoRA does not perform explicit matrix decomposition. It performs quantization of the original weight matrix and learns additive low-rank updates. In contrast, we perform explicit matrix decomposition of the original matrix into low-rank and quantized components via a matrix reconstruction objective.\n\n**Ablations**\n\n*How important is the ILP to LQ-LoRA? Can you show the performance of LQ-LoRA without the ILP?*\n\nOne of the main usages of ILP is to flexibly quantize the model to meet specific memory constraints (e.g., 2.75 bits/parameter). That being said, LQ-LoRA is effective even without the ILP. For example, Table 4 shows LQ-LoRA without ILP. Here, we use NF3 quantization configuration --- NF3 reuses the configuration of the QLoRA's NF4 quantization with 3-bit first-level quantization. \n\n*Can you show the performance of the regular LoRA method (no quantization), and also quantization (at different bit-rates) without LoRA, in Table 2?*\n\nWe want to note that the QLoRA paper already demonstrated little performance loss between regular LoRA and QLoRA (4bit). Hence, we can think of QLoRA (4bit) performance as a proxy for regular LoRA.\n\nHowever, based on your suggestion/question, we performed additional experiments using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. Specifically, we train the LQ-LoRA (Fisher, 2.75-bit, 64-rank) for one epoch on two C4 partitions and WikiText-2. We then further quantize the LoRA components themselves to 8-bit (NF8). Please see the Table below for the results, where the numbers other than LQ-LoRA are from OmniQuant [1]. Note that we use the term \"effective bits\" to denote bits per parameter that treat the LoRA components as \"overheads\" for comparing with methods without LoRA; LoRA components (with NF-8 quantization) amount to about 0.1-0.2 extra bits per parameter.\n\n| Base Model | Method                           | Effective Bits (7B, 70B) | C4 (7B) | C4 (70B) | WikiText2 (7B) | WikiText2 (70B) |\n| -------- | ---------------------------------- | ----------- | ---- | ---- | ---- | ---- |\n| LLaMA-2 | Dense                               | 16          | 6.97 | 5.52 | 5.47 | 3.31 |\n| LLaMA-2 | RTN                                 | 3.15        | 8.40 | 6.02 | 6.66 | 3.97 |\n| LLaMA-2 | GPTQ            | 3.15        | 7.89 | 5.85 | 6.29 | 3.85 |\n| LLaMA-2 | AWQ               | 3.15        | 7.84 | -    | 6.24 | -    |\n| LLaMA-2 | OmniQuant  | 3.15        | 7.75 | 5.85 | 6.03 | 3.78 |\n| LLaMA-2 | LQ-LoRA (Fisher)   | 2.95, 2.85 | 7.60 | 5.88 | 5.67 | 3.65 |\n\n[1] https://arxiv.org/abs/2308.13137\n\nThese results indicate that LQ-LoRA can also be used as a state-of-the-art PTQ method. Given the promise of LQ-LoRA as a potential alternative to PTQ, we will include this comparison against PTQ results in the next iteration of the paper.\n\n**ILP for Rank Selection**\n\nThis is a very good/interesting suggestion! We have looked into this by ILP-searching the bits and ranks together. One practical challenge is measuring the \"storages/errors\"  of the ranks of low-rank components. This is important to trade off bits of the quantized matrices and ranks of LoRA. Notice that LoRA components will be fine-tuned; searching the ranks using errors at initialization will unnecessarily favor more bits at the cost of lower ranks. We might be able to formulate this as a bi-level optimization problem (first searching the ranks and then searching the bits), but we will leave this as future work.\n\n**Questions**\n\n*Is the only difference between QLoRA+ILP, and LQ-LoRA, the initialization?*\n\nYep! \n\n*Does the ILP budget, as well as the \"bits per param\" column, also consider the low-rank components?*\n\nThis table does not take into account low-rank components since we use rank = 64 for our QLoRA and LREC (GPTQ + LoRA) baselines. However, the PTQ table above **does** take into account the contribution from the low-rank components to ensure fair comparison against other PTQ methods. We find that the LoRA components (if quantized) add about ~0.2 bits. We will clarify this further!\n\n**Suggestions**\n\nThanks for the suggestions regarding the figures and tables! We will take these into account for the final version of the paper.\n\n---\n\nWe thank the reviewer for their comments/questions. Please find our responses below.\n\n**Mixed Precision & Hardware**\n\nWe note that our work is in the **weight only** quantization regime (as are QLoRA, GPTQ, etc.), where only the weights (and not the activations) are quantized to lower bits. The actual matmul is done in full precision after dequantization. \n\nConcretely, we use just-in-time de-quantize the (quantized) matrix $\\mathbf{Q}$, execute matrix operations (e.g., `matmul(X, Q) ==> matmul(X, dequantize(Q))`), and throw away the de-quantized matrix. This supports arbitrary quantization, and the main technical difficulty is how to do de-quantization quickly. We outlined more details in the appendix.\n\nAlthough weight-only quantization cannot make use of (faster and more energy-efficient) lower-precision matmuls, weight-only quantization still has two practical benefits. First, at a macro level, having a smaller model (due to quantization) allows us to increase the batch size. This reduces the need for expensive data/model parallelism, which requires cross-device or even cross-node communication. Second, at a micro level, smaller quantized matrices allow us to reduce data movement between SRAM and HBM of the GPU. As an example, QLoRA's matrix multiplication implementation could be faster than dense matrix multiplication [1].\n\n[1] https://github.com/TimDettmers/bitsandbytes/releases\n\n**Related paper on Joint Low-rank and Quantized Matrix Decomposition**\n\nThanks for the suggestion; this is an interesting paper! We will discuss this paper in the related works section.\n\nWe primarily considered the cases in which $\\mathbf{L}_1$ and $\\mathbf{L}_2$ are floating-point parameters. This is necessary because we want to fine-tune these parameters. That being said, the techniques introduced in that paper paper might be helpful for other (future) use cases. For example, we could post-training merge $\\mathbf{Q}, \\mathbf{L}_1, \\mathbf{L}_2$, and re-decompose them into three separate quantized matrices for inference.\n\n**Question 1**\n\nYou are correct -- we will clarify this; thanks!\n\n**Question 2**\n\nYou are correct in that the techniques in this paper are not specific to NF. QLoRA paper demonstrated the superior quality of such a scheme compared to classical integer quantization. Our preliminary experiments confirmed this, and hence, we chose NF.\n\nWe conducted the following experiment to be more concrete about NF vs INT. We ran LQ (i.e., Section 3.1) on LLaMA-2 7B's dense parameters with `rank=64` and NF3/INT3/NF4/INT4 quantization configuration (i.e., `b0=3/4, b1=8, b2=fp32, B0=64, B1=256`), and measure the reconstruction errors (i.e., $||\\mathbf{W} - (\\mathbf{Q} + \\mathbf{L}_1 \\mathbf{L}_2) ||_2^2$). The table below shows that NF indeed performed better.\n\n| Method      | Reconstruction error (unit: $10^4$) |\n| ----------- | ----------- |\n| LQ with NF3      | 7.12       |\n| LQ with INT3     | 11.6        |\n| LQ with NF4      | 1.42       |\n| LQ with INT4     | 2.02        |\n\n**Question 3**\n\nThis is the same $\\delta$ in QLoRA, and we treated it as a constant for all settings. We will clarify further in the next iteration of the paper.\n\n**Question 4**\n\nWe _defined_ $\\mathbb{Q}$ as the set of matrices that can be quantized (Equation 1 is a minimization problem, not an exact decomposition problem). Thanks for pointing out the confusion; we will clarify this.\n\n**Question 5**\n\nThe middle figure represents the setting of many (Q)LoRA methods, in which the Low-Rank components are initialized to be zero. In that sense, $\\operatorname{quantize}(\\mathbf{W}) + \\mathbf{L}_1 \\mathbf{L}_2 = \\operatorname{quantize}(\\mathbf{W})$ for those methods.\n\n**Question 6**\n\nGood catch -- it's meant for \"budget\" but shared similar symbols with $B_0$ and $B_1$.\n\n**Question 7**\n\nWe say a matrix $\\mathbf{F}$ has homogenous rows or columns when either rows or columns of the matrix have identical values. We will clarify this.\n\n**Question 8**\n\nTable 4 used the quantization configuration of NF3/NF4. Specifically, they use the block sizes and second-level quantization as the original NF4, but with the first-level quantization set to 3/4-bit. Table 2, however, used ILP to search for the quantization configuration, hence the difference.\n\nWe made such decisions for Table 4 to remove the possible confounding variable of using ILP to search for the quantization configuration (i.e., we just wanted to see the effect of higher rank LoRA components). We will clarify!", "author_response": "Thanks to the reviewer for the comments! Please see our responses below.\n\n**Latency of The Iterative Algorithm**\n\nThis is a one-time cost at the initialization and takes a fraction of the total training time. Each step of the iterative algorithm consists of SVD followed by quantization. Note that we use randomized SVD (rSVD) for speed consideration, and we run the algorithm for up to 100 steps (and we stop before this if the error starts to increase).\n\nTo be more concrete, we measured the runtime to process three of the LLaMA-2 7B's matrices with `rank=64` and NF3 quantization. Please see the table below.\n\n| Data-Aware (i.e., Section 3.3) | Time / Matrix |\n| ----- | ----- |\n| No  | 1.3 to 2.1 seconds |\n| Yes | 1.3 to 2.3 seconds |\n\n**Comparison with GPTQ-based Methods**\n\nThanks for the question! We actually did implement and compare against GPTQ-LoRA. We called this \"LREC\" in the paper following this paper (https://arxiv.org/pdf/2306.08162.pdf). We find that GPTQ-LoRA underperforms both QLoRA and LQ-LoRA (Table 1).\n\n(We now realize that this naming caused some confusion. Thanks for pointing that out --- we will clarify this in the paper.)\n\n**Limitations from QLoRA**\n\nThis is a very good point! But we want to point out that inference with quantized + low-rank is not necessarily slower, especially in batch = 1 setting. Smaller quantized matrices allow us to reduce data movement between SRAM and HBM of the GPU. For example, in the latest releases of `bitsandbytes` (the low-level engine behind QLoRA) [1], they claimed their matrix-vector multiplication between quantized matrix and dense vector (the one needed during inference) could be competitive and even outperformed dense matrix-vector multiplication. \n\n[1] https://github.com/TimDettmers/bitsandbytes/releases\n\n*Integrating LoRA Layers Into Linear Layers*\n\nGiven the potential inference speed-ups, we think that practitioners could use the \"quantized + low-rank\" models without merging. Such models could be just as fast (as discussed above) as merged dense models and more memory-efficient. In that regard, you could (loosely speaking) consider this as an alternative PTQ method.\n\nBased on this potential of LQ-LoRA as an alternative to PTQ, we performed additional experiments by using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. Specifically, we train the LQ-LoRA (Fisher, 2.75-bit, 64-rank) for one epoch on two C4 partitions and WikiText-2. We then further quantize the LoRA components themselves to 8-bit (NF8). Please see the Table below for the results, where the numbers other than LQ-LoRA are from OmniQuant [1]. Note that we use the term \"effective bits\" to denote bits per parameter that treat the LoRA components as \"overheads\" for comparing with methods without LoRA; LoRA components (with NF-8 quantization) amount to about 0.1-0.2 extra bits per parameter.\n\n| Base Model | Method                           | Effective Bits (7B, 70B) | C4 (7B) | C4 (70B) | WikiText2 (7B) | WikiText2 (70B) |\n| -------- | ---------------------------------- | ----------- | ---- | ---- | ---- | ---- |\n| LLaMA-2 | Dense                               | 16          | 6.97 | 5.52 | 5.47 | 3.31 |\n| LLaMA-2 | RTN                                 | 3.15        | 8.40 | 6.02 | 6.66 | 3.97 |\n| LLaMA-2 | GPTQ            | 3.15        | 7.89 | 5.85 | 6.29 | 3.85 |\n| LLaMA-2 | AWQ               | 3.15        | 7.84 | -    | 6.24 | -    |\n| LLaMA-2 | OmniQuant  | 3.15        | 7.75 | 5.85 | 6.03 | 3.78 |\n| LLaMA-2 | LQ-LoRA (Fisher)   | 2.95, 2.85 | 7.60 | 5.88 | 5.67 | 3.65 |\n\n[1] https://arxiv.org/abs/2308.13137\n\nThese results indicate that LQ-LoRA can also be used as a state-of-the-art PTQ method. Given the promise of LQ-LoRA as a potential alternative to PTQ, we will include this comparison against PTQ results in the next iteration of the paper.\n\n\n**Novelty**\n\n*Similarly, I would like to raise a query about the paper's novelty. While this method undeniably enhances the current approach (Q-LoRA), from a PEFT perspective, there could be superior methods, particularly concerning inference challenges. On the topic of novelty, I await the insights of fellow reviewers.*\n\nWe want to reiterate three main contributions on top of QLoRA:\n1. We adopt a matrix decomposition view of initializing the quantized and low-rank components of LoRA adaptation. (Note that QLoRA does not perform explicit matrix decomposition)\n2. Our ILP formulation enables us to search for the mixed-configuration quantization that fits specific resource requirements (e.g., 2.75 bits per parameter).\n3. We extended the two methods above to a data-aware setting by incorporating the sensitivities of parameters."}
{"claim": "The paper does not compare LQ-LoRA against GPTQ or similar compensatory quantization methods as potential baselines.", "claim_type": "baseline", "paper_id": "xw29VvOMmU", "paper_title": "LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Bm88GSraZd", "reviewer": "Reviewer_CSBT", "review_text": "Summary: This paper proposes LQ-LoRA, a memory-efficient LLM adaptation method that decomposes each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. The algorithm is adapted from QLoRA and applied modification to solve the problem that zero initialization of the low-rank matrix may not be optimal when the fixed matrix is quantized. The method decomposes the matrix by an iterative algorithm and updates only the low-rank matrix weights during fine-tuning. Results showed that the proposed method outperforms QLoRA and LREC with similar bit compression rates.\n\nStrengths: -\tThe proposed method decomposes the pretrained matrix into a quantizable fixed matrix and low-rank matrix that is already optimized before fine-tuning starts, which contributes to improved accuracy.\n-\tThe paper shows that LQ-LoRA can be used as a mixed quantization strategy, and also proposes a data-aware version of the algorithm, which enables users to flexibly set a target memory budget.\n-\tResults show that the proposed method can be generalized to different model families by showing outperforming results with RoBERTa and LLaMA.\n\nWeaknesses: - The authors have introduced a method that employs an iterative algorithm for initialization. Can they provide insights regarding the computational latency associated with their approach?\n\n- The authors assert the efficiency of LQ-LoRA based on empirical evidence, yet lack theoretical backing. To strengthen the credibility of the algorithm, a comparison might be beneficial, especially with methods that initialize the Q(W) + L1L2 matrix in a manner that closely mirrors the original pretrained matrix W. Consider, for instance, the use of GPTQ as a compensatory mechanism.\n\n- It appears that this paper serves as an expanded or refined rendition of the Q-LoRA paper. As such, it seemingly inherits the same limitation, notably the inference overhead, given that this approach must fail to integrate the LoRA layer into an existing linear layer. \n\n- Similarly, I would like to raise a query about the paper's novelty. While this method undeniably enhances the current approach (Q-LoRA), from a PEFT perspective, there could be superior methods, particularly concerning inference challenges. On the topic of novelty, I await the insights of fellow reviewers.\n\nQuestions: Included in the weakness.", "labeling_timestamp": "2026-01-11T16:27:01.621118", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors explicitly state they implemented and compared against GPTQ-based baselines (referred to as GPTQ-LoRA / LREC), present experimental tables comparing LQ-LoRA to GPTQ and other PTQ methods, and conclude LQ-LoRA improves upon GPTQ-LoRA and QLoRA.", "evidence": "We actually did implement and compare against a baseline that uses OPTQ/GPTQ to quantize first before learning low-rank updates. We find that GPTQ-LoRA underperforms both QLoRA and LQ-LoRA (Table 1). In addition, our latest PTQ experiments included comparisons with RTN, GPTQ, AWQ, and OmniQuant.", "combined_author_response": "Thank you for the discussion! Please see our responses below.\n\n_1. The accuracy of LREC-4bit on MMLU almost matches that of LQ-LoRA_\n\nFor both LLaMA-2 7B and 70B models, LQ-LoRA (Fisher) with 3.25 bits can match the performance of LREC/GPTQ-LoRA 4-bits (i.e., 4.156-bits). Similarly, LQ-LoRA (Fisher) with 2.5/2.75 bits can match the performance of LREC/GPTQ-LoRA 3-bits (i.e., 3.148-bits). Please see the table below. \n\n| Method | Bits per param | MMLU (70B) | MMLU (7B) |\n| ----- | ----- | ----- | ----- |\n| GPTQ-LoRA 3-bit |  3.148 | 0.67$^\\dagger$ | 0.39 |\n| LQ-LoRA (Fisher) | 2.50 | 0.67 | 0.39 |\n| LQ-LoRA (Fisher) | 2.75 | 0.67 | 0.43 |\n\n| Method | Bits per param | MMLU (70B) | MMLU (7B) |\n| ----- | ----- | ----- | ----- |\n| LREC-4 bit | 4.156 | 0.69 | 0.45 |\n| LQ-LoRA (Fisher) | 3.25 | 0.69 | 0.46 |\n\n$^\\dagger$Note that we additionally filled in the MMLU 70B performance for GPTQ-LoRA 3-bit; this number was absent from the initial submission. Based on the above, we conclude that LQ-LoRA can meaningfully improve upon GPTQ-LoRA and QLoRA.\n\n_2. The authors compare their method with GPTQ-LoRA only_\n\nBesides GPTQ-LoRA, we also compared our method QLoRA and different variants of LQ-LoRA. In addition, our latest PTQ experiments included comparisons with RTN, GPTQ, AWQ, and OmniQuant. For details, please see \"Additional Post Training Quantization Experiments\" in our earlier response.\n\n_3. BRECQ and FlexRound_\n\nThanks for the suggestions! We think applying other quantization techniques is certainly interesting, but we would like to raise three points.\n\na) For quantizing large language models with 10B+ parameters, our understanding is that  NF quantization (from QLoRA) and GPTQ --- methods we compared with --- are near state-of-the-art quantization methods. (While we show numbers from other methods, such as Omniquant, in the PTQ comparison table above, these are, as far as we know, preprints).\n\nb) Applying fancier quantization techniques such as BRECQ/Adaround to large models is challenging. For example, for BRECQ,  the OmniQuant paper [1] notes,\n> ... BRECQ ... cannot be applied in models with billions of parameters because they are hard to optimize due to the huge solution space.\n\nSimilarly, the SpQR paper [2] stated that,\n\n> ... BRECQ ... were designed for vision models or small-scale language models, with less than 100M parameters.\n\n[1] https://arxiv.org/abs/2308.13137\n\n[2] https://arxiv.org/abs/2306.03078\n\nFlexRound is an interesting suggestion, but we were unable to find an open-source implementation of this. Hence, we focused our comparison on widely-used LLM quantization methods with open-source code, i.e., QLoRA and GPTQ-LoRA.\n\nc) Finally, (and most importantly), we note that LQ-LoRA can work with generic quantization approaches! For example, during each step of the iterative algorithm, instead of using NF-quantization, we can use other quantization methods to minimize $\\Vert \\mathbf{X}\\mathbf{W} - \\mathbf{X}(\\operatorname{quantize}(\\mathbf{W}) + \\mathbf{L}_1 \\mathbf{L}_2) \\Vert$. We focused on NF quantization because the quantization function is extremely quick (on the order of seconds), and it performs on par with GPTQ despite being data-agnostic. But we think it's worth highlighting this aspect of LQ-LoRA more, and we will discuss this further.\n\n---\n\nHi there! Please let us know if the above response answered some of your questions in the original review, and please let us know if you have follow-up questions!\n\nIn particular, we want to highlight a few key takeaways from our response:\n1. We included a baseline that uses OPTQ/GPTQ to quantize first before learning low-rank updates. We called this \"GPTQ-LoRA\" baseline \"LREC\" in the paper.\n2. We performed additional experiments using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. (Please see the above response for more details.)\n3. The techniques in this paper are not specific to NF. We chose NF because of its superior quality. To be more concrete about this, we conducted additional experiments. (Please see the above response for more details, too.)\n\n---\n\nHi there! Please let us know if the above response answered some of your questions in the original review, and please let us know if you have follow-up questions!\n\nIn particular, we want to highlight a few key takeaways from our response:\n1. The iterative algorithm adds a small extra computation head --- a fraction of the total training time.\n2. We included GPTQ-LoRA comparisons but referred to them as \"LREC\" in the paper.\n3. Inference with quantized + low-rank is not necessarily slower, especially in batch = 1 setting (per the latest releases from QLoRA; kudos to them).\n4. Practitioners could use the \"quantized + low-rank\" models without merging. Such models could be just as fast as merged dense models and more memory-efficient. Based on this potential, we performed additional experiments by using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset.\n\n---\n\nThanks for the comments --- please see our responses below.\n\n**Comparison with methods that use the data $X$ to quantize**\n\nThanks for the question! We actually did implement and compare against a baseline that uses OPTQ/GPTQ to quantize first before learning low-rank updates. In particular, we use GPTQ to first quantize the model and then learn low-rank updates. We called this \"GPTQ-LoRA\" baseline \"LREC\" in the paper following this paper (https://arxiv.org/pdf/2306.08162.pdf). We find that GPTQ-LoRA underperforms both QLoRA and LQ-LoRA (Table 1).\n\n(We now realize that this naming caused some confusion. Thanks for pointing that out --- we will clarify this in the paper.)\n\n\n**Additional Post Training Quantization Experiments**\n\nIn addition to the GPTQ-LoRA experiments already in the paper, we performed additional experiments using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. Specifically, we train the LQ-LoRA (Fisher, 2.75-bit, 64-rank) for one epoch on two C4 partitions and WikiText-2. We then further quantize the LoRA components themselves to 8-bit (NF8). Please see the Table below for the results, where the numbers other than LQ-LoRA are from OmniQuant [1]. Note that we use the term \"effective bits\" to denote bits per parameter that treat the LoRA components as \"overheads\" for comparing with methods without LoRA; LoRA components (with NF-8 quantization) amount to about 0.1-0.2 extra bits per parameter.\n\n\n\n| Base Model | Method                           | Effective Bits (7B, 70B) | C4 (7B) | C4 (70B) | WikiText2 (7B) | WikiText2 (70B) |\n| -------- | ---------------------------------- | ----------- | ---- | ---- | ---- | ---- |\n| LLaMA-2 | Dense                               | 16          | 6.97 | 5.52 | 5.47 | 3.31 |\n| LLaMA-2 | RTN                                 | 3.15        | 8.40 | 6.02 | 6.66 | 3.97 |\n| LLaMA-2 | GPTQ            | 3.15        | 7.89 | 5.85 | 6.29 | 3.85 |\n| LLaMA-2 | AWQ               | 3.15        | 7.84 | -    | 6.24 | -    |\n| LLaMA-2 | OmniQuant  | 3.15        | 7.75 | 5.85 | 6.03 | 3.78 |\n| LLaMA-2 | LQ-LoRA (Fisher)   | 2.95, 2.85 | 7.60 | 5.88 | 5.67 | 3.65 |\n\n\n\n\n[1] https://arxiv.org/abs/2308.13137\n\nThese results indicate that LQ-LoRA can also be used as a state-of-the-art PTQ method. Given the promise of LQ-LoRA as a potential alternative to PTQ, we will include this comparison against PTQ results in the next iteration of the paper.\n\n\n\n**NF-based vs INT-based Quantization**\n\nYou are correct in that the techniques in this paper are not specific to NF. QLoRA paper demonstrated the superior quality of such a scheme compared to classical integer quantization. Our preliminary experiments confirmed this, and hence, we chose NF.\n\nWe conducted the following experiment to be more concrete about NF vs INT. We ran LQ (i.e., Section 3.1) on LLaMA-2 7B's dense parameters with `rank=64` and NF3/INT3/NF4/INT4 quantization configuration (i.e., `b0=3/4, b1=8, b2=fp32, B0=64, B1=256`), and measure the reconstruction errors (i.e., $||\\mathbf{W} - (\\mathbf{Q} + \\mathbf{L}_1 \\mathbf{L}_2) ||_2^2$). The table below shows that NF indeed performed better.\n\n| Method      | Reconstruction error (unit: $10^4$) |\n| ----------- | ----------- |\n| LQ with NF3      | 7.12       |\n| LQ with INT3     | 11.6        |\n| LQ with NF4      | 1.42       |\n| LQ with INT4     | 2.02        |\n\n---\n\nThanks to the reviewer for the comments! Please see our responses below.\n\n**Latency of The Iterative Algorithm**\n\nThis is a one-time cost at the initialization and takes a fraction of the total training time. Each step of the iterative algorithm consists of SVD followed by quantization. Note that we use randomized SVD (rSVD) for speed consideration, and we run the algorithm for up to 100 steps (and we stop before this if the error starts to increase).\n\nTo be more concrete, we measured the runtime to process three of the LLaMA-2 7B's matrices with `rank=64` and NF3 quantization. Please see the table below.\n\n| Data-Aware (i.e., Section 3.3) | Time / Matrix |\n| ----- | ----- |\n| No  | 1.3 to 2.1 seconds |\n| Yes | 1.3 to 2.3 seconds |\n\n**Comparison with GPTQ-based Methods**\n\nThanks for the question! We actually did implement and compare against GPTQ-LoRA. We called this \"LREC\" in the paper following this paper (https://arxiv.org/pdf/2306.08162.pdf). We find that GPTQ-LoRA underperforms both QLoRA and LQ-LoRA (Table 1).\n\n(We now realize that this naming caused some confusion. Thanks for pointing that out --- we will clarify this in the paper.)\n\n**Limitations from QLoRA**\n\nThis is a very good point! But we want to point out that inference with quantized + low-rank is not necessarily slower, especially in batch = 1 setting. Smaller quantized matrices allow us to reduce data movement between SRAM and HBM of the GPU. For example, in the latest releases of `bitsandbytes` (the low-level engine behind QLoRA) [1], they claimed their matrix-vector multiplication between quantized matrix and dense vector (the one needed during inference) could be competitive and even outperformed dense matrix-vector multiplication. \n\n[1] https://github.com/TimDettmers/bitsandbytes/releases\n\n*Integrating LoRA Layers Into Linear Layers*\n\nGiven the potential inference speed-ups, we think that practitioners could use the \"quantized + low-rank\" models without merging. Such models could be just as fast (as discussed above) as merged dense models and more memory-efficient. In that regard, you could (loosely speaking) consider this as an alternative PTQ method.\n\nBased on this potential of LQ-LoRA as an alternative to PTQ, we performed additional experiments by using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. Specifically, we train the LQ-LoRA (Fisher, 2.75-bit, 64-rank) for one epoch on two C4 partitions and WikiText-2. We then further quantize the LoRA components themselves to 8-bit (NF8). Please see the Table below for the results, where the numbers other than LQ-LoRA are from OmniQuant [1]. Note that we use the term \"effective bits\" to denote bits per parameter that treat the LoRA components as \"overheads\" for comparing with methods without LoRA; LoRA components (with NF-8 quantization) amount to about 0.1-0.2 extra bits per parameter.\n\n| Base Model | Method                           | Effective Bits (7B, 70B) | C4 (7B) | C4 (70B) | WikiText2 (7B) | WikiText2 (70B) |\n| -------- | ---------------------------------- | ----------- | ---- | ---- | ---- | ---- |\n| LLaMA-2 | Dense                               | 16          | 6.97 | 5.52 | 5.47 | 3.31 |\n| LLaMA-2 | RTN                                 | 3.15        | 8.40 | 6.02 | 6.66 | 3.97 |\n| LLaMA-2 | GPTQ            | 3.15        | 7.89 | 5.85 | 6.29 | 3.85 |\n| LLaMA-2 | AWQ               | 3.15        | 7.84 | -    | 6.24 | -    |\n| LLaMA-2 | OmniQuant  | 3.15        | 7.75 | 5.85 | 6.03 | 3.78 |\n| LLaMA-2 | LQ-LoRA (Fisher)   | 2.95, 2.85 | 7.60 | 5.88 | 5.67 | 3.65 |\n\n[1] https://arxiv.org/abs/2308.13137\n\nThese results indicate that LQ-LoRA can also be used as a state-of-the-art PTQ method. Given the promise of LQ-LoRA as a potential alternative to PTQ, we will include this comparison against PTQ results in the next iteration of the paper.\n\n\n**Novelty**\n\n*Similarly, I would like to raise a query about the paper's novelty. While this method undeniably enhances the current approach (Q-LoRA), from a PEFT perspective, there could be superior methods, particularly concerning inference challenges. On the topic of novelty, I await the insights of fellow reviewers.*\n\nWe want to reiterate three main contributions on top of QLoRA:\n1. We adopt a matrix decomposition view of initializing the quantized and low-rank components of LoRA adaptation. (Note that QLoRA does not perform explicit matrix decomposition)\n2. Our ILP formulation enables us to search for the mixed-configuration quantization that fits specific resource requirements (e.g., 2.75 bits per parameter).\n3. We extended the two methods above to a data-aware setting by incorporating the sensitivities of parameters.\n\n---\n\nThanks for the helpful review! Please take a look at our responses below.\n\n**Novelty with respect to QLoRA**\n\nWe want to note that QLoRA does not perform explicit matrix decomposition. It performs quantization of the original weight matrix and learns additive low-rank updates. In contrast, we perform explicit matrix decomposition of the original matrix into low-rank and quantized components via a matrix reconstruction objective.\n\n**Ablations**\n\n*How important is the ILP to LQ-LoRA? Can you show the performance of LQ-LoRA without the ILP?*\n\nOne of the main usages of ILP is to flexibly quantize the model to meet specific memory constraints (e.g., 2.75 bits/parameter). That being said, LQ-LoRA is effective even without the ILP. For example, Table 4 shows LQ-LoRA without ILP. Here, we use NF3 quantization configuration --- NF3 reuses the configuration of the QLoRA's NF4 quantization with 3-bit first-level quantization. \n\n*Can you show the performance of the regular LoRA method (no quantization), and also quantization (at different bit-rates) without LoRA, in Table 2?*\n\nWe want to note that the QLoRA paper already demonstrated little performance loss between regular LoRA and QLoRA (4bit). Hence, we can think of QLoRA (4bit) performance as a proxy for regular LoRA.\n\nHowever, based on your suggestion/question, we performed additional experiments using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. Specifically, we train the LQ-LoRA (Fisher, 2.75-bit, 64-rank) for one epoch on two C4 partitions and WikiText-2. We then further quantize the LoRA components themselves to 8-bit (NF8). Please see the Table below for the results, where the numbers other than LQ-LoRA are from OmniQuant [1]. Note that we use the term \"effective bits\" to denote bits per parameter that treat the LoRA components as \"overheads\" for comparing with methods without LoRA; LoRA components (with NF-8 quantization) amount to about 0.1-0.2 extra bits per parameter.\n\n| Base Model | Method                           | Effective Bits (7B, 70B) | C4 (7B) | C4 (70B) | WikiText2 (7B) | WikiText2 (70B) |\n| -------- | ---------------------------------- | ----------- | ---- | ---- | ---- | ---- |\n| LLaMA-2 | Dense                               | 16          | 6.97 | 5.52 | 5.47 | 3.31 |\n| LLaMA-2 | RTN                                 | 3.15        | 8.40 | 6.02 | 6.66 | 3.97 |\n| LLaMA-2 | GPTQ            | 3.15        | 7.89 | 5.85 | 6.29 | 3.85 |\n| LLaMA-2 | AWQ               | 3.15        | 7.84 | -    | 6.24 | -    |\n| LLaMA-2 | OmniQuant  | 3.15        | 7.75 | 5.85 | 6.03 | 3.78 |\n| LLaMA-2 | LQ-LoRA (Fisher)   | 2.95, 2.85 | 7.60 | 5.88 | 5.67 | 3.65 |\n\n[1] https://arxiv.org/abs/2308.13137\n\nThese results indicate that LQ-LoRA can also be used as a state-of-the-art PTQ method. Given the promise of LQ-LoRA as a potential alternative to PTQ, we will include this comparison against PTQ results in the next iteration of the paper.\n\n**ILP for Rank Selection**\n\nThis is a very good/interesting suggestion! We have looked into this by ILP-searching the bits and ranks together. One practical challenge is measuring the \"storages/errors\"  of the ranks of low-rank components. This is important to trade off bits of the quantized matrices and ranks of LoRA. Notice that LoRA components will be fine-tuned; searching the ranks using errors at initialization will unnecessarily favor more bits at the cost of lower ranks. We might be able to formulate this as a bi-level optimization problem (first searching the ranks and then searching the bits), but we will leave this as future work.\n\n**Questions**\n\n*Is the only difference between QLoRA+ILP, and LQ-LoRA, the initialization?*\n\nYep! \n\n*Does the ILP budget, as well as the \"bits per param\" column, also consider the low-rank components?*\n\nThis table does not take into account low-rank components since we use rank = 64 for our QLoRA and LREC (GPTQ + LoRA) baselines. However, the PTQ table above **does** take into account the contribution from the low-rank components to ensure fair comparison against other PTQ methods. We find that the LoRA components (if quantized) add about ~0.2 bits. We will clarify this further!\n\n**Suggestions**\n\nThanks for the suggestions regarding the figures and tables! We will take these into account for the final version of the paper.\n\n---\n\nWe thank the reviewer for their comments/questions. Please find our responses below.\n\n**Mixed Precision & Hardware**\n\nWe note that our work is in the **weight only** quantization regime (as are QLoRA, GPTQ, etc.), where only the weights (and not the activations) are quantized to lower bits. The actual matmul is done in full precision after dequantization. \n\nConcretely, we use just-in-time de-quantize the (quantized) matrix $\\mathbf{Q}$, execute matrix operations (e.g., `matmul(X, Q) ==> matmul(X, dequantize(Q))`), and throw away the de-quantized matrix. This supports arbitrary quantization, and the main technical difficulty is how to do de-quantization quickly. We outlined more details in the appendix.\n\nAlthough weight-only quantization cannot make use of (faster and more energy-efficient) lower-precision matmuls, weight-only quantization still has two practical benefits. First, at a macro level, having a smaller model (due to quantization) allows us to increase the batch size. This reduces the need for expensive data/model parallelism, which requires cross-device or even cross-node communication. Second, at a micro level, smaller quantized matrices allow us to reduce data movement between SRAM and HBM of the GPU. As an example, QLoRA's matrix multiplication implementation could be faster than dense matrix multiplication [1].\n\n[1] https://github.com/TimDettmers/bitsandbytes/releases\n\n**Related paper on Joint Low-rank and Quantized Matrix Decomposition**\n\nThanks for the suggestion; this is an interesting paper! We will discuss this paper in the related works section.\n\nWe primarily considered the cases in which $\\mathbf{L}_1$ and $\\mathbf{L}_2$ are floating-point parameters. This is necessary because we want to fine-tune these parameters. That being said, the techniques introduced in that paper paper might be helpful for other (future) use cases. For example, we could post-training merge $\\mathbf{Q}, \\mathbf{L}_1, \\mathbf{L}_2$, and re-decompose them into three separate quantized matrices for inference.\n\n**Question 1**\n\nYou are correct -- we will clarify this; thanks!\n\n**Question 2**\n\nYou are correct in that the techniques in this paper are not specific to NF. QLoRA paper demonstrated the superior quality of such a scheme compared to classical integer quantization. Our preliminary experiments confirmed this, and hence, we chose NF.\n\nWe conducted the following experiment to be more concrete about NF vs INT. We ran LQ (i.e., Section 3.1) on LLaMA-2 7B's dense parameters with `rank=64` and NF3/INT3/NF4/INT4 quantization configuration (i.e., `b0=3/4, b1=8, b2=fp32, B0=64, B1=256`), and measure the reconstruction errors (i.e., $||\\mathbf{W} - (\\mathbf{Q} + \\mathbf{L}_1 \\mathbf{L}_2) ||_2^2$). The table below shows that NF indeed performed better.\n\n| Method      | Reconstruction error (unit: $10^4$) |\n| ----------- | ----------- |\n| LQ with NF3      | 7.12       |\n| LQ with INT3     | 11.6        |\n| LQ with NF4      | 1.42       |\n| LQ with INT4     | 2.02        |\n\n**Question 3**\n\nThis is the same $\\delta$ in QLoRA, and we treated it as a constant for all settings. We will clarify further in the next iteration of the paper.\n\n**Question 4**\n\nWe _defined_ $\\mathbb{Q}$ as the set of matrices that can be quantized (Equation 1 is a minimization problem, not an exact decomposition problem). Thanks for pointing out the confusion; we will clarify this.\n\n**Question 5**\n\nThe middle figure represents the setting of many (Q)LoRA methods, in which the Low-Rank components are initialized to be zero. In that sense, $\\operatorname{quantize}(\\mathbf{W}) + \\mathbf{L}_1 \\mathbf{L}_2 = \\operatorname{quantize}(\\mathbf{W})$ for those methods.\n\n**Question 6**\n\nGood catch -- it's meant for \"budget\" but shared similar symbols with $B_0$ and $B_1$.\n\n**Question 7**\n\nWe say a matrix $\\mathbf{F}$ has homogenous rows or columns when either rows or columns of the matrix have identical values. We will clarify this.\n\n**Question 8**\n\nTable 4 used the quantization configuration of NF3/NF4. Specifically, they use the block sizes and second-level quantization as the original NF4, but with the first-level quantization set to 3/4-bit. Table 2, however, used ILP to search for the quantization configuration, hence the difference.\n\nWe made such decisions for Table 4 to remove the possible confounding variable of using ILP to search for the quantization configuration (i.e., we just wanted to see the effect of higher rank LoRA components). We will clarify!", "author_response": "Thanks to the reviewer for the comments! Please see our responses below.\n\n**Latency of The Iterative Algorithm**\n\nThis is a one-time cost at the initialization and takes a fraction of the total training time. Each step of the iterative algorithm consists of SVD followed by quantization. Note that we use randomized SVD (rSVD) for speed consideration, and we run the algorithm for up to 100 steps (and we stop before this if the error starts to increase).\n\nTo be more concrete, we measured the runtime to process three of the LLaMA-2 7B's matrices with `rank=64` and NF3 quantization. Please see the table below.\n\n| Data-Aware (i.e., Section 3.3) | Time / Matrix |\n| ----- | ----- |\n| No  | 1.3 to 2.1 seconds |\n| Yes | 1.3 to 2.3 seconds |\n\n**Comparison with GPTQ-based Methods**\n\nThanks for the question! We actually did implement and compare against GPTQ-LoRA. We called this \"LREC\" in the paper following this paper (https://arxiv.org/pdf/2306.08162.pdf). We find that GPTQ-LoRA underperforms both QLoRA and LQ-LoRA (Table 1).\n\n(We now realize that this naming caused some confusion. Thanks for pointing that out --- we will clarify this in the paper.)\n\n**Limitations from QLoRA**\n\nThis is a very good point! But we want to point out that inference with quantized + low-rank is not necessarily slower, especially in batch = 1 setting. Smaller quantized matrices allow us to reduce data movement between SRAM and HBM of the GPU. For example, in the latest releases of `bitsandbytes` (the low-level engine behind QLoRA) [1], they claimed their matrix-vector multiplication between quantized matrix and dense vector (the one needed during inference) could be competitive and even outperformed dense matrix-vector multiplication. \n\n[1] https://github.com/TimDettmers/bitsandbytes/releases\n\n*Integrating LoRA Layers Into Linear Layers*\n\nGiven the potential inference speed-ups, we think that practitioners could use the \"quantized + low-rank\" models without merging. Such models could be just as fast (as discussed above) as merged dense models and more memory-efficient. In that regard, you could (loosely speaking) consider this as an alternative PTQ method.\n\nBased on this potential of LQ-LoRA as an alternative to PTQ, we performed additional experiments by using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. Specifically, we train the LQ-LoRA (Fisher, 2.75-bit, 64-rank) for one epoch on two C4 partitions and WikiText-2. We then further quantize the LoRA components themselves to 8-bit (NF8). Please see the Table below for the results, where the numbers other than LQ-LoRA are from OmniQuant [1]. Note that we use the term \"effective bits\" to denote bits per parameter that treat the LoRA components as \"overheads\" for comparing with methods without LoRA; LoRA components (with NF-8 quantization) amount to about 0.1-0.2 extra bits per parameter.\n\n| Base Model | Method                           | Effective Bits (7B, 70B) | C4 (7B) | C4 (70B) | WikiText2 (7B) | WikiText2 (70B) |\n| -------- | ---------------------------------- | ----------- | ---- | ---- | ---- | ---- |\n| LLaMA-2 | Dense                               | 16          | 6.97 | 5.52 | 5.47 | 3.31 |\n| LLaMA-2 | RTN                                 | 3.15        | 8.40 | 6.02 | 6.66 | 3.97 |\n| LLaMA-2 | GPTQ            | 3.15        | 7.89 | 5.85 | 6.29 | 3.85 |\n| LLaMA-2 | AWQ               | 3.15        | 7.84 | -    | 6.24 | -    |\n| LLaMA-2 | OmniQuant  | 3.15        | 7.75 | 5.85 | 6.03 | 3.78 |\n| LLaMA-2 | LQ-LoRA (Fisher)   | 2.95, 2.85 | 7.60 | 5.88 | 5.67 | 3.65 |\n\n[1] https://arxiv.org/abs/2308.13137\n\nThese results indicate that LQ-LoRA can also be used as a state-of-the-art PTQ method. Given the promise of LQ-LoRA as a potential alternative to PTQ, we will include this comparison against PTQ results in the next iteration of the paper.\n\n\n**Novelty**\n\n*Similarly, I would like to raise a query about the paper's novelty. While this method undeniably enhances the current approach (Q-LoRA), from a PEFT perspective, there could be superior methods, particularly concerning inference challenges. On the topic of novelty, I await the insights of fellow reviewers.*\n\nWe want to reiterate three main contributions on top of QLoRA:\n1. We adopt a matrix decomposition view of initializing the quantized and low-rank components of LoRA adaptation. (Note that QLoRA does not perform explicit matrix decomposition)\n2. Our ILP formulation enables us to search for the mixed-configuration quantization that fits specific resource requirements (e.g., 2.75 bits per parameter).\n3. We extended the two methods above to a data-aware setting by incorporating the sensitivities of parameters."}
{"claim": "LQ-LoRA appears to inherit QLoRA's inference overhead limitation because it cannot integrate the LoRA layer directly into existing linear layers.", "claim_type": "methodology", "paper_id": "xw29VvOMmU", "paper_title": "LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Bm88GSraZd", "reviewer": "Reviewer_CSBT", "review_text": "Summary: This paper proposes LQ-LoRA, a memory-efficient LLM adaptation method that decomposes each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. The algorithm is adapted from QLoRA and applied modification to solve the problem that zero initialization of the low-rank matrix may not be optimal when the fixed matrix is quantized. The method decomposes the matrix by an iterative algorithm and updates only the low-rank matrix weights during fine-tuning. Results showed that the proposed method outperforms QLoRA and LREC with similar bit compression rates.\n\nStrengths: -\tThe proposed method decomposes the pretrained matrix into a quantizable fixed matrix and low-rank matrix that is already optimized before fine-tuning starts, which contributes to improved accuracy.\n-\tThe paper shows that LQ-LoRA can be used as a mixed quantization strategy, and also proposes a data-aware version of the algorithm, which enables users to flexibly set a target memory budget.\n-\tResults show that the proposed method can be generalized to different model families by showing outperforming results with RoBERTa and LLaMA.\n\nWeaknesses: - The authors have introduced a method that employs an iterative algorithm for initialization. Can they provide insights regarding the computational latency associated with their approach?\n\n- The authors assert the efficiency of LQ-LoRA based on empirical evidence, yet lack theoretical backing. To strengthen the credibility of the algorithm, a comparison might be beneficial, especially with methods that initialize the Q(W) + L1L2 matrix in a manner that closely mirrors the original pretrained matrix W. Consider, for instance, the use of GPTQ as a compensatory mechanism.\n\n- It appears that this paper serves as an expanded or refined rendition of the Q-LoRA paper. As such, it seemingly inherits the same limitation, notably the inference overhead, given that this approach must fail to integrate the LoRA layer into an existing linear layer. \n\n- Similarly, I would like to raise a query about the paper's novelty. While this method undeniably enhances the current approach (Q-LoRA), from a PEFT perspective, there could be superior methods, particularly concerning inference challenges. On the topic of novelty, I await the insights of fellow reviewers.\n\nQuestions: Included in the weakness.", "labeling_timestamp": "2026-01-11T16:27:16.921174", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors do not claim that LQ-LoRA can be merged into existing linear layers (they do not demonstrate integration), but they explicitly argue that the inference overhead is not necessarily an issue: quantized + low-rank models can be competitive without merging, LoRA components can be quantized (adding only ~0.1–0.2 bits), and they provide experiments and citations to support comparable inference speed. Thus they partially address the reviewer's concern by disputing the inevitability of an inference overhead, while not fully showing direct integration of LoRA into linear layers.", "evidence": "“inference with quantized + low-rank is not necessarily slower, especially in batch = 1 setting (per the latest releases from QLoRA; kudos to them).”\n\n“Practitioners could use the 'quantized + low-rank' models without merging. Such models could be just as fast as merged dense models and more memory-efficient.”\n\n“We then further quantize the LoRA components themselves to 8-bit (NF8)... LoRA components (with NF-8 quantization) amount to about 0.1-0.2 extra bits per parameter.”\n\n“For example, we could post-training merge \\mathbf{Q}, \\mathbf{L}_1, \\mathbf{L}_2, and re-decompose them into three separate quantized matrices for inference.”", "combined_author_response": "Thank you for the discussion! Please see our responses below.\n\n_1. The accuracy of LREC-4bit on MMLU almost matches that of LQ-LoRA_\n\nFor both LLaMA-2 7B and 70B models, LQ-LoRA (Fisher) with 3.25 bits can match the performance of LREC/GPTQ-LoRA 4-bits (i.e., 4.156-bits). Similarly, LQ-LoRA (Fisher) with 2.5/2.75 bits can match the performance of LREC/GPTQ-LoRA 3-bits (i.e., 3.148-bits). Please see the table below. \n\n| Method | Bits per param | MMLU (70B) | MMLU (7B) |\n| ----- | ----- | ----- | ----- |\n| GPTQ-LoRA 3-bit |  3.148 | 0.67$^\\dagger$ | 0.39 |\n| LQ-LoRA (Fisher) | 2.50 | 0.67 | 0.39 |\n| LQ-LoRA (Fisher) | 2.75 | 0.67 | 0.43 |\n\n| Method | Bits per param | MMLU (70B) | MMLU (7B) |\n| ----- | ----- | ----- | ----- |\n| LREC-4 bit | 4.156 | 0.69 | 0.45 |\n| LQ-LoRA (Fisher) | 3.25 | 0.69 | 0.46 |\n\n$^\\dagger$Note that we additionally filled in the MMLU 70B performance for GPTQ-LoRA 3-bit; this number was absent from the initial submission. Based on the above, we conclude that LQ-LoRA can meaningfully improve upon GPTQ-LoRA and QLoRA.\n\n_2. The authors compare their method with GPTQ-LoRA only_\n\nBesides GPTQ-LoRA, we also compared our method QLoRA and different variants of LQ-LoRA. In addition, our latest PTQ experiments included comparisons with RTN, GPTQ, AWQ, and OmniQuant. For details, please see \"Additional Post Training Quantization Experiments\" in our earlier response.\n\n_3. BRECQ and FlexRound_\n\nThanks for the suggestions! We think applying other quantization techniques is certainly interesting, but we would like to raise three points.\n\na) For quantizing large language models with 10B+ parameters, our understanding is that  NF quantization (from QLoRA) and GPTQ --- methods we compared with --- are near state-of-the-art quantization methods. (While we show numbers from other methods, such as Omniquant, in the PTQ comparison table above, these are, as far as we know, preprints).\n\nb) Applying fancier quantization techniques such as BRECQ/Adaround to large models is challenging. For example, for BRECQ,  the OmniQuant paper [1] notes,\n> ... BRECQ ... cannot be applied in models with billions of parameters because they are hard to optimize due to the huge solution space.\n\nSimilarly, the SpQR paper [2] stated that,\n\n> ... BRECQ ... were designed for vision models or small-scale language models, with less than 100M parameters.\n\n[1] https://arxiv.org/abs/2308.13137\n\n[2] https://arxiv.org/abs/2306.03078\n\nFlexRound is an interesting suggestion, but we were unable to find an open-source implementation of this. Hence, we focused our comparison on widely-used LLM quantization methods with open-source code, i.e., QLoRA and GPTQ-LoRA.\n\nc) Finally, (and most importantly), we note that LQ-LoRA can work with generic quantization approaches! For example, during each step of the iterative algorithm, instead of using NF-quantization, we can use other quantization methods to minimize $\\Vert \\mathbf{X}\\mathbf{W} - \\mathbf{X}(\\operatorname{quantize}(\\mathbf{W}) + \\mathbf{L}_1 \\mathbf{L}_2) \\Vert$. We focused on NF quantization because the quantization function is extremely quick (on the order of seconds), and it performs on par with GPTQ despite being data-agnostic. But we think it's worth highlighting this aspect of LQ-LoRA more, and we will discuss this further.\n\n---\n\nHi there! Please let us know if the above response answered some of your questions in the original review, and please let us know if you have follow-up questions!\n\nIn particular, we want to highlight a few key takeaways from our response:\n1. We included a baseline that uses OPTQ/GPTQ to quantize first before learning low-rank updates. We called this \"GPTQ-LoRA\" baseline \"LREC\" in the paper.\n2. We performed additional experiments using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. (Please see the above response for more details.)\n3. The techniques in this paper are not specific to NF. We chose NF because of its superior quality. To be more concrete about this, we conducted additional experiments. (Please see the above response for more details, too.)\n\n---\n\nHi there! Please let us know if the above response answered some of your questions in the original review, and please let us know if you have follow-up questions!\n\nIn particular, we want to highlight a few key takeaways from our response:\n1. The iterative algorithm adds a small extra computation head --- a fraction of the total training time.\n2. We included GPTQ-LoRA comparisons but referred to them as \"LREC\" in the paper.\n3. Inference with quantized + low-rank is not necessarily slower, especially in batch = 1 setting (per the latest releases from QLoRA; kudos to them).\n4. Practitioners could use the \"quantized + low-rank\" models without merging. Such models could be just as fast as merged dense models and more memory-efficient. Based on this potential, we performed additional experiments by using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset.\n\n---\n\nThanks for the comments --- please see our responses below.\n\n**Comparison with methods that use the data $X$ to quantize**\n\nThanks for the question! We actually did implement and compare against a baseline that uses OPTQ/GPTQ to quantize first before learning low-rank updates. In particular, we use GPTQ to first quantize the model and then learn low-rank updates. We called this \"GPTQ-LoRA\" baseline \"LREC\" in the paper following this paper (https://arxiv.org/pdf/2306.08162.pdf). We find that GPTQ-LoRA underperforms both QLoRA and LQ-LoRA (Table 1).\n\n(We now realize that this naming caused some confusion. Thanks for pointing that out --- we will clarify this in the paper.)\n\n\n**Additional Post Training Quantization Experiments**\n\nIn addition to the GPTQ-LoRA experiments already in the paper, we performed additional experiments using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. Specifically, we train the LQ-LoRA (Fisher, 2.75-bit, 64-rank) for one epoch on two C4 partitions and WikiText-2. We then further quantize the LoRA components themselves to 8-bit (NF8). Please see the Table below for the results, where the numbers other than LQ-LoRA are from OmniQuant [1]. Note that we use the term \"effective bits\" to denote bits per parameter that treat the LoRA components as \"overheads\" for comparing with methods without LoRA; LoRA components (with NF-8 quantization) amount to about 0.1-0.2 extra bits per parameter.\n\n\n\n| Base Model | Method                           | Effective Bits (7B, 70B) | C4 (7B) | C4 (70B) | WikiText2 (7B) | WikiText2 (70B) |\n| -------- | ---------------------------------- | ----------- | ---- | ---- | ---- | ---- |\n| LLaMA-2 | Dense                               | 16          | 6.97 | 5.52 | 5.47 | 3.31 |\n| LLaMA-2 | RTN                                 | 3.15        | 8.40 | 6.02 | 6.66 | 3.97 |\n| LLaMA-2 | GPTQ            | 3.15        | 7.89 | 5.85 | 6.29 | 3.85 |\n| LLaMA-2 | AWQ               | 3.15        | 7.84 | -    | 6.24 | -    |\n| LLaMA-2 | OmniQuant  | 3.15        | 7.75 | 5.85 | 6.03 | 3.78 |\n| LLaMA-2 | LQ-LoRA (Fisher)   | 2.95, 2.85 | 7.60 | 5.88 | 5.67 | 3.65 |\n\n\n\n\n[1] https://arxiv.org/abs/2308.13137\n\nThese results indicate that LQ-LoRA can also be used as a state-of-the-art PTQ method. Given the promise of LQ-LoRA as a potential alternative to PTQ, we will include this comparison against PTQ results in the next iteration of the paper.\n\n\n\n**NF-based vs INT-based Quantization**\n\nYou are correct in that the techniques in this paper are not specific to NF. QLoRA paper demonstrated the superior quality of such a scheme compared to classical integer quantization. Our preliminary experiments confirmed this, and hence, we chose NF.\n\nWe conducted the following experiment to be more concrete about NF vs INT. We ran LQ (i.e., Section 3.1) on LLaMA-2 7B's dense parameters with `rank=64` and NF3/INT3/NF4/INT4 quantization configuration (i.e., `b0=3/4, b1=8, b2=fp32, B0=64, B1=256`), and measure the reconstruction errors (i.e., $||\\mathbf{W} - (\\mathbf{Q} + \\mathbf{L}_1 \\mathbf{L}_2) ||_2^2$). The table below shows that NF indeed performed better.\n\n| Method      | Reconstruction error (unit: $10^4$) |\n| ----------- | ----------- |\n| LQ with NF3      | 7.12       |\n| LQ with INT3     | 11.6        |\n| LQ with NF4      | 1.42       |\n| LQ with INT4     | 2.02        |\n\n---\n\nThanks to the reviewer for the comments! Please see our responses below.\n\n**Latency of The Iterative Algorithm**\n\nThis is a one-time cost at the initialization and takes a fraction of the total training time. Each step of the iterative algorithm consists of SVD followed by quantization. Note that we use randomized SVD (rSVD) for speed consideration, and we run the algorithm for up to 100 steps (and we stop before this if the error starts to increase).\n\nTo be more concrete, we measured the runtime to process three of the LLaMA-2 7B's matrices with `rank=64` and NF3 quantization. Please see the table below.\n\n| Data-Aware (i.e., Section 3.3) | Time / Matrix |\n| ----- | ----- |\n| No  | 1.3 to 2.1 seconds |\n| Yes | 1.3 to 2.3 seconds |\n\n**Comparison with GPTQ-based Methods**\n\nThanks for the question! We actually did implement and compare against GPTQ-LoRA. We called this \"LREC\" in the paper following this paper (https://arxiv.org/pdf/2306.08162.pdf). We find that GPTQ-LoRA underperforms both QLoRA and LQ-LoRA (Table 1).\n\n(We now realize that this naming caused some confusion. Thanks for pointing that out --- we will clarify this in the paper.)\n\n**Limitations from QLoRA**\n\nThis is a very good point! But we want to point out that inference with quantized + low-rank is not necessarily slower, especially in batch = 1 setting. Smaller quantized matrices allow us to reduce data movement between SRAM and HBM of the GPU. For example, in the latest releases of `bitsandbytes` (the low-level engine behind QLoRA) [1], they claimed their matrix-vector multiplication between quantized matrix and dense vector (the one needed during inference) could be competitive and even outperformed dense matrix-vector multiplication. \n\n[1] https://github.com/TimDettmers/bitsandbytes/releases\n\n*Integrating LoRA Layers Into Linear Layers*\n\nGiven the potential inference speed-ups, we think that practitioners could use the \"quantized + low-rank\" models without merging. Such models could be just as fast (as discussed above) as merged dense models and more memory-efficient. In that regard, you could (loosely speaking) consider this as an alternative PTQ method.\n\nBased on this potential of LQ-LoRA as an alternative to PTQ, we performed additional experiments by using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. Specifically, we train the LQ-LoRA (Fisher, 2.75-bit, 64-rank) for one epoch on two C4 partitions and WikiText-2. We then further quantize the LoRA components themselves to 8-bit (NF8). Please see the Table below for the results, where the numbers other than LQ-LoRA are from OmniQuant [1]. Note that we use the term \"effective bits\" to denote bits per parameter that treat the LoRA components as \"overheads\" for comparing with methods without LoRA; LoRA components (with NF-8 quantization) amount to about 0.1-0.2 extra bits per parameter.\n\n| Base Model | Method                           | Effective Bits (7B, 70B) | C4 (7B) | C4 (70B) | WikiText2 (7B) | WikiText2 (70B) |\n| -------- | ---------------------------------- | ----------- | ---- | ---- | ---- | ---- |\n| LLaMA-2 | Dense                               | 16          | 6.97 | 5.52 | 5.47 | 3.31 |\n| LLaMA-2 | RTN                                 | 3.15        | 8.40 | 6.02 | 6.66 | 3.97 |\n| LLaMA-2 | GPTQ            | 3.15        | 7.89 | 5.85 | 6.29 | 3.85 |\n| LLaMA-2 | AWQ               | 3.15        | 7.84 | -    | 6.24 | -    |\n| LLaMA-2 | OmniQuant  | 3.15        | 7.75 | 5.85 | 6.03 | 3.78 |\n| LLaMA-2 | LQ-LoRA (Fisher)   | 2.95, 2.85 | 7.60 | 5.88 | 5.67 | 3.65 |\n\n[1] https://arxiv.org/abs/2308.13137\n\nThese results indicate that LQ-LoRA can also be used as a state-of-the-art PTQ method. Given the promise of LQ-LoRA as a potential alternative to PTQ, we will include this comparison against PTQ results in the next iteration of the paper.\n\n\n**Novelty**\n\n*Similarly, I would like to raise a query about the paper's novelty. While this method undeniably enhances the current approach (Q-LoRA), from a PEFT perspective, there could be superior methods, particularly concerning inference challenges. On the topic of novelty, I await the insights of fellow reviewers.*\n\nWe want to reiterate three main contributions on top of QLoRA:\n1. We adopt a matrix decomposition view of initializing the quantized and low-rank components of LoRA adaptation. (Note that QLoRA does not perform explicit matrix decomposition)\n2. Our ILP formulation enables us to search for the mixed-configuration quantization that fits specific resource requirements (e.g., 2.75 bits per parameter).\n3. We extended the two methods above to a data-aware setting by incorporating the sensitivities of parameters.\n\n---\n\nThanks for the helpful review! Please take a look at our responses below.\n\n**Novelty with respect to QLoRA**\n\nWe want to note that QLoRA does not perform explicit matrix decomposition. It performs quantization of the original weight matrix and learns additive low-rank updates. In contrast, we perform explicit matrix decomposition of the original matrix into low-rank and quantized components via a matrix reconstruction objective.\n\n**Ablations**\n\n*How important is the ILP to LQ-LoRA? Can you show the performance of LQ-LoRA without the ILP?*\n\nOne of the main usages of ILP is to flexibly quantize the model to meet specific memory constraints (e.g., 2.75 bits/parameter). That being said, LQ-LoRA is effective even without the ILP. For example, Table 4 shows LQ-LoRA without ILP. Here, we use NF3 quantization configuration --- NF3 reuses the configuration of the QLoRA's NF4 quantization with 3-bit first-level quantization. \n\n*Can you show the performance of the regular LoRA method (no quantization), and also quantization (at different bit-rates) without LoRA, in Table 2?*\n\nWe want to note that the QLoRA paper already demonstrated little performance loss between regular LoRA and QLoRA (4bit). Hence, we can think of QLoRA (4bit) performance as a proxy for regular LoRA.\n\nHowever, based on your suggestion/question, we performed additional experiments using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. Specifically, we train the LQ-LoRA (Fisher, 2.75-bit, 64-rank) for one epoch on two C4 partitions and WikiText-2. We then further quantize the LoRA components themselves to 8-bit (NF8). Please see the Table below for the results, where the numbers other than LQ-LoRA are from OmniQuant [1]. Note that we use the term \"effective bits\" to denote bits per parameter that treat the LoRA components as \"overheads\" for comparing with methods without LoRA; LoRA components (with NF-8 quantization) amount to about 0.1-0.2 extra bits per parameter.\n\n| Base Model | Method                           | Effective Bits (7B, 70B) | C4 (7B) | C4 (70B) | WikiText2 (7B) | WikiText2 (70B) |\n| -------- | ---------------------------------- | ----------- | ---- | ---- | ---- | ---- |\n| LLaMA-2 | Dense                               | 16          | 6.97 | 5.52 | 5.47 | 3.31 |\n| LLaMA-2 | RTN                                 | 3.15        | 8.40 | 6.02 | 6.66 | 3.97 |\n| LLaMA-2 | GPTQ            | 3.15        | 7.89 | 5.85 | 6.29 | 3.85 |\n| LLaMA-2 | AWQ               | 3.15        | 7.84 | -    | 6.24 | -    |\n| LLaMA-2 | OmniQuant  | 3.15        | 7.75 | 5.85 | 6.03 | 3.78 |\n| LLaMA-2 | LQ-LoRA (Fisher)   | 2.95, 2.85 | 7.60 | 5.88 | 5.67 | 3.65 |\n\n[1] https://arxiv.org/abs/2308.13137\n\nThese results indicate that LQ-LoRA can also be used as a state-of-the-art PTQ method. Given the promise of LQ-LoRA as a potential alternative to PTQ, we will include this comparison against PTQ results in the next iteration of the paper.\n\n**ILP for Rank Selection**\n\nThis is a very good/interesting suggestion! We have looked into this by ILP-searching the bits and ranks together. One practical challenge is measuring the \"storages/errors\"  of the ranks of low-rank components. This is important to trade off bits of the quantized matrices and ranks of LoRA. Notice that LoRA components will be fine-tuned; searching the ranks using errors at initialization will unnecessarily favor more bits at the cost of lower ranks. We might be able to formulate this as a bi-level optimization problem (first searching the ranks and then searching the bits), but we will leave this as future work.\n\n**Questions**\n\n*Is the only difference between QLoRA+ILP, and LQ-LoRA, the initialization?*\n\nYep! \n\n*Does the ILP budget, as well as the \"bits per param\" column, also consider the low-rank components?*\n\nThis table does not take into account low-rank components since we use rank = 64 for our QLoRA and LREC (GPTQ + LoRA) baselines. However, the PTQ table above **does** take into account the contribution from the low-rank components to ensure fair comparison against other PTQ methods. We find that the LoRA components (if quantized) add about ~0.2 bits. We will clarify this further!\n\n**Suggestions**\n\nThanks for the suggestions regarding the figures and tables! We will take these into account for the final version of the paper.\n\n---\n\nWe thank the reviewer for their comments/questions. Please find our responses below.\n\n**Mixed Precision & Hardware**\n\nWe note that our work is in the **weight only** quantization regime (as are QLoRA, GPTQ, etc.), where only the weights (and not the activations) are quantized to lower bits. The actual matmul is done in full precision after dequantization. \n\nConcretely, we use just-in-time de-quantize the (quantized) matrix $\\mathbf{Q}$, execute matrix operations (e.g., `matmul(X, Q) ==> matmul(X, dequantize(Q))`), and throw away the de-quantized matrix. This supports arbitrary quantization, and the main technical difficulty is how to do de-quantization quickly. We outlined more details in the appendix.\n\nAlthough weight-only quantization cannot make use of (faster and more energy-efficient) lower-precision matmuls, weight-only quantization still has two practical benefits. First, at a macro level, having a smaller model (due to quantization) allows us to increase the batch size. This reduces the need for expensive data/model parallelism, which requires cross-device or even cross-node communication. Second, at a micro level, smaller quantized matrices allow us to reduce data movement between SRAM and HBM of the GPU. As an example, QLoRA's matrix multiplication implementation could be faster than dense matrix multiplication [1].\n\n[1] https://github.com/TimDettmers/bitsandbytes/releases\n\n**Related paper on Joint Low-rank and Quantized Matrix Decomposition**\n\nThanks for the suggestion; this is an interesting paper! We will discuss this paper in the related works section.\n\nWe primarily considered the cases in which $\\mathbf{L}_1$ and $\\mathbf{L}_2$ are floating-point parameters. This is necessary because we want to fine-tune these parameters. That being said, the techniques introduced in that paper paper might be helpful for other (future) use cases. For example, we could post-training merge $\\mathbf{Q}, \\mathbf{L}_1, \\mathbf{L}_2$, and re-decompose them into three separate quantized matrices for inference.\n\n**Question 1**\n\nYou are correct -- we will clarify this; thanks!\n\n**Question 2**\n\nYou are correct in that the techniques in this paper are not specific to NF. QLoRA paper demonstrated the superior quality of such a scheme compared to classical integer quantization. Our preliminary experiments confirmed this, and hence, we chose NF.\n\nWe conducted the following experiment to be more concrete about NF vs INT. We ran LQ (i.e., Section 3.1) on LLaMA-2 7B's dense parameters with `rank=64` and NF3/INT3/NF4/INT4 quantization configuration (i.e., `b0=3/4, b1=8, b2=fp32, B0=64, B1=256`), and measure the reconstruction errors (i.e., $||\\mathbf{W} - (\\mathbf{Q} + \\mathbf{L}_1 \\mathbf{L}_2) ||_2^2$). The table below shows that NF indeed performed better.\n\n| Method      | Reconstruction error (unit: $10^4$) |\n| ----------- | ----------- |\n| LQ with NF3      | 7.12       |\n| LQ with INT3     | 11.6        |\n| LQ with NF4      | 1.42       |\n| LQ with INT4     | 2.02        |\n\n**Question 3**\n\nThis is the same $\\delta$ in QLoRA, and we treated it as a constant for all settings. We will clarify further in the next iteration of the paper.\n\n**Question 4**\n\nWe _defined_ $\\mathbb{Q}$ as the set of matrices that can be quantized (Equation 1 is a minimization problem, not an exact decomposition problem). Thanks for pointing out the confusion; we will clarify this.\n\n**Question 5**\n\nThe middle figure represents the setting of many (Q)LoRA methods, in which the Low-Rank components are initialized to be zero. In that sense, $\\operatorname{quantize}(\\mathbf{W}) + \\mathbf{L}_1 \\mathbf{L}_2 = \\operatorname{quantize}(\\mathbf{W})$ for those methods.\n\n**Question 6**\n\nGood catch -- it's meant for \"budget\" but shared similar symbols with $B_0$ and $B_1$.\n\n**Question 7**\n\nWe say a matrix $\\mathbf{F}$ has homogenous rows or columns when either rows or columns of the matrix have identical values. We will clarify this.\n\n**Question 8**\n\nTable 4 used the quantization configuration of NF3/NF4. Specifically, they use the block sizes and second-level quantization as the original NF4, but with the first-level quantization set to 3/4-bit. Table 2, however, used ILP to search for the quantization configuration, hence the difference.\n\nWe made such decisions for Table 4 to remove the possible confounding variable of using ILP to search for the quantization configuration (i.e., we just wanted to see the effect of higher rank LoRA components). We will clarify!", "author_response": "Thanks to the reviewer for the comments! Please see our responses below.\n\n**Latency of The Iterative Algorithm**\n\nThis is a one-time cost at the initialization and takes a fraction of the total training time. Each step of the iterative algorithm consists of SVD followed by quantization. Note that we use randomized SVD (rSVD) for speed consideration, and we run the algorithm for up to 100 steps (and we stop before this if the error starts to increase).\n\nTo be more concrete, we measured the runtime to process three of the LLaMA-2 7B's matrices with `rank=64` and NF3 quantization. Please see the table below.\n\n| Data-Aware (i.e., Section 3.3) | Time / Matrix |\n| ----- | ----- |\n| No  | 1.3 to 2.1 seconds |\n| Yes | 1.3 to 2.3 seconds |\n\n**Comparison with GPTQ-based Methods**\n\nThanks for the question! We actually did implement and compare against GPTQ-LoRA. We called this \"LREC\" in the paper following this paper (https://arxiv.org/pdf/2306.08162.pdf). We find that GPTQ-LoRA underperforms both QLoRA and LQ-LoRA (Table 1).\n\n(We now realize that this naming caused some confusion. Thanks for pointing that out --- we will clarify this in the paper.)\n\n**Limitations from QLoRA**\n\nThis is a very good point! But we want to point out that inference with quantized + low-rank is not necessarily slower, especially in batch = 1 setting. Smaller quantized matrices allow us to reduce data movement between SRAM and HBM of the GPU. For example, in the latest releases of `bitsandbytes` (the low-level engine behind QLoRA) [1], they claimed their matrix-vector multiplication between quantized matrix and dense vector (the one needed during inference) could be competitive and even outperformed dense matrix-vector multiplication. \n\n[1] https://github.com/TimDettmers/bitsandbytes/releases\n\n*Integrating LoRA Layers Into Linear Layers*\n\nGiven the potential inference speed-ups, we think that practitioners could use the \"quantized + low-rank\" models without merging. Such models could be just as fast (as discussed above) as merged dense models and more memory-efficient. In that regard, you could (loosely speaking) consider this as an alternative PTQ method.\n\nBased on this potential of LQ-LoRA as an alternative to PTQ, we performed additional experiments by using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. Specifically, we train the LQ-LoRA (Fisher, 2.75-bit, 64-rank) for one epoch on two C4 partitions and WikiText-2. We then further quantize the LoRA components themselves to 8-bit (NF8). Please see the Table below for the results, where the numbers other than LQ-LoRA are from OmniQuant [1]. Note that we use the term \"effective bits\" to denote bits per parameter that treat the LoRA components as \"overheads\" for comparing with methods without LoRA; LoRA components (with NF-8 quantization) amount to about 0.1-0.2 extra bits per parameter.\n\n| Base Model | Method                           | Effective Bits (7B, 70B) | C4 (7B) | C4 (70B) | WikiText2 (7B) | WikiText2 (70B) |\n| -------- | ---------------------------------- | ----------- | ---- | ---- | ---- | ---- |\n| LLaMA-2 | Dense                               | 16          | 6.97 | 5.52 | 5.47 | 3.31 |\n| LLaMA-2 | RTN                                 | 3.15        | 8.40 | 6.02 | 6.66 | 3.97 |\n| LLaMA-2 | GPTQ            | 3.15        | 7.89 | 5.85 | 6.29 | 3.85 |\n| LLaMA-2 | AWQ               | 3.15        | 7.84 | -    | 6.24 | -    |\n| LLaMA-2 | OmniQuant  | 3.15        | 7.75 | 5.85 | 6.03 | 3.78 |\n| LLaMA-2 | LQ-LoRA (Fisher)   | 2.95, 2.85 | 7.60 | 5.88 | 5.67 | 3.65 |\n\n[1] https://arxiv.org/abs/2308.13137\n\nThese results indicate that LQ-LoRA can also be used as a state-of-the-art PTQ method. Given the promise of LQ-LoRA as a potential alternative to PTQ, we will include this comparison against PTQ results in the next iteration of the paper.\n\n\n**Novelty**\n\n*Similarly, I would like to raise a query about the paper's novelty. While this method undeniably enhances the current approach (Q-LoRA), from a PEFT perspective, there could be superior methods, particularly concerning inference challenges. On the topic of novelty, I await the insights of fellow reviewers.*\n\nWe want to reiterate three main contributions on top of QLoRA:\n1. We adopt a matrix decomposition view of initializing the quantized and low-rank components of LoRA adaptation. (Note that QLoRA does not perform explicit matrix decomposition)\n2. Our ILP formulation enables us to search for the mixed-configuration quantization that fits specific resource requirements (e.g., 2.75 bits per parameter).\n3. We extended the two methods above to a data-aware setting by incorporating the sensitivities of parameters."}
{"claim": "The paper's novelty is unclear, appearing as an incremental refinement of Q-LoRA rather than a distinct, original PEFT contribution.", "claim_type": "novelty", "paper_id": "xw29VvOMmU", "paper_title": "LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Bm88GSraZd", "reviewer": "Reviewer_CSBT", "review_text": "Summary: This paper proposes LQ-LoRA, a memory-efficient LLM adaptation method that decomposes each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. The algorithm is adapted from QLoRA and applied modification to solve the problem that zero initialization of the low-rank matrix may not be optimal when the fixed matrix is quantized. The method decomposes the matrix by an iterative algorithm and updates only the low-rank matrix weights during fine-tuning. Results showed that the proposed method outperforms QLoRA and LREC with similar bit compression rates.\n\nStrengths: -\tThe proposed method decomposes the pretrained matrix into a quantizable fixed matrix and low-rank matrix that is already optimized before fine-tuning starts, which contributes to improved accuracy.\n-\tThe paper shows that LQ-LoRA can be used as a mixed quantization strategy, and also proposes a data-aware version of the algorithm, which enables users to flexibly set a target memory budget.\n-\tResults show that the proposed method can be generalized to different model families by showing outperforming results with RoBERTa and LLaMA.\n\nWeaknesses: - The authors have introduced a method that employs an iterative algorithm for initialization. Can they provide insights regarding the computational latency associated with their approach?\n\n- The authors assert the efficiency of LQ-LoRA based on empirical evidence, yet lack theoretical backing. To strengthen the credibility of the algorithm, a comparison might be beneficial, especially with methods that initialize the Q(W) + L1L2 matrix in a manner that closely mirrors the original pretrained matrix W. Consider, for instance, the use of GPTQ as a compensatory mechanism.\n\n- It appears that this paper serves as an expanded or refined rendition of the Q-LoRA paper. As such, it seemingly inherits the same limitation, notably the inference overhead, given that this approach must fail to integrate the LoRA layer into an existing linear layer. \n\n- Similarly, I would like to raise a query about the paper's novelty. While this method undeniably enhances the current approach (Q-LoRA), from a PEFT perspective, there could be superior methods, particularly concerning inference challenges. On the topic of novelty, I await the insights of fellow reviewers.\n\nQuestions: Included in the weakness.", "labeling_timestamp": "2026-01-11T16:27:24.566777", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors explicitly defend the paper's novelty by listing three contributions beyond QLoRA and providing experimental evidence, but they also concede overlap by acknowledging that in one view the only difference is the initialization. Thus they partially refute the claim while admitting some similarity.", "evidence": "\"We want to reiterate three main contributions on top of QLoRA:\n1. We adopt a matrix decomposition view of initializing the quantized and low-rank components of LoRA adaptation. (Note that QLoRA does not perform explicit matrix decomposition)\n2. Our ILP formulation enables us to search for the mixed-configuration quantization that fits specific resource requirements (e.g., 2.75 bits per parameter).\n3. We extended the two methods above to a data-aware setting by incorporating the sensitivities of parameters.\" \n\n\"Is the only difference between QLoRA+ILP, and LQ-LoRA, the initialization?\nYep!\"", "combined_author_response": "Thank you for the discussion! Please see our responses below.\n\n_1. The accuracy of LREC-4bit on MMLU almost matches that of LQ-LoRA_\n\nFor both LLaMA-2 7B and 70B models, LQ-LoRA (Fisher) with 3.25 bits can match the performance of LREC/GPTQ-LoRA 4-bits (i.e., 4.156-bits). Similarly, LQ-LoRA (Fisher) with 2.5/2.75 bits can match the performance of LREC/GPTQ-LoRA 3-bits (i.e., 3.148-bits). Please see the table below. \n\n| Method | Bits per param | MMLU (70B) | MMLU (7B) |\n| ----- | ----- | ----- | ----- |\n| GPTQ-LoRA 3-bit |  3.148 | 0.67$^\\dagger$ | 0.39 |\n| LQ-LoRA (Fisher) | 2.50 | 0.67 | 0.39 |\n| LQ-LoRA (Fisher) | 2.75 | 0.67 | 0.43 |\n\n| Method | Bits per param | MMLU (70B) | MMLU (7B) |\n| ----- | ----- | ----- | ----- |\n| LREC-4 bit | 4.156 | 0.69 | 0.45 |\n| LQ-LoRA (Fisher) | 3.25 | 0.69 | 0.46 |\n\n$^\\dagger$Note that we additionally filled in the MMLU 70B performance for GPTQ-LoRA 3-bit; this number was absent from the initial submission. Based on the above, we conclude that LQ-LoRA can meaningfully improve upon GPTQ-LoRA and QLoRA.\n\n_2. The authors compare their method with GPTQ-LoRA only_\n\nBesides GPTQ-LoRA, we also compared our method QLoRA and different variants of LQ-LoRA. In addition, our latest PTQ experiments included comparisons with RTN, GPTQ, AWQ, and OmniQuant. For details, please see \"Additional Post Training Quantization Experiments\" in our earlier response.\n\n_3. BRECQ and FlexRound_\n\nThanks for the suggestions! We think applying other quantization techniques is certainly interesting, but we would like to raise three points.\n\na) For quantizing large language models with 10B+ parameters, our understanding is that  NF quantization (from QLoRA) and GPTQ --- methods we compared with --- are near state-of-the-art quantization methods. (While we show numbers from other methods, such as Omniquant, in the PTQ comparison table above, these are, as far as we know, preprints).\n\nb) Applying fancier quantization techniques such as BRECQ/Adaround to large models is challenging. For example, for BRECQ,  the OmniQuant paper [1] notes,\n> ... BRECQ ... cannot be applied in models with billions of parameters because they are hard to optimize due to the huge solution space.\n\nSimilarly, the SpQR paper [2] stated that,\n\n> ... BRECQ ... were designed for vision models or small-scale language models, with less than 100M parameters.\n\n[1] https://arxiv.org/abs/2308.13137\n\n[2] https://arxiv.org/abs/2306.03078\n\nFlexRound is an interesting suggestion, but we were unable to find an open-source implementation of this. Hence, we focused our comparison on widely-used LLM quantization methods with open-source code, i.e., QLoRA and GPTQ-LoRA.\n\nc) Finally, (and most importantly), we note that LQ-LoRA can work with generic quantization approaches! For example, during each step of the iterative algorithm, instead of using NF-quantization, we can use other quantization methods to minimize $\\Vert \\mathbf{X}\\mathbf{W} - \\mathbf{X}(\\operatorname{quantize}(\\mathbf{W}) + \\mathbf{L}_1 \\mathbf{L}_2) \\Vert$. We focused on NF quantization because the quantization function is extremely quick (on the order of seconds), and it performs on par with GPTQ despite being data-agnostic. But we think it's worth highlighting this aspect of LQ-LoRA more, and we will discuss this further.\n\n---\n\nHi there! Please let us know if the above response answered some of your questions in the original review, and please let us know if you have follow-up questions!\n\nIn particular, we want to highlight a few key takeaways from our response:\n1. We included a baseline that uses OPTQ/GPTQ to quantize first before learning low-rank updates. We called this \"GPTQ-LoRA\" baseline \"LREC\" in the paper.\n2. We performed additional experiments using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. (Please see the above response for more details.)\n3. The techniques in this paper are not specific to NF. We chose NF because of its superior quality. To be more concrete about this, we conducted additional experiments. (Please see the above response for more details, too.)\n\n---\n\nHi there! Please let us know if the above response answered some of your questions in the original review, and please let us know if you have follow-up questions!\n\nIn particular, we want to highlight a few key takeaways from our response:\n1. The iterative algorithm adds a small extra computation head --- a fraction of the total training time.\n2. We included GPTQ-LoRA comparisons but referred to them as \"LREC\" in the paper.\n3. Inference with quantized + low-rank is not necessarily slower, especially in batch = 1 setting (per the latest releases from QLoRA; kudos to them).\n4. Practitioners could use the \"quantized + low-rank\" models without merging. Such models could be just as fast as merged dense models and more memory-efficient. Based on this potential, we performed additional experiments by using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset.\n\n---\n\nThanks for the comments --- please see our responses below.\n\n**Comparison with methods that use the data $X$ to quantize**\n\nThanks for the question! We actually did implement and compare against a baseline that uses OPTQ/GPTQ to quantize first before learning low-rank updates. In particular, we use GPTQ to first quantize the model and then learn low-rank updates. We called this \"GPTQ-LoRA\" baseline \"LREC\" in the paper following this paper (https://arxiv.org/pdf/2306.08162.pdf). We find that GPTQ-LoRA underperforms both QLoRA and LQ-LoRA (Table 1).\n\n(We now realize that this naming caused some confusion. Thanks for pointing that out --- we will clarify this in the paper.)\n\n\n**Additional Post Training Quantization Experiments**\n\nIn addition to the GPTQ-LoRA experiments already in the paper, we performed additional experiments using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. Specifically, we train the LQ-LoRA (Fisher, 2.75-bit, 64-rank) for one epoch on two C4 partitions and WikiText-2. We then further quantize the LoRA components themselves to 8-bit (NF8). Please see the Table below for the results, where the numbers other than LQ-LoRA are from OmniQuant [1]. Note that we use the term \"effective bits\" to denote bits per parameter that treat the LoRA components as \"overheads\" for comparing with methods without LoRA; LoRA components (with NF-8 quantization) amount to about 0.1-0.2 extra bits per parameter.\n\n\n\n| Base Model | Method                           | Effective Bits (7B, 70B) | C4 (7B) | C4 (70B) | WikiText2 (7B) | WikiText2 (70B) |\n| -------- | ---------------------------------- | ----------- | ---- | ---- | ---- | ---- |\n| LLaMA-2 | Dense                               | 16          | 6.97 | 5.52 | 5.47 | 3.31 |\n| LLaMA-2 | RTN                                 | 3.15        | 8.40 | 6.02 | 6.66 | 3.97 |\n| LLaMA-2 | GPTQ            | 3.15        | 7.89 | 5.85 | 6.29 | 3.85 |\n| LLaMA-2 | AWQ               | 3.15        | 7.84 | -    | 6.24 | -    |\n| LLaMA-2 | OmniQuant  | 3.15        | 7.75 | 5.85 | 6.03 | 3.78 |\n| LLaMA-2 | LQ-LoRA (Fisher)   | 2.95, 2.85 | 7.60 | 5.88 | 5.67 | 3.65 |\n\n\n\n\n[1] https://arxiv.org/abs/2308.13137\n\nThese results indicate that LQ-LoRA can also be used as a state-of-the-art PTQ method. Given the promise of LQ-LoRA as a potential alternative to PTQ, we will include this comparison against PTQ results in the next iteration of the paper.\n\n\n\n**NF-based vs INT-based Quantization**\n\nYou are correct in that the techniques in this paper are not specific to NF. QLoRA paper demonstrated the superior quality of such a scheme compared to classical integer quantization. Our preliminary experiments confirmed this, and hence, we chose NF.\n\nWe conducted the following experiment to be more concrete about NF vs INT. We ran LQ (i.e., Section 3.1) on LLaMA-2 7B's dense parameters with `rank=64` and NF3/INT3/NF4/INT4 quantization configuration (i.e., `b0=3/4, b1=8, b2=fp32, B0=64, B1=256`), and measure the reconstruction errors (i.e., $||\\mathbf{W} - (\\mathbf{Q} + \\mathbf{L}_1 \\mathbf{L}_2) ||_2^2$). The table below shows that NF indeed performed better.\n\n| Method      | Reconstruction error (unit: $10^4$) |\n| ----------- | ----------- |\n| LQ with NF3      | 7.12       |\n| LQ with INT3     | 11.6        |\n| LQ with NF4      | 1.42       |\n| LQ with INT4     | 2.02        |\n\n---\n\nThanks to the reviewer for the comments! Please see our responses below.\n\n**Latency of The Iterative Algorithm**\n\nThis is a one-time cost at the initialization and takes a fraction of the total training time. Each step of the iterative algorithm consists of SVD followed by quantization. Note that we use randomized SVD (rSVD) for speed consideration, and we run the algorithm for up to 100 steps (and we stop before this if the error starts to increase).\n\nTo be more concrete, we measured the runtime to process three of the LLaMA-2 7B's matrices with `rank=64` and NF3 quantization. Please see the table below.\n\n| Data-Aware (i.e., Section 3.3) | Time / Matrix |\n| ----- | ----- |\n| No  | 1.3 to 2.1 seconds |\n| Yes | 1.3 to 2.3 seconds |\n\n**Comparison with GPTQ-based Methods**\n\nThanks for the question! We actually did implement and compare against GPTQ-LoRA. We called this \"LREC\" in the paper following this paper (https://arxiv.org/pdf/2306.08162.pdf). We find that GPTQ-LoRA underperforms both QLoRA and LQ-LoRA (Table 1).\n\n(We now realize that this naming caused some confusion. Thanks for pointing that out --- we will clarify this in the paper.)\n\n**Limitations from QLoRA**\n\nThis is a very good point! But we want to point out that inference with quantized + low-rank is not necessarily slower, especially in batch = 1 setting. Smaller quantized matrices allow us to reduce data movement between SRAM and HBM of the GPU. For example, in the latest releases of `bitsandbytes` (the low-level engine behind QLoRA) [1], they claimed their matrix-vector multiplication between quantized matrix and dense vector (the one needed during inference) could be competitive and even outperformed dense matrix-vector multiplication. \n\n[1] https://github.com/TimDettmers/bitsandbytes/releases\n\n*Integrating LoRA Layers Into Linear Layers*\n\nGiven the potential inference speed-ups, we think that practitioners could use the \"quantized + low-rank\" models without merging. Such models could be just as fast (as discussed above) as merged dense models and more memory-efficient. In that regard, you could (loosely speaking) consider this as an alternative PTQ method.\n\nBased on this potential of LQ-LoRA as an alternative to PTQ, we performed additional experiments by using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. Specifically, we train the LQ-LoRA (Fisher, 2.75-bit, 64-rank) for one epoch on two C4 partitions and WikiText-2. We then further quantize the LoRA components themselves to 8-bit (NF8). Please see the Table below for the results, where the numbers other than LQ-LoRA are from OmniQuant [1]. Note that we use the term \"effective bits\" to denote bits per parameter that treat the LoRA components as \"overheads\" for comparing with methods without LoRA; LoRA components (with NF-8 quantization) amount to about 0.1-0.2 extra bits per parameter.\n\n| Base Model | Method                           | Effective Bits (7B, 70B) | C4 (7B) | C4 (70B) | WikiText2 (7B) | WikiText2 (70B) |\n| -------- | ---------------------------------- | ----------- | ---- | ---- | ---- | ---- |\n| LLaMA-2 | Dense                               | 16          | 6.97 | 5.52 | 5.47 | 3.31 |\n| LLaMA-2 | RTN                                 | 3.15        | 8.40 | 6.02 | 6.66 | 3.97 |\n| LLaMA-2 | GPTQ            | 3.15        | 7.89 | 5.85 | 6.29 | 3.85 |\n| LLaMA-2 | AWQ               | 3.15        | 7.84 | -    | 6.24 | -    |\n| LLaMA-2 | OmniQuant  | 3.15        | 7.75 | 5.85 | 6.03 | 3.78 |\n| LLaMA-2 | LQ-LoRA (Fisher)   | 2.95, 2.85 | 7.60 | 5.88 | 5.67 | 3.65 |\n\n[1] https://arxiv.org/abs/2308.13137\n\nThese results indicate that LQ-LoRA can also be used as a state-of-the-art PTQ method. Given the promise of LQ-LoRA as a potential alternative to PTQ, we will include this comparison against PTQ results in the next iteration of the paper.\n\n\n**Novelty**\n\n*Similarly, I would like to raise a query about the paper's novelty. While this method undeniably enhances the current approach (Q-LoRA), from a PEFT perspective, there could be superior methods, particularly concerning inference challenges. On the topic of novelty, I await the insights of fellow reviewers.*\n\nWe want to reiterate three main contributions on top of QLoRA:\n1. We adopt a matrix decomposition view of initializing the quantized and low-rank components of LoRA adaptation. (Note that QLoRA does not perform explicit matrix decomposition)\n2. Our ILP formulation enables us to search for the mixed-configuration quantization that fits specific resource requirements (e.g., 2.75 bits per parameter).\n3. We extended the two methods above to a data-aware setting by incorporating the sensitivities of parameters.\n\n---\n\nThanks for the helpful review! Please take a look at our responses below.\n\n**Novelty with respect to QLoRA**\n\nWe want to note that QLoRA does not perform explicit matrix decomposition. It performs quantization of the original weight matrix and learns additive low-rank updates. In contrast, we perform explicit matrix decomposition of the original matrix into low-rank and quantized components via a matrix reconstruction objective.\n\n**Ablations**\n\n*How important is the ILP to LQ-LoRA? Can you show the performance of LQ-LoRA without the ILP?*\n\nOne of the main usages of ILP is to flexibly quantize the model to meet specific memory constraints (e.g., 2.75 bits/parameter). That being said, LQ-LoRA is effective even without the ILP. For example, Table 4 shows LQ-LoRA without ILP. Here, we use NF3 quantization configuration --- NF3 reuses the configuration of the QLoRA's NF4 quantization with 3-bit first-level quantization. \n\n*Can you show the performance of the regular LoRA method (no quantization), and also quantization (at different bit-rates) without LoRA, in Table 2?*\n\nWe want to note that the QLoRA paper already demonstrated little performance loss between regular LoRA and QLoRA (4bit). Hence, we can think of QLoRA (4bit) performance as a proxy for regular LoRA.\n\nHowever, based on your suggestion/question, we performed additional experiments using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. Specifically, we train the LQ-LoRA (Fisher, 2.75-bit, 64-rank) for one epoch on two C4 partitions and WikiText-2. We then further quantize the LoRA components themselves to 8-bit (NF8). Please see the Table below for the results, where the numbers other than LQ-LoRA are from OmniQuant [1]. Note that we use the term \"effective bits\" to denote bits per parameter that treat the LoRA components as \"overheads\" for comparing with methods without LoRA; LoRA components (with NF-8 quantization) amount to about 0.1-0.2 extra bits per parameter.\n\n| Base Model | Method                           | Effective Bits (7B, 70B) | C4 (7B) | C4 (70B) | WikiText2 (7B) | WikiText2 (70B) |\n| -------- | ---------------------------------- | ----------- | ---- | ---- | ---- | ---- |\n| LLaMA-2 | Dense                               | 16          | 6.97 | 5.52 | 5.47 | 3.31 |\n| LLaMA-2 | RTN                                 | 3.15        | 8.40 | 6.02 | 6.66 | 3.97 |\n| LLaMA-2 | GPTQ            | 3.15        | 7.89 | 5.85 | 6.29 | 3.85 |\n| LLaMA-2 | AWQ               | 3.15        | 7.84 | -    | 6.24 | -    |\n| LLaMA-2 | OmniQuant  | 3.15        | 7.75 | 5.85 | 6.03 | 3.78 |\n| LLaMA-2 | LQ-LoRA (Fisher)   | 2.95, 2.85 | 7.60 | 5.88 | 5.67 | 3.65 |\n\n[1] https://arxiv.org/abs/2308.13137\n\nThese results indicate that LQ-LoRA can also be used as a state-of-the-art PTQ method. Given the promise of LQ-LoRA as a potential alternative to PTQ, we will include this comparison against PTQ results in the next iteration of the paper.\n\n**ILP for Rank Selection**\n\nThis is a very good/interesting suggestion! We have looked into this by ILP-searching the bits and ranks together. One practical challenge is measuring the \"storages/errors\"  of the ranks of low-rank components. This is important to trade off bits of the quantized matrices and ranks of LoRA. Notice that LoRA components will be fine-tuned; searching the ranks using errors at initialization will unnecessarily favor more bits at the cost of lower ranks. We might be able to formulate this as a bi-level optimization problem (first searching the ranks and then searching the bits), but we will leave this as future work.\n\n**Questions**\n\n*Is the only difference between QLoRA+ILP, and LQ-LoRA, the initialization?*\n\nYep! \n\n*Does the ILP budget, as well as the \"bits per param\" column, also consider the low-rank components?*\n\nThis table does not take into account low-rank components since we use rank = 64 for our QLoRA and LREC (GPTQ + LoRA) baselines. However, the PTQ table above **does** take into account the contribution from the low-rank components to ensure fair comparison against other PTQ methods. We find that the LoRA components (if quantized) add about ~0.2 bits. We will clarify this further!\n\n**Suggestions**\n\nThanks for the suggestions regarding the figures and tables! We will take these into account for the final version of the paper.\n\n---\n\nWe thank the reviewer for their comments/questions. Please find our responses below.\n\n**Mixed Precision & Hardware**\n\nWe note that our work is in the **weight only** quantization regime (as are QLoRA, GPTQ, etc.), where only the weights (and not the activations) are quantized to lower bits. The actual matmul is done in full precision after dequantization. \n\nConcretely, we use just-in-time de-quantize the (quantized) matrix $\\mathbf{Q}$, execute matrix operations (e.g., `matmul(X, Q) ==> matmul(X, dequantize(Q))`), and throw away the de-quantized matrix. This supports arbitrary quantization, and the main technical difficulty is how to do de-quantization quickly. We outlined more details in the appendix.\n\nAlthough weight-only quantization cannot make use of (faster and more energy-efficient) lower-precision matmuls, weight-only quantization still has two practical benefits. First, at a macro level, having a smaller model (due to quantization) allows us to increase the batch size. This reduces the need for expensive data/model parallelism, which requires cross-device or even cross-node communication. Second, at a micro level, smaller quantized matrices allow us to reduce data movement between SRAM and HBM of the GPU. As an example, QLoRA's matrix multiplication implementation could be faster than dense matrix multiplication [1].\n\n[1] https://github.com/TimDettmers/bitsandbytes/releases\n\n**Related paper on Joint Low-rank and Quantized Matrix Decomposition**\n\nThanks for the suggestion; this is an interesting paper! We will discuss this paper in the related works section.\n\nWe primarily considered the cases in which $\\mathbf{L}_1$ and $\\mathbf{L}_2$ are floating-point parameters. This is necessary because we want to fine-tune these parameters. That being said, the techniques introduced in that paper paper might be helpful for other (future) use cases. For example, we could post-training merge $\\mathbf{Q}, \\mathbf{L}_1, \\mathbf{L}_2$, and re-decompose them into three separate quantized matrices for inference.\n\n**Question 1**\n\nYou are correct -- we will clarify this; thanks!\n\n**Question 2**\n\nYou are correct in that the techniques in this paper are not specific to NF. QLoRA paper demonstrated the superior quality of such a scheme compared to classical integer quantization. Our preliminary experiments confirmed this, and hence, we chose NF.\n\nWe conducted the following experiment to be more concrete about NF vs INT. We ran LQ (i.e., Section 3.1) on LLaMA-2 7B's dense parameters with `rank=64` and NF3/INT3/NF4/INT4 quantization configuration (i.e., `b0=3/4, b1=8, b2=fp32, B0=64, B1=256`), and measure the reconstruction errors (i.e., $||\\mathbf{W} - (\\mathbf{Q} + \\mathbf{L}_1 \\mathbf{L}_2) ||_2^2$). The table below shows that NF indeed performed better.\n\n| Method      | Reconstruction error (unit: $10^4$) |\n| ----------- | ----------- |\n| LQ with NF3      | 7.12       |\n| LQ with INT3     | 11.6        |\n| LQ with NF4      | 1.42       |\n| LQ with INT4     | 2.02        |\n\n**Question 3**\n\nThis is the same $\\delta$ in QLoRA, and we treated it as a constant for all settings. We will clarify further in the next iteration of the paper.\n\n**Question 4**\n\nWe _defined_ $\\mathbb{Q}$ as the set of matrices that can be quantized (Equation 1 is a minimization problem, not an exact decomposition problem). Thanks for pointing out the confusion; we will clarify this.\n\n**Question 5**\n\nThe middle figure represents the setting of many (Q)LoRA methods, in which the Low-Rank components are initialized to be zero. In that sense, $\\operatorname{quantize}(\\mathbf{W}) + \\mathbf{L}_1 \\mathbf{L}_2 = \\operatorname{quantize}(\\mathbf{W})$ for those methods.\n\n**Question 6**\n\nGood catch -- it's meant for \"budget\" but shared similar symbols with $B_0$ and $B_1$.\n\n**Question 7**\n\nWe say a matrix $\\mathbf{F}$ has homogenous rows or columns when either rows or columns of the matrix have identical values. We will clarify this.\n\n**Question 8**\n\nTable 4 used the quantization configuration of NF3/NF4. Specifically, they use the block sizes and second-level quantization as the original NF4, but with the first-level quantization set to 3/4-bit. Table 2, however, used ILP to search for the quantization configuration, hence the difference.\n\nWe made such decisions for Table 4 to remove the possible confounding variable of using ILP to search for the quantization configuration (i.e., we just wanted to see the effect of higher rank LoRA components). We will clarify!", "author_response": "Thanks to the reviewer for the comments! Please see our responses below.\n\n**Latency of The Iterative Algorithm**\n\nThis is a one-time cost at the initialization and takes a fraction of the total training time. Each step of the iterative algorithm consists of SVD followed by quantization. Note that we use randomized SVD (rSVD) for speed consideration, and we run the algorithm for up to 100 steps (and we stop before this if the error starts to increase).\n\nTo be more concrete, we measured the runtime to process three of the LLaMA-2 7B's matrices with `rank=64` and NF3 quantization. Please see the table below.\n\n| Data-Aware (i.e., Section 3.3) | Time / Matrix |\n| ----- | ----- |\n| No  | 1.3 to 2.1 seconds |\n| Yes | 1.3 to 2.3 seconds |\n\n**Comparison with GPTQ-based Methods**\n\nThanks for the question! We actually did implement and compare against GPTQ-LoRA. We called this \"LREC\" in the paper following this paper (https://arxiv.org/pdf/2306.08162.pdf). We find that GPTQ-LoRA underperforms both QLoRA and LQ-LoRA (Table 1).\n\n(We now realize that this naming caused some confusion. Thanks for pointing that out --- we will clarify this in the paper.)\n\n**Limitations from QLoRA**\n\nThis is a very good point! But we want to point out that inference with quantized + low-rank is not necessarily slower, especially in batch = 1 setting. Smaller quantized matrices allow us to reduce data movement between SRAM and HBM of the GPU. For example, in the latest releases of `bitsandbytes` (the low-level engine behind QLoRA) [1], they claimed their matrix-vector multiplication between quantized matrix and dense vector (the one needed during inference) could be competitive and even outperformed dense matrix-vector multiplication. \n\n[1] https://github.com/TimDettmers/bitsandbytes/releases\n\n*Integrating LoRA Layers Into Linear Layers*\n\nGiven the potential inference speed-ups, we think that practitioners could use the \"quantized + low-rank\" models without merging. Such models could be just as fast (as discussed above) as merged dense models and more memory-efficient. In that regard, you could (loosely speaking) consider this as an alternative PTQ method.\n\nBased on this potential of LQ-LoRA as an alternative to PTQ, we performed additional experiments by using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. Specifically, we train the LQ-LoRA (Fisher, 2.75-bit, 64-rank) for one epoch on two C4 partitions and WikiText-2. We then further quantize the LoRA components themselves to 8-bit (NF8). Please see the Table below for the results, where the numbers other than LQ-LoRA are from OmniQuant [1]. Note that we use the term \"effective bits\" to denote bits per parameter that treat the LoRA components as \"overheads\" for comparing with methods without LoRA; LoRA components (with NF-8 quantization) amount to about 0.1-0.2 extra bits per parameter.\n\n| Base Model | Method                           | Effective Bits (7B, 70B) | C4 (7B) | C4 (70B) | WikiText2 (7B) | WikiText2 (70B) |\n| -------- | ---------------------------------- | ----------- | ---- | ---- | ---- | ---- |\n| LLaMA-2 | Dense                               | 16          | 6.97 | 5.52 | 5.47 | 3.31 |\n| LLaMA-2 | RTN                                 | 3.15        | 8.40 | 6.02 | 6.66 | 3.97 |\n| LLaMA-2 | GPTQ            | 3.15        | 7.89 | 5.85 | 6.29 | 3.85 |\n| LLaMA-2 | AWQ               | 3.15        | 7.84 | -    | 6.24 | -    |\n| LLaMA-2 | OmniQuant  | 3.15        | 7.75 | 5.85 | 6.03 | 3.78 |\n| LLaMA-2 | LQ-LoRA (Fisher)   | 2.95, 2.85 | 7.60 | 5.88 | 5.67 | 3.65 |\n\n[1] https://arxiv.org/abs/2308.13137\n\nThese results indicate that LQ-LoRA can also be used as a state-of-the-art PTQ method. Given the promise of LQ-LoRA as a potential alternative to PTQ, we will include this comparison against PTQ results in the next iteration of the paper.\n\n\n**Novelty**\n\n*Similarly, I would like to raise a query about the paper's novelty. While this method undeniably enhances the current approach (Q-LoRA), from a PEFT perspective, there could be superior methods, particularly concerning inference challenges. On the topic of novelty, I await the insights of fellow reviewers.*\n\nWe want to reiterate three main contributions on top of QLoRA:\n1. We adopt a matrix decomposition view of initializing the quantized and low-rank components of LoRA adaptation. (Note that QLoRA does not perform explicit matrix decomposition)\n2. Our ILP formulation enables us to search for the mixed-configuration quantization that fits specific resource requirements (e.g., 2.75 bits per parameter).\n3. We extended the two methods above to a data-aware setting by incorporating the sensitivities of parameters."}
{"claim": "The paper does not compare with other PEFT methods that might better address inference challenges, leaving potentially superior alternatives unexamined.", "claim_type": "baseline", "paper_id": "xw29VvOMmU", "paper_title": "LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Bm88GSraZd", "reviewer": "Reviewer_CSBT", "review_text": "Summary: This paper proposes LQ-LoRA, a memory-efficient LLM adaptation method that decomposes each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. The algorithm is adapted from QLoRA and applied modification to solve the problem that zero initialization of the low-rank matrix may not be optimal when the fixed matrix is quantized. The method decomposes the matrix by an iterative algorithm and updates only the low-rank matrix weights during fine-tuning. Results showed that the proposed method outperforms QLoRA and LREC with similar bit compression rates.\n\nStrengths: -\tThe proposed method decomposes the pretrained matrix into a quantizable fixed matrix and low-rank matrix that is already optimized before fine-tuning starts, which contributes to improved accuracy.\n-\tThe paper shows that LQ-LoRA can be used as a mixed quantization strategy, and also proposes a data-aware version of the algorithm, which enables users to flexibly set a target memory budget.\n-\tResults show that the proposed method can be generalized to different model families by showing outperforming results with RoBERTa and LLaMA.\n\nWeaknesses: - The authors have introduced a method that employs an iterative algorithm for initialization. Can they provide insights regarding the computational latency associated with their approach?\n\n- The authors assert the efficiency of LQ-LoRA based on empirical evidence, yet lack theoretical backing. To strengthen the credibility of the algorithm, a comparison might be beneficial, especially with methods that initialize the Q(W) + L1L2 matrix in a manner that closely mirrors the original pretrained matrix W. Consider, for instance, the use of GPTQ as a compensatory mechanism.\n\n- It appears that this paper serves as an expanded or refined rendition of the Q-LoRA paper. As such, it seemingly inherits the same limitation, notably the inference overhead, given that this approach must fail to integrate the LoRA layer into an existing linear layer. \n\n- Similarly, I would like to raise a query about the paper's novelty. While this method undeniably enhances the current approach (Q-LoRA), from a PEFT perspective, there could be superior methods, particularly concerning inference challenges. On the topic of novelty, I await the insights of fellow reviewers.\n\nQuestions: Included in the weakness.", "labeling_timestamp": "2026-01-11T16:27:25.478189", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors respond by adding and clarifying comparisons (e.g., to GPTQ-LoRA, QLoRA, RTN, AWQ, OmniQuant) and explain why some quantization techniques (BRECQ/FlexRound) were not used, but they do not comprehensively compare to other PEFT families beyond LoRA-based approaches nor fully evaluate alternate PEFT techniques specifically targeting inference challenges. Thus the reviewer's concern is only partially addressed.", "evidence": "\"Besides GPTQ-LoRA, we also compared our method QLoRA and different variants of LQ-LoRA. In addition, our latest PTQ experiments included comparisons with RTN, GPTQ, AWQ, and OmniQuant.\"", "combined_author_response": "Thank you for the discussion! Please see our responses below.\n\n_1. The accuracy of LREC-4bit on MMLU almost matches that of LQ-LoRA_\n\nFor both LLaMA-2 7B and 70B models, LQ-LoRA (Fisher) with 3.25 bits can match the performance of LREC/GPTQ-LoRA 4-bits (i.e., 4.156-bits). Similarly, LQ-LoRA (Fisher) with 2.5/2.75 bits can match the performance of LREC/GPTQ-LoRA 3-bits (i.e., 3.148-bits). Please see the table below. \n\n| Method | Bits per param | MMLU (70B) | MMLU (7B) |\n| ----- | ----- | ----- | ----- |\n| GPTQ-LoRA 3-bit |  3.148 | 0.67$^\\dagger$ | 0.39 |\n| LQ-LoRA (Fisher) | 2.50 | 0.67 | 0.39 |\n| LQ-LoRA (Fisher) | 2.75 | 0.67 | 0.43 |\n\n| Method | Bits per param | MMLU (70B) | MMLU (7B) |\n| ----- | ----- | ----- | ----- |\n| LREC-4 bit | 4.156 | 0.69 | 0.45 |\n| LQ-LoRA (Fisher) | 3.25 | 0.69 | 0.46 |\n\n$^\\dagger$Note that we additionally filled in the MMLU 70B performance for GPTQ-LoRA 3-bit; this number was absent from the initial submission. Based on the above, we conclude that LQ-LoRA can meaningfully improve upon GPTQ-LoRA and QLoRA.\n\n_2. The authors compare their method with GPTQ-LoRA only_\n\nBesides GPTQ-LoRA, we also compared our method QLoRA and different variants of LQ-LoRA. In addition, our latest PTQ experiments included comparisons with RTN, GPTQ, AWQ, and OmniQuant. For details, please see \"Additional Post Training Quantization Experiments\" in our earlier response.\n\n_3. BRECQ and FlexRound_\n\nThanks for the suggestions! We think applying other quantization techniques is certainly interesting, but we would like to raise three points.\n\na) For quantizing large language models with 10B+ parameters, our understanding is that  NF quantization (from QLoRA) and GPTQ --- methods we compared with --- are near state-of-the-art quantization methods. (While we show numbers from other methods, such as Omniquant, in the PTQ comparison table above, these are, as far as we know, preprints).\n\nb) Applying fancier quantization techniques such as BRECQ/Adaround to large models is challenging. For example, for BRECQ,  the OmniQuant paper [1] notes,\n> ... BRECQ ... cannot be applied in models with billions of parameters because they are hard to optimize due to the huge solution space.\n\nSimilarly, the SpQR paper [2] stated that,\n\n> ... BRECQ ... were designed for vision models or small-scale language models, with less than 100M parameters.\n\n[1] https://arxiv.org/abs/2308.13137\n\n[2] https://arxiv.org/abs/2306.03078\n\nFlexRound is an interesting suggestion, but we were unable to find an open-source implementation of this. Hence, we focused our comparison on widely-used LLM quantization methods with open-source code, i.e., QLoRA and GPTQ-LoRA.\n\nc) Finally, (and most importantly), we note that LQ-LoRA can work with generic quantization approaches! For example, during each step of the iterative algorithm, instead of using NF-quantization, we can use other quantization methods to minimize $\\Vert \\mathbf{X}\\mathbf{W} - \\mathbf{X}(\\operatorname{quantize}(\\mathbf{W}) + \\mathbf{L}_1 \\mathbf{L}_2) \\Vert$. We focused on NF quantization because the quantization function is extremely quick (on the order of seconds), and it performs on par with GPTQ despite being data-agnostic. But we think it's worth highlighting this aspect of LQ-LoRA more, and we will discuss this further.\n\n---\n\nHi there! Please let us know if the above response answered some of your questions in the original review, and please let us know if you have follow-up questions!\n\nIn particular, we want to highlight a few key takeaways from our response:\n1. We included a baseline that uses OPTQ/GPTQ to quantize first before learning low-rank updates. We called this \"GPTQ-LoRA\" baseline \"LREC\" in the paper.\n2. We performed additional experiments using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. (Please see the above response for more details.)\n3. The techniques in this paper are not specific to NF. We chose NF because of its superior quality. To be more concrete about this, we conducted additional experiments. (Please see the above response for more details, too.)\n\n---\n\nHi there! Please let us know if the above response answered some of your questions in the original review, and please let us know if you have follow-up questions!\n\nIn particular, we want to highlight a few key takeaways from our response:\n1. The iterative algorithm adds a small extra computation head --- a fraction of the total training time.\n2. We included GPTQ-LoRA comparisons but referred to them as \"LREC\" in the paper.\n3. Inference with quantized + low-rank is not necessarily slower, especially in batch = 1 setting (per the latest releases from QLoRA; kudos to them).\n4. Practitioners could use the \"quantized + low-rank\" models without merging. Such models could be just as fast as merged dense models and more memory-efficient. Based on this potential, we performed additional experiments by using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset.\n\n---\n\nThanks for the comments --- please see our responses below.\n\n**Comparison with methods that use the data $X$ to quantize**\n\nThanks for the question! We actually did implement and compare against a baseline that uses OPTQ/GPTQ to quantize first before learning low-rank updates. In particular, we use GPTQ to first quantize the model and then learn low-rank updates. We called this \"GPTQ-LoRA\" baseline \"LREC\" in the paper following this paper (https://arxiv.org/pdf/2306.08162.pdf). We find that GPTQ-LoRA underperforms both QLoRA and LQ-LoRA (Table 1).\n\n(We now realize that this naming caused some confusion. Thanks for pointing that out --- we will clarify this in the paper.)\n\n\n**Additional Post Training Quantization Experiments**\n\nIn addition to the GPTQ-LoRA experiments already in the paper, we performed additional experiments using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. Specifically, we train the LQ-LoRA (Fisher, 2.75-bit, 64-rank) for one epoch on two C4 partitions and WikiText-2. We then further quantize the LoRA components themselves to 8-bit (NF8). Please see the Table below for the results, where the numbers other than LQ-LoRA are from OmniQuant [1]. Note that we use the term \"effective bits\" to denote bits per parameter that treat the LoRA components as \"overheads\" for comparing with methods without LoRA; LoRA components (with NF-8 quantization) amount to about 0.1-0.2 extra bits per parameter.\n\n\n\n| Base Model | Method                           | Effective Bits (7B, 70B) | C4 (7B) | C4 (70B) | WikiText2 (7B) | WikiText2 (70B) |\n| -------- | ---------------------------------- | ----------- | ---- | ---- | ---- | ---- |\n| LLaMA-2 | Dense                               | 16          | 6.97 | 5.52 | 5.47 | 3.31 |\n| LLaMA-2 | RTN                                 | 3.15        | 8.40 | 6.02 | 6.66 | 3.97 |\n| LLaMA-2 | GPTQ            | 3.15        | 7.89 | 5.85 | 6.29 | 3.85 |\n| LLaMA-2 | AWQ               | 3.15        | 7.84 | -    | 6.24 | -    |\n| LLaMA-2 | OmniQuant  | 3.15        | 7.75 | 5.85 | 6.03 | 3.78 |\n| LLaMA-2 | LQ-LoRA (Fisher)   | 2.95, 2.85 | 7.60 | 5.88 | 5.67 | 3.65 |\n\n\n\n\n[1] https://arxiv.org/abs/2308.13137\n\nThese results indicate that LQ-LoRA can also be used as a state-of-the-art PTQ method. Given the promise of LQ-LoRA as a potential alternative to PTQ, we will include this comparison against PTQ results in the next iteration of the paper.\n\n\n\n**NF-based vs INT-based Quantization**\n\nYou are correct in that the techniques in this paper are not specific to NF. QLoRA paper demonstrated the superior quality of such a scheme compared to classical integer quantization. Our preliminary experiments confirmed this, and hence, we chose NF.\n\nWe conducted the following experiment to be more concrete about NF vs INT. We ran LQ (i.e., Section 3.1) on LLaMA-2 7B's dense parameters with `rank=64` and NF3/INT3/NF4/INT4 quantization configuration (i.e., `b0=3/4, b1=8, b2=fp32, B0=64, B1=256`), and measure the reconstruction errors (i.e., $||\\mathbf{W} - (\\mathbf{Q} + \\mathbf{L}_1 \\mathbf{L}_2) ||_2^2$). The table below shows that NF indeed performed better.\n\n| Method      | Reconstruction error (unit: $10^4$) |\n| ----------- | ----------- |\n| LQ with NF3      | 7.12       |\n| LQ with INT3     | 11.6        |\n| LQ with NF4      | 1.42       |\n| LQ with INT4     | 2.02        |\n\n---\n\nThanks to the reviewer for the comments! Please see our responses below.\n\n**Latency of The Iterative Algorithm**\n\nThis is a one-time cost at the initialization and takes a fraction of the total training time. Each step of the iterative algorithm consists of SVD followed by quantization. Note that we use randomized SVD (rSVD) for speed consideration, and we run the algorithm for up to 100 steps (and we stop before this if the error starts to increase).\n\nTo be more concrete, we measured the runtime to process three of the LLaMA-2 7B's matrices with `rank=64` and NF3 quantization. Please see the table below.\n\n| Data-Aware (i.e., Section 3.3) | Time / Matrix |\n| ----- | ----- |\n| No  | 1.3 to 2.1 seconds |\n| Yes | 1.3 to 2.3 seconds |\n\n**Comparison with GPTQ-based Methods**\n\nThanks for the question! We actually did implement and compare against GPTQ-LoRA. We called this \"LREC\" in the paper following this paper (https://arxiv.org/pdf/2306.08162.pdf). We find that GPTQ-LoRA underperforms both QLoRA and LQ-LoRA (Table 1).\n\n(We now realize that this naming caused some confusion. Thanks for pointing that out --- we will clarify this in the paper.)\n\n**Limitations from QLoRA**\n\nThis is a very good point! But we want to point out that inference with quantized + low-rank is not necessarily slower, especially in batch = 1 setting. Smaller quantized matrices allow us to reduce data movement between SRAM and HBM of the GPU. For example, in the latest releases of `bitsandbytes` (the low-level engine behind QLoRA) [1], they claimed their matrix-vector multiplication between quantized matrix and dense vector (the one needed during inference) could be competitive and even outperformed dense matrix-vector multiplication. \n\n[1] https://github.com/TimDettmers/bitsandbytes/releases\n\n*Integrating LoRA Layers Into Linear Layers*\n\nGiven the potential inference speed-ups, we think that practitioners could use the \"quantized + low-rank\" models without merging. Such models could be just as fast (as discussed above) as merged dense models and more memory-efficient. In that regard, you could (loosely speaking) consider this as an alternative PTQ method.\n\nBased on this potential of LQ-LoRA as an alternative to PTQ, we performed additional experiments by using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. Specifically, we train the LQ-LoRA (Fisher, 2.75-bit, 64-rank) for one epoch on two C4 partitions and WikiText-2. We then further quantize the LoRA components themselves to 8-bit (NF8). Please see the Table below for the results, where the numbers other than LQ-LoRA are from OmniQuant [1]. Note that we use the term \"effective bits\" to denote bits per parameter that treat the LoRA components as \"overheads\" for comparing with methods without LoRA; LoRA components (with NF-8 quantization) amount to about 0.1-0.2 extra bits per parameter.\n\n| Base Model | Method                           | Effective Bits (7B, 70B) | C4 (7B) | C4 (70B) | WikiText2 (7B) | WikiText2 (70B) |\n| -------- | ---------------------------------- | ----------- | ---- | ---- | ---- | ---- |\n| LLaMA-2 | Dense                               | 16          | 6.97 | 5.52 | 5.47 | 3.31 |\n| LLaMA-2 | RTN                                 | 3.15        | 8.40 | 6.02 | 6.66 | 3.97 |\n| LLaMA-2 | GPTQ            | 3.15        | 7.89 | 5.85 | 6.29 | 3.85 |\n| LLaMA-2 | AWQ               | 3.15        | 7.84 | -    | 6.24 | -    |\n| LLaMA-2 | OmniQuant  | 3.15        | 7.75 | 5.85 | 6.03 | 3.78 |\n| LLaMA-2 | LQ-LoRA (Fisher)   | 2.95, 2.85 | 7.60 | 5.88 | 5.67 | 3.65 |\n\n[1] https://arxiv.org/abs/2308.13137\n\nThese results indicate that LQ-LoRA can also be used as a state-of-the-art PTQ method. Given the promise of LQ-LoRA as a potential alternative to PTQ, we will include this comparison against PTQ results in the next iteration of the paper.\n\n\n**Novelty**\n\n*Similarly, I would like to raise a query about the paper's novelty. While this method undeniably enhances the current approach (Q-LoRA), from a PEFT perspective, there could be superior methods, particularly concerning inference challenges. On the topic of novelty, I await the insights of fellow reviewers.*\n\nWe want to reiterate three main contributions on top of QLoRA:\n1. We adopt a matrix decomposition view of initializing the quantized and low-rank components of LoRA adaptation. (Note that QLoRA does not perform explicit matrix decomposition)\n2. Our ILP formulation enables us to search for the mixed-configuration quantization that fits specific resource requirements (e.g., 2.75 bits per parameter).\n3. We extended the two methods above to a data-aware setting by incorporating the sensitivities of parameters.\n\n---\n\nThanks for the helpful review! Please take a look at our responses below.\n\n**Novelty with respect to QLoRA**\n\nWe want to note that QLoRA does not perform explicit matrix decomposition. It performs quantization of the original weight matrix and learns additive low-rank updates. In contrast, we perform explicit matrix decomposition of the original matrix into low-rank and quantized components via a matrix reconstruction objective.\n\n**Ablations**\n\n*How important is the ILP to LQ-LoRA? Can you show the performance of LQ-LoRA without the ILP?*\n\nOne of the main usages of ILP is to flexibly quantize the model to meet specific memory constraints (e.g., 2.75 bits/parameter). That being said, LQ-LoRA is effective even without the ILP. For example, Table 4 shows LQ-LoRA without ILP. Here, we use NF3 quantization configuration --- NF3 reuses the configuration of the QLoRA's NF4 quantization with 3-bit first-level quantization. \n\n*Can you show the performance of the regular LoRA method (no quantization), and also quantization (at different bit-rates) without LoRA, in Table 2?*\n\nWe want to note that the QLoRA paper already demonstrated little performance loss between regular LoRA and QLoRA (4bit). Hence, we can think of QLoRA (4bit) performance as a proxy for regular LoRA.\n\nHowever, based on your suggestion/question, we performed additional experiments using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. Specifically, we train the LQ-LoRA (Fisher, 2.75-bit, 64-rank) for one epoch on two C4 partitions and WikiText-2. We then further quantize the LoRA components themselves to 8-bit (NF8). Please see the Table below for the results, where the numbers other than LQ-LoRA are from OmniQuant [1]. Note that we use the term \"effective bits\" to denote bits per parameter that treat the LoRA components as \"overheads\" for comparing with methods without LoRA; LoRA components (with NF-8 quantization) amount to about 0.1-0.2 extra bits per parameter.\n\n| Base Model | Method                           | Effective Bits (7B, 70B) | C4 (7B) | C4 (70B) | WikiText2 (7B) | WikiText2 (70B) |\n| -------- | ---------------------------------- | ----------- | ---- | ---- | ---- | ---- |\n| LLaMA-2 | Dense                               | 16          | 6.97 | 5.52 | 5.47 | 3.31 |\n| LLaMA-2 | RTN                                 | 3.15        | 8.40 | 6.02 | 6.66 | 3.97 |\n| LLaMA-2 | GPTQ            | 3.15        | 7.89 | 5.85 | 6.29 | 3.85 |\n| LLaMA-2 | AWQ               | 3.15        | 7.84 | -    | 6.24 | -    |\n| LLaMA-2 | OmniQuant  | 3.15        | 7.75 | 5.85 | 6.03 | 3.78 |\n| LLaMA-2 | LQ-LoRA (Fisher)   | 2.95, 2.85 | 7.60 | 5.88 | 5.67 | 3.65 |\n\n[1] https://arxiv.org/abs/2308.13137\n\nThese results indicate that LQ-LoRA can also be used as a state-of-the-art PTQ method. Given the promise of LQ-LoRA as a potential alternative to PTQ, we will include this comparison against PTQ results in the next iteration of the paper.\n\n**ILP for Rank Selection**\n\nThis is a very good/interesting suggestion! We have looked into this by ILP-searching the bits and ranks together. One practical challenge is measuring the \"storages/errors\"  of the ranks of low-rank components. This is important to trade off bits of the quantized matrices and ranks of LoRA. Notice that LoRA components will be fine-tuned; searching the ranks using errors at initialization will unnecessarily favor more bits at the cost of lower ranks. We might be able to formulate this as a bi-level optimization problem (first searching the ranks and then searching the bits), but we will leave this as future work.\n\n**Questions**\n\n*Is the only difference between QLoRA+ILP, and LQ-LoRA, the initialization?*\n\nYep! \n\n*Does the ILP budget, as well as the \"bits per param\" column, also consider the low-rank components?*\n\nThis table does not take into account low-rank components since we use rank = 64 for our QLoRA and LREC (GPTQ + LoRA) baselines. However, the PTQ table above **does** take into account the contribution from the low-rank components to ensure fair comparison against other PTQ methods. We find that the LoRA components (if quantized) add about ~0.2 bits. We will clarify this further!\n\n**Suggestions**\n\nThanks for the suggestions regarding the figures and tables! We will take these into account for the final version of the paper.\n\n---\n\nWe thank the reviewer for their comments/questions. Please find our responses below.\n\n**Mixed Precision & Hardware**\n\nWe note that our work is in the **weight only** quantization regime (as are QLoRA, GPTQ, etc.), where only the weights (and not the activations) are quantized to lower bits. The actual matmul is done in full precision after dequantization. \n\nConcretely, we use just-in-time de-quantize the (quantized) matrix $\\mathbf{Q}$, execute matrix operations (e.g., `matmul(X, Q) ==> matmul(X, dequantize(Q))`), and throw away the de-quantized matrix. This supports arbitrary quantization, and the main technical difficulty is how to do de-quantization quickly. We outlined more details in the appendix.\n\nAlthough weight-only quantization cannot make use of (faster and more energy-efficient) lower-precision matmuls, weight-only quantization still has two practical benefits. First, at a macro level, having a smaller model (due to quantization) allows us to increase the batch size. This reduces the need for expensive data/model parallelism, which requires cross-device or even cross-node communication. Second, at a micro level, smaller quantized matrices allow us to reduce data movement between SRAM and HBM of the GPU. As an example, QLoRA's matrix multiplication implementation could be faster than dense matrix multiplication [1].\n\n[1] https://github.com/TimDettmers/bitsandbytes/releases\n\n**Related paper on Joint Low-rank and Quantized Matrix Decomposition**\n\nThanks for the suggestion; this is an interesting paper! We will discuss this paper in the related works section.\n\nWe primarily considered the cases in which $\\mathbf{L}_1$ and $\\mathbf{L}_2$ are floating-point parameters. This is necessary because we want to fine-tune these parameters. That being said, the techniques introduced in that paper paper might be helpful for other (future) use cases. For example, we could post-training merge $\\mathbf{Q}, \\mathbf{L}_1, \\mathbf{L}_2$, and re-decompose them into three separate quantized matrices for inference.\n\n**Question 1**\n\nYou are correct -- we will clarify this; thanks!\n\n**Question 2**\n\nYou are correct in that the techniques in this paper are not specific to NF. QLoRA paper demonstrated the superior quality of such a scheme compared to classical integer quantization. Our preliminary experiments confirmed this, and hence, we chose NF.\n\nWe conducted the following experiment to be more concrete about NF vs INT. We ran LQ (i.e., Section 3.1) on LLaMA-2 7B's dense parameters with `rank=64` and NF3/INT3/NF4/INT4 quantization configuration (i.e., `b0=3/4, b1=8, b2=fp32, B0=64, B1=256`), and measure the reconstruction errors (i.e., $||\\mathbf{W} - (\\mathbf{Q} + \\mathbf{L}_1 \\mathbf{L}_2) ||_2^2$). The table below shows that NF indeed performed better.\n\n| Method      | Reconstruction error (unit: $10^4$) |\n| ----------- | ----------- |\n| LQ with NF3      | 7.12       |\n| LQ with INT3     | 11.6        |\n| LQ with NF4      | 1.42       |\n| LQ with INT4     | 2.02        |\n\n**Question 3**\n\nThis is the same $\\delta$ in QLoRA, and we treated it as a constant for all settings. We will clarify further in the next iteration of the paper.\n\n**Question 4**\n\nWe _defined_ $\\mathbb{Q}$ as the set of matrices that can be quantized (Equation 1 is a minimization problem, not an exact decomposition problem). Thanks for pointing out the confusion; we will clarify this.\n\n**Question 5**\n\nThe middle figure represents the setting of many (Q)LoRA methods, in which the Low-Rank components are initialized to be zero. In that sense, $\\operatorname{quantize}(\\mathbf{W}) + \\mathbf{L}_1 \\mathbf{L}_2 = \\operatorname{quantize}(\\mathbf{W})$ for those methods.\n\n**Question 6**\n\nGood catch -- it's meant for \"budget\" but shared similar symbols with $B_0$ and $B_1$.\n\n**Question 7**\n\nWe say a matrix $\\mathbf{F}$ has homogenous rows or columns when either rows or columns of the matrix have identical values. We will clarify this.\n\n**Question 8**\n\nTable 4 used the quantization configuration of NF3/NF4. Specifically, they use the block sizes and second-level quantization as the original NF4, but with the first-level quantization set to 3/4-bit. Table 2, however, used ILP to search for the quantization configuration, hence the difference.\n\nWe made such decisions for Table 4 to remove the possible confounding variable of using ILP to search for the quantization configuration (i.e., we just wanted to see the effect of higher rank LoRA components). We will clarify!", "author_response": "Thanks to the reviewer for the comments! Please see our responses below.\n\n**Latency of The Iterative Algorithm**\n\nThis is a one-time cost at the initialization and takes a fraction of the total training time. Each step of the iterative algorithm consists of SVD followed by quantization. Note that we use randomized SVD (rSVD) for speed consideration, and we run the algorithm for up to 100 steps (and we stop before this if the error starts to increase).\n\nTo be more concrete, we measured the runtime to process three of the LLaMA-2 7B's matrices with `rank=64` and NF3 quantization. Please see the table below.\n\n| Data-Aware (i.e., Section 3.3) | Time / Matrix |\n| ----- | ----- |\n| No  | 1.3 to 2.1 seconds |\n| Yes | 1.3 to 2.3 seconds |\n\n**Comparison with GPTQ-based Methods**\n\nThanks for the question! We actually did implement and compare against GPTQ-LoRA. We called this \"LREC\" in the paper following this paper (https://arxiv.org/pdf/2306.08162.pdf). We find that GPTQ-LoRA underperforms both QLoRA and LQ-LoRA (Table 1).\n\n(We now realize that this naming caused some confusion. Thanks for pointing that out --- we will clarify this in the paper.)\n\n**Limitations from QLoRA**\n\nThis is a very good point! But we want to point out that inference with quantized + low-rank is not necessarily slower, especially in batch = 1 setting. Smaller quantized matrices allow us to reduce data movement between SRAM and HBM of the GPU. For example, in the latest releases of `bitsandbytes` (the low-level engine behind QLoRA) [1], they claimed their matrix-vector multiplication between quantized matrix and dense vector (the one needed during inference) could be competitive and even outperformed dense matrix-vector multiplication. \n\n[1] https://github.com/TimDettmers/bitsandbytes/releases\n\n*Integrating LoRA Layers Into Linear Layers*\n\nGiven the potential inference speed-ups, we think that practitioners could use the \"quantized + low-rank\" models without merging. Such models could be just as fast (as discussed above) as merged dense models and more memory-efficient. In that regard, you could (loosely speaking) consider this as an alternative PTQ method.\n\nBased on this potential of LQ-LoRA as an alternative to PTQ, we performed additional experiments by using LQ-LoRA as a potential alternative to PTQ methods by training on a larger calibration dataset. Specifically, we train the LQ-LoRA (Fisher, 2.75-bit, 64-rank) for one epoch on two C4 partitions and WikiText-2. We then further quantize the LoRA components themselves to 8-bit (NF8). Please see the Table below for the results, where the numbers other than LQ-LoRA are from OmniQuant [1]. Note that we use the term \"effective bits\" to denote bits per parameter that treat the LoRA components as \"overheads\" for comparing with methods without LoRA; LoRA components (with NF-8 quantization) amount to about 0.1-0.2 extra bits per parameter.\n\n| Base Model | Method                           | Effective Bits (7B, 70B) | C4 (7B) | C4 (70B) | WikiText2 (7B) | WikiText2 (70B) |\n| -------- | ---------------------------------- | ----------- | ---- | ---- | ---- | ---- |\n| LLaMA-2 | Dense                               | 16          | 6.97 | 5.52 | 5.47 | 3.31 |\n| LLaMA-2 | RTN                                 | 3.15        | 8.40 | 6.02 | 6.66 | 3.97 |\n| LLaMA-2 | GPTQ            | 3.15        | 7.89 | 5.85 | 6.29 | 3.85 |\n| LLaMA-2 | AWQ               | 3.15        | 7.84 | -    | 6.24 | -    |\n| LLaMA-2 | OmniQuant  | 3.15        | 7.75 | 5.85 | 6.03 | 3.78 |\n| LLaMA-2 | LQ-LoRA (Fisher)   | 2.95, 2.85 | 7.60 | 5.88 | 5.67 | 3.65 |\n\n[1] https://arxiv.org/abs/2308.13137\n\nThese results indicate that LQ-LoRA can also be used as a state-of-the-art PTQ method. Given the promise of LQ-LoRA as a potential alternative to PTQ, we will include this comparison against PTQ results in the next iteration of the paper.\n\n\n**Novelty**\n\n*Similarly, I would like to raise a query about the paper's novelty. While this method undeniably enhances the current approach (Q-LoRA), from a PEFT perspective, there could be superior methods, particularly concerning inference challenges. On the topic of novelty, I await the insights of fellow reviewers.*\n\nWe want to reiterate three main contributions on top of QLoRA:\n1. We adopt a matrix decomposition view of initializing the quantized and low-rank components of LoRA adaptation. (Note that QLoRA does not perform explicit matrix decomposition)\n2. Our ILP formulation enables us to search for the mixed-configuration quantization that fits specific resource requirements (e.g., 2.75 bits per parameter).\n3. We extended the two methods above to a data-aware setting by incorporating the sensitivities of parameters."}
{"claim": "The reviewer is unclear about the biased approximation mentioned in Equation (7) despite the paper's formulation beginning from Equation (4).", "claim_type": "presentation", "paper_id": "c7m1HahBNf", "paper_title": "Exploring Structured Semantic Priors Underlying Diffusion Score for Test-time Adaptation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "U1a50zbUPt", "reviewer": "Reviewer_V2Wk", "review_text": "Comment: **Regarding all questions except Question 5**\n\nThank you for addressing my concerns related to the experiments. Your comprehensive work, especially given the short timeframe, has completely resolved my questions.\n\n**Regarding Question 5**\n\nI am still unclear on a particular point. While I understand that Diffusion-TTA relies on a biased approximation due to the optimization of the ELBO, I’m confused about the biased approximation mentioned in Eq (7), even though the current formulation begins from Eq (4). It seems to me that this could be considered a form of circular reasoning since the denoising estimator in Eq (7) is trained using the ELBO. Is my interpretation correct?", "labeling_timestamp": "2026-01-11T16:27:34.908835", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The authors explicitly deny that Eq (7) is a biased approximation: they state Eq (7) is exact and provide a mathematical proof that the estimators are unbiased, arguing there is no approximation or circular reasoning.", "evidence": "\"the denoising estimator in Eq (7) is actually **unbiased**, there is **no theoretical approximation** for Eq (7), and we argue that **no circular reasoning** exists in our DUSA.\" \"In Eq (7), we show that the true noise ε equals the weighted ensemble of conditional score functions ... with no approximation.\"", "combined_author_response": "Dear Reviewer V2Wk,\n\nThank you very much for your efforts in helping us enhance the rigor of our work. We truly appreciate the discussion, and we find your suggestions and in-depth questions extremely helpful in strengthening our paper.\n\nBest regards,\n\nSubmission335 Authors.\n\n---\n\nDear Reviewer oxUu,\n\nThank you for the positive feedback. We find your suggestions and questions really helpful in improving our work. We will follow your advice and add the interesting discussion on unconditional / conditional parts in Eq. 11 to the main paper in the revision.\n\nBest regards,\n\nSubmission335 Authors.\n\n---\n\nDear Reviewer V2Wk,\n\nThank you for the further discussion. We would like to remind that, according to the definition, the bias of an estimator is calculated as **the difference between the expected value of the estimator and the real value to estimate**. In our DUSA, the true noise $\\epsilon$ is estimated by $\\epsilon_\\phi(x_t,t,c_y)$, therefore with the proof in Eq $(*)$ the bias should be calculated by:\n\n$$\\text{Bias}(\\epsilon_\\phi(x_t,t,c_y),\\epsilon)=\\mathbb{E}\\_{x_t|\\epsilon,t}[\\epsilon_\\phi(x_t,t,c_y)]-\\epsilon=0,$$\n\nwhich means the conditional noise estimator $\\epsilon_\\phi(x_t,t,c_y)$ is indeed an **unbiased estimator** of the true noise $\\epsilon$.\n\nWe hope the explanations have resolved the concerns. We are more than happy to discuss if you have further questions.\n\nBest regards,\n\nSubmission335 Authors.\n\n---\n\nDear Reviewer Your,\n\nThank you for your thoughtful feedback. We greatly appreciate your acknowledgment and find your suggestions invaluable for enhancing our paper. We hope our work will shed light on the evolving field of test-time adaptation.\n\nBest regards,\n\nSubmission335 Authors.\n\n---\n\nDear Reviewer 6Z3D,\n\nIt's good to know that we have addressed your concerns. We really appreciate your thoughtful comments and response, and are sincerely grateful for your efforts in improving the quality of our work.\n\nBest regards,\n\nSubmission335 Authors.\n\n---\n\nDear Reviewer V2Wk,\n\nWe are glad to hear that your concerns related to the experiments have been completely addressed. We would like to make a further demonstration of the **unbiased estimation** in our DUSA. As a short answer, the denoising estimator in Eq (7) is actually **unbiased**, there is **no theoretical approximation** for Eq (7), and we argue that **no circular reasoning** exists in our DUSA. Please find the details below.\n\nIn Eq (7), we show that the true noise $\\epsilon$ equals the weighted ensemble of conditional score functions $\\nabla_{x_t}\\log p(x_t|y)$ **with no approximation**. The reviewer's confusion might come from our **Eq (8)**, where the conditional score functions $\\nabla_{x_t}\\log p(x_t|y)$ are further **estimated** by conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ from the diffusion model.\n\nWe would like to clarify that, the approximation symbol $\\approx$ in Eq (8) **does not indicate a bias in estimation**, but rather indicates that the conditional noise estimations **might not be totally accurate** as they are predictions from the denoising network in diffusion. Indeed, **the denoising estimators are unbiased**, as they are trained by the simplified objective in Eq (3). To train a diffusion model with Eq (3), we first randomly sample noise $\\epsilon\\sim\\mathcal{N}(0,I)$ and uniformly sample timestep $t$, and obtain noised sample $x_t$ following Eq (2). Then, the training objective in Eq (3) is optimized by minimizing:\n$$\\mathcal{L}(\\phi)=\\mathbb{E}\\_{x_t|\\epsilon,t}[\\\\|\\epsilon-\\epsilon_\\phi(x_t,t,c_y)\\\\|_2^2].$$\n\nAt the optimal point of the objective, we have:\n$$\\frac{\\partial\\mathcal{L}(\\phi)}{\\partial\\epsilon_\\phi}=\\mathbb{E}\\_{x_t|\\epsilon,t}[2(\\epsilon-\\epsilon_\\phi(x_t,t,c_y))]=0,$$\nwhich implies:\n$$\\mathbb{E}\\_{x_t|\\epsilon,t}[\\epsilon_\\phi(x_t,t,c_y)]=\\mathbb{E}\\_{x_t|\\epsilon,t}[\\epsilon]=\\epsilon,\\quad\\text{Eq}~(*)$$\nmeaning that the conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ are **unbiased** with regard to the true noise $\\epsilon$.\n\nFor a more comprehensive understanding of the **unbiased estimation** in our DUSA, we provide a step-by-step explanation below, justifying that **there is no circular reasoning**.\n\n1. We start from the **equation** Eq (4), where the unconditional score function $\\nabla_x\\log p(x)$ can be expressed by an ensemble of conditional score functions $\\nabla_x\\log p(x|y)$.\n2. With Eq (6) in Corollary 1, we show that there is a **equation** between the score function and the true noise $\\epsilon$ on every single timestep.\n3. We now replace the unconditional score function in Eq (4) with Eq (6) and get the **equation** in Eq (7).\n\nWe remind that **no approximation is made** during all steps above. Also, Corollary 1 indicates that:\n$$\\nabla_{x_t}\\log p(x_t|y)=-\\frac{\\epsilon}{\\sqrt{1-\\bar{\\alpha}_t}},\\quad\\text{Eq}~(**)$$\nas the true noise $\\epsilon$ is sampled in the forward process and thus not relevant to the existence of conditioning. The proof for conditional score functions resembles that for Corollary 1 in Appendix C.\n\n4. We then use conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ to **estimate** the true noise $\\epsilon$ in Eq $(**)$, where the estimations are all **unbiased**, as proved in Eq $(*)$. This step results in Eq (8).\n5. Combining Eq (7) and Eq (8), we instantly get Eq (9) and thus our DUSA objective in Eq (10). This step involves **no** approximation or estimation.\n\nThrough this **non-circular** chain of reasoning, we can find that in DUSA there is **no theoretical approximation** and **the estimations are all unbiased**.\n\nWe hope the proofs and chain of reasoning above have addressed the theoretical concerns. Please let us know if you have further questions or suggestions, and we are more than happy to continue the discussion.\n\nBest regards,\n\nSubmission335 Authors.", "author_response": "Dear Reviewer V2Wk,\n\nWe are glad to hear that your concerns related to the experiments have been completely addressed. We would like to make a further demonstration of the **unbiased estimation** in our DUSA. As a short answer, the denoising estimator in Eq (7) is actually **unbiased**, there is **no theoretical approximation** for Eq (7), and we argue that **no circular reasoning** exists in our DUSA. Please find the details below.\n\nIn Eq (7), we show that the true noise $\\epsilon$ equals the weighted ensemble of conditional score functions $\\nabla_{x_t}\\log p(x_t|y)$ **with no approximation**. The reviewer's confusion might come from our **Eq (8)**, where the conditional score functions $\\nabla_{x_t}\\log p(x_t|y)$ are further **estimated** by conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ from the diffusion model.\n\nWe would like to clarify that, the approximation symbol $\\approx$ in Eq (8) **does not indicate a bias in estimation**, but rather indicates that the conditional noise estimations **might not be totally accurate** as they are predictions from the denoising network in diffusion. Indeed, **the denoising estimators are unbiased**, as they are trained by the simplified objective in Eq (3). To train a diffusion model with Eq (3), we first randomly sample noise $\\epsilon\\sim\\mathcal{N}(0,I)$ and uniformly sample timestep $t$, and obtain noised sample $x_t$ following Eq (2). Then, the training objective in Eq (3) is optimized by minimizing:\n$$\\mathcal{L}(\\phi)=\\mathbb{E}\\_{x_t|\\epsilon,t}[\\\\|\\epsilon-\\epsilon_\\phi(x_t,t,c_y)\\\\|_2^2].$$\n\nAt the optimal point of the objective, we have:\n$$\\frac{\\partial\\mathcal{L}(\\phi)}{\\partial\\epsilon_\\phi}=\\mathbb{E}\\_{x_t|\\epsilon,t}[2(\\epsilon-\\epsilon_\\phi(x_t,t,c_y))]=0,$$\nwhich implies:\n$$\\mathbb{E}\\_{x_t|\\epsilon,t}[\\epsilon_\\phi(x_t,t,c_y)]=\\mathbb{E}\\_{x_t|\\epsilon,t}[\\epsilon]=\\epsilon,\\quad\\text{Eq}~(*)$$\nmeaning that the conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ are **unbiased** with regard to the true noise $\\epsilon$.\n\nFor a more comprehensive understanding of the **unbiased estimation** in our DUSA, we provide a step-by-step explanation below, justifying that **there is no circular reasoning**.\n\n1. We start from the **equation** Eq (4), where the unconditional score function $\\nabla_x\\log p(x)$ can be expressed by an ensemble of conditional score functions $\\nabla_x\\log p(x|y)$.\n2. With Eq (6) in Corollary 1, we show that there is a **equation** between the score function and the true noise $\\epsilon$ on every single timestep.\n3. We now replace the unconditional score function in Eq (4) with Eq (6) and get the **equation** in Eq (7).\n\nWe remind that **no approximation is made** during all steps above. Also, Corollary 1 indicates that:\n$$\\nabla_{x_t}\\log p(x_t|y)=-\\frac{\\epsilon}{\\sqrt{1-\\bar{\\alpha}_t}},\\quad\\text{Eq}~(**)$$\nas the true noise $\\epsilon$ is sampled in the forward process and thus not relevant to the existence of conditioning. The proof for conditional score functions resembles that for Corollary 1 in Appendix C.\n\n4. We then use conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ to **estimate** the true noise $\\epsilon$ in Eq $(**)$, where the estimations are all **unbiased**, as proved in Eq $(*)$. This step results in Eq (8).\n5. Combining Eq (7) and Eq (8), we instantly get Eq (9) and thus our DUSA objective in Eq (10). This step involves **no** approximation or estimation.\n\nThrough this **non-circular** chain of reasoning, we can find that in DUSA there is **no theoretical approximation** and **the estimations are all unbiased**.\n\nWe hope the proofs and chain of reasoning above have addressed the theoretical concerns. Please let us know if you have further questions or suggestions, and we are more than happy to continue the discussion.\n\nBest regards,\n\nSubmission335 Authors."}
{"claim": "Equation (7) appears to introduce a biased denoising estimator without clear justification relative to the derivation that begins at Equation (4).", "claim_type": "methodology", "paper_id": "c7m1HahBNf", "paper_title": "Exploring Structured Semantic Priors Underlying Diffusion Score for Test-time Adaptation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "U1a50zbUPt", "reviewer": "Reviewer_V2Wk", "review_text": "Comment: **Regarding all questions except Question 5**\n\nThank you for addressing my concerns related to the experiments. Your comprehensive work, especially given the short timeframe, has completely resolved my questions.\n\n**Regarding Question 5**\n\nI am still unclear on a particular point. While I understand that Diffusion-TTA relies on a biased approximation due to the optimization of the ELBO, I’m confused about the biased approximation mentioned in Eq (7), even though the current formulation begins from Eq (4). It seems to me that this could be considered a form of circular reasoning since the denoising estimator in Eq (7) is trained using the ELBO. Is my interpretation correct?", "labeling_timestamp": "2026-01-11T16:27:40.269045", "model": "gpt-5-mini", "label": "Contradicted", "justification": "Authors explicitly deny the claim, provide a mathematical derivation showing the estimator's expectation equals the true noise (bias = 0), and state Eq (7) involves no approximation or circular reasoning.", "evidence": "\"the bias should be calculated by: $$\\text{Bias}(\\epsilon_\\phi(x_t,t,c_y),\\epsilon)=\\mathbb{E}\\_{x_t|\\epsilon,t}[\\epsilon_\\phi(x_t,t,c_y)]-\\epsilon=0,$$ which means the conditional noise estimator \\epsilon_\\phi(x_t,t,c_y) is indeed an **unbiased estimator** of the true noise \\epsilon.\" \"As a short answer, the denoising estimator in Eq (7) is actually **unbiased**, there is **no theoretical approximation** for Eq (7), and we argue that **no circular reasoning** exists in our DUSA.\"", "combined_author_response": "Dear Reviewer V2Wk,\n\nThank you very much for your efforts in helping us enhance the rigor of our work. We truly appreciate the discussion, and we find your suggestions and in-depth questions extremely helpful in strengthening our paper.\n\nBest regards,\n\nSubmission335 Authors.\n\n---\n\nDear Reviewer oxUu,\n\nThank you for the positive feedback. We find your suggestions and questions really helpful in improving our work. We will follow your advice and add the interesting discussion on unconditional / conditional parts in Eq. 11 to the main paper in the revision.\n\nBest regards,\n\nSubmission335 Authors.\n\n---\n\nDear Reviewer V2Wk,\n\nThank you for the further discussion. We would like to remind that, according to the definition, the bias of an estimator is calculated as **the difference between the expected value of the estimator and the real value to estimate**. In our DUSA, the true noise $\\epsilon$ is estimated by $\\epsilon_\\phi(x_t,t,c_y)$, therefore with the proof in Eq $(*)$ the bias should be calculated by:\n\n$$\\text{Bias}(\\epsilon_\\phi(x_t,t,c_y),\\epsilon)=\\mathbb{E}\\_{x_t|\\epsilon,t}[\\epsilon_\\phi(x_t,t,c_y)]-\\epsilon=0,$$\n\nwhich means the conditional noise estimator $\\epsilon_\\phi(x_t,t,c_y)$ is indeed an **unbiased estimator** of the true noise $\\epsilon$.\n\nWe hope the explanations have resolved the concerns. We are more than happy to discuss if you have further questions.\n\nBest regards,\n\nSubmission335 Authors.\n\n---\n\nDear Reviewer Your,\n\nThank you for your thoughtful feedback. We greatly appreciate your acknowledgment and find your suggestions invaluable for enhancing our paper. We hope our work will shed light on the evolving field of test-time adaptation.\n\nBest regards,\n\nSubmission335 Authors.\n\n---\n\nDear Reviewer 6Z3D,\n\nIt's good to know that we have addressed your concerns. We really appreciate your thoughtful comments and response, and are sincerely grateful for your efforts in improving the quality of our work.\n\nBest regards,\n\nSubmission335 Authors.\n\n---\n\nDear Reviewer V2Wk,\n\nWe are glad to hear that your concerns related to the experiments have been completely addressed. We would like to make a further demonstration of the **unbiased estimation** in our DUSA. As a short answer, the denoising estimator in Eq (7) is actually **unbiased**, there is **no theoretical approximation** for Eq (7), and we argue that **no circular reasoning** exists in our DUSA. Please find the details below.\n\nIn Eq (7), we show that the true noise $\\epsilon$ equals the weighted ensemble of conditional score functions $\\nabla_{x_t}\\log p(x_t|y)$ **with no approximation**. The reviewer's confusion might come from our **Eq (8)**, where the conditional score functions $\\nabla_{x_t}\\log p(x_t|y)$ are further **estimated** by conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ from the diffusion model.\n\nWe would like to clarify that, the approximation symbol $\\approx$ in Eq (8) **does not indicate a bias in estimation**, but rather indicates that the conditional noise estimations **might not be totally accurate** as they are predictions from the denoising network in diffusion. Indeed, **the denoising estimators are unbiased**, as they are trained by the simplified objective in Eq (3). To train a diffusion model with Eq (3), we first randomly sample noise $\\epsilon\\sim\\mathcal{N}(0,I)$ and uniformly sample timestep $t$, and obtain noised sample $x_t$ following Eq (2). Then, the training objective in Eq (3) is optimized by minimizing:\n$$\\mathcal{L}(\\phi)=\\mathbb{E}\\_{x_t|\\epsilon,t}[\\\\|\\epsilon-\\epsilon_\\phi(x_t,t,c_y)\\\\|_2^2].$$\n\nAt the optimal point of the objective, we have:\n$$\\frac{\\partial\\mathcal{L}(\\phi)}{\\partial\\epsilon_\\phi}=\\mathbb{E}\\_{x_t|\\epsilon,t}[2(\\epsilon-\\epsilon_\\phi(x_t,t,c_y))]=0,$$\nwhich implies:\n$$\\mathbb{E}\\_{x_t|\\epsilon,t}[\\epsilon_\\phi(x_t,t,c_y)]=\\mathbb{E}\\_{x_t|\\epsilon,t}[\\epsilon]=\\epsilon,\\quad\\text{Eq}~(*)$$\nmeaning that the conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ are **unbiased** with regard to the true noise $\\epsilon$.\n\nFor a more comprehensive understanding of the **unbiased estimation** in our DUSA, we provide a step-by-step explanation below, justifying that **there is no circular reasoning**.\n\n1. We start from the **equation** Eq (4), where the unconditional score function $\\nabla_x\\log p(x)$ can be expressed by an ensemble of conditional score functions $\\nabla_x\\log p(x|y)$.\n2. With Eq (6) in Corollary 1, we show that there is a **equation** between the score function and the true noise $\\epsilon$ on every single timestep.\n3. We now replace the unconditional score function in Eq (4) with Eq (6) and get the **equation** in Eq (7).\n\nWe remind that **no approximation is made** during all steps above. Also, Corollary 1 indicates that:\n$$\\nabla_{x_t}\\log p(x_t|y)=-\\frac{\\epsilon}{\\sqrt{1-\\bar{\\alpha}_t}},\\quad\\text{Eq}~(**)$$\nas the true noise $\\epsilon$ is sampled in the forward process and thus not relevant to the existence of conditioning. The proof for conditional score functions resembles that for Corollary 1 in Appendix C.\n\n4. We then use conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ to **estimate** the true noise $\\epsilon$ in Eq $(**)$, where the estimations are all **unbiased**, as proved in Eq $(*)$. This step results in Eq (8).\n5. Combining Eq (7) and Eq (8), we instantly get Eq (9) and thus our DUSA objective in Eq (10). This step involves **no** approximation or estimation.\n\nThrough this **non-circular** chain of reasoning, we can find that in DUSA there is **no theoretical approximation** and **the estimations are all unbiased**.\n\nWe hope the proofs and chain of reasoning above have addressed the theoretical concerns. Please let us know if you have further questions or suggestions, and we are more than happy to continue the discussion.\n\nBest regards,\n\nSubmission335 Authors.", "author_response": "Dear Reviewer V2Wk,\n\nWe are glad to hear that your concerns related to the experiments have been completely addressed. We would like to make a further demonstration of the **unbiased estimation** in our DUSA. As a short answer, the denoising estimator in Eq (7) is actually **unbiased**, there is **no theoretical approximation** for Eq (7), and we argue that **no circular reasoning** exists in our DUSA. Please find the details below.\n\nIn Eq (7), we show that the true noise $\\epsilon$ equals the weighted ensemble of conditional score functions $\\nabla_{x_t}\\log p(x_t|y)$ **with no approximation**. The reviewer's confusion might come from our **Eq (8)**, where the conditional score functions $\\nabla_{x_t}\\log p(x_t|y)$ are further **estimated** by conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ from the diffusion model.\n\nWe would like to clarify that, the approximation symbol $\\approx$ in Eq (8) **does not indicate a bias in estimation**, but rather indicates that the conditional noise estimations **might not be totally accurate** as they are predictions from the denoising network in diffusion. Indeed, **the denoising estimators are unbiased**, as they are trained by the simplified objective in Eq (3). To train a diffusion model with Eq (3), we first randomly sample noise $\\epsilon\\sim\\mathcal{N}(0,I)$ and uniformly sample timestep $t$, and obtain noised sample $x_t$ following Eq (2). Then, the training objective in Eq (3) is optimized by minimizing:\n$$\\mathcal{L}(\\phi)=\\mathbb{E}\\_{x_t|\\epsilon,t}[\\\\|\\epsilon-\\epsilon_\\phi(x_t,t,c_y)\\\\|_2^2].$$\n\nAt the optimal point of the objective, we have:\n$$\\frac{\\partial\\mathcal{L}(\\phi)}{\\partial\\epsilon_\\phi}=\\mathbb{E}\\_{x_t|\\epsilon,t}[2(\\epsilon-\\epsilon_\\phi(x_t,t,c_y))]=0,$$\nwhich implies:\n$$\\mathbb{E}\\_{x_t|\\epsilon,t}[\\epsilon_\\phi(x_t,t,c_y)]=\\mathbb{E}\\_{x_t|\\epsilon,t}[\\epsilon]=\\epsilon,\\quad\\text{Eq}~(*)$$\nmeaning that the conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ are **unbiased** with regard to the true noise $\\epsilon$.\n\nFor a more comprehensive understanding of the **unbiased estimation** in our DUSA, we provide a step-by-step explanation below, justifying that **there is no circular reasoning**.\n\n1. We start from the **equation** Eq (4), where the unconditional score function $\\nabla_x\\log p(x)$ can be expressed by an ensemble of conditional score functions $\\nabla_x\\log p(x|y)$.\n2. With Eq (6) in Corollary 1, we show that there is a **equation** between the score function and the true noise $\\epsilon$ on every single timestep.\n3. We now replace the unconditional score function in Eq (4) with Eq (6) and get the **equation** in Eq (7).\n\nWe remind that **no approximation is made** during all steps above. Also, Corollary 1 indicates that:\n$$\\nabla_{x_t}\\log p(x_t|y)=-\\frac{\\epsilon}{\\sqrt{1-\\bar{\\alpha}_t}},\\quad\\text{Eq}~(**)$$\nas the true noise $\\epsilon$ is sampled in the forward process and thus not relevant to the existence of conditioning. The proof for conditional score functions resembles that for Corollary 1 in Appendix C.\n\n4. We then use conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ to **estimate** the true noise $\\epsilon$ in Eq $(**)$, where the estimations are all **unbiased**, as proved in Eq $(*)$. This step results in Eq (8).\n5. Combining Eq (7) and Eq (8), we instantly get Eq (9) and thus our DUSA objective in Eq (10). This step involves **no** approximation or estimation.\n\nThrough this **non-circular** chain of reasoning, we can find that in DUSA there is **no theoretical approximation** and **the estimations are all unbiased**.\n\nWe hope the proofs and chain of reasoning above have addressed the theoretical concerns. Please let us know if you have further questions or suggestions, and we are more than happy to continue the discussion.\n\nBest regards,\n\nSubmission335 Authors."}
{"claim": "Training the denoising estimator in Equation (7) using the ELBO may create circular reasoning, which the manuscript does not explicitly address.", "claim_type": "methodology", "paper_id": "c7m1HahBNf", "paper_title": "Exploring Structured Semantic Priors Underlying Diffusion Score for Test-time Adaptation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "U1a50zbUPt", "reviewer": "Reviewer_V2Wk", "review_text": "Comment: **Regarding all questions except Question 5**\n\nThank you for addressing my concerns related to the experiments. Your comprehensive work, especially given the short timeframe, has completely resolved my questions.\n\n**Regarding Question 5**\n\nI am still unclear on a particular point. While I understand that Diffusion-TTA relies on a biased approximation due to the optimization of the ELBO, I’m confused about the biased approximation mentioned in Eq (7), even though the current formulation begins from Eq (4). It seems to me that this could be considered a form of circular reasoning since the denoising estimator in Eq (7) is trained using the ELBO. Is my interpretation correct?", "labeling_timestamp": "2026-01-11T16:27:44.234062", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The authors explicitly deny the presence of circular reasoning, provide a proof that the denoising estimator is unbiased, and give a step-by-step, non-circular derivation linking the equations to justify their claim.", "evidence": "\"the denoising estimator in Eq (7) is actually unbiased, there is no theoretical approximation for Eq (7), and we argue that no circular reasoning exists in our DUSA.\" \"meaning that the conditional noise estimations \\epsilon_\\phi(x_t,t,c_y) are unbiased with regard to the true noise \\epsilon.\" \"Through this non-circular chain of reasoning, we can find that in DUSA there is no theoretical approximation and the estimations are all unbiased.\"", "combined_author_response": "Dear Reviewer V2Wk,\n\nThank you very much for your efforts in helping us enhance the rigor of our work. We truly appreciate the discussion, and we find your suggestions and in-depth questions extremely helpful in strengthening our paper.\n\nBest regards,\n\nSubmission335 Authors.\n\n---\n\nDear Reviewer oxUu,\n\nThank you for the positive feedback. We find your suggestions and questions really helpful in improving our work. We will follow your advice and add the interesting discussion on unconditional / conditional parts in Eq. 11 to the main paper in the revision.\n\nBest regards,\n\nSubmission335 Authors.\n\n---\n\nDear Reviewer V2Wk,\n\nThank you for the further discussion. We would like to remind that, according to the definition, the bias of an estimator is calculated as **the difference between the expected value of the estimator and the real value to estimate**. In our DUSA, the true noise $\\epsilon$ is estimated by $\\epsilon_\\phi(x_t,t,c_y)$, therefore with the proof in Eq $(*)$ the bias should be calculated by:\n\n$$\\text{Bias}(\\epsilon_\\phi(x_t,t,c_y),\\epsilon)=\\mathbb{E}\\_{x_t|\\epsilon,t}[\\epsilon_\\phi(x_t,t,c_y)]-\\epsilon=0,$$\n\nwhich means the conditional noise estimator $\\epsilon_\\phi(x_t,t,c_y)$ is indeed an **unbiased estimator** of the true noise $\\epsilon$.\n\nWe hope the explanations have resolved the concerns. We are more than happy to discuss if you have further questions.\n\nBest regards,\n\nSubmission335 Authors.\n\n---\n\nDear Reviewer Your,\n\nThank you for your thoughtful feedback. We greatly appreciate your acknowledgment and find your suggestions invaluable for enhancing our paper. We hope our work will shed light on the evolving field of test-time adaptation.\n\nBest regards,\n\nSubmission335 Authors.\n\n---\n\nDear Reviewer 6Z3D,\n\nIt's good to know that we have addressed your concerns. We really appreciate your thoughtful comments and response, and are sincerely grateful for your efforts in improving the quality of our work.\n\nBest regards,\n\nSubmission335 Authors.\n\n---\n\nDear Reviewer V2Wk,\n\nWe are glad to hear that your concerns related to the experiments have been completely addressed. We would like to make a further demonstration of the **unbiased estimation** in our DUSA. As a short answer, the denoising estimator in Eq (7) is actually **unbiased**, there is **no theoretical approximation** for Eq (7), and we argue that **no circular reasoning** exists in our DUSA. Please find the details below.\n\nIn Eq (7), we show that the true noise $\\epsilon$ equals the weighted ensemble of conditional score functions $\\nabla_{x_t}\\log p(x_t|y)$ **with no approximation**. The reviewer's confusion might come from our **Eq (8)**, where the conditional score functions $\\nabla_{x_t}\\log p(x_t|y)$ are further **estimated** by conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ from the diffusion model.\n\nWe would like to clarify that, the approximation symbol $\\approx$ in Eq (8) **does not indicate a bias in estimation**, but rather indicates that the conditional noise estimations **might not be totally accurate** as they are predictions from the denoising network in diffusion. Indeed, **the denoising estimators are unbiased**, as they are trained by the simplified objective in Eq (3). To train a diffusion model with Eq (3), we first randomly sample noise $\\epsilon\\sim\\mathcal{N}(0,I)$ and uniformly sample timestep $t$, and obtain noised sample $x_t$ following Eq (2). Then, the training objective in Eq (3) is optimized by minimizing:\n$$\\mathcal{L}(\\phi)=\\mathbb{E}\\_{x_t|\\epsilon,t}[\\\\|\\epsilon-\\epsilon_\\phi(x_t,t,c_y)\\\\|_2^2].$$\n\nAt the optimal point of the objective, we have:\n$$\\frac{\\partial\\mathcal{L}(\\phi)}{\\partial\\epsilon_\\phi}=\\mathbb{E}\\_{x_t|\\epsilon,t}[2(\\epsilon-\\epsilon_\\phi(x_t,t,c_y))]=0,$$\nwhich implies:\n$$\\mathbb{E}\\_{x_t|\\epsilon,t}[\\epsilon_\\phi(x_t,t,c_y)]=\\mathbb{E}\\_{x_t|\\epsilon,t}[\\epsilon]=\\epsilon,\\quad\\text{Eq}~(*)$$\nmeaning that the conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ are **unbiased** with regard to the true noise $\\epsilon$.\n\nFor a more comprehensive understanding of the **unbiased estimation** in our DUSA, we provide a step-by-step explanation below, justifying that **there is no circular reasoning**.\n\n1. We start from the **equation** Eq (4), where the unconditional score function $\\nabla_x\\log p(x)$ can be expressed by an ensemble of conditional score functions $\\nabla_x\\log p(x|y)$.\n2. With Eq (6) in Corollary 1, we show that there is a **equation** between the score function and the true noise $\\epsilon$ on every single timestep.\n3. We now replace the unconditional score function in Eq (4) with Eq (6) and get the **equation** in Eq (7).\n\nWe remind that **no approximation is made** during all steps above. Also, Corollary 1 indicates that:\n$$\\nabla_{x_t}\\log p(x_t|y)=-\\frac{\\epsilon}{\\sqrt{1-\\bar{\\alpha}_t}},\\quad\\text{Eq}~(**)$$\nas the true noise $\\epsilon$ is sampled in the forward process and thus not relevant to the existence of conditioning. The proof for conditional score functions resembles that for Corollary 1 in Appendix C.\n\n4. We then use conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ to **estimate** the true noise $\\epsilon$ in Eq $(**)$, where the estimations are all **unbiased**, as proved in Eq $(*)$. This step results in Eq (8).\n5. Combining Eq (7) and Eq (8), we instantly get Eq (9) and thus our DUSA objective in Eq (10). This step involves **no** approximation or estimation.\n\nThrough this **non-circular** chain of reasoning, we can find that in DUSA there is **no theoretical approximation** and **the estimations are all unbiased**.\n\nWe hope the proofs and chain of reasoning above have addressed the theoretical concerns. Please let us know if you have further questions or suggestions, and we are more than happy to continue the discussion.\n\nBest regards,\n\nSubmission335 Authors.", "author_response": "Dear Reviewer V2Wk,\n\nWe are glad to hear that your concerns related to the experiments have been completely addressed. We would like to make a further demonstration of the **unbiased estimation** in our DUSA. As a short answer, the denoising estimator in Eq (7) is actually **unbiased**, there is **no theoretical approximation** for Eq (7), and we argue that **no circular reasoning** exists in our DUSA. Please find the details below.\n\nIn Eq (7), we show that the true noise $\\epsilon$ equals the weighted ensemble of conditional score functions $\\nabla_{x_t}\\log p(x_t|y)$ **with no approximation**. The reviewer's confusion might come from our **Eq (8)**, where the conditional score functions $\\nabla_{x_t}\\log p(x_t|y)$ are further **estimated** by conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ from the diffusion model.\n\nWe would like to clarify that, the approximation symbol $\\approx$ in Eq (8) **does not indicate a bias in estimation**, but rather indicates that the conditional noise estimations **might not be totally accurate** as they are predictions from the denoising network in diffusion. Indeed, **the denoising estimators are unbiased**, as they are trained by the simplified objective in Eq (3). To train a diffusion model with Eq (3), we first randomly sample noise $\\epsilon\\sim\\mathcal{N}(0,I)$ and uniformly sample timestep $t$, and obtain noised sample $x_t$ following Eq (2). Then, the training objective in Eq (3) is optimized by minimizing:\n$$\\mathcal{L}(\\phi)=\\mathbb{E}\\_{x_t|\\epsilon,t}[\\\\|\\epsilon-\\epsilon_\\phi(x_t,t,c_y)\\\\|_2^2].$$\n\nAt the optimal point of the objective, we have:\n$$\\frac{\\partial\\mathcal{L}(\\phi)}{\\partial\\epsilon_\\phi}=\\mathbb{E}\\_{x_t|\\epsilon,t}[2(\\epsilon-\\epsilon_\\phi(x_t,t,c_y))]=0,$$\nwhich implies:\n$$\\mathbb{E}\\_{x_t|\\epsilon,t}[\\epsilon_\\phi(x_t,t,c_y)]=\\mathbb{E}\\_{x_t|\\epsilon,t}[\\epsilon]=\\epsilon,\\quad\\text{Eq}~(*)$$\nmeaning that the conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ are **unbiased** with regard to the true noise $\\epsilon$.\n\nFor a more comprehensive understanding of the **unbiased estimation** in our DUSA, we provide a step-by-step explanation below, justifying that **there is no circular reasoning**.\n\n1. We start from the **equation** Eq (4), where the unconditional score function $\\nabla_x\\log p(x)$ can be expressed by an ensemble of conditional score functions $\\nabla_x\\log p(x|y)$.\n2. With Eq (6) in Corollary 1, we show that there is a **equation** between the score function and the true noise $\\epsilon$ on every single timestep.\n3. We now replace the unconditional score function in Eq (4) with Eq (6) and get the **equation** in Eq (7).\n\nWe remind that **no approximation is made** during all steps above. Also, Corollary 1 indicates that:\n$$\\nabla_{x_t}\\log p(x_t|y)=-\\frac{\\epsilon}{\\sqrt{1-\\bar{\\alpha}_t}},\\quad\\text{Eq}~(**)$$\nas the true noise $\\epsilon$ is sampled in the forward process and thus not relevant to the existence of conditioning. The proof for conditional score functions resembles that for Corollary 1 in Appendix C.\n\n4. We then use conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ to **estimate** the true noise $\\epsilon$ in Eq $(**)$, where the estimations are all **unbiased**, as proved in Eq $(*)$. This step results in Eq (8).\n5. Combining Eq (7) and Eq (8), we instantly get Eq (9) and thus our DUSA objective in Eq (10). This step involves **no** approximation or estimation.\n\nThrough this **non-circular** chain of reasoning, we can find that in DUSA there is **no theoretical approximation** and **the estimations are all unbiased**.\n\nWe hope the proofs and chain of reasoning above have addressed the theoretical concerns. Please let us know if you have further questions or suggestions, and we are more than happy to continue the discussion.\n\nBest regards,\n\nSubmission335 Authors."}
{"claim": "The paper states Diffusion-TTA relies on a biased approximation due to ELBO optimization, raising concerns about estimator consistency and validity that are not resolved.", "claim_type": "methodology", "paper_id": "c7m1HahBNf", "paper_title": "Exploring Structured Semantic Priors Underlying Diffusion Score for Test-time Adaptation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "U1a50zbUPt", "reviewer": "Reviewer_V2Wk", "review_text": "Comment: **Regarding all questions except Question 5**\n\nThank you for addressing my concerns related to the experiments. Your comprehensive work, especially given the short timeframe, has completely resolved my questions.\n\n**Regarding Question 5**\n\nI am still unclear on a particular point. While I understand that Diffusion-TTA relies on a biased approximation due to the optimization of the ELBO, I’m confused about the biased approximation mentioned in Eq (7), even though the current formulation begins from Eq (4). It seems to me that this could be considered a form of circular reasoning since the denoising estimator in Eq (7) is trained using the ELBO. Is my interpretation correct?", "labeling_timestamp": "2026-01-11T16:27:42.934154", "model": "gpt-5-mini", "label": "Contradicted", "justification": "Authors explicitly dispute the reviewer's claim, provide mathematical derivations showing the conditional noise estimator is unbiased, and assert there is no theoretical approximation or circular reasoning in their method.", "evidence": "\"the bias should be calculated by: \\n$$\\text{Bias}(\\epsilon_\\phi(x_t,t,c_y),\\epsilon)=\\mathbb{E}\\_{x_t|\\epsilon,t}[\\epsilon_\\phi(x_t,t,c_y)]-\\epsilon=0,$$\\nwhich means the conditional noise estimator \\epsilon_\\phi(x_t,t,c_y) is indeed an **unbiased estimator** of the true noise \\epsilon.\" and \"the denoising estimator in Eq (7) is actually **unbiased**, there is **no theoretical approximation** for Eq (7), and we argue that **no circular reasoning** exists in our DUSA.\"", "combined_author_response": "Dear Reviewer V2Wk,\n\nThank you very much for your efforts in helping us enhance the rigor of our work. We truly appreciate the discussion, and we find your suggestions and in-depth questions extremely helpful in strengthening our paper.\n\nBest regards,\n\nSubmission335 Authors.\n\n---\n\nDear Reviewer oxUu,\n\nThank you for the positive feedback. We find your suggestions and questions really helpful in improving our work. We will follow your advice and add the interesting discussion on unconditional / conditional parts in Eq. 11 to the main paper in the revision.\n\nBest regards,\n\nSubmission335 Authors.\n\n---\n\nDear Reviewer V2Wk,\n\nThank you for the further discussion. We would like to remind that, according to the definition, the bias of an estimator is calculated as **the difference between the expected value of the estimator and the real value to estimate**. In our DUSA, the true noise $\\epsilon$ is estimated by $\\epsilon_\\phi(x_t,t,c_y)$, therefore with the proof in Eq $(*)$ the bias should be calculated by:\n\n$$\\text{Bias}(\\epsilon_\\phi(x_t,t,c_y),\\epsilon)=\\mathbb{E}\\_{x_t|\\epsilon,t}[\\epsilon_\\phi(x_t,t,c_y)]-\\epsilon=0,$$\n\nwhich means the conditional noise estimator $\\epsilon_\\phi(x_t,t,c_y)$ is indeed an **unbiased estimator** of the true noise $\\epsilon$.\n\nWe hope the explanations have resolved the concerns. We are more than happy to discuss if you have further questions.\n\nBest regards,\n\nSubmission335 Authors.\n\n---\n\nDear Reviewer Your,\n\nThank you for your thoughtful feedback. We greatly appreciate your acknowledgment and find your suggestions invaluable for enhancing our paper. We hope our work will shed light on the evolving field of test-time adaptation.\n\nBest regards,\n\nSubmission335 Authors.\n\n---\n\nDear Reviewer 6Z3D,\n\nIt's good to know that we have addressed your concerns. We really appreciate your thoughtful comments and response, and are sincerely grateful for your efforts in improving the quality of our work.\n\nBest regards,\n\nSubmission335 Authors.\n\n---\n\nDear Reviewer V2Wk,\n\nWe are glad to hear that your concerns related to the experiments have been completely addressed. We would like to make a further demonstration of the **unbiased estimation** in our DUSA. As a short answer, the denoising estimator in Eq (7) is actually **unbiased**, there is **no theoretical approximation** for Eq (7), and we argue that **no circular reasoning** exists in our DUSA. Please find the details below.\n\nIn Eq (7), we show that the true noise $\\epsilon$ equals the weighted ensemble of conditional score functions $\\nabla_{x_t}\\log p(x_t|y)$ **with no approximation**. The reviewer's confusion might come from our **Eq (8)**, where the conditional score functions $\\nabla_{x_t}\\log p(x_t|y)$ are further **estimated** by conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ from the diffusion model.\n\nWe would like to clarify that, the approximation symbol $\\approx$ in Eq (8) **does not indicate a bias in estimation**, but rather indicates that the conditional noise estimations **might not be totally accurate** as they are predictions from the denoising network in diffusion. Indeed, **the denoising estimators are unbiased**, as they are trained by the simplified objective in Eq (3). To train a diffusion model with Eq (3), we first randomly sample noise $\\epsilon\\sim\\mathcal{N}(0,I)$ and uniformly sample timestep $t$, and obtain noised sample $x_t$ following Eq (2). Then, the training objective in Eq (3) is optimized by minimizing:\n$$\\mathcal{L}(\\phi)=\\mathbb{E}\\_{x_t|\\epsilon,t}[\\\\|\\epsilon-\\epsilon_\\phi(x_t,t,c_y)\\\\|_2^2].$$\n\nAt the optimal point of the objective, we have:\n$$\\frac{\\partial\\mathcal{L}(\\phi)}{\\partial\\epsilon_\\phi}=\\mathbb{E}\\_{x_t|\\epsilon,t}[2(\\epsilon-\\epsilon_\\phi(x_t,t,c_y))]=0,$$\nwhich implies:\n$$\\mathbb{E}\\_{x_t|\\epsilon,t}[\\epsilon_\\phi(x_t,t,c_y)]=\\mathbb{E}\\_{x_t|\\epsilon,t}[\\epsilon]=\\epsilon,\\quad\\text{Eq}~(*)$$\nmeaning that the conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ are **unbiased** with regard to the true noise $\\epsilon$.\n\nFor a more comprehensive understanding of the **unbiased estimation** in our DUSA, we provide a step-by-step explanation below, justifying that **there is no circular reasoning**.\n\n1. We start from the **equation** Eq (4), where the unconditional score function $\\nabla_x\\log p(x)$ can be expressed by an ensemble of conditional score functions $\\nabla_x\\log p(x|y)$.\n2. With Eq (6) in Corollary 1, we show that there is a **equation** between the score function and the true noise $\\epsilon$ on every single timestep.\n3. We now replace the unconditional score function in Eq (4) with Eq (6) and get the **equation** in Eq (7).\n\nWe remind that **no approximation is made** during all steps above. Also, Corollary 1 indicates that:\n$$\\nabla_{x_t}\\log p(x_t|y)=-\\frac{\\epsilon}{\\sqrt{1-\\bar{\\alpha}_t}},\\quad\\text{Eq}~(**)$$\nas the true noise $\\epsilon$ is sampled in the forward process and thus not relevant to the existence of conditioning. The proof for conditional score functions resembles that for Corollary 1 in Appendix C.\n\n4. We then use conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ to **estimate** the true noise $\\epsilon$ in Eq $(**)$, where the estimations are all **unbiased**, as proved in Eq $(*)$. This step results in Eq (8).\n5. Combining Eq (7) and Eq (8), we instantly get Eq (9) and thus our DUSA objective in Eq (10). This step involves **no** approximation or estimation.\n\nThrough this **non-circular** chain of reasoning, we can find that in DUSA there is **no theoretical approximation** and **the estimations are all unbiased**.\n\nWe hope the proofs and chain of reasoning above have addressed the theoretical concerns. Please let us know if you have further questions or suggestions, and we are more than happy to continue the discussion.\n\nBest regards,\n\nSubmission335 Authors.", "author_response": "Dear Reviewer V2Wk,\n\nWe are glad to hear that your concerns related to the experiments have been completely addressed. We would like to make a further demonstration of the **unbiased estimation** in our DUSA. As a short answer, the denoising estimator in Eq (7) is actually **unbiased**, there is **no theoretical approximation** for Eq (7), and we argue that **no circular reasoning** exists in our DUSA. Please find the details below.\n\nIn Eq (7), we show that the true noise $\\epsilon$ equals the weighted ensemble of conditional score functions $\\nabla_{x_t}\\log p(x_t|y)$ **with no approximation**. The reviewer's confusion might come from our **Eq (8)**, where the conditional score functions $\\nabla_{x_t}\\log p(x_t|y)$ are further **estimated** by conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ from the diffusion model.\n\nWe would like to clarify that, the approximation symbol $\\approx$ in Eq (8) **does not indicate a bias in estimation**, but rather indicates that the conditional noise estimations **might not be totally accurate** as they are predictions from the denoising network in diffusion. Indeed, **the denoising estimators are unbiased**, as they are trained by the simplified objective in Eq (3). To train a diffusion model with Eq (3), we first randomly sample noise $\\epsilon\\sim\\mathcal{N}(0,I)$ and uniformly sample timestep $t$, and obtain noised sample $x_t$ following Eq (2). Then, the training objective in Eq (3) is optimized by minimizing:\n$$\\mathcal{L}(\\phi)=\\mathbb{E}\\_{x_t|\\epsilon,t}[\\\\|\\epsilon-\\epsilon_\\phi(x_t,t,c_y)\\\\|_2^2].$$\n\nAt the optimal point of the objective, we have:\n$$\\frac{\\partial\\mathcal{L}(\\phi)}{\\partial\\epsilon_\\phi}=\\mathbb{E}\\_{x_t|\\epsilon,t}[2(\\epsilon-\\epsilon_\\phi(x_t,t,c_y))]=0,$$\nwhich implies:\n$$\\mathbb{E}\\_{x_t|\\epsilon,t}[\\epsilon_\\phi(x_t,t,c_y)]=\\mathbb{E}\\_{x_t|\\epsilon,t}[\\epsilon]=\\epsilon,\\quad\\text{Eq}~(*)$$\nmeaning that the conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ are **unbiased** with regard to the true noise $\\epsilon$.\n\nFor a more comprehensive understanding of the **unbiased estimation** in our DUSA, we provide a step-by-step explanation below, justifying that **there is no circular reasoning**.\n\n1. We start from the **equation** Eq (4), where the unconditional score function $\\nabla_x\\log p(x)$ can be expressed by an ensemble of conditional score functions $\\nabla_x\\log p(x|y)$.\n2. With Eq (6) in Corollary 1, we show that there is a **equation** between the score function and the true noise $\\epsilon$ on every single timestep.\n3. We now replace the unconditional score function in Eq (4) with Eq (6) and get the **equation** in Eq (7).\n\nWe remind that **no approximation is made** during all steps above. Also, Corollary 1 indicates that:\n$$\\nabla_{x_t}\\log p(x_t|y)=-\\frac{\\epsilon}{\\sqrt{1-\\bar{\\alpha}_t}},\\quad\\text{Eq}~(**)$$\nas the true noise $\\epsilon$ is sampled in the forward process and thus not relevant to the existence of conditioning. The proof for conditional score functions resembles that for Corollary 1 in Appendix C.\n\n4. We then use conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ to **estimate** the true noise $\\epsilon$ in Eq $(**)$, where the estimations are all **unbiased**, as proved in Eq $(*)$. This step results in Eq (8).\n5. Combining Eq (7) and Eq (8), we instantly get Eq (9) and thus our DUSA objective in Eq (10). This step involves **no** approximation or estimation.\n\nThrough this **non-circular** chain of reasoning, we can find that in DUSA there is **no theoretical approximation** and **the estimations are all unbiased**.\n\nWe hope the proofs and chain of reasoning above have addressed the theoretical concerns. Please let us know if you have further questions or suggestions, and we are more than happy to continue the discussion.\n\nBest regards,\n\nSubmission335 Authors."}
{"claim": "The reviewer asks whether interpreting Equation (7) as circular reasoning, because its denoising estimator is trained with the ELBO, is correct.", "claim_type": "methodology", "paper_id": "c7m1HahBNf", "paper_title": "Exploring Structured Semantic Priors Underlying Diffusion Score for Test-time Adaptation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "U1a50zbUPt", "reviewer": "Reviewer_V2Wk", "review_text": "Comment: **Regarding all questions except Question 5**\n\nThank you for addressing my concerns related to the experiments. Your comprehensive work, especially given the short timeframe, has completely resolved my questions.\n\n**Regarding Question 5**\n\nI am still unclear on a particular point. While I understand that Diffusion-TTA relies on a biased approximation due to the optimization of the ELBO, I’m confused about the biased approximation mentioned in Eq (7), even though the current formulation begins from Eq (4). It seems to me that this could be considered a form of circular reasoning since the denoising estimator in Eq (7) is trained using the ELBO. Is my interpretation correct?", "labeling_timestamp": "2026-01-11T16:27:47.340939", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The authors explicitly deny circular reasoning, argue Eq (7) involves no approximation, and provide a derivation showing the denoising estimator is unbiased, thereby refuting the reviewer's concern.", "evidence": "As a short answer, the denoising estimator in Eq (7) is actually \"unbiased\", there is \"no theoretical approximation\" for Eq (7), and we argue that \"no circular reasoning\" exists in our DUSA.\n\nmeaning that the conditional noise estimations \\epsilon_\\phi(x_t,t,c_y) are **unbiased** with regard to the true noise \\epsilon.\n\nThrough this **non-circular** chain of reasoning, we can find that in DUSA there is **no theoretical approximation** and **the estimations are all unbiased**.", "combined_author_response": "Dear Reviewer V2Wk,\n\nThank you very much for your efforts in helping us enhance the rigor of our work. We truly appreciate the discussion, and we find your suggestions and in-depth questions extremely helpful in strengthening our paper.\n\nBest regards,\n\nSubmission335 Authors.\n\n---\n\nDear Reviewer oxUu,\n\nThank you for the positive feedback. We find your suggestions and questions really helpful in improving our work. We will follow your advice and add the interesting discussion on unconditional / conditional parts in Eq. 11 to the main paper in the revision.\n\nBest regards,\n\nSubmission335 Authors.\n\n---\n\nDear Reviewer V2Wk,\n\nThank you for the further discussion. We would like to remind that, according to the definition, the bias of an estimator is calculated as **the difference between the expected value of the estimator and the real value to estimate**. In our DUSA, the true noise $\\epsilon$ is estimated by $\\epsilon_\\phi(x_t,t,c_y)$, therefore with the proof in Eq $(*)$ the bias should be calculated by:\n\n$$\\text{Bias}(\\epsilon_\\phi(x_t,t,c_y),\\epsilon)=\\mathbb{E}\\_{x_t|\\epsilon,t}[\\epsilon_\\phi(x_t,t,c_y)]-\\epsilon=0,$$\n\nwhich means the conditional noise estimator $\\epsilon_\\phi(x_t,t,c_y)$ is indeed an **unbiased estimator** of the true noise $\\epsilon$.\n\nWe hope the explanations have resolved the concerns. We are more than happy to discuss if you have further questions.\n\nBest regards,\n\nSubmission335 Authors.\n\n---\n\nDear Reviewer Your,\n\nThank you for your thoughtful feedback. We greatly appreciate your acknowledgment and find your suggestions invaluable for enhancing our paper. We hope our work will shed light on the evolving field of test-time adaptation.\n\nBest regards,\n\nSubmission335 Authors.\n\n---\n\nDear Reviewer 6Z3D,\n\nIt's good to know that we have addressed your concerns. We really appreciate your thoughtful comments and response, and are sincerely grateful for your efforts in improving the quality of our work.\n\nBest regards,\n\nSubmission335 Authors.\n\n---\n\nDear Reviewer V2Wk,\n\nWe are glad to hear that your concerns related to the experiments have been completely addressed. We would like to make a further demonstration of the **unbiased estimation** in our DUSA. As a short answer, the denoising estimator in Eq (7) is actually **unbiased**, there is **no theoretical approximation** for Eq (7), and we argue that **no circular reasoning** exists in our DUSA. Please find the details below.\n\nIn Eq (7), we show that the true noise $\\epsilon$ equals the weighted ensemble of conditional score functions $\\nabla_{x_t}\\log p(x_t|y)$ **with no approximation**. The reviewer's confusion might come from our **Eq (8)**, where the conditional score functions $\\nabla_{x_t}\\log p(x_t|y)$ are further **estimated** by conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ from the diffusion model.\n\nWe would like to clarify that, the approximation symbol $\\approx$ in Eq (8) **does not indicate a bias in estimation**, but rather indicates that the conditional noise estimations **might not be totally accurate** as they are predictions from the denoising network in diffusion. Indeed, **the denoising estimators are unbiased**, as they are trained by the simplified objective in Eq (3). To train a diffusion model with Eq (3), we first randomly sample noise $\\epsilon\\sim\\mathcal{N}(0,I)$ and uniformly sample timestep $t$, and obtain noised sample $x_t$ following Eq (2). Then, the training objective in Eq (3) is optimized by minimizing:\n$$\\mathcal{L}(\\phi)=\\mathbb{E}\\_{x_t|\\epsilon,t}[\\\\|\\epsilon-\\epsilon_\\phi(x_t,t,c_y)\\\\|_2^2].$$\n\nAt the optimal point of the objective, we have:\n$$\\frac{\\partial\\mathcal{L}(\\phi)}{\\partial\\epsilon_\\phi}=\\mathbb{E}\\_{x_t|\\epsilon,t}[2(\\epsilon-\\epsilon_\\phi(x_t,t,c_y))]=0,$$\nwhich implies:\n$$\\mathbb{E}\\_{x_t|\\epsilon,t}[\\epsilon_\\phi(x_t,t,c_y)]=\\mathbb{E}\\_{x_t|\\epsilon,t}[\\epsilon]=\\epsilon,\\quad\\text{Eq}~(*)$$\nmeaning that the conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ are **unbiased** with regard to the true noise $\\epsilon$.\n\nFor a more comprehensive understanding of the **unbiased estimation** in our DUSA, we provide a step-by-step explanation below, justifying that **there is no circular reasoning**.\n\n1. We start from the **equation** Eq (4), where the unconditional score function $\\nabla_x\\log p(x)$ can be expressed by an ensemble of conditional score functions $\\nabla_x\\log p(x|y)$.\n2. With Eq (6) in Corollary 1, we show that there is a **equation** between the score function and the true noise $\\epsilon$ on every single timestep.\n3. We now replace the unconditional score function in Eq (4) with Eq (6) and get the **equation** in Eq (7).\n\nWe remind that **no approximation is made** during all steps above. Also, Corollary 1 indicates that:\n$$\\nabla_{x_t}\\log p(x_t|y)=-\\frac{\\epsilon}{\\sqrt{1-\\bar{\\alpha}_t}},\\quad\\text{Eq}~(**)$$\nas the true noise $\\epsilon$ is sampled in the forward process and thus not relevant to the existence of conditioning. The proof for conditional score functions resembles that for Corollary 1 in Appendix C.\n\n4. We then use conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ to **estimate** the true noise $\\epsilon$ in Eq $(**)$, where the estimations are all **unbiased**, as proved in Eq $(*)$. This step results in Eq (8).\n5. Combining Eq (7) and Eq (8), we instantly get Eq (9) and thus our DUSA objective in Eq (10). This step involves **no** approximation or estimation.\n\nThrough this **non-circular** chain of reasoning, we can find that in DUSA there is **no theoretical approximation** and **the estimations are all unbiased**.\n\nWe hope the proofs and chain of reasoning above have addressed the theoretical concerns. Please let us know if you have further questions or suggestions, and we are more than happy to continue the discussion.\n\nBest regards,\n\nSubmission335 Authors.", "author_response": "Dear Reviewer V2Wk,\n\nWe are glad to hear that your concerns related to the experiments have been completely addressed. We would like to make a further demonstration of the **unbiased estimation** in our DUSA. As a short answer, the denoising estimator in Eq (7) is actually **unbiased**, there is **no theoretical approximation** for Eq (7), and we argue that **no circular reasoning** exists in our DUSA. Please find the details below.\n\nIn Eq (7), we show that the true noise $\\epsilon$ equals the weighted ensemble of conditional score functions $\\nabla_{x_t}\\log p(x_t|y)$ **with no approximation**. The reviewer's confusion might come from our **Eq (8)**, where the conditional score functions $\\nabla_{x_t}\\log p(x_t|y)$ are further **estimated** by conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ from the diffusion model.\n\nWe would like to clarify that, the approximation symbol $\\approx$ in Eq (8) **does not indicate a bias in estimation**, but rather indicates that the conditional noise estimations **might not be totally accurate** as they are predictions from the denoising network in diffusion. Indeed, **the denoising estimators are unbiased**, as they are trained by the simplified objective in Eq (3). To train a diffusion model with Eq (3), we first randomly sample noise $\\epsilon\\sim\\mathcal{N}(0,I)$ and uniformly sample timestep $t$, and obtain noised sample $x_t$ following Eq (2). Then, the training objective in Eq (3) is optimized by minimizing:\n$$\\mathcal{L}(\\phi)=\\mathbb{E}\\_{x_t|\\epsilon,t}[\\\\|\\epsilon-\\epsilon_\\phi(x_t,t,c_y)\\\\|_2^2].$$\n\nAt the optimal point of the objective, we have:\n$$\\frac{\\partial\\mathcal{L}(\\phi)}{\\partial\\epsilon_\\phi}=\\mathbb{E}\\_{x_t|\\epsilon,t}[2(\\epsilon-\\epsilon_\\phi(x_t,t,c_y))]=0,$$\nwhich implies:\n$$\\mathbb{E}\\_{x_t|\\epsilon,t}[\\epsilon_\\phi(x_t,t,c_y)]=\\mathbb{E}\\_{x_t|\\epsilon,t}[\\epsilon]=\\epsilon,\\quad\\text{Eq}~(*)$$\nmeaning that the conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ are **unbiased** with regard to the true noise $\\epsilon$.\n\nFor a more comprehensive understanding of the **unbiased estimation** in our DUSA, we provide a step-by-step explanation below, justifying that **there is no circular reasoning**.\n\n1. We start from the **equation** Eq (4), where the unconditional score function $\\nabla_x\\log p(x)$ can be expressed by an ensemble of conditional score functions $\\nabla_x\\log p(x|y)$.\n2. With Eq (6) in Corollary 1, we show that there is a **equation** between the score function and the true noise $\\epsilon$ on every single timestep.\n3. We now replace the unconditional score function in Eq (4) with Eq (6) and get the **equation** in Eq (7).\n\nWe remind that **no approximation is made** during all steps above. Also, Corollary 1 indicates that:\n$$\\nabla_{x_t}\\log p(x_t|y)=-\\frac{\\epsilon}{\\sqrt{1-\\bar{\\alpha}_t}},\\quad\\text{Eq}~(**)$$\nas the true noise $\\epsilon$ is sampled in the forward process and thus not relevant to the existence of conditioning. The proof for conditional score functions resembles that for Corollary 1 in Appendix C.\n\n4. We then use conditional noise estimations $\\epsilon_\\phi(x_t,t,c_y)$ to **estimate** the true noise $\\epsilon$ in Eq $(**)$, where the estimations are all **unbiased**, as proved in Eq $(*)$. This step results in Eq (8).\n5. Combining Eq (7) and Eq (8), we instantly get Eq (9) and thus our DUSA objective in Eq (10). This step involves **no** approximation or estimation.\n\nThrough this **non-circular** chain of reasoning, we can find that in DUSA there is **no theoretical approximation** and **the estimations are all unbiased**.\n\nWe hope the proofs and chain of reasoning above have addressed the theoretical concerns. Please let us know if you have further questions or suggestions, and we are more than happy to continue the discussion.\n\nBest regards,\n\nSubmission335 Authors."}
{"claim": "The paper does not report results from gpt-3.5-turbo-16k-0613 on the example without prompt engineering, few-shot learning, or chain-of-thought.", "claim_type": "experimental", "paper_id": "UkxJd64mki", "paper_title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "fGEvjzIyly", "reviewer": "Reviewer_D2vF", "review_text": "Comment: Thanks again for your thorough explanations. Can authors let me know what they expect to get from \"gpt-3.5-turbo-16k-0613\" without using any prompt engineering, few shot learning, and CoT if they pose the following question directly to the prompt?\n\"Suppose that I have 6 different books, 2 of which are math books. In how many ways can I stack my 6 books on a shelf if I want a math book on both ends of the stack?\" My observation is that for this particular example, even a simple prompt without using more advanced approaches such as CoT or StrategyML should be able to solve the problem. Therefore, I suspect something in the CoT implementation is not working as intended and attributing a bug/issue in CoT implementation to its inferior performance compared with StrategyML is not advisable.", "labeling_timestamp": "2026-01-11T16:28:04.915266", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors acknowledge that gpt-3.5-turbo-16k-0613 was not used (it is deprecated) and instead report using gpt-3.5-turbo-1106, so they effectively confirm that results from the specified deprecated model were not provided.", "evidence": "\"Since 'gpt-3.5-turbo-16k-0613' has been deprecated, we used an improved version, 'gpt-3.5-turbo-1106,' to generate a prediction with a temperature setting of 0.\"", "combined_author_response": "Thank you for your thoughtful consideration and for adjusting your evaluation based on our latest discussions. We truly appreciate your willingness to engage deeply with our work. We fully understand the importance of clearly articulating how StrategyLLM’s structured reasoning framework helps to mitigate common pitfalls that may occur in CoT or ZS approaches. Based on your valuable feedback, we will revise our paper to better highlight these complexities and provide detailed explanations of why StrategyLLM excels in handling them.\n\n---\n\nThank you for your thoughtful feedback and for taking the time to review our rebuttal. We are pleased to hear that the new results have helped clarify the necessity and importance of the various facets of our StrategyLLM approach. We appreciate your suggestions and will incorporate these clarifications into our revised manuscript to enhance the overall clarity and comprehensiveness of our work.\n\n---\n\n**A More Complicated Example:**\n\nWe present a more complicated example, with a difficulty level of 5, to highlight the benefits of StrategyLLM. “ZS” refers to the zero-shot baseline without any prompt engineering.\n\n**Question:** How many ways are there to arrange $6$ beads of distinct colors in a $2 \\times 3$ grid if reflections and rotations are considered the same? (In other words, two arrangements are considered the same if I can rotate and/or reflect one arrangement to get the other.)\n\n**Gold Answer:** 180\n\n**ZS Prediction:**\n\n```\nThere are $6!$ ways to arrange the $6$ beads in a line. However, when considering a $2 \\times 3$ grid, there are $6$ different ways to arrange the beads in a line that result in the same grid when considering reflections and rotations. Therefore, the total number of distinct arrangements is $\\frac{6!}{6} = 120$.\n```\n\n**CoT Prediction:**\n\n```\nWe can consider the $2 \\times 3$ grid as a $6$-element cycle. There are $6!$ ways to arrange the $6$ beads in a line, but we must divide by $6$ to account for rotations. However, this overcounts the number of arrangements by a factor of $2$ because we are also counting reflections. Therefore, the final answer is $\\frac{6!}{6 \\cdot 2} = \\boxed{60}$.\nAnswer: The answer is 60\n```\n**StrategyLLM Prediction:**\n\n```\n1. Understand the problem statement:\n  - We have 6 beads of distinct colors.\n  - We need to arrange these beads in a 2x3 grid.\n  - Reflections and rotations are considered the same, meaning two arrangements are considered the same if one can be obtained from the other by rotating or reflecting.\n\n2. Identify the counting or probability concept:\n  - We need to use the concept of combinations to solve this problem.\n\n3. Break down the problem:\n  - We have 6 beads to arrange in a 2x3 grid.\n  - We need to consider the symmetries of the grid to eliminate redundant cases.\n  - We need to determine the number of distinct arrangements considering reflections and rotations.\n\n4. Calculate the required counting or probability:\n  - To calculate the number of distinct arrangements, we need to count the number of combinations that satisfy the given conditions.\n  - We can use combinations to calculate this.\n\n  - Let's consider the possible arrangements:\n    - We can choose any of the 6 beads for the first position in the grid.\n    - We can choose any of the remaining 5 beads for the second position.\n    - We can choose any of the remaining 4 beads for the third position.\n    - We can choose any of the remaining 3 beads for the fourth position.\n    - We can choose any of the remaining 2 beads for the fifth position.\n    - We can choose the last remaining bead for the sixth position.\n\n  - Total number of distinct arrangements = 6 x 5 x 4 x 3 x 2 x 1 = 720.\n\n5. Simplify or reduce the problem:\n  - We need to consider the symmetries of the grid to eliminate redundant cases.\n  - The 2x3 grid has rotational symmetry of order 2, meaning it can be rotated 180 degrees to look the same.\n  - The 2x3 grid also has reflectional symmetry, meaning it can be reflected horizontally or vertically to look the same.\n  - Therefore, we need to divide the total number of distinct arrangements by 2 to account for the rotational symmetry.\n  - We also need to divide the total number of distinct arrangements by 2 to account for the reflectional symmetry.\n\n6. Calculate the total number of distinct arrangements:\n  - Total number of distinct arrangements = 720 / 2 / 2 = 180.\n\nAnswer: The number of ways to arrange 6 beads of distinct colors in a 2x3 grid, considering reflections and rotations as the same, is 180\n```\n\nWe observe that both ZS and CoT handle rotations and reflections incorrectly and fail to provide adequate details on how to address them. In contrast, StrategyLLM offers comprehensive details and applies correct logic, resulting in the correct solution. This example clearly demonstrates the superiority of StrategyLLM in complex reasoning tasks.\n\n**Conclusion:**\n\nIn summary, while a simple prompt may occasionally yield correct solutions, our findings indicate that advanced methods like CoT, and more importantly, our StrategyLLM framework, provide more reliable and consistent results across a wide range of examples. We hope this explanation addresses your concerns and clarifies the rationale behind our approach.\n\n---\n\nThank you for your feedback. We appreciate the opportunity for further clarification.\n\n**Clarification:**\n\nWe would like to clarify that the example provided in our discussion was primarily intended to illustrate the motivation and benefits of our proposed StrategyLLM framework. The true measure of a method's effectiveness comes from its performance across diverse test sets of established benchmarks. Our extensive experimental results clearly demonstrate the superiority of StrategyLLM over baseline approaches, including CoT, across a wide range of LLMs and tasks.\n\n**Understanding LLM Behavior:**\n\nIt’s important to recognize that LLMs, such as gpt-3.5-turbo, are autoregressive text generators. This means that the same problem can yield different solutions depending on the prompt used, or even when using the same prompt with temperature sampling. Therefore, it is not surprising that for certain examples, an LLM may generate a correct solution without any prompt engineering or advanced techniques. However, this does not imply a flaw in the CoT implementation. **We want to reassure you that there is no bug or issue in our CoT implementation.** While CoT does not guarantee outperformance over a simple zero-shot baseline on every individual example, its effectiveness is evident when evaluated across a larger test set, where it generally achieves higher accuracy.\n\nOur StrategyLLM framework is designed to increase the likelihood of obtaining correct answers by guiding the LLM through a structured reasoning process, ensuring a more consistent and reliable approach across different problems.\n\n\n**Prediction of Simple Prompt:**\n\nGiven the optimization of frontier chat LLMs like “gpt-3.5-turbo” for step-by-step reasoning, we expect that even a simple prompt can yield a step-by-step solution for the provided example using “gpt-3.5-turbo-16k-0613”. However, predicting the exact reasoning steps in advance is quite challenging due to the inherent variability in LLM outputs. Since \"gpt-3.5-turbo-16k-0613\" has been deprecated, we used an improved version, \"gpt-3.5-turbo-1106,\" to generate a prediction with a temperature setting of 0. The model produced the following solution:\n\n```\nFirst, we need to consider the number of ways to arrange the 4 non-math books in the middle of the stack. This can be calculated using the formula for permutations of n objects, which is n!. In this case, there are 4 non-math books, so the number of ways to arrange them is 4!.\n\nNext, we need to consider the number of ways to arrange the 2 math books on the ends of the stack. Since there are 2 math books, the number of ways to arrange them is 2!.\n\nFinally, we can multiply these two numbers together to find the total number of ways to stack the books with a math book on both ends:\n\n4! * 2! = 24 * 2 = 48\n\nSo, there are 48 different ways to stack the 6 books on the shelf with a math book on both ends.\n```\n\nThis solution indeed derives the correct answer. However, as mentioned earlier, solving a single example correctly is not sufficient to establish the overall effectiveness of a method. To provide a broader comparison, we tested both the zero-shot baseline and the CoT approach on the CP dataset using \"gpt-3.5-turbo-1106\". The results were 33.0 for the zero-shot baseline and 36.5 for CoT, indicating that the CoT approach outperforms the simple baseline on the full test set.\n\n**A More Complicated Example:**\n\nThe discussion regarding this complicated example is shown in the second part of our response (i.e., the next official comment).\n\n---\n\nThank you for your review and for confirming that our paper does not raise ethical issues related to discrimination and bias.\n\n---\n\nThank you for your valuable feedback regarding our submission. We appreciate your acknowledgment that our paper does not directly present ethical issues concerning gender and race bias. As suggested, we will consider including a discussion in the discussion section of our paper to address how StrategyLLM might influence gender and race bias.\n\n---\n\nThank you for your continued feedback and for highlighting the need for further clarification regarding the differentiation between the CoT approach and our proposed StrategyLLM method. We appreciate the opportunity to elaborate on this critical aspect.\n\n**Clarification:**\n\nWe would like to clarify that the underlying LLM for the strategy generator is the same as the LLM being tested. Additionally, CoT and StrategyLLM leverage the same LLM for problem interpretation and resolution. However, the fundamental difference between these methods lies in their approach to guiding the LLM’s reasoning process.\n\n**Understanding LLM Capabilities:**\n\nIt’s important to recognize that the LLM itself has the capacity to understand the details of a problem, such as the requirement to place math books at both ends of a stack. However, how effectively the LLM applies this understanding can vary significantly depending on the prompting method used.\n\n**The Role of Structured Strategy in Mitigating Misinterpretation:**\n\nThe StrategyLLM approach is designed to mitigate the likelihood of misinterpretations through a structured reasoning process. Unlike CoT, which primarily guides the model to think step-by-step, StrategyLLM encourages a more holistic understanding of the problem by enforcing a structured strategy that must be followed consistently across examples. \n\nFor instance, in the provided example, StrategyLLM ensures a systematic approach to solving it:\n\n*Step 1: Problem Understanding:* The strategy mandates a clear understanding of the problem requirements, including the necessity for math books at both ends of the stack.\n\n*Step 2: Identifying Relevant Concepts:* The strategy requires identifying the relevant counting or probability concepts needed to solve the problem, ensuring that the LLM considers the correct mathematical approach.\n\n*Step 3: Breakdown of the Problem:* The strategy explicitly requires identifying the critical elements of the problem, such as the placement of the math books and the arrangement of the remaining books.\n\n*Step 4: Calculation:* The strategy then guides the model to apply relevant mathematical concepts (e.g., permutations) to derive the correct solution.\n\nWe can observe that the strategy itself does not directly identify critical details; rather, it strongly encourages the LLM to recognize these details for each problem. This approach significantly reduces the risk of overlooking crucial aspects.\n\n**Differentiating from CoT:**\n\nCoT allows for a more flexible reasoning process, which can sometimes lead to inconsistencies or omissions in understanding and solving problems. In the case of the example provided, CoT failed to capture the requirement of placing math books at both ends because the step-by-step reasoning did not enforce a structured breakdown of the problem. Instead, it focused on treating the two math books as a single entity, leading to an incorrect interpretation.\n\n**Conclusion:**\n\nIn conclusion, while both CoT and StrategyLLM rely on the LLM’s interpretation, StrategyLLM’s structured approach reduces the likelihood of critical misinterpretations. This structured reasoning process ensures that all essential aspects of a problem are considered, leading to more accurate and reliable solutions. \n\nWe hope this clarifies the distinction and addresses your concern. We appreciate your feedback and are committed to addressing any further questions or concerns you may have.", "author_response": "Thank you for your feedback. We appreciate the opportunity for further clarification.\n\n**Clarification:**\n\nWe would like to clarify that the example provided in our discussion was primarily intended to illustrate the motivation and benefits of our proposed StrategyLLM framework. The true measure of a method's effectiveness comes from its performance across diverse test sets of established benchmarks. Our extensive experimental results clearly demonstrate the superiority of StrategyLLM over baseline approaches, including CoT, across a wide range of LLMs and tasks.\n\n**Understanding LLM Behavior:**\n\nIt’s important to recognize that LLMs, such as gpt-3.5-turbo, are autoregressive text generators. This means that the same problem can yield different solutions depending on the prompt used, or even when using the same prompt with temperature sampling. Therefore, it is not surprising that for certain examples, an LLM may generate a correct solution without any prompt engineering or advanced techniques. However, this does not imply a flaw in the CoT implementation. **We want to reassure you that there is no bug or issue in our CoT implementation.** While CoT does not guarantee outperformance over a simple zero-shot baseline on every individual example, its effectiveness is evident when evaluated across a larger test set, where it generally achieves higher accuracy.\n\nOur StrategyLLM framework is designed to increase the likelihood of obtaining correct answers by guiding the LLM through a structured reasoning process, ensuring a more consistent and reliable approach across different problems.\n\n\n**Prediction of Simple Prompt:**\n\nGiven the optimization of frontier chat LLMs like “gpt-3.5-turbo” for step-by-step reasoning, we expect that even a simple prompt can yield a step-by-step solution for the provided example using “gpt-3.5-turbo-16k-0613”. However, predicting the exact reasoning steps in advance is quite challenging due to the inherent variability in LLM outputs. Since \"gpt-3.5-turbo-16k-0613\" has been deprecated, we used an improved version, \"gpt-3.5-turbo-1106,\" to generate a prediction with a temperature setting of 0. The model produced the following solution:\n\n```\nFirst, we need to consider the number of ways to arrange the 4 non-math books in the middle of the stack. This can be calculated using the formula for permutations of n objects, which is n!. In this case, there are 4 non-math books, so the number of ways to arrange them is 4!.\n\nNext, we need to consider the number of ways to arrange the 2 math books on the ends of the stack. Since there are 2 math books, the number of ways to arrange them is 2!.\n\nFinally, we can multiply these two numbers together to find the total number of ways to stack the books with a math book on both ends:\n\n4! * 2! = 24 * 2 = 48\n\nSo, there are 48 different ways to stack the 6 books on the shelf with a math book on both ends.\n```\n\nThis solution indeed derives the correct answer. However, as mentioned earlier, solving a single example correctly is not sufficient to establish the overall effectiveness of a method. To provide a broader comparison, we tested both the zero-shot baseline and the CoT approach on the CP dataset using \"gpt-3.5-turbo-1106\". The results were 33.0 for the zero-shot baseline and 36.5 for CoT, indicating that the CoT approach outperforms the simple baseline on the full test set.\n\n**A More Complicated Example:**\n\nThe discussion regarding this complicated example is shown in the second part of our response (i.e., the next official comment)."}
{"claim": "A simple direct prompt without advanced techniques like CoT or StrategyML should be able to solve the given combinatorics example.", "claim_type": "methodology", "paper_id": "UkxJd64mki", "paper_title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "fGEvjzIyly", "reviewer": "Reviewer_D2vF", "review_text": "Comment: Thanks again for your thorough explanations. Can authors let me know what they expect to get from \"gpt-3.5-turbo-16k-0613\" without using any prompt engineering, few shot learning, and CoT if they pose the following question directly to the prompt?\n\"Suppose that I have 6 different books, 2 of which are math books. In how many ways can I stack my 6 books on a shelf if I want a math book on both ends of the stack?\" My observation is that for this particular example, even a simple prompt without using more advanced approaches such as CoT or StrategyML should be able to solve the problem. Therefore, I suspect something in the CoT implementation is not working as intended and attributing a bug/issue in CoT implementation to its inferior performance compared with StrategyML is not advisable.", "labeling_timestamp": "2026-01-11T16:28:05.392868", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors acknowledge that a simple prompt can produce the correct solution for the example, but they argue this is not reliable or sufficient evidence and emphasize the need for structured methods for consistent performance across datasets.", "evidence": "\"it is not surprising that for certain examples, an LLM may generate a correct solution without any prompt engineering or advanced techniques.\" \"This solution indeed derives the correct answer. However, as mentioned earlier, solving a single example correctly is not sufficient to establish the overall effectiveness of a method.\"", "combined_author_response": "Thank you for your thoughtful consideration and for adjusting your evaluation based on our latest discussions. We truly appreciate your willingness to engage deeply with our work. We fully understand the importance of clearly articulating how StrategyLLM’s structured reasoning framework helps to mitigate common pitfalls that may occur in CoT or ZS approaches. Based on your valuable feedback, we will revise our paper to better highlight these complexities and provide detailed explanations of why StrategyLLM excels in handling them.\n\n---\n\nThank you for your thoughtful feedback and for taking the time to review our rebuttal. We are pleased to hear that the new results have helped clarify the necessity and importance of the various facets of our StrategyLLM approach. We appreciate your suggestions and will incorporate these clarifications into our revised manuscript to enhance the overall clarity and comprehensiveness of our work.\n\n---\n\n**A More Complicated Example:**\n\nWe present a more complicated example, with a difficulty level of 5, to highlight the benefits of StrategyLLM. “ZS” refers to the zero-shot baseline without any prompt engineering.\n\n**Question:** How many ways are there to arrange $6$ beads of distinct colors in a $2 \\times 3$ grid if reflections and rotations are considered the same? (In other words, two arrangements are considered the same if I can rotate and/or reflect one arrangement to get the other.)\n\n**Gold Answer:** 180\n\n**ZS Prediction:**\n\n```\nThere are $6!$ ways to arrange the $6$ beads in a line. However, when considering a $2 \\times 3$ grid, there are $6$ different ways to arrange the beads in a line that result in the same grid when considering reflections and rotations. Therefore, the total number of distinct arrangements is $\\frac{6!}{6} = 120$.\n```\n\n**CoT Prediction:**\n\n```\nWe can consider the $2 \\times 3$ grid as a $6$-element cycle. There are $6!$ ways to arrange the $6$ beads in a line, but we must divide by $6$ to account for rotations. However, this overcounts the number of arrangements by a factor of $2$ because we are also counting reflections. Therefore, the final answer is $\\frac{6!}{6 \\cdot 2} = \\boxed{60}$.\nAnswer: The answer is 60\n```\n**StrategyLLM Prediction:**\n\n```\n1. Understand the problem statement:\n  - We have 6 beads of distinct colors.\n  - We need to arrange these beads in a 2x3 grid.\n  - Reflections and rotations are considered the same, meaning two arrangements are considered the same if one can be obtained from the other by rotating or reflecting.\n\n2. Identify the counting or probability concept:\n  - We need to use the concept of combinations to solve this problem.\n\n3. Break down the problem:\n  - We have 6 beads to arrange in a 2x3 grid.\n  - We need to consider the symmetries of the grid to eliminate redundant cases.\n  - We need to determine the number of distinct arrangements considering reflections and rotations.\n\n4. Calculate the required counting or probability:\n  - To calculate the number of distinct arrangements, we need to count the number of combinations that satisfy the given conditions.\n  - We can use combinations to calculate this.\n\n  - Let's consider the possible arrangements:\n    - We can choose any of the 6 beads for the first position in the grid.\n    - We can choose any of the remaining 5 beads for the second position.\n    - We can choose any of the remaining 4 beads for the third position.\n    - We can choose any of the remaining 3 beads for the fourth position.\n    - We can choose any of the remaining 2 beads for the fifth position.\n    - We can choose the last remaining bead for the sixth position.\n\n  - Total number of distinct arrangements = 6 x 5 x 4 x 3 x 2 x 1 = 720.\n\n5. Simplify or reduce the problem:\n  - We need to consider the symmetries of the grid to eliminate redundant cases.\n  - The 2x3 grid has rotational symmetry of order 2, meaning it can be rotated 180 degrees to look the same.\n  - The 2x3 grid also has reflectional symmetry, meaning it can be reflected horizontally or vertically to look the same.\n  - Therefore, we need to divide the total number of distinct arrangements by 2 to account for the rotational symmetry.\n  - We also need to divide the total number of distinct arrangements by 2 to account for the reflectional symmetry.\n\n6. Calculate the total number of distinct arrangements:\n  - Total number of distinct arrangements = 720 / 2 / 2 = 180.\n\nAnswer: The number of ways to arrange 6 beads of distinct colors in a 2x3 grid, considering reflections and rotations as the same, is 180\n```\n\nWe observe that both ZS and CoT handle rotations and reflections incorrectly and fail to provide adequate details on how to address them. In contrast, StrategyLLM offers comprehensive details and applies correct logic, resulting in the correct solution. This example clearly demonstrates the superiority of StrategyLLM in complex reasoning tasks.\n\n**Conclusion:**\n\nIn summary, while a simple prompt may occasionally yield correct solutions, our findings indicate that advanced methods like CoT, and more importantly, our StrategyLLM framework, provide more reliable and consistent results across a wide range of examples. We hope this explanation addresses your concerns and clarifies the rationale behind our approach.\n\n---\n\nThank you for your feedback. We appreciate the opportunity for further clarification.\n\n**Clarification:**\n\nWe would like to clarify that the example provided in our discussion was primarily intended to illustrate the motivation and benefits of our proposed StrategyLLM framework. The true measure of a method's effectiveness comes from its performance across diverse test sets of established benchmarks. Our extensive experimental results clearly demonstrate the superiority of StrategyLLM over baseline approaches, including CoT, across a wide range of LLMs and tasks.\n\n**Understanding LLM Behavior:**\n\nIt’s important to recognize that LLMs, such as gpt-3.5-turbo, are autoregressive text generators. This means that the same problem can yield different solutions depending on the prompt used, or even when using the same prompt with temperature sampling. Therefore, it is not surprising that for certain examples, an LLM may generate a correct solution without any prompt engineering or advanced techniques. However, this does not imply a flaw in the CoT implementation. **We want to reassure you that there is no bug or issue in our CoT implementation.** While CoT does not guarantee outperformance over a simple zero-shot baseline on every individual example, its effectiveness is evident when evaluated across a larger test set, where it generally achieves higher accuracy.\n\nOur StrategyLLM framework is designed to increase the likelihood of obtaining correct answers by guiding the LLM through a structured reasoning process, ensuring a more consistent and reliable approach across different problems.\n\n\n**Prediction of Simple Prompt:**\n\nGiven the optimization of frontier chat LLMs like “gpt-3.5-turbo” for step-by-step reasoning, we expect that even a simple prompt can yield a step-by-step solution for the provided example using “gpt-3.5-turbo-16k-0613”. However, predicting the exact reasoning steps in advance is quite challenging due to the inherent variability in LLM outputs. Since \"gpt-3.5-turbo-16k-0613\" has been deprecated, we used an improved version, \"gpt-3.5-turbo-1106,\" to generate a prediction with a temperature setting of 0. The model produced the following solution:\n\n```\nFirst, we need to consider the number of ways to arrange the 4 non-math books in the middle of the stack. This can be calculated using the formula for permutations of n objects, which is n!. In this case, there are 4 non-math books, so the number of ways to arrange them is 4!.\n\nNext, we need to consider the number of ways to arrange the 2 math books on the ends of the stack. Since there are 2 math books, the number of ways to arrange them is 2!.\n\nFinally, we can multiply these two numbers together to find the total number of ways to stack the books with a math book on both ends:\n\n4! * 2! = 24 * 2 = 48\n\nSo, there are 48 different ways to stack the 6 books on the shelf with a math book on both ends.\n```\n\nThis solution indeed derives the correct answer. However, as mentioned earlier, solving a single example correctly is not sufficient to establish the overall effectiveness of a method. To provide a broader comparison, we tested both the zero-shot baseline and the CoT approach on the CP dataset using \"gpt-3.5-turbo-1106\". The results were 33.0 for the zero-shot baseline and 36.5 for CoT, indicating that the CoT approach outperforms the simple baseline on the full test set.\n\n**A More Complicated Example:**\n\nThe discussion regarding this complicated example is shown in the second part of our response (i.e., the next official comment).\n\n---\n\nThank you for your review and for confirming that our paper does not raise ethical issues related to discrimination and bias.\n\n---\n\nThank you for your valuable feedback regarding our submission. We appreciate your acknowledgment that our paper does not directly present ethical issues concerning gender and race bias. As suggested, we will consider including a discussion in the discussion section of our paper to address how StrategyLLM might influence gender and race bias.\n\n---\n\nThank you for your continued feedback and for highlighting the need for further clarification regarding the differentiation between the CoT approach and our proposed StrategyLLM method. We appreciate the opportunity to elaborate on this critical aspect.\n\n**Clarification:**\n\nWe would like to clarify that the underlying LLM for the strategy generator is the same as the LLM being tested. Additionally, CoT and StrategyLLM leverage the same LLM for problem interpretation and resolution. However, the fundamental difference between these methods lies in their approach to guiding the LLM’s reasoning process.\n\n**Understanding LLM Capabilities:**\n\nIt’s important to recognize that the LLM itself has the capacity to understand the details of a problem, such as the requirement to place math books at both ends of a stack. However, how effectively the LLM applies this understanding can vary significantly depending on the prompting method used.\n\n**The Role of Structured Strategy in Mitigating Misinterpretation:**\n\nThe StrategyLLM approach is designed to mitigate the likelihood of misinterpretations through a structured reasoning process. Unlike CoT, which primarily guides the model to think step-by-step, StrategyLLM encourages a more holistic understanding of the problem by enforcing a structured strategy that must be followed consistently across examples. \n\nFor instance, in the provided example, StrategyLLM ensures a systematic approach to solving it:\n\n*Step 1: Problem Understanding:* The strategy mandates a clear understanding of the problem requirements, including the necessity for math books at both ends of the stack.\n\n*Step 2: Identifying Relevant Concepts:* The strategy requires identifying the relevant counting or probability concepts needed to solve the problem, ensuring that the LLM considers the correct mathematical approach.\n\n*Step 3: Breakdown of the Problem:* The strategy explicitly requires identifying the critical elements of the problem, such as the placement of the math books and the arrangement of the remaining books.\n\n*Step 4: Calculation:* The strategy then guides the model to apply relevant mathematical concepts (e.g., permutations) to derive the correct solution.\n\nWe can observe that the strategy itself does not directly identify critical details; rather, it strongly encourages the LLM to recognize these details for each problem. This approach significantly reduces the risk of overlooking crucial aspects.\n\n**Differentiating from CoT:**\n\nCoT allows for a more flexible reasoning process, which can sometimes lead to inconsistencies or omissions in understanding and solving problems. In the case of the example provided, CoT failed to capture the requirement of placing math books at both ends because the step-by-step reasoning did not enforce a structured breakdown of the problem. Instead, it focused on treating the two math books as a single entity, leading to an incorrect interpretation.\n\n**Conclusion:**\n\nIn conclusion, while both CoT and StrategyLLM rely on the LLM’s interpretation, StrategyLLM’s structured approach reduces the likelihood of critical misinterpretations. This structured reasoning process ensures that all essential aspects of a problem are considered, leading to more accurate and reliable solutions. \n\nWe hope this clarifies the distinction and addresses your concern. We appreciate your feedback and are committed to addressing any further questions or concerns you may have.", "author_response": "Thank you for your feedback. We appreciate the opportunity for further clarification.\n\n**Clarification:**\n\nWe would like to clarify that the example provided in our discussion was primarily intended to illustrate the motivation and benefits of our proposed StrategyLLM framework. The true measure of a method's effectiveness comes from its performance across diverse test sets of established benchmarks. Our extensive experimental results clearly demonstrate the superiority of StrategyLLM over baseline approaches, including CoT, across a wide range of LLMs and tasks.\n\n**Understanding LLM Behavior:**\n\nIt’s important to recognize that LLMs, such as gpt-3.5-turbo, are autoregressive text generators. This means that the same problem can yield different solutions depending on the prompt used, or even when using the same prompt with temperature sampling. Therefore, it is not surprising that for certain examples, an LLM may generate a correct solution without any prompt engineering or advanced techniques. However, this does not imply a flaw in the CoT implementation. **We want to reassure you that there is no bug or issue in our CoT implementation.** While CoT does not guarantee outperformance over a simple zero-shot baseline on every individual example, its effectiveness is evident when evaluated across a larger test set, where it generally achieves higher accuracy.\n\nOur StrategyLLM framework is designed to increase the likelihood of obtaining correct answers by guiding the LLM through a structured reasoning process, ensuring a more consistent and reliable approach across different problems.\n\n\n**Prediction of Simple Prompt:**\n\nGiven the optimization of frontier chat LLMs like “gpt-3.5-turbo” for step-by-step reasoning, we expect that even a simple prompt can yield a step-by-step solution for the provided example using “gpt-3.5-turbo-16k-0613”. However, predicting the exact reasoning steps in advance is quite challenging due to the inherent variability in LLM outputs. Since \"gpt-3.5-turbo-16k-0613\" has been deprecated, we used an improved version, \"gpt-3.5-turbo-1106,\" to generate a prediction with a temperature setting of 0. The model produced the following solution:\n\n```\nFirst, we need to consider the number of ways to arrange the 4 non-math books in the middle of the stack. This can be calculated using the formula for permutations of n objects, which is n!. In this case, there are 4 non-math books, so the number of ways to arrange them is 4!.\n\nNext, we need to consider the number of ways to arrange the 2 math books on the ends of the stack. Since there are 2 math books, the number of ways to arrange them is 2!.\n\nFinally, we can multiply these two numbers together to find the total number of ways to stack the books with a math book on both ends:\n\n4! * 2! = 24 * 2 = 48\n\nSo, there are 48 different ways to stack the 6 books on the shelf with a math book on both ends.\n```\n\nThis solution indeed derives the correct answer. However, as mentioned earlier, solving a single example correctly is not sufficient to establish the overall effectiveness of a method. To provide a broader comparison, we tested both the zero-shot baseline and the CoT approach on the CP dataset using \"gpt-3.5-turbo-1106\". The results were 33.0 for the zero-shot baseline and 36.5 for CoT, indicating that the CoT approach outperforms the simple baseline on the full test set.\n\n**A More Complicated Example:**\n\nThe discussion regarding this complicated example is shown in the second part of our response (i.e., the next official comment)."}
{"claim": "Something in the chain-of-thought (CoT) implementation appears to be malfunctioning or not working as intended for the provided example.", "claim_type": "methodology", "paper_id": "UkxJd64mki", "paper_title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "fGEvjzIyly", "reviewer": "Reviewer_D2vF", "review_text": "Comment: Thanks again for your thorough explanations. Can authors let me know what they expect to get from \"gpt-3.5-turbo-16k-0613\" without using any prompt engineering, few shot learning, and CoT if they pose the following question directly to the prompt?\n\"Suppose that I have 6 different books, 2 of which are math books. In how many ways can I stack my 6 books on a shelf if I want a math book on both ends of the stack?\" My observation is that for this particular example, even a simple prompt without using more advanced approaches such as CoT or StrategyML should be able to solve the problem. Therefore, I suspect something in the CoT implementation is not working as intended and attributing a bug/issue in CoT implementation to its inferior performance compared with StrategyML is not advisable.", "labeling_timestamp": "2026-01-11T16:27:56.428258", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The authors explicitly deny that the CoT implementation is malfunctioning, attributing errors on individual examples to LLM variability and methodological differences rather than a bug.", "evidence": "\"We want to reassure you that there is no bug or issue in our CoT implementation.\"", "combined_author_response": "Thank you for your thoughtful consideration and for adjusting your evaluation based on our latest discussions. We truly appreciate your willingness to engage deeply with our work. We fully understand the importance of clearly articulating how StrategyLLM’s structured reasoning framework helps to mitigate common pitfalls that may occur in CoT or ZS approaches. Based on your valuable feedback, we will revise our paper to better highlight these complexities and provide detailed explanations of why StrategyLLM excels in handling them.\n\n---\n\nThank you for your thoughtful feedback and for taking the time to review our rebuttal. We are pleased to hear that the new results have helped clarify the necessity and importance of the various facets of our StrategyLLM approach. We appreciate your suggestions and will incorporate these clarifications into our revised manuscript to enhance the overall clarity and comprehensiveness of our work.\n\n---\n\n**A More Complicated Example:**\n\nWe present a more complicated example, with a difficulty level of 5, to highlight the benefits of StrategyLLM. “ZS” refers to the zero-shot baseline without any prompt engineering.\n\n**Question:** How many ways are there to arrange $6$ beads of distinct colors in a $2 \\times 3$ grid if reflections and rotations are considered the same? (In other words, two arrangements are considered the same if I can rotate and/or reflect one arrangement to get the other.)\n\n**Gold Answer:** 180\n\n**ZS Prediction:**\n\n```\nThere are $6!$ ways to arrange the $6$ beads in a line. However, when considering a $2 \\times 3$ grid, there are $6$ different ways to arrange the beads in a line that result in the same grid when considering reflections and rotations. Therefore, the total number of distinct arrangements is $\\frac{6!}{6} = 120$.\n```\n\n**CoT Prediction:**\n\n```\nWe can consider the $2 \\times 3$ grid as a $6$-element cycle. There are $6!$ ways to arrange the $6$ beads in a line, but we must divide by $6$ to account for rotations. However, this overcounts the number of arrangements by a factor of $2$ because we are also counting reflections. Therefore, the final answer is $\\frac{6!}{6 \\cdot 2} = \\boxed{60}$.\nAnswer: The answer is 60\n```\n**StrategyLLM Prediction:**\n\n```\n1. Understand the problem statement:\n  - We have 6 beads of distinct colors.\n  - We need to arrange these beads in a 2x3 grid.\n  - Reflections and rotations are considered the same, meaning two arrangements are considered the same if one can be obtained from the other by rotating or reflecting.\n\n2. Identify the counting or probability concept:\n  - We need to use the concept of combinations to solve this problem.\n\n3. Break down the problem:\n  - We have 6 beads to arrange in a 2x3 grid.\n  - We need to consider the symmetries of the grid to eliminate redundant cases.\n  - We need to determine the number of distinct arrangements considering reflections and rotations.\n\n4. Calculate the required counting or probability:\n  - To calculate the number of distinct arrangements, we need to count the number of combinations that satisfy the given conditions.\n  - We can use combinations to calculate this.\n\n  - Let's consider the possible arrangements:\n    - We can choose any of the 6 beads for the first position in the grid.\n    - We can choose any of the remaining 5 beads for the second position.\n    - We can choose any of the remaining 4 beads for the third position.\n    - We can choose any of the remaining 3 beads for the fourth position.\n    - We can choose any of the remaining 2 beads for the fifth position.\n    - We can choose the last remaining bead for the sixth position.\n\n  - Total number of distinct arrangements = 6 x 5 x 4 x 3 x 2 x 1 = 720.\n\n5. Simplify or reduce the problem:\n  - We need to consider the symmetries of the grid to eliminate redundant cases.\n  - The 2x3 grid has rotational symmetry of order 2, meaning it can be rotated 180 degrees to look the same.\n  - The 2x3 grid also has reflectional symmetry, meaning it can be reflected horizontally or vertically to look the same.\n  - Therefore, we need to divide the total number of distinct arrangements by 2 to account for the rotational symmetry.\n  - We also need to divide the total number of distinct arrangements by 2 to account for the reflectional symmetry.\n\n6. Calculate the total number of distinct arrangements:\n  - Total number of distinct arrangements = 720 / 2 / 2 = 180.\n\nAnswer: The number of ways to arrange 6 beads of distinct colors in a 2x3 grid, considering reflections and rotations as the same, is 180\n```\n\nWe observe that both ZS and CoT handle rotations and reflections incorrectly and fail to provide adequate details on how to address them. In contrast, StrategyLLM offers comprehensive details and applies correct logic, resulting in the correct solution. This example clearly demonstrates the superiority of StrategyLLM in complex reasoning tasks.\n\n**Conclusion:**\n\nIn summary, while a simple prompt may occasionally yield correct solutions, our findings indicate that advanced methods like CoT, and more importantly, our StrategyLLM framework, provide more reliable and consistent results across a wide range of examples. We hope this explanation addresses your concerns and clarifies the rationale behind our approach.\n\n---\n\nThank you for your feedback. We appreciate the opportunity for further clarification.\n\n**Clarification:**\n\nWe would like to clarify that the example provided in our discussion was primarily intended to illustrate the motivation and benefits of our proposed StrategyLLM framework. The true measure of a method's effectiveness comes from its performance across diverse test sets of established benchmarks. Our extensive experimental results clearly demonstrate the superiority of StrategyLLM over baseline approaches, including CoT, across a wide range of LLMs and tasks.\n\n**Understanding LLM Behavior:**\n\nIt’s important to recognize that LLMs, such as gpt-3.5-turbo, are autoregressive text generators. This means that the same problem can yield different solutions depending on the prompt used, or even when using the same prompt with temperature sampling. Therefore, it is not surprising that for certain examples, an LLM may generate a correct solution without any prompt engineering or advanced techniques. However, this does not imply a flaw in the CoT implementation. **We want to reassure you that there is no bug or issue in our CoT implementation.** While CoT does not guarantee outperformance over a simple zero-shot baseline on every individual example, its effectiveness is evident when evaluated across a larger test set, where it generally achieves higher accuracy.\n\nOur StrategyLLM framework is designed to increase the likelihood of obtaining correct answers by guiding the LLM through a structured reasoning process, ensuring a more consistent and reliable approach across different problems.\n\n\n**Prediction of Simple Prompt:**\n\nGiven the optimization of frontier chat LLMs like “gpt-3.5-turbo” for step-by-step reasoning, we expect that even a simple prompt can yield a step-by-step solution for the provided example using “gpt-3.5-turbo-16k-0613”. However, predicting the exact reasoning steps in advance is quite challenging due to the inherent variability in LLM outputs. Since \"gpt-3.5-turbo-16k-0613\" has been deprecated, we used an improved version, \"gpt-3.5-turbo-1106,\" to generate a prediction with a temperature setting of 0. The model produced the following solution:\n\n```\nFirst, we need to consider the number of ways to arrange the 4 non-math books in the middle of the stack. This can be calculated using the formula for permutations of n objects, which is n!. In this case, there are 4 non-math books, so the number of ways to arrange them is 4!.\n\nNext, we need to consider the number of ways to arrange the 2 math books on the ends of the stack. Since there are 2 math books, the number of ways to arrange them is 2!.\n\nFinally, we can multiply these two numbers together to find the total number of ways to stack the books with a math book on both ends:\n\n4! * 2! = 24 * 2 = 48\n\nSo, there are 48 different ways to stack the 6 books on the shelf with a math book on both ends.\n```\n\nThis solution indeed derives the correct answer. However, as mentioned earlier, solving a single example correctly is not sufficient to establish the overall effectiveness of a method. To provide a broader comparison, we tested both the zero-shot baseline and the CoT approach on the CP dataset using \"gpt-3.5-turbo-1106\". The results were 33.0 for the zero-shot baseline and 36.5 for CoT, indicating that the CoT approach outperforms the simple baseline on the full test set.\n\n**A More Complicated Example:**\n\nThe discussion regarding this complicated example is shown in the second part of our response (i.e., the next official comment).\n\n---\n\nThank you for your review and for confirming that our paper does not raise ethical issues related to discrimination and bias.\n\n---\n\nThank you for your valuable feedback regarding our submission. We appreciate your acknowledgment that our paper does not directly present ethical issues concerning gender and race bias. As suggested, we will consider including a discussion in the discussion section of our paper to address how StrategyLLM might influence gender and race bias.\n\n---\n\nThank you for your continued feedback and for highlighting the need for further clarification regarding the differentiation between the CoT approach and our proposed StrategyLLM method. We appreciate the opportunity to elaborate on this critical aspect.\n\n**Clarification:**\n\nWe would like to clarify that the underlying LLM for the strategy generator is the same as the LLM being tested. Additionally, CoT and StrategyLLM leverage the same LLM for problem interpretation and resolution. However, the fundamental difference between these methods lies in their approach to guiding the LLM’s reasoning process.\n\n**Understanding LLM Capabilities:**\n\nIt’s important to recognize that the LLM itself has the capacity to understand the details of a problem, such as the requirement to place math books at both ends of a stack. However, how effectively the LLM applies this understanding can vary significantly depending on the prompting method used.\n\n**The Role of Structured Strategy in Mitigating Misinterpretation:**\n\nThe StrategyLLM approach is designed to mitigate the likelihood of misinterpretations through a structured reasoning process. Unlike CoT, which primarily guides the model to think step-by-step, StrategyLLM encourages a more holistic understanding of the problem by enforcing a structured strategy that must be followed consistently across examples. \n\nFor instance, in the provided example, StrategyLLM ensures a systematic approach to solving it:\n\n*Step 1: Problem Understanding:* The strategy mandates a clear understanding of the problem requirements, including the necessity for math books at both ends of the stack.\n\n*Step 2: Identifying Relevant Concepts:* The strategy requires identifying the relevant counting or probability concepts needed to solve the problem, ensuring that the LLM considers the correct mathematical approach.\n\n*Step 3: Breakdown of the Problem:* The strategy explicitly requires identifying the critical elements of the problem, such as the placement of the math books and the arrangement of the remaining books.\n\n*Step 4: Calculation:* The strategy then guides the model to apply relevant mathematical concepts (e.g., permutations) to derive the correct solution.\n\nWe can observe that the strategy itself does not directly identify critical details; rather, it strongly encourages the LLM to recognize these details for each problem. This approach significantly reduces the risk of overlooking crucial aspects.\n\n**Differentiating from CoT:**\n\nCoT allows for a more flexible reasoning process, which can sometimes lead to inconsistencies or omissions in understanding and solving problems. In the case of the example provided, CoT failed to capture the requirement of placing math books at both ends because the step-by-step reasoning did not enforce a structured breakdown of the problem. Instead, it focused on treating the two math books as a single entity, leading to an incorrect interpretation.\n\n**Conclusion:**\n\nIn conclusion, while both CoT and StrategyLLM rely on the LLM’s interpretation, StrategyLLM’s structured approach reduces the likelihood of critical misinterpretations. This structured reasoning process ensures that all essential aspects of a problem are considered, leading to more accurate and reliable solutions. \n\nWe hope this clarifies the distinction and addresses your concern. We appreciate your feedback and are committed to addressing any further questions or concerns you may have.", "author_response": "Thank you for your feedback. We appreciate the opportunity for further clarification.\n\n**Clarification:**\n\nWe would like to clarify that the example provided in our discussion was primarily intended to illustrate the motivation and benefits of our proposed StrategyLLM framework. The true measure of a method's effectiveness comes from its performance across diverse test sets of established benchmarks. Our extensive experimental results clearly demonstrate the superiority of StrategyLLM over baseline approaches, including CoT, across a wide range of LLMs and tasks.\n\n**Understanding LLM Behavior:**\n\nIt’s important to recognize that LLMs, such as gpt-3.5-turbo, are autoregressive text generators. This means that the same problem can yield different solutions depending on the prompt used, or even when using the same prompt with temperature sampling. Therefore, it is not surprising that for certain examples, an LLM may generate a correct solution without any prompt engineering or advanced techniques. However, this does not imply a flaw in the CoT implementation. **We want to reassure you that there is no bug or issue in our CoT implementation.** While CoT does not guarantee outperformance over a simple zero-shot baseline on every individual example, its effectiveness is evident when evaluated across a larger test set, where it generally achieves higher accuracy.\n\nOur StrategyLLM framework is designed to increase the likelihood of obtaining correct answers by guiding the LLM through a structured reasoning process, ensuring a more consistent and reliable approach across different problems.\n\n\n**Prediction of Simple Prompt:**\n\nGiven the optimization of frontier chat LLMs like “gpt-3.5-turbo” for step-by-step reasoning, we expect that even a simple prompt can yield a step-by-step solution for the provided example using “gpt-3.5-turbo-16k-0613”. However, predicting the exact reasoning steps in advance is quite challenging due to the inherent variability in LLM outputs. Since \"gpt-3.5-turbo-16k-0613\" has been deprecated, we used an improved version, \"gpt-3.5-turbo-1106,\" to generate a prediction with a temperature setting of 0. The model produced the following solution:\n\n```\nFirst, we need to consider the number of ways to arrange the 4 non-math books in the middle of the stack. This can be calculated using the formula for permutations of n objects, which is n!. In this case, there are 4 non-math books, so the number of ways to arrange them is 4!.\n\nNext, we need to consider the number of ways to arrange the 2 math books on the ends of the stack. Since there are 2 math books, the number of ways to arrange them is 2!.\n\nFinally, we can multiply these two numbers together to find the total number of ways to stack the books with a math book on both ends:\n\n4! * 2! = 24 * 2 = 48\n\nSo, there are 48 different ways to stack the 6 books on the shelf with a math book on both ends.\n```\n\nThis solution indeed derives the correct answer. However, as mentioned earlier, solving a single example correctly is not sufficient to establish the overall effectiveness of a method. To provide a broader comparison, we tested both the zero-shot baseline and the CoT approach on the CP dataset using \"gpt-3.5-turbo-1106\". The results were 33.0 for the zero-shot baseline and 36.5 for CoT, indicating that the CoT approach outperforms the simple baseline on the full test set.\n\n**A More Complicated Example:**\n\nThe discussion regarding this complicated example is shown in the second part of our response (i.e., the next official comment)."}
{"claim": "Attributing CoT's inferior performance relative to StrategyML to a bug in the CoT implementation is not justified without further investigation.", "claim_type": "subjective", "paper_id": "UkxJd64mki", "paper_title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "fGEvjzIyly", "reviewer": "Reviewer_D2vF", "review_text": "Comment: Thanks again for your thorough explanations. Can authors let me know what they expect to get from \"gpt-3.5-turbo-16k-0613\" without using any prompt engineering, few shot learning, and CoT if they pose the following question directly to the prompt?\n\"Suppose that I have 6 different books, 2 of which are math books. In how many ways can I stack my 6 books on a shelf if I want a math book on both ends of the stack?\" My observation is that for this particular example, even a simple prompt without using more advanced approaches such as CoT or StrategyML should be able to solve the problem. Therefore, I suspect something in the CoT implementation is not working as intended and attributing a bug/issue in CoT implementation to its inferior performance compared with StrategyML is not advisable.", "labeling_timestamp": "2026-01-11T16:28:07.501443", "model": "gpt-5-mini", "label": "Supported", "justification": "Authors explicitly agree that poor CoT performance is not due to a bug: they reassure there is no bug, explain variability of LLM outputs, and point to test-set results showing CoT's effectiveness rather than a buggy implementation.", "evidence": "\"We want to reassure you that there is no bug or issue in our CoT implementation.\"", "combined_author_response": "Thank you for your thoughtful consideration and for adjusting your evaluation based on our latest discussions. We truly appreciate your willingness to engage deeply with our work. We fully understand the importance of clearly articulating how StrategyLLM’s structured reasoning framework helps to mitigate common pitfalls that may occur in CoT or ZS approaches. Based on your valuable feedback, we will revise our paper to better highlight these complexities and provide detailed explanations of why StrategyLLM excels in handling them.\n\n---\n\nThank you for your thoughtful feedback and for taking the time to review our rebuttal. We are pleased to hear that the new results have helped clarify the necessity and importance of the various facets of our StrategyLLM approach. We appreciate your suggestions and will incorporate these clarifications into our revised manuscript to enhance the overall clarity and comprehensiveness of our work.\n\n---\n\n**A More Complicated Example:**\n\nWe present a more complicated example, with a difficulty level of 5, to highlight the benefits of StrategyLLM. “ZS” refers to the zero-shot baseline without any prompt engineering.\n\n**Question:** How many ways are there to arrange $6$ beads of distinct colors in a $2 \\times 3$ grid if reflections and rotations are considered the same? (In other words, two arrangements are considered the same if I can rotate and/or reflect one arrangement to get the other.)\n\n**Gold Answer:** 180\n\n**ZS Prediction:**\n\n```\nThere are $6!$ ways to arrange the $6$ beads in a line. However, when considering a $2 \\times 3$ grid, there are $6$ different ways to arrange the beads in a line that result in the same grid when considering reflections and rotations. Therefore, the total number of distinct arrangements is $\\frac{6!}{6} = 120$.\n```\n\n**CoT Prediction:**\n\n```\nWe can consider the $2 \\times 3$ grid as a $6$-element cycle. There are $6!$ ways to arrange the $6$ beads in a line, but we must divide by $6$ to account for rotations. However, this overcounts the number of arrangements by a factor of $2$ because we are also counting reflections. Therefore, the final answer is $\\frac{6!}{6 \\cdot 2} = \\boxed{60}$.\nAnswer: The answer is 60\n```\n**StrategyLLM Prediction:**\n\n```\n1. Understand the problem statement:\n  - We have 6 beads of distinct colors.\n  - We need to arrange these beads in a 2x3 grid.\n  - Reflections and rotations are considered the same, meaning two arrangements are considered the same if one can be obtained from the other by rotating or reflecting.\n\n2. Identify the counting or probability concept:\n  - We need to use the concept of combinations to solve this problem.\n\n3. Break down the problem:\n  - We have 6 beads to arrange in a 2x3 grid.\n  - We need to consider the symmetries of the grid to eliminate redundant cases.\n  - We need to determine the number of distinct arrangements considering reflections and rotations.\n\n4. Calculate the required counting or probability:\n  - To calculate the number of distinct arrangements, we need to count the number of combinations that satisfy the given conditions.\n  - We can use combinations to calculate this.\n\n  - Let's consider the possible arrangements:\n    - We can choose any of the 6 beads for the first position in the grid.\n    - We can choose any of the remaining 5 beads for the second position.\n    - We can choose any of the remaining 4 beads for the third position.\n    - We can choose any of the remaining 3 beads for the fourth position.\n    - We can choose any of the remaining 2 beads for the fifth position.\n    - We can choose the last remaining bead for the sixth position.\n\n  - Total number of distinct arrangements = 6 x 5 x 4 x 3 x 2 x 1 = 720.\n\n5. Simplify or reduce the problem:\n  - We need to consider the symmetries of the grid to eliminate redundant cases.\n  - The 2x3 grid has rotational symmetry of order 2, meaning it can be rotated 180 degrees to look the same.\n  - The 2x3 grid also has reflectional symmetry, meaning it can be reflected horizontally or vertically to look the same.\n  - Therefore, we need to divide the total number of distinct arrangements by 2 to account for the rotational symmetry.\n  - We also need to divide the total number of distinct arrangements by 2 to account for the reflectional symmetry.\n\n6. Calculate the total number of distinct arrangements:\n  - Total number of distinct arrangements = 720 / 2 / 2 = 180.\n\nAnswer: The number of ways to arrange 6 beads of distinct colors in a 2x3 grid, considering reflections and rotations as the same, is 180\n```\n\nWe observe that both ZS and CoT handle rotations and reflections incorrectly and fail to provide adequate details on how to address them. In contrast, StrategyLLM offers comprehensive details and applies correct logic, resulting in the correct solution. This example clearly demonstrates the superiority of StrategyLLM in complex reasoning tasks.\n\n**Conclusion:**\n\nIn summary, while a simple prompt may occasionally yield correct solutions, our findings indicate that advanced methods like CoT, and more importantly, our StrategyLLM framework, provide more reliable and consistent results across a wide range of examples. We hope this explanation addresses your concerns and clarifies the rationale behind our approach.\n\n---\n\nThank you for your feedback. We appreciate the opportunity for further clarification.\n\n**Clarification:**\n\nWe would like to clarify that the example provided in our discussion was primarily intended to illustrate the motivation and benefits of our proposed StrategyLLM framework. The true measure of a method's effectiveness comes from its performance across diverse test sets of established benchmarks. Our extensive experimental results clearly demonstrate the superiority of StrategyLLM over baseline approaches, including CoT, across a wide range of LLMs and tasks.\n\n**Understanding LLM Behavior:**\n\nIt’s important to recognize that LLMs, such as gpt-3.5-turbo, are autoregressive text generators. This means that the same problem can yield different solutions depending on the prompt used, or even when using the same prompt with temperature sampling. Therefore, it is not surprising that for certain examples, an LLM may generate a correct solution without any prompt engineering or advanced techniques. However, this does not imply a flaw in the CoT implementation. **We want to reassure you that there is no bug or issue in our CoT implementation.** While CoT does not guarantee outperformance over a simple zero-shot baseline on every individual example, its effectiveness is evident when evaluated across a larger test set, where it generally achieves higher accuracy.\n\nOur StrategyLLM framework is designed to increase the likelihood of obtaining correct answers by guiding the LLM through a structured reasoning process, ensuring a more consistent and reliable approach across different problems.\n\n\n**Prediction of Simple Prompt:**\n\nGiven the optimization of frontier chat LLMs like “gpt-3.5-turbo” for step-by-step reasoning, we expect that even a simple prompt can yield a step-by-step solution for the provided example using “gpt-3.5-turbo-16k-0613”. However, predicting the exact reasoning steps in advance is quite challenging due to the inherent variability in LLM outputs. Since \"gpt-3.5-turbo-16k-0613\" has been deprecated, we used an improved version, \"gpt-3.5-turbo-1106,\" to generate a prediction with a temperature setting of 0. The model produced the following solution:\n\n```\nFirst, we need to consider the number of ways to arrange the 4 non-math books in the middle of the stack. This can be calculated using the formula for permutations of n objects, which is n!. In this case, there are 4 non-math books, so the number of ways to arrange them is 4!.\n\nNext, we need to consider the number of ways to arrange the 2 math books on the ends of the stack. Since there are 2 math books, the number of ways to arrange them is 2!.\n\nFinally, we can multiply these two numbers together to find the total number of ways to stack the books with a math book on both ends:\n\n4! * 2! = 24 * 2 = 48\n\nSo, there are 48 different ways to stack the 6 books on the shelf with a math book on both ends.\n```\n\nThis solution indeed derives the correct answer. However, as mentioned earlier, solving a single example correctly is not sufficient to establish the overall effectiveness of a method. To provide a broader comparison, we tested both the zero-shot baseline and the CoT approach on the CP dataset using \"gpt-3.5-turbo-1106\". The results were 33.0 for the zero-shot baseline and 36.5 for CoT, indicating that the CoT approach outperforms the simple baseline on the full test set.\n\n**A More Complicated Example:**\n\nThe discussion regarding this complicated example is shown in the second part of our response (i.e., the next official comment).\n\n---\n\nThank you for your review and for confirming that our paper does not raise ethical issues related to discrimination and bias.\n\n---\n\nThank you for your valuable feedback regarding our submission. We appreciate your acknowledgment that our paper does not directly present ethical issues concerning gender and race bias. As suggested, we will consider including a discussion in the discussion section of our paper to address how StrategyLLM might influence gender and race bias.\n\n---\n\nThank you for your continued feedback and for highlighting the need for further clarification regarding the differentiation between the CoT approach and our proposed StrategyLLM method. We appreciate the opportunity to elaborate on this critical aspect.\n\n**Clarification:**\n\nWe would like to clarify that the underlying LLM for the strategy generator is the same as the LLM being tested. Additionally, CoT and StrategyLLM leverage the same LLM for problem interpretation and resolution. However, the fundamental difference between these methods lies in their approach to guiding the LLM’s reasoning process.\n\n**Understanding LLM Capabilities:**\n\nIt’s important to recognize that the LLM itself has the capacity to understand the details of a problem, such as the requirement to place math books at both ends of a stack. However, how effectively the LLM applies this understanding can vary significantly depending on the prompting method used.\n\n**The Role of Structured Strategy in Mitigating Misinterpretation:**\n\nThe StrategyLLM approach is designed to mitigate the likelihood of misinterpretations through a structured reasoning process. Unlike CoT, which primarily guides the model to think step-by-step, StrategyLLM encourages a more holistic understanding of the problem by enforcing a structured strategy that must be followed consistently across examples. \n\nFor instance, in the provided example, StrategyLLM ensures a systematic approach to solving it:\n\n*Step 1: Problem Understanding:* The strategy mandates a clear understanding of the problem requirements, including the necessity for math books at both ends of the stack.\n\n*Step 2: Identifying Relevant Concepts:* The strategy requires identifying the relevant counting or probability concepts needed to solve the problem, ensuring that the LLM considers the correct mathematical approach.\n\n*Step 3: Breakdown of the Problem:* The strategy explicitly requires identifying the critical elements of the problem, such as the placement of the math books and the arrangement of the remaining books.\n\n*Step 4: Calculation:* The strategy then guides the model to apply relevant mathematical concepts (e.g., permutations) to derive the correct solution.\n\nWe can observe that the strategy itself does not directly identify critical details; rather, it strongly encourages the LLM to recognize these details for each problem. This approach significantly reduces the risk of overlooking crucial aspects.\n\n**Differentiating from CoT:**\n\nCoT allows for a more flexible reasoning process, which can sometimes lead to inconsistencies or omissions in understanding and solving problems. In the case of the example provided, CoT failed to capture the requirement of placing math books at both ends because the step-by-step reasoning did not enforce a structured breakdown of the problem. Instead, it focused on treating the two math books as a single entity, leading to an incorrect interpretation.\n\n**Conclusion:**\n\nIn conclusion, while both CoT and StrategyLLM rely on the LLM’s interpretation, StrategyLLM’s structured approach reduces the likelihood of critical misinterpretations. This structured reasoning process ensures that all essential aspects of a problem are considered, leading to more accurate and reliable solutions. \n\nWe hope this clarifies the distinction and addresses your concern. We appreciate your feedback and are committed to addressing any further questions or concerns you may have.", "author_response": "Thank you for your feedback. We appreciate the opportunity for further clarification.\n\n**Clarification:**\n\nWe would like to clarify that the example provided in our discussion was primarily intended to illustrate the motivation and benefits of our proposed StrategyLLM framework. The true measure of a method's effectiveness comes from its performance across diverse test sets of established benchmarks. Our extensive experimental results clearly demonstrate the superiority of StrategyLLM over baseline approaches, including CoT, across a wide range of LLMs and tasks.\n\n**Understanding LLM Behavior:**\n\nIt’s important to recognize that LLMs, such as gpt-3.5-turbo, are autoregressive text generators. This means that the same problem can yield different solutions depending on the prompt used, or even when using the same prompt with temperature sampling. Therefore, it is not surprising that for certain examples, an LLM may generate a correct solution without any prompt engineering or advanced techniques. However, this does not imply a flaw in the CoT implementation. **We want to reassure you that there is no bug or issue in our CoT implementation.** While CoT does not guarantee outperformance over a simple zero-shot baseline on every individual example, its effectiveness is evident when evaluated across a larger test set, where it generally achieves higher accuracy.\n\nOur StrategyLLM framework is designed to increase the likelihood of obtaining correct answers by guiding the LLM through a structured reasoning process, ensuring a more consistent and reliable approach across different problems.\n\n\n**Prediction of Simple Prompt:**\n\nGiven the optimization of frontier chat LLMs like “gpt-3.5-turbo” for step-by-step reasoning, we expect that even a simple prompt can yield a step-by-step solution for the provided example using “gpt-3.5-turbo-16k-0613”. However, predicting the exact reasoning steps in advance is quite challenging due to the inherent variability in LLM outputs. Since \"gpt-3.5-turbo-16k-0613\" has been deprecated, we used an improved version, \"gpt-3.5-turbo-1106,\" to generate a prediction with a temperature setting of 0. The model produced the following solution:\n\n```\nFirst, we need to consider the number of ways to arrange the 4 non-math books in the middle of the stack. This can be calculated using the formula for permutations of n objects, which is n!. In this case, there are 4 non-math books, so the number of ways to arrange them is 4!.\n\nNext, we need to consider the number of ways to arrange the 2 math books on the ends of the stack. Since there are 2 math books, the number of ways to arrange them is 2!.\n\nFinally, we can multiply these two numbers together to find the total number of ways to stack the books with a math book on both ends:\n\n4! * 2! = 24 * 2 = 48\n\nSo, there are 48 different ways to stack the 6 books on the shelf with a math book on both ends.\n```\n\nThis solution indeed derives the correct answer. However, as mentioned earlier, solving a single example correctly is not sufficient to establish the overall effectiveness of a method. To provide a broader comparison, we tested both the zero-shot baseline and the CoT approach on the CP dataset using \"gpt-3.5-turbo-1106\". The results were 33.0 for the zero-shot baseline and 36.5 for CoT, indicating that the CoT approach outperforms the simple baseline on the full test set.\n\n**A More Complicated Example:**\n\nThe discussion regarding this complicated example is shown in the second part of our response (i.e., the next official comment)."}
{"claim": "The comparison between CoT and StrategyML may be invalid because potential CoT implementation issues could explain observed performance differences.", "claim_type": "baseline", "paper_id": "UkxJd64mki", "paper_title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "fGEvjzIyly", "reviewer": "Reviewer_D2vF", "review_text": "Comment: Thanks again for your thorough explanations. Can authors let me know what they expect to get from \"gpt-3.5-turbo-16k-0613\" without using any prompt engineering, few shot learning, and CoT if they pose the following question directly to the prompt?\n\"Suppose that I have 6 different books, 2 of which are math books. In how many ways can I stack my 6 books on a shelf if I want a math book on both ends of the stack?\" My observation is that for this particular example, even a simple prompt without using more advanced approaches such as CoT or StrategyML should be able to solve the problem. Therefore, I suspect something in the CoT implementation is not working as intended and attributing a bug/issue in CoT implementation to its inferior performance compared with StrategyML is not advisable.", "labeling_timestamp": "2026-01-11T16:28:03.524920", "model": "gpt-5-mini", "label": "Contradicted", "justification": "Authors explicitly deny implementation issues with CoT, present aggregate results showing CoT outperforms zero-shot on the full test set, and argue differences arise from methodological merits rather than bugs.", "evidence": "\"We want to reassure you that there is no bug or issue in our CoT implementation.\"; \"To provide a broader comparison, we tested both the zero-shot baseline and the CoT approach on the CP dataset using \\\"gpt-3.5-turbo-1106\\\". The results were 33.0 for the zero-shot baseline and 36.5 for CoT, indicating that the CoT approach outperforms the simple baseline on the full test set.\"", "combined_author_response": "Thank you for your thoughtful consideration and for adjusting your evaluation based on our latest discussions. We truly appreciate your willingness to engage deeply with our work. We fully understand the importance of clearly articulating how StrategyLLM’s structured reasoning framework helps to mitigate common pitfalls that may occur in CoT or ZS approaches. Based on your valuable feedback, we will revise our paper to better highlight these complexities and provide detailed explanations of why StrategyLLM excels in handling them.\n\n---\n\nThank you for your thoughtful feedback and for taking the time to review our rebuttal. We are pleased to hear that the new results have helped clarify the necessity and importance of the various facets of our StrategyLLM approach. We appreciate your suggestions and will incorporate these clarifications into our revised manuscript to enhance the overall clarity and comprehensiveness of our work.\n\n---\n\n**A More Complicated Example:**\n\nWe present a more complicated example, with a difficulty level of 5, to highlight the benefits of StrategyLLM. “ZS” refers to the zero-shot baseline without any prompt engineering.\n\n**Question:** How many ways are there to arrange $6$ beads of distinct colors in a $2 \\times 3$ grid if reflections and rotations are considered the same? (In other words, two arrangements are considered the same if I can rotate and/or reflect one arrangement to get the other.)\n\n**Gold Answer:** 180\n\n**ZS Prediction:**\n\n```\nThere are $6!$ ways to arrange the $6$ beads in a line. However, when considering a $2 \\times 3$ grid, there are $6$ different ways to arrange the beads in a line that result in the same grid when considering reflections and rotations. Therefore, the total number of distinct arrangements is $\\frac{6!}{6} = 120$.\n```\n\n**CoT Prediction:**\n\n```\nWe can consider the $2 \\times 3$ grid as a $6$-element cycle. There are $6!$ ways to arrange the $6$ beads in a line, but we must divide by $6$ to account for rotations. However, this overcounts the number of arrangements by a factor of $2$ because we are also counting reflections. Therefore, the final answer is $\\frac{6!}{6 \\cdot 2} = \\boxed{60}$.\nAnswer: The answer is 60\n```\n**StrategyLLM Prediction:**\n\n```\n1. Understand the problem statement:\n  - We have 6 beads of distinct colors.\n  - We need to arrange these beads in a 2x3 grid.\n  - Reflections and rotations are considered the same, meaning two arrangements are considered the same if one can be obtained from the other by rotating or reflecting.\n\n2. Identify the counting or probability concept:\n  - We need to use the concept of combinations to solve this problem.\n\n3. Break down the problem:\n  - We have 6 beads to arrange in a 2x3 grid.\n  - We need to consider the symmetries of the grid to eliminate redundant cases.\n  - We need to determine the number of distinct arrangements considering reflections and rotations.\n\n4. Calculate the required counting or probability:\n  - To calculate the number of distinct arrangements, we need to count the number of combinations that satisfy the given conditions.\n  - We can use combinations to calculate this.\n\n  - Let's consider the possible arrangements:\n    - We can choose any of the 6 beads for the first position in the grid.\n    - We can choose any of the remaining 5 beads for the second position.\n    - We can choose any of the remaining 4 beads for the third position.\n    - We can choose any of the remaining 3 beads for the fourth position.\n    - We can choose any of the remaining 2 beads for the fifth position.\n    - We can choose the last remaining bead for the sixth position.\n\n  - Total number of distinct arrangements = 6 x 5 x 4 x 3 x 2 x 1 = 720.\n\n5. Simplify or reduce the problem:\n  - We need to consider the symmetries of the grid to eliminate redundant cases.\n  - The 2x3 grid has rotational symmetry of order 2, meaning it can be rotated 180 degrees to look the same.\n  - The 2x3 grid also has reflectional symmetry, meaning it can be reflected horizontally or vertically to look the same.\n  - Therefore, we need to divide the total number of distinct arrangements by 2 to account for the rotational symmetry.\n  - We also need to divide the total number of distinct arrangements by 2 to account for the reflectional symmetry.\n\n6. Calculate the total number of distinct arrangements:\n  - Total number of distinct arrangements = 720 / 2 / 2 = 180.\n\nAnswer: The number of ways to arrange 6 beads of distinct colors in a 2x3 grid, considering reflections and rotations as the same, is 180\n```\n\nWe observe that both ZS and CoT handle rotations and reflections incorrectly and fail to provide adequate details on how to address them. In contrast, StrategyLLM offers comprehensive details and applies correct logic, resulting in the correct solution. This example clearly demonstrates the superiority of StrategyLLM in complex reasoning tasks.\n\n**Conclusion:**\n\nIn summary, while a simple prompt may occasionally yield correct solutions, our findings indicate that advanced methods like CoT, and more importantly, our StrategyLLM framework, provide more reliable and consistent results across a wide range of examples. We hope this explanation addresses your concerns and clarifies the rationale behind our approach.\n\n---\n\nThank you for your feedback. We appreciate the opportunity for further clarification.\n\n**Clarification:**\n\nWe would like to clarify that the example provided in our discussion was primarily intended to illustrate the motivation and benefits of our proposed StrategyLLM framework. The true measure of a method's effectiveness comes from its performance across diverse test sets of established benchmarks. Our extensive experimental results clearly demonstrate the superiority of StrategyLLM over baseline approaches, including CoT, across a wide range of LLMs and tasks.\n\n**Understanding LLM Behavior:**\n\nIt’s important to recognize that LLMs, such as gpt-3.5-turbo, are autoregressive text generators. This means that the same problem can yield different solutions depending on the prompt used, or even when using the same prompt with temperature sampling. Therefore, it is not surprising that for certain examples, an LLM may generate a correct solution without any prompt engineering or advanced techniques. However, this does not imply a flaw in the CoT implementation. **We want to reassure you that there is no bug or issue in our CoT implementation.** While CoT does not guarantee outperformance over a simple zero-shot baseline on every individual example, its effectiveness is evident when evaluated across a larger test set, where it generally achieves higher accuracy.\n\nOur StrategyLLM framework is designed to increase the likelihood of obtaining correct answers by guiding the LLM through a structured reasoning process, ensuring a more consistent and reliable approach across different problems.\n\n\n**Prediction of Simple Prompt:**\n\nGiven the optimization of frontier chat LLMs like “gpt-3.5-turbo” for step-by-step reasoning, we expect that even a simple prompt can yield a step-by-step solution for the provided example using “gpt-3.5-turbo-16k-0613”. However, predicting the exact reasoning steps in advance is quite challenging due to the inherent variability in LLM outputs. Since \"gpt-3.5-turbo-16k-0613\" has been deprecated, we used an improved version, \"gpt-3.5-turbo-1106,\" to generate a prediction with a temperature setting of 0. The model produced the following solution:\n\n```\nFirst, we need to consider the number of ways to arrange the 4 non-math books in the middle of the stack. This can be calculated using the formula for permutations of n objects, which is n!. In this case, there are 4 non-math books, so the number of ways to arrange them is 4!.\n\nNext, we need to consider the number of ways to arrange the 2 math books on the ends of the stack. Since there are 2 math books, the number of ways to arrange them is 2!.\n\nFinally, we can multiply these two numbers together to find the total number of ways to stack the books with a math book on both ends:\n\n4! * 2! = 24 * 2 = 48\n\nSo, there are 48 different ways to stack the 6 books on the shelf with a math book on both ends.\n```\n\nThis solution indeed derives the correct answer. However, as mentioned earlier, solving a single example correctly is not sufficient to establish the overall effectiveness of a method. To provide a broader comparison, we tested both the zero-shot baseline and the CoT approach on the CP dataset using \"gpt-3.5-turbo-1106\". The results were 33.0 for the zero-shot baseline and 36.5 for CoT, indicating that the CoT approach outperforms the simple baseline on the full test set.\n\n**A More Complicated Example:**\n\nThe discussion regarding this complicated example is shown in the second part of our response (i.e., the next official comment).\n\n---\n\nThank you for your review and for confirming that our paper does not raise ethical issues related to discrimination and bias.\n\n---\n\nThank you for your valuable feedback regarding our submission. We appreciate your acknowledgment that our paper does not directly present ethical issues concerning gender and race bias. As suggested, we will consider including a discussion in the discussion section of our paper to address how StrategyLLM might influence gender and race bias.\n\n---\n\nThank you for your continued feedback and for highlighting the need for further clarification regarding the differentiation between the CoT approach and our proposed StrategyLLM method. We appreciate the opportunity to elaborate on this critical aspect.\n\n**Clarification:**\n\nWe would like to clarify that the underlying LLM for the strategy generator is the same as the LLM being tested. Additionally, CoT and StrategyLLM leverage the same LLM for problem interpretation and resolution. However, the fundamental difference between these methods lies in their approach to guiding the LLM’s reasoning process.\n\n**Understanding LLM Capabilities:**\n\nIt’s important to recognize that the LLM itself has the capacity to understand the details of a problem, such as the requirement to place math books at both ends of a stack. However, how effectively the LLM applies this understanding can vary significantly depending on the prompting method used.\n\n**The Role of Structured Strategy in Mitigating Misinterpretation:**\n\nThe StrategyLLM approach is designed to mitigate the likelihood of misinterpretations through a structured reasoning process. Unlike CoT, which primarily guides the model to think step-by-step, StrategyLLM encourages a more holistic understanding of the problem by enforcing a structured strategy that must be followed consistently across examples. \n\nFor instance, in the provided example, StrategyLLM ensures a systematic approach to solving it:\n\n*Step 1: Problem Understanding:* The strategy mandates a clear understanding of the problem requirements, including the necessity for math books at both ends of the stack.\n\n*Step 2: Identifying Relevant Concepts:* The strategy requires identifying the relevant counting or probability concepts needed to solve the problem, ensuring that the LLM considers the correct mathematical approach.\n\n*Step 3: Breakdown of the Problem:* The strategy explicitly requires identifying the critical elements of the problem, such as the placement of the math books and the arrangement of the remaining books.\n\n*Step 4: Calculation:* The strategy then guides the model to apply relevant mathematical concepts (e.g., permutations) to derive the correct solution.\n\nWe can observe that the strategy itself does not directly identify critical details; rather, it strongly encourages the LLM to recognize these details for each problem. This approach significantly reduces the risk of overlooking crucial aspects.\n\n**Differentiating from CoT:**\n\nCoT allows for a more flexible reasoning process, which can sometimes lead to inconsistencies or omissions in understanding and solving problems. In the case of the example provided, CoT failed to capture the requirement of placing math books at both ends because the step-by-step reasoning did not enforce a structured breakdown of the problem. Instead, it focused on treating the two math books as a single entity, leading to an incorrect interpretation.\n\n**Conclusion:**\n\nIn conclusion, while both CoT and StrategyLLM rely on the LLM’s interpretation, StrategyLLM’s structured approach reduces the likelihood of critical misinterpretations. This structured reasoning process ensures that all essential aspects of a problem are considered, leading to more accurate and reliable solutions. \n\nWe hope this clarifies the distinction and addresses your concern. We appreciate your feedback and are committed to addressing any further questions or concerns you may have.", "author_response": "Thank you for your feedback. We appreciate the opportunity for further clarification.\n\n**Clarification:**\n\nWe would like to clarify that the example provided in our discussion was primarily intended to illustrate the motivation and benefits of our proposed StrategyLLM framework. The true measure of a method's effectiveness comes from its performance across diverse test sets of established benchmarks. Our extensive experimental results clearly demonstrate the superiority of StrategyLLM over baseline approaches, including CoT, across a wide range of LLMs and tasks.\n\n**Understanding LLM Behavior:**\n\nIt’s important to recognize that LLMs, such as gpt-3.5-turbo, are autoregressive text generators. This means that the same problem can yield different solutions depending on the prompt used, or even when using the same prompt with temperature sampling. Therefore, it is not surprising that for certain examples, an LLM may generate a correct solution without any prompt engineering or advanced techniques. However, this does not imply a flaw in the CoT implementation. **We want to reassure you that there is no bug or issue in our CoT implementation.** While CoT does not guarantee outperformance over a simple zero-shot baseline on every individual example, its effectiveness is evident when evaluated across a larger test set, where it generally achieves higher accuracy.\n\nOur StrategyLLM framework is designed to increase the likelihood of obtaining correct answers by guiding the LLM through a structured reasoning process, ensuring a more consistent and reliable approach across different problems.\n\n\n**Prediction of Simple Prompt:**\n\nGiven the optimization of frontier chat LLMs like “gpt-3.5-turbo” for step-by-step reasoning, we expect that even a simple prompt can yield a step-by-step solution for the provided example using “gpt-3.5-turbo-16k-0613”. However, predicting the exact reasoning steps in advance is quite challenging due to the inherent variability in LLM outputs. Since \"gpt-3.5-turbo-16k-0613\" has been deprecated, we used an improved version, \"gpt-3.5-turbo-1106,\" to generate a prediction with a temperature setting of 0. The model produced the following solution:\n\n```\nFirst, we need to consider the number of ways to arrange the 4 non-math books in the middle of the stack. This can be calculated using the formula for permutations of n objects, which is n!. In this case, there are 4 non-math books, so the number of ways to arrange them is 4!.\n\nNext, we need to consider the number of ways to arrange the 2 math books on the ends of the stack. Since there are 2 math books, the number of ways to arrange them is 2!.\n\nFinally, we can multiply these two numbers together to find the total number of ways to stack the books with a math book on both ends:\n\n4! * 2! = 24 * 2 = 48\n\nSo, there are 48 different ways to stack the 6 books on the shelf with a math book on both ends.\n```\n\nThis solution indeed derives the correct answer. However, as mentioned earlier, solving a single example correctly is not sufficient to establish the overall effectiveness of a method. To provide a broader comparison, we tested both the zero-shot baseline and the CoT approach on the CP dataset using \"gpt-3.5-turbo-1106\". The results were 33.0 for the zero-shot baseline and 36.5 for CoT, indicating that the CoT approach outperforms the simple baseline on the full test set.\n\n**A More Complicated Example:**\n\nThe discussion regarding this complicated example is shown in the second part of our response (i.e., the next official comment)."}
{"claim": "The paper lacks clarity regarding the expected outputs from the baseline gpt-3.5-turbo-16k-0613 model when given the example without any prompt engineering.", "claim_type": "baseline", "paper_id": "UkxJd64mki", "paper_title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "fGEvjzIyly", "reviewer": "Reviewer_D2vF", "review_text": "Comment: Thanks again for your thorough explanations. Can authors let me know what they expect to get from \"gpt-3.5-turbo-16k-0613\" without using any prompt engineering, few shot learning, and CoT if they pose the following question directly to the prompt?\n\"Suppose that I have 6 different books, 2 of which are math books. In how many ways can I stack my 6 books on a shelf if I want a math book on both ends of the stack?\" My observation is that for this particular example, even a simple prompt without using more advanced approaches such as CoT or StrategyML should be able to solve the problem. Therefore, I suspect something in the CoT implementation is not working as intended and attributing a bug/issue in CoT implementation to its inferior performance compared with StrategyML is not advisable.", "labeling_timestamp": "2026-01-11T16:28:24.716398", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors acknowledge the clarity concern, promise to revise the paper, and provide zero-shot (ZS) example outputs and discussion; however they do not definitively present the exact expected output from gpt-3.5-turbo-16k-0613 (they note variability and that the model is deprecated and used a different model for some tests), so the reviewer’s specific request is only partially addressed.", "evidence": "\"ZS refers to the zero-shot baseline without any prompt engineering.\" ... \"Given the optimization of frontier chat LLMs like “gpt-3.5-turbo” for step-by-step reasoning, we expect that even a simple prompt can yield a step-by-step solution for the provided example using “gpt-3.5-turbo-16k-0613”. However, predicting the exact reasoning steps in advance is quite challenging due to the inherent variability in LLM outputs.\"", "combined_author_response": "Thank you for your thoughtful consideration and for adjusting your evaluation based on our latest discussions. We truly appreciate your willingness to engage deeply with our work. We fully understand the importance of clearly articulating how StrategyLLM’s structured reasoning framework helps to mitigate common pitfalls that may occur in CoT or ZS approaches. Based on your valuable feedback, we will revise our paper to better highlight these complexities and provide detailed explanations of why StrategyLLM excels in handling them.\n\n---\n\nThank you for your thoughtful feedback and for taking the time to review our rebuttal. We are pleased to hear that the new results have helped clarify the necessity and importance of the various facets of our StrategyLLM approach. We appreciate your suggestions and will incorporate these clarifications into our revised manuscript to enhance the overall clarity and comprehensiveness of our work.\n\n---\n\n**A More Complicated Example:**\n\nWe present a more complicated example, with a difficulty level of 5, to highlight the benefits of StrategyLLM. “ZS” refers to the zero-shot baseline without any prompt engineering.\n\n**Question:** How many ways are there to arrange $6$ beads of distinct colors in a $2 \\times 3$ grid if reflections and rotations are considered the same? (In other words, two arrangements are considered the same if I can rotate and/or reflect one arrangement to get the other.)\n\n**Gold Answer:** 180\n\n**ZS Prediction:**\n\n```\nThere are $6!$ ways to arrange the $6$ beads in a line. However, when considering a $2 \\times 3$ grid, there are $6$ different ways to arrange the beads in a line that result in the same grid when considering reflections and rotations. Therefore, the total number of distinct arrangements is $\\frac{6!}{6} = 120$.\n```\n\n**CoT Prediction:**\n\n```\nWe can consider the $2 \\times 3$ grid as a $6$-element cycle. There are $6!$ ways to arrange the $6$ beads in a line, but we must divide by $6$ to account for rotations. However, this overcounts the number of arrangements by a factor of $2$ because we are also counting reflections. Therefore, the final answer is $\\frac{6!}{6 \\cdot 2} = \\boxed{60}$.\nAnswer: The answer is 60\n```\n**StrategyLLM Prediction:**\n\n```\n1. Understand the problem statement:\n  - We have 6 beads of distinct colors.\n  - We need to arrange these beads in a 2x3 grid.\n  - Reflections and rotations are considered the same, meaning two arrangements are considered the same if one can be obtained from the other by rotating or reflecting.\n\n2. Identify the counting or probability concept:\n  - We need to use the concept of combinations to solve this problem.\n\n3. Break down the problem:\n  - We have 6 beads to arrange in a 2x3 grid.\n  - We need to consider the symmetries of the grid to eliminate redundant cases.\n  - We need to determine the number of distinct arrangements considering reflections and rotations.\n\n4. Calculate the required counting or probability:\n  - To calculate the number of distinct arrangements, we need to count the number of combinations that satisfy the given conditions.\n  - We can use combinations to calculate this.\n\n  - Let's consider the possible arrangements:\n    - We can choose any of the 6 beads for the first position in the grid.\n    - We can choose any of the remaining 5 beads for the second position.\n    - We can choose any of the remaining 4 beads for the third position.\n    - We can choose any of the remaining 3 beads for the fourth position.\n    - We can choose any of the remaining 2 beads for the fifth position.\n    - We can choose the last remaining bead for the sixth position.\n\n  - Total number of distinct arrangements = 6 x 5 x 4 x 3 x 2 x 1 = 720.\n\n5. Simplify or reduce the problem:\n  - We need to consider the symmetries of the grid to eliminate redundant cases.\n  - The 2x3 grid has rotational symmetry of order 2, meaning it can be rotated 180 degrees to look the same.\n  - The 2x3 grid also has reflectional symmetry, meaning it can be reflected horizontally or vertically to look the same.\n  - Therefore, we need to divide the total number of distinct arrangements by 2 to account for the rotational symmetry.\n  - We also need to divide the total number of distinct arrangements by 2 to account for the reflectional symmetry.\n\n6. Calculate the total number of distinct arrangements:\n  - Total number of distinct arrangements = 720 / 2 / 2 = 180.\n\nAnswer: The number of ways to arrange 6 beads of distinct colors in a 2x3 grid, considering reflections and rotations as the same, is 180\n```\n\nWe observe that both ZS and CoT handle rotations and reflections incorrectly and fail to provide adequate details on how to address them. In contrast, StrategyLLM offers comprehensive details and applies correct logic, resulting in the correct solution. This example clearly demonstrates the superiority of StrategyLLM in complex reasoning tasks.\n\n**Conclusion:**\n\nIn summary, while a simple prompt may occasionally yield correct solutions, our findings indicate that advanced methods like CoT, and more importantly, our StrategyLLM framework, provide more reliable and consistent results across a wide range of examples. We hope this explanation addresses your concerns and clarifies the rationale behind our approach.\n\n---\n\nThank you for your feedback. We appreciate the opportunity for further clarification.\n\n**Clarification:**\n\nWe would like to clarify that the example provided in our discussion was primarily intended to illustrate the motivation and benefits of our proposed StrategyLLM framework. The true measure of a method's effectiveness comes from its performance across diverse test sets of established benchmarks. Our extensive experimental results clearly demonstrate the superiority of StrategyLLM over baseline approaches, including CoT, across a wide range of LLMs and tasks.\n\n**Understanding LLM Behavior:**\n\nIt’s important to recognize that LLMs, such as gpt-3.5-turbo, are autoregressive text generators. This means that the same problem can yield different solutions depending on the prompt used, or even when using the same prompt with temperature sampling. Therefore, it is not surprising that for certain examples, an LLM may generate a correct solution without any prompt engineering or advanced techniques. However, this does not imply a flaw in the CoT implementation. **We want to reassure you that there is no bug or issue in our CoT implementation.** While CoT does not guarantee outperformance over a simple zero-shot baseline on every individual example, its effectiveness is evident when evaluated across a larger test set, where it generally achieves higher accuracy.\n\nOur StrategyLLM framework is designed to increase the likelihood of obtaining correct answers by guiding the LLM through a structured reasoning process, ensuring a more consistent and reliable approach across different problems.\n\n\n**Prediction of Simple Prompt:**\n\nGiven the optimization of frontier chat LLMs like “gpt-3.5-turbo” for step-by-step reasoning, we expect that even a simple prompt can yield a step-by-step solution for the provided example using “gpt-3.5-turbo-16k-0613”. However, predicting the exact reasoning steps in advance is quite challenging due to the inherent variability in LLM outputs. Since \"gpt-3.5-turbo-16k-0613\" has been deprecated, we used an improved version, \"gpt-3.5-turbo-1106,\" to generate a prediction with a temperature setting of 0. The model produced the following solution:\n\n```\nFirst, we need to consider the number of ways to arrange the 4 non-math books in the middle of the stack. This can be calculated using the formula for permutations of n objects, which is n!. In this case, there are 4 non-math books, so the number of ways to arrange them is 4!.\n\nNext, we need to consider the number of ways to arrange the 2 math books on the ends of the stack. Since there are 2 math books, the number of ways to arrange them is 2!.\n\nFinally, we can multiply these two numbers together to find the total number of ways to stack the books with a math book on both ends:\n\n4! * 2! = 24 * 2 = 48\n\nSo, there are 48 different ways to stack the 6 books on the shelf with a math book on both ends.\n```\n\nThis solution indeed derives the correct answer. However, as mentioned earlier, solving a single example correctly is not sufficient to establish the overall effectiveness of a method. To provide a broader comparison, we tested both the zero-shot baseline and the CoT approach on the CP dataset using \"gpt-3.5-turbo-1106\". The results were 33.0 for the zero-shot baseline and 36.5 for CoT, indicating that the CoT approach outperforms the simple baseline on the full test set.\n\n**A More Complicated Example:**\n\nThe discussion regarding this complicated example is shown in the second part of our response (i.e., the next official comment).\n\n---\n\nThank you for your review and for confirming that our paper does not raise ethical issues related to discrimination and bias.\n\n---\n\nThank you for your valuable feedback regarding our submission. We appreciate your acknowledgment that our paper does not directly present ethical issues concerning gender and race bias. As suggested, we will consider including a discussion in the discussion section of our paper to address how StrategyLLM might influence gender and race bias.\n\n---\n\nThank you for your continued feedback and for highlighting the need for further clarification regarding the differentiation between the CoT approach and our proposed StrategyLLM method. We appreciate the opportunity to elaborate on this critical aspect.\n\n**Clarification:**\n\nWe would like to clarify that the underlying LLM for the strategy generator is the same as the LLM being tested. Additionally, CoT and StrategyLLM leverage the same LLM for problem interpretation and resolution. However, the fundamental difference between these methods lies in their approach to guiding the LLM’s reasoning process.\n\n**Understanding LLM Capabilities:**\n\nIt’s important to recognize that the LLM itself has the capacity to understand the details of a problem, such as the requirement to place math books at both ends of a stack. However, how effectively the LLM applies this understanding can vary significantly depending on the prompting method used.\n\n**The Role of Structured Strategy in Mitigating Misinterpretation:**\n\nThe StrategyLLM approach is designed to mitigate the likelihood of misinterpretations through a structured reasoning process. Unlike CoT, which primarily guides the model to think step-by-step, StrategyLLM encourages a more holistic understanding of the problem by enforcing a structured strategy that must be followed consistently across examples. \n\nFor instance, in the provided example, StrategyLLM ensures a systematic approach to solving it:\n\n*Step 1: Problem Understanding:* The strategy mandates a clear understanding of the problem requirements, including the necessity for math books at both ends of the stack.\n\n*Step 2: Identifying Relevant Concepts:* The strategy requires identifying the relevant counting or probability concepts needed to solve the problem, ensuring that the LLM considers the correct mathematical approach.\n\n*Step 3: Breakdown of the Problem:* The strategy explicitly requires identifying the critical elements of the problem, such as the placement of the math books and the arrangement of the remaining books.\n\n*Step 4: Calculation:* The strategy then guides the model to apply relevant mathematical concepts (e.g., permutations) to derive the correct solution.\n\nWe can observe that the strategy itself does not directly identify critical details; rather, it strongly encourages the LLM to recognize these details for each problem. This approach significantly reduces the risk of overlooking crucial aspects.\n\n**Differentiating from CoT:**\n\nCoT allows for a more flexible reasoning process, which can sometimes lead to inconsistencies or omissions in understanding and solving problems. In the case of the example provided, CoT failed to capture the requirement of placing math books at both ends because the step-by-step reasoning did not enforce a structured breakdown of the problem. Instead, it focused on treating the two math books as a single entity, leading to an incorrect interpretation.\n\n**Conclusion:**\n\nIn conclusion, while both CoT and StrategyLLM rely on the LLM’s interpretation, StrategyLLM’s structured approach reduces the likelihood of critical misinterpretations. This structured reasoning process ensures that all essential aspects of a problem are considered, leading to more accurate and reliable solutions. \n\nWe hope this clarifies the distinction and addresses your concern. We appreciate your feedback and are committed to addressing any further questions or concerns you may have.", "author_response": "Thank you for your feedback. We appreciate the opportunity for further clarification.\n\n**Clarification:**\n\nWe would like to clarify that the example provided in our discussion was primarily intended to illustrate the motivation and benefits of our proposed StrategyLLM framework. The true measure of a method's effectiveness comes from its performance across diverse test sets of established benchmarks. Our extensive experimental results clearly demonstrate the superiority of StrategyLLM over baseline approaches, including CoT, across a wide range of LLMs and tasks.\n\n**Understanding LLM Behavior:**\n\nIt’s important to recognize that LLMs, such as gpt-3.5-turbo, are autoregressive text generators. This means that the same problem can yield different solutions depending on the prompt used, or even when using the same prompt with temperature sampling. Therefore, it is not surprising that for certain examples, an LLM may generate a correct solution without any prompt engineering or advanced techniques. However, this does not imply a flaw in the CoT implementation. **We want to reassure you that there is no bug or issue in our CoT implementation.** While CoT does not guarantee outperformance over a simple zero-shot baseline on every individual example, its effectiveness is evident when evaluated across a larger test set, where it generally achieves higher accuracy.\n\nOur StrategyLLM framework is designed to increase the likelihood of obtaining correct answers by guiding the LLM through a structured reasoning process, ensuring a more consistent and reliable approach across different problems.\n\n\n**Prediction of Simple Prompt:**\n\nGiven the optimization of frontier chat LLMs like “gpt-3.5-turbo” for step-by-step reasoning, we expect that even a simple prompt can yield a step-by-step solution for the provided example using “gpt-3.5-turbo-16k-0613”. However, predicting the exact reasoning steps in advance is quite challenging due to the inherent variability in LLM outputs. Since \"gpt-3.5-turbo-16k-0613\" has been deprecated, we used an improved version, \"gpt-3.5-turbo-1106,\" to generate a prediction with a temperature setting of 0. The model produced the following solution:\n\n```\nFirst, we need to consider the number of ways to arrange the 4 non-math books in the middle of the stack. This can be calculated using the formula for permutations of n objects, which is n!. In this case, there are 4 non-math books, so the number of ways to arrange them is 4!.\n\nNext, we need to consider the number of ways to arrange the 2 math books on the ends of the stack. Since there are 2 math books, the number of ways to arrange them is 2!.\n\nFinally, we can multiply these two numbers together to find the total number of ways to stack the books with a math book on both ends:\n\n4! * 2! = 24 * 2 = 48\n\nSo, there are 48 different ways to stack the 6 books on the shelf with a math book on both ends.\n```\n\nThis solution indeed derives the correct answer. However, as mentioned earlier, solving a single example correctly is not sufficient to establish the overall effectiveness of a method. To provide a broader comparison, we tested both the zero-shot baseline and the CoT approach on the CP dataset using \"gpt-3.5-turbo-1106\". The results were 33.0 for the zero-shot baseline and 36.5 for CoT, indicating that the CoT approach outperforms the simple baseline on the full test set.\n\n**A More Complicated Example:**\n\nThe discussion regarding this complicated example is shown in the second part of our response (i.e., the next official comment)."}
{"claim": "The paper reports results only at 20K training steps and lacks experiments exploring stability or performance when training increases to 60K or more.", "claim_type": "experimental", "paper_id": "tQukGCDaNT", "paper_title": "Improved Distribution Matching Distillation for Fast Image Synthesis", "paper_venue": "neurips2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "K5GmPvDd81", "reviewer": "Reviewer_aF5v", "review_text": "Comment: Happy to raise my score.\n\nOne thing i want to understand from authors, is how stable is this formulation when number of steps go from 20K to say 60K or more? Do you see performance peak around 20K or further training improves quality? unlike original DMD as authors report only 20K was curious if there were any interesting empirical findings which would provide further insights to community.\n\nThis goes back to fake score function alignment, does it help to reinitialize fake score function with teacher model again as student converges well to teacher? As it seems a bit unclear on properties/fit of fake score function and how it effects overall training stability.\n\nLooking forward for more results in later version of paper.", "labeling_timestamp": "2026-01-11T16:28:20.130119", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "Authors acknowledge the limitation, cite compute constraints, provide a preliminary experiment extending to 30K with improved results, and state intent to explore longer training, but do not provide extensive experiments up to 60K or more.", "evidence": "we currently utilize 20K iterations because this represents almost the maximum compute we can afford (64 GPUs over 3 days). ... in a trial where we extended to 30K iterations, we managed to improve the FID from 19.3 to 18.7. We are eager to explore extending the training duration in our revised version to further confirm the model's stability.", "combined_author_response": "We are pleased that our responses have addressed your concerns and appreciate your consideration to raise the score!\n\nRegarding your further inquiries, we currently utilize 20K iterations because this represents almost the maximum compute we can afford (64 GPUs over 3 days). However, we have not yet observed the peak performance of our models. For example, in a trial where we extended to 30K iterations, we managed to improve the FID from 19.3 to 18.7. We are eager to explore extending the training duration in our revised version to further confirm the model's stability.\n\nThe suggestion to reinitialize the fake diffusion model at a later stage is intriguing, and we look forward to experimenting with this approach. Thank you once again for your invaluable suggestions and insights!", "author_response": "We are pleased that our responses have addressed your concerns and appreciate your consideration to raise the score!\n\nRegarding your further inquiries, we currently utilize 20K iterations because this represents almost the maximum compute we can afford (64 GPUs over 3 days). However, we have not yet observed the peak performance of our models. For example, in a trial where we extended to 30K iterations, we managed to improve the FID from 19.3 to 18.7. We are eager to explore extending the training duration in our revised version to further confirm the model's stability.\n\nThe suggestion to reinitialize the fake diffusion model at a later stage is intriguing, and we look forward to experimenting with this approach. Thank you once again for your invaluable suggestions and insights!"}
{"claim": "It is unclear whether model performance peaks around 20K training steps or continues to improve with further training beyond that point.", "claim_type": "experimental", "paper_id": "tQukGCDaNT", "paper_title": "Improved Distribution Matching Distillation for Fast Image Synthesis", "paper_venue": "neurips2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "K5GmPvDd81", "reviewer": "Reviewer_aF5v", "review_text": "Comment: Happy to raise my score.\n\nOne thing i want to understand from authors, is how stable is this formulation when number of steps go from 20K to say 60K or more? Do you see performance peak around 20K or further training improves quality? unlike original DMD as authors report only 20K was curious if there were any interesting empirical findings which would provide further insights to community.\n\nThis goes back to fake score function alignment, does it help to reinitialize fake score function with teacher model again as student converges well to teacher? As it seems a bit unclear on properties/fit of fake score function and how it effects overall training stability.\n\nLooking forward for more results in later version of paper.", "labeling_timestamp": "2026-01-11T16:28:22.016288", "model": "gpt-5-mini", "label": "Supported", "justification": "Authors acknowledge they have not observed a performance peak at 20K and report that extending training to 30K improved FID, indicating the question remains open and performance can continue to improve beyond 20K.", "evidence": "\"we have not yet observed the peak performance of our models. For example, in a trial where we extended to 30K iterations, we managed to improve the FID from 19.3 to 18.7.\"", "combined_author_response": "We are pleased that our responses have addressed your concerns and appreciate your consideration to raise the score!\n\nRegarding your further inquiries, we currently utilize 20K iterations because this represents almost the maximum compute we can afford (64 GPUs over 3 days). However, we have not yet observed the peak performance of our models. For example, in a trial where we extended to 30K iterations, we managed to improve the FID from 19.3 to 18.7. We are eager to explore extending the training duration in our revised version to further confirm the model's stability.\n\nThe suggestion to reinitialize the fake diffusion model at a later stage is intriguing, and we look forward to experimenting with this approach. Thank you once again for your invaluable suggestions and insights!", "author_response": "We are pleased that our responses have addressed your concerns and appreciate your consideration to raise the score!\n\nRegarding your further inquiries, we currently utilize 20K iterations because this represents almost the maximum compute we can afford (64 GPUs over 3 days). However, we have not yet observed the peak performance of our models. For example, in a trial where we extended to 30K iterations, we managed to improve the FID from 19.3 to 18.7. We are eager to explore extending the training duration in our revised version to further confirm the model's stability.\n\nThe suggestion to reinitialize the fake diffusion model at a later stage is intriguing, and we look forward to experimenting with this approach. Thank you once again for your invaluable suggestions and insights!"}
{"claim": "Unlike the original DMD work, the authors present only 20K-step results, preventing meaningful comparison across longer training schedules.", "claim_type": "experimental", "paper_id": "tQukGCDaNT", "paper_title": "Improved Distribution Matching Distillation for Fast Image Synthesis", "paper_venue": "neurips2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "K5GmPvDd81", "reviewer": "Reviewer_aF5v", "review_text": "Comment: Happy to raise my score.\n\nOne thing i want to understand from authors, is how stable is this formulation when number of steps go from 20K to say 60K or more? Do you see performance peak around 20K or further training improves quality? unlike original DMD as authors report only 20K was curious if there were any interesting empirical findings which would provide further insights to community.\n\nThis goes back to fake score function alignment, does it help to reinitialize fake score function with teacher model again as student converges well to teacher? As it seems a bit unclear on properties/fit of fake score function and how it effects overall training stability.\n\nLooking forward for more results in later version of paper.", "labeling_timestamp": "2026-01-11T16:28:29.931911", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "Authors acknowledge the limitation (only 20K due to compute) and provide preliminary 30K results and intent to extend training, but have not fully resolved the lack of longer-schedule comparisons.", "evidence": "\"we currently utilize 20K iterations because this represents almost the maximum compute we can afford (64 GPUs over 3 days).\"; \"in a trial where we extended to 30K iterations, we managed to improve the FID from 19.3 to 18.7. We are eager to explore extending the training duration in our revised version to further confirm the model's stability.\"", "combined_author_response": "We are pleased that our responses have addressed your concerns and appreciate your consideration to raise the score!\n\nRegarding your further inquiries, we currently utilize 20K iterations because this represents almost the maximum compute we can afford (64 GPUs over 3 days). However, we have not yet observed the peak performance of our models. For example, in a trial where we extended to 30K iterations, we managed to improve the FID from 19.3 to 18.7. We are eager to explore extending the training duration in our revised version to further confirm the model's stability.\n\nThe suggestion to reinitialize the fake diffusion model at a later stage is intriguing, and we look forward to experimenting with this approach. Thank you once again for your invaluable suggestions and insights!", "author_response": "We are pleased that our responses have addressed your concerns and appreciate your consideration to raise the score!\n\nRegarding your further inquiries, we currently utilize 20K iterations because this represents almost the maximum compute we can afford (64 GPUs over 3 days). However, we have not yet observed the peak performance of our models. For example, in a trial where we extended to 30K iterations, we managed to improve the FID from 19.3 to 18.7. We are eager to explore extending the training duration in our revised version to further confirm the model's stability.\n\nThe suggestion to reinitialize the fake diffusion model at a later stage is intriguing, and we look forward to experimenting with this approach. Thank you once again for your invaluable suggestions and insights!"}
{"claim": "The paper does not evaluate whether reinitializing the fake score function from the teacher model helps training as the student converges.", "claim_type": "experimental", "paper_id": "tQukGCDaNT", "paper_title": "Improved Distribution Matching Distillation for Fast Image Synthesis", "paper_venue": "neurips2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "K5GmPvDd81", "reviewer": "Reviewer_aF5v", "review_text": "Comment: Happy to raise my score.\n\nOne thing i want to understand from authors, is how stable is this formulation when number of steps go from 20K to say 60K or more? Do you see performance peak around 20K or further training improves quality? unlike original DMD as authors report only 20K was curious if there were any interesting empirical findings which would provide further insights to community.\n\nThis goes back to fake score function alignment, does it help to reinitialize fake score function with teacher model again as student converges well to teacher? As it seems a bit unclear on properties/fit of fake score function and how it effects overall training stability.\n\nLooking forward for more results in later version of paper.", "labeling_timestamp": "2026-01-11T16:28:26.977370", "model": "gpt-5-mini", "label": "Supported", "justification": "Authors acknowledge they have not tried reinitializing the fake model and indicate they plan to experiment with this suggestion, thus agreeing the paper currently lacks that evaluation.", "evidence": "\"we have not yet observed the peak performance of our models... The suggestion to reinitialize the fake diffusion model at a later stage is intriguing, and we look forward to experimenting with this approach.\"", "combined_author_response": "We are pleased that our responses have addressed your concerns and appreciate your consideration to raise the score!\n\nRegarding your further inquiries, we currently utilize 20K iterations because this represents almost the maximum compute we can afford (64 GPUs over 3 days). However, we have not yet observed the peak performance of our models. For example, in a trial where we extended to 30K iterations, we managed to improve the FID from 19.3 to 18.7. We are eager to explore extending the training duration in our revised version to further confirm the model's stability.\n\nThe suggestion to reinitialize the fake diffusion model at a later stage is intriguing, and we look forward to experimenting with this approach. Thank you once again for your invaluable suggestions and insights!", "author_response": "We are pleased that our responses have addressed your concerns and appreciate your consideration to raise the score!\n\nRegarding your further inquiries, we currently utilize 20K iterations because this represents almost the maximum compute we can afford (64 GPUs over 3 days). However, we have not yet observed the peak performance of our models. For example, in a trial where we extended to 30K iterations, we managed to improve the FID from 19.3 to 18.7. We are eager to explore extending the training duration in our revised version to further confirm the model's stability.\n\nThe suggestion to reinitialize the fake diffusion model at a later stage is intriguing, and we look forward to experimenting with this approach. Thank you once again for your invaluable suggestions and insights!"}
{"claim": "The properties, fit, and behavior of the fake score function are not characterized, and its effect on overall training stability is unclear.", "claim_type": "methodology", "paper_id": "tQukGCDaNT", "paper_title": "Improved Distribution Matching Distillation for Fast Image Synthesis", "paper_venue": "neurips2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "K5GmPvDd81", "reviewer": "Reviewer_aF5v", "review_text": "Comment: Happy to raise my score.\n\nOne thing i want to understand from authors, is how stable is this formulation when number of steps go from 20K to say 60K or more? Do you see performance peak around 20K or further training improves quality? unlike original DMD as authors report only 20K was curious if there were any interesting empirical findings which would provide further insights to community.\n\nThis goes back to fake score function alignment, does it help to reinitialize fake score function with teacher model again as student converges well to teacher? As it seems a bit unclear on properties/fit of fake score function and how it effects overall training stability.\n\nLooking forward for more results in later version of paper.", "labeling_timestamp": "2026-01-11T16:28:37.141136", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "Authors provide limited empirical evidence (longer training improved FID) and express willingness to try suggested interventions, but they do not fully characterize the fake score function's properties, fit, or detailed effects on training stability.", "evidence": "“we have not yet observed the peak performance of our models. For example, in a trial where we extended to 30K iterations, we managed to improve the FID from 19.3 to 18.7.” \"The suggestion to reinitialize the fake diffusion model at a later stage is intriguing, and we look forward to experimenting with this approach.\"", "combined_author_response": "We are pleased that our responses have addressed your concerns and appreciate your consideration to raise the score!\n\nRegarding your further inquiries, we currently utilize 20K iterations because this represents almost the maximum compute we can afford (64 GPUs over 3 days). However, we have not yet observed the peak performance of our models. For example, in a trial where we extended to 30K iterations, we managed to improve the FID from 19.3 to 18.7. We are eager to explore extending the training duration in our revised version to further confirm the model's stability.\n\nThe suggestion to reinitialize the fake diffusion model at a later stage is intriguing, and we look forward to experimenting with this approach. Thank you once again for your invaluable suggestions and insights!", "author_response": "We are pleased that our responses have addressed your concerns and appreciate your consideration to raise the score!\n\nRegarding your further inquiries, we currently utilize 20K iterations because this represents almost the maximum compute we can afford (64 GPUs over 3 days). However, we have not yet observed the peak performance of our models. For example, in a trial where we extended to 30K iterations, we managed to improve the FID from 19.3 to 18.7. We are eager to explore extending the training duration in our revised version to further confirm the model's stability.\n\nThe suggestion to reinitialize the fake diffusion model at a later stage is intriguing, and we look forward to experimenting with this approach. Thank you once again for your invaluable suggestions and insights!"}
{"claim": "The combination of EGNN and ESM2 limits the novelty of the method.", "claim_type": "novelty", "paper_id": "Dr4qD9bzZd", "paper_title": "Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "a9hHYfEm0G", "reviewer": "Reviewer_Wq6M", "review_text": "Summary: This paper proposes NAEPro, a model to jointly design Protein sequence and structure. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from the nearest amino acids in three-dimensional (3D) space. The global attention sub-layer parameters are initialized with ESM-2. The author combines ESM2 and EGNN for co-modeling protein sequence and structure.\n\nStrengths: 1. The reported performance is good.\n2. The method is simple.\n\nWeaknesses: 1. Novelty: Both EGNN and ESM2 are existing models, and there are many existing works on antibody structure and sequence co-design. The combination may limit the novelty of this method.  \n2. Significance: It is likely that previous works can be readily applied to the motif-conditioned setting, and the authors can easily adapt their method to antibody design tasks. It would be beneficial if the authors further elaborate on the significance of their work\n3. Code: The authors do not provide code for checking the soundness of the methods.\n4. Experiment setting: The authors do not provide results on standard benchmarks, such as the CATH dataset, for fair comparison on both sequence and structure design.\n\nQuestions: 1. Could you provide experimental results from the CATH dataset to compare with the original SMCDiff and FrameDiff results?\n2. Similarly, could you provide head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design? Please follow the same setting.\n3. Could you provide the code for checking the results?", "labeling_timestamp": "2026-01-11T16:28:50.390394", "model": "gpt-5-mini", "label": "Contradicted", "justification": "Authors explicitly reject the claim, describe architectural novelties that differentiate NAEPro from an EGNN+ESM2 combination, and provide empirical results showing EGNN+ESM2 performs worse than NAEPro.", "evidence": "\"We would like to point out that our NAEPro is not a combination of ESM2 and EGNN. Instead our key innovations are (1) interleaving layers of sequence-level attention and local neighborhood equivariant sub-layer (2) accelerating the local sub-layer with the k-nearest neighbors.\" ... \"As the results show, EGNN+ESM2 performs much worse than our NAEPro, and removing the ESM2 initialization will not influence the performance too much, demonstrating that our model is much more superior than EGNN+ESM2.\"", "combined_author_response": "We much appreciate your valuable comments and insightful suggestions again, which helps a lot to improve the quality of our paper. As the discussion deadline is approaching, please let us know if you have any further concerns or questions. We are happy to have any further discussion!\n\n---\n\n**4. \"Through this way, amino-acid combinations frequently occur in the same context would draw higher attention scores\". Have you checked this statement? Could you provide some visualizations of the attention map and correlated frequences of amino-acid combinations?**  \n\nAns: Thank the reviewer for this suggestion. We randomly pick one sentence in the training set and visualization its attention matrix from the last layer in Appendix Figure 7(a) and also visualize the pairwise amino acid co-occurrence matrix in Figure 7(b). We find it’s consistent with our statement that residues tend to more connect to their neighboring residues. However, we don’t see any strong correlation between the attention matrix and the pairwise amino acid co-occurrence. Therefore, we revised the corresponding statement in our paper and uploaded a revised version accordingly.\n\n**5. Updating residue representations and coordinates in 3D space with only nearest neighbors enables more efficient and economic message passing compared to prior approaches which compute messages on the complete pairwise residue graph\". To my knowledge, most of the previous methods such as proteinmpnn adopt knn for constructing sparse graph in the 3D spcace. You should provide evidence to support the significance of your statement.**\n\nAns: We mean from the architecture level, previous methods like SE(3)-Transformer [1] and EGNN [2] adopts information flow from all other atoms.\n\n[1] SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks. Fabian B. Fuchs et al. NeurIPS 2020. \n\n[2] E(n) Equivariant Graph Neural Networks. Victor Garcia Satorras et al. ICML 2021. \n\n**6. Novelty: I do not observe enough novelty from the perspective of machine learning in algorithm design.**\n\nAns: Yes, from machine learning algorithm level, our method is not new. However, as admitted by reviewer MgyU, we design a new architecture for protein design. The key innovations of our architecture include: (1) interleaving layers of sequence-level attention and local neighborhood equivariant sub-layer (2) accelerating the local sub-layer with the k-nearest neighbors.\n\n---\n\nThanks for the responses! Our answers to your follow-up questions are provided as follows:\n\n**1. About Weakness4 & Q1 Page 2, \"We carry out experiments on two metalloproteins, including β-lactamase and myoglobin.\" I would like to say that the experimental setting is the main concern to me. Since the authors have only evaluated the proposed method on two structures, I am not sure if the method can be extended to generalized situations. I know that you are doing protein sequence-structure inpaintning problem, in which case a comparison with a previous baselines [1,2,3] on stantard datasets may help to reveal the generalization capabilities of your algorithm.** \n\nAns: We agree with reviewer Wq6M and additionally compare our method with PROTSEED on B12 protein. Due to the approaching ddl and limited time, PROTSEED has only been trained for 100 epochs, the same as our method. (suggested training epoch is 2000 on their GitHub)\n\nThe results are reported as follows: \n\n**Binding affinity score for designed B12**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|PROTSEED|-9.45$\\pm$ 0.03|-9.36$\\pm$0.09|-9.04$\\pm$0.27|-8.46|\n|NAEPro|**-11.06$\\pm$0.14**|**-10.96$\\pm$0.14**|**-10.61$\\pm$0.29**|**-9.11**|\n\n**2. About Q2 Your methods are not carefully designed for protein inverse folding. However, the presented results outperformed current SOTA by a large margin (curent SOTA is about 50%-60% AAR), which could not convince me. I doubt the veracity and correctness of the experimental results.** \n\nAns: Sorry for the confusion. We evaluate the inverse-folding task on our own two datasets instead of the original CATH. For our datasets, the proteins are from the same family, and thus they may have much overlap on the sequence level, which definitely will lead to high AAR. Besides, our model is initialized with ESM2 weights, which will also improve the AAR. \n\nAlthough the sequence prediction based a fixed backbone structure is not our objective, **we adapt our method to CATH setting by keeping all the CA coordinates and masking all residues. Due to the approaching ddl and limited time, our model has only been trained for 10 epochs, while usually the suggested training epoch would be 100 like GVP, PiFold, etc.** (See below: ESMIF, ProteinMPNN, PiFold results are quoted from PiFold paper). \n\nCATH 4.2 results\n\n|Model| AAR (%)|PPL |\n|:-----|:----:|:----:|\n|ProteinMPNN|45.96|4.61|\n|ESMIF|38.30|6.44|\n|PiFold|51.66|4.55|\n|NAEPro|9.16|15.26|\n\n**Again, we did not claim the superiority of our method on inverse folding task on CATH dataset.**\n\n**3. Page 1, \"Despite their great potential for novel structure design, such sequential design policy fails to cross-condition on sequence and structure, which might lead to inconsistent proteins and inefficient design process\". I can not agree with this statement. RFDiffusion's authors say that they also considered simultaneously designing structure and sequence within RFdiffusion, but combining ProteinMPNN with the diffusion of structure alone provides the excellent performance, as shown in your Table.1. Unfortunately, I observe that you deleted the consistency metric in Table.1, which seems like dishonest behavior to me.**\n\nAns: We do not intend to hide any information. As suggested by reviewer MgyU, the binding affinity is a better metric for functional protein design rather than the accuracy metrics (consistency, AAR, RMSD, etc.). Therefore, we replaced all the accuracy metrics in Table 1 and 2 in our paper according with the binding affinity scores calculated by Gnina. The full table with all scores are listed below:\n\n$\\beta$-lactamase\n\n|Model| AAR (\\%,$\\uparrow$) | RMSD ({ \\AA},$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination | $4.79$ | $--$ | $30.5511$ | $0.2918$ | $--$|\n|Inpainting | $16.73$ | $4.0599$ | $61.7679$ | $0.3790$ | $6.2578$|\n|SMCDiff+ProteinMPNN| $19.94$ | $10.3960$ | $42.0375$ | $0.3458$ |$10.2117$|\n| PROTSEED | $37.63$ | $3.0142$ | $64.3861$| $0.4637$ | $3.3748$ |\n| FrameDiff+IF |$26.20$ | $6.0151$ | $65.6445$ | $0.3657$ | $7.8703$|\n|RFDiffusion+IF | $22.93$ | $6.0438$ | **83.4058** | $0.3747$ |**0.5565**|\n|NAEPro|**65.71**| **2.9916**| 66.8705|**0.4812**| 7.3760|\n\n\n**Myoglobin**\n|Model| AAR (\\%,$\\uparrow$) | RMSD (\\AA,$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination| $4.81$ | $--$ | $38.2817$ | $0.2754$ |--|\n|Inpainting |39.59|3.3751|67.0813|0.4391 |3.2108|\n|SMCDiff+IF | $12.47$ | $8.0067$ | $34.5914$ | $0.2235$ | $8.8754$ |\n| PROTSEED | $48.55$ | $2.8753$ | $61.5588$ | $0.5466$ | $0.9764$ |\n| FrameDiff+IF | $20.43$ | $5.9739$ | $61.3945$ | $0.3757$ |$3.5078$|\n|RFDiffusion+IF | $32.70$ | $3.9930$ | $78.9868$ | $0.4147$  | **0.4375** |\n|NAEPro | **89.37**| **2.6307**|**81.9507**|**0.5692**|4.3865|\n\n---\n\nThanks for the further response! Please see our responses to your follow-up questions and concerns as follows:\n\n**Response to Weakness 3 & Q1: My point in mentioning these three papers is not to introduce additional (and possibly meaningless) in-silico comparisons to the authors. Instead, they validated their designs with solid wet-lab experiments demonstrating that it is possible to generate novel protein sequences with fixed backbones.**\n\nAns: We agree on the reviewer’s opinion and have accordingly updated the statement in Introduction.  We have uploaded a revised version of our paper.\n\n**Response to Weakness 4: I completely agree that all the existing evaluation metrics we used in deep learning are only indirect indicators, and they only imply a higher chance that the assessed protein will perform its function. However, I have trouble following the logic that \"*Therefore, a low TM-score cannot indicate the designed protein doesn’t have the corresponding function*\" suggests \"proteins with TM-scores that are lower than 0.5 (which is a wildly-used standard) are still reliable in terms of their functionality in general.\" Similarly, if we all agree that the existing metrics are suboptimal and none of them provides 100% accurate evaluations, it is then meaningless to criticize the widely applied standards with one or two counter-examples. After all, you don't have any better evidence (such as wet-lab results) that is more reliable than the existing criteria.**\n\nAns: We agree with Reviewer VH6U. As suggested by reviewer MgyU, the binding affinity is a better metric for functional protein design. We calculated the binding affinity scores for top-5, top-10, and top-30 candidates and also median score on the test set using Gnina. The results are reported as follows:\n\n**Binding affinity score for designed myoglobin**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-8.18$\\pm$ 0.01|-8.07$\\pm$0.03|-7.97$\\pm$0.23|-7.25|\n|Inpainting|-13.47$\\pm$0.02|-13.12$\\pm$0.12|-12.31$\\pm$0.54|-9.56|\n|SMCDiff+ProteinMPNN|-11.37$\\pm$0.03|-11.12$\\pm$0.31|-10.87$\\pm$0.42|-8.76|\n|PROTSEED|-13.21$\\pm$0.13|-12.89$\\pm$0.42|-11.98$\\pm$0.52|-10.23|\n|FrameDiff+ProteinMPNN|-13.13$\\pm$0.05|-12.92$\\pm$0.16|-12.21$\\pm$0.23|-10.08|\n|RFDiffusion+ProteinMPNN|-13.68$\\pm$0.02|-13.03$\\pm$0.21|-12.56$\\pm$0.43|-10.15|\n|NAEPro|**-14.12$\\pm$0.01**|**-13.85$\\pm$0.10**|**-13.06$\\pm$0.38**|**-10.74**|\n\n**Binding affinity score for designed beta-lactamase**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-6.98$\\pm$0.01|-6.87$\\pm$0.02|-6.69$\\pm$0.05|-6.29|\n|Inpainting|-9.89$\\pm$0.03|-9.54$\\pm$0.16|-9.13$\\pm$0.43|-7.24|\n|SMCDiff+ProteinMPNN|-9.10$\\pm$0.01|-9.05 $\\pm$0.02|-8.98$\\pm$0.01|-6.97|\n|PROTSEED|-9.88$\\pm$0.21|-9.51$\\pm$0.41|-9.01$\\pm$0.62|-7.31|\n|FrameDiff+ProteinMPNN|-9.54$\\pm$0.03|-9.56$\\pm$0.23|-8.89$\\pm$0.35|-7.03|\n|RFDiffusion+ProteinMPNN|-9.87$\\pm$0.05|-9.56$\\pm$0.23|-9.12$\\pm$0.53|-7.51|\n|NAEPro|**-10.06$\\pm$0.05**|**-9.79$\\pm$0.10**|**-9.39$\\pm$0.12**|**-7.66**|\n\n**Response to Q6: Because ESMFold is a less reliable tool for evaluating novel proteins, it should not be used here to assess your designs at all.**\n\nAns: We totally agree with reviewer VH6U. We provide the mean pLDDT for top-5, top-10, top-30 candidates calculated by AlphaFold2 as follows:\n\n**AlphaFold2 pLDDT for designed myoglobin**\n\n|Model| top-5|top-10|top-30|\n|:-----|:----:|:----:|:----:|\n|Hallucination|76.7243|73.5953|68.6060|\n|Inpainting|97.6171|97.6063|97.5681|\n|SMCDiff+ProteinMPNN|85.7051|84.7744|84.9220|\n|PROTSEED|92.9770|91.9216|90.4293|\n|FrameDiff+ProteinMPNN|97.3997|97.3329|96.4279|\n|RFDiffusion+ProteinMPNN|97.9722|97.9174|97.6063|\n|NAEPro|**98.1057**|**98.0706**|**97.9197**|\n\n**AlphaFold2 pLDDT for designed $\\beta$-lactamase**\n\n|Model| top-5|top-10|top-30|\n|:-----|:----:|:----:|:----:|\n|Hallucination|74.7760|74.7032|71.2932|\n|Inpainting|94.5489|94.4328|93.2941|\n|SMCDiff+ProteinMPNN|86.2510|85.7072|83.0376|\n|PROTSEED|98.2896|98.2740|97.9683|\n|FrameDiff+ProteinMPNN|98.3049|98.2922|98.1267|\n|RFDiffusion+ProteinMPNN|**98.6225**|**98.5116**|98.0643|\n|NAEPro|98.5385|98.4481|**98.2841**|\n\n---\n\nDear reviewer,\n\n    We sincerely appreciate the time and effort you dedicated to reviewing our paper and reading our responses. Your valuable suggestions and insightful comments have significantly contributed to refining the quality of our work. No matter what the final result will be, the thoughtful communication with you has better clarified our paper's goals and logic. We would like to express our great gratitude to you!\n\nBest,\n\nAuthors of paper 4608\n\n---\n\n**Could the authors briefly explain how do you construct the one-to-one mapping between the design structure and the target structure in PDB? Is it determined by retrieving the one in PDB with the smallest RMSD? As far as I know, there is no common practice to determine this for the task of protein design, and I appreciate any explanation that clues me in.**\n\nAns: Yes, we construct the one-to-one mapping between the designed structure and the target structure in PDB. It is not determined by retrieving the one in PDB with the smallest RMSD. We extract the motif fragments for each protein, based on which our training objective is to recover the original protein sequence and backbone structure. Therefore, we compute the RMSD between the designed structure and the target one which we want to recover.\n\n---\n\n**Corrected statement: But it can still bind the corresponding metallocofactor heme in the AlphaFill simulation. For the claim “binding rates of 100% and 90%”, I wonder how is the criterion/threshold of “binding” determined? This success rate is rather unrealistic and probably fails to reflect the real performance. Thus, the statement in Section 5.2. saying “It is evident that our method can generate proteins that express the basic and important functions.” can be quite misleading for readers who are not familiar with the task.** \n\nAns: Sorry for the misleading statement. We mean through the AlphaFill prediction, the designed proteins are highly potential to bind the corresponding metallocofactors. We have corrected all the claims in our paper. We provided the process of how to decide if the designed protein can “bind” the metallocofactors in Appendix B.3. We would like to further explain it here.  First of all, we use AlphaFill to do the first-step prediction. AlphaFill will return a complex based on the input protein structure. If the complex includes the corresponding metal ions, we think the designed protein is potential to have functions. Then to guarantee the results convincing, we further use some additional constraints to ensure the designed proteins are potential to express functions. Specifically, we assume if a designed protein share the similar active site environments with their natural counterparts (target protein in our dataset), then it’s highly potential to bind the corresponding metal ions. For myoglobin, we calculate the distance between axial histidine ligand to Fe ion. Besides, we also detect the presence of distal histidine within the active sites, which plays an important role in the oxygen molecule binding function of myoglobin . If the distance is between 2.0Å and 2.5Å, and the distal histidine exists, we think the designed myoglobins share similar active site environments to natural ones. For $beta$-lactamase, we detect the residues that directly contact with zinc ion. If they show similar chemistry properties with natural $beta$-lactamases, we think they can highly potentially bind the corresponding metallocofactors. For example, in Figure 5(a), the active site possesses one zinc atom coordinated to three histidines, while the second zinc atom is coordinated to one histidine, one cysteine, and one aspartate, demonstrating a high degree of amino acid analogy to the natural protein.\n\n**Could the author intuitively describe how does the NAEL deal with the reflection case so as to be simply SE(3)-equivariant instead of E(3)?** \n\nAns: Intuitively, applying reflection to the input structure, the output structure will also be reflected. The conformation of the original backbone will be changed from L-amino acid to D-amino acid (We provide an illustration in Figure 6 in appendix). Therefore, the reflection will change the chirality of the protein which may cause the deficiency of binding to the ligand and eventually its function. However, the protein sequence in our model will not be influenced. Therefore, the 3D structure of the designed sequence which is L-amino acid and the output structure  which becomes D-amino acid will be inconsistent. From this aspect, our model doesn’t satisfy reflection equivariance.\n\n**The authors are encouraged to encompass appropriate comparison with EGNN/SE(3)-transformer in their writing to better contextualize the proposed NAEL.**\n\nAns: We provide the comparison with EGNN+ESM2 on myoglobin in the following Table and also updated the results in Table2 accordingly.\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|EGNN+ESM2|-13.30$\\pm$0.61 | -12.77$\\pm$0.69 |-12.07$\\pm$0.65 | -9.63|\n|NAEPro|**-14.12$\\pm$0.01**|**-13.85$\\pm$0.10**|**-13.06$\\pm$0.38**|**-10.74**|\n\n**It is surprising to see scaling the model does not yield consistent performance gain. Could the authors help explain this behavior? Also, I noticed that the authors merge the two myoglobin and lactamase data for training newly in the rebuttal response. Is there any insight for doing such multi-task learning?**\n\nAns: Enlarging the model hasn’t improved the performance too much. Our interpretation is that the training data size is moderate, and thus a model with moderate size can fit the data well. To validate if this guess holds, we merge the two protein data and then train a unified model. As we can see in the previous table, our model did achieve obvious better results on $\\beta$-lactamase, i.e., TM-score from below 0.5 (0.4919) to over 0.5 (0.5055).\n\n---\n\nWe much appreciate the reviewer’s responses and suggestions, which help a lot to improve our paper.  We have accordingly revised our paper, including Table 1, Table 2, Section 4.2-4.4 and Section 5.2. Our responses to the reviewer’s follow-up questions are provided as follows:\n\n**About the evaluation metrics: I think the binding affinity could better support the effectiveness of NAEPro and be more suitable for the task (functional protein design) rather than the accuracy metrics (RMSD, AAR, etc.). Do the authors agree on this or not?** C**ould the authors explain how the Binding Affinity is calculated for both model based on Gnina? Specifically, I am worried that the metrics is averaged among the test set similar to other metrics.  If so, I suggest that the authors report median or top-k mean as evaluation.**\n\nAns: We completely agree with reviewer MgyU that the binding affinity is a better metric for functional protein design rather than the accuracy metrics. We revised Table 1 and 2 in our paper according to your suggestion. The binding affinity is calculated using Gnina as the average score on the test set. Thank the reviewer for this valuable suggestion. We provide  top-5, top-10, top-30 mean and variance as well as the median in the following table and also revise our paper accordingly. From the results, we can see that our model achieves the best median and top-K binding affinity scores on both datasets.\n\n**Myoglobin**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-8.18$\\pm$ 0.01|-8.07$\\pm$0.03|-7.97$\\pm$0.23|-7.25|\n|Inpainting|-13.47$\\pm$0.02|-13.12$\\pm$0.12|-12.31$\\pm$0.54|-9.56|\n|SMCDiff+ProteinMPNN|-11.37$\\pm$0.03|-11.12$\\pm$0.31|-10.87$\\pm$0.42|-8.76|\n|PROTSEED|-13.21$\\pm$0.13|-12.89$\\pm$0.42|-11.98$\\pm$0.52|-10.23|\n|FrameDiff+ProteinMPNN|-13.13$\\pm$0.05|-12.92$\\pm$0.16|-12.21$\\pm$0.23|-10.08|\n|RFDiffusion+ProteinMPNN|-13.68$\\pm$0.02|-13.03$\\pm$0.21|-12.56$\\pm$0.43|-10.15|\n|NAEPro|**-14.12$\\pm$0.01**|**-13.85$\\pm$0.10**|**-13.06$\\pm$0.38**|**-10.74**|\n\n**Beta-lactamase**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-6.98$\\pm$0.01|-6.87$\\pm$0.02|-6.69$\\pm$0.05|-6.29|\n|Inpainting|-9.89$\\pm$0.03|-9.54$\\pm$0.16|-9.13$\\pm$0.43|-7.24|\n|SMCDiff+ProteinMPNN|-9.10$\\pm$0.01|-9.05 $\\pm$0.02|-8.98$\\pm$0.01|-6.97|\n|PROTSEED|-9.88$\\pm$0.21|-9.51$\\pm$0.41|-9.01$\\pm$0.62|-7.31|\n|FrameDiff+ProteinMPNN|-9.54$\\pm$0.03|-9.56$\\pm$0.23|-8.89$\\pm$0.35|-7.03|\n|RFDiffusion+ProteinMPNN|-9.87$\\pm$0.05|-9.56$\\pm$0.23|-9.12$\\pm$0.53|-7.51|\n|NAEPro|**-10.06$\\pm$0.05**|**-9.79$\\pm$0.10**|**-9.39$\\pm$0.12**|**-7.66**|\n\n**Corrected statement: ESMFold might not accurately predict the structure for a novel sequence.**\n\nAns: Sorry for the misunderstood claim. We want to express we agree on reviewer VH6U’s comment in question 6 “for a novel sequence, its prediction is worse than AlphaFold2.”  As suggested by reviewer VH6U, we re-calculate pLDDT by AlphaFold2. Our model respectively achieves 91.4139 and 76.9550  on myoglobin and $\\beta$-lactamase on average. As claimed in previous work [1,2], pLDDT between 70~90 are classified to be confident and pLDDT ≥ 90 indicates residues predicted with extremely high confidence.\n\n[1] pLDDT Values in AlphaFold2 Protein Models Are Unrelated to Globular Protein Local Flexibility. Oliviero Carugo. Crystals. 2023.\n\n[2] Generating new protein sequences by using dense network and attention mechanism. Feng Wang et al. Mathematical Biosciences and Engineering. 2023.\n\n---\n\nThanks for your valuable review and comments again. We have addressed all your concerns and questions.\n\nIf you have any other questions, please feel free to discuss with us.\n\n---\n\nThanks for your valuable review and comments again. We have addressed all your concerns and questions.\n\nIf you have any other questions, please feel free to discuss with us.\n\n---\n\nThanks for your valuable review and comments again. We have addressed all your concerns and questions.\n\nIf you have any other questions, please feel free to discuss with us.\n\n---\n\n**Q3: In section 4.2, the author(s) described that the weights of NAEPro is partially initialized from pretrained ESM2-8M, of the smallest model size among the ESM2 series. This is very problematic but the recent common practices leverage at least the 650M model. Do the author(s) try much larger model size as initialization? I found in the Table 2 that the 8M model has only marginal improvement upon random initialization.**\n\nAns: Yes, we tried to partially initialize our model with 35M ESM2 (NAEPro-12) and also train this model on the merged myoglobin and beta-lactamase data (NAEPro-12-Unified). For myoglobin, enlarging the model to 12 layers will not influence the performance too much, but training the larger model on the merged dataset will increase the pLDDT and TM-score. For beta-lactamase, enlarging the model from 6 layers to 12 layers is beneficial and will increase the overall performance. Particularly, training the 12-layer model on merged dataset will significantly improve the performance of beta-lactamase, say improve the average TM-score from below 0.5 to above 0.5. Then we also partially initialize a 30-layer model with 150M ESM2 (NAEPro-30-Unified) and train the model on merged dataset. The performance for both myoglobin and beta-lactamase will be further improved.\n\n**myoglobin**\n\n|Model| AAR (%)|RMSD|pLDDT |TM-score |Parameters|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|NAEPro-6 |**89.37**|**2.6307**|81.9507|0.5692|11.6M|\n|NAEPro-12 |88.42|2.6333|82.2849|0.5673|52M|\n|NAEPro-12-Unified|86.81|2.6443|82.9736|0.5736|52M|\n|NAEPro-30-Unified|86.62|2.6410|**83.0173**|**0.5749**|223.7M|\n\n**$\\beta$-lactamase**\n\n|Model| AAR (%)|RMSD|pLDDT |TM-score |Parameters|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|NAEPro-6 |65.71|2.9916|66.8705|0.4812|11.6M|\n|NAEPro-12 |67.28|2.9864|68.7394|0.4919|52M|\n|NAEPro-12-Unified|70.93|**2.9707**|69.9333|0.5055|52M|\n|NAEPro-30-Unified|**72.60**|2.9781|**71.0429**|**0.5089**|223.7M|\n\n**Q4: How is the “target structure” in evaluation metrics(Section 4.2) determined? What is the difference between metrics RMSD and consistency?**\n\nAns: Target structure means the natural protein structure provided in PDB. RMSD is calculated as the RMSD between the designed protein structure and the one given in PDB. Consistency is calculated as the RMSD between the designed structure and the one predicted by ESMFold for the designed sequence.\n\nWe really hope our responses address your concerns. If you have any other questions, we are very happy to continue discussions!\n\n---\n\n**Weakness 4: For the evaluation metrics, the problem to be studied is functional protein (as it is claimed in the title) design instead of general backbone generation(eg., RFDiffusion) or sequence generation (eg., ProteinMPNN). Thus, I found the the five of the metrics fail to (or indirectly if they potentially do) reflect the performance of functional protein design or scaffolding. All these metrics in Table 1 & 2 show the somehow sequence-structure matching between the predicted and native, but are not very suitable for proteins such as enzyme.**\n\nAns: In our analysis (Section 5.2), we evaluated the function for binding the corresponding metallocofactors of the designed proteins. Specifically, we first randomly select 20 cases from the top100 sequences (pLDDT ranking). We then employ AlphaFold2 for protein structure prediction, followed by inputting these structures into AlphaFill [1] to predict the associated ligands. Notably, our results indicate that all designed myoglobins exhibit heme binding capability, while 18 beta-lactamases demonstrate the ability to bind zinc ions, resulting in metallocofactor binding rates of 100% and 90%, respectively. It is evident that our method can generate proteins that express the basic and important functions, i.e. binding the corresponding metallocofactors. To systematically illustrate the function of our designed proteins, we use docking tool Gnina[2] to compute the binding affinity between the designed metalloproteins and the corresponding metallocofactors. The results in the following table shows that our model achieves the best binding affinity scores on both metalloproteins, demonstrating the proteins designed by our model are highly potential to actively exhibit biochemical functions.\n\n|Model| myoglobin (kcal/mol)|beta-lactamase (kcal/mol)|\n|:-----|:----:|:----:|\n|Hallucination|-7.23|-6.37|\n|Inpainting|-9.32|-7.63|\n|SMCDiff+ProteinMPNN|-8.79|-6.89|\n|PROTSEED|-10.07|-7.68|\n|FrameDiff+ProteinMPNN|-9.54|-7.21|\n|RFDiffusion+ProteinMPNN|-9.76|-7.59|\n|NAEPro|**-10.14**|**-7.80**|\n\n**Weakness 5: The potential theoretical flaw in the equivariance analysis in Section 3.5.** \n\nAns: Thank the reviewer for pointing out this typo. Our model satisfies only roto-translation equivariance without reflection equivariance. We have accordingly updated the paper.\n\n**Q1: The proposed architecture encoding both sequence and structure via the masked language modeling (MLM) scheme is somehow interesting. How do the author(s) see the  difference between the proposed NAEL layer and tensor field convolutional layer in SE(3)-transformer ? Also, what is the advantage of NAEL over the SeqIPA of the PROTSEED, the most competitive baseline in Table 1?** \n\nAns: The most biggest difference between our NAEPro and tensor field convolutional layer in SE(3)-transformer is:  NAEPro uses interleaving layers of global sequence-level attention and local neighborhood equivariant sub-layer, from which the residue representation is updated from both the whole sequence interaction and 3D neighbor message passing; instead, if we enable kNN message passing in SE(3)-transformer, then the node feature will only be updated based on 3D neighboring information. To clearly demonstrate the advantage of NAEL over SeqIPA in PROTSEED, we replace our NAEL with the SeqIPA-Addition without secondary structure feature. Taking myoglobin as an example, the results are shown in the following table. NAEPro with NAEL performs better than with SeqIPA.\n\n|Model| AAR (%)|RMSD|\n|:-----|:----:|:----:|\n|NAEPro -w/- SeqIPA|65.33|2.7078|\n|NAEPro|**89.37**|**2.6307**|\n\n**Q2:  In Section 3 - opening paragraph, the author(s) define/formulate the target task as generate a protein sequence and all 3D coordinates of N residues. However, in later model definition, the proposed NAEPro only operates on the C-alpha(CA) coordinates. Please explain this ambiguity.**\n\nAns: We are sorry for the ambiguity caused. When we mention “we formulate the target task as generate a protein sequence and all 3D coordinates of N residues”, we mean 3D coordinates of alpha-carbon of all the residues. Following the setting in [3], we represent the backbone structure as the CA-only coordinates of all residues. We have updated the paper accordingly in the new version.\n\n[3] Diffusion probabilistic modeling of protein backbones in 3D for the motif-scaffolding problem. Brian L. Trippe et al. ICLR. 2023.\n\n---\n\nWe appreciate the reviewer’s valuable and insightful suggestions, which are very helpful for us to improve our paper. We have clarified all your concerns, added the experiments and updated the paper accordingly. We address the specific concerns as follows:\n\n **Weakness 1: Regarding the consistency metrics reported, I noticed that the consistency metric of NAEPro in Table 1 (as the main results) underperformed half of the baselines.  Such key observation greatly weakened the method. The authors can elaborate on this point.**\n\nAns: Yes, we agree with the reviewer’s opinion. We would like to clarify our task more clearly. Our goal is to design functional proteins with novel sequence and structure. As shown in Figure 5(b), the designed myoglobin has a RMSD of 3.943\\AA with the most similar one in PDB and 26.7% AAR with the most similar one in Uniprot, but it can still bind the corresponding metallocofactor heme in the AlphaFill simulation [1]. In our paper, we use ESMFold to predict the structure of the designed sequence. However, as pointed out by reviewer VH6U, ESMFold might not accurately predict the structure for a novel sequence. To further clarify this point, we calculate the scRMSD of motifs between the designed structure and the predicted structure from the designed sequence in the following table. As the results show, our NAEPro achieves smaller RMSD than RFDiffusion+ProteinMPNN on myoglobin and comparable results on beta-lactamase.\n\n|Model| myoglobin|beta-lactamase|\n|:-----|:----:|:----:|\n|Inpainting|0.5292|0.7819|\n|SMCDiff+ProteinMPNN|1.3816|1.4523|\n|PROTSEED|0.4987|**0.3892**|\n|FrameDiff+ProteinMPNN|0.8765|0.9815|\n|RFDiffusion+ProteinMPNN|0.4936|0.3918|\n|NAEPro|**0.4832**|0.5239|\n\n[1] Alphafill: enriching alphafold models with ligands and cofactors. Maarten L Hekkelman et al. Nature methods. 2023.\n\n**Weakness 2: For the experiments, how is the model trained to obtain the evaluation digits in Table 1 for each baseline. This is very important.** \n\nAns: We are sorry for the confusion. We got the codes of all baselines from their GitHub repos, and then re-trained all the models separately on myoglobin and beta-lactamase datasets. The evaluation procedure among all the baselines and our model follows the same process, and thus we believe the comparisons in Table 1 are fair.\n\n**Weakness 3: For the dataset, I found that in section 4.1, the author(s) mentioned that both beta-lactamase and myoglobin datasets are split randomly. This is worrying because the entries in PDB can usually be redundant, especially for such large and well-studied families that the authors selected for evaluation. The author(s) can elaborate on this point. Moreover, in the Table 3 (in appendix), I am curious about the column name “PDB”, does it mean the number of PDB entries (with unique ID) or the number of single chains?** \n\nAns: Yes, this is a very good question. Since we focus on sequence and structure co-design task, we aim to design both functional sequence and structure. Therefore, our input data is the <sequence, structure> pair. From this point, even though the sequence might be the same, the structure is somehow different. Therefore, our paired input data are different and would not be redundant. We appreciate the reviewer for carefully reviewing our paper and also reading the appendix. In Table 3, PDB means the number of different PDB entries. To eliminate the reviewer’s concerns, we re-cleaned the test set by filtering pairs whose sequences have over 30% sequence identity to any sample in the training set after doing pairwise alignment for myoglobin. We compare our NAEPro and ProtSeed (the most competitive baseline) in the following table. As the results show, our model can design proteins with higher binding affinity (by Gnina [2]) on samples that have lower similarity to training data.\n\n|Model| RMSD|Binding Affinity (kcal/mol)|\n|:-----|:----:|:----:|\n|PROTSEED|**2.4031**|-9.62|\n|NAEPro|2.6634|**-9.98**|\n\n[2] GNINA 1.0: Molecular docking with deep learning. A McNutt et al. Cheminformatics. 2021.\n\n---\n\n**Weakness4 & Q1: The authors do not provide results on standard benchmarks, such as the CATH dataset, for fair comparison on both sequence and structure design. Could you provide experimental results from the CATH dataset to compare with the original SMCDiff and FrameDiff results?**\n\nAns: We would like to clarify our task more clearly. Our task aims to co-design functional and novel protein sequence and structure. In our method, we use MSAs to automatically find motifs to guarantee the protein function, which is achieved within the same protein family. Instead, CATH is a protein dataset consisting of proteins from diverse families, making the motif extraction difficult. Besides, CATH is usually used to evaluate inverse folding task, say protein sequence design based on fixed backbone structure, which is not the goal of our paper. We hope we didn’t misunderstand the reviewer’s meaning by “the original SMCDiff and FrameDiff results” as SMCDiff and FrameDiff haven’t been evaluated on CATH dataset. Since SMCDiff and FrameDiff focus on protein structure design, CATH is usually used to evaluate protein sequence design based on fixed backbone structure, which may have some discrepancies on the design goals. \n\n**Q2: Could you provide head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design? Please follow the same setting.**\n\nAns: We provide protein sequence design given backbone structures on our two datasets in the following table. In this setting, we slightly modified our method to make it adaptable to the task as the reviewer asked. Specifically, we provided the whole backbone structure as the model input and designed the whole protein sequence. As the results show, even though we only provide  backbone structure without any motif residues, our model can still achieves higher AAR and pLDDT on myoglobin, and higher AAR on beta-lactamase.\n\n**myoglobin**\n\n|Model| AAR (%)|pLDDT |\n|:-----|:----:|:----:|\n|ProteinMPNN|81.37|81.9817|\n|ESMIF|76.49|78.0986|\n|PiFold|78.93|80.2912|\n|NAEPro|**85.56**|**82.3871**|\n\n$\\beta$-lactamase\n\n|Model| AAR (%)|pLDDT |\n|:-----|:----:|:----:|\n|ProteinMPNN|53.59|**70.6723**|\n|ESMIF|57.49|61.3928|\n|PiFold|63.38|64.9017|\n|NAEPro|**68.39**|65.8218|\n\n**Q3: Could you provide the code for checking the results?**\n\nAns: We have provided the code and data in the supplementary material. Due to the memory limitation, we couldn’t upload the model checkpoints. However, the training process is highly efficient, and the reviewer could train the model if he/she is interested.\n\n---\n\nWe thank the reviewer for the positive reviews as well as the suggestions for improvement. We have clarified all your concerns, added the experiments and updated the paper accordingly. Our responses to the reviewer’s concerns and questions are provided below:\n\n**Weakness 1: Both EGNN and ESM2 are existing models, and there are many existing works on antibody structure and sequence co-design. The combination may limit the novelty of this method.**\n\nAns: We would like to point out that our NAEPro is not a combination of ESM2 and EGNN. Instead our key innovations are (1) interleaving layers of sequence-level attention and local neighborhood equivariant sub-layer (2) accelerating the local sub-layer with the k-nearest neighbors.  On the contrary,  EGNN is a fully-connected graph and updates message, coordinates and atom features based on all other atoms in 3D space, which is not efficient for long proteins. Besides, EGNN+ESM2 is a sequential process and can not cross-condition on sequence and structure. We compare our method with EGNN+ESM2, NAEPro w/o ESM2 initialization on myoglobin in the Following Table (Table 2 in the paper).  As the results show, EGNN+ESM2 performs much worse than our NAEPro, and removing the ESM2 initialization will not influence the performance too much, demonstrating that our model is much more superior than EGNN+ESM2.\n\n|Model| AAR (%)|RMSD|pLDDT |TM-score |consistency|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|EGNN+ESM2|51.12|2.9891|77.3399|0.4656|4.9827|\n|NAEPro-w/o-ESM2 initialization|79.82|2.6398|76.3032|0.5159|4.7273|\n|NAEPro|**89.37**|**2.6307**|**81.9507**|**0.5692**|**4.3865**|\n\n**Weakness 2: It is likely that previous works can be readily applied to the motif-conditioned setting, and the authors can easily adapt their method to antibody design tasks. It would be beneficial if the authors further elaborate on the significance of their work**\n\nAns: We would like to clarify our goal more clearly. Our task is to co-design functional and novel protein sequence and structure. Previous work which is most similar to ours is Inpainting [1]. However, [1] provides the functional sites manually while we automatically mined the functional sites (plus the conserved sites as suggested by reviewer ****VH6U****) by MSAs. Previous methods considering only sequence design constrained by fitness value [2,3] or sequence design based on given backbone structure (inverse folding) [4] is not suitable for our task. For antibody design, antibody is always Y-shaped, while one of our goal is to design novel and diverse protein structure. For example, in Figure 4, the designed beta-lactamases have different fold categories and belong to three different sub-classes. In Figure 5 (b), the designed myoglobin has a RMSD of 3.943\\AA with the most similar one in PDB, but it can still bind the corresponding metallocofactor heme in the AlphaFill simulation [5]. From this aspect, we think antibody design models can not be directly applied to our task without any modification and vice versa. \n\n[1] Scaffolding protein functional sites using deep learning.  Jue wang. et al. Science. 2022.\n\n[2] Biological Sequence Design with GFlowNets. [Moksh Jain](https://arxiv.org/search/q-bio?searchtype=author&query=Jain,+M) et al. ICML 2022. \n\n[3] Proximal Exploration for Model-guided Protein Sequence Design. Zhizhou Ren et al. ICML 2022. \n\n[4] Robust deep learning based protein sequence design using ProteinMPNN. J. Dauparas et al. Science. 2022.\n\n[5] Alphafill: enriching alphafold models with ligands and cofactors. Maarten L Hekkelman et al. Nature methods. 2023.\n\n**Weakness 3:** **The authors do not provide code for checking the soundness of the methods.**\n\nAns: We have provided the code and data in the supplementary material. Due to the memory limitation, we couldn’t upload the model checkpoints. However, the training process is highly efficient, and the reviewer could train the model if he/she is interested.\n\n---\n\n**Q6: For a novel sequence, ESMFold’s prediction is worse than AlphaFold2. Consequently, evaluating novel sequences should use AlphaFold2 instead of ESMFold.**\n\nAns: Due to limited time and computing resources, we provide the AlphaFold2 results for ProtSeed, RFDiffusion+ProteinMPNN and our NAEPro. As the results show, even though ESMFold achieves lower scores than AlphaFold2, the tendencies (ranking) among different models are similar.  Particularly, our model achieves an average TM-score over 0.5 on beta-lactamase (ESMFold 0.4812) and pLDDT over 70 (ESMFold 66.8705), which gives stronger evidence that our model has the ability to design proteins which have stable structures.\n\n**myoglobin**\n\n|Model| pLDDT|TM-score|\n|:-----|:----:|:----:|\n|PROTSEED|77.9032|0.5621|\n|RFDiffusion+ProteinMPNN|88.2130|0.5103|\n|NAEPro|**91.4139**|**0.5912**|\n\n**$\\beta$-lactamase**\n\n|Model| pLDDT|TM-score|\n|:-----|:----:|:----:|\n|PROTSEED|73.3176|0.4901|\n|RFDiffusion+ProteinMPNN|**87.8092**|0.4806|\n|NAEPro|76.9550|**0.5009**|\n\n **Q7: \"Consistency - What is the designed structure and the predicted structure? Which is from NAEPro? Where does the other one come from?**\n\nAns: Consistency is calculated as the RMSD between the designed structure from NAEPro and the one predicted by ESMFold for the designed sequence from NAEPro.\n\n---\n\n**Weakness 4:  Results in Table 1 do not support the superiority of the proposed method. For instance, RMSD>1.7 \\AA means the two structures are different for myoglobin. For pLDDT, the prediction on lactamase is ~66%, which is much lower than 80%-90% which is conventionally believed reliable for folding predictions. The TM-score, if they are smaller than 0.5, then the two structures are believed different.** \n\nAns: We would like to clarify our goal more clearly.  Our task is to design the sequence and structure of proteins with effective functions.  We calculated TM-score by AlphaFold2 as suggested by the reviewer, and the two proteins designed by our model both achieves an average score over 0.5 (myoglobin **0.5912** and beta-lactamase **0.5009**). However, as found by previous method [7],  protein pairs with a TM-score >0.5 are most likely in the same fold while those with a TM-score <0.5 are mainly not in the same fold. Therefore, low TM-score can not indicate the designed protein doesn’t have the corresponding function. As claimed in previous work [9,10], pLDDT between 70~90 are classified to be confident. We calculated pLDDT by AlphaFold2 as suggested by the reviewer and the two protein families designed by our model are confident on an average aspect (myoglobin **91.4139** and beta-lactamase **76.9550**).  Similarly, as observed in previous work[11,12], pLDDT has a weak correlation with protein function. Therefore, a moderate TM-score and pLDDT can not indicate a protein expresses no or poor function. As shown in figure 5(b), the designed myoglobin has a RMSD of 3.943\\AA with the most similar one in PDB, but it can still bind the corresponding metallocofactor heme in the AlphaFill simulation [8]. To systematically illustrate the function of our designed proteins, we use docking method Gnina [13] to compute the binding affinity between the designed metalloproteins and the corresponding metallocofactors. The results in the following table shows that our model achieves the best binding affinity scores on both metalloproteins, demonstrating the proteins designed by our model are highly potential to actively exhibit biochemical functions.\n\n|Model|myoglobin (kcal/mol)|beta-lactamase (kcal/mol)|\n|:-----|:----:|:----:|\n|Hallucination|-7.23|-6.37|\n|Inpainting|-9.32|-7.63|\n|SMCDiff+ProteinMPNN|-8.79|-6.89|\n|PROTSEED|-10.07|-7.68|\n|FrameDiff+ProteinMPNN|-9.54|-7.21|\n|RFDiffusion+ProteinMPNN|-9.76|-7.59|\n|NAEPro|**-10.14**|**-7.80**|\n\n[7] How significant is a protein structure similarity with TM-score= 0.5? Jinrui Xu and Yang Zhang. *Bioinformatics. 2010.* \n\n[8] Alphafill: enriching alphafold models with ligands and cofactors. Maarten L Hekkelman et al. Nature methods. 2023.\n\n[9] pLDDT Values in AlphaFold2 Protein Models Are Unrelated to Globular Protein Local Flexibility. Oliviero Carugo. Crystals. 2023\n\n[10] Generating new protein sequences by using dense network and attention mechanism. Feng Wang et al. Mathematical Biosciences and Engineering. 2023.\n\n[11] Using AlphaFold to predict the impact of single mutations on protein stability and function. Marina A. Pak et al. PloS one. 2023.\n\n[12] Peptide binder design with inverse folding and protein structure prediction. Patrick Bryant et al. Nature Communications Chemistry. 2023.\n\n[13] GNINA 1.0: Molecular docking with deep learning. A McNutt et al. Cheminformatics. 2021.\n\n**Q1 & Q2: \"The use of separate models for sequence and structure cannot ensure the consistency between the generated sequence and structure.\" \"knowing the topology of a protein before design process is difficult and also cannot guarantee the designed proteins have the desired functions.\" I disagree with this statement.** **I disagree with this statement.** \n\nAns: Thanks for the suggestion! We agree with the reviewer and updated the statement in our paper. \n\n**Q3: what is the meaning of \"generally-encoded\" for 20 types of amino acids?** \n\nAns: It means the 20 common amino acids. We have updated the paper accordingly.\n\n**Q4: \"The selection of motif varies from setting to setting...\". Can the authors please provide a clearer explanation?**\n\nAns: We mean the motifs have different meanings in different tasks. For de novo enzyme design [14], it means the binding sites where enzyme binds the metallocofactor and substrate. For de novo binder design [15], it means the binding sites where the protein binder binds the protein targets.\n\n[14] De novo enzyme design using rosetta3. Florian Richter et al. PloS one. 2011.\n\n[15] De novo design of protein interactions with learned surface fingerprints. Pablo Gainza. Nature. 2023.\n\n---\n\nWe thank the reviewer for the insightful questions. We have clarified all your concerns, added the experiments and updated the paper accordingly. Answers to specific points are provided below:\n\n**Weakness 1 & weakness 2 & Q5: The identified problem or the motivation for designing such a method is not supported by existing literature.  Some designed modules counter the intuition in biology, such as \"motif mining\"**\n\nAns: We follow the design insight proposed by [1], which starts from the functional site and fills in additional sequence and structure to create a viable protein scaffold in a single forward pass. Different from [1], we provide the functional sites which are automatically mined from MSA as previous methods [2, 3] while [1] provides the functional sites manually. Besides, we design a new architecture called NAEPro to generate the whole protein sequence and structure based on the given partial protein fragments, while [1] achieves this goal by finetuning the RoseTTAFold. However, we agree with the reviewer’s opinion in question 5 that MSA discovers not only protein functional sites as addressed in [2,3] but also finds conversed sites which may not directly relate to the protein function. Therefore, we change all the term “motif/functional sites” in our paper to  “meaningful protein fragments”.\n\n[1] Scaffolding protein functional sites using deep learning.  Jue wang. et al. Science. 2022.\n\n[2] Emerging methods in protein co-evolution. David de Juan et al. Nature Review Genetics. 2013.\n\n[3] Evolutionary information for specifying a protein fold. Michael Socolich et al. Nature. 2005.\n\n**Weakness 3 & Q1: The method is not 'extensively evaluated' as claimed by the authors (they only selectively analyzed 2 proteins), and many results do not suggest that the proposed method is SOTA among baselines. Moreover, the evaluation does not include all relevant methods, which makes it too aggressive for the authors to claim in the Introduction that their method \"achieves the HIGHEST...among ALL competitors\" and \"is faster than the FASTEST method\". Empirically, It has been proven by several recent research that the de novo method can generate new sequences for fixed backbone or desired functions [4-6].**\n\nAns: Sorry for the confusion. Our method achieves the best performance on 4/5 metrics on myoglobin and 3/5 metrics on beta-lactamase among all the compared representative baselines in our paper. Our model is faster than all baselines compared in our paper. We have revised all these statements in our paper (highlighted in red). For other relevant methods as the reviewer suggested in reference [4-6], we have already compared with [4]. [5] was posted on Oct. 3, 2023, which is later than the ICLR submission ddl, and [3] was posted on Aug. 14, 2023, which is within 3 months before ICLR submission ddl. We appreciate the reviewer’s suggestion, and we tried to compare with [3], but they haven’t released their code now. We emailed the authors of [3] to ask for code and they said their work is under review now and refused to provide code currently. We are trying to reimplement the model now and will update the results in our revised version.\n\n[4] Watson et al., De novo design of protein structure and function with RFdiffusion (2023).\n\n[5] Sumida et al., Improving protein expression, stability, and function with ProteinMPNN (2023).\n\n[6] Zhou et al., Conditional Protein Denoising Diffusion Generates Programmable Endonuclease Sequences (2023).\n\n---\n\nWe thank all the reviewers' valuable suggestions.\n\nWe have uploaded a revised draft that incorporates feedback from reviewers VH6U and MgyU, with the updated portions highlighted in red. Additionally, in response to reviewer Wq6M's suggestion, we have included the code and data we used in our paper in the supplementary material (Due to the maximum memory limitation -100M, we couldn’t upload the model checkpoints).\n\nHere's an overview highlighting the principal modifications to our paper:\n\n1. The revised term from “motif/functional sites” to “meaningful fragments” in abstract, introduction third paragraph, method opening paragraph, method 3.1-3.4, experiments 4.1-4.2, analysis 5.1, Conclusion (Reviewer VH6U, weakness 1 & weakness 2 & Q5)\n2. The corrected claim in abstract and last paragraph in introduction (review VH6U, weakness3) \n3. The corrected claim in Introduction second paragraph (Reviewer VH6U, Q1)\n4. The corrected claim in Related Work second paragraph (Reviewer VH6U, Q2)\n5. The clarified explanation of 20 common amino acids in Method opening paragraph(Reviewer VH6U, Q3)\n6. The corrected theoretical claim in Method 3.5 and Appendix A. 1 (reviewer MgyU, weakness 5) \n7. The clarified task definition in Method opening paragraph (reviewer MgyU, Q2)\n\nWe look forward to any additional reading and feedback. If you have any further questions, we are very happy to continue discussion!", "author_response": "Thanks for the responses! Our answers to your follow-up questions are provided as follows:\n\n**1. About Weakness4 & Q1 Page 2, \"We carry out experiments on two metalloproteins, including β-lactamase and myoglobin.\" I would like to say that the experimental setting is the main concern to me. Since the authors have only evaluated the proposed method on two structures, I am not sure if the method can be extended to generalized situations. I know that you are doing protein sequence-structure inpaintning problem, in which case a comparison with a previous baselines [1,2,3] on stantard datasets may help to reveal the generalization capabilities of your algorithm.** \n\nAns: We agree with reviewer Wq6M and additionally compare our method with PROTSEED on B12 protein. Due to the approaching ddl and limited time, PROTSEED has only been trained for 100 epochs, the same as our method. (suggested training epoch is 2000 on their GitHub)\n\nThe results are reported as follows: \n\n**Binding affinity score for designed B12**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|PROTSEED|-9.45$\\pm$ 0.03|-9.36$\\pm$0.09|-9.04$\\pm$0.27|-8.46|\n|NAEPro|**-11.06$\\pm$0.14**|**-10.96$\\pm$0.14**|**-10.61$\\pm$0.29**|**-9.11**|\n\n**2. About Q2 Your methods are not carefully designed for protein inverse folding. However, the presented results outperformed current SOTA by a large margin (curent SOTA is about 50%-60% AAR), which could not convince me. I doubt the veracity and correctness of the experimental results.** \n\nAns: Sorry for the confusion. We evaluate the inverse-folding task on our own two datasets instead of the original CATH. For our datasets, the proteins are from the same family, and thus they may have much overlap on the sequence level, which definitely will lead to high AAR. Besides, our model is initialized with ESM2 weights, which will also improve the AAR. \n\nAlthough the sequence prediction based a fixed backbone structure is not our objective, **we adapt our method to CATH setting by keeping all the CA coordinates and masking all residues. Due to the approaching ddl and limited time, our model has only been trained for 10 epochs, while usually the suggested training epoch would be 100 like GVP, PiFold, etc.** (See below: ESMIF, ProteinMPNN, PiFold results are quoted from PiFold paper). \n\nCATH 4.2 results\n\n|Model| AAR (%)|PPL |\n|:-----|:----:|:----:|\n|ProteinMPNN|45.96|4.61|\n|ESMIF|38.30|6.44|\n|PiFold|51.66|4.55|\n|NAEPro|9.16|15.26|\n\n**Again, we did not claim the superiority of our method on inverse folding task on CATH dataset.**\n\n**3. Page 1, \"Despite their great potential for novel structure design, such sequential design policy fails to cross-condition on sequence and structure, which might lead to inconsistent proteins and inefficient design process\". I can not agree with this statement. RFDiffusion's authors say that they also considered simultaneously designing structure and sequence within RFdiffusion, but combining ProteinMPNN with the diffusion of structure alone provides the excellent performance, as shown in your Table.1. Unfortunately, I observe that you deleted the consistency metric in Table.1, which seems like dishonest behavior to me.**\n\nAns: We do not intend to hide any information. As suggested by reviewer MgyU, the binding affinity is a better metric for functional protein design rather than the accuracy metrics (consistency, AAR, RMSD, etc.). Therefore, we replaced all the accuracy metrics in Table 1 and 2 in our paper according with the binding affinity scores calculated by Gnina. The full table with all scores are listed below:\n\n$\\beta$-lactamase\n\n|Model| AAR (\\%,$\\uparrow$) | RMSD ({ \\AA},$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination | $4.79$ | $--$ | $30.5511$ | $0.2918$ | $--$|\n|Inpainting | $16.73$ | $4.0599$ | $61.7679$ | $0.3790$ | $6.2578$|\n|SMCDiff+ProteinMPNN| $19.94$ | $10.3960$ | $42.0375$ | $0.3458$ |$10.2117$|\n| PROTSEED | $37.63$ | $3.0142$ | $64.3861$| $0.4637$ | $3.3748$ |\n| FrameDiff+IF |$26.20$ | $6.0151$ | $65.6445$ | $0.3657$ | $7.8703$|\n|RFDiffusion+IF | $22.93$ | $6.0438$ | **83.4058** | $0.3747$ |**0.5565**|\n|NAEPro|**65.71**| **2.9916**| 66.8705|**0.4812**| 7.3760|\n\n\n**Myoglobin**\n|Model| AAR (\\%,$\\uparrow$) | RMSD (\\AA,$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination| $4.81$ | $--$ | $38.2817$ | $0.2754$ |--|\n|Inpainting |39.59|3.3751|67.0813|0.4391 |3.2108|\n|SMCDiff+IF | $12.47$ | $8.0067$ | $34.5914$ | $0.2235$ | $8.8754$ |\n| PROTSEED | $48.55$ | $2.8753$ | $61.5588$ | $0.5466$ | $0.9764$ |\n| FrameDiff+IF | $20.43$ | $5.9739$ | $61.3945$ | $0.3757$ |$3.5078$|\n|RFDiffusion+IF | $32.70$ | $3.9930$ | $78.9868$ | $0.4147$  | **0.4375** |\n|NAEPro | **89.37**| **2.6307**|**81.9507**|**0.5692**|4.3865|"}
{"claim": "Many existing works on antibody structure and sequence co-design reduce the novelty of this approach.", "claim_type": "novelty", "paper_id": "Dr4qD9bzZd", "paper_title": "Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "a9hHYfEm0G", "reviewer": "Reviewer_Wq6M", "review_text": "Summary: This paper proposes NAEPro, a model to jointly design Protein sequence and structure. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from the nearest amino acids in three-dimensional (3D) space. The global attention sub-layer parameters are initialized with ESM-2. The author combines ESM2 and EGNN for co-modeling protein sequence and structure.\n\nStrengths: 1. The reported performance is good.\n2. The method is simple.\n\nWeaknesses: 1. Novelty: Both EGNN and ESM2 are existing models, and there are many existing works on antibody structure and sequence co-design. The combination may limit the novelty of this method.  \n2. Significance: It is likely that previous works can be readily applied to the motif-conditioned setting, and the authors can easily adapt their method to antibody design tasks. It would be beneficial if the authors further elaborate on the significance of their work\n3. Code: The authors do not provide code for checking the soundness of the methods.\n4. Experiment setting: The authors do not provide results on standard benchmarks, such as the CATH dataset, for fair comparison on both sequence and structure design.\n\nQuestions: 1. Could you provide experimental results from the CATH dataset to compare with the original SMCDiff and FrameDiff results?\n2. Similarly, could you provide head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design? Please follow the same setting.\n3. Could you provide the code for checking the results?", "labeling_timestamp": "2026-01-11T16:28:45.232754", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The authors explicitly dispute the reviewer's claim, acknowledging some reused components but arguing their overall architecture is novel (interleaving sequence-level attention with local equivariant sub-layers and kNN acceleration) and providing empirical comparisons to show it outperforms straightforward combinations of existing models.", "evidence": "Yes, from machine learning algorithm level, our method is not new. However, as admitted by reviewer MgyU, we design a new architecture for protein design. The key innovations of our architecture include: (1) interleaving layers of sequence-level attention and local neighborhood equivariant sub-layer (2) accelerating the local sub-layer with the k-nearest neighbors.\n\nWe would like to point out that our NAEPro is not a combination of ESM2 and EGNN. Instead our key innovations are (1) interleaving layers of sequence-level attention and local neighborhood equivariant sub-layer (2) accelerating the local sub-layer with the k-nearest neighbors.  On the contrary,  EGNN is a fully-connected graph and updates message, coordinates and atom features based on all other atoms in 3D space, which is not efficient for long proteins. Besides, EGNN+ESM2 is a sequential process and can not cross-condition on sequence and structure. We compare our method with EGNN+ESM2, NAEPro w/o ESM2 initialization on myoglobin in the Following Table (Table 2 in the paper).  As the results show, EGNN+ESM2 performs much worse than our NAEPro, and removing the ESM2 initialization will not influence the performance too much, demonstrating that our model is much more superior than EGNN+ESM2.", "combined_author_response": "We much appreciate your valuable comments and insightful suggestions again, which helps a lot to improve the quality of our paper. As the discussion deadline is approaching, please let us know if you have any further concerns or questions. We are happy to have any further discussion!\n\n---\n\n**4. \"Through this way, amino-acid combinations frequently occur in the same context would draw higher attention scores\". Have you checked this statement? Could you provide some visualizations of the attention map and correlated frequences of amino-acid combinations?**  \n\nAns: Thank the reviewer for this suggestion. We randomly pick one sentence in the training set and visualization its attention matrix from the last layer in Appendix Figure 7(a) and also visualize the pairwise amino acid co-occurrence matrix in Figure 7(b). We find it’s consistent with our statement that residues tend to more connect to their neighboring residues. However, we don’t see any strong correlation between the attention matrix and the pairwise amino acid co-occurrence. Therefore, we revised the corresponding statement in our paper and uploaded a revised version accordingly.\n\n**5. Updating residue representations and coordinates in 3D space with only nearest neighbors enables more efficient and economic message passing compared to prior approaches which compute messages on the complete pairwise residue graph\". To my knowledge, most of the previous methods such as proteinmpnn adopt knn for constructing sparse graph in the 3D spcace. You should provide evidence to support the significance of your statement.**\n\nAns: We mean from the architecture level, previous methods like SE(3)-Transformer [1] and EGNN [2] adopts information flow from all other atoms.\n\n[1] SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks. Fabian B. Fuchs et al. NeurIPS 2020. \n\n[2] E(n) Equivariant Graph Neural Networks. Victor Garcia Satorras et al. ICML 2021. \n\n**6. Novelty: I do not observe enough novelty from the perspective of machine learning in algorithm design.**\n\nAns: Yes, from machine learning algorithm level, our method is not new. However, as admitted by reviewer MgyU, we design a new architecture for protein design. The key innovations of our architecture include: (1) interleaving layers of sequence-level attention and local neighborhood equivariant sub-layer (2) accelerating the local sub-layer with the k-nearest neighbors.\n\n---\n\nThanks for the responses! Our answers to your follow-up questions are provided as follows:\n\n**1. About Weakness4 & Q1 Page 2, \"We carry out experiments on two metalloproteins, including β-lactamase and myoglobin.\" I would like to say that the experimental setting is the main concern to me. Since the authors have only evaluated the proposed method on two structures, I am not sure if the method can be extended to generalized situations. I know that you are doing protein sequence-structure inpaintning problem, in which case a comparison with a previous baselines [1,2,3] on stantard datasets may help to reveal the generalization capabilities of your algorithm.** \n\nAns: We agree with reviewer Wq6M and additionally compare our method with PROTSEED on B12 protein. Due to the approaching ddl and limited time, PROTSEED has only been trained for 100 epochs, the same as our method. (suggested training epoch is 2000 on their GitHub)\n\nThe results are reported as follows: \n\n**Binding affinity score for designed B12**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|PROTSEED|-9.45$\\pm$ 0.03|-9.36$\\pm$0.09|-9.04$\\pm$0.27|-8.46|\n|NAEPro|**-11.06$\\pm$0.14**|**-10.96$\\pm$0.14**|**-10.61$\\pm$0.29**|**-9.11**|\n\n**2. About Q2 Your methods are not carefully designed for protein inverse folding. However, the presented results outperformed current SOTA by a large margin (curent SOTA is about 50%-60% AAR), which could not convince me. I doubt the veracity and correctness of the experimental results.** \n\nAns: Sorry for the confusion. We evaluate the inverse-folding task on our own two datasets instead of the original CATH. For our datasets, the proteins are from the same family, and thus they may have much overlap on the sequence level, which definitely will lead to high AAR. Besides, our model is initialized with ESM2 weights, which will also improve the AAR. \n\nAlthough the sequence prediction based a fixed backbone structure is not our objective, **we adapt our method to CATH setting by keeping all the CA coordinates and masking all residues. Due to the approaching ddl and limited time, our model has only been trained for 10 epochs, while usually the suggested training epoch would be 100 like GVP, PiFold, etc.** (See below: ESMIF, ProteinMPNN, PiFold results are quoted from PiFold paper). \n\nCATH 4.2 results\n\n|Model| AAR (%)|PPL |\n|:-----|:----:|:----:|\n|ProteinMPNN|45.96|4.61|\n|ESMIF|38.30|6.44|\n|PiFold|51.66|4.55|\n|NAEPro|9.16|15.26|\n\n**Again, we did not claim the superiority of our method on inverse folding task on CATH dataset.**\n\n**3. Page 1, \"Despite their great potential for novel structure design, such sequential design policy fails to cross-condition on sequence and structure, which might lead to inconsistent proteins and inefficient design process\". I can not agree with this statement. RFDiffusion's authors say that they also considered simultaneously designing structure and sequence within RFdiffusion, but combining ProteinMPNN with the diffusion of structure alone provides the excellent performance, as shown in your Table.1. Unfortunately, I observe that you deleted the consistency metric in Table.1, which seems like dishonest behavior to me.**\n\nAns: We do not intend to hide any information. As suggested by reviewer MgyU, the binding affinity is a better metric for functional protein design rather than the accuracy metrics (consistency, AAR, RMSD, etc.). Therefore, we replaced all the accuracy metrics in Table 1 and 2 in our paper according with the binding affinity scores calculated by Gnina. The full table with all scores are listed below:\n\n$\\beta$-lactamase\n\n|Model| AAR (\\%,$\\uparrow$) | RMSD ({ \\AA},$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination | $4.79$ | $--$ | $30.5511$ | $0.2918$ | $--$|\n|Inpainting | $16.73$ | $4.0599$ | $61.7679$ | $0.3790$ | $6.2578$|\n|SMCDiff+ProteinMPNN| $19.94$ | $10.3960$ | $42.0375$ | $0.3458$ |$10.2117$|\n| PROTSEED | $37.63$ | $3.0142$ | $64.3861$| $0.4637$ | $3.3748$ |\n| FrameDiff+IF |$26.20$ | $6.0151$ | $65.6445$ | $0.3657$ | $7.8703$|\n|RFDiffusion+IF | $22.93$ | $6.0438$ | **83.4058** | $0.3747$ |**0.5565**|\n|NAEPro|**65.71**| **2.9916**| 66.8705|**0.4812**| 7.3760|\n\n\n**Myoglobin**\n|Model| AAR (\\%,$\\uparrow$) | RMSD (\\AA,$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination| $4.81$ | $--$ | $38.2817$ | $0.2754$ |--|\n|Inpainting |39.59|3.3751|67.0813|0.4391 |3.2108|\n|SMCDiff+IF | $12.47$ | $8.0067$ | $34.5914$ | $0.2235$ | $8.8754$ |\n| PROTSEED | $48.55$ | $2.8753$ | $61.5588$ | $0.5466$ | $0.9764$ |\n| FrameDiff+IF | $20.43$ | $5.9739$ | $61.3945$ | $0.3757$ |$3.5078$|\n|RFDiffusion+IF | $32.70$ | $3.9930$ | $78.9868$ | $0.4147$  | **0.4375** |\n|NAEPro | **89.37**| **2.6307**|**81.9507**|**0.5692**|4.3865|\n\n---\n\nThanks for the further response! Please see our responses to your follow-up questions and concerns as follows:\n\n**Response to Weakness 3 & Q1: My point in mentioning these three papers is not to introduce additional (and possibly meaningless) in-silico comparisons to the authors. Instead, they validated their designs with solid wet-lab experiments demonstrating that it is possible to generate novel protein sequences with fixed backbones.**\n\nAns: We agree on the reviewer’s opinion and have accordingly updated the statement in Introduction.  We have uploaded a revised version of our paper.\n\n**Response to Weakness 4: I completely agree that all the existing evaluation metrics we used in deep learning are only indirect indicators, and they only imply a higher chance that the assessed protein will perform its function. However, I have trouble following the logic that \"*Therefore, a low TM-score cannot indicate the designed protein doesn’t have the corresponding function*\" suggests \"proteins with TM-scores that are lower than 0.5 (which is a wildly-used standard) are still reliable in terms of their functionality in general.\" Similarly, if we all agree that the existing metrics are suboptimal and none of them provides 100% accurate evaluations, it is then meaningless to criticize the widely applied standards with one or two counter-examples. After all, you don't have any better evidence (such as wet-lab results) that is more reliable than the existing criteria.**\n\nAns: We agree with Reviewer VH6U. As suggested by reviewer MgyU, the binding affinity is a better metric for functional protein design. We calculated the binding affinity scores for top-5, top-10, and top-30 candidates and also median score on the test set using Gnina. The results are reported as follows:\n\n**Binding affinity score for designed myoglobin**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-8.18$\\pm$ 0.01|-8.07$\\pm$0.03|-7.97$\\pm$0.23|-7.25|\n|Inpainting|-13.47$\\pm$0.02|-13.12$\\pm$0.12|-12.31$\\pm$0.54|-9.56|\n|SMCDiff+ProteinMPNN|-11.37$\\pm$0.03|-11.12$\\pm$0.31|-10.87$\\pm$0.42|-8.76|\n|PROTSEED|-13.21$\\pm$0.13|-12.89$\\pm$0.42|-11.98$\\pm$0.52|-10.23|\n|FrameDiff+ProteinMPNN|-13.13$\\pm$0.05|-12.92$\\pm$0.16|-12.21$\\pm$0.23|-10.08|\n|RFDiffusion+ProteinMPNN|-13.68$\\pm$0.02|-13.03$\\pm$0.21|-12.56$\\pm$0.43|-10.15|\n|NAEPro|**-14.12$\\pm$0.01**|**-13.85$\\pm$0.10**|**-13.06$\\pm$0.38**|**-10.74**|\n\n**Binding affinity score for designed beta-lactamase**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-6.98$\\pm$0.01|-6.87$\\pm$0.02|-6.69$\\pm$0.05|-6.29|\n|Inpainting|-9.89$\\pm$0.03|-9.54$\\pm$0.16|-9.13$\\pm$0.43|-7.24|\n|SMCDiff+ProteinMPNN|-9.10$\\pm$0.01|-9.05 $\\pm$0.02|-8.98$\\pm$0.01|-6.97|\n|PROTSEED|-9.88$\\pm$0.21|-9.51$\\pm$0.41|-9.01$\\pm$0.62|-7.31|\n|FrameDiff+ProteinMPNN|-9.54$\\pm$0.03|-9.56$\\pm$0.23|-8.89$\\pm$0.35|-7.03|\n|RFDiffusion+ProteinMPNN|-9.87$\\pm$0.05|-9.56$\\pm$0.23|-9.12$\\pm$0.53|-7.51|\n|NAEPro|**-10.06$\\pm$0.05**|**-9.79$\\pm$0.10**|**-9.39$\\pm$0.12**|**-7.66**|\n\n**Response to Q6: Because ESMFold is a less reliable tool for evaluating novel proteins, it should not be used here to assess your designs at all.**\n\nAns: We totally agree with reviewer VH6U. We provide the mean pLDDT for top-5, top-10, top-30 candidates calculated by AlphaFold2 as follows:\n\n**AlphaFold2 pLDDT for designed myoglobin**\n\n|Model| top-5|top-10|top-30|\n|:-----|:----:|:----:|:----:|\n|Hallucination|76.7243|73.5953|68.6060|\n|Inpainting|97.6171|97.6063|97.5681|\n|SMCDiff+ProteinMPNN|85.7051|84.7744|84.9220|\n|PROTSEED|92.9770|91.9216|90.4293|\n|FrameDiff+ProteinMPNN|97.3997|97.3329|96.4279|\n|RFDiffusion+ProteinMPNN|97.9722|97.9174|97.6063|\n|NAEPro|**98.1057**|**98.0706**|**97.9197**|\n\n**AlphaFold2 pLDDT for designed $\\beta$-lactamase**\n\n|Model| top-5|top-10|top-30|\n|:-----|:----:|:----:|:----:|\n|Hallucination|74.7760|74.7032|71.2932|\n|Inpainting|94.5489|94.4328|93.2941|\n|SMCDiff+ProteinMPNN|86.2510|85.7072|83.0376|\n|PROTSEED|98.2896|98.2740|97.9683|\n|FrameDiff+ProteinMPNN|98.3049|98.2922|98.1267|\n|RFDiffusion+ProteinMPNN|**98.6225**|**98.5116**|98.0643|\n|NAEPro|98.5385|98.4481|**98.2841**|\n\n---\n\nDear reviewer,\n\n    We sincerely appreciate the time and effort you dedicated to reviewing our paper and reading our responses. Your valuable suggestions and insightful comments have significantly contributed to refining the quality of our work. No matter what the final result will be, the thoughtful communication with you has better clarified our paper's goals and logic. We would like to express our great gratitude to you!\n\nBest,\n\nAuthors of paper 4608\n\n---\n\n**Could the authors briefly explain how do you construct the one-to-one mapping between the design structure and the target structure in PDB? Is it determined by retrieving the one in PDB with the smallest RMSD? As far as I know, there is no common practice to determine this for the task of protein design, and I appreciate any explanation that clues me in.**\n\nAns: Yes, we construct the one-to-one mapping between the designed structure and the target structure in PDB. It is not determined by retrieving the one in PDB with the smallest RMSD. We extract the motif fragments for each protein, based on which our training objective is to recover the original protein sequence and backbone structure. Therefore, we compute the RMSD between the designed structure and the target one which we want to recover.\n\n---\n\n**Corrected statement: But it can still bind the corresponding metallocofactor heme in the AlphaFill simulation. For the claim “binding rates of 100% and 90%”, I wonder how is the criterion/threshold of “binding” determined? This success rate is rather unrealistic and probably fails to reflect the real performance. Thus, the statement in Section 5.2. saying “It is evident that our method can generate proteins that express the basic and important functions.” can be quite misleading for readers who are not familiar with the task.** \n\nAns: Sorry for the misleading statement. We mean through the AlphaFill prediction, the designed proteins are highly potential to bind the corresponding metallocofactors. We have corrected all the claims in our paper. We provided the process of how to decide if the designed protein can “bind” the metallocofactors in Appendix B.3. We would like to further explain it here.  First of all, we use AlphaFill to do the first-step prediction. AlphaFill will return a complex based on the input protein structure. If the complex includes the corresponding metal ions, we think the designed protein is potential to have functions. Then to guarantee the results convincing, we further use some additional constraints to ensure the designed proteins are potential to express functions. Specifically, we assume if a designed protein share the similar active site environments with their natural counterparts (target protein in our dataset), then it’s highly potential to bind the corresponding metal ions. For myoglobin, we calculate the distance between axial histidine ligand to Fe ion. Besides, we also detect the presence of distal histidine within the active sites, which plays an important role in the oxygen molecule binding function of myoglobin . If the distance is between 2.0Å and 2.5Å, and the distal histidine exists, we think the designed myoglobins share similar active site environments to natural ones. For $beta$-lactamase, we detect the residues that directly contact with zinc ion. If they show similar chemistry properties with natural $beta$-lactamases, we think they can highly potentially bind the corresponding metallocofactors. For example, in Figure 5(a), the active site possesses one zinc atom coordinated to three histidines, while the second zinc atom is coordinated to one histidine, one cysteine, and one aspartate, demonstrating a high degree of amino acid analogy to the natural protein.\n\n**Could the author intuitively describe how does the NAEL deal with the reflection case so as to be simply SE(3)-equivariant instead of E(3)?** \n\nAns: Intuitively, applying reflection to the input structure, the output structure will also be reflected. The conformation of the original backbone will be changed from L-amino acid to D-amino acid (We provide an illustration in Figure 6 in appendix). Therefore, the reflection will change the chirality of the protein which may cause the deficiency of binding to the ligand and eventually its function. However, the protein sequence in our model will not be influenced. Therefore, the 3D structure of the designed sequence which is L-amino acid and the output structure  which becomes D-amino acid will be inconsistent. From this aspect, our model doesn’t satisfy reflection equivariance.\n\n**The authors are encouraged to encompass appropriate comparison with EGNN/SE(3)-transformer in their writing to better contextualize the proposed NAEL.**\n\nAns: We provide the comparison with EGNN+ESM2 on myoglobin in the following Table and also updated the results in Table2 accordingly.\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|EGNN+ESM2|-13.30$\\pm$0.61 | -12.77$\\pm$0.69 |-12.07$\\pm$0.65 | -9.63|\n|NAEPro|**-14.12$\\pm$0.01**|**-13.85$\\pm$0.10**|**-13.06$\\pm$0.38**|**-10.74**|\n\n**It is surprising to see scaling the model does not yield consistent performance gain. Could the authors help explain this behavior? Also, I noticed that the authors merge the two myoglobin and lactamase data for training newly in the rebuttal response. Is there any insight for doing such multi-task learning?**\n\nAns: Enlarging the model hasn’t improved the performance too much. Our interpretation is that the training data size is moderate, and thus a model with moderate size can fit the data well. To validate if this guess holds, we merge the two protein data and then train a unified model. As we can see in the previous table, our model did achieve obvious better results on $\\beta$-lactamase, i.e., TM-score from below 0.5 (0.4919) to over 0.5 (0.5055).\n\n---\n\nWe much appreciate the reviewer’s responses and suggestions, which help a lot to improve our paper.  We have accordingly revised our paper, including Table 1, Table 2, Section 4.2-4.4 and Section 5.2. Our responses to the reviewer’s follow-up questions are provided as follows:\n\n**About the evaluation metrics: I think the binding affinity could better support the effectiveness of NAEPro and be more suitable for the task (functional protein design) rather than the accuracy metrics (RMSD, AAR, etc.). Do the authors agree on this or not?** C**ould the authors explain how the Binding Affinity is calculated for both model based on Gnina? Specifically, I am worried that the metrics is averaged among the test set similar to other metrics.  If so, I suggest that the authors report median or top-k mean as evaluation.**\n\nAns: We completely agree with reviewer MgyU that the binding affinity is a better metric for functional protein design rather than the accuracy metrics. We revised Table 1 and 2 in our paper according to your suggestion. The binding affinity is calculated using Gnina as the average score on the test set. Thank the reviewer for this valuable suggestion. We provide  top-5, top-10, top-30 mean and variance as well as the median in the following table and also revise our paper accordingly. From the results, we can see that our model achieves the best median and top-K binding affinity scores on both datasets.\n\n**Myoglobin**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-8.18$\\pm$ 0.01|-8.07$\\pm$0.03|-7.97$\\pm$0.23|-7.25|\n|Inpainting|-13.47$\\pm$0.02|-13.12$\\pm$0.12|-12.31$\\pm$0.54|-9.56|\n|SMCDiff+ProteinMPNN|-11.37$\\pm$0.03|-11.12$\\pm$0.31|-10.87$\\pm$0.42|-8.76|\n|PROTSEED|-13.21$\\pm$0.13|-12.89$\\pm$0.42|-11.98$\\pm$0.52|-10.23|\n|FrameDiff+ProteinMPNN|-13.13$\\pm$0.05|-12.92$\\pm$0.16|-12.21$\\pm$0.23|-10.08|\n|RFDiffusion+ProteinMPNN|-13.68$\\pm$0.02|-13.03$\\pm$0.21|-12.56$\\pm$0.43|-10.15|\n|NAEPro|**-14.12$\\pm$0.01**|**-13.85$\\pm$0.10**|**-13.06$\\pm$0.38**|**-10.74**|\n\n**Beta-lactamase**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-6.98$\\pm$0.01|-6.87$\\pm$0.02|-6.69$\\pm$0.05|-6.29|\n|Inpainting|-9.89$\\pm$0.03|-9.54$\\pm$0.16|-9.13$\\pm$0.43|-7.24|\n|SMCDiff+ProteinMPNN|-9.10$\\pm$0.01|-9.05 $\\pm$0.02|-8.98$\\pm$0.01|-6.97|\n|PROTSEED|-9.88$\\pm$0.21|-9.51$\\pm$0.41|-9.01$\\pm$0.62|-7.31|\n|FrameDiff+ProteinMPNN|-9.54$\\pm$0.03|-9.56$\\pm$0.23|-8.89$\\pm$0.35|-7.03|\n|RFDiffusion+ProteinMPNN|-9.87$\\pm$0.05|-9.56$\\pm$0.23|-9.12$\\pm$0.53|-7.51|\n|NAEPro|**-10.06$\\pm$0.05**|**-9.79$\\pm$0.10**|**-9.39$\\pm$0.12**|**-7.66**|\n\n**Corrected statement: ESMFold might not accurately predict the structure for a novel sequence.**\n\nAns: Sorry for the misunderstood claim. We want to express we agree on reviewer VH6U’s comment in question 6 “for a novel sequence, its prediction is worse than AlphaFold2.”  As suggested by reviewer VH6U, we re-calculate pLDDT by AlphaFold2. Our model respectively achieves 91.4139 and 76.9550  on myoglobin and $\\beta$-lactamase on average. As claimed in previous work [1,2], pLDDT between 70~90 are classified to be confident and pLDDT ≥ 90 indicates residues predicted with extremely high confidence.\n\n[1] pLDDT Values in AlphaFold2 Protein Models Are Unrelated to Globular Protein Local Flexibility. Oliviero Carugo. Crystals. 2023.\n\n[2] Generating new protein sequences by using dense network and attention mechanism. Feng Wang et al. Mathematical Biosciences and Engineering. 2023.\n\n---\n\nThanks for your valuable review and comments again. We have addressed all your concerns and questions.\n\nIf you have any other questions, please feel free to discuss with us.\n\n---\n\nThanks for your valuable review and comments again. We have addressed all your concerns and questions.\n\nIf you have any other questions, please feel free to discuss with us.\n\n---\n\nThanks for your valuable review and comments again. We have addressed all your concerns and questions.\n\nIf you have any other questions, please feel free to discuss with us.\n\n---\n\n**Q3: In section 4.2, the author(s) described that the weights of NAEPro is partially initialized from pretrained ESM2-8M, of the smallest model size among the ESM2 series. This is very problematic but the recent common practices leverage at least the 650M model. Do the author(s) try much larger model size as initialization? I found in the Table 2 that the 8M model has only marginal improvement upon random initialization.**\n\nAns: Yes, we tried to partially initialize our model with 35M ESM2 (NAEPro-12) and also train this model on the merged myoglobin and beta-lactamase data (NAEPro-12-Unified). For myoglobin, enlarging the model to 12 layers will not influence the performance too much, but training the larger model on the merged dataset will increase the pLDDT and TM-score. For beta-lactamase, enlarging the model from 6 layers to 12 layers is beneficial and will increase the overall performance. Particularly, training the 12-layer model on merged dataset will significantly improve the performance of beta-lactamase, say improve the average TM-score from below 0.5 to above 0.5. Then we also partially initialize a 30-layer model with 150M ESM2 (NAEPro-30-Unified) and train the model on merged dataset. The performance for both myoglobin and beta-lactamase will be further improved.\n\n**myoglobin**\n\n|Model| AAR (%)|RMSD|pLDDT |TM-score |Parameters|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|NAEPro-6 |**89.37**|**2.6307**|81.9507|0.5692|11.6M|\n|NAEPro-12 |88.42|2.6333|82.2849|0.5673|52M|\n|NAEPro-12-Unified|86.81|2.6443|82.9736|0.5736|52M|\n|NAEPro-30-Unified|86.62|2.6410|**83.0173**|**0.5749**|223.7M|\n\n**$\\beta$-lactamase**\n\n|Model| AAR (%)|RMSD|pLDDT |TM-score |Parameters|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|NAEPro-6 |65.71|2.9916|66.8705|0.4812|11.6M|\n|NAEPro-12 |67.28|2.9864|68.7394|0.4919|52M|\n|NAEPro-12-Unified|70.93|**2.9707**|69.9333|0.5055|52M|\n|NAEPro-30-Unified|**72.60**|2.9781|**71.0429**|**0.5089**|223.7M|\n\n**Q4: How is the “target structure” in evaluation metrics(Section 4.2) determined? What is the difference between metrics RMSD and consistency?**\n\nAns: Target structure means the natural protein structure provided in PDB. RMSD is calculated as the RMSD between the designed protein structure and the one given in PDB. Consistency is calculated as the RMSD between the designed structure and the one predicted by ESMFold for the designed sequence.\n\nWe really hope our responses address your concerns. If you have any other questions, we are very happy to continue discussions!\n\n---\n\n**Weakness 4: For the evaluation metrics, the problem to be studied is functional protein (as it is claimed in the title) design instead of general backbone generation(eg., RFDiffusion) or sequence generation (eg., ProteinMPNN). Thus, I found the the five of the metrics fail to (or indirectly if they potentially do) reflect the performance of functional protein design or scaffolding. All these metrics in Table 1 & 2 show the somehow sequence-structure matching between the predicted and native, but are not very suitable for proteins such as enzyme.**\n\nAns: In our analysis (Section 5.2), we evaluated the function for binding the corresponding metallocofactors of the designed proteins. Specifically, we first randomly select 20 cases from the top100 sequences (pLDDT ranking). We then employ AlphaFold2 for protein structure prediction, followed by inputting these structures into AlphaFill [1] to predict the associated ligands. Notably, our results indicate that all designed myoglobins exhibit heme binding capability, while 18 beta-lactamases demonstrate the ability to bind zinc ions, resulting in metallocofactor binding rates of 100% and 90%, respectively. It is evident that our method can generate proteins that express the basic and important functions, i.e. binding the corresponding metallocofactors. To systematically illustrate the function of our designed proteins, we use docking tool Gnina[2] to compute the binding affinity between the designed metalloproteins and the corresponding metallocofactors. The results in the following table shows that our model achieves the best binding affinity scores on both metalloproteins, demonstrating the proteins designed by our model are highly potential to actively exhibit biochemical functions.\n\n|Model| myoglobin (kcal/mol)|beta-lactamase (kcal/mol)|\n|:-----|:----:|:----:|\n|Hallucination|-7.23|-6.37|\n|Inpainting|-9.32|-7.63|\n|SMCDiff+ProteinMPNN|-8.79|-6.89|\n|PROTSEED|-10.07|-7.68|\n|FrameDiff+ProteinMPNN|-9.54|-7.21|\n|RFDiffusion+ProteinMPNN|-9.76|-7.59|\n|NAEPro|**-10.14**|**-7.80**|\n\n**Weakness 5: The potential theoretical flaw in the equivariance analysis in Section 3.5.** \n\nAns: Thank the reviewer for pointing out this typo. Our model satisfies only roto-translation equivariance without reflection equivariance. We have accordingly updated the paper.\n\n**Q1: The proposed architecture encoding both sequence and structure via the masked language modeling (MLM) scheme is somehow interesting. How do the author(s) see the  difference between the proposed NAEL layer and tensor field convolutional layer in SE(3)-transformer ? Also, what is the advantage of NAEL over the SeqIPA of the PROTSEED, the most competitive baseline in Table 1?** \n\nAns: The most biggest difference between our NAEPro and tensor field convolutional layer in SE(3)-transformer is:  NAEPro uses interleaving layers of global sequence-level attention and local neighborhood equivariant sub-layer, from which the residue representation is updated from both the whole sequence interaction and 3D neighbor message passing; instead, if we enable kNN message passing in SE(3)-transformer, then the node feature will only be updated based on 3D neighboring information. To clearly demonstrate the advantage of NAEL over SeqIPA in PROTSEED, we replace our NAEL with the SeqIPA-Addition without secondary structure feature. Taking myoglobin as an example, the results are shown in the following table. NAEPro with NAEL performs better than with SeqIPA.\n\n|Model| AAR (%)|RMSD|\n|:-----|:----:|:----:|\n|NAEPro -w/- SeqIPA|65.33|2.7078|\n|NAEPro|**89.37**|**2.6307**|\n\n**Q2:  In Section 3 - opening paragraph, the author(s) define/formulate the target task as generate a protein sequence and all 3D coordinates of N residues. However, in later model definition, the proposed NAEPro only operates on the C-alpha(CA) coordinates. Please explain this ambiguity.**\n\nAns: We are sorry for the ambiguity caused. When we mention “we formulate the target task as generate a protein sequence and all 3D coordinates of N residues”, we mean 3D coordinates of alpha-carbon of all the residues. Following the setting in [3], we represent the backbone structure as the CA-only coordinates of all residues. We have updated the paper accordingly in the new version.\n\n[3] Diffusion probabilistic modeling of protein backbones in 3D for the motif-scaffolding problem. Brian L. Trippe et al. ICLR. 2023.\n\n---\n\nWe appreciate the reviewer’s valuable and insightful suggestions, which are very helpful for us to improve our paper. We have clarified all your concerns, added the experiments and updated the paper accordingly. We address the specific concerns as follows:\n\n **Weakness 1: Regarding the consistency metrics reported, I noticed that the consistency metric of NAEPro in Table 1 (as the main results) underperformed half of the baselines.  Such key observation greatly weakened the method. The authors can elaborate on this point.**\n\nAns: Yes, we agree with the reviewer’s opinion. We would like to clarify our task more clearly. Our goal is to design functional proteins with novel sequence and structure. As shown in Figure 5(b), the designed myoglobin has a RMSD of 3.943\\AA with the most similar one in PDB and 26.7% AAR with the most similar one in Uniprot, but it can still bind the corresponding metallocofactor heme in the AlphaFill simulation [1]. In our paper, we use ESMFold to predict the structure of the designed sequence. However, as pointed out by reviewer VH6U, ESMFold might not accurately predict the structure for a novel sequence. To further clarify this point, we calculate the scRMSD of motifs between the designed structure and the predicted structure from the designed sequence in the following table. As the results show, our NAEPro achieves smaller RMSD than RFDiffusion+ProteinMPNN on myoglobin and comparable results on beta-lactamase.\n\n|Model| myoglobin|beta-lactamase|\n|:-----|:----:|:----:|\n|Inpainting|0.5292|0.7819|\n|SMCDiff+ProteinMPNN|1.3816|1.4523|\n|PROTSEED|0.4987|**0.3892**|\n|FrameDiff+ProteinMPNN|0.8765|0.9815|\n|RFDiffusion+ProteinMPNN|0.4936|0.3918|\n|NAEPro|**0.4832**|0.5239|\n\n[1] Alphafill: enriching alphafold models with ligands and cofactors. Maarten L Hekkelman et al. Nature methods. 2023.\n\n**Weakness 2: For the experiments, how is the model trained to obtain the evaluation digits in Table 1 for each baseline. This is very important.** \n\nAns: We are sorry for the confusion. We got the codes of all baselines from their GitHub repos, and then re-trained all the models separately on myoglobin and beta-lactamase datasets. The evaluation procedure among all the baselines and our model follows the same process, and thus we believe the comparisons in Table 1 are fair.\n\n**Weakness 3: For the dataset, I found that in section 4.1, the author(s) mentioned that both beta-lactamase and myoglobin datasets are split randomly. This is worrying because the entries in PDB can usually be redundant, especially for such large and well-studied families that the authors selected for evaluation. The author(s) can elaborate on this point. Moreover, in the Table 3 (in appendix), I am curious about the column name “PDB”, does it mean the number of PDB entries (with unique ID) or the number of single chains?** \n\nAns: Yes, this is a very good question. Since we focus on sequence and structure co-design task, we aim to design both functional sequence and structure. Therefore, our input data is the <sequence, structure> pair. From this point, even though the sequence might be the same, the structure is somehow different. Therefore, our paired input data are different and would not be redundant. We appreciate the reviewer for carefully reviewing our paper and also reading the appendix. In Table 3, PDB means the number of different PDB entries. To eliminate the reviewer’s concerns, we re-cleaned the test set by filtering pairs whose sequences have over 30% sequence identity to any sample in the training set after doing pairwise alignment for myoglobin. We compare our NAEPro and ProtSeed (the most competitive baseline) in the following table. As the results show, our model can design proteins with higher binding affinity (by Gnina [2]) on samples that have lower similarity to training data.\n\n|Model| RMSD|Binding Affinity (kcal/mol)|\n|:-----|:----:|:----:|\n|PROTSEED|**2.4031**|-9.62|\n|NAEPro|2.6634|**-9.98**|\n\n[2] GNINA 1.0: Molecular docking with deep learning. A McNutt et al. Cheminformatics. 2021.\n\n---\n\n**Weakness4 & Q1: The authors do not provide results on standard benchmarks, such as the CATH dataset, for fair comparison on both sequence and structure design. Could you provide experimental results from the CATH dataset to compare with the original SMCDiff and FrameDiff results?**\n\nAns: We would like to clarify our task more clearly. Our task aims to co-design functional and novel protein sequence and structure. In our method, we use MSAs to automatically find motifs to guarantee the protein function, which is achieved within the same protein family. Instead, CATH is a protein dataset consisting of proteins from diverse families, making the motif extraction difficult. Besides, CATH is usually used to evaluate inverse folding task, say protein sequence design based on fixed backbone structure, which is not the goal of our paper. We hope we didn’t misunderstand the reviewer’s meaning by “the original SMCDiff and FrameDiff results” as SMCDiff and FrameDiff haven’t been evaluated on CATH dataset. Since SMCDiff and FrameDiff focus on protein structure design, CATH is usually used to evaluate protein sequence design based on fixed backbone structure, which may have some discrepancies on the design goals. \n\n**Q2: Could you provide head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design? Please follow the same setting.**\n\nAns: We provide protein sequence design given backbone structures on our two datasets in the following table. In this setting, we slightly modified our method to make it adaptable to the task as the reviewer asked. Specifically, we provided the whole backbone structure as the model input and designed the whole protein sequence. As the results show, even though we only provide  backbone structure without any motif residues, our model can still achieves higher AAR and pLDDT on myoglobin, and higher AAR on beta-lactamase.\n\n**myoglobin**\n\n|Model| AAR (%)|pLDDT |\n|:-----|:----:|:----:|\n|ProteinMPNN|81.37|81.9817|\n|ESMIF|76.49|78.0986|\n|PiFold|78.93|80.2912|\n|NAEPro|**85.56**|**82.3871**|\n\n$\\beta$-lactamase\n\n|Model| AAR (%)|pLDDT |\n|:-----|:----:|:----:|\n|ProteinMPNN|53.59|**70.6723**|\n|ESMIF|57.49|61.3928|\n|PiFold|63.38|64.9017|\n|NAEPro|**68.39**|65.8218|\n\n**Q3: Could you provide the code for checking the results?**\n\nAns: We have provided the code and data in the supplementary material. Due to the memory limitation, we couldn’t upload the model checkpoints. However, the training process is highly efficient, and the reviewer could train the model if he/she is interested.\n\n---\n\nWe thank the reviewer for the positive reviews as well as the suggestions for improvement. We have clarified all your concerns, added the experiments and updated the paper accordingly. Our responses to the reviewer’s concerns and questions are provided below:\n\n**Weakness 1: Both EGNN and ESM2 are existing models, and there are many existing works on antibody structure and sequence co-design. The combination may limit the novelty of this method.**\n\nAns: We would like to point out that our NAEPro is not a combination of ESM2 and EGNN. Instead our key innovations are (1) interleaving layers of sequence-level attention and local neighborhood equivariant sub-layer (2) accelerating the local sub-layer with the k-nearest neighbors.  On the contrary,  EGNN is a fully-connected graph and updates message, coordinates and atom features based on all other atoms in 3D space, which is not efficient for long proteins. Besides, EGNN+ESM2 is a sequential process and can not cross-condition on sequence and structure. We compare our method with EGNN+ESM2, NAEPro w/o ESM2 initialization on myoglobin in the Following Table (Table 2 in the paper).  As the results show, EGNN+ESM2 performs much worse than our NAEPro, and removing the ESM2 initialization will not influence the performance too much, demonstrating that our model is much more superior than EGNN+ESM2.\n\n|Model| AAR (%)|RMSD|pLDDT |TM-score |consistency|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|EGNN+ESM2|51.12|2.9891|77.3399|0.4656|4.9827|\n|NAEPro-w/o-ESM2 initialization|79.82|2.6398|76.3032|0.5159|4.7273|\n|NAEPro|**89.37**|**2.6307**|**81.9507**|**0.5692**|**4.3865**|\n\n**Weakness 2: It is likely that previous works can be readily applied to the motif-conditioned setting, and the authors can easily adapt their method to antibody design tasks. It would be beneficial if the authors further elaborate on the significance of their work**\n\nAns: We would like to clarify our goal more clearly. Our task is to co-design functional and novel protein sequence and structure. Previous work which is most similar to ours is Inpainting [1]. However, [1] provides the functional sites manually while we automatically mined the functional sites (plus the conserved sites as suggested by reviewer ****VH6U****) by MSAs. Previous methods considering only sequence design constrained by fitness value [2,3] or sequence design based on given backbone structure (inverse folding) [4] is not suitable for our task. For antibody design, antibody is always Y-shaped, while one of our goal is to design novel and diverse protein structure. For example, in Figure 4, the designed beta-lactamases have different fold categories and belong to three different sub-classes. In Figure 5 (b), the designed myoglobin has a RMSD of 3.943\\AA with the most similar one in PDB, but it can still bind the corresponding metallocofactor heme in the AlphaFill simulation [5]. From this aspect, we think antibody design models can not be directly applied to our task without any modification and vice versa. \n\n[1] Scaffolding protein functional sites using deep learning.  Jue wang. et al. Science. 2022.\n\n[2] Biological Sequence Design with GFlowNets. [Moksh Jain](https://arxiv.org/search/q-bio?searchtype=author&query=Jain,+M) et al. ICML 2022. \n\n[3] Proximal Exploration for Model-guided Protein Sequence Design. Zhizhou Ren et al. ICML 2022. \n\n[4] Robust deep learning based protein sequence design using ProteinMPNN. J. Dauparas et al. Science. 2022.\n\n[5] Alphafill: enriching alphafold models with ligands and cofactors. Maarten L Hekkelman et al. Nature methods. 2023.\n\n**Weakness 3:** **The authors do not provide code for checking the soundness of the methods.**\n\nAns: We have provided the code and data in the supplementary material. Due to the memory limitation, we couldn’t upload the model checkpoints. However, the training process is highly efficient, and the reviewer could train the model if he/she is interested.\n\n---\n\n**Q6: For a novel sequence, ESMFold’s prediction is worse than AlphaFold2. Consequently, evaluating novel sequences should use AlphaFold2 instead of ESMFold.**\n\nAns: Due to limited time and computing resources, we provide the AlphaFold2 results for ProtSeed, RFDiffusion+ProteinMPNN and our NAEPro. As the results show, even though ESMFold achieves lower scores than AlphaFold2, the tendencies (ranking) among different models are similar.  Particularly, our model achieves an average TM-score over 0.5 on beta-lactamase (ESMFold 0.4812) and pLDDT over 70 (ESMFold 66.8705), which gives stronger evidence that our model has the ability to design proteins which have stable structures.\n\n**myoglobin**\n\n|Model| pLDDT|TM-score|\n|:-----|:----:|:----:|\n|PROTSEED|77.9032|0.5621|\n|RFDiffusion+ProteinMPNN|88.2130|0.5103|\n|NAEPro|**91.4139**|**0.5912**|\n\n**$\\beta$-lactamase**\n\n|Model| pLDDT|TM-score|\n|:-----|:----:|:----:|\n|PROTSEED|73.3176|0.4901|\n|RFDiffusion+ProteinMPNN|**87.8092**|0.4806|\n|NAEPro|76.9550|**0.5009**|\n\n **Q7: \"Consistency - What is the designed structure and the predicted structure? Which is from NAEPro? Where does the other one come from?**\n\nAns: Consistency is calculated as the RMSD between the designed structure from NAEPro and the one predicted by ESMFold for the designed sequence from NAEPro.\n\n---\n\n**Weakness 4:  Results in Table 1 do not support the superiority of the proposed method. For instance, RMSD>1.7 \\AA means the two structures are different for myoglobin. For pLDDT, the prediction on lactamase is ~66%, which is much lower than 80%-90% which is conventionally believed reliable for folding predictions. The TM-score, if they are smaller than 0.5, then the two structures are believed different.** \n\nAns: We would like to clarify our goal more clearly.  Our task is to design the sequence and structure of proteins with effective functions.  We calculated TM-score by AlphaFold2 as suggested by the reviewer, and the two proteins designed by our model both achieves an average score over 0.5 (myoglobin **0.5912** and beta-lactamase **0.5009**). However, as found by previous method [7],  protein pairs with a TM-score >0.5 are most likely in the same fold while those with a TM-score <0.5 are mainly not in the same fold. Therefore, low TM-score can not indicate the designed protein doesn’t have the corresponding function. As claimed in previous work [9,10], pLDDT between 70~90 are classified to be confident. We calculated pLDDT by AlphaFold2 as suggested by the reviewer and the two protein families designed by our model are confident on an average aspect (myoglobin **91.4139** and beta-lactamase **76.9550**).  Similarly, as observed in previous work[11,12], pLDDT has a weak correlation with protein function. Therefore, a moderate TM-score and pLDDT can not indicate a protein expresses no or poor function. As shown in figure 5(b), the designed myoglobin has a RMSD of 3.943\\AA with the most similar one in PDB, but it can still bind the corresponding metallocofactor heme in the AlphaFill simulation [8]. To systematically illustrate the function of our designed proteins, we use docking method Gnina [13] to compute the binding affinity between the designed metalloproteins and the corresponding metallocofactors. The results in the following table shows that our model achieves the best binding affinity scores on both metalloproteins, demonstrating the proteins designed by our model are highly potential to actively exhibit biochemical functions.\n\n|Model|myoglobin (kcal/mol)|beta-lactamase (kcal/mol)|\n|:-----|:----:|:----:|\n|Hallucination|-7.23|-6.37|\n|Inpainting|-9.32|-7.63|\n|SMCDiff+ProteinMPNN|-8.79|-6.89|\n|PROTSEED|-10.07|-7.68|\n|FrameDiff+ProteinMPNN|-9.54|-7.21|\n|RFDiffusion+ProteinMPNN|-9.76|-7.59|\n|NAEPro|**-10.14**|**-7.80**|\n\n[7] How significant is a protein structure similarity with TM-score= 0.5? Jinrui Xu and Yang Zhang. *Bioinformatics. 2010.* \n\n[8] Alphafill: enriching alphafold models with ligands and cofactors. Maarten L Hekkelman et al. Nature methods. 2023.\n\n[9] pLDDT Values in AlphaFold2 Protein Models Are Unrelated to Globular Protein Local Flexibility. Oliviero Carugo. Crystals. 2023\n\n[10] Generating new protein sequences by using dense network and attention mechanism. Feng Wang et al. Mathematical Biosciences and Engineering. 2023.\n\n[11] Using AlphaFold to predict the impact of single mutations on protein stability and function. Marina A. Pak et al. PloS one. 2023.\n\n[12] Peptide binder design with inverse folding and protein structure prediction. Patrick Bryant et al. Nature Communications Chemistry. 2023.\n\n[13] GNINA 1.0: Molecular docking with deep learning. A McNutt et al. Cheminformatics. 2021.\n\n**Q1 & Q2: \"The use of separate models for sequence and structure cannot ensure the consistency between the generated sequence and structure.\" \"knowing the topology of a protein before design process is difficult and also cannot guarantee the designed proteins have the desired functions.\" I disagree with this statement.** **I disagree with this statement.** \n\nAns: Thanks for the suggestion! We agree with the reviewer and updated the statement in our paper. \n\n**Q3: what is the meaning of \"generally-encoded\" for 20 types of amino acids?** \n\nAns: It means the 20 common amino acids. We have updated the paper accordingly.\n\n**Q4: \"The selection of motif varies from setting to setting...\". Can the authors please provide a clearer explanation?**\n\nAns: We mean the motifs have different meanings in different tasks. For de novo enzyme design [14], it means the binding sites where enzyme binds the metallocofactor and substrate. For de novo binder design [15], it means the binding sites where the protein binder binds the protein targets.\n\n[14] De novo enzyme design using rosetta3. Florian Richter et al. PloS one. 2011.\n\n[15] De novo design of protein interactions with learned surface fingerprints. Pablo Gainza. Nature. 2023.\n\n---\n\nWe thank the reviewer for the insightful questions. We have clarified all your concerns, added the experiments and updated the paper accordingly. Answers to specific points are provided below:\n\n**Weakness 1 & weakness 2 & Q5: The identified problem or the motivation for designing such a method is not supported by existing literature.  Some designed modules counter the intuition in biology, such as \"motif mining\"**\n\nAns: We follow the design insight proposed by [1], which starts from the functional site and fills in additional sequence and structure to create a viable protein scaffold in a single forward pass. Different from [1], we provide the functional sites which are automatically mined from MSA as previous methods [2, 3] while [1] provides the functional sites manually. Besides, we design a new architecture called NAEPro to generate the whole protein sequence and structure based on the given partial protein fragments, while [1] achieves this goal by finetuning the RoseTTAFold. However, we agree with the reviewer’s opinion in question 5 that MSA discovers not only protein functional sites as addressed in [2,3] but also finds conversed sites which may not directly relate to the protein function. Therefore, we change all the term “motif/functional sites” in our paper to  “meaningful protein fragments”.\n\n[1] Scaffolding protein functional sites using deep learning.  Jue wang. et al. Science. 2022.\n\n[2] Emerging methods in protein co-evolution. David de Juan et al. Nature Review Genetics. 2013.\n\n[3] Evolutionary information for specifying a protein fold. Michael Socolich et al. Nature. 2005.\n\n**Weakness 3 & Q1: The method is not 'extensively evaluated' as claimed by the authors (they only selectively analyzed 2 proteins), and many results do not suggest that the proposed method is SOTA among baselines. Moreover, the evaluation does not include all relevant methods, which makes it too aggressive for the authors to claim in the Introduction that their method \"achieves the HIGHEST...among ALL competitors\" and \"is faster than the FASTEST method\". Empirically, It has been proven by several recent research that the de novo method can generate new sequences for fixed backbone or desired functions [4-6].**\n\nAns: Sorry for the confusion. Our method achieves the best performance on 4/5 metrics on myoglobin and 3/5 metrics on beta-lactamase among all the compared representative baselines in our paper. Our model is faster than all baselines compared in our paper. We have revised all these statements in our paper (highlighted in red). For other relevant methods as the reviewer suggested in reference [4-6], we have already compared with [4]. [5] was posted on Oct. 3, 2023, which is later than the ICLR submission ddl, and [3] was posted on Aug. 14, 2023, which is within 3 months before ICLR submission ddl. We appreciate the reviewer’s suggestion, and we tried to compare with [3], but they haven’t released their code now. We emailed the authors of [3] to ask for code and they said their work is under review now and refused to provide code currently. We are trying to reimplement the model now and will update the results in our revised version.\n\n[4] Watson et al., De novo design of protein structure and function with RFdiffusion (2023).\n\n[5] Sumida et al., Improving protein expression, stability, and function with ProteinMPNN (2023).\n\n[6] Zhou et al., Conditional Protein Denoising Diffusion Generates Programmable Endonuclease Sequences (2023).\n\n---\n\nWe thank all the reviewers' valuable suggestions.\n\nWe have uploaded a revised draft that incorporates feedback from reviewers VH6U and MgyU, with the updated portions highlighted in red. Additionally, in response to reviewer Wq6M's suggestion, we have included the code and data we used in our paper in the supplementary material (Due to the maximum memory limitation -100M, we couldn’t upload the model checkpoints).\n\nHere's an overview highlighting the principal modifications to our paper:\n\n1. The revised term from “motif/functional sites” to “meaningful fragments” in abstract, introduction third paragraph, method opening paragraph, method 3.1-3.4, experiments 4.1-4.2, analysis 5.1, Conclusion (Reviewer VH6U, weakness 1 & weakness 2 & Q5)\n2. The corrected claim in abstract and last paragraph in introduction (review VH6U, weakness3) \n3. The corrected claim in Introduction second paragraph (Reviewer VH6U, Q1)\n4. The corrected claim in Related Work second paragraph (Reviewer VH6U, Q2)\n5. The clarified explanation of 20 common amino acids in Method opening paragraph(Reviewer VH6U, Q3)\n6. The corrected theoretical claim in Method 3.5 and Appendix A. 1 (reviewer MgyU, weakness 5) \n7. The clarified task definition in Method opening paragraph (reviewer MgyU, Q2)\n\nWe look forward to any additional reading and feedback. If you have any further questions, we are very happy to continue discussion!", "author_response": "Thanks for the responses! Our answers to your follow-up questions are provided as follows:\n\n**1. About Weakness4 & Q1 Page 2, \"We carry out experiments on two metalloproteins, including β-lactamase and myoglobin.\" I would like to say that the experimental setting is the main concern to me. Since the authors have only evaluated the proposed method on two structures, I am not sure if the method can be extended to generalized situations. I know that you are doing protein sequence-structure inpaintning problem, in which case a comparison with a previous baselines [1,2,3] on stantard datasets may help to reveal the generalization capabilities of your algorithm.** \n\nAns: We agree with reviewer Wq6M and additionally compare our method with PROTSEED on B12 protein. Due to the approaching ddl and limited time, PROTSEED has only been trained for 100 epochs, the same as our method. (suggested training epoch is 2000 on their GitHub)\n\nThe results are reported as follows: \n\n**Binding affinity score for designed B12**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|PROTSEED|-9.45$\\pm$ 0.03|-9.36$\\pm$0.09|-9.04$\\pm$0.27|-8.46|\n|NAEPro|**-11.06$\\pm$0.14**|**-10.96$\\pm$0.14**|**-10.61$\\pm$0.29**|**-9.11**|\n\n**2. About Q2 Your methods are not carefully designed for protein inverse folding. However, the presented results outperformed current SOTA by a large margin (curent SOTA is about 50%-60% AAR), which could not convince me. I doubt the veracity and correctness of the experimental results.** \n\nAns: Sorry for the confusion. We evaluate the inverse-folding task on our own two datasets instead of the original CATH. For our datasets, the proteins are from the same family, and thus they may have much overlap on the sequence level, which definitely will lead to high AAR. Besides, our model is initialized with ESM2 weights, which will also improve the AAR. \n\nAlthough the sequence prediction based a fixed backbone structure is not our objective, **we adapt our method to CATH setting by keeping all the CA coordinates and masking all residues. Due to the approaching ddl and limited time, our model has only been trained for 10 epochs, while usually the suggested training epoch would be 100 like GVP, PiFold, etc.** (See below: ESMIF, ProteinMPNN, PiFold results are quoted from PiFold paper). \n\nCATH 4.2 results\n\n|Model| AAR (%)|PPL |\n|:-----|:----:|:----:|\n|ProteinMPNN|45.96|4.61|\n|ESMIF|38.30|6.44|\n|PiFold|51.66|4.55|\n|NAEPro|9.16|15.26|\n\n**Again, we did not claim the superiority of our method on inverse folding task on CATH dataset.**\n\n**3. Page 1, \"Despite their great potential for novel structure design, such sequential design policy fails to cross-condition on sequence and structure, which might lead to inconsistent proteins and inefficient design process\". I can not agree with this statement. RFDiffusion's authors say that they also considered simultaneously designing structure and sequence within RFdiffusion, but combining ProteinMPNN with the diffusion of structure alone provides the excellent performance, as shown in your Table.1. Unfortunately, I observe that you deleted the consistency metric in Table.1, which seems like dishonest behavior to me.**\n\nAns: We do not intend to hide any information. As suggested by reviewer MgyU, the binding affinity is a better metric for functional protein design rather than the accuracy metrics (consistency, AAR, RMSD, etc.). Therefore, we replaced all the accuracy metrics in Table 1 and 2 in our paper according with the binding affinity scores calculated by Gnina. The full table with all scores are listed below:\n\n$\\beta$-lactamase\n\n|Model| AAR (\\%,$\\uparrow$) | RMSD ({ \\AA},$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination | $4.79$ | $--$ | $30.5511$ | $0.2918$ | $--$|\n|Inpainting | $16.73$ | $4.0599$ | $61.7679$ | $0.3790$ | $6.2578$|\n|SMCDiff+ProteinMPNN| $19.94$ | $10.3960$ | $42.0375$ | $0.3458$ |$10.2117$|\n| PROTSEED | $37.63$ | $3.0142$ | $64.3861$| $0.4637$ | $3.3748$ |\n| FrameDiff+IF |$26.20$ | $6.0151$ | $65.6445$ | $0.3657$ | $7.8703$|\n|RFDiffusion+IF | $22.93$ | $6.0438$ | **83.4058** | $0.3747$ |**0.5565**|\n|NAEPro|**65.71**| **2.9916**| 66.8705|**0.4812**| 7.3760|\n\n\n**Myoglobin**\n|Model| AAR (\\%,$\\uparrow$) | RMSD (\\AA,$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination| $4.81$ | $--$ | $38.2817$ | $0.2754$ |--|\n|Inpainting |39.59|3.3751|67.0813|0.4391 |3.2108|\n|SMCDiff+IF | $12.47$ | $8.0067$ | $34.5914$ | $0.2235$ | $8.8754$ |\n| PROTSEED | $48.55$ | $2.8753$ | $61.5588$ | $0.5466$ | $0.9764$ |\n| FrameDiff+IF | $20.43$ | $5.9739$ | $61.3945$ | $0.3757$ |$3.5078$|\n|RFDiffusion+IF | $32.70$ | $3.9930$ | $78.9868$ | $0.4147$  | **0.4375** |\n|NAEPro | **89.37**| **2.6307**|**81.9507**|**0.5692**|4.3865|"}
{"claim": "Previous works can be readily applied to the motif-conditioned setting, which calls into question this paper's significance.", "claim_type": "novelty", "paper_id": "Dr4qD9bzZd", "paper_title": "Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "a9hHYfEm0G", "reviewer": "Reviewer_Wq6M", "review_text": "Summary: This paper proposes NAEPro, a model to jointly design Protein sequence and structure. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from the nearest amino acids in three-dimensional (3D) space. The global attention sub-layer parameters are initialized with ESM-2. The author combines ESM2 and EGNN for co-modeling protein sequence and structure.\n\nStrengths: 1. The reported performance is good.\n2. The method is simple.\n\nWeaknesses: 1. Novelty: Both EGNN and ESM2 are existing models, and there are many existing works on antibody structure and sequence co-design. The combination may limit the novelty of this method.  \n2. Significance: It is likely that previous works can be readily applied to the motif-conditioned setting, and the authors can easily adapt their method to antibody design tasks. It would be beneficial if the authors further elaborate on the significance of their work\n3. Code: The authors do not provide code for checking the soundness of the methods.\n4. Experiment setting: The authors do not provide results on standard benchmarks, such as the CATH dataset, for fair comparison on both sequence and structure design.\n\nQuestions: 1. Could you provide experimental results from the CATH dataset to compare with the original SMCDiff and FrameDiff results?\n2. Similarly, could you provide head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design? Please follow the same setting.\n3. Could you provide the code for checking the results?", "labeling_timestamp": "2026-01-11T16:28:52.030986", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The authors explicitly argue that prior methods are not readily applicable to their motif-conditioned co-design task, citing architectural and task differences (automatic motif mining vs manual, inability to cross-condition sequence and structure, inefficiency of prior equivariant networks) and providing empirical comparisons to support their point.", "evidence": "\"Previous work which is most similar to ours is Inpainting [1]. However, [1] provides the functional sites manually while we automatically mined the functional sites ... by MSAs.\"; \"EGNN+ESM2 is a sequential process and can not cross-condition on sequence and structure.\"; \"we think antibody design models can not be directly applied to our task without any modification and vice versa.\"", "combined_author_response": "We much appreciate your valuable comments and insightful suggestions again, which helps a lot to improve the quality of our paper. As the discussion deadline is approaching, please let us know if you have any further concerns or questions. We are happy to have any further discussion!\n\n---\n\n**4. \"Through this way, amino-acid combinations frequently occur in the same context would draw higher attention scores\". Have you checked this statement? Could you provide some visualizations of the attention map and correlated frequences of amino-acid combinations?**  \n\nAns: Thank the reviewer for this suggestion. We randomly pick one sentence in the training set and visualization its attention matrix from the last layer in Appendix Figure 7(a) and also visualize the pairwise amino acid co-occurrence matrix in Figure 7(b). We find it’s consistent with our statement that residues tend to more connect to their neighboring residues. However, we don’t see any strong correlation between the attention matrix and the pairwise amino acid co-occurrence. Therefore, we revised the corresponding statement in our paper and uploaded a revised version accordingly.\n\n**5. Updating residue representations and coordinates in 3D space with only nearest neighbors enables more efficient and economic message passing compared to prior approaches which compute messages on the complete pairwise residue graph\". To my knowledge, most of the previous methods such as proteinmpnn adopt knn for constructing sparse graph in the 3D spcace. You should provide evidence to support the significance of your statement.**\n\nAns: We mean from the architecture level, previous methods like SE(3)-Transformer [1] and EGNN [2] adopts information flow from all other atoms.\n\n[1] SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks. Fabian B. Fuchs et al. NeurIPS 2020. \n\n[2] E(n) Equivariant Graph Neural Networks. Victor Garcia Satorras et al. ICML 2021. \n\n**6. Novelty: I do not observe enough novelty from the perspective of machine learning in algorithm design.**\n\nAns: Yes, from machine learning algorithm level, our method is not new. However, as admitted by reviewer MgyU, we design a new architecture for protein design. The key innovations of our architecture include: (1) interleaving layers of sequence-level attention and local neighborhood equivariant sub-layer (2) accelerating the local sub-layer with the k-nearest neighbors.\n\n---\n\nThanks for the responses! Our answers to your follow-up questions are provided as follows:\n\n**1. About Weakness4 & Q1 Page 2, \"We carry out experiments on two metalloproteins, including β-lactamase and myoglobin.\" I would like to say that the experimental setting is the main concern to me. Since the authors have only evaluated the proposed method on two structures, I am not sure if the method can be extended to generalized situations. I know that you are doing protein sequence-structure inpaintning problem, in which case a comparison with a previous baselines [1,2,3] on stantard datasets may help to reveal the generalization capabilities of your algorithm.** \n\nAns: We agree with reviewer Wq6M and additionally compare our method with PROTSEED on B12 protein. Due to the approaching ddl and limited time, PROTSEED has only been trained for 100 epochs, the same as our method. (suggested training epoch is 2000 on their GitHub)\n\nThe results are reported as follows: \n\n**Binding affinity score for designed B12**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|PROTSEED|-9.45$\\pm$ 0.03|-9.36$\\pm$0.09|-9.04$\\pm$0.27|-8.46|\n|NAEPro|**-11.06$\\pm$0.14**|**-10.96$\\pm$0.14**|**-10.61$\\pm$0.29**|**-9.11**|\n\n**2. About Q2 Your methods are not carefully designed for protein inverse folding. However, the presented results outperformed current SOTA by a large margin (curent SOTA is about 50%-60% AAR), which could not convince me. I doubt the veracity and correctness of the experimental results.** \n\nAns: Sorry for the confusion. We evaluate the inverse-folding task on our own two datasets instead of the original CATH. For our datasets, the proteins are from the same family, and thus they may have much overlap on the sequence level, which definitely will lead to high AAR. Besides, our model is initialized with ESM2 weights, which will also improve the AAR. \n\nAlthough the sequence prediction based a fixed backbone structure is not our objective, **we adapt our method to CATH setting by keeping all the CA coordinates and masking all residues. Due to the approaching ddl and limited time, our model has only been trained for 10 epochs, while usually the suggested training epoch would be 100 like GVP, PiFold, etc.** (See below: ESMIF, ProteinMPNN, PiFold results are quoted from PiFold paper). \n\nCATH 4.2 results\n\n|Model| AAR (%)|PPL |\n|:-----|:----:|:----:|\n|ProteinMPNN|45.96|4.61|\n|ESMIF|38.30|6.44|\n|PiFold|51.66|4.55|\n|NAEPro|9.16|15.26|\n\n**Again, we did not claim the superiority of our method on inverse folding task on CATH dataset.**\n\n**3. Page 1, \"Despite their great potential for novel structure design, such sequential design policy fails to cross-condition on sequence and structure, which might lead to inconsistent proteins and inefficient design process\". I can not agree with this statement. RFDiffusion's authors say that they also considered simultaneously designing structure and sequence within RFdiffusion, but combining ProteinMPNN with the diffusion of structure alone provides the excellent performance, as shown in your Table.1. Unfortunately, I observe that you deleted the consistency metric in Table.1, which seems like dishonest behavior to me.**\n\nAns: We do not intend to hide any information. As suggested by reviewer MgyU, the binding affinity is a better metric for functional protein design rather than the accuracy metrics (consistency, AAR, RMSD, etc.). Therefore, we replaced all the accuracy metrics in Table 1 and 2 in our paper according with the binding affinity scores calculated by Gnina. The full table with all scores are listed below:\n\n$\\beta$-lactamase\n\n|Model| AAR (\\%,$\\uparrow$) | RMSD ({ \\AA},$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination | $4.79$ | $--$ | $30.5511$ | $0.2918$ | $--$|\n|Inpainting | $16.73$ | $4.0599$ | $61.7679$ | $0.3790$ | $6.2578$|\n|SMCDiff+ProteinMPNN| $19.94$ | $10.3960$ | $42.0375$ | $0.3458$ |$10.2117$|\n| PROTSEED | $37.63$ | $3.0142$ | $64.3861$| $0.4637$ | $3.3748$ |\n| FrameDiff+IF |$26.20$ | $6.0151$ | $65.6445$ | $0.3657$ | $7.8703$|\n|RFDiffusion+IF | $22.93$ | $6.0438$ | **83.4058** | $0.3747$ |**0.5565**|\n|NAEPro|**65.71**| **2.9916**| 66.8705|**0.4812**| 7.3760|\n\n\n**Myoglobin**\n|Model| AAR (\\%,$\\uparrow$) | RMSD (\\AA,$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination| $4.81$ | $--$ | $38.2817$ | $0.2754$ |--|\n|Inpainting |39.59|3.3751|67.0813|0.4391 |3.2108|\n|SMCDiff+IF | $12.47$ | $8.0067$ | $34.5914$ | $0.2235$ | $8.8754$ |\n| PROTSEED | $48.55$ | $2.8753$ | $61.5588$ | $0.5466$ | $0.9764$ |\n| FrameDiff+IF | $20.43$ | $5.9739$ | $61.3945$ | $0.3757$ |$3.5078$|\n|RFDiffusion+IF | $32.70$ | $3.9930$ | $78.9868$ | $0.4147$  | **0.4375** |\n|NAEPro | **89.37**| **2.6307**|**81.9507**|**0.5692**|4.3865|\n\n---\n\nThanks for the further response! Please see our responses to your follow-up questions and concerns as follows:\n\n**Response to Weakness 3 & Q1: My point in mentioning these three papers is not to introduce additional (and possibly meaningless) in-silico comparisons to the authors. Instead, they validated their designs with solid wet-lab experiments demonstrating that it is possible to generate novel protein sequences with fixed backbones.**\n\nAns: We agree on the reviewer’s opinion and have accordingly updated the statement in Introduction.  We have uploaded a revised version of our paper.\n\n**Response to Weakness 4: I completely agree that all the existing evaluation metrics we used in deep learning are only indirect indicators, and they only imply a higher chance that the assessed protein will perform its function. However, I have trouble following the logic that \"*Therefore, a low TM-score cannot indicate the designed protein doesn’t have the corresponding function*\" suggests \"proteins with TM-scores that are lower than 0.5 (which is a wildly-used standard) are still reliable in terms of their functionality in general.\" Similarly, if we all agree that the existing metrics are suboptimal and none of them provides 100% accurate evaluations, it is then meaningless to criticize the widely applied standards with one or two counter-examples. After all, you don't have any better evidence (such as wet-lab results) that is more reliable than the existing criteria.**\n\nAns: We agree with Reviewer VH6U. As suggested by reviewer MgyU, the binding affinity is a better metric for functional protein design. We calculated the binding affinity scores for top-5, top-10, and top-30 candidates and also median score on the test set using Gnina. The results are reported as follows:\n\n**Binding affinity score for designed myoglobin**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-8.18$\\pm$ 0.01|-8.07$\\pm$0.03|-7.97$\\pm$0.23|-7.25|\n|Inpainting|-13.47$\\pm$0.02|-13.12$\\pm$0.12|-12.31$\\pm$0.54|-9.56|\n|SMCDiff+ProteinMPNN|-11.37$\\pm$0.03|-11.12$\\pm$0.31|-10.87$\\pm$0.42|-8.76|\n|PROTSEED|-13.21$\\pm$0.13|-12.89$\\pm$0.42|-11.98$\\pm$0.52|-10.23|\n|FrameDiff+ProteinMPNN|-13.13$\\pm$0.05|-12.92$\\pm$0.16|-12.21$\\pm$0.23|-10.08|\n|RFDiffusion+ProteinMPNN|-13.68$\\pm$0.02|-13.03$\\pm$0.21|-12.56$\\pm$0.43|-10.15|\n|NAEPro|**-14.12$\\pm$0.01**|**-13.85$\\pm$0.10**|**-13.06$\\pm$0.38**|**-10.74**|\n\n**Binding affinity score for designed beta-lactamase**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-6.98$\\pm$0.01|-6.87$\\pm$0.02|-6.69$\\pm$0.05|-6.29|\n|Inpainting|-9.89$\\pm$0.03|-9.54$\\pm$0.16|-9.13$\\pm$0.43|-7.24|\n|SMCDiff+ProteinMPNN|-9.10$\\pm$0.01|-9.05 $\\pm$0.02|-8.98$\\pm$0.01|-6.97|\n|PROTSEED|-9.88$\\pm$0.21|-9.51$\\pm$0.41|-9.01$\\pm$0.62|-7.31|\n|FrameDiff+ProteinMPNN|-9.54$\\pm$0.03|-9.56$\\pm$0.23|-8.89$\\pm$0.35|-7.03|\n|RFDiffusion+ProteinMPNN|-9.87$\\pm$0.05|-9.56$\\pm$0.23|-9.12$\\pm$0.53|-7.51|\n|NAEPro|**-10.06$\\pm$0.05**|**-9.79$\\pm$0.10**|**-9.39$\\pm$0.12**|**-7.66**|\n\n**Response to Q6: Because ESMFold is a less reliable tool for evaluating novel proteins, it should not be used here to assess your designs at all.**\n\nAns: We totally agree with reviewer VH6U. We provide the mean pLDDT for top-5, top-10, top-30 candidates calculated by AlphaFold2 as follows:\n\n**AlphaFold2 pLDDT for designed myoglobin**\n\n|Model| top-5|top-10|top-30|\n|:-----|:----:|:----:|:----:|\n|Hallucination|76.7243|73.5953|68.6060|\n|Inpainting|97.6171|97.6063|97.5681|\n|SMCDiff+ProteinMPNN|85.7051|84.7744|84.9220|\n|PROTSEED|92.9770|91.9216|90.4293|\n|FrameDiff+ProteinMPNN|97.3997|97.3329|96.4279|\n|RFDiffusion+ProteinMPNN|97.9722|97.9174|97.6063|\n|NAEPro|**98.1057**|**98.0706**|**97.9197**|\n\n**AlphaFold2 pLDDT for designed $\\beta$-lactamase**\n\n|Model| top-5|top-10|top-30|\n|:-----|:----:|:----:|:----:|\n|Hallucination|74.7760|74.7032|71.2932|\n|Inpainting|94.5489|94.4328|93.2941|\n|SMCDiff+ProteinMPNN|86.2510|85.7072|83.0376|\n|PROTSEED|98.2896|98.2740|97.9683|\n|FrameDiff+ProteinMPNN|98.3049|98.2922|98.1267|\n|RFDiffusion+ProteinMPNN|**98.6225**|**98.5116**|98.0643|\n|NAEPro|98.5385|98.4481|**98.2841**|\n\n---\n\nDear reviewer,\n\n    We sincerely appreciate the time and effort you dedicated to reviewing our paper and reading our responses. Your valuable suggestions and insightful comments have significantly contributed to refining the quality of our work. No matter what the final result will be, the thoughtful communication with you has better clarified our paper's goals and logic. We would like to express our great gratitude to you!\n\nBest,\n\nAuthors of paper 4608\n\n---\n\n**Could the authors briefly explain how do you construct the one-to-one mapping between the design structure and the target structure in PDB? Is it determined by retrieving the one in PDB with the smallest RMSD? As far as I know, there is no common practice to determine this for the task of protein design, and I appreciate any explanation that clues me in.**\n\nAns: Yes, we construct the one-to-one mapping between the designed structure and the target structure in PDB. It is not determined by retrieving the one in PDB with the smallest RMSD. We extract the motif fragments for each protein, based on which our training objective is to recover the original protein sequence and backbone structure. Therefore, we compute the RMSD between the designed structure and the target one which we want to recover.\n\n---\n\n**Corrected statement: But it can still bind the corresponding metallocofactor heme in the AlphaFill simulation. For the claim “binding rates of 100% and 90%”, I wonder how is the criterion/threshold of “binding” determined? This success rate is rather unrealistic and probably fails to reflect the real performance. Thus, the statement in Section 5.2. saying “It is evident that our method can generate proteins that express the basic and important functions.” can be quite misleading for readers who are not familiar with the task.** \n\nAns: Sorry for the misleading statement. We mean through the AlphaFill prediction, the designed proteins are highly potential to bind the corresponding metallocofactors. We have corrected all the claims in our paper. We provided the process of how to decide if the designed protein can “bind” the metallocofactors in Appendix B.3. We would like to further explain it here.  First of all, we use AlphaFill to do the first-step prediction. AlphaFill will return a complex based on the input protein structure. If the complex includes the corresponding metal ions, we think the designed protein is potential to have functions. Then to guarantee the results convincing, we further use some additional constraints to ensure the designed proteins are potential to express functions. Specifically, we assume if a designed protein share the similar active site environments with their natural counterparts (target protein in our dataset), then it’s highly potential to bind the corresponding metal ions. For myoglobin, we calculate the distance between axial histidine ligand to Fe ion. Besides, we also detect the presence of distal histidine within the active sites, which plays an important role in the oxygen molecule binding function of myoglobin . If the distance is between 2.0Å and 2.5Å, and the distal histidine exists, we think the designed myoglobins share similar active site environments to natural ones. For $beta$-lactamase, we detect the residues that directly contact with zinc ion. If they show similar chemistry properties with natural $beta$-lactamases, we think they can highly potentially bind the corresponding metallocofactors. For example, in Figure 5(a), the active site possesses one zinc atom coordinated to three histidines, while the second zinc atom is coordinated to one histidine, one cysteine, and one aspartate, demonstrating a high degree of amino acid analogy to the natural protein.\n\n**Could the author intuitively describe how does the NAEL deal with the reflection case so as to be simply SE(3)-equivariant instead of E(3)?** \n\nAns: Intuitively, applying reflection to the input structure, the output structure will also be reflected. The conformation of the original backbone will be changed from L-amino acid to D-amino acid (We provide an illustration in Figure 6 in appendix). Therefore, the reflection will change the chirality of the protein which may cause the deficiency of binding to the ligand and eventually its function. However, the protein sequence in our model will not be influenced. Therefore, the 3D structure of the designed sequence which is L-amino acid and the output structure  which becomes D-amino acid will be inconsistent. From this aspect, our model doesn’t satisfy reflection equivariance.\n\n**The authors are encouraged to encompass appropriate comparison with EGNN/SE(3)-transformer in their writing to better contextualize the proposed NAEL.**\n\nAns: We provide the comparison with EGNN+ESM2 on myoglobin in the following Table and also updated the results in Table2 accordingly.\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|EGNN+ESM2|-13.30$\\pm$0.61 | -12.77$\\pm$0.69 |-12.07$\\pm$0.65 | -9.63|\n|NAEPro|**-14.12$\\pm$0.01**|**-13.85$\\pm$0.10**|**-13.06$\\pm$0.38**|**-10.74**|\n\n**It is surprising to see scaling the model does not yield consistent performance gain. Could the authors help explain this behavior? Also, I noticed that the authors merge the two myoglobin and lactamase data for training newly in the rebuttal response. Is there any insight for doing such multi-task learning?**\n\nAns: Enlarging the model hasn’t improved the performance too much. Our interpretation is that the training data size is moderate, and thus a model with moderate size can fit the data well. To validate if this guess holds, we merge the two protein data and then train a unified model. As we can see in the previous table, our model did achieve obvious better results on $\\beta$-lactamase, i.e., TM-score from below 0.5 (0.4919) to over 0.5 (0.5055).\n\n---\n\nWe much appreciate the reviewer’s responses and suggestions, which help a lot to improve our paper.  We have accordingly revised our paper, including Table 1, Table 2, Section 4.2-4.4 and Section 5.2. Our responses to the reviewer’s follow-up questions are provided as follows:\n\n**About the evaluation metrics: I think the binding affinity could better support the effectiveness of NAEPro and be more suitable for the task (functional protein design) rather than the accuracy metrics (RMSD, AAR, etc.). Do the authors agree on this or not?** C**ould the authors explain how the Binding Affinity is calculated for both model based on Gnina? Specifically, I am worried that the metrics is averaged among the test set similar to other metrics.  If so, I suggest that the authors report median or top-k mean as evaluation.**\n\nAns: We completely agree with reviewer MgyU that the binding affinity is a better metric for functional protein design rather than the accuracy metrics. We revised Table 1 and 2 in our paper according to your suggestion. The binding affinity is calculated using Gnina as the average score on the test set. Thank the reviewer for this valuable suggestion. We provide  top-5, top-10, top-30 mean and variance as well as the median in the following table and also revise our paper accordingly. From the results, we can see that our model achieves the best median and top-K binding affinity scores on both datasets.\n\n**Myoglobin**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-8.18$\\pm$ 0.01|-8.07$\\pm$0.03|-7.97$\\pm$0.23|-7.25|\n|Inpainting|-13.47$\\pm$0.02|-13.12$\\pm$0.12|-12.31$\\pm$0.54|-9.56|\n|SMCDiff+ProteinMPNN|-11.37$\\pm$0.03|-11.12$\\pm$0.31|-10.87$\\pm$0.42|-8.76|\n|PROTSEED|-13.21$\\pm$0.13|-12.89$\\pm$0.42|-11.98$\\pm$0.52|-10.23|\n|FrameDiff+ProteinMPNN|-13.13$\\pm$0.05|-12.92$\\pm$0.16|-12.21$\\pm$0.23|-10.08|\n|RFDiffusion+ProteinMPNN|-13.68$\\pm$0.02|-13.03$\\pm$0.21|-12.56$\\pm$0.43|-10.15|\n|NAEPro|**-14.12$\\pm$0.01**|**-13.85$\\pm$0.10**|**-13.06$\\pm$0.38**|**-10.74**|\n\n**Beta-lactamase**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-6.98$\\pm$0.01|-6.87$\\pm$0.02|-6.69$\\pm$0.05|-6.29|\n|Inpainting|-9.89$\\pm$0.03|-9.54$\\pm$0.16|-9.13$\\pm$0.43|-7.24|\n|SMCDiff+ProteinMPNN|-9.10$\\pm$0.01|-9.05 $\\pm$0.02|-8.98$\\pm$0.01|-6.97|\n|PROTSEED|-9.88$\\pm$0.21|-9.51$\\pm$0.41|-9.01$\\pm$0.62|-7.31|\n|FrameDiff+ProteinMPNN|-9.54$\\pm$0.03|-9.56$\\pm$0.23|-8.89$\\pm$0.35|-7.03|\n|RFDiffusion+ProteinMPNN|-9.87$\\pm$0.05|-9.56$\\pm$0.23|-9.12$\\pm$0.53|-7.51|\n|NAEPro|**-10.06$\\pm$0.05**|**-9.79$\\pm$0.10**|**-9.39$\\pm$0.12**|**-7.66**|\n\n**Corrected statement: ESMFold might not accurately predict the structure for a novel sequence.**\n\nAns: Sorry for the misunderstood claim. We want to express we agree on reviewer VH6U’s comment in question 6 “for a novel sequence, its prediction is worse than AlphaFold2.”  As suggested by reviewer VH6U, we re-calculate pLDDT by AlphaFold2. Our model respectively achieves 91.4139 and 76.9550  on myoglobin and $\\beta$-lactamase on average. As claimed in previous work [1,2], pLDDT between 70~90 are classified to be confident and pLDDT ≥ 90 indicates residues predicted with extremely high confidence.\n\n[1] pLDDT Values in AlphaFold2 Protein Models Are Unrelated to Globular Protein Local Flexibility. Oliviero Carugo. Crystals. 2023.\n\n[2] Generating new protein sequences by using dense network and attention mechanism. Feng Wang et al. Mathematical Biosciences and Engineering. 2023.\n\n---\n\nThanks for your valuable review and comments again. We have addressed all your concerns and questions.\n\nIf you have any other questions, please feel free to discuss with us.\n\n---\n\nThanks for your valuable review and comments again. We have addressed all your concerns and questions.\n\nIf you have any other questions, please feel free to discuss with us.\n\n---\n\nThanks for your valuable review and comments again. We have addressed all your concerns and questions.\n\nIf you have any other questions, please feel free to discuss with us.\n\n---\n\n**Q3: In section 4.2, the author(s) described that the weights of NAEPro is partially initialized from pretrained ESM2-8M, of the smallest model size among the ESM2 series. This is very problematic but the recent common practices leverage at least the 650M model. Do the author(s) try much larger model size as initialization? I found in the Table 2 that the 8M model has only marginal improvement upon random initialization.**\n\nAns: Yes, we tried to partially initialize our model with 35M ESM2 (NAEPro-12) and also train this model on the merged myoglobin and beta-lactamase data (NAEPro-12-Unified). For myoglobin, enlarging the model to 12 layers will not influence the performance too much, but training the larger model on the merged dataset will increase the pLDDT and TM-score. For beta-lactamase, enlarging the model from 6 layers to 12 layers is beneficial and will increase the overall performance. Particularly, training the 12-layer model on merged dataset will significantly improve the performance of beta-lactamase, say improve the average TM-score from below 0.5 to above 0.5. Then we also partially initialize a 30-layer model with 150M ESM2 (NAEPro-30-Unified) and train the model on merged dataset. The performance for both myoglobin and beta-lactamase will be further improved.\n\n**myoglobin**\n\n|Model| AAR (%)|RMSD|pLDDT |TM-score |Parameters|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|NAEPro-6 |**89.37**|**2.6307**|81.9507|0.5692|11.6M|\n|NAEPro-12 |88.42|2.6333|82.2849|0.5673|52M|\n|NAEPro-12-Unified|86.81|2.6443|82.9736|0.5736|52M|\n|NAEPro-30-Unified|86.62|2.6410|**83.0173**|**0.5749**|223.7M|\n\n**$\\beta$-lactamase**\n\n|Model| AAR (%)|RMSD|pLDDT |TM-score |Parameters|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|NAEPro-6 |65.71|2.9916|66.8705|0.4812|11.6M|\n|NAEPro-12 |67.28|2.9864|68.7394|0.4919|52M|\n|NAEPro-12-Unified|70.93|**2.9707**|69.9333|0.5055|52M|\n|NAEPro-30-Unified|**72.60**|2.9781|**71.0429**|**0.5089**|223.7M|\n\n**Q4: How is the “target structure” in evaluation metrics(Section 4.2) determined? What is the difference between metrics RMSD and consistency?**\n\nAns: Target structure means the natural protein structure provided in PDB. RMSD is calculated as the RMSD between the designed protein structure and the one given in PDB. Consistency is calculated as the RMSD between the designed structure and the one predicted by ESMFold for the designed sequence.\n\nWe really hope our responses address your concerns. If you have any other questions, we are very happy to continue discussions!\n\n---\n\n**Weakness 4: For the evaluation metrics, the problem to be studied is functional protein (as it is claimed in the title) design instead of general backbone generation(eg., RFDiffusion) or sequence generation (eg., ProteinMPNN). Thus, I found the the five of the metrics fail to (or indirectly if they potentially do) reflect the performance of functional protein design or scaffolding. All these metrics in Table 1 & 2 show the somehow sequence-structure matching between the predicted and native, but are not very suitable for proteins such as enzyme.**\n\nAns: In our analysis (Section 5.2), we evaluated the function for binding the corresponding metallocofactors of the designed proteins. Specifically, we first randomly select 20 cases from the top100 sequences (pLDDT ranking). We then employ AlphaFold2 for protein structure prediction, followed by inputting these structures into AlphaFill [1] to predict the associated ligands. Notably, our results indicate that all designed myoglobins exhibit heme binding capability, while 18 beta-lactamases demonstrate the ability to bind zinc ions, resulting in metallocofactor binding rates of 100% and 90%, respectively. It is evident that our method can generate proteins that express the basic and important functions, i.e. binding the corresponding metallocofactors. To systematically illustrate the function of our designed proteins, we use docking tool Gnina[2] to compute the binding affinity between the designed metalloproteins and the corresponding metallocofactors. The results in the following table shows that our model achieves the best binding affinity scores on both metalloproteins, demonstrating the proteins designed by our model are highly potential to actively exhibit biochemical functions.\n\n|Model| myoglobin (kcal/mol)|beta-lactamase (kcal/mol)|\n|:-----|:----:|:----:|\n|Hallucination|-7.23|-6.37|\n|Inpainting|-9.32|-7.63|\n|SMCDiff+ProteinMPNN|-8.79|-6.89|\n|PROTSEED|-10.07|-7.68|\n|FrameDiff+ProteinMPNN|-9.54|-7.21|\n|RFDiffusion+ProteinMPNN|-9.76|-7.59|\n|NAEPro|**-10.14**|**-7.80**|\n\n**Weakness 5: The potential theoretical flaw in the equivariance analysis in Section 3.5.** \n\nAns: Thank the reviewer for pointing out this typo. Our model satisfies only roto-translation equivariance without reflection equivariance. We have accordingly updated the paper.\n\n**Q1: The proposed architecture encoding both sequence and structure via the masked language modeling (MLM) scheme is somehow interesting. How do the author(s) see the  difference between the proposed NAEL layer and tensor field convolutional layer in SE(3)-transformer ? Also, what is the advantage of NAEL over the SeqIPA of the PROTSEED, the most competitive baseline in Table 1?** \n\nAns: The most biggest difference between our NAEPro and tensor field convolutional layer in SE(3)-transformer is:  NAEPro uses interleaving layers of global sequence-level attention and local neighborhood equivariant sub-layer, from which the residue representation is updated from both the whole sequence interaction and 3D neighbor message passing; instead, if we enable kNN message passing in SE(3)-transformer, then the node feature will only be updated based on 3D neighboring information. To clearly demonstrate the advantage of NAEL over SeqIPA in PROTSEED, we replace our NAEL with the SeqIPA-Addition without secondary structure feature. Taking myoglobin as an example, the results are shown in the following table. NAEPro with NAEL performs better than with SeqIPA.\n\n|Model| AAR (%)|RMSD|\n|:-----|:----:|:----:|\n|NAEPro -w/- SeqIPA|65.33|2.7078|\n|NAEPro|**89.37**|**2.6307**|\n\n**Q2:  In Section 3 - opening paragraph, the author(s) define/formulate the target task as generate a protein sequence and all 3D coordinates of N residues. However, in later model definition, the proposed NAEPro only operates on the C-alpha(CA) coordinates. Please explain this ambiguity.**\n\nAns: We are sorry for the ambiguity caused. When we mention “we formulate the target task as generate a protein sequence and all 3D coordinates of N residues”, we mean 3D coordinates of alpha-carbon of all the residues. Following the setting in [3], we represent the backbone structure as the CA-only coordinates of all residues. We have updated the paper accordingly in the new version.\n\n[3] Diffusion probabilistic modeling of protein backbones in 3D for the motif-scaffolding problem. Brian L. Trippe et al. ICLR. 2023.\n\n---\n\nWe appreciate the reviewer’s valuable and insightful suggestions, which are very helpful for us to improve our paper. We have clarified all your concerns, added the experiments and updated the paper accordingly. We address the specific concerns as follows:\n\n **Weakness 1: Regarding the consistency metrics reported, I noticed that the consistency metric of NAEPro in Table 1 (as the main results) underperformed half of the baselines.  Such key observation greatly weakened the method. The authors can elaborate on this point.**\n\nAns: Yes, we agree with the reviewer’s opinion. We would like to clarify our task more clearly. Our goal is to design functional proteins with novel sequence and structure. As shown in Figure 5(b), the designed myoglobin has a RMSD of 3.943\\AA with the most similar one in PDB and 26.7% AAR with the most similar one in Uniprot, but it can still bind the corresponding metallocofactor heme in the AlphaFill simulation [1]. In our paper, we use ESMFold to predict the structure of the designed sequence. However, as pointed out by reviewer VH6U, ESMFold might not accurately predict the structure for a novel sequence. To further clarify this point, we calculate the scRMSD of motifs between the designed structure and the predicted structure from the designed sequence in the following table. As the results show, our NAEPro achieves smaller RMSD than RFDiffusion+ProteinMPNN on myoglobin and comparable results on beta-lactamase.\n\n|Model| myoglobin|beta-lactamase|\n|:-----|:----:|:----:|\n|Inpainting|0.5292|0.7819|\n|SMCDiff+ProteinMPNN|1.3816|1.4523|\n|PROTSEED|0.4987|**0.3892**|\n|FrameDiff+ProteinMPNN|0.8765|0.9815|\n|RFDiffusion+ProteinMPNN|0.4936|0.3918|\n|NAEPro|**0.4832**|0.5239|\n\n[1] Alphafill: enriching alphafold models with ligands and cofactors. Maarten L Hekkelman et al. Nature methods. 2023.\n\n**Weakness 2: For the experiments, how is the model trained to obtain the evaluation digits in Table 1 for each baseline. This is very important.** \n\nAns: We are sorry for the confusion. We got the codes of all baselines from their GitHub repos, and then re-trained all the models separately on myoglobin and beta-lactamase datasets. The evaluation procedure among all the baselines and our model follows the same process, and thus we believe the comparisons in Table 1 are fair.\n\n**Weakness 3: For the dataset, I found that in section 4.1, the author(s) mentioned that both beta-lactamase and myoglobin datasets are split randomly. This is worrying because the entries in PDB can usually be redundant, especially for such large and well-studied families that the authors selected for evaluation. The author(s) can elaborate on this point. Moreover, in the Table 3 (in appendix), I am curious about the column name “PDB”, does it mean the number of PDB entries (with unique ID) or the number of single chains?** \n\nAns: Yes, this is a very good question. Since we focus on sequence and structure co-design task, we aim to design both functional sequence and structure. Therefore, our input data is the <sequence, structure> pair. From this point, even though the sequence might be the same, the structure is somehow different. Therefore, our paired input data are different and would not be redundant. We appreciate the reviewer for carefully reviewing our paper and also reading the appendix. In Table 3, PDB means the number of different PDB entries. To eliminate the reviewer’s concerns, we re-cleaned the test set by filtering pairs whose sequences have over 30% sequence identity to any sample in the training set after doing pairwise alignment for myoglobin. We compare our NAEPro and ProtSeed (the most competitive baseline) in the following table. As the results show, our model can design proteins with higher binding affinity (by Gnina [2]) on samples that have lower similarity to training data.\n\n|Model| RMSD|Binding Affinity (kcal/mol)|\n|:-----|:----:|:----:|\n|PROTSEED|**2.4031**|-9.62|\n|NAEPro|2.6634|**-9.98**|\n\n[2] GNINA 1.0: Molecular docking with deep learning. A McNutt et al. Cheminformatics. 2021.\n\n---\n\n**Weakness4 & Q1: The authors do not provide results on standard benchmarks, such as the CATH dataset, for fair comparison on both sequence and structure design. Could you provide experimental results from the CATH dataset to compare with the original SMCDiff and FrameDiff results?**\n\nAns: We would like to clarify our task more clearly. Our task aims to co-design functional and novel protein sequence and structure. In our method, we use MSAs to automatically find motifs to guarantee the protein function, which is achieved within the same protein family. Instead, CATH is a protein dataset consisting of proteins from diverse families, making the motif extraction difficult. Besides, CATH is usually used to evaluate inverse folding task, say protein sequence design based on fixed backbone structure, which is not the goal of our paper. We hope we didn’t misunderstand the reviewer’s meaning by “the original SMCDiff and FrameDiff results” as SMCDiff and FrameDiff haven’t been evaluated on CATH dataset. Since SMCDiff and FrameDiff focus on protein structure design, CATH is usually used to evaluate protein sequence design based on fixed backbone structure, which may have some discrepancies on the design goals. \n\n**Q2: Could you provide head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design? Please follow the same setting.**\n\nAns: We provide protein sequence design given backbone structures on our two datasets in the following table. In this setting, we slightly modified our method to make it adaptable to the task as the reviewer asked. Specifically, we provided the whole backbone structure as the model input and designed the whole protein sequence. As the results show, even though we only provide  backbone structure without any motif residues, our model can still achieves higher AAR and pLDDT on myoglobin, and higher AAR on beta-lactamase.\n\n**myoglobin**\n\n|Model| AAR (%)|pLDDT |\n|:-----|:----:|:----:|\n|ProteinMPNN|81.37|81.9817|\n|ESMIF|76.49|78.0986|\n|PiFold|78.93|80.2912|\n|NAEPro|**85.56**|**82.3871**|\n\n$\\beta$-lactamase\n\n|Model| AAR (%)|pLDDT |\n|:-----|:----:|:----:|\n|ProteinMPNN|53.59|**70.6723**|\n|ESMIF|57.49|61.3928|\n|PiFold|63.38|64.9017|\n|NAEPro|**68.39**|65.8218|\n\n**Q3: Could you provide the code for checking the results?**\n\nAns: We have provided the code and data in the supplementary material. Due to the memory limitation, we couldn’t upload the model checkpoints. However, the training process is highly efficient, and the reviewer could train the model if he/she is interested.\n\n---\n\nWe thank the reviewer for the positive reviews as well as the suggestions for improvement. We have clarified all your concerns, added the experiments and updated the paper accordingly. Our responses to the reviewer’s concerns and questions are provided below:\n\n**Weakness 1: Both EGNN and ESM2 are existing models, and there are many existing works on antibody structure and sequence co-design. The combination may limit the novelty of this method.**\n\nAns: We would like to point out that our NAEPro is not a combination of ESM2 and EGNN. Instead our key innovations are (1) interleaving layers of sequence-level attention and local neighborhood equivariant sub-layer (2) accelerating the local sub-layer with the k-nearest neighbors.  On the contrary,  EGNN is a fully-connected graph and updates message, coordinates and atom features based on all other atoms in 3D space, which is not efficient for long proteins. Besides, EGNN+ESM2 is a sequential process and can not cross-condition on sequence and structure. We compare our method with EGNN+ESM2, NAEPro w/o ESM2 initialization on myoglobin in the Following Table (Table 2 in the paper).  As the results show, EGNN+ESM2 performs much worse than our NAEPro, and removing the ESM2 initialization will not influence the performance too much, demonstrating that our model is much more superior than EGNN+ESM2.\n\n|Model| AAR (%)|RMSD|pLDDT |TM-score |consistency|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|EGNN+ESM2|51.12|2.9891|77.3399|0.4656|4.9827|\n|NAEPro-w/o-ESM2 initialization|79.82|2.6398|76.3032|0.5159|4.7273|\n|NAEPro|**89.37**|**2.6307**|**81.9507**|**0.5692**|**4.3865**|\n\n**Weakness 2: It is likely that previous works can be readily applied to the motif-conditioned setting, and the authors can easily adapt their method to antibody design tasks. It would be beneficial if the authors further elaborate on the significance of their work**\n\nAns: We would like to clarify our goal more clearly. Our task is to co-design functional and novel protein sequence and structure. Previous work which is most similar to ours is Inpainting [1]. However, [1] provides the functional sites manually while we automatically mined the functional sites (plus the conserved sites as suggested by reviewer ****VH6U****) by MSAs. Previous methods considering only sequence design constrained by fitness value [2,3] or sequence design based on given backbone structure (inverse folding) [4] is not suitable for our task. For antibody design, antibody is always Y-shaped, while one of our goal is to design novel and diverse protein structure. For example, in Figure 4, the designed beta-lactamases have different fold categories and belong to three different sub-classes. In Figure 5 (b), the designed myoglobin has a RMSD of 3.943\\AA with the most similar one in PDB, but it can still bind the corresponding metallocofactor heme in the AlphaFill simulation [5]. From this aspect, we think antibody design models can not be directly applied to our task without any modification and vice versa. \n\n[1] Scaffolding protein functional sites using deep learning.  Jue wang. et al. Science. 2022.\n\n[2] Biological Sequence Design with GFlowNets. [Moksh Jain](https://arxiv.org/search/q-bio?searchtype=author&query=Jain,+M) et al. ICML 2022. \n\n[3] Proximal Exploration for Model-guided Protein Sequence Design. Zhizhou Ren et al. ICML 2022. \n\n[4] Robust deep learning based protein sequence design using ProteinMPNN. J. Dauparas et al. Science. 2022.\n\n[5] Alphafill: enriching alphafold models with ligands and cofactors. Maarten L Hekkelman et al. Nature methods. 2023.\n\n**Weakness 3:** **The authors do not provide code for checking the soundness of the methods.**\n\nAns: We have provided the code and data in the supplementary material. Due to the memory limitation, we couldn’t upload the model checkpoints. However, the training process is highly efficient, and the reviewer could train the model if he/she is interested.\n\n---\n\n**Q6: For a novel sequence, ESMFold’s prediction is worse than AlphaFold2. Consequently, evaluating novel sequences should use AlphaFold2 instead of ESMFold.**\n\nAns: Due to limited time and computing resources, we provide the AlphaFold2 results for ProtSeed, RFDiffusion+ProteinMPNN and our NAEPro. As the results show, even though ESMFold achieves lower scores than AlphaFold2, the tendencies (ranking) among different models are similar.  Particularly, our model achieves an average TM-score over 0.5 on beta-lactamase (ESMFold 0.4812) and pLDDT over 70 (ESMFold 66.8705), which gives stronger evidence that our model has the ability to design proteins which have stable structures.\n\n**myoglobin**\n\n|Model| pLDDT|TM-score|\n|:-----|:----:|:----:|\n|PROTSEED|77.9032|0.5621|\n|RFDiffusion+ProteinMPNN|88.2130|0.5103|\n|NAEPro|**91.4139**|**0.5912**|\n\n**$\\beta$-lactamase**\n\n|Model| pLDDT|TM-score|\n|:-----|:----:|:----:|\n|PROTSEED|73.3176|0.4901|\n|RFDiffusion+ProteinMPNN|**87.8092**|0.4806|\n|NAEPro|76.9550|**0.5009**|\n\n **Q7: \"Consistency - What is the designed structure and the predicted structure? Which is from NAEPro? Where does the other one come from?**\n\nAns: Consistency is calculated as the RMSD between the designed structure from NAEPro and the one predicted by ESMFold for the designed sequence from NAEPro.\n\n---\n\n**Weakness 4:  Results in Table 1 do not support the superiority of the proposed method. For instance, RMSD>1.7 \\AA means the two structures are different for myoglobin. For pLDDT, the prediction on lactamase is ~66%, which is much lower than 80%-90% which is conventionally believed reliable for folding predictions. The TM-score, if they are smaller than 0.5, then the two structures are believed different.** \n\nAns: We would like to clarify our goal more clearly.  Our task is to design the sequence and structure of proteins with effective functions.  We calculated TM-score by AlphaFold2 as suggested by the reviewer, and the two proteins designed by our model both achieves an average score over 0.5 (myoglobin **0.5912** and beta-lactamase **0.5009**). However, as found by previous method [7],  protein pairs with a TM-score >0.5 are most likely in the same fold while those with a TM-score <0.5 are mainly not in the same fold. Therefore, low TM-score can not indicate the designed protein doesn’t have the corresponding function. As claimed in previous work [9,10], pLDDT between 70~90 are classified to be confident. We calculated pLDDT by AlphaFold2 as suggested by the reviewer and the two protein families designed by our model are confident on an average aspect (myoglobin **91.4139** and beta-lactamase **76.9550**).  Similarly, as observed in previous work[11,12], pLDDT has a weak correlation with protein function. Therefore, a moderate TM-score and pLDDT can not indicate a protein expresses no or poor function. As shown in figure 5(b), the designed myoglobin has a RMSD of 3.943\\AA with the most similar one in PDB, but it can still bind the corresponding metallocofactor heme in the AlphaFill simulation [8]. To systematically illustrate the function of our designed proteins, we use docking method Gnina [13] to compute the binding affinity between the designed metalloproteins and the corresponding metallocofactors. The results in the following table shows that our model achieves the best binding affinity scores on both metalloproteins, demonstrating the proteins designed by our model are highly potential to actively exhibit biochemical functions.\n\n|Model|myoglobin (kcal/mol)|beta-lactamase (kcal/mol)|\n|:-----|:----:|:----:|\n|Hallucination|-7.23|-6.37|\n|Inpainting|-9.32|-7.63|\n|SMCDiff+ProteinMPNN|-8.79|-6.89|\n|PROTSEED|-10.07|-7.68|\n|FrameDiff+ProteinMPNN|-9.54|-7.21|\n|RFDiffusion+ProteinMPNN|-9.76|-7.59|\n|NAEPro|**-10.14**|**-7.80**|\n\n[7] How significant is a protein structure similarity with TM-score= 0.5? Jinrui Xu and Yang Zhang. *Bioinformatics. 2010.* \n\n[8] Alphafill: enriching alphafold models with ligands and cofactors. Maarten L Hekkelman et al. Nature methods. 2023.\n\n[9] pLDDT Values in AlphaFold2 Protein Models Are Unrelated to Globular Protein Local Flexibility. Oliviero Carugo. Crystals. 2023\n\n[10] Generating new protein sequences by using dense network and attention mechanism. Feng Wang et al. Mathematical Biosciences and Engineering. 2023.\n\n[11] Using AlphaFold to predict the impact of single mutations on protein stability and function. Marina A. Pak et al. PloS one. 2023.\n\n[12] Peptide binder design with inverse folding and protein structure prediction. Patrick Bryant et al. Nature Communications Chemistry. 2023.\n\n[13] GNINA 1.0: Molecular docking with deep learning. A McNutt et al. Cheminformatics. 2021.\n\n**Q1 & Q2: \"The use of separate models for sequence and structure cannot ensure the consistency between the generated sequence and structure.\" \"knowing the topology of a protein before design process is difficult and also cannot guarantee the designed proteins have the desired functions.\" I disagree with this statement.** **I disagree with this statement.** \n\nAns: Thanks for the suggestion! We agree with the reviewer and updated the statement in our paper. \n\n**Q3: what is the meaning of \"generally-encoded\" for 20 types of amino acids?** \n\nAns: It means the 20 common amino acids. We have updated the paper accordingly.\n\n**Q4: \"The selection of motif varies from setting to setting...\". Can the authors please provide a clearer explanation?**\n\nAns: We mean the motifs have different meanings in different tasks. For de novo enzyme design [14], it means the binding sites where enzyme binds the metallocofactor and substrate. For de novo binder design [15], it means the binding sites where the protein binder binds the protein targets.\n\n[14] De novo enzyme design using rosetta3. Florian Richter et al. PloS one. 2011.\n\n[15] De novo design of protein interactions with learned surface fingerprints. Pablo Gainza. Nature. 2023.\n\n---\n\nWe thank the reviewer for the insightful questions. We have clarified all your concerns, added the experiments and updated the paper accordingly. Answers to specific points are provided below:\n\n**Weakness 1 & weakness 2 & Q5: The identified problem or the motivation for designing such a method is not supported by existing literature.  Some designed modules counter the intuition in biology, such as \"motif mining\"**\n\nAns: We follow the design insight proposed by [1], which starts from the functional site and fills in additional sequence and structure to create a viable protein scaffold in a single forward pass. Different from [1], we provide the functional sites which are automatically mined from MSA as previous methods [2, 3] while [1] provides the functional sites manually. Besides, we design a new architecture called NAEPro to generate the whole protein sequence and structure based on the given partial protein fragments, while [1] achieves this goal by finetuning the RoseTTAFold. However, we agree with the reviewer’s opinion in question 5 that MSA discovers not only protein functional sites as addressed in [2,3] but also finds conversed sites which may not directly relate to the protein function. Therefore, we change all the term “motif/functional sites” in our paper to  “meaningful protein fragments”.\n\n[1] Scaffolding protein functional sites using deep learning.  Jue wang. et al. Science. 2022.\n\n[2] Emerging methods in protein co-evolution. David de Juan et al. Nature Review Genetics. 2013.\n\n[3] Evolutionary information for specifying a protein fold. Michael Socolich et al. Nature. 2005.\n\n**Weakness 3 & Q1: The method is not 'extensively evaluated' as claimed by the authors (they only selectively analyzed 2 proteins), and many results do not suggest that the proposed method is SOTA among baselines. Moreover, the evaluation does not include all relevant methods, which makes it too aggressive for the authors to claim in the Introduction that their method \"achieves the HIGHEST...among ALL competitors\" and \"is faster than the FASTEST method\". Empirically, It has been proven by several recent research that the de novo method can generate new sequences for fixed backbone or desired functions [4-6].**\n\nAns: Sorry for the confusion. Our method achieves the best performance on 4/5 metrics on myoglobin and 3/5 metrics on beta-lactamase among all the compared representative baselines in our paper. Our model is faster than all baselines compared in our paper. We have revised all these statements in our paper (highlighted in red). For other relevant methods as the reviewer suggested in reference [4-6], we have already compared with [4]. [5] was posted on Oct. 3, 2023, which is later than the ICLR submission ddl, and [3] was posted on Aug. 14, 2023, which is within 3 months before ICLR submission ddl. We appreciate the reviewer’s suggestion, and we tried to compare with [3], but they haven’t released their code now. We emailed the authors of [3] to ask for code and they said their work is under review now and refused to provide code currently. We are trying to reimplement the model now and will update the results in our revised version.\n\n[4] Watson et al., De novo design of protein structure and function with RFdiffusion (2023).\n\n[5] Sumida et al., Improving protein expression, stability, and function with ProteinMPNN (2023).\n\n[6] Zhou et al., Conditional Protein Denoising Diffusion Generates Programmable Endonuclease Sequences (2023).\n\n---\n\nWe thank all the reviewers' valuable suggestions.\n\nWe have uploaded a revised draft that incorporates feedback from reviewers VH6U and MgyU, with the updated portions highlighted in red. Additionally, in response to reviewer Wq6M's suggestion, we have included the code and data we used in our paper in the supplementary material (Due to the maximum memory limitation -100M, we couldn’t upload the model checkpoints).\n\nHere's an overview highlighting the principal modifications to our paper:\n\n1. The revised term from “motif/functional sites” to “meaningful fragments” in abstract, introduction third paragraph, method opening paragraph, method 3.1-3.4, experiments 4.1-4.2, analysis 5.1, Conclusion (Reviewer VH6U, weakness 1 & weakness 2 & Q5)\n2. The corrected claim in abstract and last paragraph in introduction (review VH6U, weakness3) \n3. The corrected claim in Introduction second paragraph (Reviewer VH6U, Q1)\n4. The corrected claim in Related Work second paragraph (Reviewer VH6U, Q2)\n5. The clarified explanation of 20 common amino acids in Method opening paragraph(Reviewer VH6U, Q3)\n6. The corrected theoretical claim in Method 3.5 and Appendix A. 1 (reviewer MgyU, weakness 5) \n7. The clarified task definition in Method opening paragraph (reviewer MgyU, Q2)\n\nWe look forward to any additional reading and feedback. If you have any further questions, we are very happy to continue discussion!", "author_response": "Thanks for the responses! Our answers to your follow-up questions are provided as follows:\n\n**1. About Weakness4 & Q1 Page 2, \"We carry out experiments on two metalloproteins, including β-lactamase and myoglobin.\" I would like to say that the experimental setting is the main concern to me. Since the authors have only evaluated the proposed method on two structures, I am not sure if the method can be extended to generalized situations. I know that you are doing protein sequence-structure inpaintning problem, in which case a comparison with a previous baselines [1,2,3] on stantard datasets may help to reveal the generalization capabilities of your algorithm.** \n\nAns: We agree with reviewer Wq6M and additionally compare our method with PROTSEED on B12 protein. Due to the approaching ddl and limited time, PROTSEED has only been trained for 100 epochs, the same as our method. (suggested training epoch is 2000 on their GitHub)\n\nThe results are reported as follows: \n\n**Binding affinity score for designed B12**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|PROTSEED|-9.45$\\pm$ 0.03|-9.36$\\pm$0.09|-9.04$\\pm$0.27|-8.46|\n|NAEPro|**-11.06$\\pm$0.14**|**-10.96$\\pm$0.14**|**-10.61$\\pm$0.29**|**-9.11**|\n\n**2. About Q2 Your methods are not carefully designed for protein inverse folding. However, the presented results outperformed current SOTA by a large margin (curent SOTA is about 50%-60% AAR), which could not convince me. I doubt the veracity and correctness of the experimental results.** \n\nAns: Sorry for the confusion. We evaluate the inverse-folding task on our own two datasets instead of the original CATH. For our datasets, the proteins are from the same family, and thus they may have much overlap on the sequence level, which definitely will lead to high AAR. Besides, our model is initialized with ESM2 weights, which will also improve the AAR. \n\nAlthough the sequence prediction based a fixed backbone structure is not our objective, **we adapt our method to CATH setting by keeping all the CA coordinates and masking all residues. Due to the approaching ddl and limited time, our model has only been trained for 10 epochs, while usually the suggested training epoch would be 100 like GVP, PiFold, etc.** (See below: ESMIF, ProteinMPNN, PiFold results are quoted from PiFold paper). \n\nCATH 4.2 results\n\n|Model| AAR (%)|PPL |\n|:-----|:----:|:----:|\n|ProteinMPNN|45.96|4.61|\n|ESMIF|38.30|6.44|\n|PiFold|51.66|4.55|\n|NAEPro|9.16|15.26|\n\n**Again, we did not claim the superiority of our method on inverse folding task on CATH dataset.**\n\n**3. Page 1, \"Despite their great potential for novel structure design, such sequential design policy fails to cross-condition on sequence and structure, which might lead to inconsistent proteins and inefficient design process\". I can not agree with this statement. RFDiffusion's authors say that they also considered simultaneously designing structure and sequence within RFdiffusion, but combining ProteinMPNN with the diffusion of structure alone provides the excellent performance, as shown in your Table.1. Unfortunately, I observe that you deleted the consistency metric in Table.1, which seems like dishonest behavior to me.**\n\nAns: We do not intend to hide any information. As suggested by reviewer MgyU, the binding affinity is a better metric for functional protein design rather than the accuracy metrics (consistency, AAR, RMSD, etc.). Therefore, we replaced all the accuracy metrics in Table 1 and 2 in our paper according with the binding affinity scores calculated by Gnina. The full table with all scores are listed below:\n\n$\\beta$-lactamase\n\n|Model| AAR (\\%,$\\uparrow$) | RMSD ({ \\AA},$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination | $4.79$ | $--$ | $30.5511$ | $0.2918$ | $--$|\n|Inpainting | $16.73$ | $4.0599$ | $61.7679$ | $0.3790$ | $6.2578$|\n|SMCDiff+ProteinMPNN| $19.94$ | $10.3960$ | $42.0375$ | $0.3458$ |$10.2117$|\n| PROTSEED | $37.63$ | $3.0142$ | $64.3861$| $0.4637$ | $3.3748$ |\n| FrameDiff+IF |$26.20$ | $6.0151$ | $65.6445$ | $0.3657$ | $7.8703$|\n|RFDiffusion+IF | $22.93$ | $6.0438$ | **83.4058** | $0.3747$ |**0.5565**|\n|NAEPro|**65.71**| **2.9916**| 66.8705|**0.4812**| 7.3760|\n\n\n**Myoglobin**\n|Model| AAR (\\%,$\\uparrow$) | RMSD (\\AA,$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination| $4.81$ | $--$ | $38.2817$ | $0.2754$ |--|\n|Inpainting |39.59|3.3751|67.0813|0.4391 |3.2108|\n|SMCDiff+IF | $12.47$ | $8.0067$ | $34.5914$ | $0.2235$ | $8.8754$ |\n| PROTSEED | $48.55$ | $2.8753$ | $61.5588$ | $0.5466$ | $0.9764$ |\n| FrameDiff+IF | $20.43$ | $5.9739$ | $61.3945$ | $0.3757$ |$3.5078$|\n|RFDiffusion+IF | $32.70$ | $3.9930$ | $78.9868$ | $0.4147$  | **0.4375** |\n|NAEPro | **89.37**| **2.6307**|**81.9507**|**0.5692**|4.3865|"}
{"claim": "The authors can easily adapt their method to antibody design tasks, reducing the work's perceived significance.", "claim_type": "novelty", "paper_id": "Dr4qD9bzZd", "paper_title": "Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "a9hHYfEm0G", "reviewer": "Reviewer_Wq6M", "review_text": "Summary: This paper proposes NAEPro, a model to jointly design Protein sequence and structure. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from the nearest amino acids in three-dimensional (3D) space. The global attention sub-layer parameters are initialized with ESM-2. The author combines ESM2 and EGNN for co-modeling protein sequence and structure.\n\nStrengths: 1. The reported performance is good.\n2. The method is simple.\n\nWeaknesses: 1. Novelty: Both EGNN and ESM2 are existing models, and there are many existing works on antibody structure and sequence co-design. The combination may limit the novelty of this method.  \n2. Significance: It is likely that previous works can be readily applied to the motif-conditioned setting, and the authors can easily adapt their method to antibody design tasks. It would be beneficial if the authors further elaborate on the significance of their work\n3. Code: The authors do not provide code for checking the soundness of the methods.\n4. Experiment setting: The authors do not provide results on standard benchmarks, such as the CATH dataset, for fair comparison on both sequence and structure design.\n\nQuestions: 1. Could you provide experimental results from the CATH dataset to compare with the original SMCDiff and FrameDiff results?\n2. Similarly, could you provide head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design? Please follow the same setting.\n3. Could you provide the code for checking the results?", "labeling_timestamp": "2026-01-11T16:29:24.266208", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The authors explicitly disagree that their method can be readily adapted to antibody design or that prior methods can be straightforwardly applied. They argue their goal (co-design of functional and novel sequences/structures) differs, that they automatically mine meaningful fragments via MSAs (unlike prior works), and that antibody-design models are not directly applicable to their task without modification.", "evidence": "\"[1] provides the functional sites manually while we automatically mined the functional sites (plus the conserved sites as suggested by reviewer ****VH6U****) by MSAs. Previous methods considering only sequence design constrained by fitness value [2,3] or sequence design based on given backbone structure (inverse folding) [4] is not suitable for our task.\" \"For antibody design, antibody is always Y-shaped, while one of our goal is to design novel and diverse protein structure. ... From this aspect, we think antibody design models can not be directly applied to our task without any modification and vice versa.\"", "combined_author_response": "We much appreciate your valuable comments and insightful suggestions again, which helps a lot to improve the quality of our paper. As the discussion deadline is approaching, please let us know if you have any further concerns or questions. We are happy to have any further discussion!\n\n---\n\n**4. \"Through this way, amino-acid combinations frequently occur in the same context would draw higher attention scores\". Have you checked this statement? Could you provide some visualizations of the attention map and correlated frequences of amino-acid combinations?**  \n\nAns: Thank the reviewer for this suggestion. We randomly pick one sentence in the training set and visualization its attention matrix from the last layer in Appendix Figure 7(a) and also visualize the pairwise amino acid co-occurrence matrix in Figure 7(b). We find it’s consistent with our statement that residues tend to more connect to their neighboring residues. However, we don’t see any strong correlation between the attention matrix and the pairwise amino acid co-occurrence. Therefore, we revised the corresponding statement in our paper and uploaded a revised version accordingly.\n\n**5. Updating residue representations and coordinates in 3D space with only nearest neighbors enables more efficient and economic message passing compared to prior approaches which compute messages on the complete pairwise residue graph\". To my knowledge, most of the previous methods such as proteinmpnn adopt knn for constructing sparse graph in the 3D spcace. You should provide evidence to support the significance of your statement.**\n\nAns: We mean from the architecture level, previous methods like SE(3)-Transformer [1] and EGNN [2] adopts information flow from all other atoms.\n\n[1] SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks. Fabian B. Fuchs et al. NeurIPS 2020. \n\n[2] E(n) Equivariant Graph Neural Networks. Victor Garcia Satorras et al. ICML 2021. \n\n**6. Novelty: I do not observe enough novelty from the perspective of machine learning in algorithm design.**\n\nAns: Yes, from machine learning algorithm level, our method is not new. However, as admitted by reviewer MgyU, we design a new architecture for protein design. The key innovations of our architecture include: (1) interleaving layers of sequence-level attention and local neighborhood equivariant sub-layer (2) accelerating the local sub-layer with the k-nearest neighbors.\n\n---\n\nThanks for the responses! Our answers to your follow-up questions are provided as follows:\n\n**1. About Weakness4 & Q1 Page 2, \"We carry out experiments on two metalloproteins, including β-lactamase and myoglobin.\" I would like to say that the experimental setting is the main concern to me. Since the authors have only evaluated the proposed method on two structures, I am not sure if the method can be extended to generalized situations. I know that you are doing protein sequence-structure inpaintning problem, in which case a comparison with a previous baselines [1,2,3] on stantard datasets may help to reveal the generalization capabilities of your algorithm.** \n\nAns: We agree with reviewer Wq6M and additionally compare our method with PROTSEED on B12 protein. Due to the approaching ddl and limited time, PROTSEED has only been trained for 100 epochs, the same as our method. (suggested training epoch is 2000 on their GitHub)\n\nThe results are reported as follows: \n\n**Binding affinity score for designed B12**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|PROTSEED|-9.45$\\pm$ 0.03|-9.36$\\pm$0.09|-9.04$\\pm$0.27|-8.46|\n|NAEPro|**-11.06$\\pm$0.14**|**-10.96$\\pm$0.14**|**-10.61$\\pm$0.29**|**-9.11**|\n\n**2. About Q2 Your methods are not carefully designed for protein inverse folding. However, the presented results outperformed current SOTA by a large margin (curent SOTA is about 50%-60% AAR), which could not convince me. I doubt the veracity and correctness of the experimental results.** \n\nAns: Sorry for the confusion. We evaluate the inverse-folding task on our own two datasets instead of the original CATH. For our datasets, the proteins are from the same family, and thus they may have much overlap on the sequence level, which definitely will lead to high AAR. Besides, our model is initialized with ESM2 weights, which will also improve the AAR. \n\nAlthough the sequence prediction based a fixed backbone structure is not our objective, **we adapt our method to CATH setting by keeping all the CA coordinates and masking all residues. Due to the approaching ddl and limited time, our model has only been trained for 10 epochs, while usually the suggested training epoch would be 100 like GVP, PiFold, etc.** (See below: ESMIF, ProteinMPNN, PiFold results are quoted from PiFold paper). \n\nCATH 4.2 results\n\n|Model| AAR (%)|PPL |\n|:-----|:----:|:----:|\n|ProteinMPNN|45.96|4.61|\n|ESMIF|38.30|6.44|\n|PiFold|51.66|4.55|\n|NAEPro|9.16|15.26|\n\n**Again, we did not claim the superiority of our method on inverse folding task on CATH dataset.**\n\n**3. Page 1, \"Despite their great potential for novel structure design, such sequential design policy fails to cross-condition on sequence and structure, which might lead to inconsistent proteins and inefficient design process\". I can not agree with this statement. RFDiffusion's authors say that they also considered simultaneously designing structure and sequence within RFdiffusion, but combining ProteinMPNN with the diffusion of structure alone provides the excellent performance, as shown in your Table.1. Unfortunately, I observe that you deleted the consistency metric in Table.1, which seems like dishonest behavior to me.**\n\nAns: We do not intend to hide any information. As suggested by reviewer MgyU, the binding affinity is a better metric for functional protein design rather than the accuracy metrics (consistency, AAR, RMSD, etc.). Therefore, we replaced all the accuracy metrics in Table 1 and 2 in our paper according with the binding affinity scores calculated by Gnina. The full table with all scores are listed below:\n\n$\\beta$-lactamase\n\n|Model| AAR (\\%,$\\uparrow$) | RMSD ({ \\AA},$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination | $4.79$ | $--$ | $30.5511$ | $0.2918$ | $--$|\n|Inpainting | $16.73$ | $4.0599$ | $61.7679$ | $0.3790$ | $6.2578$|\n|SMCDiff+ProteinMPNN| $19.94$ | $10.3960$ | $42.0375$ | $0.3458$ |$10.2117$|\n| PROTSEED | $37.63$ | $3.0142$ | $64.3861$| $0.4637$ | $3.3748$ |\n| FrameDiff+IF |$26.20$ | $6.0151$ | $65.6445$ | $0.3657$ | $7.8703$|\n|RFDiffusion+IF | $22.93$ | $6.0438$ | **83.4058** | $0.3747$ |**0.5565**|\n|NAEPro|**65.71**| **2.9916**| 66.8705|**0.4812**| 7.3760|\n\n\n**Myoglobin**\n|Model| AAR (\\%,$\\uparrow$) | RMSD (\\AA,$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination| $4.81$ | $--$ | $38.2817$ | $0.2754$ |--|\n|Inpainting |39.59|3.3751|67.0813|0.4391 |3.2108|\n|SMCDiff+IF | $12.47$ | $8.0067$ | $34.5914$ | $0.2235$ | $8.8754$ |\n| PROTSEED | $48.55$ | $2.8753$ | $61.5588$ | $0.5466$ | $0.9764$ |\n| FrameDiff+IF | $20.43$ | $5.9739$ | $61.3945$ | $0.3757$ |$3.5078$|\n|RFDiffusion+IF | $32.70$ | $3.9930$ | $78.9868$ | $0.4147$  | **0.4375** |\n|NAEPro | **89.37**| **2.6307**|**81.9507**|**0.5692**|4.3865|\n\n---\n\nThanks for the further response! Please see our responses to your follow-up questions and concerns as follows:\n\n**Response to Weakness 3 & Q1: My point in mentioning these three papers is not to introduce additional (and possibly meaningless) in-silico comparisons to the authors. Instead, they validated their designs with solid wet-lab experiments demonstrating that it is possible to generate novel protein sequences with fixed backbones.**\n\nAns: We agree on the reviewer’s opinion and have accordingly updated the statement in Introduction.  We have uploaded a revised version of our paper.\n\n**Response to Weakness 4: I completely agree that all the existing evaluation metrics we used in deep learning are only indirect indicators, and they only imply a higher chance that the assessed protein will perform its function. However, I have trouble following the logic that \"*Therefore, a low TM-score cannot indicate the designed protein doesn’t have the corresponding function*\" suggests \"proteins with TM-scores that are lower than 0.5 (which is a wildly-used standard) are still reliable in terms of their functionality in general.\" Similarly, if we all agree that the existing metrics are suboptimal and none of them provides 100% accurate evaluations, it is then meaningless to criticize the widely applied standards with one or two counter-examples. After all, you don't have any better evidence (such as wet-lab results) that is more reliable than the existing criteria.**\n\nAns: We agree with Reviewer VH6U. As suggested by reviewer MgyU, the binding affinity is a better metric for functional protein design. We calculated the binding affinity scores for top-5, top-10, and top-30 candidates and also median score on the test set using Gnina. The results are reported as follows:\n\n**Binding affinity score for designed myoglobin**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-8.18$\\pm$ 0.01|-8.07$\\pm$0.03|-7.97$\\pm$0.23|-7.25|\n|Inpainting|-13.47$\\pm$0.02|-13.12$\\pm$0.12|-12.31$\\pm$0.54|-9.56|\n|SMCDiff+ProteinMPNN|-11.37$\\pm$0.03|-11.12$\\pm$0.31|-10.87$\\pm$0.42|-8.76|\n|PROTSEED|-13.21$\\pm$0.13|-12.89$\\pm$0.42|-11.98$\\pm$0.52|-10.23|\n|FrameDiff+ProteinMPNN|-13.13$\\pm$0.05|-12.92$\\pm$0.16|-12.21$\\pm$0.23|-10.08|\n|RFDiffusion+ProteinMPNN|-13.68$\\pm$0.02|-13.03$\\pm$0.21|-12.56$\\pm$0.43|-10.15|\n|NAEPro|**-14.12$\\pm$0.01**|**-13.85$\\pm$0.10**|**-13.06$\\pm$0.38**|**-10.74**|\n\n**Binding affinity score for designed beta-lactamase**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-6.98$\\pm$0.01|-6.87$\\pm$0.02|-6.69$\\pm$0.05|-6.29|\n|Inpainting|-9.89$\\pm$0.03|-9.54$\\pm$0.16|-9.13$\\pm$0.43|-7.24|\n|SMCDiff+ProteinMPNN|-9.10$\\pm$0.01|-9.05 $\\pm$0.02|-8.98$\\pm$0.01|-6.97|\n|PROTSEED|-9.88$\\pm$0.21|-9.51$\\pm$0.41|-9.01$\\pm$0.62|-7.31|\n|FrameDiff+ProteinMPNN|-9.54$\\pm$0.03|-9.56$\\pm$0.23|-8.89$\\pm$0.35|-7.03|\n|RFDiffusion+ProteinMPNN|-9.87$\\pm$0.05|-9.56$\\pm$0.23|-9.12$\\pm$0.53|-7.51|\n|NAEPro|**-10.06$\\pm$0.05**|**-9.79$\\pm$0.10**|**-9.39$\\pm$0.12**|**-7.66**|\n\n**Response to Q6: Because ESMFold is a less reliable tool for evaluating novel proteins, it should not be used here to assess your designs at all.**\n\nAns: We totally agree with reviewer VH6U. We provide the mean pLDDT for top-5, top-10, top-30 candidates calculated by AlphaFold2 as follows:\n\n**AlphaFold2 pLDDT for designed myoglobin**\n\n|Model| top-5|top-10|top-30|\n|:-----|:----:|:----:|:----:|\n|Hallucination|76.7243|73.5953|68.6060|\n|Inpainting|97.6171|97.6063|97.5681|\n|SMCDiff+ProteinMPNN|85.7051|84.7744|84.9220|\n|PROTSEED|92.9770|91.9216|90.4293|\n|FrameDiff+ProteinMPNN|97.3997|97.3329|96.4279|\n|RFDiffusion+ProteinMPNN|97.9722|97.9174|97.6063|\n|NAEPro|**98.1057**|**98.0706**|**97.9197**|\n\n**AlphaFold2 pLDDT for designed $\\beta$-lactamase**\n\n|Model| top-5|top-10|top-30|\n|:-----|:----:|:----:|:----:|\n|Hallucination|74.7760|74.7032|71.2932|\n|Inpainting|94.5489|94.4328|93.2941|\n|SMCDiff+ProteinMPNN|86.2510|85.7072|83.0376|\n|PROTSEED|98.2896|98.2740|97.9683|\n|FrameDiff+ProteinMPNN|98.3049|98.2922|98.1267|\n|RFDiffusion+ProteinMPNN|**98.6225**|**98.5116**|98.0643|\n|NAEPro|98.5385|98.4481|**98.2841**|\n\n---\n\nDear reviewer,\n\n    We sincerely appreciate the time and effort you dedicated to reviewing our paper and reading our responses. Your valuable suggestions and insightful comments have significantly contributed to refining the quality of our work. No matter what the final result will be, the thoughtful communication with you has better clarified our paper's goals and logic. We would like to express our great gratitude to you!\n\nBest,\n\nAuthors of paper 4608\n\n---\n\n**Could the authors briefly explain how do you construct the one-to-one mapping between the design structure and the target structure in PDB? Is it determined by retrieving the one in PDB with the smallest RMSD? As far as I know, there is no common practice to determine this for the task of protein design, and I appreciate any explanation that clues me in.**\n\nAns: Yes, we construct the one-to-one mapping between the designed structure and the target structure in PDB. It is not determined by retrieving the one in PDB with the smallest RMSD. We extract the motif fragments for each protein, based on which our training objective is to recover the original protein sequence and backbone structure. Therefore, we compute the RMSD between the designed structure and the target one which we want to recover.\n\n---\n\n**Corrected statement: But it can still bind the corresponding metallocofactor heme in the AlphaFill simulation. For the claim “binding rates of 100% and 90%”, I wonder how is the criterion/threshold of “binding” determined? This success rate is rather unrealistic and probably fails to reflect the real performance. Thus, the statement in Section 5.2. saying “It is evident that our method can generate proteins that express the basic and important functions.” can be quite misleading for readers who are not familiar with the task.** \n\nAns: Sorry for the misleading statement. We mean through the AlphaFill prediction, the designed proteins are highly potential to bind the corresponding metallocofactors. We have corrected all the claims in our paper. We provided the process of how to decide if the designed protein can “bind” the metallocofactors in Appendix B.3. We would like to further explain it here.  First of all, we use AlphaFill to do the first-step prediction. AlphaFill will return a complex based on the input protein structure. If the complex includes the corresponding metal ions, we think the designed protein is potential to have functions. Then to guarantee the results convincing, we further use some additional constraints to ensure the designed proteins are potential to express functions. Specifically, we assume if a designed protein share the similar active site environments with their natural counterparts (target protein in our dataset), then it’s highly potential to bind the corresponding metal ions. For myoglobin, we calculate the distance between axial histidine ligand to Fe ion. Besides, we also detect the presence of distal histidine within the active sites, which plays an important role in the oxygen molecule binding function of myoglobin . If the distance is between 2.0Å and 2.5Å, and the distal histidine exists, we think the designed myoglobins share similar active site environments to natural ones. For $beta$-lactamase, we detect the residues that directly contact with zinc ion. If they show similar chemistry properties with natural $beta$-lactamases, we think they can highly potentially bind the corresponding metallocofactors. For example, in Figure 5(a), the active site possesses one zinc atom coordinated to three histidines, while the second zinc atom is coordinated to one histidine, one cysteine, and one aspartate, demonstrating a high degree of amino acid analogy to the natural protein.\n\n**Could the author intuitively describe how does the NAEL deal with the reflection case so as to be simply SE(3)-equivariant instead of E(3)?** \n\nAns: Intuitively, applying reflection to the input structure, the output structure will also be reflected. The conformation of the original backbone will be changed from L-amino acid to D-amino acid (We provide an illustration in Figure 6 in appendix). Therefore, the reflection will change the chirality of the protein which may cause the deficiency of binding to the ligand and eventually its function. However, the protein sequence in our model will not be influenced. Therefore, the 3D structure of the designed sequence which is L-amino acid and the output structure  which becomes D-amino acid will be inconsistent. From this aspect, our model doesn’t satisfy reflection equivariance.\n\n**The authors are encouraged to encompass appropriate comparison with EGNN/SE(3)-transformer in their writing to better contextualize the proposed NAEL.**\n\nAns: We provide the comparison with EGNN+ESM2 on myoglobin in the following Table and also updated the results in Table2 accordingly.\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|EGNN+ESM2|-13.30$\\pm$0.61 | -12.77$\\pm$0.69 |-12.07$\\pm$0.65 | -9.63|\n|NAEPro|**-14.12$\\pm$0.01**|**-13.85$\\pm$0.10**|**-13.06$\\pm$0.38**|**-10.74**|\n\n**It is surprising to see scaling the model does not yield consistent performance gain. Could the authors help explain this behavior? Also, I noticed that the authors merge the two myoglobin and lactamase data for training newly in the rebuttal response. Is there any insight for doing such multi-task learning?**\n\nAns: Enlarging the model hasn’t improved the performance too much. Our interpretation is that the training data size is moderate, and thus a model with moderate size can fit the data well. To validate if this guess holds, we merge the two protein data and then train a unified model. As we can see in the previous table, our model did achieve obvious better results on $\\beta$-lactamase, i.e., TM-score from below 0.5 (0.4919) to over 0.5 (0.5055).\n\n---\n\nWe much appreciate the reviewer’s responses and suggestions, which help a lot to improve our paper.  We have accordingly revised our paper, including Table 1, Table 2, Section 4.2-4.4 and Section 5.2. Our responses to the reviewer’s follow-up questions are provided as follows:\n\n**About the evaluation metrics: I think the binding affinity could better support the effectiveness of NAEPro and be more suitable for the task (functional protein design) rather than the accuracy metrics (RMSD, AAR, etc.). Do the authors agree on this or not?** C**ould the authors explain how the Binding Affinity is calculated for both model based on Gnina? Specifically, I am worried that the metrics is averaged among the test set similar to other metrics.  If so, I suggest that the authors report median or top-k mean as evaluation.**\n\nAns: We completely agree with reviewer MgyU that the binding affinity is a better metric for functional protein design rather than the accuracy metrics. We revised Table 1 and 2 in our paper according to your suggestion. The binding affinity is calculated using Gnina as the average score on the test set. Thank the reviewer for this valuable suggestion. We provide  top-5, top-10, top-30 mean and variance as well as the median in the following table and also revise our paper accordingly. From the results, we can see that our model achieves the best median and top-K binding affinity scores on both datasets.\n\n**Myoglobin**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-8.18$\\pm$ 0.01|-8.07$\\pm$0.03|-7.97$\\pm$0.23|-7.25|\n|Inpainting|-13.47$\\pm$0.02|-13.12$\\pm$0.12|-12.31$\\pm$0.54|-9.56|\n|SMCDiff+ProteinMPNN|-11.37$\\pm$0.03|-11.12$\\pm$0.31|-10.87$\\pm$0.42|-8.76|\n|PROTSEED|-13.21$\\pm$0.13|-12.89$\\pm$0.42|-11.98$\\pm$0.52|-10.23|\n|FrameDiff+ProteinMPNN|-13.13$\\pm$0.05|-12.92$\\pm$0.16|-12.21$\\pm$0.23|-10.08|\n|RFDiffusion+ProteinMPNN|-13.68$\\pm$0.02|-13.03$\\pm$0.21|-12.56$\\pm$0.43|-10.15|\n|NAEPro|**-14.12$\\pm$0.01**|**-13.85$\\pm$0.10**|**-13.06$\\pm$0.38**|**-10.74**|\n\n**Beta-lactamase**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-6.98$\\pm$0.01|-6.87$\\pm$0.02|-6.69$\\pm$0.05|-6.29|\n|Inpainting|-9.89$\\pm$0.03|-9.54$\\pm$0.16|-9.13$\\pm$0.43|-7.24|\n|SMCDiff+ProteinMPNN|-9.10$\\pm$0.01|-9.05 $\\pm$0.02|-8.98$\\pm$0.01|-6.97|\n|PROTSEED|-9.88$\\pm$0.21|-9.51$\\pm$0.41|-9.01$\\pm$0.62|-7.31|\n|FrameDiff+ProteinMPNN|-9.54$\\pm$0.03|-9.56$\\pm$0.23|-8.89$\\pm$0.35|-7.03|\n|RFDiffusion+ProteinMPNN|-9.87$\\pm$0.05|-9.56$\\pm$0.23|-9.12$\\pm$0.53|-7.51|\n|NAEPro|**-10.06$\\pm$0.05**|**-9.79$\\pm$0.10**|**-9.39$\\pm$0.12**|**-7.66**|\n\n**Corrected statement: ESMFold might not accurately predict the structure for a novel sequence.**\n\nAns: Sorry for the misunderstood claim. We want to express we agree on reviewer VH6U’s comment in question 6 “for a novel sequence, its prediction is worse than AlphaFold2.”  As suggested by reviewer VH6U, we re-calculate pLDDT by AlphaFold2. Our model respectively achieves 91.4139 and 76.9550  on myoglobin and $\\beta$-lactamase on average. As claimed in previous work [1,2], pLDDT between 70~90 are classified to be confident and pLDDT ≥ 90 indicates residues predicted with extremely high confidence.\n\n[1] pLDDT Values in AlphaFold2 Protein Models Are Unrelated to Globular Protein Local Flexibility. Oliviero Carugo. Crystals. 2023.\n\n[2] Generating new protein sequences by using dense network and attention mechanism. Feng Wang et al. Mathematical Biosciences and Engineering. 2023.\n\n---\n\nThanks for your valuable review and comments again. We have addressed all your concerns and questions.\n\nIf you have any other questions, please feel free to discuss with us.\n\n---\n\nThanks for your valuable review and comments again. We have addressed all your concerns and questions.\n\nIf you have any other questions, please feel free to discuss with us.\n\n---\n\nThanks for your valuable review and comments again. We have addressed all your concerns and questions.\n\nIf you have any other questions, please feel free to discuss with us.\n\n---\n\n**Q3: In section 4.2, the author(s) described that the weights of NAEPro is partially initialized from pretrained ESM2-8M, of the smallest model size among the ESM2 series. This is very problematic but the recent common practices leverage at least the 650M model. Do the author(s) try much larger model size as initialization? I found in the Table 2 that the 8M model has only marginal improvement upon random initialization.**\n\nAns: Yes, we tried to partially initialize our model with 35M ESM2 (NAEPro-12) and also train this model on the merged myoglobin and beta-lactamase data (NAEPro-12-Unified). For myoglobin, enlarging the model to 12 layers will not influence the performance too much, but training the larger model on the merged dataset will increase the pLDDT and TM-score. For beta-lactamase, enlarging the model from 6 layers to 12 layers is beneficial and will increase the overall performance. Particularly, training the 12-layer model on merged dataset will significantly improve the performance of beta-lactamase, say improve the average TM-score from below 0.5 to above 0.5. Then we also partially initialize a 30-layer model with 150M ESM2 (NAEPro-30-Unified) and train the model on merged dataset. The performance for both myoglobin and beta-lactamase will be further improved.\n\n**myoglobin**\n\n|Model| AAR (%)|RMSD|pLDDT |TM-score |Parameters|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|NAEPro-6 |**89.37**|**2.6307**|81.9507|0.5692|11.6M|\n|NAEPro-12 |88.42|2.6333|82.2849|0.5673|52M|\n|NAEPro-12-Unified|86.81|2.6443|82.9736|0.5736|52M|\n|NAEPro-30-Unified|86.62|2.6410|**83.0173**|**0.5749**|223.7M|\n\n**$\\beta$-lactamase**\n\n|Model| AAR (%)|RMSD|pLDDT |TM-score |Parameters|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|NAEPro-6 |65.71|2.9916|66.8705|0.4812|11.6M|\n|NAEPro-12 |67.28|2.9864|68.7394|0.4919|52M|\n|NAEPro-12-Unified|70.93|**2.9707**|69.9333|0.5055|52M|\n|NAEPro-30-Unified|**72.60**|2.9781|**71.0429**|**0.5089**|223.7M|\n\n**Q4: How is the “target structure” in evaluation metrics(Section 4.2) determined? What is the difference between metrics RMSD and consistency?**\n\nAns: Target structure means the natural protein structure provided in PDB. RMSD is calculated as the RMSD between the designed protein structure and the one given in PDB. Consistency is calculated as the RMSD between the designed structure and the one predicted by ESMFold for the designed sequence.\n\nWe really hope our responses address your concerns. If you have any other questions, we are very happy to continue discussions!\n\n---\n\n**Weakness 4: For the evaluation metrics, the problem to be studied is functional protein (as it is claimed in the title) design instead of general backbone generation(eg., RFDiffusion) or sequence generation (eg., ProteinMPNN). Thus, I found the the five of the metrics fail to (or indirectly if they potentially do) reflect the performance of functional protein design or scaffolding. All these metrics in Table 1 & 2 show the somehow sequence-structure matching between the predicted and native, but are not very suitable for proteins such as enzyme.**\n\nAns: In our analysis (Section 5.2), we evaluated the function for binding the corresponding metallocofactors of the designed proteins. Specifically, we first randomly select 20 cases from the top100 sequences (pLDDT ranking). We then employ AlphaFold2 for protein structure prediction, followed by inputting these structures into AlphaFill [1] to predict the associated ligands. Notably, our results indicate that all designed myoglobins exhibit heme binding capability, while 18 beta-lactamases demonstrate the ability to bind zinc ions, resulting in metallocofactor binding rates of 100% and 90%, respectively. It is evident that our method can generate proteins that express the basic and important functions, i.e. binding the corresponding metallocofactors. To systematically illustrate the function of our designed proteins, we use docking tool Gnina[2] to compute the binding affinity between the designed metalloproteins and the corresponding metallocofactors. The results in the following table shows that our model achieves the best binding affinity scores on both metalloproteins, demonstrating the proteins designed by our model are highly potential to actively exhibit biochemical functions.\n\n|Model| myoglobin (kcal/mol)|beta-lactamase (kcal/mol)|\n|:-----|:----:|:----:|\n|Hallucination|-7.23|-6.37|\n|Inpainting|-9.32|-7.63|\n|SMCDiff+ProteinMPNN|-8.79|-6.89|\n|PROTSEED|-10.07|-7.68|\n|FrameDiff+ProteinMPNN|-9.54|-7.21|\n|RFDiffusion+ProteinMPNN|-9.76|-7.59|\n|NAEPro|**-10.14**|**-7.80**|\n\n**Weakness 5: The potential theoretical flaw in the equivariance analysis in Section 3.5.** \n\nAns: Thank the reviewer for pointing out this typo. Our model satisfies only roto-translation equivariance without reflection equivariance. We have accordingly updated the paper.\n\n**Q1: The proposed architecture encoding both sequence and structure via the masked language modeling (MLM) scheme is somehow interesting. How do the author(s) see the  difference between the proposed NAEL layer and tensor field convolutional layer in SE(3)-transformer ? Also, what is the advantage of NAEL over the SeqIPA of the PROTSEED, the most competitive baseline in Table 1?** \n\nAns: The most biggest difference between our NAEPro and tensor field convolutional layer in SE(3)-transformer is:  NAEPro uses interleaving layers of global sequence-level attention and local neighborhood equivariant sub-layer, from which the residue representation is updated from both the whole sequence interaction and 3D neighbor message passing; instead, if we enable kNN message passing in SE(3)-transformer, then the node feature will only be updated based on 3D neighboring information. To clearly demonstrate the advantage of NAEL over SeqIPA in PROTSEED, we replace our NAEL with the SeqIPA-Addition without secondary structure feature. Taking myoglobin as an example, the results are shown in the following table. NAEPro with NAEL performs better than with SeqIPA.\n\n|Model| AAR (%)|RMSD|\n|:-----|:----:|:----:|\n|NAEPro -w/- SeqIPA|65.33|2.7078|\n|NAEPro|**89.37**|**2.6307**|\n\n**Q2:  In Section 3 - opening paragraph, the author(s) define/formulate the target task as generate a protein sequence and all 3D coordinates of N residues. However, in later model definition, the proposed NAEPro only operates on the C-alpha(CA) coordinates. Please explain this ambiguity.**\n\nAns: We are sorry for the ambiguity caused. When we mention “we formulate the target task as generate a protein sequence and all 3D coordinates of N residues”, we mean 3D coordinates of alpha-carbon of all the residues. Following the setting in [3], we represent the backbone structure as the CA-only coordinates of all residues. We have updated the paper accordingly in the new version.\n\n[3] Diffusion probabilistic modeling of protein backbones in 3D for the motif-scaffolding problem. Brian L. Trippe et al. ICLR. 2023.\n\n---\n\nWe appreciate the reviewer’s valuable and insightful suggestions, which are very helpful for us to improve our paper. We have clarified all your concerns, added the experiments and updated the paper accordingly. We address the specific concerns as follows:\n\n **Weakness 1: Regarding the consistency metrics reported, I noticed that the consistency metric of NAEPro in Table 1 (as the main results) underperformed half of the baselines.  Such key observation greatly weakened the method. The authors can elaborate on this point.**\n\nAns: Yes, we agree with the reviewer’s opinion. We would like to clarify our task more clearly. Our goal is to design functional proteins with novel sequence and structure. As shown in Figure 5(b), the designed myoglobin has a RMSD of 3.943\\AA with the most similar one in PDB and 26.7% AAR with the most similar one in Uniprot, but it can still bind the corresponding metallocofactor heme in the AlphaFill simulation [1]. In our paper, we use ESMFold to predict the structure of the designed sequence. However, as pointed out by reviewer VH6U, ESMFold might not accurately predict the structure for a novel sequence. To further clarify this point, we calculate the scRMSD of motifs between the designed structure and the predicted structure from the designed sequence in the following table. As the results show, our NAEPro achieves smaller RMSD than RFDiffusion+ProteinMPNN on myoglobin and comparable results on beta-lactamase.\n\n|Model| myoglobin|beta-lactamase|\n|:-----|:----:|:----:|\n|Inpainting|0.5292|0.7819|\n|SMCDiff+ProteinMPNN|1.3816|1.4523|\n|PROTSEED|0.4987|**0.3892**|\n|FrameDiff+ProteinMPNN|0.8765|0.9815|\n|RFDiffusion+ProteinMPNN|0.4936|0.3918|\n|NAEPro|**0.4832**|0.5239|\n\n[1] Alphafill: enriching alphafold models with ligands and cofactors. Maarten L Hekkelman et al. Nature methods. 2023.\n\n**Weakness 2: For the experiments, how is the model trained to obtain the evaluation digits in Table 1 for each baseline. This is very important.** \n\nAns: We are sorry for the confusion. We got the codes of all baselines from their GitHub repos, and then re-trained all the models separately on myoglobin and beta-lactamase datasets. The evaluation procedure among all the baselines and our model follows the same process, and thus we believe the comparisons in Table 1 are fair.\n\n**Weakness 3: For the dataset, I found that in section 4.1, the author(s) mentioned that both beta-lactamase and myoglobin datasets are split randomly. This is worrying because the entries in PDB can usually be redundant, especially for such large and well-studied families that the authors selected for evaluation. The author(s) can elaborate on this point. Moreover, in the Table 3 (in appendix), I am curious about the column name “PDB”, does it mean the number of PDB entries (with unique ID) or the number of single chains?** \n\nAns: Yes, this is a very good question. Since we focus on sequence and structure co-design task, we aim to design both functional sequence and structure. Therefore, our input data is the <sequence, structure> pair. From this point, even though the sequence might be the same, the structure is somehow different. Therefore, our paired input data are different and would not be redundant. We appreciate the reviewer for carefully reviewing our paper and also reading the appendix. In Table 3, PDB means the number of different PDB entries. To eliminate the reviewer’s concerns, we re-cleaned the test set by filtering pairs whose sequences have over 30% sequence identity to any sample in the training set after doing pairwise alignment for myoglobin. We compare our NAEPro and ProtSeed (the most competitive baseline) in the following table. As the results show, our model can design proteins with higher binding affinity (by Gnina [2]) on samples that have lower similarity to training data.\n\n|Model| RMSD|Binding Affinity (kcal/mol)|\n|:-----|:----:|:----:|\n|PROTSEED|**2.4031**|-9.62|\n|NAEPro|2.6634|**-9.98**|\n\n[2] GNINA 1.0: Molecular docking with deep learning. A McNutt et al. Cheminformatics. 2021.\n\n---\n\n**Weakness4 & Q1: The authors do not provide results on standard benchmarks, such as the CATH dataset, for fair comparison on both sequence and structure design. Could you provide experimental results from the CATH dataset to compare with the original SMCDiff and FrameDiff results?**\n\nAns: We would like to clarify our task more clearly. Our task aims to co-design functional and novel protein sequence and structure. In our method, we use MSAs to automatically find motifs to guarantee the protein function, which is achieved within the same protein family. Instead, CATH is a protein dataset consisting of proteins from diverse families, making the motif extraction difficult. Besides, CATH is usually used to evaluate inverse folding task, say protein sequence design based on fixed backbone structure, which is not the goal of our paper. We hope we didn’t misunderstand the reviewer’s meaning by “the original SMCDiff and FrameDiff results” as SMCDiff and FrameDiff haven’t been evaluated on CATH dataset. Since SMCDiff and FrameDiff focus on protein structure design, CATH is usually used to evaluate protein sequence design based on fixed backbone structure, which may have some discrepancies on the design goals. \n\n**Q2: Could you provide head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design? Please follow the same setting.**\n\nAns: We provide protein sequence design given backbone structures on our two datasets in the following table. In this setting, we slightly modified our method to make it adaptable to the task as the reviewer asked. Specifically, we provided the whole backbone structure as the model input and designed the whole protein sequence. As the results show, even though we only provide  backbone structure without any motif residues, our model can still achieves higher AAR and pLDDT on myoglobin, and higher AAR on beta-lactamase.\n\n**myoglobin**\n\n|Model| AAR (%)|pLDDT |\n|:-----|:----:|:----:|\n|ProteinMPNN|81.37|81.9817|\n|ESMIF|76.49|78.0986|\n|PiFold|78.93|80.2912|\n|NAEPro|**85.56**|**82.3871**|\n\n$\\beta$-lactamase\n\n|Model| AAR (%)|pLDDT |\n|:-----|:----:|:----:|\n|ProteinMPNN|53.59|**70.6723**|\n|ESMIF|57.49|61.3928|\n|PiFold|63.38|64.9017|\n|NAEPro|**68.39**|65.8218|\n\n**Q3: Could you provide the code for checking the results?**\n\nAns: We have provided the code and data in the supplementary material. Due to the memory limitation, we couldn’t upload the model checkpoints. However, the training process is highly efficient, and the reviewer could train the model if he/she is interested.\n\n---\n\nWe thank the reviewer for the positive reviews as well as the suggestions for improvement. We have clarified all your concerns, added the experiments and updated the paper accordingly. Our responses to the reviewer’s concerns and questions are provided below:\n\n**Weakness 1: Both EGNN and ESM2 are existing models, and there are many existing works on antibody structure and sequence co-design. The combination may limit the novelty of this method.**\n\nAns: We would like to point out that our NAEPro is not a combination of ESM2 and EGNN. Instead our key innovations are (1) interleaving layers of sequence-level attention and local neighborhood equivariant sub-layer (2) accelerating the local sub-layer with the k-nearest neighbors.  On the contrary,  EGNN is a fully-connected graph and updates message, coordinates and atom features based on all other atoms in 3D space, which is not efficient for long proteins. Besides, EGNN+ESM2 is a sequential process and can not cross-condition on sequence and structure. We compare our method with EGNN+ESM2, NAEPro w/o ESM2 initialization on myoglobin in the Following Table (Table 2 in the paper).  As the results show, EGNN+ESM2 performs much worse than our NAEPro, and removing the ESM2 initialization will not influence the performance too much, demonstrating that our model is much more superior than EGNN+ESM2.\n\n|Model| AAR (%)|RMSD|pLDDT |TM-score |consistency|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|EGNN+ESM2|51.12|2.9891|77.3399|0.4656|4.9827|\n|NAEPro-w/o-ESM2 initialization|79.82|2.6398|76.3032|0.5159|4.7273|\n|NAEPro|**89.37**|**2.6307**|**81.9507**|**0.5692**|**4.3865**|\n\n**Weakness 2: It is likely that previous works can be readily applied to the motif-conditioned setting, and the authors can easily adapt their method to antibody design tasks. It would be beneficial if the authors further elaborate on the significance of their work**\n\nAns: We would like to clarify our goal more clearly. Our task is to co-design functional and novel protein sequence and structure. Previous work which is most similar to ours is Inpainting [1]. However, [1] provides the functional sites manually while we automatically mined the functional sites (plus the conserved sites as suggested by reviewer ****VH6U****) by MSAs. Previous methods considering only sequence design constrained by fitness value [2,3] or sequence design based on given backbone structure (inverse folding) [4] is not suitable for our task. For antibody design, antibody is always Y-shaped, while one of our goal is to design novel and diverse protein structure. For example, in Figure 4, the designed beta-lactamases have different fold categories and belong to three different sub-classes. In Figure 5 (b), the designed myoglobin has a RMSD of 3.943\\AA with the most similar one in PDB, but it can still bind the corresponding metallocofactor heme in the AlphaFill simulation [5]. From this aspect, we think antibody design models can not be directly applied to our task without any modification and vice versa. \n\n[1] Scaffolding protein functional sites using deep learning.  Jue wang. et al. Science. 2022.\n\n[2] Biological Sequence Design with GFlowNets. [Moksh Jain](https://arxiv.org/search/q-bio?searchtype=author&query=Jain,+M) et al. ICML 2022. \n\n[3] Proximal Exploration for Model-guided Protein Sequence Design. Zhizhou Ren et al. ICML 2022. \n\n[4] Robust deep learning based protein sequence design using ProteinMPNN. J. Dauparas et al. Science. 2022.\n\n[5] Alphafill: enriching alphafold models with ligands and cofactors. Maarten L Hekkelman et al. Nature methods. 2023.\n\n**Weakness 3:** **The authors do not provide code for checking the soundness of the methods.**\n\nAns: We have provided the code and data in the supplementary material. Due to the memory limitation, we couldn’t upload the model checkpoints. However, the training process is highly efficient, and the reviewer could train the model if he/she is interested.\n\n---\n\n**Q6: For a novel sequence, ESMFold’s prediction is worse than AlphaFold2. Consequently, evaluating novel sequences should use AlphaFold2 instead of ESMFold.**\n\nAns: Due to limited time and computing resources, we provide the AlphaFold2 results for ProtSeed, RFDiffusion+ProteinMPNN and our NAEPro. As the results show, even though ESMFold achieves lower scores than AlphaFold2, the tendencies (ranking) among different models are similar.  Particularly, our model achieves an average TM-score over 0.5 on beta-lactamase (ESMFold 0.4812) and pLDDT over 70 (ESMFold 66.8705), which gives stronger evidence that our model has the ability to design proteins which have stable structures.\n\n**myoglobin**\n\n|Model| pLDDT|TM-score|\n|:-----|:----:|:----:|\n|PROTSEED|77.9032|0.5621|\n|RFDiffusion+ProteinMPNN|88.2130|0.5103|\n|NAEPro|**91.4139**|**0.5912**|\n\n**$\\beta$-lactamase**\n\n|Model| pLDDT|TM-score|\n|:-----|:----:|:----:|\n|PROTSEED|73.3176|0.4901|\n|RFDiffusion+ProteinMPNN|**87.8092**|0.4806|\n|NAEPro|76.9550|**0.5009**|\n\n **Q7: \"Consistency - What is the designed structure and the predicted structure? Which is from NAEPro? Where does the other one come from?**\n\nAns: Consistency is calculated as the RMSD between the designed structure from NAEPro and the one predicted by ESMFold for the designed sequence from NAEPro.\n\n---\n\n**Weakness 4:  Results in Table 1 do not support the superiority of the proposed method. For instance, RMSD>1.7 \\AA means the two structures are different for myoglobin. For pLDDT, the prediction on lactamase is ~66%, which is much lower than 80%-90% which is conventionally believed reliable for folding predictions. The TM-score, if they are smaller than 0.5, then the two structures are believed different.** \n\nAns: We would like to clarify our goal more clearly.  Our task is to design the sequence and structure of proteins with effective functions.  We calculated TM-score by AlphaFold2 as suggested by the reviewer, and the two proteins designed by our model both achieves an average score over 0.5 (myoglobin **0.5912** and beta-lactamase **0.5009**). However, as found by previous method [7],  protein pairs with a TM-score >0.5 are most likely in the same fold while those with a TM-score <0.5 are mainly not in the same fold. Therefore, low TM-score can not indicate the designed protein doesn’t have the corresponding function. As claimed in previous work [9,10], pLDDT between 70~90 are classified to be confident. We calculated pLDDT by AlphaFold2 as suggested by the reviewer and the two protein families designed by our model are confident on an average aspect (myoglobin **91.4139** and beta-lactamase **76.9550**).  Similarly, as observed in previous work[11,12], pLDDT has a weak correlation with protein function. Therefore, a moderate TM-score and pLDDT can not indicate a protein expresses no or poor function. As shown in figure 5(b), the designed myoglobin has a RMSD of 3.943\\AA with the most similar one in PDB, but it can still bind the corresponding metallocofactor heme in the AlphaFill simulation [8]. To systematically illustrate the function of our designed proteins, we use docking method Gnina [13] to compute the binding affinity between the designed metalloproteins and the corresponding metallocofactors. The results in the following table shows that our model achieves the best binding affinity scores on both metalloproteins, demonstrating the proteins designed by our model are highly potential to actively exhibit biochemical functions.\n\n|Model|myoglobin (kcal/mol)|beta-lactamase (kcal/mol)|\n|:-----|:----:|:----:|\n|Hallucination|-7.23|-6.37|\n|Inpainting|-9.32|-7.63|\n|SMCDiff+ProteinMPNN|-8.79|-6.89|\n|PROTSEED|-10.07|-7.68|\n|FrameDiff+ProteinMPNN|-9.54|-7.21|\n|RFDiffusion+ProteinMPNN|-9.76|-7.59|\n|NAEPro|**-10.14**|**-7.80**|\n\n[7] How significant is a protein structure similarity with TM-score= 0.5? Jinrui Xu and Yang Zhang. *Bioinformatics. 2010.* \n\n[8] Alphafill: enriching alphafold models with ligands and cofactors. Maarten L Hekkelman et al. Nature methods. 2023.\n\n[9] pLDDT Values in AlphaFold2 Protein Models Are Unrelated to Globular Protein Local Flexibility. Oliviero Carugo. Crystals. 2023\n\n[10] Generating new protein sequences by using dense network and attention mechanism. Feng Wang et al. Mathematical Biosciences and Engineering. 2023.\n\n[11] Using AlphaFold to predict the impact of single mutations on protein stability and function. Marina A. Pak et al. PloS one. 2023.\n\n[12] Peptide binder design with inverse folding and protein structure prediction. Patrick Bryant et al. Nature Communications Chemistry. 2023.\n\n[13] GNINA 1.0: Molecular docking with deep learning. A McNutt et al. Cheminformatics. 2021.\n\n**Q1 & Q2: \"The use of separate models for sequence and structure cannot ensure the consistency between the generated sequence and structure.\" \"knowing the topology of a protein before design process is difficult and also cannot guarantee the designed proteins have the desired functions.\" I disagree with this statement.** **I disagree with this statement.** \n\nAns: Thanks for the suggestion! We agree with the reviewer and updated the statement in our paper. \n\n**Q3: what is the meaning of \"generally-encoded\" for 20 types of amino acids?** \n\nAns: It means the 20 common amino acids. We have updated the paper accordingly.\n\n**Q4: \"The selection of motif varies from setting to setting...\". Can the authors please provide a clearer explanation?**\n\nAns: We mean the motifs have different meanings in different tasks. For de novo enzyme design [14], it means the binding sites where enzyme binds the metallocofactor and substrate. For de novo binder design [15], it means the binding sites where the protein binder binds the protein targets.\n\n[14] De novo enzyme design using rosetta3. Florian Richter et al. PloS one. 2011.\n\n[15] De novo design of protein interactions with learned surface fingerprints. Pablo Gainza. Nature. 2023.\n\n---\n\nWe thank the reviewer for the insightful questions. We have clarified all your concerns, added the experiments and updated the paper accordingly. Answers to specific points are provided below:\n\n**Weakness 1 & weakness 2 & Q5: The identified problem or the motivation for designing such a method is not supported by existing literature.  Some designed modules counter the intuition in biology, such as \"motif mining\"**\n\nAns: We follow the design insight proposed by [1], which starts from the functional site and fills in additional sequence and structure to create a viable protein scaffold in a single forward pass. Different from [1], we provide the functional sites which are automatically mined from MSA as previous methods [2, 3] while [1] provides the functional sites manually. Besides, we design a new architecture called NAEPro to generate the whole protein sequence and structure based on the given partial protein fragments, while [1] achieves this goal by finetuning the RoseTTAFold. However, we agree with the reviewer’s opinion in question 5 that MSA discovers not only protein functional sites as addressed in [2,3] but also finds conversed sites which may not directly relate to the protein function. Therefore, we change all the term “motif/functional sites” in our paper to  “meaningful protein fragments”.\n\n[1] Scaffolding protein functional sites using deep learning.  Jue wang. et al. Science. 2022.\n\n[2] Emerging methods in protein co-evolution. David de Juan et al. Nature Review Genetics. 2013.\n\n[3] Evolutionary information for specifying a protein fold. Michael Socolich et al. Nature. 2005.\n\n**Weakness 3 & Q1: The method is not 'extensively evaluated' as claimed by the authors (they only selectively analyzed 2 proteins), and many results do not suggest that the proposed method is SOTA among baselines. Moreover, the evaluation does not include all relevant methods, which makes it too aggressive for the authors to claim in the Introduction that their method \"achieves the HIGHEST...among ALL competitors\" and \"is faster than the FASTEST method\". Empirically, It has been proven by several recent research that the de novo method can generate new sequences for fixed backbone or desired functions [4-6].**\n\nAns: Sorry for the confusion. Our method achieves the best performance on 4/5 metrics on myoglobin and 3/5 metrics on beta-lactamase among all the compared representative baselines in our paper. Our model is faster than all baselines compared in our paper. We have revised all these statements in our paper (highlighted in red). For other relevant methods as the reviewer suggested in reference [4-6], we have already compared with [4]. [5] was posted on Oct. 3, 2023, which is later than the ICLR submission ddl, and [3] was posted on Aug. 14, 2023, which is within 3 months before ICLR submission ddl. We appreciate the reviewer’s suggestion, and we tried to compare with [3], but they haven’t released their code now. We emailed the authors of [3] to ask for code and they said their work is under review now and refused to provide code currently. We are trying to reimplement the model now and will update the results in our revised version.\n\n[4] Watson et al., De novo design of protein structure and function with RFdiffusion (2023).\n\n[5] Sumida et al., Improving protein expression, stability, and function with ProteinMPNN (2023).\n\n[6] Zhou et al., Conditional Protein Denoising Diffusion Generates Programmable Endonuclease Sequences (2023).\n\n---\n\nWe thank all the reviewers' valuable suggestions.\n\nWe have uploaded a revised draft that incorporates feedback from reviewers VH6U and MgyU, with the updated portions highlighted in red. Additionally, in response to reviewer Wq6M's suggestion, we have included the code and data we used in our paper in the supplementary material (Due to the maximum memory limitation -100M, we couldn’t upload the model checkpoints).\n\nHere's an overview highlighting the principal modifications to our paper:\n\n1. The revised term from “motif/functional sites” to “meaningful fragments” in abstract, introduction third paragraph, method opening paragraph, method 3.1-3.4, experiments 4.1-4.2, analysis 5.1, Conclusion (Reviewer VH6U, weakness 1 & weakness 2 & Q5)\n2. The corrected claim in abstract and last paragraph in introduction (review VH6U, weakness3) \n3. The corrected claim in Introduction second paragraph (Reviewer VH6U, Q1)\n4. The corrected claim in Related Work second paragraph (Reviewer VH6U, Q2)\n5. The clarified explanation of 20 common amino acids in Method opening paragraph(Reviewer VH6U, Q3)\n6. The corrected theoretical claim in Method 3.5 and Appendix A. 1 (reviewer MgyU, weakness 5) \n7. The clarified task definition in Method opening paragraph (reviewer MgyU, Q2)\n\nWe look forward to any additional reading and feedback. If you have any further questions, we are very happy to continue discussion!", "author_response": "Thanks for the responses! Our answers to your follow-up questions are provided as follows:\n\n**1. About Weakness4 & Q1 Page 2, \"We carry out experiments on two metalloproteins, including β-lactamase and myoglobin.\" I would like to say that the experimental setting is the main concern to me. Since the authors have only evaluated the proposed method on two structures, I am not sure if the method can be extended to generalized situations. I know that you are doing protein sequence-structure inpaintning problem, in which case a comparison with a previous baselines [1,2,3] on stantard datasets may help to reveal the generalization capabilities of your algorithm.** \n\nAns: We agree with reviewer Wq6M and additionally compare our method with PROTSEED on B12 protein. Due to the approaching ddl and limited time, PROTSEED has only been trained for 100 epochs, the same as our method. (suggested training epoch is 2000 on their GitHub)\n\nThe results are reported as follows: \n\n**Binding affinity score for designed B12**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|PROTSEED|-9.45$\\pm$ 0.03|-9.36$\\pm$0.09|-9.04$\\pm$0.27|-8.46|\n|NAEPro|**-11.06$\\pm$0.14**|**-10.96$\\pm$0.14**|**-10.61$\\pm$0.29**|**-9.11**|\n\n**2. About Q2 Your methods are not carefully designed for protein inverse folding. However, the presented results outperformed current SOTA by a large margin (curent SOTA is about 50%-60% AAR), which could not convince me. I doubt the veracity and correctness of the experimental results.** \n\nAns: Sorry for the confusion. We evaluate the inverse-folding task on our own two datasets instead of the original CATH. For our datasets, the proteins are from the same family, and thus they may have much overlap on the sequence level, which definitely will lead to high AAR. Besides, our model is initialized with ESM2 weights, which will also improve the AAR. \n\nAlthough the sequence prediction based a fixed backbone structure is not our objective, **we adapt our method to CATH setting by keeping all the CA coordinates and masking all residues. Due to the approaching ddl and limited time, our model has only been trained for 10 epochs, while usually the suggested training epoch would be 100 like GVP, PiFold, etc.** (See below: ESMIF, ProteinMPNN, PiFold results are quoted from PiFold paper). \n\nCATH 4.2 results\n\n|Model| AAR (%)|PPL |\n|:-----|:----:|:----:|\n|ProteinMPNN|45.96|4.61|\n|ESMIF|38.30|6.44|\n|PiFold|51.66|4.55|\n|NAEPro|9.16|15.26|\n\n**Again, we did not claim the superiority of our method on inverse folding task on CATH dataset.**\n\n**3. Page 1, \"Despite their great potential for novel structure design, such sequential design policy fails to cross-condition on sequence and structure, which might lead to inconsistent proteins and inefficient design process\". I can not agree with this statement. RFDiffusion's authors say that they also considered simultaneously designing structure and sequence within RFdiffusion, but combining ProteinMPNN with the diffusion of structure alone provides the excellent performance, as shown in your Table.1. Unfortunately, I observe that you deleted the consistency metric in Table.1, which seems like dishonest behavior to me.**\n\nAns: We do not intend to hide any information. As suggested by reviewer MgyU, the binding affinity is a better metric for functional protein design rather than the accuracy metrics (consistency, AAR, RMSD, etc.). Therefore, we replaced all the accuracy metrics in Table 1 and 2 in our paper according with the binding affinity scores calculated by Gnina. The full table with all scores are listed below:\n\n$\\beta$-lactamase\n\n|Model| AAR (\\%,$\\uparrow$) | RMSD ({ \\AA},$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination | $4.79$ | $--$ | $30.5511$ | $0.2918$ | $--$|\n|Inpainting | $16.73$ | $4.0599$ | $61.7679$ | $0.3790$ | $6.2578$|\n|SMCDiff+ProteinMPNN| $19.94$ | $10.3960$ | $42.0375$ | $0.3458$ |$10.2117$|\n| PROTSEED | $37.63$ | $3.0142$ | $64.3861$| $0.4637$ | $3.3748$ |\n| FrameDiff+IF |$26.20$ | $6.0151$ | $65.6445$ | $0.3657$ | $7.8703$|\n|RFDiffusion+IF | $22.93$ | $6.0438$ | **83.4058** | $0.3747$ |**0.5565**|\n|NAEPro|**65.71**| **2.9916**| 66.8705|**0.4812**| 7.3760|\n\n\n**Myoglobin**\n|Model| AAR (\\%,$\\uparrow$) | RMSD (\\AA,$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination| $4.81$ | $--$ | $38.2817$ | $0.2754$ |--|\n|Inpainting |39.59|3.3751|67.0813|0.4391 |3.2108|\n|SMCDiff+IF | $12.47$ | $8.0067$ | $34.5914$ | $0.2235$ | $8.8754$ |\n| PROTSEED | $48.55$ | $2.8753$ | $61.5588$ | $0.5466$ | $0.9764$ |\n| FrameDiff+IF | $20.43$ | $5.9739$ | $61.3945$ | $0.3757$ |$3.5078$|\n|RFDiffusion+IF | $32.70$ | $3.9930$ | $78.9868$ | $0.4147$  | **0.4375** |\n|NAEPro | **89.37**| **2.6307**|**81.9507**|**0.5692**|4.3865|"}
{"claim": "The authors do not provide code for checking the soundness or reproducing the methods.", "claim_type": "methodology", "paper_id": "Dr4qD9bzZd", "paper_title": "Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "a9hHYfEm0G", "reviewer": "Reviewer_Wq6M", "review_text": "Summary: This paper proposes NAEPro, a model to jointly design Protein sequence and structure. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from the nearest amino acids in three-dimensional (3D) space. The global attention sub-layer parameters are initialized with ESM-2. The author combines ESM2 and EGNN for co-modeling protein sequence and structure.\n\nStrengths: 1. The reported performance is good.\n2. The method is simple.\n\nWeaknesses: 1. Novelty: Both EGNN and ESM2 are existing models, and there are many existing works on antibody structure and sequence co-design. The combination may limit the novelty of this method.  \n2. Significance: It is likely that previous works can be readily applied to the motif-conditioned setting, and the authors can easily adapt their method to antibody design tasks. It would be beneficial if the authors further elaborate on the significance of their work\n3. Code: The authors do not provide code for checking the soundness of the methods.\n4. Experiment setting: The authors do not provide results on standard benchmarks, such as the CATH dataset, for fair comparison on both sequence and structure design.\n\nQuestions: 1. Could you provide experimental results from the CATH dataset to compare with the original SMCDiff and FrameDiff results?\n2. Similarly, could you provide head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design? Please follow the same setting.\n3. Could you provide the code for checking the results?", "labeling_timestamp": "2026-01-11T16:28:59.626068", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors state they provided the code and data in the supplementary material (though not model checkpoints due to size limits), directly addressing the reviewer's complaint about missing code.", "evidence": "\"We have provided the code and data in the supplementary material. Due to the memory limitation, we couldn’t upload the model checkpoints. However, the training process is highly efficient, and the reviewer could train the model if he/she is interested.\"", "combined_author_response": "We much appreciate your valuable comments and insightful suggestions again, which helps a lot to improve the quality of our paper. As the discussion deadline is approaching, please let us know if you have any further concerns or questions. We are happy to have any further discussion!\n\n---\n\n**4. \"Through this way, amino-acid combinations frequently occur in the same context would draw higher attention scores\". Have you checked this statement? Could you provide some visualizations of the attention map and correlated frequences of amino-acid combinations?**  \n\nAns: Thank the reviewer for this suggestion. We randomly pick one sentence in the training set and visualization its attention matrix from the last layer in Appendix Figure 7(a) and also visualize the pairwise amino acid co-occurrence matrix in Figure 7(b). We find it’s consistent with our statement that residues tend to more connect to their neighboring residues. However, we don’t see any strong correlation between the attention matrix and the pairwise amino acid co-occurrence. Therefore, we revised the corresponding statement in our paper and uploaded a revised version accordingly.\n\n**5. Updating residue representations and coordinates in 3D space with only nearest neighbors enables more efficient and economic message passing compared to prior approaches which compute messages on the complete pairwise residue graph\". To my knowledge, most of the previous methods such as proteinmpnn adopt knn for constructing sparse graph in the 3D spcace. You should provide evidence to support the significance of your statement.**\n\nAns: We mean from the architecture level, previous methods like SE(3)-Transformer [1] and EGNN [2] adopts information flow from all other atoms.\n\n[1] SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks. Fabian B. Fuchs et al. NeurIPS 2020. \n\n[2] E(n) Equivariant Graph Neural Networks. Victor Garcia Satorras et al. ICML 2021. \n\n**6. Novelty: I do not observe enough novelty from the perspective of machine learning in algorithm design.**\n\nAns: Yes, from machine learning algorithm level, our method is not new. However, as admitted by reviewer MgyU, we design a new architecture for protein design. The key innovations of our architecture include: (1) interleaving layers of sequence-level attention and local neighborhood equivariant sub-layer (2) accelerating the local sub-layer with the k-nearest neighbors.\n\n---\n\nThanks for the responses! Our answers to your follow-up questions are provided as follows:\n\n**1. About Weakness4 & Q1 Page 2, \"We carry out experiments on two metalloproteins, including β-lactamase and myoglobin.\" I would like to say that the experimental setting is the main concern to me. Since the authors have only evaluated the proposed method on two structures, I am not sure if the method can be extended to generalized situations. I know that you are doing protein sequence-structure inpaintning problem, in which case a comparison with a previous baselines [1,2,3] on stantard datasets may help to reveal the generalization capabilities of your algorithm.** \n\nAns: We agree with reviewer Wq6M and additionally compare our method with PROTSEED on B12 protein. Due to the approaching ddl and limited time, PROTSEED has only been trained for 100 epochs, the same as our method. (suggested training epoch is 2000 on their GitHub)\n\nThe results are reported as follows: \n\n**Binding affinity score for designed B12**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|PROTSEED|-9.45$\\pm$ 0.03|-9.36$\\pm$0.09|-9.04$\\pm$0.27|-8.46|\n|NAEPro|**-11.06$\\pm$0.14**|**-10.96$\\pm$0.14**|**-10.61$\\pm$0.29**|**-9.11**|\n\n**2. About Q2 Your methods are not carefully designed for protein inverse folding. However, the presented results outperformed current SOTA by a large margin (curent SOTA is about 50%-60% AAR), which could not convince me. I doubt the veracity and correctness of the experimental results.** \n\nAns: Sorry for the confusion. We evaluate the inverse-folding task on our own two datasets instead of the original CATH. For our datasets, the proteins are from the same family, and thus they may have much overlap on the sequence level, which definitely will lead to high AAR. Besides, our model is initialized with ESM2 weights, which will also improve the AAR. \n\nAlthough the sequence prediction based a fixed backbone structure is not our objective, **we adapt our method to CATH setting by keeping all the CA coordinates and masking all residues. Due to the approaching ddl and limited time, our model has only been trained for 10 epochs, while usually the suggested training epoch would be 100 like GVP, PiFold, etc.** (See below: ESMIF, ProteinMPNN, PiFold results are quoted from PiFold paper). \n\nCATH 4.2 results\n\n|Model| AAR (%)|PPL |\n|:-----|:----:|:----:|\n|ProteinMPNN|45.96|4.61|\n|ESMIF|38.30|6.44|\n|PiFold|51.66|4.55|\n|NAEPro|9.16|15.26|\n\n**Again, we did not claim the superiority of our method on inverse folding task on CATH dataset.**\n\n**3. Page 1, \"Despite their great potential for novel structure design, such sequential design policy fails to cross-condition on sequence and structure, which might lead to inconsistent proteins and inefficient design process\". I can not agree with this statement. RFDiffusion's authors say that they also considered simultaneously designing structure and sequence within RFdiffusion, but combining ProteinMPNN with the diffusion of structure alone provides the excellent performance, as shown in your Table.1. Unfortunately, I observe that you deleted the consistency metric in Table.1, which seems like dishonest behavior to me.**\n\nAns: We do not intend to hide any information. As suggested by reviewer MgyU, the binding affinity is a better metric for functional protein design rather than the accuracy metrics (consistency, AAR, RMSD, etc.). Therefore, we replaced all the accuracy metrics in Table 1 and 2 in our paper according with the binding affinity scores calculated by Gnina. The full table with all scores are listed below:\n\n$\\beta$-lactamase\n\n|Model| AAR (\\%,$\\uparrow$) | RMSD ({ \\AA},$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination | $4.79$ | $--$ | $30.5511$ | $0.2918$ | $--$|\n|Inpainting | $16.73$ | $4.0599$ | $61.7679$ | $0.3790$ | $6.2578$|\n|SMCDiff+ProteinMPNN| $19.94$ | $10.3960$ | $42.0375$ | $0.3458$ |$10.2117$|\n| PROTSEED | $37.63$ | $3.0142$ | $64.3861$| $0.4637$ | $3.3748$ |\n| FrameDiff+IF |$26.20$ | $6.0151$ | $65.6445$ | $0.3657$ | $7.8703$|\n|RFDiffusion+IF | $22.93$ | $6.0438$ | **83.4058** | $0.3747$ |**0.5565**|\n|NAEPro|**65.71**| **2.9916**| 66.8705|**0.4812**| 7.3760|\n\n\n**Myoglobin**\n|Model| AAR (\\%,$\\uparrow$) | RMSD (\\AA,$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination| $4.81$ | $--$ | $38.2817$ | $0.2754$ |--|\n|Inpainting |39.59|3.3751|67.0813|0.4391 |3.2108|\n|SMCDiff+IF | $12.47$ | $8.0067$ | $34.5914$ | $0.2235$ | $8.8754$ |\n| PROTSEED | $48.55$ | $2.8753$ | $61.5588$ | $0.5466$ | $0.9764$ |\n| FrameDiff+IF | $20.43$ | $5.9739$ | $61.3945$ | $0.3757$ |$3.5078$|\n|RFDiffusion+IF | $32.70$ | $3.9930$ | $78.9868$ | $0.4147$  | **0.4375** |\n|NAEPro | **89.37**| **2.6307**|**81.9507**|**0.5692**|4.3865|\n\n---\n\nThanks for the further response! Please see our responses to your follow-up questions and concerns as follows:\n\n**Response to Weakness 3 & Q1: My point in mentioning these three papers is not to introduce additional (and possibly meaningless) in-silico comparisons to the authors. Instead, they validated their designs with solid wet-lab experiments demonstrating that it is possible to generate novel protein sequences with fixed backbones.**\n\nAns: We agree on the reviewer’s opinion and have accordingly updated the statement in Introduction.  We have uploaded a revised version of our paper.\n\n**Response to Weakness 4: I completely agree that all the existing evaluation metrics we used in deep learning are only indirect indicators, and they only imply a higher chance that the assessed protein will perform its function. However, I have trouble following the logic that \"*Therefore, a low TM-score cannot indicate the designed protein doesn’t have the corresponding function*\" suggests \"proteins with TM-scores that are lower than 0.5 (which is a wildly-used standard) are still reliable in terms of their functionality in general.\" Similarly, if we all agree that the existing metrics are suboptimal and none of them provides 100% accurate evaluations, it is then meaningless to criticize the widely applied standards with one or two counter-examples. After all, you don't have any better evidence (such as wet-lab results) that is more reliable than the existing criteria.**\n\nAns: We agree with Reviewer VH6U. As suggested by reviewer MgyU, the binding affinity is a better metric for functional protein design. We calculated the binding affinity scores for top-5, top-10, and top-30 candidates and also median score on the test set using Gnina. The results are reported as follows:\n\n**Binding affinity score for designed myoglobin**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-8.18$\\pm$ 0.01|-8.07$\\pm$0.03|-7.97$\\pm$0.23|-7.25|\n|Inpainting|-13.47$\\pm$0.02|-13.12$\\pm$0.12|-12.31$\\pm$0.54|-9.56|\n|SMCDiff+ProteinMPNN|-11.37$\\pm$0.03|-11.12$\\pm$0.31|-10.87$\\pm$0.42|-8.76|\n|PROTSEED|-13.21$\\pm$0.13|-12.89$\\pm$0.42|-11.98$\\pm$0.52|-10.23|\n|FrameDiff+ProteinMPNN|-13.13$\\pm$0.05|-12.92$\\pm$0.16|-12.21$\\pm$0.23|-10.08|\n|RFDiffusion+ProteinMPNN|-13.68$\\pm$0.02|-13.03$\\pm$0.21|-12.56$\\pm$0.43|-10.15|\n|NAEPro|**-14.12$\\pm$0.01**|**-13.85$\\pm$0.10**|**-13.06$\\pm$0.38**|**-10.74**|\n\n**Binding affinity score for designed beta-lactamase**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-6.98$\\pm$0.01|-6.87$\\pm$0.02|-6.69$\\pm$0.05|-6.29|\n|Inpainting|-9.89$\\pm$0.03|-9.54$\\pm$0.16|-9.13$\\pm$0.43|-7.24|\n|SMCDiff+ProteinMPNN|-9.10$\\pm$0.01|-9.05 $\\pm$0.02|-8.98$\\pm$0.01|-6.97|\n|PROTSEED|-9.88$\\pm$0.21|-9.51$\\pm$0.41|-9.01$\\pm$0.62|-7.31|\n|FrameDiff+ProteinMPNN|-9.54$\\pm$0.03|-9.56$\\pm$0.23|-8.89$\\pm$0.35|-7.03|\n|RFDiffusion+ProteinMPNN|-9.87$\\pm$0.05|-9.56$\\pm$0.23|-9.12$\\pm$0.53|-7.51|\n|NAEPro|**-10.06$\\pm$0.05**|**-9.79$\\pm$0.10**|**-9.39$\\pm$0.12**|**-7.66**|\n\n**Response to Q6: Because ESMFold is a less reliable tool for evaluating novel proteins, it should not be used here to assess your designs at all.**\n\nAns: We totally agree with reviewer VH6U. We provide the mean pLDDT for top-5, top-10, top-30 candidates calculated by AlphaFold2 as follows:\n\n**AlphaFold2 pLDDT for designed myoglobin**\n\n|Model| top-5|top-10|top-30|\n|:-----|:----:|:----:|:----:|\n|Hallucination|76.7243|73.5953|68.6060|\n|Inpainting|97.6171|97.6063|97.5681|\n|SMCDiff+ProteinMPNN|85.7051|84.7744|84.9220|\n|PROTSEED|92.9770|91.9216|90.4293|\n|FrameDiff+ProteinMPNN|97.3997|97.3329|96.4279|\n|RFDiffusion+ProteinMPNN|97.9722|97.9174|97.6063|\n|NAEPro|**98.1057**|**98.0706**|**97.9197**|\n\n**AlphaFold2 pLDDT for designed $\\beta$-lactamase**\n\n|Model| top-5|top-10|top-30|\n|:-----|:----:|:----:|:----:|\n|Hallucination|74.7760|74.7032|71.2932|\n|Inpainting|94.5489|94.4328|93.2941|\n|SMCDiff+ProteinMPNN|86.2510|85.7072|83.0376|\n|PROTSEED|98.2896|98.2740|97.9683|\n|FrameDiff+ProteinMPNN|98.3049|98.2922|98.1267|\n|RFDiffusion+ProteinMPNN|**98.6225**|**98.5116**|98.0643|\n|NAEPro|98.5385|98.4481|**98.2841**|\n\n---\n\nDear reviewer,\n\n    We sincerely appreciate the time and effort you dedicated to reviewing our paper and reading our responses. Your valuable suggestions and insightful comments have significantly contributed to refining the quality of our work. No matter what the final result will be, the thoughtful communication with you has better clarified our paper's goals and logic. We would like to express our great gratitude to you!\n\nBest,\n\nAuthors of paper 4608\n\n---\n\n**Could the authors briefly explain how do you construct the one-to-one mapping between the design structure and the target structure in PDB? Is it determined by retrieving the one in PDB with the smallest RMSD? As far as I know, there is no common practice to determine this for the task of protein design, and I appreciate any explanation that clues me in.**\n\nAns: Yes, we construct the one-to-one mapping between the designed structure and the target structure in PDB. It is not determined by retrieving the one in PDB with the smallest RMSD. We extract the motif fragments for each protein, based on which our training objective is to recover the original protein sequence and backbone structure. Therefore, we compute the RMSD between the designed structure and the target one which we want to recover.\n\n---\n\n**Corrected statement: But it can still bind the corresponding metallocofactor heme in the AlphaFill simulation. For the claim “binding rates of 100% and 90%”, I wonder how is the criterion/threshold of “binding” determined? This success rate is rather unrealistic and probably fails to reflect the real performance. Thus, the statement in Section 5.2. saying “It is evident that our method can generate proteins that express the basic and important functions.” can be quite misleading for readers who are not familiar with the task.** \n\nAns: Sorry for the misleading statement. We mean through the AlphaFill prediction, the designed proteins are highly potential to bind the corresponding metallocofactors. We have corrected all the claims in our paper. We provided the process of how to decide if the designed protein can “bind” the metallocofactors in Appendix B.3. We would like to further explain it here.  First of all, we use AlphaFill to do the first-step prediction. AlphaFill will return a complex based on the input protein structure. If the complex includes the corresponding metal ions, we think the designed protein is potential to have functions. Then to guarantee the results convincing, we further use some additional constraints to ensure the designed proteins are potential to express functions. Specifically, we assume if a designed protein share the similar active site environments with their natural counterparts (target protein in our dataset), then it’s highly potential to bind the corresponding metal ions. For myoglobin, we calculate the distance between axial histidine ligand to Fe ion. Besides, we also detect the presence of distal histidine within the active sites, which plays an important role in the oxygen molecule binding function of myoglobin . If the distance is between 2.0Å and 2.5Å, and the distal histidine exists, we think the designed myoglobins share similar active site environments to natural ones. For $beta$-lactamase, we detect the residues that directly contact with zinc ion. If they show similar chemistry properties with natural $beta$-lactamases, we think they can highly potentially bind the corresponding metallocofactors. For example, in Figure 5(a), the active site possesses one zinc atom coordinated to three histidines, while the second zinc atom is coordinated to one histidine, one cysteine, and one aspartate, demonstrating a high degree of amino acid analogy to the natural protein.\n\n**Could the author intuitively describe how does the NAEL deal with the reflection case so as to be simply SE(3)-equivariant instead of E(3)?** \n\nAns: Intuitively, applying reflection to the input structure, the output structure will also be reflected. The conformation of the original backbone will be changed from L-amino acid to D-amino acid (We provide an illustration in Figure 6 in appendix). Therefore, the reflection will change the chirality of the protein which may cause the deficiency of binding to the ligand and eventually its function. However, the protein sequence in our model will not be influenced. Therefore, the 3D structure of the designed sequence which is L-amino acid and the output structure  which becomes D-amino acid will be inconsistent. From this aspect, our model doesn’t satisfy reflection equivariance.\n\n**The authors are encouraged to encompass appropriate comparison with EGNN/SE(3)-transformer in their writing to better contextualize the proposed NAEL.**\n\nAns: We provide the comparison with EGNN+ESM2 on myoglobin in the following Table and also updated the results in Table2 accordingly.\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|EGNN+ESM2|-13.30$\\pm$0.61 | -12.77$\\pm$0.69 |-12.07$\\pm$0.65 | -9.63|\n|NAEPro|**-14.12$\\pm$0.01**|**-13.85$\\pm$0.10**|**-13.06$\\pm$0.38**|**-10.74**|\n\n**It is surprising to see scaling the model does not yield consistent performance gain. Could the authors help explain this behavior? Also, I noticed that the authors merge the two myoglobin and lactamase data for training newly in the rebuttal response. Is there any insight for doing such multi-task learning?**\n\nAns: Enlarging the model hasn’t improved the performance too much. Our interpretation is that the training data size is moderate, and thus a model with moderate size can fit the data well. To validate if this guess holds, we merge the two protein data and then train a unified model. As we can see in the previous table, our model did achieve obvious better results on $\\beta$-lactamase, i.e., TM-score from below 0.5 (0.4919) to over 0.5 (0.5055).\n\n---\n\nWe much appreciate the reviewer’s responses and suggestions, which help a lot to improve our paper.  We have accordingly revised our paper, including Table 1, Table 2, Section 4.2-4.4 and Section 5.2. Our responses to the reviewer’s follow-up questions are provided as follows:\n\n**About the evaluation metrics: I think the binding affinity could better support the effectiveness of NAEPro and be more suitable for the task (functional protein design) rather than the accuracy metrics (RMSD, AAR, etc.). Do the authors agree on this or not?** C**ould the authors explain how the Binding Affinity is calculated for both model based on Gnina? Specifically, I am worried that the metrics is averaged among the test set similar to other metrics.  If so, I suggest that the authors report median or top-k mean as evaluation.**\n\nAns: We completely agree with reviewer MgyU that the binding affinity is a better metric for functional protein design rather than the accuracy metrics. We revised Table 1 and 2 in our paper according to your suggestion. The binding affinity is calculated using Gnina as the average score on the test set. Thank the reviewer for this valuable suggestion. We provide  top-5, top-10, top-30 mean and variance as well as the median in the following table and also revise our paper accordingly. From the results, we can see that our model achieves the best median and top-K binding affinity scores on both datasets.\n\n**Myoglobin**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-8.18$\\pm$ 0.01|-8.07$\\pm$0.03|-7.97$\\pm$0.23|-7.25|\n|Inpainting|-13.47$\\pm$0.02|-13.12$\\pm$0.12|-12.31$\\pm$0.54|-9.56|\n|SMCDiff+ProteinMPNN|-11.37$\\pm$0.03|-11.12$\\pm$0.31|-10.87$\\pm$0.42|-8.76|\n|PROTSEED|-13.21$\\pm$0.13|-12.89$\\pm$0.42|-11.98$\\pm$0.52|-10.23|\n|FrameDiff+ProteinMPNN|-13.13$\\pm$0.05|-12.92$\\pm$0.16|-12.21$\\pm$0.23|-10.08|\n|RFDiffusion+ProteinMPNN|-13.68$\\pm$0.02|-13.03$\\pm$0.21|-12.56$\\pm$0.43|-10.15|\n|NAEPro|**-14.12$\\pm$0.01**|**-13.85$\\pm$0.10**|**-13.06$\\pm$0.38**|**-10.74**|\n\n**Beta-lactamase**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-6.98$\\pm$0.01|-6.87$\\pm$0.02|-6.69$\\pm$0.05|-6.29|\n|Inpainting|-9.89$\\pm$0.03|-9.54$\\pm$0.16|-9.13$\\pm$0.43|-7.24|\n|SMCDiff+ProteinMPNN|-9.10$\\pm$0.01|-9.05 $\\pm$0.02|-8.98$\\pm$0.01|-6.97|\n|PROTSEED|-9.88$\\pm$0.21|-9.51$\\pm$0.41|-9.01$\\pm$0.62|-7.31|\n|FrameDiff+ProteinMPNN|-9.54$\\pm$0.03|-9.56$\\pm$0.23|-8.89$\\pm$0.35|-7.03|\n|RFDiffusion+ProteinMPNN|-9.87$\\pm$0.05|-9.56$\\pm$0.23|-9.12$\\pm$0.53|-7.51|\n|NAEPro|**-10.06$\\pm$0.05**|**-9.79$\\pm$0.10**|**-9.39$\\pm$0.12**|**-7.66**|\n\n**Corrected statement: ESMFold might not accurately predict the structure for a novel sequence.**\n\nAns: Sorry for the misunderstood claim. We want to express we agree on reviewer VH6U’s comment in question 6 “for a novel sequence, its prediction is worse than AlphaFold2.”  As suggested by reviewer VH6U, we re-calculate pLDDT by AlphaFold2. Our model respectively achieves 91.4139 and 76.9550  on myoglobin and $\\beta$-lactamase on average. As claimed in previous work [1,2], pLDDT between 70~90 are classified to be confident and pLDDT ≥ 90 indicates residues predicted with extremely high confidence.\n\n[1] pLDDT Values in AlphaFold2 Protein Models Are Unrelated to Globular Protein Local Flexibility. Oliviero Carugo. Crystals. 2023.\n\n[2] Generating new protein sequences by using dense network and attention mechanism. Feng Wang et al. Mathematical Biosciences and Engineering. 2023.\n\n---\n\nThanks for your valuable review and comments again. We have addressed all your concerns and questions.\n\nIf you have any other questions, please feel free to discuss with us.\n\n---\n\nThanks for your valuable review and comments again. We have addressed all your concerns and questions.\n\nIf you have any other questions, please feel free to discuss with us.\n\n---\n\nThanks for your valuable review and comments again. We have addressed all your concerns and questions.\n\nIf you have any other questions, please feel free to discuss with us.\n\n---\n\n**Q3: In section 4.2, the author(s) described that the weights of NAEPro is partially initialized from pretrained ESM2-8M, of the smallest model size among the ESM2 series. This is very problematic but the recent common practices leverage at least the 650M model. Do the author(s) try much larger model size as initialization? I found in the Table 2 that the 8M model has only marginal improvement upon random initialization.**\n\nAns: Yes, we tried to partially initialize our model with 35M ESM2 (NAEPro-12) and also train this model on the merged myoglobin and beta-lactamase data (NAEPro-12-Unified). For myoglobin, enlarging the model to 12 layers will not influence the performance too much, but training the larger model on the merged dataset will increase the pLDDT and TM-score. For beta-lactamase, enlarging the model from 6 layers to 12 layers is beneficial and will increase the overall performance. Particularly, training the 12-layer model on merged dataset will significantly improve the performance of beta-lactamase, say improve the average TM-score from below 0.5 to above 0.5. Then we also partially initialize a 30-layer model with 150M ESM2 (NAEPro-30-Unified) and train the model on merged dataset. The performance for both myoglobin and beta-lactamase will be further improved.\n\n**myoglobin**\n\n|Model| AAR (%)|RMSD|pLDDT |TM-score |Parameters|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|NAEPro-6 |**89.37**|**2.6307**|81.9507|0.5692|11.6M|\n|NAEPro-12 |88.42|2.6333|82.2849|0.5673|52M|\n|NAEPro-12-Unified|86.81|2.6443|82.9736|0.5736|52M|\n|NAEPro-30-Unified|86.62|2.6410|**83.0173**|**0.5749**|223.7M|\n\n**$\\beta$-lactamase**\n\n|Model| AAR (%)|RMSD|pLDDT |TM-score |Parameters|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|NAEPro-6 |65.71|2.9916|66.8705|0.4812|11.6M|\n|NAEPro-12 |67.28|2.9864|68.7394|0.4919|52M|\n|NAEPro-12-Unified|70.93|**2.9707**|69.9333|0.5055|52M|\n|NAEPro-30-Unified|**72.60**|2.9781|**71.0429**|**0.5089**|223.7M|\n\n**Q4: How is the “target structure” in evaluation metrics(Section 4.2) determined? What is the difference between metrics RMSD and consistency?**\n\nAns: Target structure means the natural protein structure provided in PDB. RMSD is calculated as the RMSD between the designed protein structure and the one given in PDB. Consistency is calculated as the RMSD between the designed structure and the one predicted by ESMFold for the designed sequence.\n\nWe really hope our responses address your concerns. If you have any other questions, we are very happy to continue discussions!\n\n---\n\n**Weakness 4: For the evaluation metrics, the problem to be studied is functional protein (as it is claimed in the title) design instead of general backbone generation(eg., RFDiffusion) or sequence generation (eg., ProteinMPNN). Thus, I found the the five of the metrics fail to (or indirectly if they potentially do) reflect the performance of functional protein design or scaffolding. All these metrics in Table 1 & 2 show the somehow sequence-structure matching between the predicted and native, but are not very suitable for proteins such as enzyme.**\n\nAns: In our analysis (Section 5.2), we evaluated the function for binding the corresponding metallocofactors of the designed proteins. Specifically, we first randomly select 20 cases from the top100 sequences (pLDDT ranking). We then employ AlphaFold2 for protein structure prediction, followed by inputting these structures into AlphaFill [1] to predict the associated ligands. Notably, our results indicate that all designed myoglobins exhibit heme binding capability, while 18 beta-lactamases demonstrate the ability to bind zinc ions, resulting in metallocofactor binding rates of 100% and 90%, respectively. It is evident that our method can generate proteins that express the basic and important functions, i.e. binding the corresponding metallocofactors. To systematically illustrate the function of our designed proteins, we use docking tool Gnina[2] to compute the binding affinity between the designed metalloproteins and the corresponding metallocofactors. The results in the following table shows that our model achieves the best binding affinity scores on both metalloproteins, demonstrating the proteins designed by our model are highly potential to actively exhibit biochemical functions.\n\n|Model| myoglobin (kcal/mol)|beta-lactamase (kcal/mol)|\n|:-----|:----:|:----:|\n|Hallucination|-7.23|-6.37|\n|Inpainting|-9.32|-7.63|\n|SMCDiff+ProteinMPNN|-8.79|-6.89|\n|PROTSEED|-10.07|-7.68|\n|FrameDiff+ProteinMPNN|-9.54|-7.21|\n|RFDiffusion+ProteinMPNN|-9.76|-7.59|\n|NAEPro|**-10.14**|**-7.80**|\n\n**Weakness 5: The potential theoretical flaw in the equivariance analysis in Section 3.5.** \n\nAns: Thank the reviewer for pointing out this typo. Our model satisfies only roto-translation equivariance without reflection equivariance. We have accordingly updated the paper.\n\n**Q1: The proposed architecture encoding both sequence and structure via the masked language modeling (MLM) scheme is somehow interesting. How do the author(s) see the  difference between the proposed NAEL layer and tensor field convolutional layer in SE(3)-transformer ? Also, what is the advantage of NAEL over the SeqIPA of the PROTSEED, the most competitive baseline in Table 1?** \n\nAns: The most biggest difference between our NAEPro and tensor field convolutional layer in SE(3)-transformer is:  NAEPro uses interleaving layers of global sequence-level attention and local neighborhood equivariant sub-layer, from which the residue representation is updated from both the whole sequence interaction and 3D neighbor message passing; instead, if we enable kNN message passing in SE(3)-transformer, then the node feature will only be updated based on 3D neighboring information. To clearly demonstrate the advantage of NAEL over SeqIPA in PROTSEED, we replace our NAEL with the SeqIPA-Addition without secondary structure feature. Taking myoglobin as an example, the results are shown in the following table. NAEPro with NAEL performs better than with SeqIPA.\n\n|Model| AAR (%)|RMSD|\n|:-----|:----:|:----:|\n|NAEPro -w/- SeqIPA|65.33|2.7078|\n|NAEPro|**89.37**|**2.6307**|\n\n**Q2:  In Section 3 - opening paragraph, the author(s) define/formulate the target task as generate a protein sequence and all 3D coordinates of N residues. However, in later model definition, the proposed NAEPro only operates on the C-alpha(CA) coordinates. Please explain this ambiguity.**\n\nAns: We are sorry for the ambiguity caused. When we mention “we formulate the target task as generate a protein sequence and all 3D coordinates of N residues”, we mean 3D coordinates of alpha-carbon of all the residues. Following the setting in [3], we represent the backbone structure as the CA-only coordinates of all residues. We have updated the paper accordingly in the new version.\n\n[3] Diffusion probabilistic modeling of protein backbones in 3D for the motif-scaffolding problem. Brian L. Trippe et al. ICLR. 2023.\n\n---\n\nWe appreciate the reviewer’s valuable and insightful suggestions, which are very helpful for us to improve our paper. We have clarified all your concerns, added the experiments and updated the paper accordingly. We address the specific concerns as follows:\n\n **Weakness 1: Regarding the consistency metrics reported, I noticed that the consistency metric of NAEPro in Table 1 (as the main results) underperformed half of the baselines.  Such key observation greatly weakened the method. The authors can elaborate on this point.**\n\nAns: Yes, we agree with the reviewer’s opinion. We would like to clarify our task more clearly. Our goal is to design functional proteins with novel sequence and structure. As shown in Figure 5(b), the designed myoglobin has a RMSD of 3.943\\AA with the most similar one in PDB and 26.7% AAR with the most similar one in Uniprot, but it can still bind the corresponding metallocofactor heme in the AlphaFill simulation [1]. In our paper, we use ESMFold to predict the structure of the designed sequence. However, as pointed out by reviewer VH6U, ESMFold might not accurately predict the structure for a novel sequence. To further clarify this point, we calculate the scRMSD of motifs between the designed structure and the predicted structure from the designed sequence in the following table. As the results show, our NAEPro achieves smaller RMSD than RFDiffusion+ProteinMPNN on myoglobin and comparable results on beta-lactamase.\n\n|Model| myoglobin|beta-lactamase|\n|:-----|:----:|:----:|\n|Inpainting|0.5292|0.7819|\n|SMCDiff+ProteinMPNN|1.3816|1.4523|\n|PROTSEED|0.4987|**0.3892**|\n|FrameDiff+ProteinMPNN|0.8765|0.9815|\n|RFDiffusion+ProteinMPNN|0.4936|0.3918|\n|NAEPro|**0.4832**|0.5239|\n\n[1] Alphafill: enriching alphafold models with ligands and cofactors. Maarten L Hekkelman et al. Nature methods. 2023.\n\n**Weakness 2: For the experiments, how is the model trained to obtain the evaluation digits in Table 1 for each baseline. This is very important.** \n\nAns: We are sorry for the confusion. We got the codes of all baselines from their GitHub repos, and then re-trained all the models separately on myoglobin and beta-lactamase datasets. The evaluation procedure among all the baselines and our model follows the same process, and thus we believe the comparisons in Table 1 are fair.\n\n**Weakness 3: For the dataset, I found that in section 4.1, the author(s) mentioned that both beta-lactamase and myoglobin datasets are split randomly. This is worrying because the entries in PDB can usually be redundant, especially for such large and well-studied families that the authors selected for evaluation. The author(s) can elaborate on this point. Moreover, in the Table 3 (in appendix), I am curious about the column name “PDB”, does it mean the number of PDB entries (with unique ID) or the number of single chains?** \n\nAns: Yes, this is a very good question. Since we focus on sequence and structure co-design task, we aim to design both functional sequence and structure. Therefore, our input data is the <sequence, structure> pair. From this point, even though the sequence might be the same, the structure is somehow different. Therefore, our paired input data are different and would not be redundant. We appreciate the reviewer for carefully reviewing our paper and also reading the appendix. In Table 3, PDB means the number of different PDB entries. To eliminate the reviewer’s concerns, we re-cleaned the test set by filtering pairs whose sequences have over 30% sequence identity to any sample in the training set after doing pairwise alignment for myoglobin. We compare our NAEPro and ProtSeed (the most competitive baseline) in the following table. As the results show, our model can design proteins with higher binding affinity (by Gnina [2]) on samples that have lower similarity to training data.\n\n|Model| RMSD|Binding Affinity (kcal/mol)|\n|:-----|:----:|:----:|\n|PROTSEED|**2.4031**|-9.62|\n|NAEPro|2.6634|**-9.98**|\n\n[2] GNINA 1.0: Molecular docking with deep learning. A McNutt et al. Cheminformatics. 2021.\n\n---\n\n**Weakness4 & Q1: The authors do not provide results on standard benchmarks, such as the CATH dataset, for fair comparison on both sequence and structure design. Could you provide experimental results from the CATH dataset to compare with the original SMCDiff and FrameDiff results?**\n\nAns: We would like to clarify our task more clearly. Our task aims to co-design functional and novel protein sequence and structure. In our method, we use MSAs to automatically find motifs to guarantee the protein function, which is achieved within the same protein family. Instead, CATH is a protein dataset consisting of proteins from diverse families, making the motif extraction difficult. Besides, CATH is usually used to evaluate inverse folding task, say protein sequence design based on fixed backbone structure, which is not the goal of our paper. We hope we didn’t misunderstand the reviewer’s meaning by “the original SMCDiff and FrameDiff results” as SMCDiff and FrameDiff haven’t been evaluated on CATH dataset. Since SMCDiff and FrameDiff focus on protein structure design, CATH is usually used to evaluate protein sequence design based on fixed backbone structure, which may have some discrepancies on the design goals. \n\n**Q2: Could you provide head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design? Please follow the same setting.**\n\nAns: We provide protein sequence design given backbone structures on our two datasets in the following table. In this setting, we slightly modified our method to make it adaptable to the task as the reviewer asked. Specifically, we provided the whole backbone structure as the model input and designed the whole protein sequence. As the results show, even though we only provide  backbone structure without any motif residues, our model can still achieves higher AAR and pLDDT on myoglobin, and higher AAR on beta-lactamase.\n\n**myoglobin**\n\n|Model| AAR (%)|pLDDT |\n|:-----|:----:|:----:|\n|ProteinMPNN|81.37|81.9817|\n|ESMIF|76.49|78.0986|\n|PiFold|78.93|80.2912|\n|NAEPro|**85.56**|**82.3871**|\n\n$\\beta$-lactamase\n\n|Model| AAR (%)|pLDDT |\n|:-----|:----:|:----:|\n|ProteinMPNN|53.59|**70.6723**|\n|ESMIF|57.49|61.3928|\n|PiFold|63.38|64.9017|\n|NAEPro|**68.39**|65.8218|\n\n**Q3: Could you provide the code for checking the results?**\n\nAns: We have provided the code and data in the supplementary material. Due to the memory limitation, we couldn’t upload the model checkpoints. However, the training process is highly efficient, and the reviewer could train the model if he/she is interested.\n\n---\n\nWe thank the reviewer for the positive reviews as well as the suggestions for improvement. We have clarified all your concerns, added the experiments and updated the paper accordingly. Our responses to the reviewer’s concerns and questions are provided below:\n\n**Weakness 1: Both EGNN and ESM2 are existing models, and there are many existing works on antibody structure and sequence co-design. The combination may limit the novelty of this method.**\n\nAns: We would like to point out that our NAEPro is not a combination of ESM2 and EGNN. Instead our key innovations are (1) interleaving layers of sequence-level attention and local neighborhood equivariant sub-layer (2) accelerating the local sub-layer with the k-nearest neighbors.  On the contrary,  EGNN is a fully-connected graph and updates message, coordinates and atom features based on all other atoms in 3D space, which is not efficient for long proteins. Besides, EGNN+ESM2 is a sequential process and can not cross-condition on sequence and structure. We compare our method with EGNN+ESM2, NAEPro w/o ESM2 initialization on myoglobin in the Following Table (Table 2 in the paper).  As the results show, EGNN+ESM2 performs much worse than our NAEPro, and removing the ESM2 initialization will not influence the performance too much, demonstrating that our model is much more superior than EGNN+ESM2.\n\n|Model| AAR (%)|RMSD|pLDDT |TM-score |consistency|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|EGNN+ESM2|51.12|2.9891|77.3399|0.4656|4.9827|\n|NAEPro-w/o-ESM2 initialization|79.82|2.6398|76.3032|0.5159|4.7273|\n|NAEPro|**89.37**|**2.6307**|**81.9507**|**0.5692**|**4.3865**|\n\n**Weakness 2: It is likely that previous works can be readily applied to the motif-conditioned setting, and the authors can easily adapt their method to antibody design tasks. It would be beneficial if the authors further elaborate on the significance of their work**\n\nAns: We would like to clarify our goal more clearly. Our task is to co-design functional and novel protein sequence and structure. Previous work which is most similar to ours is Inpainting [1]. However, [1] provides the functional sites manually while we automatically mined the functional sites (plus the conserved sites as suggested by reviewer ****VH6U****) by MSAs. Previous methods considering only sequence design constrained by fitness value [2,3] or sequence design based on given backbone structure (inverse folding) [4] is not suitable for our task. For antibody design, antibody is always Y-shaped, while one of our goal is to design novel and diverse protein structure. For example, in Figure 4, the designed beta-lactamases have different fold categories and belong to three different sub-classes. In Figure 5 (b), the designed myoglobin has a RMSD of 3.943\\AA with the most similar one in PDB, but it can still bind the corresponding metallocofactor heme in the AlphaFill simulation [5]. From this aspect, we think antibody design models can not be directly applied to our task without any modification and vice versa. \n\n[1] Scaffolding protein functional sites using deep learning.  Jue wang. et al. Science. 2022.\n\n[2] Biological Sequence Design with GFlowNets. [Moksh Jain](https://arxiv.org/search/q-bio?searchtype=author&query=Jain,+M) et al. ICML 2022. \n\n[3] Proximal Exploration for Model-guided Protein Sequence Design. Zhizhou Ren et al. ICML 2022. \n\n[4] Robust deep learning based protein sequence design using ProteinMPNN. J. Dauparas et al. Science. 2022.\n\n[5] Alphafill: enriching alphafold models with ligands and cofactors. Maarten L Hekkelman et al. Nature methods. 2023.\n\n**Weakness 3:** **The authors do not provide code for checking the soundness of the methods.**\n\nAns: We have provided the code and data in the supplementary material. Due to the memory limitation, we couldn’t upload the model checkpoints. However, the training process is highly efficient, and the reviewer could train the model if he/she is interested.\n\n---\n\n**Q6: For a novel sequence, ESMFold’s prediction is worse than AlphaFold2. Consequently, evaluating novel sequences should use AlphaFold2 instead of ESMFold.**\n\nAns: Due to limited time and computing resources, we provide the AlphaFold2 results for ProtSeed, RFDiffusion+ProteinMPNN and our NAEPro. As the results show, even though ESMFold achieves lower scores than AlphaFold2, the tendencies (ranking) among different models are similar.  Particularly, our model achieves an average TM-score over 0.5 on beta-lactamase (ESMFold 0.4812) and pLDDT over 70 (ESMFold 66.8705), which gives stronger evidence that our model has the ability to design proteins which have stable structures.\n\n**myoglobin**\n\n|Model| pLDDT|TM-score|\n|:-----|:----:|:----:|\n|PROTSEED|77.9032|0.5621|\n|RFDiffusion+ProteinMPNN|88.2130|0.5103|\n|NAEPro|**91.4139**|**0.5912**|\n\n**$\\beta$-lactamase**\n\n|Model| pLDDT|TM-score|\n|:-----|:----:|:----:|\n|PROTSEED|73.3176|0.4901|\n|RFDiffusion+ProteinMPNN|**87.8092**|0.4806|\n|NAEPro|76.9550|**0.5009**|\n\n **Q7: \"Consistency - What is the designed structure and the predicted structure? Which is from NAEPro? Where does the other one come from?**\n\nAns: Consistency is calculated as the RMSD between the designed structure from NAEPro and the one predicted by ESMFold for the designed sequence from NAEPro.\n\n---\n\n**Weakness 4:  Results in Table 1 do not support the superiority of the proposed method. For instance, RMSD>1.7 \\AA means the two structures are different for myoglobin. For pLDDT, the prediction on lactamase is ~66%, which is much lower than 80%-90% which is conventionally believed reliable for folding predictions. The TM-score, if they are smaller than 0.5, then the two structures are believed different.** \n\nAns: We would like to clarify our goal more clearly.  Our task is to design the sequence and structure of proteins with effective functions.  We calculated TM-score by AlphaFold2 as suggested by the reviewer, and the two proteins designed by our model both achieves an average score over 0.5 (myoglobin **0.5912** and beta-lactamase **0.5009**). However, as found by previous method [7],  protein pairs with a TM-score >0.5 are most likely in the same fold while those with a TM-score <0.5 are mainly not in the same fold. Therefore, low TM-score can not indicate the designed protein doesn’t have the corresponding function. As claimed in previous work [9,10], pLDDT between 70~90 are classified to be confident. We calculated pLDDT by AlphaFold2 as suggested by the reviewer and the two protein families designed by our model are confident on an average aspect (myoglobin **91.4139** and beta-lactamase **76.9550**).  Similarly, as observed in previous work[11,12], pLDDT has a weak correlation with protein function. Therefore, a moderate TM-score and pLDDT can not indicate a protein expresses no or poor function. As shown in figure 5(b), the designed myoglobin has a RMSD of 3.943\\AA with the most similar one in PDB, but it can still bind the corresponding metallocofactor heme in the AlphaFill simulation [8]. To systematically illustrate the function of our designed proteins, we use docking method Gnina [13] to compute the binding affinity between the designed metalloproteins and the corresponding metallocofactors. The results in the following table shows that our model achieves the best binding affinity scores on both metalloproteins, demonstrating the proteins designed by our model are highly potential to actively exhibit biochemical functions.\n\n|Model|myoglobin (kcal/mol)|beta-lactamase (kcal/mol)|\n|:-----|:----:|:----:|\n|Hallucination|-7.23|-6.37|\n|Inpainting|-9.32|-7.63|\n|SMCDiff+ProteinMPNN|-8.79|-6.89|\n|PROTSEED|-10.07|-7.68|\n|FrameDiff+ProteinMPNN|-9.54|-7.21|\n|RFDiffusion+ProteinMPNN|-9.76|-7.59|\n|NAEPro|**-10.14**|**-7.80**|\n\n[7] How significant is a protein structure similarity with TM-score= 0.5? Jinrui Xu and Yang Zhang. *Bioinformatics. 2010.* \n\n[8] Alphafill: enriching alphafold models with ligands and cofactors. Maarten L Hekkelman et al. Nature methods. 2023.\n\n[9] pLDDT Values in AlphaFold2 Protein Models Are Unrelated to Globular Protein Local Flexibility. Oliviero Carugo. Crystals. 2023\n\n[10] Generating new protein sequences by using dense network and attention mechanism. Feng Wang et al. Mathematical Biosciences and Engineering. 2023.\n\n[11] Using AlphaFold to predict the impact of single mutations on protein stability and function. Marina A. Pak et al. PloS one. 2023.\n\n[12] Peptide binder design with inverse folding and protein structure prediction. Patrick Bryant et al. Nature Communications Chemistry. 2023.\n\n[13] GNINA 1.0: Molecular docking with deep learning. A McNutt et al. Cheminformatics. 2021.\n\n**Q1 & Q2: \"The use of separate models for sequence and structure cannot ensure the consistency between the generated sequence and structure.\" \"knowing the topology of a protein before design process is difficult and also cannot guarantee the designed proteins have the desired functions.\" I disagree with this statement.** **I disagree with this statement.** \n\nAns: Thanks for the suggestion! We agree with the reviewer and updated the statement in our paper. \n\n**Q3: what is the meaning of \"generally-encoded\" for 20 types of amino acids?** \n\nAns: It means the 20 common amino acids. We have updated the paper accordingly.\n\n**Q4: \"The selection of motif varies from setting to setting...\". Can the authors please provide a clearer explanation?**\n\nAns: We mean the motifs have different meanings in different tasks. For de novo enzyme design [14], it means the binding sites where enzyme binds the metallocofactor and substrate. For de novo binder design [15], it means the binding sites where the protein binder binds the protein targets.\n\n[14] De novo enzyme design using rosetta3. Florian Richter et al. PloS one. 2011.\n\n[15] De novo design of protein interactions with learned surface fingerprints. Pablo Gainza. Nature. 2023.\n\n---\n\nWe thank the reviewer for the insightful questions. We have clarified all your concerns, added the experiments and updated the paper accordingly. Answers to specific points are provided below:\n\n**Weakness 1 & weakness 2 & Q5: The identified problem or the motivation for designing such a method is not supported by existing literature.  Some designed modules counter the intuition in biology, such as \"motif mining\"**\n\nAns: We follow the design insight proposed by [1], which starts from the functional site and fills in additional sequence and structure to create a viable protein scaffold in a single forward pass. Different from [1], we provide the functional sites which are automatically mined from MSA as previous methods [2, 3] while [1] provides the functional sites manually. Besides, we design a new architecture called NAEPro to generate the whole protein sequence and structure based on the given partial protein fragments, while [1] achieves this goal by finetuning the RoseTTAFold. However, we agree with the reviewer’s opinion in question 5 that MSA discovers not only protein functional sites as addressed in [2,3] but also finds conversed sites which may not directly relate to the protein function. Therefore, we change all the term “motif/functional sites” in our paper to  “meaningful protein fragments”.\n\n[1] Scaffolding protein functional sites using deep learning.  Jue wang. et al. Science. 2022.\n\n[2] Emerging methods in protein co-evolution. David de Juan et al. Nature Review Genetics. 2013.\n\n[3] Evolutionary information for specifying a protein fold. Michael Socolich et al. Nature. 2005.\n\n**Weakness 3 & Q1: The method is not 'extensively evaluated' as claimed by the authors (they only selectively analyzed 2 proteins), and many results do not suggest that the proposed method is SOTA among baselines. Moreover, the evaluation does not include all relevant methods, which makes it too aggressive for the authors to claim in the Introduction that their method \"achieves the HIGHEST...among ALL competitors\" and \"is faster than the FASTEST method\". Empirically, It has been proven by several recent research that the de novo method can generate new sequences for fixed backbone or desired functions [4-6].**\n\nAns: Sorry for the confusion. Our method achieves the best performance on 4/5 metrics on myoglobin and 3/5 metrics on beta-lactamase among all the compared representative baselines in our paper. Our model is faster than all baselines compared in our paper. We have revised all these statements in our paper (highlighted in red). For other relevant methods as the reviewer suggested in reference [4-6], we have already compared with [4]. [5] was posted on Oct. 3, 2023, which is later than the ICLR submission ddl, and [3] was posted on Aug. 14, 2023, which is within 3 months before ICLR submission ddl. We appreciate the reviewer’s suggestion, and we tried to compare with [3], but they haven’t released their code now. We emailed the authors of [3] to ask for code and they said their work is under review now and refused to provide code currently. We are trying to reimplement the model now and will update the results in our revised version.\n\n[4] Watson et al., De novo design of protein structure and function with RFdiffusion (2023).\n\n[5] Sumida et al., Improving protein expression, stability, and function with ProteinMPNN (2023).\n\n[6] Zhou et al., Conditional Protein Denoising Diffusion Generates Programmable Endonuclease Sequences (2023).\n\n---\n\nWe thank all the reviewers' valuable suggestions.\n\nWe have uploaded a revised draft that incorporates feedback from reviewers VH6U and MgyU, with the updated portions highlighted in red. Additionally, in response to reviewer Wq6M's suggestion, we have included the code and data we used in our paper in the supplementary material (Due to the maximum memory limitation -100M, we couldn’t upload the model checkpoints).\n\nHere's an overview highlighting the principal modifications to our paper:\n\n1. The revised term from “motif/functional sites” to “meaningful fragments” in abstract, introduction third paragraph, method opening paragraph, method 3.1-3.4, experiments 4.1-4.2, analysis 5.1, Conclusion (Reviewer VH6U, weakness 1 & weakness 2 & Q5)\n2. The corrected claim in abstract and last paragraph in introduction (review VH6U, weakness3) \n3. The corrected claim in Introduction second paragraph (Reviewer VH6U, Q1)\n4. The corrected claim in Related Work second paragraph (Reviewer VH6U, Q2)\n5. The clarified explanation of 20 common amino acids in Method opening paragraph(Reviewer VH6U, Q3)\n6. The corrected theoretical claim in Method 3.5 and Appendix A. 1 (reviewer MgyU, weakness 5) \n7. The clarified task definition in Method opening paragraph (reviewer MgyU, Q2)\n\nWe look forward to any additional reading and feedback. If you have any further questions, we are very happy to continue discussion!", "author_response": "Thanks for the responses! Our answers to your follow-up questions are provided as follows:\n\n**1. About Weakness4 & Q1 Page 2, \"We carry out experiments on two metalloproteins, including β-lactamase and myoglobin.\" I would like to say that the experimental setting is the main concern to me. Since the authors have only evaluated the proposed method on two structures, I am not sure if the method can be extended to generalized situations. I know that you are doing protein sequence-structure inpaintning problem, in which case a comparison with a previous baselines [1,2,3] on stantard datasets may help to reveal the generalization capabilities of your algorithm.** \n\nAns: We agree with reviewer Wq6M and additionally compare our method with PROTSEED on B12 protein. Due to the approaching ddl and limited time, PROTSEED has only been trained for 100 epochs, the same as our method. (suggested training epoch is 2000 on their GitHub)\n\nThe results are reported as follows: \n\n**Binding affinity score for designed B12**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|PROTSEED|-9.45$\\pm$ 0.03|-9.36$\\pm$0.09|-9.04$\\pm$0.27|-8.46|\n|NAEPro|**-11.06$\\pm$0.14**|**-10.96$\\pm$0.14**|**-10.61$\\pm$0.29**|**-9.11**|\n\n**2. About Q2 Your methods are not carefully designed for protein inverse folding. However, the presented results outperformed current SOTA by a large margin (curent SOTA is about 50%-60% AAR), which could not convince me. I doubt the veracity and correctness of the experimental results.** \n\nAns: Sorry for the confusion. We evaluate the inverse-folding task on our own two datasets instead of the original CATH. For our datasets, the proteins are from the same family, and thus they may have much overlap on the sequence level, which definitely will lead to high AAR. Besides, our model is initialized with ESM2 weights, which will also improve the AAR. \n\nAlthough the sequence prediction based a fixed backbone structure is not our objective, **we adapt our method to CATH setting by keeping all the CA coordinates and masking all residues. Due to the approaching ddl and limited time, our model has only been trained for 10 epochs, while usually the suggested training epoch would be 100 like GVP, PiFold, etc.** (See below: ESMIF, ProteinMPNN, PiFold results are quoted from PiFold paper). \n\nCATH 4.2 results\n\n|Model| AAR (%)|PPL |\n|:-----|:----:|:----:|\n|ProteinMPNN|45.96|4.61|\n|ESMIF|38.30|6.44|\n|PiFold|51.66|4.55|\n|NAEPro|9.16|15.26|\n\n**Again, we did not claim the superiority of our method on inverse folding task on CATH dataset.**\n\n**3. Page 1, \"Despite their great potential for novel structure design, such sequential design policy fails to cross-condition on sequence and structure, which might lead to inconsistent proteins and inefficient design process\". I can not agree with this statement. RFDiffusion's authors say that they also considered simultaneously designing structure and sequence within RFdiffusion, but combining ProteinMPNN with the diffusion of structure alone provides the excellent performance, as shown in your Table.1. Unfortunately, I observe that you deleted the consistency metric in Table.1, which seems like dishonest behavior to me.**\n\nAns: We do not intend to hide any information. As suggested by reviewer MgyU, the binding affinity is a better metric for functional protein design rather than the accuracy metrics (consistency, AAR, RMSD, etc.). Therefore, we replaced all the accuracy metrics in Table 1 and 2 in our paper according with the binding affinity scores calculated by Gnina. The full table with all scores are listed below:\n\n$\\beta$-lactamase\n\n|Model| AAR (\\%,$\\uparrow$) | RMSD ({ \\AA},$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination | $4.79$ | $--$ | $30.5511$ | $0.2918$ | $--$|\n|Inpainting | $16.73$ | $4.0599$ | $61.7679$ | $0.3790$ | $6.2578$|\n|SMCDiff+ProteinMPNN| $19.94$ | $10.3960$ | $42.0375$ | $0.3458$ |$10.2117$|\n| PROTSEED | $37.63$ | $3.0142$ | $64.3861$| $0.4637$ | $3.3748$ |\n| FrameDiff+IF |$26.20$ | $6.0151$ | $65.6445$ | $0.3657$ | $7.8703$|\n|RFDiffusion+IF | $22.93$ | $6.0438$ | **83.4058** | $0.3747$ |**0.5565**|\n|NAEPro|**65.71**| **2.9916**| 66.8705|**0.4812**| 7.3760|\n\n\n**Myoglobin**\n|Model| AAR (\\%,$\\uparrow$) | RMSD (\\AA,$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination| $4.81$ | $--$ | $38.2817$ | $0.2754$ |--|\n|Inpainting |39.59|3.3751|67.0813|0.4391 |3.2108|\n|SMCDiff+IF | $12.47$ | $8.0067$ | $34.5914$ | $0.2235$ | $8.8754$ |\n| PROTSEED | $48.55$ | $2.8753$ | $61.5588$ | $0.5466$ | $0.9764$ |\n| FrameDiff+IF | $20.43$ | $5.9739$ | $61.3945$ | $0.3757$ |$3.5078$|\n|RFDiffusion+IF | $32.70$ | $3.9930$ | $78.9868$ | $0.4147$  | **0.4375** |\n|NAEPro | **89.37**| **2.6307**|**81.9507**|**0.5692**|4.3865|"}
{"claim": "The paper does not report results on standard benchmarks such as the CATH dataset for fair comparison on sequence and structure design.", "claim_type": "experimental", "paper_id": "Dr4qD9bzZd", "paper_title": "Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "a9hHYfEm0G", "reviewer": "Reviewer_Wq6M", "review_text": "Summary: This paper proposes NAEPro, a model to jointly design Protein sequence and structure. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from the nearest amino acids in three-dimensional (3D) space. The global attention sub-layer parameters are initialized with ESM-2. The author combines ESM2 and EGNN for co-modeling protein sequence and structure.\n\nStrengths: 1. The reported performance is good.\n2. The method is simple.\n\nWeaknesses: 1. Novelty: Both EGNN and ESM2 are existing models, and there are many existing works on antibody structure and sequence co-design. The combination may limit the novelty of this method.  \n2. Significance: It is likely that previous works can be readily applied to the motif-conditioned setting, and the authors can easily adapt their method to antibody design tasks. It would be beneficial if the authors further elaborate on the significance of their work\n3. Code: The authors do not provide code for checking the soundness of the methods.\n4. Experiment setting: The authors do not provide results on standard benchmarks, such as the CATH dataset, for fair comparison on both sequence and structure design.\n\nQuestions: 1. Could you provide experimental results from the CATH dataset to compare with the original SMCDiff and FrameDiff results?\n2. Similarly, could you provide head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design? Please follow the same setting.\n3. Could you provide the code for checking the results?", "labeling_timestamp": "2026-01-11T16:29:05.017289", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "Authors argue CATH is not an appropriate benchmark for their motif-based, within-family functional co-design task and thus justify not using it, but they also ran a limited adaptation to CATH (10 epochs) and reported results, showing they partially addressed the reviewer's request while explaining limitations.", "evidence": "\"Our task aims to co-design functional and novel protein sequence and structure. ... CATH is a protein dataset consisting of proteins from diverse families, making the motif extraction difficult. Besides, CATH is usually used to evaluate inverse folding task ... which is not the goal of our paper.\"  ... \"we adapt our method to CATH setting by keeping all the CA coordinates and masking all residues. Due to the approaching ddl and limited time, our model has only been trained for 10 epochs, while usually the suggested training epoch would be 100 like GVP, PiFold, etc.\"", "combined_author_response": "We much appreciate your valuable comments and insightful suggestions again, which helps a lot to improve the quality of our paper. As the discussion deadline is approaching, please let us know if you have any further concerns or questions. We are happy to have any further discussion!\n\n---\n\n**4. \"Through this way, amino-acid combinations frequently occur in the same context would draw higher attention scores\". Have you checked this statement? Could you provide some visualizations of the attention map and correlated frequences of amino-acid combinations?**  \n\nAns: Thank the reviewer for this suggestion. We randomly pick one sentence in the training set and visualization its attention matrix from the last layer in Appendix Figure 7(a) and also visualize the pairwise amino acid co-occurrence matrix in Figure 7(b). We find it’s consistent with our statement that residues tend to more connect to their neighboring residues. However, we don’t see any strong correlation between the attention matrix and the pairwise amino acid co-occurrence. Therefore, we revised the corresponding statement in our paper and uploaded a revised version accordingly.\n\n**5. Updating residue representations and coordinates in 3D space with only nearest neighbors enables more efficient and economic message passing compared to prior approaches which compute messages on the complete pairwise residue graph\". To my knowledge, most of the previous methods such as proteinmpnn adopt knn for constructing sparse graph in the 3D spcace. You should provide evidence to support the significance of your statement.**\n\nAns: We mean from the architecture level, previous methods like SE(3)-Transformer [1] and EGNN [2] adopts information flow from all other atoms.\n\n[1] SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks. Fabian B. Fuchs et al. NeurIPS 2020. \n\n[2] E(n) Equivariant Graph Neural Networks. Victor Garcia Satorras et al. ICML 2021. \n\n**6. Novelty: I do not observe enough novelty from the perspective of machine learning in algorithm design.**\n\nAns: Yes, from machine learning algorithm level, our method is not new. However, as admitted by reviewer MgyU, we design a new architecture for protein design. The key innovations of our architecture include: (1) interleaving layers of sequence-level attention and local neighborhood equivariant sub-layer (2) accelerating the local sub-layer with the k-nearest neighbors.\n\n---\n\nThanks for the responses! Our answers to your follow-up questions are provided as follows:\n\n**1. About Weakness4 & Q1 Page 2, \"We carry out experiments on two metalloproteins, including β-lactamase and myoglobin.\" I would like to say that the experimental setting is the main concern to me. Since the authors have only evaluated the proposed method on two structures, I am not sure if the method can be extended to generalized situations. I know that you are doing protein sequence-structure inpaintning problem, in which case a comparison with a previous baselines [1,2,3] on stantard datasets may help to reveal the generalization capabilities of your algorithm.** \n\nAns: We agree with reviewer Wq6M and additionally compare our method with PROTSEED on B12 protein. Due to the approaching ddl and limited time, PROTSEED has only been trained for 100 epochs, the same as our method. (suggested training epoch is 2000 on their GitHub)\n\nThe results are reported as follows: \n\n**Binding affinity score for designed B12**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|PROTSEED|-9.45$\\pm$ 0.03|-9.36$\\pm$0.09|-9.04$\\pm$0.27|-8.46|\n|NAEPro|**-11.06$\\pm$0.14**|**-10.96$\\pm$0.14**|**-10.61$\\pm$0.29**|**-9.11**|\n\n**2. About Q2 Your methods are not carefully designed for protein inverse folding. However, the presented results outperformed current SOTA by a large margin (curent SOTA is about 50%-60% AAR), which could not convince me. I doubt the veracity and correctness of the experimental results.** \n\nAns: Sorry for the confusion. We evaluate the inverse-folding task on our own two datasets instead of the original CATH. For our datasets, the proteins are from the same family, and thus they may have much overlap on the sequence level, which definitely will lead to high AAR. Besides, our model is initialized with ESM2 weights, which will also improve the AAR. \n\nAlthough the sequence prediction based a fixed backbone structure is not our objective, **we adapt our method to CATH setting by keeping all the CA coordinates and masking all residues. Due to the approaching ddl and limited time, our model has only been trained for 10 epochs, while usually the suggested training epoch would be 100 like GVP, PiFold, etc.** (See below: ESMIF, ProteinMPNN, PiFold results are quoted from PiFold paper). \n\nCATH 4.2 results\n\n|Model| AAR (%)|PPL |\n|:-----|:----:|:----:|\n|ProteinMPNN|45.96|4.61|\n|ESMIF|38.30|6.44|\n|PiFold|51.66|4.55|\n|NAEPro|9.16|15.26|\n\n**Again, we did not claim the superiority of our method on inverse folding task on CATH dataset.**\n\n**3. Page 1, \"Despite their great potential for novel structure design, such sequential design policy fails to cross-condition on sequence and structure, which might lead to inconsistent proteins and inefficient design process\". I can not agree with this statement. RFDiffusion's authors say that they also considered simultaneously designing structure and sequence within RFdiffusion, but combining ProteinMPNN with the diffusion of structure alone provides the excellent performance, as shown in your Table.1. Unfortunately, I observe that you deleted the consistency metric in Table.1, which seems like dishonest behavior to me.**\n\nAns: We do not intend to hide any information. As suggested by reviewer MgyU, the binding affinity is a better metric for functional protein design rather than the accuracy metrics (consistency, AAR, RMSD, etc.). Therefore, we replaced all the accuracy metrics in Table 1 and 2 in our paper according with the binding affinity scores calculated by Gnina. The full table with all scores are listed below:\n\n$\\beta$-lactamase\n\n|Model| AAR (\\%,$\\uparrow$) | RMSD ({ \\AA},$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination | $4.79$ | $--$ | $30.5511$ | $0.2918$ | $--$|\n|Inpainting | $16.73$ | $4.0599$ | $61.7679$ | $0.3790$ | $6.2578$|\n|SMCDiff+ProteinMPNN| $19.94$ | $10.3960$ | $42.0375$ | $0.3458$ |$10.2117$|\n| PROTSEED | $37.63$ | $3.0142$ | $64.3861$| $0.4637$ | $3.3748$ |\n| FrameDiff+IF |$26.20$ | $6.0151$ | $65.6445$ | $0.3657$ | $7.8703$|\n|RFDiffusion+IF | $22.93$ | $6.0438$ | **83.4058** | $0.3747$ |**0.5565**|\n|NAEPro|**65.71**| **2.9916**| 66.8705|**0.4812**| 7.3760|\n\n\n**Myoglobin**\n|Model| AAR (\\%,$\\uparrow$) | RMSD (\\AA,$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination| $4.81$ | $--$ | $38.2817$ | $0.2754$ |--|\n|Inpainting |39.59|3.3751|67.0813|0.4391 |3.2108|\n|SMCDiff+IF | $12.47$ | $8.0067$ | $34.5914$ | $0.2235$ | $8.8754$ |\n| PROTSEED | $48.55$ | $2.8753$ | $61.5588$ | $0.5466$ | $0.9764$ |\n| FrameDiff+IF | $20.43$ | $5.9739$ | $61.3945$ | $0.3757$ |$3.5078$|\n|RFDiffusion+IF | $32.70$ | $3.9930$ | $78.9868$ | $0.4147$  | **0.4375** |\n|NAEPro | **89.37**| **2.6307**|**81.9507**|**0.5692**|4.3865|\n\n---\n\nThanks for the further response! Please see our responses to your follow-up questions and concerns as follows:\n\n**Response to Weakness 3 & Q1: My point in mentioning these three papers is not to introduce additional (and possibly meaningless) in-silico comparisons to the authors. Instead, they validated their designs with solid wet-lab experiments demonstrating that it is possible to generate novel protein sequences with fixed backbones.**\n\nAns: We agree on the reviewer’s opinion and have accordingly updated the statement in Introduction.  We have uploaded a revised version of our paper.\n\n**Response to Weakness 4: I completely agree that all the existing evaluation metrics we used in deep learning are only indirect indicators, and they only imply a higher chance that the assessed protein will perform its function. However, I have trouble following the logic that \"*Therefore, a low TM-score cannot indicate the designed protein doesn’t have the corresponding function*\" suggests \"proteins with TM-scores that are lower than 0.5 (which is a wildly-used standard) are still reliable in terms of their functionality in general.\" Similarly, if we all agree that the existing metrics are suboptimal and none of them provides 100% accurate evaluations, it is then meaningless to criticize the widely applied standards with one or two counter-examples. After all, you don't have any better evidence (such as wet-lab results) that is more reliable than the existing criteria.**\n\nAns: We agree with Reviewer VH6U. As suggested by reviewer MgyU, the binding affinity is a better metric for functional protein design. We calculated the binding affinity scores for top-5, top-10, and top-30 candidates and also median score on the test set using Gnina. The results are reported as follows:\n\n**Binding affinity score for designed myoglobin**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-8.18$\\pm$ 0.01|-8.07$\\pm$0.03|-7.97$\\pm$0.23|-7.25|\n|Inpainting|-13.47$\\pm$0.02|-13.12$\\pm$0.12|-12.31$\\pm$0.54|-9.56|\n|SMCDiff+ProteinMPNN|-11.37$\\pm$0.03|-11.12$\\pm$0.31|-10.87$\\pm$0.42|-8.76|\n|PROTSEED|-13.21$\\pm$0.13|-12.89$\\pm$0.42|-11.98$\\pm$0.52|-10.23|\n|FrameDiff+ProteinMPNN|-13.13$\\pm$0.05|-12.92$\\pm$0.16|-12.21$\\pm$0.23|-10.08|\n|RFDiffusion+ProteinMPNN|-13.68$\\pm$0.02|-13.03$\\pm$0.21|-12.56$\\pm$0.43|-10.15|\n|NAEPro|**-14.12$\\pm$0.01**|**-13.85$\\pm$0.10**|**-13.06$\\pm$0.38**|**-10.74**|\n\n**Binding affinity score for designed beta-lactamase**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-6.98$\\pm$0.01|-6.87$\\pm$0.02|-6.69$\\pm$0.05|-6.29|\n|Inpainting|-9.89$\\pm$0.03|-9.54$\\pm$0.16|-9.13$\\pm$0.43|-7.24|\n|SMCDiff+ProteinMPNN|-9.10$\\pm$0.01|-9.05 $\\pm$0.02|-8.98$\\pm$0.01|-6.97|\n|PROTSEED|-9.88$\\pm$0.21|-9.51$\\pm$0.41|-9.01$\\pm$0.62|-7.31|\n|FrameDiff+ProteinMPNN|-9.54$\\pm$0.03|-9.56$\\pm$0.23|-8.89$\\pm$0.35|-7.03|\n|RFDiffusion+ProteinMPNN|-9.87$\\pm$0.05|-9.56$\\pm$0.23|-9.12$\\pm$0.53|-7.51|\n|NAEPro|**-10.06$\\pm$0.05**|**-9.79$\\pm$0.10**|**-9.39$\\pm$0.12**|**-7.66**|\n\n**Response to Q6: Because ESMFold is a less reliable tool for evaluating novel proteins, it should not be used here to assess your designs at all.**\n\nAns: We totally agree with reviewer VH6U. We provide the mean pLDDT for top-5, top-10, top-30 candidates calculated by AlphaFold2 as follows:\n\n**AlphaFold2 pLDDT for designed myoglobin**\n\n|Model| top-5|top-10|top-30|\n|:-----|:----:|:----:|:----:|\n|Hallucination|76.7243|73.5953|68.6060|\n|Inpainting|97.6171|97.6063|97.5681|\n|SMCDiff+ProteinMPNN|85.7051|84.7744|84.9220|\n|PROTSEED|92.9770|91.9216|90.4293|\n|FrameDiff+ProteinMPNN|97.3997|97.3329|96.4279|\n|RFDiffusion+ProteinMPNN|97.9722|97.9174|97.6063|\n|NAEPro|**98.1057**|**98.0706**|**97.9197**|\n\n**AlphaFold2 pLDDT for designed $\\beta$-lactamase**\n\n|Model| top-5|top-10|top-30|\n|:-----|:----:|:----:|:----:|\n|Hallucination|74.7760|74.7032|71.2932|\n|Inpainting|94.5489|94.4328|93.2941|\n|SMCDiff+ProteinMPNN|86.2510|85.7072|83.0376|\n|PROTSEED|98.2896|98.2740|97.9683|\n|FrameDiff+ProteinMPNN|98.3049|98.2922|98.1267|\n|RFDiffusion+ProteinMPNN|**98.6225**|**98.5116**|98.0643|\n|NAEPro|98.5385|98.4481|**98.2841**|\n\n---\n\nDear reviewer,\n\n    We sincerely appreciate the time and effort you dedicated to reviewing our paper and reading our responses. Your valuable suggestions and insightful comments have significantly contributed to refining the quality of our work. No matter what the final result will be, the thoughtful communication with you has better clarified our paper's goals and logic. We would like to express our great gratitude to you!\n\nBest,\n\nAuthors of paper 4608\n\n---\n\n**Could the authors briefly explain how do you construct the one-to-one mapping between the design structure and the target structure in PDB? Is it determined by retrieving the one in PDB with the smallest RMSD? As far as I know, there is no common practice to determine this for the task of protein design, and I appreciate any explanation that clues me in.**\n\nAns: Yes, we construct the one-to-one mapping between the designed structure and the target structure in PDB. It is not determined by retrieving the one in PDB with the smallest RMSD. We extract the motif fragments for each protein, based on which our training objective is to recover the original protein sequence and backbone structure. Therefore, we compute the RMSD between the designed structure and the target one which we want to recover.\n\n---\n\n**Corrected statement: But it can still bind the corresponding metallocofactor heme in the AlphaFill simulation. For the claim “binding rates of 100% and 90%”, I wonder how is the criterion/threshold of “binding” determined? This success rate is rather unrealistic and probably fails to reflect the real performance. Thus, the statement in Section 5.2. saying “It is evident that our method can generate proteins that express the basic and important functions.” can be quite misleading for readers who are not familiar with the task.** \n\nAns: Sorry for the misleading statement. We mean through the AlphaFill prediction, the designed proteins are highly potential to bind the corresponding metallocofactors. We have corrected all the claims in our paper. We provided the process of how to decide if the designed protein can “bind” the metallocofactors in Appendix B.3. We would like to further explain it here.  First of all, we use AlphaFill to do the first-step prediction. AlphaFill will return a complex based on the input protein structure. If the complex includes the corresponding metal ions, we think the designed protein is potential to have functions. Then to guarantee the results convincing, we further use some additional constraints to ensure the designed proteins are potential to express functions. Specifically, we assume if a designed protein share the similar active site environments with their natural counterparts (target protein in our dataset), then it’s highly potential to bind the corresponding metal ions. For myoglobin, we calculate the distance between axial histidine ligand to Fe ion. Besides, we also detect the presence of distal histidine within the active sites, which plays an important role in the oxygen molecule binding function of myoglobin . If the distance is between 2.0Å and 2.5Å, and the distal histidine exists, we think the designed myoglobins share similar active site environments to natural ones. For $beta$-lactamase, we detect the residues that directly contact with zinc ion. If they show similar chemistry properties with natural $beta$-lactamases, we think they can highly potentially bind the corresponding metallocofactors. For example, in Figure 5(a), the active site possesses one zinc atom coordinated to three histidines, while the second zinc atom is coordinated to one histidine, one cysteine, and one aspartate, demonstrating a high degree of amino acid analogy to the natural protein.\n\n**Could the author intuitively describe how does the NAEL deal with the reflection case so as to be simply SE(3)-equivariant instead of E(3)?** \n\nAns: Intuitively, applying reflection to the input structure, the output structure will also be reflected. The conformation of the original backbone will be changed from L-amino acid to D-amino acid (We provide an illustration in Figure 6 in appendix). Therefore, the reflection will change the chirality of the protein which may cause the deficiency of binding to the ligand and eventually its function. However, the protein sequence in our model will not be influenced. Therefore, the 3D structure of the designed sequence which is L-amino acid and the output structure  which becomes D-amino acid will be inconsistent. From this aspect, our model doesn’t satisfy reflection equivariance.\n\n**The authors are encouraged to encompass appropriate comparison with EGNN/SE(3)-transformer in their writing to better contextualize the proposed NAEL.**\n\nAns: We provide the comparison with EGNN+ESM2 on myoglobin in the following Table and also updated the results in Table2 accordingly.\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|EGNN+ESM2|-13.30$\\pm$0.61 | -12.77$\\pm$0.69 |-12.07$\\pm$0.65 | -9.63|\n|NAEPro|**-14.12$\\pm$0.01**|**-13.85$\\pm$0.10**|**-13.06$\\pm$0.38**|**-10.74**|\n\n**It is surprising to see scaling the model does not yield consistent performance gain. Could the authors help explain this behavior? Also, I noticed that the authors merge the two myoglobin and lactamase data for training newly in the rebuttal response. Is there any insight for doing such multi-task learning?**\n\nAns: Enlarging the model hasn’t improved the performance too much. Our interpretation is that the training data size is moderate, and thus a model with moderate size can fit the data well. To validate if this guess holds, we merge the two protein data and then train a unified model. As we can see in the previous table, our model did achieve obvious better results on $\\beta$-lactamase, i.e., TM-score from below 0.5 (0.4919) to over 0.5 (0.5055).\n\n---\n\nWe much appreciate the reviewer’s responses and suggestions, which help a lot to improve our paper.  We have accordingly revised our paper, including Table 1, Table 2, Section 4.2-4.4 and Section 5.2. Our responses to the reviewer’s follow-up questions are provided as follows:\n\n**About the evaluation metrics: I think the binding affinity could better support the effectiveness of NAEPro and be more suitable for the task (functional protein design) rather than the accuracy metrics (RMSD, AAR, etc.). Do the authors agree on this or not?** C**ould the authors explain how the Binding Affinity is calculated for both model based on Gnina? Specifically, I am worried that the metrics is averaged among the test set similar to other metrics.  If so, I suggest that the authors report median or top-k mean as evaluation.**\n\nAns: We completely agree with reviewer MgyU that the binding affinity is a better metric for functional protein design rather than the accuracy metrics. We revised Table 1 and 2 in our paper according to your suggestion. The binding affinity is calculated using Gnina as the average score on the test set. Thank the reviewer for this valuable suggestion. We provide  top-5, top-10, top-30 mean and variance as well as the median in the following table and also revise our paper accordingly. From the results, we can see that our model achieves the best median and top-K binding affinity scores on both datasets.\n\n**Myoglobin**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-8.18$\\pm$ 0.01|-8.07$\\pm$0.03|-7.97$\\pm$0.23|-7.25|\n|Inpainting|-13.47$\\pm$0.02|-13.12$\\pm$0.12|-12.31$\\pm$0.54|-9.56|\n|SMCDiff+ProteinMPNN|-11.37$\\pm$0.03|-11.12$\\pm$0.31|-10.87$\\pm$0.42|-8.76|\n|PROTSEED|-13.21$\\pm$0.13|-12.89$\\pm$0.42|-11.98$\\pm$0.52|-10.23|\n|FrameDiff+ProteinMPNN|-13.13$\\pm$0.05|-12.92$\\pm$0.16|-12.21$\\pm$0.23|-10.08|\n|RFDiffusion+ProteinMPNN|-13.68$\\pm$0.02|-13.03$\\pm$0.21|-12.56$\\pm$0.43|-10.15|\n|NAEPro|**-14.12$\\pm$0.01**|**-13.85$\\pm$0.10**|**-13.06$\\pm$0.38**|**-10.74**|\n\n**Beta-lactamase**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-6.98$\\pm$0.01|-6.87$\\pm$0.02|-6.69$\\pm$0.05|-6.29|\n|Inpainting|-9.89$\\pm$0.03|-9.54$\\pm$0.16|-9.13$\\pm$0.43|-7.24|\n|SMCDiff+ProteinMPNN|-9.10$\\pm$0.01|-9.05 $\\pm$0.02|-8.98$\\pm$0.01|-6.97|\n|PROTSEED|-9.88$\\pm$0.21|-9.51$\\pm$0.41|-9.01$\\pm$0.62|-7.31|\n|FrameDiff+ProteinMPNN|-9.54$\\pm$0.03|-9.56$\\pm$0.23|-8.89$\\pm$0.35|-7.03|\n|RFDiffusion+ProteinMPNN|-9.87$\\pm$0.05|-9.56$\\pm$0.23|-9.12$\\pm$0.53|-7.51|\n|NAEPro|**-10.06$\\pm$0.05**|**-9.79$\\pm$0.10**|**-9.39$\\pm$0.12**|**-7.66**|\n\n**Corrected statement: ESMFold might not accurately predict the structure for a novel sequence.**\n\nAns: Sorry for the misunderstood claim. We want to express we agree on reviewer VH6U’s comment in question 6 “for a novel sequence, its prediction is worse than AlphaFold2.”  As suggested by reviewer VH6U, we re-calculate pLDDT by AlphaFold2. Our model respectively achieves 91.4139 and 76.9550  on myoglobin and $\\beta$-lactamase on average. As claimed in previous work [1,2], pLDDT between 70~90 are classified to be confident and pLDDT ≥ 90 indicates residues predicted with extremely high confidence.\n\n[1] pLDDT Values in AlphaFold2 Protein Models Are Unrelated to Globular Protein Local Flexibility. Oliviero Carugo. Crystals. 2023.\n\n[2] Generating new protein sequences by using dense network and attention mechanism. Feng Wang et al. Mathematical Biosciences and Engineering. 2023.\n\n---\n\nThanks for your valuable review and comments again. We have addressed all your concerns and questions.\n\nIf you have any other questions, please feel free to discuss with us.\n\n---\n\nThanks for your valuable review and comments again. We have addressed all your concerns and questions.\n\nIf you have any other questions, please feel free to discuss with us.\n\n---\n\nThanks for your valuable review and comments again. We have addressed all your concerns and questions.\n\nIf you have any other questions, please feel free to discuss with us.\n\n---\n\n**Q3: In section 4.2, the author(s) described that the weights of NAEPro is partially initialized from pretrained ESM2-8M, of the smallest model size among the ESM2 series. This is very problematic but the recent common practices leverage at least the 650M model. Do the author(s) try much larger model size as initialization? I found in the Table 2 that the 8M model has only marginal improvement upon random initialization.**\n\nAns: Yes, we tried to partially initialize our model with 35M ESM2 (NAEPro-12) and also train this model on the merged myoglobin and beta-lactamase data (NAEPro-12-Unified). For myoglobin, enlarging the model to 12 layers will not influence the performance too much, but training the larger model on the merged dataset will increase the pLDDT and TM-score. For beta-lactamase, enlarging the model from 6 layers to 12 layers is beneficial and will increase the overall performance. Particularly, training the 12-layer model on merged dataset will significantly improve the performance of beta-lactamase, say improve the average TM-score from below 0.5 to above 0.5. Then we also partially initialize a 30-layer model with 150M ESM2 (NAEPro-30-Unified) and train the model on merged dataset. The performance for both myoglobin and beta-lactamase will be further improved.\n\n**myoglobin**\n\n|Model| AAR (%)|RMSD|pLDDT |TM-score |Parameters|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|NAEPro-6 |**89.37**|**2.6307**|81.9507|0.5692|11.6M|\n|NAEPro-12 |88.42|2.6333|82.2849|0.5673|52M|\n|NAEPro-12-Unified|86.81|2.6443|82.9736|0.5736|52M|\n|NAEPro-30-Unified|86.62|2.6410|**83.0173**|**0.5749**|223.7M|\n\n**$\\beta$-lactamase**\n\n|Model| AAR (%)|RMSD|pLDDT |TM-score |Parameters|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|NAEPro-6 |65.71|2.9916|66.8705|0.4812|11.6M|\n|NAEPro-12 |67.28|2.9864|68.7394|0.4919|52M|\n|NAEPro-12-Unified|70.93|**2.9707**|69.9333|0.5055|52M|\n|NAEPro-30-Unified|**72.60**|2.9781|**71.0429**|**0.5089**|223.7M|\n\n**Q4: How is the “target structure” in evaluation metrics(Section 4.2) determined? What is the difference between metrics RMSD and consistency?**\n\nAns: Target structure means the natural protein structure provided in PDB. RMSD is calculated as the RMSD between the designed protein structure and the one given in PDB. Consistency is calculated as the RMSD between the designed structure and the one predicted by ESMFold for the designed sequence.\n\nWe really hope our responses address your concerns. If you have any other questions, we are very happy to continue discussions!\n\n---\n\n**Weakness 4: For the evaluation metrics, the problem to be studied is functional protein (as it is claimed in the title) design instead of general backbone generation(eg., RFDiffusion) or sequence generation (eg., ProteinMPNN). Thus, I found the the five of the metrics fail to (or indirectly if they potentially do) reflect the performance of functional protein design or scaffolding. All these metrics in Table 1 & 2 show the somehow sequence-structure matching between the predicted and native, but are not very suitable for proteins such as enzyme.**\n\nAns: In our analysis (Section 5.2), we evaluated the function for binding the corresponding metallocofactors of the designed proteins. Specifically, we first randomly select 20 cases from the top100 sequences (pLDDT ranking). We then employ AlphaFold2 for protein structure prediction, followed by inputting these structures into AlphaFill [1] to predict the associated ligands. Notably, our results indicate that all designed myoglobins exhibit heme binding capability, while 18 beta-lactamases demonstrate the ability to bind zinc ions, resulting in metallocofactor binding rates of 100% and 90%, respectively. It is evident that our method can generate proteins that express the basic and important functions, i.e. binding the corresponding metallocofactors. To systematically illustrate the function of our designed proteins, we use docking tool Gnina[2] to compute the binding affinity between the designed metalloproteins and the corresponding metallocofactors. The results in the following table shows that our model achieves the best binding affinity scores on both metalloproteins, demonstrating the proteins designed by our model are highly potential to actively exhibit biochemical functions.\n\n|Model| myoglobin (kcal/mol)|beta-lactamase (kcal/mol)|\n|:-----|:----:|:----:|\n|Hallucination|-7.23|-6.37|\n|Inpainting|-9.32|-7.63|\n|SMCDiff+ProteinMPNN|-8.79|-6.89|\n|PROTSEED|-10.07|-7.68|\n|FrameDiff+ProteinMPNN|-9.54|-7.21|\n|RFDiffusion+ProteinMPNN|-9.76|-7.59|\n|NAEPro|**-10.14**|**-7.80**|\n\n**Weakness 5: The potential theoretical flaw in the equivariance analysis in Section 3.5.** \n\nAns: Thank the reviewer for pointing out this typo. Our model satisfies only roto-translation equivariance without reflection equivariance. We have accordingly updated the paper.\n\n**Q1: The proposed architecture encoding both sequence and structure via the masked language modeling (MLM) scheme is somehow interesting. How do the author(s) see the  difference between the proposed NAEL layer and tensor field convolutional layer in SE(3)-transformer ? Also, what is the advantage of NAEL over the SeqIPA of the PROTSEED, the most competitive baseline in Table 1?** \n\nAns: The most biggest difference between our NAEPro and tensor field convolutional layer in SE(3)-transformer is:  NAEPro uses interleaving layers of global sequence-level attention and local neighborhood equivariant sub-layer, from which the residue representation is updated from both the whole sequence interaction and 3D neighbor message passing; instead, if we enable kNN message passing in SE(3)-transformer, then the node feature will only be updated based on 3D neighboring information. To clearly demonstrate the advantage of NAEL over SeqIPA in PROTSEED, we replace our NAEL with the SeqIPA-Addition without secondary structure feature. Taking myoglobin as an example, the results are shown in the following table. NAEPro with NAEL performs better than with SeqIPA.\n\n|Model| AAR (%)|RMSD|\n|:-----|:----:|:----:|\n|NAEPro -w/- SeqIPA|65.33|2.7078|\n|NAEPro|**89.37**|**2.6307**|\n\n**Q2:  In Section 3 - opening paragraph, the author(s) define/formulate the target task as generate a protein sequence and all 3D coordinates of N residues. However, in later model definition, the proposed NAEPro only operates on the C-alpha(CA) coordinates. Please explain this ambiguity.**\n\nAns: We are sorry for the ambiguity caused. When we mention “we formulate the target task as generate a protein sequence and all 3D coordinates of N residues”, we mean 3D coordinates of alpha-carbon of all the residues. Following the setting in [3], we represent the backbone structure as the CA-only coordinates of all residues. We have updated the paper accordingly in the new version.\n\n[3] Diffusion probabilistic modeling of protein backbones in 3D for the motif-scaffolding problem. Brian L. Trippe et al. ICLR. 2023.\n\n---\n\nWe appreciate the reviewer’s valuable and insightful suggestions, which are very helpful for us to improve our paper. We have clarified all your concerns, added the experiments and updated the paper accordingly. We address the specific concerns as follows:\n\n **Weakness 1: Regarding the consistency metrics reported, I noticed that the consistency metric of NAEPro in Table 1 (as the main results) underperformed half of the baselines.  Such key observation greatly weakened the method. The authors can elaborate on this point.**\n\nAns: Yes, we agree with the reviewer’s opinion. We would like to clarify our task more clearly. Our goal is to design functional proteins with novel sequence and structure. As shown in Figure 5(b), the designed myoglobin has a RMSD of 3.943\\AA with the most similar one in PDB and 26.7% AAR with the most similar one in Uniprot, but it can still bind the corresponding metallocofactor heme in the AlphaFill simulation [1]. In our paper, we use ESMFold to predict the structure of the designed sequence. However, as pointed out by reviewer VH6U, ESMFold might not accurately predict the structure for a novel sequence. To further clarify this point, we calculate the scRMSD of motifs between the designed structure and the predicted structure from the designed sequence in the following table. As the results show, our NAEPro achieves smaller RMSD than RFDiffusion+ProteinMPNN on myoglobin and comparable results on beta-lactamase.\n\n|Model| myoglobin|beta-lactamase|\n|:-----|:----:|:----:|\n|Inpainting|0.5292|0.7819|\n|SMCDiff+ProteinMPNN|1.3816|1.4523|\n|PROTSEED|0.4987|**0.3892**|\n|FrameDiff+ProteinMPNN|0.8765|0.9815|\n|RFDiffusion+ProteinMPNN|0.4936|0.3918|\n|NAEPro|**0.4832**|0.5239|\n\n[1] Alphafill: enriching alphafold models with ligands and cofactors. Maarten L Hekkelman et al. Nature methods. 2023.\n\n**Weakness 2: For the experiments, how is the model trained to obtain the evaluation digits in Table 1 for each baseline. This is very important.** \n\nAns: We are sorry for the confusion. We got the codes of all baselines from their GitHub repos, and then re-trained all the models separately on myoglobin and beta-lactamase datasets. The evaluation procedure among all the baselines and our model follows the same process, and thus we believe the comparisons in Table 1 are fair.\n\n**Weakness 3: For the dataset, I found that in section 4.1, the author(s) mentioned that both beta-lactamase and myoglobin datasets are split randomly. This is worrying because the entries in PDB can usually be redundant, especially for such large and well-studied families that the authors selected for evaluation. The author(s) can elaborate on this point. Moreover, in the Table 3 (in appendix), I am curious about the column name “PDB”, does it mean the number of PDB entries (with unique ID) or the number of single chains?** \n\nAns: Yes, this is a very good question. Since we focus on sequence and structure co-design task, we aim to design both functional sequence and structure. Therefore, our input data is the <sequence, structure> pair. From this point, even though the sequence might be the same, the structure is somehow different. Therefore, our paired input data are different and would not be redundant. We appreciate the reviewer for carefully reviewing our paper and also reading the appendix. In Table 3, PDB means the number of different PDB entries. To eliminate the reviewer’s concerns, we re-cleaned the test set by filtering pairs whose sequences have over 30% sequence identity to any sample in the training set after doing pairwise alignment for myoglobin. We compare our NAEPro and ProtSeed (the most competitive baseline) in the following table. As the results show, our model can design proteins with higher binding affinity (by Gnina [2]) on samples that have lower similarity to training data.\n\n|Model| RMSD|Binding Affinity (kcal/mol)|\n|:-----|:----:|:----:|\n|PROTSEED|**2.4031**|-9.62|\n|NAEPro|2.6634|**-9.98**|\n\n[2] GNINA 1.0: Molecular docking with deep learning. A McNutt et al. Cheminformatics. 2021.\n\n---\n\n**Weakness4 & Q1: The authors do not provide results on standard benchmarks, such as the CATH dataset, for fair comparison on both sequence and structure design. Could you provide experimental results from the CATH dataset to compare with the original SMCDiff and FrameDiff results?**\n\nAns: We would like to clarify our task more clearly. Our task aims to co-design functional and novel protein sequence and structure. In our method, we use MSAs to automatically find motifs to guarantee the protein function, which is achieved within the same protein family. Instead, CATH is a protein dataset consisting of proteins from diverse families, making the motif extraction difficult. Besides, CATH is usually used to evaluate inverse folding task, say protein sequence design based on fixed backbone structure, which is not the goal of our paper. We hope we didn’t misunderstand the reviewer’s meaning by “the original SMCDiff and FrameDiff results” as SMCDiff and FrameDiff haven’t been evaluated on CATH dataset. Since SMCDiff and FrameDiff focus on protein structure design, CATH is usually used to evaluate protein sequence design based on fixed backbone structure, which may have some discrepancies on the design goals. \n\n**Q2: Could you provide head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design? Please follow the same setting.**\n\nAns: We provide protein sequence design given backbone structures on our two datasets in the following table. In this setting, we slightly modified our method to make it adaptable to the task as the reviewer asked. Specifically, we provided the whole backbone structure as the model input and designed the whole protein sequence. As the results show, even though we only provide  backbone structure without any motif residues, our model can still achieves higher AAR and pLDDT on myoglobin, and higher AAR on beta-lactamase.\n\n**myoglobin**\n\n|Model| AAR (%)|pLDDT |\n|:-----|:----:|:----:|\n|ProteinMPNN|81.37|81.9817|\n|ESMIF|76.49|78.0986|\n|PiFold|78.93|80.2912|\n|NAEPro|**85.56**|**82.3871**|\n\n$\\beta$-lactamase\n\n|Model| AAR (%)|pLDDT |\n|:-----|:----:|:----:|\n|ProteinMPNN|53.59|**70.6723**|\n|ESMIF|57.49|61.3928|\n|PiFold|63.38|64.9017|\n|NAEPro|**68.39**|65.8218|\n\n**Q3: Could you provide the code for checking the results?**\n\nAns: We have provided the code and data in the supplementary material. Due to the memory limitation, we couldn’t upload the model checkpoints. However, the training process is highly efficient, and the reviewer could train the model if he/she is interested.\n\n---\n\nWe thank the reviewer for the positive reviews as well as the suggestions for improvement. We have clarified all your concerns, added the experiments and updated the paper accordingly. Our responses to the reviewer’s concerns and questions are provided below:\n\n**Weakness 1: Both EGNN and ESM2 are existing models, and there are many existing works on antibody structure and sequence co-design. The combination may limit the novelty of this method.**\n\nAns: We would like to point out that our NAEPro is not a combination of ESM2 and EGNN. Instead our key innovations are (1) interleaving layers of sequence-level attention and local neighborhood equivariant sub-layer (2) accelerating the local sub-layer with the k-nearest neighbors.  On the contrary,  EGNN is a fully-connected graph and updates message, coordinates and atom features based on all other atoms in 3D space, which is not efficient for long proteins. Besides, EGNN+ESM2 is a sequential process and can not cross-condition on sequence and structure. We compare our method with EGNN+ESM2, NAEPro w/o ESM2 initialization on myoglobin in the Following Table (Table 2 in the paper).  As the results show, EGNN+ESM2 performs much worse than our NAEPro, and removing the ESM2 initialization will not influence the performance too much, demonstrating that our model is much more superior than EGNN+ESM2.\n\n|Model| AAR (%)|RMSD|pLDDT |TM-score |consistency|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|EGNN+ESM2|51.12|2.9891|77.3399|0.4656|4.9827|\n|NAEPro-w/o-ESM2 initialization|79.82|2.6398|76.3032|0.5159|4.7273|\n|NAEPro|**89.37**|**2.6307**|**81.9507**|**0.5692**|**4.3865**|\n\n**Weakness 2: It is likely that previous works can be readily applied to the motif-conditioned setting, and the authors can easily adapt their method to antibody design tasks. It would be beneficial if the authors further elaborate on the significance of their work**\n\nAns: We would like to clarify our goal more clearly. Our task is to co-design functional and novel protein sequence and structure. Previous work which is most similar to ours is Inpainting [1]. However, [1] provides the functional sites manually while we automatically mined the functional sites (plus the conserved sites as suggested by reviewer ****VH6U****) by MSAs. Previous methods considering only sequence design constrained by fitness value [2,3] or sequence design based on given backbone structure (inverse folding) [4] is not suitable for our task. For antibody design, antibody is always Y-shaped, while one of our goal is to design novel and diverse protein structure. For example, in Figure 4, the designed beta-lactamases have different fold categories and belong to three different sub-classes. In Figure 5 (b), the designed myoglobin has a RMSD of 3.943\\AA with the most similar one in PDB, but it can still bind the corresponding metallocofactor heme in the AlphaFill simulation [5]. From this aspect, we think antibody design models can not be directly applied to our task without any modification and vice versa. \n\n[1] Scaffolding protein functional sites using deep learning.  Jue wang. et al. Science. 2022.\n\n[2] Biological Sequence Design with GFlowNets. [Moksh Jain](https://arxiv.org/search/q-bio?searchtype=author&query=Jain,+M) et al. ICML 2022. \n\n[3] Proximal Exploration for Model-guided Protein Sequence Design. Zhizhou Ren et al. ICML 2022. \n\n[4] Robust deep learning based protein sequence design using ProteinMPNN. J. Dauparas et al. Science. 2022.\n\n[5] Alphafill: enriching alphafold models with ligands and cofactors. Maarten L Hekkelman et al. Nature methods. 2023.\n\n**Weakness 3:** **The authors do not provide code for checking the soundness of the methods.**\n\nAns: We have provided the code and data in the supplementary material. Due to the memory limitation, we couldn’t upload the model checkpoints. However, the training process is highly efficient, and the reviewer could train the model if he/she is interested.\n\n---\n\n**Q6: For a novel sequence, ESMFold’s prediction is worse than AlphaFold2. Consequently, evaluating novel sequences should use AlphaFold2 instead of ESMFold.**\n\nAns: Due to limited time and computing resources, we provide the AlphaFold2 results for ProtSeed, RFDiffusion+ProteinMPNN and our NAEPro. As the results show, even though ESMFold achieves lower scores than AlphaFold2, the tendencies (ranking) among different models are similar.  Particularly, our model achieves an average TM-score over 0.5 on beta-lactamase (ESMFold 0.4812) and pLDDT over 70 (ESMFold 66.8705), which gives stronger evidence that our model has the ability to design proteins which have stable structures.\n\n**myoglobin**\n\n|Model| pLDDT|TM-score|\n|:-----|:----:|:----:|\n|PROTSEED|77.9032|0.5621|\n|RFDiffusion+ProteinMPNN|88.2130|0.5103|\n|NAEPro|**91.4139**|**0.5912**|\n\n**$\\beta$-lactamase**\n\n|Model| pLDDT|TM-score|\n|:-----|:----:|:----:|\n|PROTSEED|73.3176|0.4901|\n|RFDiffusion+ProteinMPNN|**87.8092**|0.4806|\n|NAEPro|76.9550|**0.5009**|\n\n **Q7: \"Consistency - What is the designed structure and the predicted structure? Which is from NAEPro? Where does the other one come from?**\n\nAns: Consistency is calculated as the RMSD between the designed structure from NAEPro and the one predicted by ESMFold for the designed sequence from NAEPro.\n\n---\n\n**Weakness 4:  Results in Table 1 do not support the superiority of the proposed method. For instance, RMSD>1.7 \\AA means the two structures are different for myoglobin. For pLDDT, the prediction on lactamase is ~66%, which is much lower than 80%-90% which is conventionally believed reliable for folding predictions. The TM-score, if they are smaller than 0.5, then the two structures are believed different.** \n\nAns: We would like to clarify our goal more clearly.  Our task is to design the sequence and structure of proteins with effective functions.  We calculated TM-score by AlphaFold2 as suggested by the reviewer, and the two proteins designed by our model both achieves an average score over 0.5 (myoglobin **0.5912** and beta-lactamase **0.5009**). However, as found by previous method [7],  protein pairs with a TM-score >0.5 are most likely in the same fold while those with a TM-score <0.5 are mainly not in the same fold. Therefore, low TM-score can not indicate the designed protein doesn’t have the corresponding function. As claimed in previous work [9,10], pLDDT between 70~90 are classified to be confident. We calculated pLDDT by AlphaFold2 as suggested by the reviewer and the two protein families designed by our model are confident on an average aspect (myoglobin **91.4139** and beta-lactamase **76.9550**).  Similarly, as observed in previous work[11,12], pLDDT has a weak correlation with protein function. Therefore, a moderate TM-score and pLDDT can not indicate a protein expresses no or poor function. As shown in figure 5(b), the designed myoglobin has a RMSD of 3.943\\AA with the most similar one in PDB, but it can still bind the corresponding metallocofactor heme in the AlphaFill simulation [8]. To systematically illustrate the function of our designed proteins, we use docking method Gnina [13] to compute the binding affinity between the designed metalloproteins and the corresponding metallocofactors. The results in the following table shows that our model achieves the best binding affinity scores on both metalloproteins, demonstrating the proteins designed by our model are highly potential to actively exhibit biochemical functions.\n\n|Model|myoglobin (kcal/mol)|beta-lactamase (kcal/mol)|\n|:-----|:----:|:----:|\n|Hallucination|-7.23|-6.37|\n|Inpainting|-9.32|-7.63|\n|SMCDiff+ProteinMPNN|-8.79|-6.89|\n|PROTSEED|-10.07|-7.68|\n|FrameDiff+ProteinMPNN|-9.54|-7.21|\n|RFDiffusion+ProteinMPNN|-9.76|-7.59|\n|NAEPro|**-10.14**|**-7.80**|\n\n[7] How significant is a protein structure similarity with TM-score= 0.5? Jinrui Xu and Yang Zhang. *Bioinformatics. 2010.* \n\n[8] Alphafill: enriching alphafold models with ligands and cofactors. Maarten L Hekkelman et al. Nature methods. 2023.\n\n[9] pLDDT Values in AlphaFold2 Protein Models Are Unrelated to Globular Protein Local Flexibility. Oliviero Carugo. Crystals. 2023\n\n[10] Generating new protein sequences by using dense network and attention mechanism. Feng Wang et al. Mathematical Biosciences and Engineering. 2023.\n\n[11] Using AlphaFold to predict the impact of single mutations on protein stability and function. Marina A. Pak et al. PloS one. 2023.\n\n[12] Peptide binder design with inverse folding and protein structure prediction. Patrick Bryant et al. Nature Communications Chemistry. 2023.\n\n[13] GNINA 1.0: Molecular docking with deep learning. A McNutt et al. Cheminformatics. 2021.\n\n**Q1 & Q2: \"The use of separate models for sequence and structure cannot ensure the consistency between the generated sequence and structure.\" \"knowing the topology of a protein before design process is difficult and also cannot guarantee the designed proteins have the desired functions.\" I disagree with this statement.** **I disagree with this statement.** \n\nAns: Thanks for the suggestion! We agree with the reviewer and updated the statement in our paper. \n\n**Q3: what is the meaning of \"generally-encoded\" for 20 types of amino acids?** \n\nAns: It means the 20 common amino acids. We have updated the paper accordingly.\n\n**Q4: \"The selection of motif varies from setting to setting...\". Can the authors please provide a clearer explanation?**\n\nAns: We mean the motifs have different meanings in different tasks. For de novo enzyme design [14], it means the binding sites where enzyme binds the metallocofactor and substrate. For de novo binder design [15], it means the binding sites where the protein binder binds the protein targets.\n\n[14] De novo enzyme design using rosetta3. Florian Richter et al. PloS one. 2011.\n\n[15] De novo design of protein interactions with learned surface fingerprints. Pablo Gainza. Nature. 2023.\n\n---\n\nWe thank the reviewer for the insightful questions. We have clarified all your concerns, added the experiments and updated the paper accordingly. Answers to specific points are provided below:\n\n**Weakness 1 & weakness 2 & Q5: The identified problem or the motivation for designing such a method is not supported by existing literature.  Some designed modules counter the intuition in biology, such as \"motif mining\"**\n\nAns: We follow the design insight proposed by [1], which starts from the functional site and fills in additional sequence and structure to create a viable protein scaffold in a single forward pass. Different from [1], we provide the functional sites which are automatically mined from MSA as previous methods [2, 3] while [1] provides the functional sites manually. Besides, we design a new architecture called NAEPro to generate the whole protein sequence and structure based on the given partial protein fragments, while [1] achieves this goal by finetuning the RoseTTAFold. However, we agree with the reviewer’s opinion in question 5 that MSA discovers not only protein functional sites as addressed in [2,3] but also finds conversed sites which may not directly relate to the protein function. Therefore, we change all the term “motif/functional sites” in our paper to  “meaningful protein fragments”.\n\n[1] Scaffolding protein functional sites using deep learning.  Jue wang. et al. Science. 2022.\n\n[2] Emerging methods in protein co-evolution. David de Juan et al. Nature Review Genetics. 2013.\n\n[3] Evolutionary information for specifying a protein fold. Michael Socolich et al. Nature. 2005.\n\n**Weakness 3 & Q1: The method is not 'extensively evaluated' as claimed by the authors (they only selectively analyzed 2 proteins), and many results do not suggest that the proposed method is SOTA among baselines. Moreover, the evaluation does not include all relevant methods, which makes it too aggressive for the authors to claim in the Introduction that their method \"achieves the HIGHEST...among ALL competitors\" and \"is faster than the FASTEST method\". Empirically, It has been proven by several recent research that the de novo method can generate new sequences for fixed backbone or desired functions [4-6].**\n\nAns: Sorry for the confusion. Our method achieves the best performance on 4/5 metrics on myoglobin and 3/5 metrics on beta-lactamase among all the compared representative baselines in our paper. Our model is faster than all baselines compared in our paper. We have revised all these statements in our paper (highlighted in red). For other relevant methods as the reviewer suggested in reference [4-6], we have already compared with [4]. [5] was posted on Oct. 3, 2023, which is later than the ICLR submission ddl, and [3] was posted on Aug. 14, 2023, which is within 3 months before ICLR submission ddl. We appreciate the reviewer’s suggestion, and we tried to compare with [3], but they haven’t released their code now. We emailed the authors of [3] to ask for code and they said their work is under review now and refused to provide code currently. We are trying to reimplement the model now and will update the results in our revised version.\n\n[4] Watson et al., De novo design of protein structure and function with RFdiffusion (2023).\n\n[5] Sumida et al., Improving protein expression, stability, and function with ProteinMPNN (2023).\n\n[6] Zhou et al., Conditional Protein Denoising Diffusion Generates Programmable Endonuclease Sequences (2023).\n\n---\n\nWe thank all the reviewers' valuable suggestions.\n\nWe have uploaded a revised draft that incorporates feedback from reviewers VH6U and MgyU, with the updated portions highlighted in red. Additionally, in response to reviewer Wq6M's suggestion, we have included the code and data we used in our paper in the supplementary material (Due to the maximum memory limitation -100M, we couldn’t upload the model checkpoints).\n\nHere's an overview highlighting the principal modifications to our paper:\n\n1. The revised term from “motif/functional sites” to “meaningful fragments” in abstract, introduction third paragraph, method opening paragraph, method 3.1-3.4, experiments 4.1-4.2, analysis 5.1, Conclusion (Reviewer VH6U, weakness 1 & weakness 2 & Q5)\n2. The corrected claim in abstract and last paragraph in introduction (review VH6U, weakness3) \n3. The corrected claim in Introduction second paragraph (Reviewer VH6U, Q1)\n4. The corrected claim in Related Work second paragraph (Reviewer VH6U, Q2)\n5. The clarified explanation of 20 common amino acids in Method opening paragraph(Reviewer VH6U, Q3)\n6. The corrected theoretical claim in Method 3.5 and Appendix A. 1 (reviewer MgyU, weakness 5) \n7. The clarified task definition in Method opening paragraph (reviewer MgyU, Q2)\n\nWe look forward to any additional reading and feedback. If you have any further questions, we are very happy to continue discussion!", "author_response": "Thanks for the responses! Our answers to your follow-up questions are provided as follows:\n\n**1. About Weakness4 & Q1 Page 2, \"We carry out experiments on two metalloproteins, including β-lactamase and myoglobin.\" I would like to say that the experimental setting is the main concern to me. Since the authors have only evaluated the proposed method on two structures, I am not sure if the method can be extended to generalized situations. I know that you are doing protein sequence-structure inpaintning problem, in which case a comparison with a previous baselines [1,2,3] on stantard datasets may help to reveal the generalization capabilities of your algorithm.** \n\nAns: We agree with reviewer Wq6M and additionally compare our method with PROTSEED on B12 protein. Due to the approaching ddl and limited time, PROTSEED has only been trained for 100 epochs, the same as our method. (suggested training epoch is 2000 on their GitHub)\n\nThe results are reported as follows: \n\n**Binding affinity score for designed B12**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|PROTSEED|-9.45$\\pm$ 0.03|-9.36$\\pm$0.09|-9.04$\\pm$0.27|-8.46|\n|NAEPro|**-11.06$\\pm$0.14**|**-10.96$\\pm$0.14**|**-10.61$\\pm$0.29**|**-9.11**|\n\n**2. About Q2 Your methods are not carefully designed for protein inverse folding. However, the presented results outperformed current SOTA by a large margin (curent SOTA is about 50%-60% AAR), which could not convince me. I doubt the veracity and correctness of the experimental results.** \n\nAns: Sorry for the confusion. We evaluate the inverse-folding task on our own two datasets instead of the original CATH. For our datasets, the proteins are from the same family, and thus they may have much overlap on the sequence level, which definitely will lead to high AAR. Besides, our model is initialized with ESM2 weights, which will also improve the AAR. \n\nAlthough the sequence prediction based a fixed backbone structure is not our objective, **we adapt our method to CATH setting by keeping all the CA coordinates and masking all residues. Due to the approaching ddl and limited time, our model has only been trained for 10 epochs, while usually the suggested training epoch would be 100 like GVP, PiFold, etc.** (See below: ESMIF, ProteinMPNN, PiFold results are quoted from PiFold paper). \n\nCATH 4.2 results\n\n|Model| AAR (%)|PPL |\n|:-----|:----:|:----:|\n|ProteinMPNN|45.96|4.61|\n|ESMIF|38.30|6.44|\n|PiFold|51.66|4.55|\n|NAEPro|9.16|15.26|\n\n**Again, we did not claim the superiority of our method on inverse folding task on CATH dataset.**\n\n**3. Page 1, \"Despite their great potential for novel structure design, such sequential design policy fails to cross-condition on sequence and structure, which might lead to inconsistent proteins and inefficient design process\". I can not agree with this statement. RFDiffusion's authors say that they also considered simultaneously designing structure and sequence within RFdiffusion, but combining ProteinMPNN with the diffusion of structure alone provides the excellent performance, as shown in your Table.1. Unfortunately, I observe that you deleted the consistency metric in Table.1, which seems like dishonest behavior to me.**\n\nAns: We do not intend to hide any information. As suggested by reviewer MgyU, the binding affinity is a better metric for functional protein design rather than the accuracy metrics (consistency, AAR, RMSD, etc.). Therefore, we replaced all the accuracy metrics in Table 1 and 2 in our paper according with the binding affinity scores calculated by Gnina. The full table with all scores are listed below:\n\n$\\beta$-lactamase\n\n|Model| AAR (\\%,$\\uparrow$) | RMSD ({ \\AA},$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination | $4.79$ | $--$ | $30.5511$ | $0.2918$ | $--$|\n|Inpainting | $16.73$ | $4.0599$ | $61.7679$ | $0.3790$ | $6.2578$|\n|SMCDiff+ProteinMPNN| $19.94$ | $10.3960$ | $42.0375$ | $0.3458$ |$10.2117$|\n| PROTSEED | $37.63$ | $3.0142$ | $64.3861$| $0.4637$ | $3.3748$ |\n| FrameDiff+IF |$26.20$ | $6.0151$ | $65.6445$ | $0.3657$ | $7.8703$|\n|RFDiffusion+IF | $22.93$ | $6.0438$ | **83.4058** | $0.3747$ |**0.5565**|\n|NAEPro|**65.71**| **2.9916**| 66.8705|**0.4812**| 7.3760|\n\n\n**Myoglobin**\n|Model| AAR (\\%,$\\uparrow$) | RMSD (\\AA,$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination| $4.81$ | $--$ | $38.2817$ | $0.2754$ |--|\n|Inpainting |39.59|3.3751|67.0813|0.4391 |3.2108|\n|SMCDiff+IF | $12.47$ | $8.0067$ | $34.5914$ | $0.2235$ | $8.8754$ |\n| PROTSEED | $48.55$ | $2.8753$ | $61.5588$ | $0.5466$ | $0.9764$ |\n| FrameDiff+IF | $20.43$ | $5.9739$ | $61.3945$ | $0.3757$ |$3.5078$|\n|RFDiffusion+IF | $32.70$ | $3.9930$ | $78.9868$ | $0.4147$  | **0.4375** |\n|NAEPro | **89.37**| **2.6307**|**81.9507**|**0.5692**|4.3865|"}
{"claim": "No experimental results are provided on the CATH dataset to compare with SMCDiff and FrameDiff.", "claim_type": "experimental", "paper_id": "Dr4qD9bzZd", "paper_title": "Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "a9hHYfEm0G", "reviewer": "Reviewer_Wq6M", "review_text": "Summary: This paper proposes NAEPro, a model to jointly design Protein sequence and structure. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from the nearest amino acids in three-dimensional (3D) space. The global attention sub-layer parameters are initialized with ESM-2. The author combines ESM2 and EGNN for co-modeling protein sequence and structure.\n\nStrengths: 1. The reported performance is good.\n2. The method is simple.\n\nWeaknesses: 1. Novelty: Both EGNN and ESM2 are existing models, and there are many existing works on antibody structure and sequence co-design. The combination may limit the novelty of this method.  \n2. Significance: It is likely that previous works can be readily applied to the motif-conditioned setting, and the authors can easily adapt their method to antibody design tasks. It would be beneficial if the authors further elaborate on the significance of their work\n3. Code: The authors do not provide code for checking the soundness of the methods.\n4. Experiment setting: The authors do not provide results on standard benchmarks, such as the CATH dataset, for fair comparison on both sequence and structure design.\n\nQuestions: 1. Could you provide experimental results from the CATH dataset to compare with the original SMCDiff and FrameDiff results?\n2. Similarly, could you provide head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design? Please follow the same setting.\n3. Could you provide the code for checking the results?", "labeling_timestamp": "2026-01-11T16:29:15.301521", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors acknowledge they did not provide experiments on the CATH dataset and justify this by explaining CATH is a different benchmark (inverse-folding, diverse families) and that SMCDiff/FrameDiff have not been evaluated on CATH, so they did not run those comparisons.", "evidence": "We would like to clarify our task more clearly. Our task aims to co-design functional and novel protein sequence and structure. In our method, we use MSAs to automatically find motifs to guarantee the protein function, which is achieved within the same protein family. Instead, CATH is a protein dataset consisting of proteins from diverse families, making the motif extraction difficult. Besides, CATH is usually used to evaluate inverse folding task, say protein sequence design based on fixed backbone structure, which is not the goal of our paper. We hope we didn’t misunderstand the reviewer’s meaning by “the original SMCDiff and FrameDiff results” as SMCDiff and FrameDiff haven’t been evaluated on CATH dataset.", "combined_author_response": "We much appreciate your valuable comments and insightful suggestions again, which helps a lot to improve the quality of our paper. As the discussion deadline is approaching, please let us know if you have any further concerns or questions. We are happy to have any further discussion!\n\n---\n\n**4. \"Through this way, amino-acid combinations frequently occur in the same context would draw higher attention scores\". Have you checked this statement? Could you provide some visualizations of the attention map and correlated frequences of amino-acid combinations?**  \n\nAns: Thank the reviewer for this suggestion. We randomly pick one sentence in the training set and visualization its attention matrix from the last layer in Appendix Figure 7(a) and also visualize the pairwise amino acid co-occurrence matrix in Figure 7(b). We find it’s consistent with our statement that residues tend to more connect to their neighboring residues. However, we don’t see any strong correlation between the attention matrix and the pairwise amino acid co-occurrence. Therefore, we revised the corresponding statement in our paper and uploaded a revised version accordingly.\n\n**5. Updating residue representations and coordinates in 3D space with only nearest neighbors enables more efficient and economic message passing compared to prior approaches which compute messages on the complete pairwise residue graph\". To my knowledge, most of the previous methods such as proteinmpnn adopt knn for constructing sparse graph in the 3D spcace. You should provide evidence to support the significance of your statement.**\n\nAns: We mean from the architecture level, previous methods like SE(3)-Transformer [1] and EGNN [2] adopts information flow from all other atoms.\n\n[1] SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks. Fabian B. Fuchs et al. NeurIPS 2020. \n\n[2] E(n) Equivariant Graph Neural Networks. Victor Garcia Satorras et al. ICML 2021. \n\n**6. Novelty: I do not observe enough novelty from the perspective of machine learning in algorithm design.**\n\nAns: Yes, from machine learning algorithm level, our method is not new. However, as admitted by reviewer MgyU, we design a new architecture for protein design. The key innovations of our architecture include: (1) interleaving layers of sequence-level attention and local neighborhood equivariant sub-layer (2) accelerating the local sub-layer with the k-nearest neighbors.\n\n---\n\nThanks for the responses! Our answers to your follow-up questions are provided as follows:\n\n**1. About Weakness4 & Q1 Page 2, \"We carry out experiments on two metalloproteins, including β-lactamase and myoglobin.\" I would like to say that the experimental setting is the main concern to me. Since the authors have only evaluated the proposed method on two structures, I am not sure if the method can be extended to generalized situations. I know that you are doing protein sequence-structure inpaintning problem, in which case a comparison with a previous baselines [1,2,3] on stantard datasets may help to reveal the generalization capabilities of your algorithm.** \n\nAns: We agree with reviewer Wq6M and additionally compare our method with PROTSEED on B12 protein. Due to the approaching ddl and limited time, PROTSEED has only been trained for 100 epochs, the same as our method. (suggested training epoch is 2000 on their GitHub)\n\nThe results are reported as follows: \n\n**Binding affinity score for designed B12**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|PROTSEED|-9.45$\\pm$ 0.03|-9.36$\\pm$0.09|-9.04$\\pm$0.27|-8.46|\n|NAEPro|**-11.06$\\pm$0.14**|**-10.96$\\pm$0.14**|**-10.61$\\pm$0.29**|**-9.11**|\n\n**2. About Q2 Your methods are not carefully designed for protein inverse folding. However, the presented results outperformed current SOTA by a large margin (curent SOTA is about 50%-60% AAR), which could not convince me. I doubt the veracity and correctness of the experimental results.** \n\nAns: Sorry for the confusion. We evaluate the inverse-folding task on our own two datasets instead of the original CATH. For our datasets, the proteins are from the same family, and thus they may have much overlap on the sequence level, which definitely will lead to high AAR. Besides, our model is initialized with ESM2 weights, which will also improve the AAR. \n\nAlthough the sequence prediction based a fixed backbone structure is not our objective, **we adapt our method to CATH setting by keeping all the CA coordinates and masking all residues. Due to the approaching ddl and limited time, our model has only been trained for 10 epochs, while usually the suggested training epoch would be 100 like GVP, PiFold, etc.** (See below: ESMIF, ProteinMPNN, PiFold results are quoted from PiFold paper). \n\nCATH 4.2 results\n\n|Model| AAR (%)|PPL |\n|:-----|:----:|:----:|\n|ProteinMPNN|45.96|4.61|\n|ESMIF|38.30|6.44|\n|PiFold|51.66|4.55|\n|NAEPro|9.16|15.26|\n\n**Again, we did not claim the superiority of our method on inverse folding task on CATH dataset.**\n\n**3. Page 1, \"Despite their great potential for novel structure design, such sequential design policy fails to cross-condition on sequence and structure, which might lead to inconsistent proteins and inefficient design process\". I can not agree with this statement. RFDiffusion's authors say that they also considered simultaneously designing structure and sequence within RFdiffusion, but combining ProteinMPNN with the diffusion of structure alone provides the excellent performance, as shown in your Table.1. Unfortunately, I observe that you deleted the consistency metric in Table.1, which seems like dishonest behavior to me.**\n\nAns: We do not intend to hide any information. As suggested by reviewer MgyU, the binding affinity is a better metric for functional protein design rather than the accuracy metrics (consistency, AAR, RMSD, etc.). Therefore, we replaced all the accuracy metrics in Table 1 and 2 in our paper according with the binding affinity scores calculated by Gnina. The full table with all scores are listed below:\n\n$\\beta$-lactamase\n\n|Model| AAR (\\%,$\\uparrow$) | RMSD ({ \\AA},$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination | $4.79$ | $--$ | $30.5511$ | $0.2918$ | $--$|\n|Inpainting | $16.73$ | $4.0599$ | $61.7679$ | $0.3790$ | $6.2578$|\n|SMCDiff+ProteinMPNN| $19.94$ | $10.3960$ | $42.0375$ | $0.3458$ |$10.2117$|\n| PROTSEED | $37.63$ | $3.0142$ | $64.3861$| $0.4637$ | $3.3748$ |\n| FrameDiff+IF |$26.20$ | $6.0151$ | $65.6445$ | $0.3657$ | $7.8703$|\n|RFDiffusion+IF | $22.93$ | $6.0438$ | **83.4058** | $0.3747$ |**0.5565**|\n|NAEPro|**65.71**| **2.9916**| 66.8705|**0.4812**| 7.3760|\n\n\n**Myoglobin**\n|Model| AAR (\\%,$\\uparrow$) | RMSD (\\AA,$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination| $4.81$ | $--$ | $38.2817$ | $0.2754$ |--|\n|Inpainting |39.59|3.3751|67.0813|0.4391 |3.2108|\n|SMCDiff+IF | $12.47$ | $8.0067$ | $34.5914$ | $0.2235$ | $8.8754$ |\n| PROTSEED | $48.55$ | $2.8753$ | $61.5588$ | $0.5466$ | $0.9764$ |\n| FrameDiff+IF | $20.43$ | $5.9739$ | $61.3945$ | $0.3757$ |$3.5078$|\n|RFDiffusion+IF | $32.70$ | $3.9930$ | $78.9868$ | $0.4147$  | **0.4375** |\n|NAEPro | **89.37**| **2.6307**|**81.9507**|**0.5692**|4.3865|\n\n---\n\nThanks for the further response! Please see our responses to your follow-up questions and concerns as follows:\n\n**Response to Weakness 3 & Q1: My point in mentioning these three papers is not to introduce additional (and possibly meaningless) in-silico comparisons to the authors. Instead, they validated their designs with solid wet-lab experiments demonstrating that it is possible to generate novel protein sequences with fixed backbones.**\n\nAns: We agree on the reviewer’s opinion and have accordingly updated the statement in Introduction.  We have uploaded a revised version of our paper.\n\n**Response to Weakness 4: I completely agree that all the existing evaluation metrics we used in deep learning are only indirect indicators, and they only imply a higher chance that the assessed protein will perform its function. However, I have trouble following the logic that \"*Therefore, a low TM-score cannot indicate the designed protein doesn’t have the corresponding function*\" suggests \"proteins with TM-scores that are lower than 0.5 (which is a wildly-used standard) are still reliable in terms of their functionality in general.\" Similarly, if we all agree that the existing metrics are suboptimal and none of them provides 100% accurate evaluations, it is then meaningless to criticize the widely applied standards with one or two counter-examples. After all, you don't have any better evidence (such as wet-lab results) that is more reliable than the existing criteria.**\n\nAns: We agree with Reviewer VH6U. As suggested by reviewer MgyU, the binding affinity is a better metric for functional protein design. We calculated the binding affinity scores for top-5, top-10, and top-30 candidates and also median score on the test set using Gnina. The results are reported as follows:\n\n**Binding affinity score for designed myoglobin**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-8.18$\\pm$ 0.01|-8.07$\\pm$0.03|-7.97$\\pm$0.23|-7.25|\n|Inpainting|-13.47$\\pm$0.02|-13.12$\\pm$0.12|-12.31$\\pm$0.54|-9.56|\n|SMCDiff+ProteinMPNN|-11.37$\\pm$0.03|-11.12$\\pm$0.31|-10.87$\\pm$0.42|-8.76|\n|PROTSEED|-13.21$\\pm$0.13|-12.89$\\pm$0.42|-11.98$\\pm$0.52|-10.23|\n|FrameDiff+ProteinMPNN|-13.13$\\pm$0.05|-12.92$\\pm$0.16|-12.21$\\pm$0.23|-10.08|\n|RFDiffusion+ProteinMPNN|-13.68$\\pm$0.02|-13.03$\\pm$0.21|-12.56$\\pm$0.43|-10.15|\n|NAEPro|**-14.12$\\pm$0.01**|**-13.85$\\pm$0.10**|**-13.06$\\pm$0.38**|**-10.74**|\n\n**Binding affinity score for designed beta-lactamase**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-6.98$\\pm$0.01|-6.87$\\pm$0.02|-6.69$\\pm$0.05|-6.29|\n|Inpainting|-9.89$\\pm$0.03|-9.54$\\pm$0.16|-9.13$\\pm$0.43|-7.24|\n|SMCDiff+ProteinMPNN|-9.10$\\pm$0.01|-9.05 $\\pm$0.02|-8.98$\\pm$0.01|-6.97|\n|PROTSEED|-9.88$\\pm$0.21|-9.51$\\pm$0.41|-9.01$\\pm$0.62|-7.31|\n|FrameDiff+ProteinMPNN|-9.54$\\pm$0.03|-9.56$\\pm$0.23|-8.89$\\pm$0.35|-7.03|\n|RFDiffusion+ProteinMPNN|-9.87$\\pm$0.05|-9.56$\\pm$0.23|-9.12$\\pm$0.53|-7.51|\n|NAEPro|**-10.06$\\pm$0.05**|**-9.79$\\pm$0.10**|**-9.39$\\pm$0.12**|**-7.66**|\n\n**Response to Q6: Because ESMFold is a less reliable tool for evaluating novel proteins, it should not be used here to assess your designs at all.**\n\nAns: We totally agree with reviewer VH6U. We provide the mean pLDDT for top-5, top-10, top-30 candidates calculated by AlphaFold2 as follows:\n\n**AlphaFold2 pLDDT for designed myoglobin**\n\n|Model| top-5|top-10|top-30|\n|:-----|:----:|:----:|:----:|\n|Hallucination|76.7243|73.5953|68.6060|\n|Inpainting|97.6171|97.6063|97.5681|\n|SMCDiff+ProteinMPNN|85.7051|84.7744|84.9220|\n|PROTSEED|92.9770|91.9216|90.4293|\n|FrameDiff+ProteinMPNN|97.3997|97.3329|96.4279|\n|RFDiffusion+ProteinMPNN|97.9722|97.9174|97.6063|\n|NAEPro|**98.1057**|**98.0706**|**97.9197**|\n\n**AlphaFold2 pLDDT for designed $\\beta$-lactamase**\n\n|Model| top-5|top-10|top-30|\n|:-----|:----:|:----:|:----:|\n|Hallucination|74.7760|74.7032|71.2932|\n|Inpainting|94.5489|94.4328|93.2941|\n|SMCDiff+ProteinMPNN|86.2510|85.7072|83.0376|\n|PROTSEED|98.2896|98.2740|97.9683|\n|FrameDiff+ProteinMPNN|98.3049|98.2922|98.1267|\n|RFDiffusion+ProteinMPNN|**98.6225**|**98.5116**|98.0643|\n|NAEPro|98.5385|98.4481|**98.2841**|\n\n---\n\nDear reviewer,\n\n    We sincerely appreciate the time and effort you dedicated to reviewing our paper and reading our responses. Your valuable suggestions and insightful comments have significantly contributed to refining the quality of our work. No matter what the final result will be, the thoughtful communication with you has better clarified our paper's goals and logic. We would like to express our great gratitude to you!\n\nBest,\n\nAuthors of paper 4608\n\n---\n\n**Could the authors briefly explain how do you construct the one-to-one mapping between the design structure and the target structure in PDB? Is it determined by retrieving the one in PDB with the smallest RMSD? As far as I know, there is no common practice to determine this for the task of protein design, and I appreciate any explanation that clues me in.**\n\nAns: Yes, we construct the one-to-one mapping between the designed structure and the target structure in PDB. It is not determined by retrieving the one in PDB with the smallest RMSD. We extract the motif fragments for each protein, based on which our training objective is to recover the original protein sequence and backbone structure. Therefore, we compute the RMSD between the designed structure and the target one which we want to recover.\n\n---\n\n**Corrected statement: But it can still bind the corresponding metallocofactor heme in the AlphaFill simulation. For the claim “binding rates of 100% and 90%”, I wonder how is the criterion/threshold of “binding” determined? This success rate is rather unrealistic and probably fails to reflect the real performance. Thus, the statement in Section 5.2. saying “It is evident that our method can generate proteins that express the basic and important functions.” can be quite misleading for readers who are not familiar with the task.** \n\nAns: Sorry for the misleading statement. We mean through the AlphaFill prediction, the designed proteins are highly potential to bind the corresponding metallocofactors. We have corrected all the claims in our paper. We provided the process of how to decide if the designed protein can “bind” the metallocofactors in Appendix B.3. We would like to further explain it here.  First of all, we use AlphaFill to do the first-step prediction. AlphaFill will return a complex based on the input protein structure. If the complex includes the corresponding metal ions, we think the designed protein is potential to have functions. Then to guarantee the results convincing, we further use some additional constraints to ensure the designed proteins are potential to express functions. Specifically, we assume if a designed protein share the similar active site environments with their natural counterparts (target protein in our dataset), then it’s highly potential to bind the corresponding metal ions. For myoglobin, we calculate the distance between axial histidine ligand to Fe ion. Besides, we also detect the presence of distal histidine within the active sites, which plays an important role in the oxygen molecule binding function of myoglobin . If the distance is between 2.0Å and 2.5Å, and the distal histidine exists, we think the designed myoglobins share similar active site environments to natural ones. For $beta$-lactamase, we detect the residues that directly contact with zinc ion. If they show similar chemistry properties with natural $beta$-lactamases, we think they can highly potentially bind the corresponding metallocofactors. For example, in Figure 5(a), the active site possesses one zinc atom coordinated to three histidines, while the second zinc atom is coordinated to one histidine, one cysteine, and one aspartate, demonstrating a high degree of amino acid analogy to the natural protein.\n\n**Could the author intuitively describe how does the NAEL deal with the reflection case so as to be simply SE(3)-equivariant instead of E(3)?** \n\nAns: Intuitively, applying reflection to the input structure, the output structure will also be reflected. The conformation of the original backbone will be changed from L-amino acid to D-amino acid (We provide an illustration in Figure 6 in appendix). Therefore, the reflection will change the chirality of the protein which may cause the deficiency of binding to the ligand and eventually its function. However, the protein sequence in our model will not be influenced. Therefore, the 3D structure of the designed sequence which is L-amino acid and the output structure  which becomes D-amino acid will be inconsistent. From this aspect, our model doesn’t satisfy reflection equivariance.\n\n**The authors are encouraged to encompass appropriate comparison with EGNN/SE(3)-transformer in their writing to better contextualize the proposed NAEL.**\n\nAns: We provide the comparison with EGNN+ESM2 on myoglobin in the following Table and also updated the results in Table2 accordingly.\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|EGNN+ESM2|-13.30$\\pm$0.61 | -12.77$\\pm$0.69 |-12.07$\\pm$0.65 | -9.63|\n|NAEPro|**-14.12$\\pm$0.01**|**-13.85$\\pm$0.10**|**-13.06$\\pm$0.38**|**-10.74**|\n\n**It is surprising to see scaling the model does not yield consistent performance gain. Could the authors help explain this behavior? Also, I noticed that the authors merge the two myoglobin and lactamase data for training newly in the rebuttal response. Is there any insight for doing such multi-task learning?**\n\nAns: Enlarging the model hasn’t improved the performance too much. Our interpretation is that the training data size is moderate, and thus a model with moderate size can fit the data well. To validate if this guess holds, we merge the two protein data and then train a unified model. As we can see in the previous table, our model did achieve obvious better results on $\\beta$-lactamase, i.e., TM-score from below 0.5 (0.4919) to over 0.5 (0.5055).\n\n---\n\nWe much appreciate the reviewer’s responses and suggestions, which help a lot to improve our paper.  We have accordingly revised our paper, including Table 1, Table 2, Section 4.2-4.4 and Section 5.2. Our responses to the reviewer’s follow-up questions are provided as follows:\n\n**About the evaluation metrics: I think the binding affinity could better support the effectiveness of NAEPro and be more suitable for the task (functional protein design) rather than the accuracy metrics (RMSD, AAR, etc.). Do the authors agree on this or not?** C**ould the authors explain how the Binding Affinity is calculated for both model based on Gnina? Specifically, I am worried that the metrics is averaged among the test set similar to other metrics.  If so, I suggest that the authors report median or top-k mean as evaluation.**\n\nAns: We completely agree with reviewer MgyU that the binding affinity is a better metric for functional protein design rather than the accuracy metrics. We revised Table 1 and 2 in our paper according to your suggestion. The binding affinity is calculated using Gnina as the average score on the test set. Thank the reviewer for this valuable suggestion. We provide  top-5, top-10, top-30 mean and variance as well as the median in the following table and also revise our paper accordingly. From the results, we can see that our model achieves the best median and top-K binding affinity scores on both datasets.\n\n**Myoglobin**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-8.18$\\pm$ 0.01|-8.07$\\pm$0.03|-7.97$\\pm$0.23|-7.25|\n|Inpainting|-13.47$\\pm$0.02|-13.12$\\pm$0.12|-12.31$\\pm$0.54|-9.56|\n|SMCDiff+ProteinMPNN|-11.37$\\pm$0.03|-11.12$\\pm$0.31|-10.87$\\pm$0.42|-8.76|\n|PROTSEED|-13.21$\\pm$0.13|-12.89$\\pm$0.42|-11.98$\\pm$0.52|-10.23|\n|FrameDiff+ProteinMPNN|-13.13$\\pm$0.05|-12.92$\\pm$0.16|-12.21$\\pm$0.23|-10.08|\n|RFDiffusion+ProteinMPNN|-13.68$\\pm$0.02|-13.03$\\pm$0.21|-12.56$\\pm$0.43|-10.15|\n|NAEPro|**-14.12$\\pm$0.01**|**-13.85$\\pm$0.10**|**-13.06$\\pm$0.38**|**-10.74**|\n\n**Beta-lactamase**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-6.98$\\pm$0.01|-6.87$\\pm$0.02|-6.69$\\pm$0.05|-6.29|\n|Inpainting|-9.89$\\pm$0.03|-9.54$\\pm$0.16|-9.13$\\pm$0.43|-7.24|\n|SMCDiff+ProteinMPNN|-9.10$\\pm$0.01|-9.05 $\\pm$0.02|-8.98$\\pm$0.01|-6.97|\n|PROTSEED|-9.88$\\pm$0.21|-9.51$\\pm$0.41|-9.01$\\pm$0.62|-7.31|\n|FrameDiff+ProteinMPNN|-9.54$\\pm$0.03|-9.56$\\pm$0.23|-8.89$\\pm$0.35|-7.03|\n|RFDiffusion+ProteinMPNN|-9.87$\\pm$0.05|-9.56$\\pm$0.23|-9.12$\\pm$0.53|-7.51|\n|NAEPro|**-10.06$\\pm$0.05**|**-9.79$\\pm$0.10**|**-9.39$\\pm$0.12**|**-7.66**|\n\n**Corrected statement: ESMFold might not accurately predict the structure for a novel sequence.**\n\nAns: Sorry for the misunderstood claim. We want to express we agree on reviewer VH6U’s comment in question 6 “for a novel sequence, its prediction is worse than AlphaFold2.”  As suggested by reviewer VH6U, we re-calculate pLDDT by AlphaFold2. Our model respectively achieves 91.4139 and 76.9550  on myoglobin and $\\beta$-lactamase on average. As claimed in previous work [1,2], pLDDT between 70~90 are classified to be confident and pLDDT ≥ 90 indicates residues predicted with extremely high confidence.\n\n[1] pLDDT Values in AlphaFold2 Protein Models Are Unrelated to Globular Protein Local Flexibility. Oliviero Carugo. Crystals. 2023.\n\n[2] Generating new protein sequences by using dense network and attention mechanism. Feng Wang et al. Mathematical Biosciences and Engineering. 2023.\n\n---\n\nThanks for your valuable review and comments again. We have addressed all your concerns and questions.\n\nIf you have any other questions, please feel free to discuss with us.\n\n---\n\nThanks for your valuable review and comments again. We have addressed all your concerns and questions.\n\nIf you have any other questions, please feel free to discuss with us.\n\n---\n\nThanks for your valuable review and comments again. We have addressed all your concerns and questions.\n\nIf you have any other questions, please feel free to discuss with us.\n\n---\n\n**Q3: In section 4.2, the author(s) described that the weights of NAEPro is partially initialized from pretrained ESM2-8M, of the smallest model size among the ESM2 series. This is very problematic but the recent common practices leverage at least the 650M model. Do the author(s) try much larger model size as initialization? I found in the Table 2 that the 8M model has only marginal improvement upon random initialization.**\n\nAns: Yes, we tried to partially initialize our model with 35M ESM2 (NAEPro-12) and also train this model on the merged myoglobin and beta-lactamase data (NAEPro-12-Unified). For myoglobin, enlarging the model to 12 layers will not influence the performance too much, but training the larger model on the merged dataset will increase the pLDDT and TM-score. For beta-lactamase, enlarging the model from 6 layers to 12 layers is beneficial and will increase the overall performance. Particularly, training the 12-layer model on merged dataset will significantly improve the performance of beta-lactamase, say improve the average TM-score from below 0.5 to above 0.5. Then we also partially initialize a 30-layer model with 150M ESM2 (NAEPro-30-Unified) and train the model on merged dataset. The performance for both myoglobin and beta-lactamase will be further improved.\n\n**myoglobin**\n\n|Model| AAR (%)|RMSD|pLDDT |TM-score |Parameters|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|NAEPro-6 |**89.37**|**2.6307**|81.9507|0.5692|11.6M|\n|NAEPro-12 |88.42|2.6333|82.2849|0.5673|52M|\n|NAEPro-12-Unified|86.81|2.6443|82.9736|0.5736|52M|\n|NAEPro-30-Unified|86.62|2.6410|**83.0173**|**0.5749**|223.7M|\n\n**$\\beta$-lactamase**\n\n|Model| AAR (%)|RMSD|pLDDT |TM-score |Parameters|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|NAEPro-6 |65.71|2.9916|66.8705|0.4812|11.6M|\n|NAEPro-12 |67.28|2.9864|68.7394|0.4919|52M|\n|NAEPro-12-Unified|70.93|**2.9707**|69.9333|0.5055|52M|\n|NAEPro-30-Unified|**72.60**|2.9781|**71.0429**|**0.5089**|223.7M|\n\n**Q4: How is the “target structure” in evaluation metrics(Section 4.2) determined? What is the difference between metrics RMSD and consistency?**\n\nAns: Target structure means the natural protein structure provided in PDB. RMSD is calculated as the RMSD between the designed protein structure and the one given in PDB. Consistency is calculated as the RMSD between the designed structure and the one predicted by ESMFold for the designed sequence.\n\nWe really hope our responses address your concerns. If you have any other questions, we are very happy to continue discussions!\n\n---\n\n**Weakness 4: For the evaluation metrics, the problem to be studied is functional protein (as it is claimed in the title) design instead of general backbone generation(eg., RFDiffusion) or sequence generation (eg., ProteinMPNN). Thus, I found the the five of the metrics fail to (or indirectly if they potentially do) reflect the performance of functional protein design or scaffolding. All these metrics in Table 1 & 2 show the somehow sequence-structure matching between the predicted and native, but are not very suitable for proteins such as enzyme.**\n\nAns: In our analysis (Section 5.2), we evaluated the function for binding the corresponding metallocofactors of the designed proteins. Specifically, we first randomly select 20 cases from the top100 sequences (pLDDT ranking). We then employ AlphaFold2 for protein structure prediction, followed by inputting these structures into AlphaFill [1] to predict the associated ligands. Notably, our results indicate that all designed myoglobins exhibit heme binding capability, while 18 beta-lactamases demonstrate the ability to bind zinc ions, resulting in metallocofactor binding rates of 100% and 90%, respectively. It is evident that our method can generate proteins that express the basic and important functions, i.e. binding the corresponding metallocofactors. To systematically illustrate the function of our designed proteins, we use docking tool Gnina[2] to compute the binding affinity between the designed metalloproteins and the corresponding metallocofactors. The results in the following table shows that our model achieves the best binding affinity scores on both metalloproteins, demonstrating the proteins designed by our model are highly potential to actively exhibit biochemical functions.\n\n|Model| myoglobin (kcal/mol)|beta-lactamase (kcal/mol)|\n|:-----|:----:|:----:|\n|Hallucination|-7.23|-6.37|\n|Inpainting|-9.32|-7.63|\n|SMCDiff+ProteinMPNN|-8.79|-6.89|\n|PROTSEED|-10.07|-7.68|\n|FrameDiff+ProteinMPNN|-9.54|-7.21|\n|RFDiffusion+ProteinMPNN|-9.76|-7.59|\n|NAEPro|**-10.14**|**-7.80**|\n\n**Weakness 5: The potential theoretical flaw in the equivariance analysis in Section 3.5.** \n\nAns: Thank the reviewer for pointing out this typo. Our model satisfies only roto-translation equivariance without reflection equivariance. We have accordingly updated the paper.\n\n**Q1: The proposed architecture encoding both sequence and structure via the masked language modeling (MLM) scheme is somehow interesting. How do the author(s) see the  difference between the proposed NAEL layer and tensor field convolutional layer in SE(3)-transformer ? Also, what is the advantage of NAEL over the SeqIPA of the PROTSEED, the most competitive baseline in Table 1?** \n\nAns: The most biggest difference between our NAEPro and tensor field convolutional layer in SE(3)-transformer is:  NAEPro uses interleaving layers of global sequence-level attention and local neighborhood equivariant sub-layer, from which the residue representation is updated from both the whole sequence interaction and 3D neighbor message passing; instead, if we enable kNN message passing in SE(3)-transformer, then the node feature will only be updated based on 3D neighboring information. To clearly demonstrate the advantage of NAEL over SeqIPA in PROTSEED, we replace our NAEL with the SeqIPA-Addition without secondary structure feature. Taking myoglobin as an example, the results are shown in the following table. NAEPro with NAEL performs better than with SeqIPA.\n\n|Model| AAR (%)|RMSD|\n|:-----|:----:|:----:|\n|NAEPro -w/- SeqIPA|65.33|2.7078|\n|NAEPro|**89.37**|**2.6307**|\n\n**Q2:  In Section 3 - opening paragraph, the author(s) define/formulate the target task as generate a protein sequence and all 3D coordinates of N residues. However, in later model definition, the proposed NAEPro only operates on the C-alpha(CA) coordinates. Please explain this ambiguity.**\n\nAns: We are sorry for the ambiguity caused. When we mention “we formulate the target task as generate a protein sequence and all 3D coordinates of N residues”, we mean 3D coordinates of alpha-carbon of all the residues. Following the setting in [3], we represent the backbone structure as the CA-only coordinates of all residues. We have updated the paper accordingly in the new version.\n\n[3] Diffusion probabilistic modeling of protein backbones in 3D for the motif-scaffolding problem. Brian L. Trippe et al. ICLR. 2023.\n\n---\n\nWe appreciate the reviewer’s valuable and insightful suggestions, which are very helpful for us to improve our paper. We have clarified all your concerns, added the experiments and updated the paper accordingly. We address the specific concerns as follows:\n\n **Weakness 1: Regarding the consistency metrics reported, I noticed that the consistency metric of NAEPro in Table 1 (as the main results) underperformed half of the baselines.  Such key observation greatly weakened the method. The authors can elaborate on this point.**\n\nAns: Yes, we agree with the reviewer’s opinion. We would like to clarify our task more clearly. Our goal is to design functional proteins with novel sequence and structure. As shown in Figure 5(b), the designed myoglobin has a RMSD of 3.943\\AA with the most similar one in PDB and 26.7% AAR with the most similar one in Uniprot, but it can still bind the corresponding metallocofactor heme in the AlphaFill simulation [1]. In our paper, we use ESMFold to predict the structure of the designed sequence. However, as pointed out by reviewer VH6U, ESMFold might not accurately predict the structure for a novel sequence. To further clarify this point, we calculate the scRMSD of motifs between the designed structure and the predicted structure from the designed sequence in the following table. As the results show, our NAEPro achieves smaller RMSD than RFDiffusion+ProteinMPNN on myoglobin and comparable results on beta-lactamase.\n\n|Model| myoglobin|beta-lactamase|\n|:-----|:----:|:----:|\n|Inpainting|0.5292|0.7819|\n|SMCDiff+ProteinMPNN|1.3816|1.4523|\n|PROTSEED|0.4987|**0.3892**|\n|FrameDiff+ProteinMPNN|0.8765|0.9815|\n|RFDiffusion+ProteinMPNN|0.4936|0.3918|\n|NAEPro|**0.4832**|0.5239|\n\n[1] Alphafill: enriching alphafold models with ligands and cofactors. Maarten L Hekkelman et al. Nature methods. 2023.\n\n**Weakness 2: For the experiments, how is the model trained to obtain the evaluation digits in Table 1 for each baseline. This is very important.** \n\nAns: We are sorry for the confusion. We got the codes of all baselines from their GitHub repos, and then re-trained all the models separately on myoglobin and beta-lactamase datasets. The evaluation procedure among all the baselines and our model follows the same process, and thus we believe the comparisons in Table 1 are fair.\n\n**Weakness 3: For the dataset, I found that in section 4.1, the author(s) mentioned that both beta-lactamase and myoglobin datasets are split randomly. This is worrying because the entries in PDB can usually be redundant, especially for such large and well-studied families that the authors selected for evaluation. The author(s) can elaborate on this point. Moreover, in the Table 3 (in appendix), I am curious about the column name “PDB”, does it mean the number of PDB entries (with unique ID) or the number of single chains?** \n\nAns: Yes, this is a very good question. Since we focus on sequence and structure co-design task, we aim to design both functional sequence and structure. Therefore, our input data is the <sequence, structure> pair. From this point, even though the sequence might be the same, the structure is somehow different. Therefore, our paired input data are different and would not be redundant. We appreciate the reviewer for carefully reviewing our paper and also reading the appendix. In Table 3, PDB means the number of different PDB entries. To eliminate the reviewer’s concerns, we re-cleaned the test set by filtering pairs whose sequences have over 30% sequence identity to any sample in the training set after doing pairwise alignment for myoglobin. We compare our NAEPro and ProtSeed (the most competitive baseline) in the following table. As the results show, our model can design proteins with higher binding affinity (by Gnina [2]) on samples that have lower similarity to training data.\n\n|Model| RMSD|Binding Affinity (kcal/mol)|\n|:-----|:----:|:----:|\n|PROTSEED|**2.4031**|-9.62|\n|NAEPro|2.6634|**-9.98**|\n\n[2] GNINA 1.0: Molecular docking with deep learning. A McNutt et al. Cheminformatics. 2021.\n\n---\n\n**Weakness4 & Q1: The authors do not provide results on standard benchmarks, such as the CATH dataset, for fair comparison on both sequence and structure design. Could you provide experimental results from the CATH dataset to compare with the original SMCDiff and FrameDiff results?**\n\nAns: We would like to clarify our task more clearly. Our task aims to co-design functional and novel protein sequence and structure. In our method, we use MSAs to automatically find motifs to guarantee the protein function, which is achieved within the same protein family. Instead, CATH is a protein dataset consisting of proteins from diverse families, making the motif extraction difficult. Besides, CATH is usually used to evaluate inverse folding task, say protein sequence design based on fixed backbone structure, which is not the goal of our paper. We hope we didn’t misunderstand the reviewer’s meaning by “the original SMCDiff and FrameDiff results” as SMCDiff and FrameDiff haven’t been evaluated on CATH dataset. Since SMCDiff and FrameDiff focus on protein structure design, CATH is usually used to evaluate protein sequence design based on fixed backbone structure, which may have some discrepancies on the design goals. \n\n**Q2: Could you provide head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design? Please follow the same setting.**\n\nAns: We provide protein sequence design given backbone structures on our two datasets in the following table. In this setting, we slightly modified our method to make it adaptable to the task as the reviewer asked. Specifically, we provided the whole backbone structure as the model input and designed the whole protein sequence. As the results show, even though we only provide  backbone structure without any motif residues, our model can still achieves higher AAR and pLDDT on myoglobin, and higher AAR on beta-lactamase.\n\n**myoglobin**\n\n|Model| AAR (%)|pLDDT |\n|:-----|:----:|:----:|\n|ProteinMPNN|81.37|81.9817|\n|ESMIF|76.49|78.0986|\n|PiFold|78.93|80.2912|\n|NAEPro|**85.56**|**82.3871**|\n\n$\\beta$-lactamase\n\n|Model| AAR (%)|pLDDT |\n|:-----|:----:|:----:|\n|ProteinMPNN|53.59|**70.6723**|\n|ESMIF|57.49|61.3928|\n|PiFold|63.38|64.9017|\n|NAEPro|**68.39**|65.8218|\n\n**Q3: Could you provide the code for checking the results?**\n\nAns: We have provided the code and data in the supplementary material. Due to the memory limitation, we couldn’t upload the model checkpoints. However, the training process is highly efficient, and the reviewer could train the model if he/she is interested.\n\n---\n\nWe thank the reviewer for the positive reviews as well as the suggestions for improvement. We have clarified all your concerns, added the experiments and updated the paper accordingly. Our responses to the reviewer’s concerns and questions are provided below:\n\n**Weakness 1: Both EGNN and ESM2 are existing models, and there are many existing works on antibody structure and sequence co-design. The combination may limit the novelty of this method.**\n\nAns: We would like to point out that our NAEPro is not a combination of ESM2 and EGNN. Instead our key innovations are (1) interleaving layers of sequence-level attention and local neighborhood equivariant sub-layer (2) accelerating the local sub-layer with the k-nearest neighbors.  On the contrary,  EGNN is a fully-connected graph and updates message, coordinates and atom features based on all other atoms in 3D space, which is not efficient for long proteins. Besides, EGNN+ESM2 is a sequential process and can not cross-condition on sequence and structure. We compare our method with EGNN+ESM2, NAEPro w/o ESM2 initialization on myoglobin in the Following Table (Table 2 in the paper).  As the results show, EGNN+ESM2 performs much worse than our NAEPro, and removing the ESM2 initialization will not influence the performance too much, demonstrating that our model is much more superior than EGNN+ESM2.\n\n|Model| AAR (%)|RMSD|pLDDT |TM-score |consistency|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|EGNN+ESM2|51.12|2.9891|77.3399|0.4656|4.9827|\n|NAEPro-w/o-ESM2 initialization|79.82|2.6398|76.3032|0.5159|4.7273|\n|NAEPro|**89.37**|**2.6307**|**81.9507**|**0.5692**|**4.3865**|\n\n**Weakness 2: It is likely that previous works can be readily applied to the motif-conditioned setting, and the authors can easily adapt their method to antibody design tasks. It would be beneficial if the authors further elaborate on the significance of their work**\n\nAns: We would like to clarify our goal more clearly. Our task is to co-design functional and novel protein sequence and structure. Previous work which is most similar to ours is Inpainting [1]. However, [1] provides the functional sites manually while we automatically mined the functional sites (plus the conserved sites as suggested by reviewer ****VH6U****) by MSAs. Previous methods considering only sequence design constrained by fitness value [2,3] or sequence design based on given backbone structure (inverse folding) [4] is not suitable for our task. For antibody design, antibody is always Y-shaped, while one of our goal is to design novel and diverse protein structure. For example, in Figure 4, the designed beta-lactamases have different fold categories and belong to three different sub-classes. In Figure 5 (b), the designed myoglobin has a RMSD of 3.943\\AA with the most similar one in PDB, but it can still bind the corresponding metallocofactor heme in the AlphaFill simulation [5]. From this aspect, we think antibody design models can not be directly applied to our task without any modification and vice versa. \n\n[1] Scaffolding protein functional sites using deep learning.  Jue wang. et al. Science. 2022.\n\n[2] Biological Sequence Design with GFlowNets. [Moksh Jain](https://arxiv.org/search/q-bio?searchtype=author&query=Jain,+M) et al. ICML 2022. \n\n[3] Proximal Exploration for Model-guided Protein Sequence Design. Zhizhou Ren et al. ICML 2022. \n\n[4] Robust deep learning based protein sequence design using ProteinMPNN. J. Dauparas et al. Science. 2022.\n\n[5] Alphafill: enriching alphafold models with ligands and cofactors. Maarten L Hekkelman et al. Nature methods. 2023.\n\n**Weakness 3:** **The authors do not provide code for checking the soundness of the methods.**\n\nAns: We have provided the code and data in the supplementary material. Due to the memory limitation, we couldn’t upload the model checkpoints. However, the training process is highly efficient, and the reviewer could train the model if he/she is interested.\n\n---\n\n**Q6: For a novel sequence, ESMFold’s prediction is worse than AlphaFold2. Consequently, evaluating novel sequences should use AlphaFold2 instead of ESMFold.**\n\nAns: Due to limited time and computing resources, we provide the AlphaFold2 results for ProtSeed, RFDiffusion+ProteinMPNN and our NAEPro. As the results show, even though ESMFold achieves lower scores than AlphaFold2, the tendencies (ranking) among different models are similar.  Particularly, our model achieves an average TM-score over 0.5 on beta-lactamase (ESMFold 0.4812) and pLDDT over 70 (ESMFold 66.8705), which gives stronger evidence that our model has the ability to design proteins which have stable structures.\n\n**myoglobin**\n\n|Model| pLDDT|TM-score|\n|:-----|:----:|:----:|\n|PROTSEED|77.9032|0.5621|\n|RFDiffusion+ProteinMPNN|88.2130|0.5103|\n|NAEPro|**91.4139**|**0.5912**|\n\n**$\\beta$-lactamase**\n\n|Model| pLDDT|TM-score|\n|:-----|:----:|:----:|\n|PROTSEED|73.3176|0.4901|\n|RFDiffusion+ProteinMPNN|**87.8092**|0.4806|\n|NAEPro|76.9550|**0.5009**|\n\n **Q7: \"Consistency - What is the designed structure and the predicted structure? Which is from NAEPro? Where does the other one come from?**\n\nAns: Consistency is calculated as the RMSD between the designed structure from NAEPro and the one predicted by ESMFold for the designed sequence from NAEPro.\n\n---\n\n**Weakness 4:  Results in Table 1 do not support the superiority of the proposed method. For instance, RMSD>1.7 \\AA means the two structures are different for myoglobin. For pLDDT, the prediction on lactamase is ~66%, which is much lower than 80%-90% which is conventionally believed reliable for folding predictions. The TM-score, if they are smaller than 0.5, then the two structures are believed different.** \n\nAns: We would like to clarify our goal more clearly.  Our task is to design the sequence and structure of proteins with effective functions.  We calculated TM-score by AlphaFold2 as suggested by the reviewer, and the two proteins designed by our model both achieves an average score over 0.5 (myoglobin **0.5912** and beta-lactamase **0.5009**). However, as found by previous method [7],  protein pairs with a TM-score >0.5 are most likely in the same fold while those with a TM-score <0.5 are mainly not in the same fold. Therefore, low TM-score can not indicate the designed protein doesn’t have the corresponding function. As claimed in previous work [9,10], pLDDT between 70~90 are classified to be confident. We calculated pLDDT by AlphaFold2 as suggested by the reviewer and the two protein families designed by our model are confident on an average aspect (myoglobin **91.4139** and beta-lactamase **76.9550**).  Similarly, as observed in previous work[11,12], pLDDT has a weak correlation with protein function. Therefore, a moderate TM-score and pLDDT can not indicate a protein expresses no or poor function. As shown in figure 5(b), the designed myoglobin has a RMSD of 3.943\\AA with the most similar one in PDB, but it can still bind the corresponding metallocofactor heme in the AlphaFill simulation [8]. To systematically illustrate the function of our designed proteins, we use docking method Gnina [13] to compute the binding affinity between the designed metalloproteins and the corresponding metallocofactors. The results in the following table shows that our model achieves the best binding affinity scores on both metalloproteins, demonstrating the proteins designed by our model are highly potential to actively exhibit biochemical functions.\n\n|Model|myoglobin (kcal/mol)|beta-lactamase (kcal/mol)|\n|:-----|:----:|:----:|\n|Hallucination|-7.23|-6.37|\n|Inpainting|-9.32|-7.63|\n|SMCDiff+ProteinMPNN|-8.79|-6.89|\n|PROTSEED|-10.07|-7.68|\n|FrameDiff+ProteinMPNN|-9.54|-7.21|\n|RFDiffusion+ProteinMPNN|-9.76|-7.59|\n|NAEPro|**-10.14**|**-7.80**|\n\n[7] How significant is a protein structure similarity with TM-score= 0.5? Jinrui Xu and Yang Zhang. *Bioinformatics. 2010.* \n\n[8] Alphafill: enriching alphafold models with ligands and cofactors. Maarten L Hekkelman et al. Nature methods. 2023.\n\n[9] pLDDT Values in AlphaFold2 Protein Models Are Unrelated to Globular Protein Local Flexibility. Oliviero Carugo. Crystals. 2023\n\n[10] Generating new protein sequences by using dense network and attention mechanism. Feng Wang et al. Mathematical Biosciences and Engineering. 2023.\n\n[11] Using AlphaFold to predict the impact of single mutations on protein stability and function. Marina A. Pak et al. PloS one. 2023.\n\n[12] Peptide binder design with inverse folding and protein structure prediction. Patrick Bryant et al. Nature Communications Chemistry. 2023.\n\n[13] GNINA 1.0: Molecular docking with deep learning. A McNutt et al. Cheminformatics. 2021.\n\n**Q1 & Q2: \"The use of separate models for sequence and structure cannot ensure the consistency between the generated sequence and structure.\" \"knowing the topology of a protein before design process is difficult and also cannot guarantee the designed proteins have the desired functions.\" I disagree with this statement.** **I disagree with this statement.** \n\nAns: Thanks for the suggestion! We agree with the reviewer and updated the statement in our paper. \n\n**Q3: what is the meaning of \"generally-encoded\" for 20 types of amino acids?** \n\nAns: It means the 20 common amino acids. We have updated the paper accordingly.\n\n**Q4: \"The selection of motif varies from setting to setting...\". Can the authors please provide a clearer explanation?**\n\nAns: We mean the motifs have different meanings in different tasks. For de novo enzyme design [14], it means the binding sites where enzyme binds the metallocofactor and substrate. For de novo binder design [15], it means the binding sites where the protein binder binds the protein targets.\n\n[14] De novo enzyme design using rosetta3. Florian Richter et al. PloS one. 2011.\n\n[15] De novo design of protein interactions with learned surface fingerprints. Pablo Gainza. Nature. 2023.\n\n---\n\nWe thank the reviewer for the insightful questions. We have clarified all your concerns, added the experiments and updated the paper accordingly. Answers to specific points are provided below:\n\n**Weakness 1 & weakness 2 & Q5: The identified problem or the motivation for designing such a method is not supported by existing literature.  Some designed modules counter the intuition in biology, such as \"motif mining\"**\n\nAns: We follow the design insight proposed by [1], which starts from the functional site and fills in additional sequence and structure to create a viable protein scaffold in a single forward pass. Different from [1], we provide the functional sites which are automatically mined from MSA as previous methods [2, 3] while [1] provides the functional sites manually. Besides, we design a new architecture called NAEPro to generate the whole protein sequence and structure based on the given partial protein fragments, while [1] achieves this goal by finetuning the RoseTTAFold. However, we agree with the reviewer’s opinion in question 5 that MSA discovers not only protein functional sites as addressed in [2,3] but also finds conversed sites which may not directly relate to the protein function. Therefore, we change all the term “motif/functional sites” in our paper to  “meaningful protein fragments”.\n\n[1] Scaffolding protein functional sites using deep learning.  Jue wang. et al. Science. 2022.\n\n[2] Emerging methods in protein co-evolution. David de Juan et al. Nature Review Genetics. 2013.\n\n[3] Evolutionary information for specifying a protein fold. Michael Socolich et al. Nature. 2005.\n\n**Weakness 3 & Q1: The method is not 'extensively evaluated' as claimed by the authors (they only selectively analyzed 2 proteins), and many results do not suggest that the proposed method is SOTA among baselines. Moreover, the evaluation does not include all relevant methods, which makes it too aggressive for the authors to claim in the Introduction that their method \"achieves the HIGHEST...among ALL competitors\" and \"is faster than the FASTEST method\". Empirically, It has been proven by several recent research that the de novo method can generate new sequences for fixed backbone or desired functions [4-6].**\n\nAns: Sorry for the confusion. Our method achieves the best performance on 4/5 metrics on myoglobin and 3/5 metrics on beta-lactamase among all the compared representative baselines in our paper. Our model is faster than all baselines compared in our paper. We have revised all these statements in our paper (highlighted in red). For other relevant methods as the reviewer suggested in reference [4-6], we have already compared with [4]. [5] was posted on Oct. 3, 2023, which is later than the ICLR submission ddl, and [3] was posted on Aug. 14, 2023, which is within 3 months before ICLR submission ddl. We appreciate the reviewer’s suggestion, and we tried to compare with [3], but they haven’t released their code now. We emailed the authors of [3] to ask for code and they said their work is under review now and refused to provide code currently. We are trying to reimplement the model now and will update the results in our revised version.\n\n[4] Watson et al., De novo design of protein structure and function with RFdiffusion (2023).\n\n[5] Sumida et al., Improving protein expression, stability, and function with ProteinMPNN (2023).\n\n[6] Zhou et al., Conditional Protein Denoising Diffusion Generates Programmable Endonuclease Sequences (2023).\n\n---\n\nWe thank all the reviewers' valuable suggestions.\n\nWe have uploaded a revised draft that incorporates feedback from reviewers VH6U and MgyU, with the updated portions highlighted in red. Additionally, in response to reviewer Wq6M's suggestion, we have included the code and data we used in our paper in the supplementary material (Due to the maximum memory limitation -100M, we couldn’t upload the model checkpoints).\n\nHere's an overview highlighting the principal modifications to our paper:\n\n1. The revised term from “motif/functional sites” to “meaningful fragments” in abstract, introduction third paragraph, method opening paragraph, method 3.1-3.4, experiments 4.1-4.2, analysis 5.1, Conclusion (Reviewer VH6U, weakness 1 & weakness 2 & Q5)\n2. The corrected claim in abstract and last paragraph in introduction (review VH6U, weakness3) \n3. The corrected claim in Introduction second paragraph (Reviewer VH6U, Q1)\n4. The corrected claim in Related Work second paragraph (Reviewer VH6U, Q2)\n5. The clarified explanation of 20 common amino acids in Method opening paragraph(Reviewer VH6U, Q3)\n6. The corrected theoretical claim in Method 3.5 and Appendix A. 1 (reviewer MgyU, weakness 5) \n7. The clarified task definition in Method opening paragraph (reviewer MgyU, Q2)\n\nWe look forward to any additional reading and feedback. If you have any further questions, we are very happy to continue discussion!", "author_response": "Thanks for the responses! Our answers to your follow-up questions are provided as follows:\n\n**1. About Weakness4 & Q1 Page 2, \"We carry out experiments on two metalloproteins, including β-lactamase and myoglobin.\" I would like to say that the experimental setting is the main concern to me. Since the authors have only evaluated the proposed method on two structures, I am not sure if the method can be extended to generalized situations. I know that you are doing protein sequence-structure inpaintning problem, in which case a comparison with a previous baselines [1,2,3] on stantard datasets may help to reveal the generalization capabilities of your algorithm.** \n\nAns: We agree with reviewer Wq6M and additionally compare our method with PROTSEED on B12 protein. Due to the approaching ddl and limited time, PROTSEED has only been trained for 100 epochs, the same as our method. (suggested training epoch is 2000 on their GitHub)\n\nThe results are reported as follows: \n\n**Binding affinity score for designed B12**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|PROTSEED|-9.45$\\pm$ 0.03|-9.36$\\pm$0.09|-9.04$\\pm$0.27|-8.46|\n|NAEPro|**-11.06$\\pm$0.14**|**-10.96$\\pm$0.14**|**-10.61$\\pm$0.29**|**-9.11**|\n\n**2. About Q2 Your methods are not carefully designed for protein inverse folding. However, the presented results outperformed current SOTA by a large margin (curent SOTA is about 50%-60% AAR), which could not convince me. I doubt the veracity and correctness of the experimental results.** \n\nAns: Sorry for the confusion. We evaluate the inverse-folding task on our own two datasets instead of the original CATH. For our datasets, the proteins are from the same family, and thus they may have much overlap on the sequence level, which definitely will lead to high AAR. Besides, our model is initialized with ESM2 weights, which will also improve the AAR. \n\nAlthough the sequence prediction based a fixed backbone structure is not our objective, **we adapt our method to CATH setting by keeping all the CA coordinates and masking all residues. Due to the approaching ddl and limited time, our model has only been trained for 10 epochs, while usually the suggested training epoch would be 100 like GVP, PiFold, etc.** (See below: ESMIF, ProteinMPNN, PiFold results are quoted from PiFold paper). \n\nCATH 4.2 results\n\n|Model| AAR (%)|PPL |\n|:-----|:----:|:----:|\n|ProteinMPNN|45.96|4.61|\n|ESMIF|38.30|6.44|\n|PiFold|51.66|4.55|\n|NAEPro|9.16|15.26|\n\n**Again, we did not claim the superiority of our method on inverse folding task on CATH dataset.**\n\n**3. Page 1, \"Despite their great potential for novel structure design, such sequential design policy fails to cross-condition on sequence and structure, which might lead to inconsistent proteins and inefficient design process\". I can not agree with this statement. RFDiffusion's authors say that they also considered simultaneously designing structure and sequence within RFdiffusion, but combining ProteinMPNN with the diffusion of structure alone provides the excellent performance, as shown in your Table.1. Unfortunately, I observe that you deleted the consistency metric in Table.1, which seems like dishonest behavior to me.**\n\nAns: We do not intend to hide any information. As suggested by reviewer MgyU, the binding affinity is a better metric for functional protein design rather than the accuracy metrics (consistency, AAR, RMSD, etc.). Therefore, we replaced all the accuracy metrics in Table 1 and 2 in our paper according with the binding affinity scores calculated by Gnina. The full table with all scores are listed below:\n\n$\\beta$-lactamase\n\n|Model| AAR (\\%,$\\uparrow$) | RMSD ({ \\AA},$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination | $4.79$ | $--$ | $30.5511$ | $0.2918$ | $--$|\n|Inpainting | $16.73$ | $4.0599$ | $61.7679$ | $0.3790$ | $6.2578$|\n|SMCDiff+ProteinMPNN| $19.94$ | $10.3960$ | $42.0375$ | $0.3458$ |$10.2117$|\n| PROTSEED | $37.63$ | $3.0142$ | $64.3861$| $0.4637$ | $3.3748$ |\n| FrameDiff+IF |$26.20$ | $6.0151$ | $65.6445$ | $0.3657$ | $7.8703$|\n|RFDiffusion+IF | $22.93$ | $6.0438$ | **83.4058** | $0.3747$ |**0.5565**|\n|NAEPro|**65.71**| **2.9916**| 66.8705|**0.4812**| 7.3760|\n\n\n**Myoglobin**\n|Model| AAR (\\%,$\\uparrow$) | RMSD (\\AA,$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination| $4.81$ | $--$ | $38.2817$ | $0.2754$ |--|\n|Inpainting |39.59|3.3751|67.0813|0.4391 |3.2108|\n|SMCDiff+IF | $12.47$ | $8.0067$ | $34.5914$ | $0.2235$ | $8.8754$ |\n| PROTSEED | $48.55$ | $2.8753$ | $61.5588$ | $0.5466$ | $0.9764$ |\n| FrameDiff+IF | $20.43$ | $5.9739$ | $61.3945$ | $0.3757$ |$3.5078$|\n|RFDiffusion+IF | $32.70$ | $3.9930$ | $78.9868$ | $0.4147$  | **0.4375** |\n|NAEPro | **89.37**| **2.6307**|**81.9507**|**0.5692**|4.3865|"}
{"claim": "The paper lacks head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design under the same evaluation setting.", "claim_type": "baseline", "paper_id": "Dr4qD9bzZd", "paper_title": "Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "a9hHYfEm0G", "reviewer": "Reviewer_Wq6M", "review_text": "Summary: This paper proposes NAEPro, a model to jointly design Protein sequence and structure. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from the nearest amino acids in three-dimensional (3D) space. The global attention sub-layer parameters are initialized with ESM-2. The author combines ESM2 and EGNN for co-modeling protein sequence and structure.\n\nStrengths: 1. The reported performance is good.\n2. The method is simple.\n\nWeaknesses: 1. Novelty: Both EGNN and ESM2 are existing models, and there are many existing works on antibody structure and sequence co-design. The combination may limit the novelty of this method.  \n2. Significance: It is likely that previous works can be readily applied to the motif-conditioned setting, and the authors can easily adapt their method to antibody design tasks. It would be beneficial if the authors further elaborate on the significance of their work\n3. Code: The authors do not provide code for checking the soundness of the methods.\n4. Experiment setting: The authors do not provide results on standard benchmarks, such as the CATH dataset, for fair comparison on both sequence and structure design.\n\nQuestions: 1. Could you provide experimental results from the CATH dataset to compare with the original SMCDiff and FrameDiff results?\n2. Similarly, could you provide head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design? Please follow the same setting.\n3. Could you provide the code for checking the results?", "labeling_timestamp": "2026-01-11T16:29:47.864649", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors directly provided head-to-head comparison results against ProteinMPNN, ESMIF, and PiFold for protein sequence design (backbone-to-sequence setting) and reported metrics in tables, addressing the reviewer's request.", "evidence": "Q2: Could you provide head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design? Please follow the same setting.\n\nAns: We provide protein sequence design given backbone structures on our two datasets in the following table. In this setting, we slightly modified our method to make it adaptable to the task as the reviewer asked. Specifically, we provided the whole backbone structure as the model input and designed the whole protein sequence. As the results show, even though we only provide backbone structure without any motif residues, our model can still achieves higher AAR and pLDDT on myoglobin, and higher AAR on beta-lactamase.\n\nmyoglobin\n\n|Model| AAR (%)|pLDDT |\n|:-----|:----:|:----:|\n|ProteinMPNN|81.37|81.9817|\n|ESMIF|76.49|78.0986|\n|PiFold|78.93|80.2912|\n|NAEPro|**85.56**|**82.3871**|\n\nβ-lactamase\n\n|Model| AAR (%)|pLDDT |\n|:-----|:----:|:----:|\n|ProteinMPNN|53.59|**70.6723**|\n|ESMIF|57.49|61.3928|\n|PiFold|63.38|64.9017|\n|NAEPro|**68.39**|65.8218|", "combined_author_response": "We much appreciate your valuable comments and insightful suggestions again, which helps a lot to improve the quality of our paper. As the discussion deadline is approaching, please let us know if you have any further concerns or questions. We are happy to have any further discussion!\n\n---\n\n**4. \"Through this way, amino-acid combinations frequently occur in the same context would draw higher attention scores\". Have you checked this statement? Could you provide some visualizations of the attention map and correlated frequences of amino-acid combinations?**  \n\nAns: Thank the reviewer for this suggestion. We randomly pick one sentence in the training set and visualization its attention matrix from the last layer in Appendix Figure 7(a) and also visualize the pairwise amino acid co-occurrence matrix in Figure 7(b). We find it’s consistent with our statement that residues tend to more connect to their neighboring residues. However, we don’t see any strong correlation between the attention matrix and the pairwise amino acid co-occurrence. Therefore, we revised the corresponding statement in our paper and uploaded a revised version accordingly.\n\n**5. Updating residue representations and coordinates in 3D space with only nearest neighbors enables more efficient and economic message passing compared to prior approaches which compute messages on the complete pairwise residue graph\". To my knowledge, most of the previous methods such as proteinmpnn adopt knn for constructing sparse graph in the 3D spcace. You should provide evidence to support the significance of your statement.**\n\nAns: We mean from the architecture level, previous methods like SE(3)-Transformer [1] and EGNN [2] adopts information flow from all other atoms.\n\n[1] SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks. Fabian B. Fuchs et al. NeurIPS 2020. \n\n[2] E(n) Equivariant Graph Neural Networks. Victor Garcia Satorras et al. ICML 2021. \n\n**6. Novelty: I do not observe enough novelty from the perspective of machine learning in algorithm design.**\n\nAns: Yes, from machine learning algorithm level, our method is not new. However, as admitted by reviewer MgyU, we design a new architecture for protein design. The key innovations of our architecture include: (1) interleaving layers of sequence-level attention and local neighborhood equivariant sub-layer (2) accelerating the local sub-layer with the k-nearest neighbors.\n\n---\n\nThanks for the responses! Our answers to your follow-up questions are provided as follows:\n\n**1. About Weakness4 & Q1 Page 2, \"We carry out experiments on two metalloproteins, including β-lactamase and myoglobin.\" I would like to say that the experimental setting is the main concern to me. Since the authors have only evaluated the proposed method on two structures, I am not sure if the method can be extended to generalized situations. I know that you are doing protein sequence-structure inpaintning problem, in which case a comparison with a previous baselines [1,2,3] on stantard datasets may help to reveal the generalization capabilities of your algorithm.** \n\nAns: We agree with reviewer Wq6M and additionally compare our method with PROTSEED on B12 protein. Due to the approaching ddl and limited time, PROTSEED has only been trained for 100 epochs, the same as our method. (suggested training epoch is 2000 on their GitHub)\n\nThe results are reported as follows: \n\n**Binding affinity score for designed B12**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|PROTSEED|-9.45$\\pm$ 0.03|-9.36$\\pm$0.09|-9.04$\\pm$0.27|-8.46|\n|NAEPro|**-11.06$\\pm$0.14**|**-10.96$\\pm$0.14**|**-10.61$\\pm$0.29**|**-9.11**|\n\n**2. About Q2 Your methods are not carefully designed for protein inverse folding. However, the presented results outperformed current SOTA by a large margin (curent SOTA is about 50%-60% AAR), which could not convince me. I doubt the veracity and correctness of the experimental results.** \n\nAns: Sorry for the confusion. We evaluate the inverse-folding task on our own two datasets instead of the original CATH. For our datasets, the proteins are from the same family, and thus they may have much overlap on the sequence level, which definitely will lead to high AAR. Besides, our model is initialized with ESM2 weights, which will also improve the AAR. \n\nAlthough the sequence prediction based a fixed backbone structure is not our objective, **we adapt our method to CATH setting by keeping all the CA coordinates and masking all residues. Due to the approaching ddl and limited time, our model has only been trained for 10 epochs, while usually the suggested training epoch would be 100 like GVP, PiFold, etc.** (See below: ESMIF, ProteinMPNN, PiFold results are quoted from PiFold paper). \n\nCATH 4.2 results\n\n|Model| AAR (%)|PPL |\n|:-----|:----:|:----:|\n|ProteinMPNN|45.96|4.61|\n|ESMIF|38.30|6.44|\n|PiFold|51.66|4.55|\n|NAEPro|9.16|15.26|\n\n**Again, we did not claim the superiority of our method on inverse folding task on CATH dataset.**\n\n**3. Page 1, \"Despite their great potential for novel structure design, such sequential design policy fails to cross-condition on sequence and structure, which might lead to inconsistent proteins and inefficient design process\". I can not agree with this statement. RFDiffusion's authors say that they also considered simultaneously designing structure and sequence within RFdiffusion, but combining ProteinMPNN with the diffusion of structure alone provides the excellent performance, as shown in your Table.1. Unfortunately, I observe that you deleted the consistency metric in Table.1, which seems like dishonest behavior to me.**\n\nAns: We do not intend to hide any information. As suggested by reviewer MgyU, the binding affinity is a better metric for functional protein design rather than the accuracy metrics (consistency, AAR, RMSD, etc.). Therefore, we replaced all the accuracy metrics in Table 1 and 2 in our paper according with the binding affinity scores calculated by Gnina. The full table with all scores are listed below:\n\n$\\beta$-lactamase\n\n|Model| AAR (\\%,$\\uparrow$) | RMSD ({ \\AA},$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination | $4.79$ | $--$ | $30.5511$ | $0.2918$ | $--$|\n|Inpainting | $16.73$ | $4.0599$ | $61.7679$ | $0.3790$ | $6.2578$|\n|SMCDiff+ProteinMPNN| $19.94$ | $10.3960$ | $42.0375$ | $0.3458$ |$10.2117$|\n| PROTSEED | $37.63$ | $3.0142$ | $64.3861$| $0.4637$ | $3.3748$ |\n| FrameDiff+IF |$26.20$ | $6.0151$ | $65.6445$ | $0.3657$ | $7.8703$|\n|RFDiffusion+IF | $22.93$ | $6.0438$ | **83.4058** | $0.3747$ |**0.5565**|\n|NAEPro|**65.71**| **2.9916**| 66.8705|**0.4812**| 7.3760|\n\n\n**Myoglobin**\n|Model| AAR (\\%,$\\uparrow$) | RMSD (\\AA,$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination| $4.81$ | $--$ | $38.2817$ | $0.2754$ |--|\n|Inpainting |39.59|3.3751|67.0813|0.4391 |3.2108|\n|SMCDiff+IF | $12.47$ | $8.0067$ | $34.5914$ | $0.2235$ | $8.8754$ |\n| PROTSEED | $48.55$ | $2.8753$ | $61.5588$ | $0.5466$ | $0.9764$ |\n| FrameDiff+IF | $20.43$ | $5.9739$ | $61.3945$ | $0.3757$ |$3.5078$|\n|RFDiffusion+IF | $32.70$ | $3.9930$ | $78.9868$ | $0.4147$  | **0.4375** |\n|NAEPro | **89.37**| **2.6307**|**81.9507**|**0.5692**|4.3865|\n\n---\n\nThanks for the further response! Please see our responses to your follow-up questions and concerns as follows:\n\n**Response to Weakness 3 & Q1: My point in mentioning these three papers is not to introduce additional (and possibly meaningless) in-silico comparisons to the authors. Instead, they validated their designs with solid wet-lab experiments demonstrating that it is possible to generate novel protein sequences with fixed backbones.**\n\nAns: We agree on the reviewer’s opinion and have accordingly updated the statement in Introduction.  We have uploaded a revised version of our paper.\n\n**Response to Weakness 4: I completely agree that all the existing evaluation metrics we used in deep learning are only indirect indicators, and they only imply a higher chance that the assessed protein will perform its function. However, I have trouble following the logic that \"*Therefore, a low TM-score cannot indicate the designed protein doesn’t have the corresponding function*\" suggests \"proteins with TM-scores that are lower than 0.5 (which is a wildly-used standard) are still reliable in terms of their functionality in general.\" Similarly, if we all agree that the existing metrics are suboptimal and none of them provides 100% accurate evaluations, it is then meaningless to criticize the widely applied standards with one or two counter-examples. After all, you don't have any better evidence (such as wet-lab results) that is more reliable than the existing criteria.**\n\nAns: We agree with Reviewer VH6U. As suggested by reviewer MgyU, the binding affinity is a better metric for functional protein design. We calculated the binding affinity scores for top-5, top-10, and top-30 candidates and also median score on the test set using Gnina. The results are reported as follows:\n\n**Binding affinity score for designed myoglobin**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-8.18$\\pm$ 0.01|-8.07$\\pm$0.03|-7.97$\\pm$0.23|-7.25|\n|Inpainting|-13.47$\\pm$0.02|-13.12$\\pm$0.12|-12.31$\\pm$0.54|-9.56|\n|SMCDiff+ProteinMPNN|-11.37$\\pm$0.03|-11.12$\\pm$0.31|-10.87$\\pm$0.42|-8.76|\n|PROTSEED|-13.21$\\pm$0.13|-12.89$\\pm$0.42|-11.98$\\pm$0.52|-10.23|\n|FrameDiff+ProteinMPNN|-13.13$\\pm$0.05|-12.92$\\pm$0.16|-12.21$\\pm$0.23|-10.08|\n|RFDiffusion+ProteinMPNN|-13.68$\\pm$0.02|-13.03$\\pm$0.21|-12.56$\\pm$0.43|-10.15|\n|NAEPro|**-14.12$\\pm$0.01**|**-13.85$\\pm$0.10**|**-13.06$\\pm$0.38**|**-10.74**|\n\n**Binding affinity score for designed beta-lactamase**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-6.98$\\pm$0.01|-6.87$\\pm$0.02|-6.69$\\pm$0.05|-6.29|\n|Inpainting|-9.89$\\pm$0.03|-9.54$\\pm$0.16|-9.13$\\pm$0.43|-7.24|\n|SMCDiff+ProteinMPNN|-9.10$\\pm$0.01|-9.05 $\\pm$0.02|-8.98$\\pm$0.01|-6.97|\n|PROTSEED|-9.88$\\pm$0.21|-9.51$\\pm$0.41|-9.01$\\pm$0.62|-7.31|\n|FrameDiff+ProteinMPNN|-9.54$\\pm$0.03|-9.56$\\pm$0.23|-8.89$\\pm$0.35|-7.03|\n|RFDiffusion+ProteinMPNN|-9.87$\\pm$0.05|-9.56$\\pm$0.23|-9.12$\\pm$0.53|-7.51|\n|NAEPro|**-10.06$\\pm$0.05**|**-9.79$\\pm$0.10**|**-9.39$\\pm$0.12**|**-7.66**|\n\n**Response to Q6: Because ESMFold is a less reliable tool for evaluating novel proteins, it should not be used here to assess your designs at all.**\n\nAns: We totally agree with reviewer VH6U. We provide the mean pLDDT for top-5, top-10, top-30 candidates calculated by AlphaFold2 as follows:\n\n**AlphaFold2 pLDDT for designed myoglobin**\n\n|Model| top-5|top-10|top-30|\n|:-----|:----:|:----:|:----:|\n|Hallucination|76.7243|73.5953|68.6060|\n|Inpainting|97.6171|97.6063|97.5681|\n|SMCDiff+ProteinMPNN|85.7051|84.7744|84.9220|\n|PROTSEED|92.9770|91.9216|90.4293|\n|FrameDiff+ProteinMPNN|97.3997|97.3329|96.4279|\n|RFDiffusion+ProteinMPNN|97.9722|97.9174|97.6063|\n|NAEPro|**98.1057**|**98.0706**|**97.9197**|\n\n**AlphaFold2 pLDDT for designed $\\beta$-lactamase**\n\n|Model| top-5|top-10|top-30|\n|:-----|:----:|:----:|:----:|\n|Hallucination|74.7760|74.7032|71.2932|\n|Inpainting|94.5489|94.4328|93.2941|\n|SMCDiff+ProteinMPNN|86.2510|85.7072|83.0376|\n|PROTSEED|98.2896|98.2740|97.9683|\n|FrameDiff+ProteinMPNN|98.3049|98.2922|98.1267|\n|RFDiffusion+ProteinMPNN|**98.6225**|**98.5116**|98.0643|\n|NAEPro|98.5385|98.4481|**98.2841**|\n\n---\n\nDear reviewer,\n\n    We sincerely appreciate the time and effort you dedicated to reviewing our paper and reading our responses. Your valuable suggestions and insightful comments have significantly contributed to refining the quality of our work. No matter what the final result will be, the thoughtful communication with you has better clarified our paper's goals and logic. We would like to express our great gratitude to you!\n\nBest,\n\nAuthors of paper 4608\n\n---\n\n**Could the authors briefly explain how do you construct the one-to-one mapping between the design structure and the target structure in PDB? Is it determined by retrieving the one in PDB with the smallest RMSD? As far as I know, there is no common practice to determine this for the task of protein design, and I appreciate any explanation that clues me in.**\n\nAns: Yes, we construct the one-to-one mapping between the designed structure and the target structure in PDB. It is not determined by retrieving the one in PDB with the smallest RMSD. We extract the motif fragments for each protein, based on which our training objective is to recover the original protein sequence and backbone structure. Therefore, we compute the RMSD between the designed structure and the target one which we want to recover.\n\n---\n\n**Corrected statement: But it can still bind the corresponding metallocofactor heme in the AlphaFill simulation. For the claim “binding rates of 100% and 90%”, I wonder how is the criterion/threshold of “binding” determined? This success rate is rather unrealistic and probably fails to reflect the real performance. Thus, the statement in Section 5.2. saying “It is evident that our method can generate proteins that express the basic and important functions.” can be quite misleading for readers who are not familiar with the task.** \n\nAns: Sorry for the misleading statement. We mean through the AlphaFill prediction, the designed proteins are highly potential to bind the corresponding metallocofactors. We have corrected all the claims in our paper. We provided the process of how to decide if the designed protein can “bind” the metallocofactors in Appendix B.3. We would like to further explain it here.  First of all, we use AlphaFill to do the first-step prediction. AlphaFill will return a complex based on the input protein structure. If the complex includes the corresponding metal ions, we think the designed protein is potential to have functions. Then to guarantee the results convincing, we further use some additional constraints to ensure the designed proteins are potential to express functions. Specifically, we assume if a designed protein share the similar active site environments with their natural counterparts (target protein in our dataset), then it’s highly potential to bind the corresponding metal ions. For myoglobin, we calculate the distance between axial histidine ligand to Fe ion. Besides, we also detect the presence of distal histidine within the active sites, which plays an important role in the oxygen molecule binding function of myoglobin . If the distance is between 2.0Å and 2.5Å, and the distal histidine exists, we think the designed myoglobins share similar active site environments to natural ones. For $beta$-lactamase, we detect the residues that directly contact with zinc ion. If they show similar chemistry properties with natural $beta$-lactamases, we think they can highly potentially bind the corresponding metallocofactors. For example, in Figure 5(a), the active site possesses one zinc atom coordinated to three histidines, while the second zinc atom is coordinated to one histidine, one cysteine, and one aspartate, demonstrating a high degree of amino acid analogy to the natural protein.\n\n**Could the author intuitively describe how does the NAEL deal with the reflection case so as to be simply SE(3)-equivariant instead of E(3)?** \n\nAns: Intuitively, applying reflection to the input structure, the output structure will also be reflected. The conformation of the original backbone will be changed from L-amino acid to D-amino acid (We provide an illustration in Figure 6 in appendix). Therefore, the reflection will change the chirality of the protein which may cause the deficiency of binding to the ligand and eventually its function. However, the protein sequence in our model will not be influenced. Therefore, the 3D structure of the designed sequence which is L-amino acid and the output structure  which becomes D-amino acid will be inconsistent. From this aspect, our model doesn’t satisfy reflection equivariance.\n\n**The authors are encouraged to encompass appropriate comparison with EGNN/SE(3)-transformer in their writing to better contextualize the proposed NAEL.**\n\nAns: We provide the comparison with EGNN+ESM2 on myoglobin in the following Table and also updated the results in Table2 accordingly.\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|EGNN+ESM2|-13.30$\\pm$0.61 | -12.77$\\pm$0.69 |-12.07$\\pm$0.65 | -9.63|\n|NAEPro|**-14.12$\\pm$0.01**|**-13.85$\\pm$0.10**|**-13.06$\\pm$0.38**|**-10.74**|\n\n**It is surprising to see scaling the model does not yield consistent performance gain. Could the authors help explain this behavior? Also, I noticed that the authors merge the two myoglobin and lactamase data for training newly in the rebuttal response. Is there any insight for doing such multi-task learning?**\n\nAns: Enlarging the model hasn’t improved the performance too much. Our interpretation is that the training data size is moderate, and thus a model with moderate size can fit the data well. To validate if this guess holds, we merge the two protein data and then train a unified model. As we can see in the previous table, our model did achieve obvious better results on $\\beta$-lactamase, i.e., TM-score from below 0.5 (0.4919) to over 0.5 (0.5055).\n\n---\n\nWe much appreciate the reviewer’s responses and suggestions, which help a lot to improve our paper.  We have accordingly revised our paper, including Table 1, Table 2, Section 4.2-4.4 and Section 5.2. Our responses to the reviewer’s follow-up questions are provided as follows:\n\n**About the evaluation metrics: I think the binding affinity could better support the effectiveness of NAEPro and be more suitable for the task (functional protein design) rather than the accuracy metrics (RMSD, AAR, etc.). Do the authors agree on this or not?** C**ould the authors explain how the Binding Affinity is calculated for both model based on Gnina? Specifically, I am worried that the metrics is averaged among the test set similar to other metrics.  If so, I suggest that the authors report median or top-k mean as evaluation.**\n\nAns: We completely agree with reviewer MgyU that the binding affinity is a better metric for functional protein design rather than the accuracy metrics. We revised Table 1 and 2 in our paper according to your suggestion. The binding affinity is calculated using Gnina as the average score on the test set. Thank the reviewer for this valuable suggestion. We provide  top-5, top-10, top-30 mean and variance as well as the median in the following table and also revise our paper accordingly. From the results, we can see that our model achieves the best median and top-K binding affinity scores on both datasets.\n\n**Myoglobin**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-8.18$\\pm$ 0.01|-8.07$\\pm$0.03|-7.97$\\pm$0.23|-7.25|\n|Inpainting|-13.47$\\pm$0.02|-13.12$\\pm$0.12|-12.31$\\pm$0.54|-9.56|\n|SMCDiff+ProteinMPNN|-11.37$\\pm$0.03|-11.12$\\pm$0.31|-10.87$\\pm$0.42|-8.76|\n|PROTSEED|-13.21$\\pm$0.13|-12.89$\\pm$0.42|-11.98$\\pm$0.52|-10.23|\n|FrameDiff+ProteinMPNN|-13.13$\\pm$0.05|-12.92$\\pm$0.16|-12.21$\\pm$0.23|-10.08|\n|RFDiffusion+ProteinMPNN|-13.68$\\pm$0.02|-13.03$\\pm$0.21|-12.56$\\pm$0.43|-10.15|\n|NAEPro|**-14.12$\\pm$0.01**|**-13.85$\\pm$0.10**|**-13.06$\\pm$0.38**|**-10.74**|\n\n**Beta-lactamase**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-6.98$\\pm$0.01|-6.87$\\pm$0.02|-6.69$\\pm$0.05|-6.29|\n|Inpainting|-9.89$\\pm$0.03|-9.54$\\pm$0.16|-9.13$\\pm$0.43|-7.24|\n|SMCDiff+ProteinMPNN|-9.10$\\pm$0.01|-9.05 $\\pm$0.02|-8.98$\\pm$0.01|-6.97|\n|PROTSEED|-9.88$\\pm$0.21|-9.51$\\pm$0.41|-9.01$\\pm$0.62|-7.31|\n|FrameDiff+ProteinMPNN|-9.54$\\pm$0.03|-9.56$\\pm$0.23|-8.89$\\pm$0.35|-7.03|\n|RFDiffusion+ProteinMPNN|-9.87$\\pm$0.05|-9.56$\\pm$0.23|-9.12$\\pm$0.53|-7.51|\n|NAEPro|**-10.06$\\pm$0.05**|**-9.79$\\pm$0.10**|**-9.39$\\pm$0.12**|**-7.66**|\n\n**Corrected statement: ESMFold might not accurately predict the structure for a novel sequence.**\n\nAns: Sorry for the misunderstood claim. We want to express we agree on reviewer VH6U’s comment in question 6 “for a novel sequence, its prediction is worse than AlphaFold2.”  As suggested by reviewer VH6U, we re-calculate pLDDT by AlphaFold2. Our model respectively achieves 91.4139 and 76.9550  on myoglobin and $\\beta$-lactamase on average. As claimed in previous work [1,2], pLDDT between 70~90 are classified to be confident and pLDDT ≥ 90 indicates residues predicted with extremely high confidence.\n\n[1] pLDDT Values in AlphaFold2 Protein Models Are Unrelated to Globular Protein Local Flexibility. Oliviero Carugo. Crystals. 2023.\n\n[2] Generating new protein sequences by using dense network and attention mechanism. Feng Wang et al. Mathematical Biosciences and Engineering. 2023.\n\n---\n\nThanks for your valuable review and comments again. We have addressed all your concerns and questions.\n\nIf you have any other questions, please feel free to discuss with us.\n\n---\n\nThanks for your valuable review and comments again. We have addressed all your concerns and questions.\n\nIf you have any other questions, please feel free to discuss with us.\n\n---\n\nThanks for your valuable review and comments again. We have addressed all your concerns and questions.\n\nIf you have any other questions, please feel free to discuss with us.\n\n---\n\n**Q3: In section 4.2, the author(s) described that the weights of NAEPro is partially initialized from pretrained ESM2-8M, of the smallest model size among the ESM2 series. This is very problematic but the recent common practices leverage at least the 650M model. Do the author(s) try much larger model size as initialization? I found in the Table 2 that the 8M model has only marginal improvement upon random initialization.**\n\nAns: Yes, we tried to partially initialize our model with 35M ESM2 (NAEPro-12) and also train this model on the merged myoglobin and beta-lactamase data (NAEPro-12-Unified). For myoglobin, enlarging the model to 12 layers will not influence the performance too much, but training the larger model on the merged dataset will increase the pLDDT and TM-score. For beta-lactamase, enlarging the model from 6 layers to 12 layers is beneficial and will increase the overall performance. Particularly, training the 12-layer model on merged dataset will significantly improve the performance of beta-lactamase, say improve the average TM-score from below 0.5 to above 0.5. Then we also partially initialize a 30-layer model with 150M ESM2 (NAEPro-30-Unified) and train the model on merged dataset. The performance for both myoglobin and beta-lactamase will be further improved.\n\n**myoglobin**\n\n|Model| AAR (%)|RMSD|pLDDT |TM-score |Parameters|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|NAEPro-6 |**89.37**|**2.6307**|81.9507|0.5692|11.6M|\n|NAEPro-12 |88.42|2.6333|82.2849|0.5673|52M|\n|NAEPro-12-Unified|86.81|2.6443|82.9736|0.5736|52M|\n|NAEPro-30-Unified|86.62|2.6410|**83.0173**|**0.5749**|223.7M|\n\n**$\\beta$-lactamase**\n\n|Model| AAR (%)|RMSD|pLDDT |TM-score |Parameters|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|NAEPro-6 |65.71|2.9916|66.8705|0.4812|11.6M|\n|NAEPro-12 |67.28|2.9864|68.7394|0.4919|52M|\n|NAEPro-12-Unified|70.93|**2.9707**|69.9333|0.5055|52M|\n|NAEPro-30-Unified|**72.60**|2.9781|**71.0429**|**0.5089**|223.7M|\n\n**Q4: How is the “target structure” in evaluation metrics(Section 4.2) determined? What is the difference between metrics RMSD and consistency?**\n\nAns: Target structure means the natural protein structure provided in PDB. RMSD is calculated as the RMSD between the designed protein structure and the one given in PDB. Consistency is calculated as the RMSD between the designed structure and the one predicted by ESMFold for the designed sequence.\n\nWe really hope our responses address your concerns. If you have any other questions, we are very happy to continue discussions!\n\n---\n\n**Weakness 4: For the evaluation metrics, the problem to be studied is functional protein (as it is claimed in the title) design instead of general backbone generation(eg., RFDiffusion) or sequence generation (eg., ProteinMPNN). Thus, I found the the five of the metrics fail to (or indirectly if they potentially do) reflect the performance of functional protein design or scaffolding. All these metrics in Table 1 & 2 show the somehow sequence-structure matching between the predicted and native, but are not very suitable for proteins such as enzyme.**\n\nAns: In our analysis (Section 5.2), we evaluated the function for binding the corresponding metallocofactors of the designed proteins. Specifically, we first randomly select 20 cases from the top100 sequences (pLDDT ranking). We then employ AlphaFold2 for protein structure prediction, followed by inputting these structures into AlphaFill [1] to predict the associated ligands. Notably, our results indicate that all designed myoglobins exhibit heme binding capability, while 18 beta-lactamases demonstrate the ability to bind zinc ions, resulting in metallocofactor binding rates of 100% and 90%, respectively. It is evident that our method can generate proteins that express the basic and important functions, i.e. binding the corresponding metallocofactors. To systematically illustrate the function of our designed proteins, we use docking tool Gnina[2] to compute the binding affinity between the designed metalloproteins and the corresponding metallocofactors. The results in the following table shows that our model achieves the best binding affinity scores on both metalloproteins, demonstrating the proteins designed by our model are highly potential to actively exhibit biochemical functions.\n\n|Model| myoglobin (kcal/mol)|beta-lactamase (kcal/mol)|\n|:-----|:----:|:----:|\n|Hallucination|-7.23|-6.37|\n|Inpainting|-9.32|-7.63|\n|SMCDiff+ProteinMPNN|-8.79|-6.89|\n|PROTSEED|-10.07|-7.68|\n|FrameDiff+ProteinMPNN|-9.54|-7.21|\n|RFDiffusion+ProteinMPNN|-9.76|-7.59|\n|NAEPro|**-10.14**|**-7.80**|\n\n**Weakness 5: The potential theoretical flaw in the equivariance analysis in Section 3.5.** \n\nAns: Thank the reviewer for pointing out this typo. Our model satisfies only roto-translation equivariance without reflection equivariance. We have accordingly updated the paper.\n\n**Q1: The proposed architecture encoding both sequence and structure via the masked language modeling (MLM) scheme is somehow interesting. How do the author(s) see the  difference between the proposed NAEL layer and tensor field convolutional layer in SE(3)-transformer ? Also, what is the advantage of NAEL over the SeqIPA of the PROTSEED, the most competitive baseline in Table 1?** \n\nAns: The most biggest difference between our NAEPro and tensor field convolutional layer in SE(3)-transformer is:  NAEPro uses interleaving layers of global sequence-level attention and local neighborhood equivariant sub-layer, from which the residue representation is updated from both the whole sequence interaction and 3D neighbor message passing; instead, if we enable kNN message passing in SE(3)-transformer, then the node feature will only be updated based on 3D neighboring information. To clearly demonstrate the advantage of NAEL over SeqIPA in PROTSEED, we replace our NAEL with the SeqIPA-Addition without secondary structure feature. Taking myoglobin as an example, the results are shown in the following table. NAEPro with NAEL performs better than with SeqIPA.\n\n|Model| AAR (%)|RMSD|\n|:-----|:----:|:----:|\n|NAEPro -w/- SeqIPA|65.33|2.7078|\n|NAEPro|**89.37**|**2.6307**|\n\n**Q2:  In Section 3 - opening paragraph, the author(s) define/formulate the target task as generate a protein sequence and all 3D coordinates of N residues. However, in later model definition, the proposed NAEPro only operates on the C-alpha(CA) coordinates. Please explain this ambiguity.**\n\nAns: We are sorry for the ambiguity caused. When we mention “we formulate the target task as generate a protein sequence and all 3D coordinates of N residues”, we mean 3D coordinates of alpha-carbon of all the residues. Following the setting in [3], we represent the backbone structure as the CA-only coordinates of all residues. We have updated the paper accordingly in the new version.\n\n[3] Diffusion probabilistic modeling of protein backbones in 3D for the motif-scaffolding problem. Brian L. Trippe et al. ICLR. 2023.\n\n---\n\nWe appreciate the reviewer’s valuable and insightful suggestions, which are very helpful for us to improve our paper. We have clarified all your concerns, added the experiments and updated the paper accordingly. We address the specific concerns as follows:\n\n **Weakness 1: Regarding the consistency metrics reported, I noticed that the consistency metric of NAEPro in Table 1 (as the main results) underperformed half of the baselines.  Such key observation greatly weakened the method. The authors can elaborate on this point.**\n\nAns: Yes, we agree with the reviewer’s opinion. We would like to clarify our task more clearly. Our goal is to design functional proteins with novel sequence and structure. As shown in Figure 5(b), the designed myoglobin has a RMSD of 3.943\\AA with the most similar one in PDB and 26.7% AAR with the most similar one in Uniprot, but it can still bind the corresponding metallocofactor heme in the AlphaFill simulation [1]. In our paper, we use ESMFold to predict the structure of the designed sequence. However, as pointed out by reviewer VH6U, ESMFold might not accurately predict the structure for a novel sequence. To further clarify this point, we calculate the scRMSD of motifs between the designed structure and the predicted structure from the designed sequence in the following table. As the results show, our NAEPro achieves smaller RMSD than RFDiffusion+ProteinMPNN on myoglobin and comparable results on beta-lactamase.\n\n|Model| myoglobin|beta-lactamase|\n|:-----|:----:|:----:|\n|Inpainting|0.5292|0.7819|\n|SMCDiff+ProteinMPNN|1.3816|1.4523|\n|PROTSEED|0.4987|**0.3892**|\n|FrameDiff+ProteinMPNN|0.8765|0.9815|\n|RFDiffusion+ProteinMPNN|0.4936|0.3918|\n|NAEPro|**0.4832**|0.5239|\n\n[1] Alphafill: enriching alphafold models with ligands and cofactors. Maarten L Hekkelman et al. Nature methods. 2023.\n\n**Weakness 2: For the experiments, how is the model trained to obtain the evaluation digits in Table 1 for each baseline. This is very important.** \n\nAns: We are sorry for the confusion. We got the codes of all baselines from their GitHub repos, and then re-trained all the models separately on myoglobin and beta-lactamase datasets. The evaluation procedure among all the baselines and our model follows the same process, and thus we believe the comparisons in Table 1 are fair.\n\n**Weakness 3: For the dataset, I found that in section 4.1, the author(s) mentioned that both beta-lactamase and myoglobin datasets are split randomly. This is worrying because the entries in PDB can usually be redundant, especially for such large and well-studied families that the authors selected for evaluation. The author(s) can elaborate on this point. Moreover, in the Table 3 (in appendix), I am curious about the column name “PDB”, does it mean the number of PDB entries (with unique ID) or the number of single chains?** \n\nAns: Yes, this is a very good question. Since we focus on sequence and structure co-design task, we aim to design both functional sequence and structure. Therefore, our input data is the <sequence, structure> pair. From this point, even though the sequence might be the same, the structure is somehow different. Therefore, our paired input data are different and would not be redundant. We appreciate the reviewer for carefully reviewing our paper and also reading the appendix. In Table 3, PDB means the number of different PDB entries. To eliminate the reviewer’s concerns, we re-cleaned the test set by filtering pairs whose sequences have over 30% sequence identity to any sample in the training set after doing pairwise alignment for myoglobin. We compare our NAEPro and ProtSeed (the most competitive baseline) in the following table. As the results show, our model can design proteins with higher binding affinity (by Gnina [2]) on samples that have lower similarity to training data.\n\n|Model| RMSD|Binding Affinity (kcal/mol)|\n|:-----|:----:|:----:|\n|PROTSEED|**2.4031**|-9.62|\n|NAEPro|2.6634|**-9.98**|\n\n[2] GNINA 1.0: Molecular docking with deep learning. A McNutt et al. Cheminformatics. 2021.\n\n---\n\n**Weakness4 & Q1: The authors do not provide results on standard benchmarks, such as the CATH dataset, for fair comparison on both sequence and structure design. Could you provide experimental results from the CATH dataset to compare with the original SMCDiff and FrameDiff results?**\n\nAns: We would like to clarify our task more clearly. Our task aims to co-design functional and novel protein sequence and structure. In our method, we use MSAs to automatically find motifs to guarantee the protein function, which is achieved within the same protein family. Instead, CATH is a protein dataset consisting of proteins from diverse families, making the motif extraction difficult. Besides, CATH is usually used to evaluate inverse folding task, say protein sequence design based on fixed backbone structure, which is not the goal of our paper. We hope we didn’t misunderstand the reviewer’s meaning by “the original SMCDiff and FrameDiff results” as SMCDiff and FrameDiff haven’t been evaluated on CATH dataset. Since SMCDiff and FrameDiff focus on protein structure design, CATH is usually used to evaluate protein sequence design based on fixed backbone structure, which may have some discrepancies on the design goals. \n\n**Q2: Could you provide head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design? Please follow the same setting.**\n\nAns: We provide protein sequence design given backbone structures on our two datasets in the following table. In this setting, we slightly modified our method to make it adaptable to the task as the reviewer asked. Specifically, we provided the whole backbone structure as the model input and designed the whole protein sequence. As the results show, even though we only provide  backbone structure without any motif residues, our model can still achieves higher AAR and pLDDT on myoglobin, and higher AAR on beta-lactamase.\n\n**myoglobin**\n\n|Model| AAR (%)|pLDDT |\n|:-----|:----:|:----:|\n|ProteinMPNN|81.37|81.9817|\n|ESMIF|76.49|78.0986|\n|PiFold|78.93|80.2912|\n|NAEPro|**85.56**|**82.3871**|\n\n$\\beta$-lactamase\n\n|Model| AAR (%)|pLDDT |\n|:-----|:----:|:----:|\n|ProteinMPNN|53.59|**70.6723**|\n|ESMIF|57.49|61.3928|\n|PiFold|63.38|64.9017|\n|NAEPro|**68.39**|65.8218|\n\n**Q3: Could you provide the code for checking the results?**\n\nAns: We have provided the code and data in the supplementary material. Due to the memory limitation, we couldn’t upload the model checkpoints. However, the training process is highly efficient, and the reviewer could train the model if he/she is interested.\n\n---\n\nWe thank the reviewer for the positive reviews as well as the suggestions for improvement. We have clarified all your concerns, added the experiments and updated the paper accordingly. Our responses to the reviewer’s concerns and questions are provided below:\n\n**Weakness 1: Both EGNN and ESM2 are existing models, and there are many existing works on antibody structure and sequence co-design. The combination may limit the novelty of this method.**\n\nAns: We would like to point out that our NAEPro is not a combination of ESM2 and EGNN. Instead our key innovations are (1) interleaving layers of sequence-level attention and local neighborhood equivariant sub-layer (2) accelerating the local sub-layer with the k-nearest neighbors.  On the contrary,  EGNN is a fully-connected graph and updates message, coordinates and atom features based on all other atoms in 3D space, which is not efficient for long proteins. Besides, EGNN+ESM2 is a sequential process and can not cross-condition on sequence and structure. We compare our method with EGNN+ESM2, NAEPro w/o ESM2 initialization on myoglobin in the Following Table (Table 2 in the paper).  As the results show, EGNN+ESM2 performs much worse than our NAEPro, and removing the ESM2 initialization will not influence the performance too much, demonstrating that our model is much more superior than EGNN+ESM2.\n\n|Model| AAR (%)|RMSD|pLDDT |TM-score |consistency|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|EGNN+ESM2|51.12|2.9891|77.3399|0.4656|4.9827|\n|NAEPro-w/o-ESM2 initialization|79.82|2.6398|76.3032|0.5159|4.7273|\n|NAEPro|**89.37**|**2.6307**|**81.9507**|**0.5692**|**4.3865**|\n\n**Weakness 2: It is likely that previous works can be readily applied to the motif-conditioned setting, and the authors can easily adapt their method to antibody design tasks. It would be beneficial if the authors further elaborate on the significance of their work**\n\nAns: We would like to clarify our goal more clearly. Our task is to co-design functional and novel protein sequence and structure. Previous work which is most similar to ours is Inpainting [1]. However, [1] provides the functional sites manually while we automatically mined the functional sites (plus the conserved sites as suggested by reviewer ****VH6U****) by MSAs. Previous methods considering only sequence design constrained by fitness value [2,3] or sequence design based on given backbone structure (inverse folding) [4] is not suitable for our task. For antibody design, antibody is always Y-shaped, while one of our goal is to design novel and diverse protein structure. For example, in Figure 4, the designed beta-lactamases have different fold categories and belong to three different sub-classes. In Figure 5 (b), the designed myoglobin has a RMSD of 3.943\\AA with the most similar one in PDB, but it can still bind the corresponding metallocofactor heme in the AlphaFill simulation [5]. From this aspect, we think antibody design models can not be directly applied to our task without any modification and vice versa. \n\n[1] Scaffolding protein functional sites using deep learning.  Jue wang. et al. Science. 2022.\n\n[2] Biological Sequence Design with GFlowNets. [Moksh Jain](https://arxiv.org/search/q-bio?searchtype=author&query=Jain,+M) et al. ICML 2022. \n\n[3] Proximal Exploration for Model-guided Protein Sequence Design. Zhizhou Ren et al. ICML 2022. \n\n[4] Robust deep learning based protein sequence design using ProteinMPNN. J. Dauparas et al. Science. 2022.\n\n[5] Alphafill: enriching alphafold models with ligands and cofactors. Maarten L Hekkelman et al. Nature methods. 2023.\n\n**Weakness 3:** **The authors do not provide code for checking the soundness of the methods.**\n\nAns: We have provided the code and data in the supplementary material. Due to the memory limitation, we couldn’t upload the model checkpoints. However, the training process is highly efficient, and the reviewer could train the model if he/she is interested.\n\n---\n\n**Q6: For a novel sequence, ESMFold’s prediction is worse than AlphaFold2. Consequently, evaluating novel sequences should use AlphaFold2 instead of ESMFold.**\n\nAns: Due to limited time and computing resources, we provide the AlphaFold2 results for ProtSeed, RFDiffusion+ProteinMPNN and our NAEPro. As the results show, even though ESMFold achieves lower scores than AlphaFold2, the tendencies (ranking) among different models are similar.  Particularly, our model achieves an average TM-score over 0.5 on beta-lactamase (ESMFold 0.4812) and pLDDT over 70 (ESMFold 66.8705), which gives stronger evidence that our model has the ability to design proteins which have stable structures.\n\n**myoglobin**\n\n|Model| pLDDT|TM-score|\n|:-----|:----:|:----:|\n|PROTSEED|77.9032|0.5621|\n|RFDiffusion+ProteinMPNN|88.2130|0.5103|\n|NAEPro|**91.4139**|**0.5912**|\n\n**$\\beta$-lactamase**\n\n|Model| pLDDT|TM-score|\n|:-----|:----:|:----:|\n|PROTSEED|73.3176|0.4901|\n|RFDiffusion+ProteinMPNN|**87.8092**|0.4806|\n|NAEPro|76.9550|**0.5009**|\n\n **Q7: \"Consistency - What is the designed structure and the predicted structure? Which is from NAEPro? Where does the other one come from?**\n\nAns: Consistency is calculated as the RMSD between the designed structure from NAEPro and the one predicted by ESMFold for the designed sequence from NAEPro.\n\n---\n\n**Weakness 4:  Results in Table 1 do not support the superiority of the proposed method. For instance, RMSD>1.7 \\AA means the two structures are different for myoglobin. For pLDDT, the prediction on lactamase is ~66%, which is much lower than 80%-90% which is conventionally believed reliable for folding predictions. The TM-score, if they are smaller than 0.5, then the two structures are believed different.** \n\nAns: We would like to clarify our goal more clearly.  Our task is to design the sequence and structure of proteins with effective functions.  We calculated TM-score by AlphaFold2 as suggested by the reviewer, and the two proteins designed by our model both achieves an average score over 0.5 (myoglobin **0.5912** and beta-lactamase **0.5009**). However, as found by previous method [7],  protein pairs with a TM-score >0.5 are most likely in the same fold while those with a TM-score <0.5 are mainly not in the same fold. Therefore, low TM-score can not indicate the designed protein doesn’t have the corresponding function. As claimed in previous work [9,10], pLDDT between 70~90 are classified to be confident. We calculated pLDDT by AlphaFold2 as suggested by the reviewer and the two protein families designed by our model are confident on an average aspect (myoglobin **91.4139** and beta-lactamase **76.9550**).  Similarly, as observed in previous work[11,12], pLDDT has a weak correlation with protein function. Therefore, a moderate TM-score and pLDDT can not indicate a protein expresses no or poor function. As shown in figure 5(b), the designed myoglobin has a RMSD of 3.943\\AA with the most similar one in PDB, but it can still bind the corresponding metallocofactor heme in the AlphaFill simulation [8]. To systematically illustrate the function of our designed proteins, we use docking method Gnina [13] to compute the binding affinity between the designed metalloproteins and the corresponding metallocofactors. The results in the following table shows that our model achieves the best binding affinity scores on both metalloproteins, demonstrating the proteins designed by our model are highly potential to actively exhibit biochemical functions.\n\n|Model|myoglobin (kcal/mol)|beta-lactamase (kcal/mol)|\n|:-----|:----:|:----:|\n|Hallucination|-7.23|-6.37|\n|Inpainting|-9.32|-7.63|\n|SMCDiff+ProteinMPNN|-8.79|-6.89|\n|PROTSEED|-10.07|-7.68|\n|FrameDiff+ProteinMPNN|-9.54|-7.21|\n|RFDiffusion+ProteinMPNN|-9.76|-7.59|\n|NAEPro|**-10.14**|**-7.80**|\n\n[7] How significant is a protein structure similarity with TM-score= 0.5? Jinrui Xu and Yang Zhang. *Bioinformatics. 2010.* \n\n[8] Alphafill: enriching alphafold models with ligands and cofactors. Maarten L Hekkelman et al. Nature methods. 2023.\n\n[9] pLDDT Values in AlphaFold2 Protein Models Are Unrelated to Globular Protein Local Flexibility. Oliviero Carugo. Crystals. 2023\n\n[10] Generating new protein sequences by using dense network and attention mechanism. Feng Wang et al. Mathematical Biosciences and Engineering. 2023.\n\n[11] Using AlphaFold to predict the impact of single mutations on protein stability and function. Marina A. Pak et al. PloS one. 2023.\n\n[12] Peptide binder design with inverse folding and protein structure prediction. Patrick Bryant et al. Nature Communications Chemistry. 2023.\n\n[13] GNINA 1.0: Molecular docking with deep learning. A McNutt et al. Cheminformatics. 2021.\n\n**Q1 & Q2: \"The use of separate models for sequence and structure cannot ensure the consistency between the generated sequence and structure.\" \"knowing the topology of a protein before design process is difficult and also cannot guarantee the designed proteins have the desired functions.\" I disagree with this statement.** **I disagree with this statement.** \n\nAns: Thanks for the suggestion! We agree with the reviewer and updated the statement in our paper. \n\n**Q3: what is the meaning of \"generally-encoded\" for 20 types of amino acids?** \n\nAns: It means the 20 common amino acids. We have updated the paper accordingly.\n\n**Q4: \"The selection of motif varies from setting to setting...\". Can the authors please provide a clearer explanation?**\n\nAns: We mean the motifs have different meanings in different tasks. For de novo enzyme design [14], it means the binding sites where enzyme binds the metallocofactor and substrate. For de novo binder design [15], it means the binding sites where the protein binder binds the protein targets.\n\n[14] De novo enzyme design using rosetta3. Florian Richter et al. PloS one. 2011.\n\n[15] De novo design of protein interactions with learned surface fingerprints. Pablo Gainza. Nature. 2023.\n\n---\n\nWe thank the reviewer for the insightful questions. We have clarified all your concerns, added the experiments and updated the paper accordingly. Answers to specific points are provided below:\n\n**Weakness 1 & weakness 2 & Q5: The identified problem or the motivation for designing such a method is not supported by existing literature.  Some designed modules counter the intuition in biology, such as \"motif mining\"**\n\nAns: We follow the design insight proposed by [1], which starts from the functional site and fills in additional sequence and structure to create a viable protein scaffold in a single forward pass. Different from [1], we provide the functional sites which are automatically mined from MSA as previous methods [2, 3] while [1] provides the functional sites manually. Besides, we design a new architecture called NAEPro to generate the whole protein sequence and structure based on the given partial protein fragments, while [1] achieves this goal by finetuning the RoseTTAFold. However, we agree with the reviewer’s opinion in question 5 that MSA discovers not only protein functional sites as addressed in [2,3] but also finds conversed sites which may not directly relate to the protein function. Therefore, we change all the term “motif/functional sites” in our paper to  “meaningful protein fragments”.\n\n[1] Scaffolding protein functional sites using deep learning.  Jue wang. et al. Science. 2022.\n\n[2] Emerging methods in protein co-evolution. David de Juan et al. Nature Review Genetics. 2013.\n\n[3] Evolutionary information for specifying a protein fold. Michael Socolich et al. Nature. 2005.\n\n**Weakness 3 & Q1: The method is not 'extensively evaluated' as claimed by the authors (they only selectively analyzed 2 proteins), and many results do not suggest that the proposed method is SOTA among baselines. Moreover, the evaluation does not include all relevant methods, which makes it too aggressive for the authors to claim in the Introduction that their method \"achieves the HIGHEST...among ALL competitors\" and \"is faster than the FASTEST method\". Empirically, It has been proven by several recent research that the de novo method can generate new sequences for fixed backbone or desired functions [4-6].**\n\nAns: Sorry for the confusion. Our method achieves the best performance on 4/5 metrics on myoglobin and 3/5 metrics on beta-lactamase among all the compared representative baselines in our paper. Our model is faster than all baselines compared in our paper. We have revised all these statements in our paper (highlighted in red). For other relevant methods as the reviewer suggested in reference [4-6], we have already compared with [4]. [5] was posted on Oct. 3, 2023, which is later than the ICLR submission ddl, and [3] was posted on Aug. 14, 2023, which is within 3 months before ICLR submission ddl. We appreciate the reviewer’s suggestion, and we tried to compare with [3], but they haven’t released their code now. We emailed the authors of [3] to ask for code and they said their work is under review now and refused to provide code currently. We are trying to reimplement the model now and will update the results in our revised version.\n\n[4] Watson et al., De novo design of protein structure and function with RFdiffusion (2023).\n\n[5] Sumida et al., Improving protein expression, stability, and function with ProteinMPNN (2023).\n\n[6] Zhou et al., Conditional Protein Denoising Diffusion Generates Programmable Endonuclease Sequences (2023).\n\n---\n\nWe thank all the reviewers' valuable suggestions.\n\nWe have uploaded a revised draft that incorporates feedback from reviewers VH6U and MgyU, with the updated portions highlighted in red. Additionally, in response to reviewer Wq6M's suggestion, we have included the code and data we used in our paper in the supplementary material (Due to the maximum memory limitation -100M, we couldn’t upload the model checkpoints).\n\nHere's an overview highlighting the principal modifications to our paper:\n\n1. The revised term from “motif/functional sites” to “meaningful fragments” in abstract, introduction third paragraph, method opening paragraph, method 3.1-3.4, experiments 4.1-4.2, analysis 5.1, Conclusion (Reviewer VH6U, weakness 1 & weakness 2 & Q5)\n2. The corrected claim in abstract and last paragraph in introduction (review VH6U, weakness3) \n3. The corrected claim in Introduction second paragraph (Reviewer VH6U, Q1)\n4. The corrected claim in Related Work second paragraph (Reviewer VH6U, Q2)\n5. The clarified explanation of 20 common amino acids in Method opening paragraph(Reviewer VH6U, Q3)\n6. The corrected theoretical claim in Method 3.5 and Appendix A. 1 (reviewer MgyU, weakness 5) \n7. The clarified task definition in Method opening paragraph (reviewer MgyU, Q2)\n\nWe look forward to any additional reading and feedback. If you have any further questions, we are very happy to continue discussion!", "author_response": "Thanks for the responses! Our answers to your follow-up questions are provided as follows:\n\n**1. About Weakness4 & Q1 Page 2, \"We carry out experiments on two metalloproteins, including β-lactamase and myoglobin.\" I would like to say that the experimental setting is the main concern to me. Since the authors have only evaluated the proposed method on two structures, I am not sure if the method can be extended to generalized situations. I know that you are doing protein sequence-structure inpaintning problem, in which case a comparison with a previous baselines [1,2,3] on stantard datasets may help to reveal the generalization capabilities of your algorithm.** \n\nAns: We agree with reviewer Wq6M and additionally compare our method with PROTSEED on B12 protein. Due to the approaching ddl and limited time, PROTSEED has only been trained for 100 epochs, the same as our method. (suggested training epoch is 2000 on their GitHub)\n\nThe results are reported as follows: \n\n**Binding affinity score for designed B12**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|PROTSEED|-9.45$\\pm$ 0.03|-9.36$\\pm$0.09|-9.04$\\pm$0.27|-8.46|\n|NAEPro|**-11.06$\\pm$0.14**|**-10.96$\\pm$0.14**|**-10.61$\\pm$0.29**|**-9.11**|\n\n**2. About Q2 Your methods are not carefully designed for protein inverse folding. However, the presented results outperformed current SOTA by a large margin (curent SOTA is about 50%-60% AAR), which could not convince me. I doubt the veracity and correctness of the experimental results.** \n\nAns: Sorry for the confusion. We evaluate the inverse-folding task on our own two datasets instead of the original CATH. For our datasets, the proteins are from the same family, and thus they may have much overlap on the sequence level, which definitely will lead to high AAR. Besides, our model is initialized with ESM2 weights, which will also improve the AAR. \n\nAlthough the sequence prediction based a fixed backbone structure is not our objective, **we adapt our method to CATH setting by keeping all the CA coordinates and masking all residues. Due to the approaching ddl and limited time, our model has only been trained for 10 epochs, while usually the suggested training epoch would be 100 like GVP, PiFold, etc.** (See below: ESMIF, ProteinMPNN, PiFold results are quoted from PiFold paper). \n\nCATH 4.2 results\n\n|Model| AAR (%)|PPL |\n|:-----|:----:|:----:|\n|ProteinMPNN|45.96|4.61|\n|ESMIF|38.30|6.44|\n|PiFold|51.66|4.55|\n|NAEPro|9.16|15.26|\n\n**Again, we did not claim the superiority of our method on inverse folding task on CATH dataset.**\n\n**3. Page 1, \"Despite their great potential for novel structure design, such sequential design policy fails to cross-condition on sequence and structure, which might lead to inconsistent proteins and inefficient design process\". I can not agree with this statement. RFDiffusion's authors say that they also considered simultaneously designing structure and sequence within RFdiffusion, but combining ProteinMPNN with the diffusion of structure alone provides the excellent performance, as shown in your Table.1. Unfortunately, I observe that you deleted the consistency metric in Table.1, which seems like dishonest behavior to me.**\n\nAns: We do not intend to hide any information. As suggested by reviewer MgyU, the binding affinity is a better metric for functional protein design rather than the accuracy metrics (consistency, AAR, RMSD, etc.). Therefore, we replaced all the accuracy metrics in Table 1 and 2 in our paper according with the binding affinity scores calculated by Gnina. The full table with all scores are listed below:\n\n$\\beta$-lactamase\n\n|Model| AAR (\\%,$\\uparrow$) | RMSD ({ \\AA},$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination | $4.79$ | $--$ | $30.5511$ | $0.2918$ | $--$|\n|Inpainting | $16.73$ | $4.0599$ | $61.7679$ | $0.3790$ | $6.2578$|\n|SMCDiff+ProteinMPNN| $19.94$ | $10.3960$ | $42.0375$ | $0.3458$ |$10.2117$|\n| PROTSEED | $37.63$ | $3.0142$ | $64.3861$| $0.4637$ | $3.3748$ |\n| FrameDiff+IF |$26.20$ | $6.0151$ | $65.6445$ | $0.3657$ | $7.8703$|\n|RFDiffusion+IF | $22.93$ | $6.0438$ | **83.4058** | $0.3747$ |**0.5565**|\n|NAEPro|**65.71**| **2.9916**| 66.8705|**0.4812**| 7.3760|\n\n\n**Myoglobin**\n|Model| AAR (\\%,$\\uparrow$) | RMSD (\\AA,$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination| $4.81$ | $--$ | $38.2817$ | $0.2754$ |--|\n|Inpainting |39.59|3.3751|67.0813|0.4391 |3.2108|\n|SMCDiff+IF | $12.47$ | $8.0067$ | $34.5914$ | $0.2235$ | $8.8754$ |\n| PROTSEED | $48.55$ | $2.8753$ | $61.5588$ | $0.5466$ | $0.9764$ |\n| FrameDiff+IF | $20.43$ | $5.9739$ | $61.3945$ | $0.3757$ |$3.5078$|\n|RFDiffusion+IF | $32.70$ | $3.9930$ | $78.9868$ | $0.4147$  | **0.4375** |\n|NAEPro | **89.37**| **2.6307**|**81.9507**|**0.5692**|4.3865|"}
{"claim": "The paper does not sufficiently elaborate on the significance of the work.", "claim_type": "subjective", "paper_id": "Dr4qD9bzZd", "paper_title": "Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "a9hHYfEm0G", "reviewer": "Reviewer_Wq6M", "review_text": "Summary: This paper proposes NAEPro, a model to jointly design Protein sequence and structure. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from the nearest amino acids in three-dimensional (3D) space. The global attention sub-layer parameters are initialized with ESM-2. The author combines ESM2 and EGNN for co-modeling protein sequence and structure.\n\nStrengths: 1. The reported performance is good.\n2. The method is simple.\n\nWeaknesses: 1. Novelty: Both EGNN and ESM2 are existing models, and there are many existing works on antibody structure and sequence co-design. The combination may limit the novelty of this method.  \n2. Significance: It is likely that previous works can be readily applied to the motif-conditioned setting, and the authors can easily adapt their method to antibody design tasks. It would be beneficial if the authors further elaborate on the significance of their work\n3. Code: The authors do not provide code for checking the soundness of the methods.\n4. Experiment setting: The authors do not provide results on standard benchmarks, such as the CATH dataset, for fair comparison on both sequence and structure design.\n\nQuestions: 1. Could you provide experimental results from the CATH dataset to compare with the original SMCDiff and FrameDiff results?\n2. Similarly, could you provide head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design? Please follow the same setting.\n3. Could you provide the code for checking the results?", "labeling_timestamp": "2026-01-11T16:29:20.454705", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors acknowledge the concern and repeatedly state they clarified the paper’s goals, elaborated novelty/significance, added experiments and comparisons, and revised the manuscript accordingly to better motivate and contextualize the work.", "evidence": "\"We would like to clarify our goal more clearly. Our task is to co-design functional and novel protein sequence and structure. ... We have accordingly revised our paper, including Table 1, Table 2, Section 4.2-4.4 and Section 5.2.\"", "combined_author_response": "We much appreciate your valuable comments and insightful suggestions again, which helps a lot to improve the quality of our paper. As the discussion deadline is approaching, please let us know if you have any further concerns or questions. We are happy to have any further discussion!\n\n---\n\n**4. \"Through this way, amino-acid combinations frequently occur in the same context would draw higher attention scores\". Have you checked this statement? Could you provide some visualizations of the attention map and correlated frequences of amino-acid combinations?**  \n\nAns: Thank the reviewer for this suggestion. We randomly pick one sentence in the training set and visualization its attention matrix from the last layer in Appendix Figure 7(a) and also visualize the pairwise amino acid co-occurrence matrix in Figure 7(b). We find it’s consistent with our statement that residues tend to more connect to their neighboring residues. However, we don’t see any strong correlation between the attention matrix and the pairwise amino acid co-occurrence. Therefore, we revised the corresponding statement in our paper and uploaded a revised version accordingly.\n\n**5. Updating residue representations and coordinates in 3D space with only nearest neighbors enables more efficient and economic message passing compared to prior approaches which compute messages on the complete pairwise residue graph\". To my knowledge, most of the previous methods such as proteinmpnn adopt knn for constructing sparse graph in the 3D spcace. You should provide evidence to support the significance of your statement.**\n\nAns: We mean from the architecture level, previous methods like SE(3)-Transformer [1] and EGNN [2] adopts information flow from all other atoms.\n\n[1] SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks. Fabian B. Fuchs et al. NeurIPS 2020. \n\n[2] E(n) Equivariant Graph Neural Networks. Victor Garcia Satorras et al. ICML 2021. \n\n**6. Novelty: I do not observe enough novelty from the perspective of machine learning in algorithm design.**\n\nAns: Yes, from machine learning algorithm level, our method is not new. However, as admitted by reviewer MgyU, we design a new architecture for protein design. The key innovations of our architecture include: (1) interleaving layers of sequence-level attention and local neighborhood equivariant sub-layer (2) accelerating the local sub-layer with the k-nearest neighbors.\n\n---\n\nThanks for the responses! Our answers to your follow-up questions are provided as follows:\n\n**1. About Weakness4 & Q1 Page 2, \"We carry out experiments on two metalloproteins, including β-lactamase and myoglobin.\" I would like to say that the experimental setting is the main concern to me. Since the authors have only evaluated the proposed method on two structures, I am not sure if the method can be extended to generalized situations. I know that you are doing protein sequence-structure inpaintning problem, in which case a comparison with a previous baselines [1,2,3] on stantard datasets may help to reveal the generalization capabilities of your algorithm.** \n\nAns: We agree with reviewer Wq6M and additionally compare our method with PROTSEED on B12 protein. Due to the approaching ddl and limited time, PROTSEED has only been trained for 100 epochs, the same as our method. (suggested training epoch is 2000 on their GitHub)\n\nThe results are reported as follows: \n\n**Binding affinity score for designed B12**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|PROTSEED|-9.45$\\pm$ 0.03|-9.36$\\pm$0.09|-9.04$\\pm$0.27|-8.46|\n|NAEPro|**-11.06$\\pm$0.14**|**-10.96$\\pm$0.14**|**-10.61$\\pm$0.29**|**-9.11**|\n\n**2. About Q2 Your methods are not carefully designed for protein inverse folding. However, the presented results outperformed current SOTA by a large margin (curent SOTA is about 50%-60% AAR), which could not convince me. I doubt the veracity and correctness of the experimental results.** \n\nAns: Sorry for the confusion. We evaluate the inverse-folding task on our own two datasets instead of the original CATH. For our datasets, the proteins are from the same family, and thus they may have much overlap on the sequence level, which definitely will lead to high AAR. Besides, our model is initialized with ESM2 weights, which will also improve the AAR. \n\nAlthough the sequence prediction based a fixed backbone structure is not our objective, **we adapt our method to CATH setting by keeping all the CA coordinates and masking all residues. Due to the approaching ddl and limited time, our model has only been trained for 10 epochs, while usually the suggested training epoch would be 100 like GVP, PiFold, etc.** (See below: ESMIF, ProteinMPNN, PiFold results are quoted from PiFold paper). \n\nCATH 4.2 results\n\n|Model| AAR (%)|PPL |\n|:-----|:----:|:----:|\n|ProteinMPNN|45.96|4.61|\n|ESMIF|38.30|6.44|\n|PiFold|51.66|4.55|\n|NAEPro|9.16|15.26|\n\n**Again, we did not claim the superiority of our method on inverse folding task on CATH dataset.**\n\n**3. Page 1, \"Despite their great potential for novel structure design, such sequential design policy fails to cross-condition on sequence and structure, which might lead to inconsistent proteins and inefficient design process\". I can not agree with this statement. RFDiffusion's authors say that they also considered simultaneously designing structure and sequence within RFdiffusion, but combining ProteinMPNN with the diffusion of structure alone provides the excellent performance, as shown in your Table.1. Unfortunately, I observe that you deleted the consistency metric in Table.1, which seems like dishonest behavior to me.**\n\nAns: We do not intend to hide any information. As suggested by reviewer MgyU, the binding affinity is a better metric for functional protein design rather than the accuracy metrics (consistency, AAR, RMSD, etc.). Therefore, we replaced all the accuracy metrics in Table 1 and 2 in our paper according with the binding affinity scores calculated by Gnina. The full table with all scores are listed below:\n\n$\\beta$-lactamase\n\n|Model| AAR (\\%,$\\uparrow$) | RMSD ({ \\AA},$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination | $4.79$ | $--$ | $30.5511$ | $0.2918$ | $--$|\n|Inpainting | $16.73$ | $4.0599$ | $61.7679$ | $0.3790$ | $6.2578$|\n|SMCDiff+ProteinMPNN| $19.94$ | $10.3960$ | $42.0375$ | $0.3458$ |$10.2117$|\n| PROTSEED | $37.63$ | $3.0142$ | $64.3861$| $0.4637$ | $3.3748$ |\n| FrameDiff+IF |$26.20$ | $6.0151$ | $65.6445$ | $0.3657$ | $7.8703$|\n|RFDiffusion+IF | $22.93$ | $6.0438$ | **83.4058** | $0.3747$ |**0.5565**|\n|NAEPro|**65.71**| **2.9916**| 66.8705|**0.4812**| 7.3760|\n\n\n**Myoglobin**\n|Model| AAR (\\%,$\\uparrow$) | RMSD (\\AA,$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination| $4.81$ | $--$ | $38.2817$ | $0.2754$ |--|\n|Inpainting |39.59|3.3751|67.0813|0.4391 |3.2108|\n|SMCDiff+IF | $12.47$ | $8.0067$ | $34.5914$ | $0.2235$ | $8.8754$ |\n| PROTSEED | $48.55$ | $2.8753$ | $61.5588$ | $0.5466$ | $0.9764$ |\n| FrameDiff+IF | $20.43$ | $5.9739$ | $61.3945$ | $0.3757$ |$3.5078$|\n|RFDiffusion+IF | $32.70$ | $3.9930$ | $78.9868$ | $0.4147$  | **0.4375** |\n|NAEPro | **89.37**| **2.6307**|**81.9507**|**0.5692**|4.3865|\n\n---\n\nThanks for the further response! Please see our responses to your follow-up questions and concerns as follows:\n\n**Response to Weakness 3 & Q1: My point in mentioning these three papers is not to introduce additional (and possibly meaningless) in-silico comparisons to the authors. Instead, they validated their designs with solid wet-lab experiments demonstrating that it is possible to generate novel protein sequences with fixed backbones.**\n\nAns: We agree on the reviewer’s opinion and have accordingly updated the statement in Introduction.  We have uploaded a revised version of our paper.\n\n**Response to Weakness 4: I completely agree that all the existing evaluation metrics we used in deep learning are only indirect indicators, and they only imply a higher chance that the assessed protein will perform its function. However, I have trouble following the logic that \"*Therefore, a low TM-score cannot indicate the designed protein doesn’t have the corresponding function*\" suggests \"proteins with TM-scores that are lower than 0.5 (which is a wildly-used standard) are still reliable in terms of their functionality in general.\" Similarly, if we all agree that the existing metrics are suboptimal and none of them provides 100% accurate evaluations, it is then meaningless to criticize the widely applied standards with one or two counter-examples. After all, you don't have any better evidence (such as wet-lab results) that is more reliable than the existing criteria.**\n\nAns: We agree with Reviewer VH6U. As suggested by reviewer MgyU, the binding affinity is a better metric for functional protein design. We calculated the binding affinity scores for top-5, top-10, and top-30 candidates and also median score on the test set using Gnina. The results are reported as follows:\n\n**Binding affinity score for designed myoglobin**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-8.18$\\pm$ 0.01|-8.07$\\pm$0.03|-7.97$\\pm$0.23|-7.25|\n|Inpainting|-13.47$\\pm$0.02|-13.12$\\pm$0.12|-12.31$\\pm$0.54|-9.56|\n|SMCDiff+ProteinMPNN|-11.37$\\pm$0.03|-11.12$\\pm$0.31|-10.87$\\pm$0.42|-8.76|\n|PROTSEED|-13.21$\\pm$0.13|-12.89$\\pm$0.42|-11.98$\\pm$0.52|-10.23|\n|FrameDiff+ProteinMPNN|-13.13$\\pm$0.05|-12.92$\\pm$0.16|-12.21$\\pm$0.23|-10.08|\n|RFDiffusion+ProteinMPNN|-13.68$\\pm$0.02|-13.03$\\pm$0.21|-12.56$\\pm$0.43|-10.15|\n|NAEPro|**-14.12$\\pm$0.01**|**-13.85$\\pm$0.10**|**-13.06$\\pm$0.38**|**-10.74**|\n\n**Binding affinity score for designed beta-lactamase**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-6.98$\\pm$0.01|-6.87$\\pm$0.02|-6.69$\\pm$0.05|-6.29|\n|Inpainting|-9.89$\\pm$0.03|-9.54$\\pm$0.16|-9.13$\\pm$0.43|-7.24|\n|SMCDiff+ProteinMPNN|-9.10$\\pm$0.01|-9.05 $\\pm$0.02|-8.98$\\pm$0.01|-6.97|\n|PROTSEED|-9.88$\\pm$0.21|-9.51$\\pm$0.41|-9.01$\\pm$0.62|-7.31|\n|FrameDiff+ProteinMPNN|-9.54$\\pm$0.03|-9.56$\\pm$0.23|-8.89$\\pm$0.35|-7.03|\n|RFDiffusion+ProteinMPNN|-9.87$\\pm$0.05|-9.56$\\pm$0.23|-9.12$\\pm$0.53|-7.51|\n|NAEPro|**-10.06$\\pm$0.05**|**-9.79$\\pm$0.10**|**-9.39$\\pm$0.12**|**-7.66**|\n\n**Response to Q6: Because ESMFold is a less reliable tool for evaluating novel proteins, it should not be used here to assess your designs at all.**\n\nAns: We totally agree with reviewer VH6U. We provide the mean pLDDT for top-5, top-10, top-30 candidates calculated by AlphaFold2 as follows:\n\n**AlphaFold2 pLDDT for designed myoglobin**\n\n|Model| top-5|top-10|top-30|\n|:-----|:----:|:----:|:----:|\n|Hallucination|76.7243|73.5953|68.6060|\n|Inpainting|97.6171|97.6063|97.5681|\n|SMCDiff+ProteinMPNN|85.7051|84.7744|84.9220|\n|PROTSEED|92.9770|91.9216|90.4293|\n|FrameDiff+ProteinMPNN|97.3997|97.3329|96.4279|\n|RFDiffusion+ProteinMPNN|97.9722|97.9174|97.6063|\n|NAEPro|**98.1057**|**98.0706**|**97.9197**|\n\n**AlphaFold2 pLDDT for designed $\\beta$-lactamase**\n\n|Model| top-5|top-10|top-30|\n|:-----|:----:|:----:|:----:|\n|Hallucination|74.7760|74.7032|71.2932|\n|Inpainting|94.5489|94.4328|93.2941|\n|SMCDiff+ProteinMPNN|86.2510|85.7072|83.0376|\n|PROTSEED|98.2896|98.2740|97.9683|\n|FrameDiff+ProteinMPNN|98.3049|98.2922|98.1267|\n|RFDiffusion+ProteinMPNN|**98.6225**|**98.5116**|98.0643|\n|NAEPro|98.5385|98.4481|**98.2841**|\n\n---\n\nDear reviewer,\n\n    We sincerely appreciate the time and effort you dedicated to reviewing our paper and reading our responses. Your valuable suggestions and insightful comments have significantly contributed to refining the quality of our work. No matter what the final result will be, the thoughtful communication with you has better clarified our paper's goals and logic. We would like to express our great gratitude to you!\n\nBest,\n\nAuthors of paper 4608\n\n---\n\n**Could the authors briefly explain how do you construct the one-to-one mapping between the design structure and the target structure in PDB? Is it determined by retrieving the one in PDB with the smallest RMSD? As far as I know, there is no common practice to determine this for the task of protein design, and I appreciate any explanation that clues me in.**\n\nAns: Yes, we construct the one-to-one mapping between the designed structure and the target structure in PDB. It is not determined by retrieving the one in PDB with the smallest RMSD. We extract the motif fragments for each protein, based on which our training objective is to recover the original protein sequence and backbone structure. Therefore, we compute the RMSD between the designed structure and the target one which we want to recover.\n\n---\n\n**Corrected statement: But it can still bind the corresponding metallocofactor heme in the AlphaFill simulation. For the claim “binding rates of 100% and 90%”, I wonder how is the criterion/threshold of “binding” determined? This success rate is rather unrealistic and probably fails to reflect the real performance. Thus, the statement in Section 5.2. saying “It is evident that our method can generate proteins that express the basic and important functions.” can be quite misleading for readers who are not familiar with the task.** \n\nAns: Sorry for the misleading statement. We mean through the AlphaFill prediction, the designed proteins are highly potential to bind the corresponding metallocofactors. We have corrected all the claims in our paper. We provided the process of how to decide if the designed protein can “bind” the metallocofactors in Appendix B.3. We would like to further explain it here.  First of all, we use AlphaFill to do the first-step prediction. AlphaFill will return a complex based on the input protein structure. If the complex includes the corresponding metal ions, we think the designed protein is potential to have functions. Then to guarantee the results convincing, we further use some additional constraints to ensure the designed proteins are potential to express functions. Specifically, we assume if a designed protein share the similar active site environments with their natural counterparts (target protein in our dataset), then it’s highly potential to bind the corresponding metal ions. For myoglobin, we calculate the distance between axial histidine ligand to Fe ion. Besides, we also detect the presence of distal histidine within the active sites, which plays an important role in the oxygen molecule binding function of myoglobin . If the distance is between 2.0Å and 2.5Å, and the distal histidine exists, we think the designed myoglobins share similar active site environments to natural ones. For $beta$-lactamase, we detect the residues that directly contact with zinc ion. If they show similar chemistry properties with natural $beta$-lactamases, we think they can highly potentially bind the corresponding metallocofactors. For example, in Figure 5(a), the active site possesses one zinc atom coordinated to three histidines, while the second zinc atom is coordinated to one histidine, one cysteine, and one aspartate, demonstrating a high degree of amino acid analogy to the natural protein.\n\n**Could the author intuitively describe how does the NAEL deal with the reflection case so as to be simply SE(3)-equivariant instead of E(3)?** \n\nAns: Intuitively, applying reflection to the input structure, the output structure will also be reflected. The conformation of the original backbone will be changed from L-amino acid to D-amino acid (We provide an illustration in Figure 6 in appendix). Therefore, the reflection will change the chirality of the protein which may cause the deficiency of binding to the ligand and eventually its function. However, the protein sequence in our model will not be influenced. Therefore, the 3D structure of the designed sequence which is L-amino acid and the output structure  which becomes D-amino acid will be inconsistent. From this aspect, our model doesn’t satisfy reflection equivariance.\n\n**The authors are encouraged to encompass appropriate comparison with EGNN/SE(3)-transformer in their writing to better contextualize the proposed NAEL.**\n\nAns: We provide the comparison with EGNN+ESM2 on myoglobin in the following Table and also updated the results in Table2 accordingly.\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|EGNN+ESM2|-13.30$\\pm$0.61 | -12.77$\\pm$0.69 |-12.07$\\pm$0.65 | -9.63|\n|NAEPro|**-14.12$\\pm$0.01**|**-13.85$\\pm$0.10**|**-13.06$\\pm$0.38**|**-10.74**|\n\n**It is surprising to see scaling the model does not yield consistent performance gain. Could the authors help explain this behavior? Also, I noticed that the authors merge the two myoglobin and lactamase data for training newly in the rebuttal response. Is there any insight for doing such multi-task learning?**\n\nAns: Enlarging the model hasn’t improved the performance too much. Our interpretation is that the training data size is moderate, and thus a model with moderate size can fit the data well. To validate if this guess holds, we merge the two protein data and then train a unified model. As we can see in the previous table, our model did achieve obvious better results on $\\beta$-lactamase, i.e., TM-score from below 0.5 (0.4919) to over 0.5 (0.5055).\n\n---\n\nWe much appreciate the reviewer’s responses and suggestions, which help a lot to improve our paper.  We have accordingly revised our paper, including Table 1, Table 2, Section 4.2-4.4 and Section 5.2. Our responses to the reviewer’s follow-up questions are provided as follows:\n\n**About the evaluation metrics: I think the binding affinity could better support the effectiveness of NAEPro and be more suitable for the task (functional protein design) rather than the accuracy metrics (RMSD, AAR, etc.). Do the authors agree on this or not?** C**ould the authors explain how the Binding Affinity is calculated for both model based on Gnina? Specifically, I am worried that the metrics is averaged among the test set similar to other metrics.  If so, I suggest that the authors report median or top-k mean as evaluation.**\n\nAns: We completely agree with reviewer MgyU that the binding affinity is a better metric for functional protein design rather than the accuracy metrics. We revised Table 1 and 2 in our paper according to your suggestion. The binding affinity is calculated using Gnina as the average score on the test set. Thank the reviewer for this valuable suggestion. We provide  top-5, top-10, top-30 mean and variance as well as the median in the following table and also revise our paper accordingly. From the results, we can see that our model achieves the best median and top-K binding affinity scores on both datasets.\n\n**Myoglobin**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-8.18$\\pm$ 0.01|-8.07$\\pm$0.03|-7.97$\\pm$0.23|-7.25|\n|Inpainting|-13.47$\\pm$0.02|-13.12$\\pm$0.12|-12.31$\\pm$0.54|-9.56|\n|SMCDiff+ProteinMPNN|-11.37$\\pm$0.03|-11.12$\\pm$0.31|-10.87$\\pm$0.42|-8.76|\n|PROTSEED|-13.21$\\pm$0.13|-12.89$\\pm$0.42|-11.98$\\pm$0.52|-10.23|\n|FrameDiff+ProteinMPNN|-13.13$\\pm$0.05|-12.92$\\pm$0.16|-12.21$\\pm$0.23|-10.08|\n|RFDiffusion+ProteinMPNN|-13.68$\\pm$0.02|-13.03$\\pm$0.21|-12.56$\\pm$0.43|-10.15|\n|NAEPro|**-14.12$\\pm$0.01**|**-13.85$\\pm$0.10**|**-13.06$\\pm$0.38**|**-10.74**|\n\n**Beta-lactamase**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|Hallucination|-6.98$\\pm$0.01|-6.87$\\pm$0.02|-6.69$\\pm$0.05|-6.29|\n|Inpainting|-9.89$\\pm$0.03|-9.54$\\pm$0.16|-9.13$\\pm$0.43|-7.24|\n|SMCDiff+ProteinMPNN|-9.10$\\pm$0.01|-9.05 $\\pm$0.02|-8.98$\\pm$0.01|-6.97|\n|PROTSEED|-9.88$\\pm$0.21|-9.51$\\pm$0.41|-9.01$\\pm$0.62|-7.31|\n|FrameDiff+ProteinMPNN|-9.54$\\pm$0.03|-9.56$\\pm$0.23|-8.89$\\pm$0.35|-7.03|\n|RFDiffusion+ProteinMPNN|-9.87$\\pm$0.05|-9.56$\\pm$0.23|-9.12$\\pm$0.53|-7.51|\n|NAEPro|**-10.06$\\pm$0.05**|**-9.79$\\pm$0.10**|**-9.39$\\pm$0.12**|**-7.66**|\n\n**Corrected statement: ESMFold might not accurately predict the structure for a novel sequence.**\n\nAns: Sorry for the misunderstood claim. We want to express we agree on reviewer VH6U’s comment in question 6 “for a novel sequence, its prediction is worse than AlphaFold2.”  As suggested by reviewer VH6U, we re-calculate pLDDT by AlphaFold2. Our model respectively achieves 91.4139 and 76.9550  on myoglobin and $\\beta$-lactamase on average. As claimed in previous work [1,2], pLDDT between 70~90 are classified to be confident and pLDDT ≥ 90 indicates residues predicted with extremely high confidence.\n\n[1] pLDDT Values in AlphaFold2 Protein Models Are Unrelated to Globular Protein Local Flexibility. Oliviero Carugo. Crystals. 2023.\n\n[2] Generating new protein sequences by using dense network and attention mechanism. Feng Wang et al. Mathematical Biosciences and Engineering. 2023.\n\n---\n\nThanks for your valuable review and comments again. We have addressed all your concerns and questions.\n\nIf you have any other questions, please feel free to discuss with us.\n\n---\n\nThanks for your valuable review and comments again. We have addressed all your concerns and questions.\n\nIf you have any other questions, please feel free to discuss with us.\n\n---\n\nThanks for your valuable review and comments again. We have addressed all your concerns and questions.\n\nIf you have any other questions, please feel free to discuss with us.\n\n---\n\n**Q3: In section 4.2, the author(s) described that the weights of NAEPro is partially initialized from pretrained ESM2-8M, of the smallest model size among the ESM2 series. This is very problematic but the recent common practices leverage at least the 650M model. Do the author(s) try much larger model size as initialization? I found in the Table 2 that the 8M model has only marginal improvement upon random initialization.**\n\nAns: Yes, we tried to partially initialize our model with 35M ESM2 (NAEPro-12) and also train this model on the merged myoglobin and beta-lactamase data (NAEPro-12-Unified). For myoglobin, enlarging the model to 12 layers will not influence the performance too much, but training the larger model on the merged dataset will increase the pLDDT and TM-score. For beta-lactamase, enlarging the model from 6 layers to 12 layers is beneficial and will increase the overall performance. Particularly, training the 12-layer model on merged dataset will significantly improve the performance of beta-lactamase, say improve the average TM-score from below 0.5 to above 0.5. Then we also partially initialize a 30-layer model with 150M ESM2 (NAEPro-30-Unified) and train the model on merged dataset. The performance for both myoglobin and beta-lactamase will be further improved.\n\n**myoglobin**\n\n|Model| AAR (%)|RMSD|pLDDT |TM-score |Parameters|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|NAEPro-6 |**89.37**|**2.6307**|81.9507|0.5692|11.6M|\n|NAEPro-12 |88.42|2.6333|82.2849|0.5673|52M|\n|NAEPro-12-Unified|86.81|2.6443|82.9736|0.5736|52M|\n|NAEPro-30-Unified|86.62|2.6410|**83.0173**|**0.5749**|223.7M|\n\n**$\\beta$-lactamase**\n\n|Model| AAR (%)|RMSD|pLDDT |TM-score |Parameters|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|NAEPro-6 |65.71|2.9916|66.8705|0.4812|11.6M|\n|NAEPro-12 |67.28|2.9864|68.7394|0.4919|52M|\n|NAEPro-12-Unified|70.93|**2.9707**|69.9333|0.5055|52M|\n|NAEPro-30-Unified|**72.60**|2.9781|**71.0429**|**0.5089**|223.7M|\n\n**Q4: How is the “target structure” in evaluation metrics(Section 4.2) determined? What is the difference between metrics RMSD and consistency?**\n\nAns: Target structure means the natural protein structure provided in PDB. RMSD is calculated as the RMSD between the designed protein structure and the one given in PDB. Consistency is calculated as the RMSD between the designed structure and the one predicted by ESMFold for the designed sequence.\n\nWe really hope our responses address your concerns. If you have any other questions, we are very happy to continue discussions!\n\n---\n\n**Weakness 4: For the evaluation metrics, the problem to be studied is functional protein (as it is claimed in the title) design instead of general backbone generation(eg., RFDiffusion) or sequence generation (eg., ProteinMPNN). Thus, I found the the five of the metrics fail to (or indirectly if they potentially do) reflect the performance of functional protein design or scaffolding. All these metrics in Table 1 & 2 show the somehow sequence-structure matching between the predicted and native, but are not very suitable for proteins such as enzyme.**\n\nAns: In our analysis (Section 5.2), we evaluated the function for binding the corresponding metallocofactors of the designed proteins. Specifically, we first randomly select 20 cases from the top100 sequences (pLDDT ranking). We then employ AlphaFold2 for protein structure prediction, followed by inputting these structures into AlphaFill [1] to predict the associated ligands. Notably, our results indicate that all designed myoglobins exhibit heme binding capability, while 18 beta-lactamases demonstrate the ability to bind zinc ions, resulting in metallocofactor binding rates of 100% and 90%, respectively. It is evident that our method can generate proteins that express the basic and important functions, i.e. binding the corresponding metallocofactors. To systematically illustrate the function of our designed proteins, we use docking tool Gnina[2] to compute the binding affinity between the designed metalloproteins and the corresponding metallocofactors. The results in the following table shows that our model achieves the best binding affinity scores on both metalloproteins, demonstrating the proteins designed by our model are highly potential to actively exhibit biochemical functions.\n\n|Model| myoglobin (kcal/mol)|beta-lactamase (kcal/mol)|\n|:-----|:----:|:----:|\n|Hallucination|-7.23|-6.37|\n|Inpainting|-9.32|-7.63|\n|SMCDiff+ProteinMPNN|-8.79|-6.89|\n|PROTSEED|-10.07|-7.68|\n|FrameDiff+ProteinMPNN|-9.54|-7.21|\n|RFDiffusion+ProteinMPNN|-9.76|-7.59|\n|NAEPro|**-10.14**|**-7.80**|\n\n**Weakness 5: The potential theoretical flaw in the equivariance analysis in Section 3.5.** \n\nAns: Thank the reviewer for pointing out this typo. Our model satisfies only roto-translation equivariance without reflection equivariance. We have accordingly updated the paper.\n\n**Q1: The proposed architecture encoding both sequence and structure via the masked language modeling (MLM) scheme is somehow interesting. How do the author(s) see the  difference between the proposed NAEL layer and tensor field convolutional layer in SE(3)-transformer ? Also, what is the advantage of NAEL over the SeqIPA of the PROTSEED, the most competitive baseline in Table 1?** \n\nAns: The most biggest difference between our NAEPro and tensor field convolutional layer in SE(3)-transformer is:  NAEPro uses interleaving layers of global sequence-level attention and local neighborhood equivariant sub-layer, from which the residue representation is updated from both the whole sequence interaction and 3D neighbor message passing; instead, if we enable kNN message passing in SE(3)-transformer, then the node feature will only be updated based on 3D neighboring information. To clearly demonstrate the advantage of NAEL over SeqIPA in PROTSEED, we replace our NAEL with the SeqIPA-Addition without secondary structure feature. Taking myoglobin as an example, the results are shown in the following table. NAEPro with NAEL performs better than with SeqIPA.\n\n|Model| AAR (%)|RMSD|\n|:-----|:----:|:----:|\n|NAEPro -w/- SeqIPA|65.33|2.7078|\n|NAEPro|**89.37**|**2.6307**|\n\n**Q2:  In Section 3 - opening paragraph, the author(s) define/formulate the target task as generate a protein sequence and all 3D coordinates of N residues. However, in later model definition, the proposed NAEPro only operates on the C-alpha(CA) coordinates. Please explain this ambiguity.**\n\nAns: We are sorry for the ambiguity caused. When we mention “we formulate the target task as generate a protein sequence and all 3D coordinates of N residues”, we mean 3D coordinates of alpha-carbon of all the residues. Following the setting in [3], we represent the backbone structure as the CA-only coordinates of all residues. We have updated the paper accordingly in the new version.\n\n[3] Diffusion probabilistic modeling of protein backbones in 3D for the motif-scaffolding problem. Brian L. Trippe et al. ICLR. 2023.\n\n---\n\nWe appreciate the reviewer’s valuable and insightful suggestions, which are very helpful for us to improve our paper. We have clarified all your concerns, added the experiments and updated the paper accordingly. We address the specific concerns as follows:\n\n **Weakness 1: Regarding the consistency metrics reported, I noticed that the consistency metric of NAEPro in Table 1 (as the main results) underperformed half of the baselines.  Such key observation greatly weakened the method. The authors can elaborate on this point.**\n\nAns: Yes, we agree with the reviewer’s opinion. We would like to clarify our task more clearly. Our goal is to design functional proteins with novel sequence and structure. As shown in Figure 5(b), the designed myoglobin has a RMSD of 3.943\\AA with the most similar one in PDB and 26.7% AAR with the most similar one in Uniprot, but it can still bind the corresponding metallocofactor heme in the AlphaFill simulation [1]. In our paper, we use ESMFold to predict the structure of the designed sequence. However, as pointed out by reviewer VH6U, ESMFold might not accurately predict the structure for a novel sequence. To further clarify this point, we calculate the scRMSD of motifs between the designed structure and the predicted structure from the designed sequence in the following table. As the results show, our NAEPro achieves smaller RMSD than RFDiffusion+ProteinMPNN on myoglobin and comparable results on beta-lactamase.\n\n|Model| myoglobin|beta-lactamase|\n|:-----|:----:|:----:|\n|Inpainting|0.5292|0.7819|\n|SMCDiff+ProteinMPNN|1.3816|1.4523|\n|PROTSEED|0.4987|**0.3892**|\n|FrameDiff+ProteinMPNN|0.8765|0.9815|\n|RFDiffusion+ProteinMPNN|0.4936|0.3918|\n|NAEPro|**0.4832**|0.5239|\n\n[1] Alphafill: enriching alphafold models with ligands and cofactors. Maarten L Hekkelman et al. Nature methods. 2023.\n\n**Weakness 2: For the experiments, how is the model trained to obtain the evaluation digits in Table 1 for each baseline. This is very important.** \n\nAns: We are sorry for the confusion. We got the codes of all baselines from their GitHub repos, and then re-trained all the models separately on myoglobin and beta-lactamase datasets. The evaluation procedure among all the baselines and our model follows the same process, and thus we believe the comparisons in Table 1 are fair.\n\n**Weakness 3: For the dataset, I found that in section 4.1, the author(s) mentioned that both beta-lactamase and myoglobin datasets are split randomly. This is worrying because the entries in PDB can usually be redundant, especially for such large and well-studied families that the authors selected for evaluation. The author(s) can elaborate on this point. Moreover, in the Table 3 (in appendix), I am curious about the column name “PDB”, does it mean the number of PDB entries (with unique ID) or the number of single chains?** \n\nAns: Yes, this is a very good question. Since we focus on sequence and structure co-design task, we aim to design both functional sequence and structure. Therefore, our input data is the <sequence, structure> pair. From this point, even though the sequence might be the same, the structure is somehow different. Therefore, our paired input data are different and would not be redundant. We appreciate the reviewer for carefully reviewing our paper and also reading the appendix. In Table 3, PDB means the number of different PDB entries. To eliminate the reviewer’s concerns, we re-cleaned the test set by filtering pairs whose sequences have over 30% sequence identity to any sample in the training set after doing pairwise alignment for myoglobin. We compare our NAEPro and ProtSeed (the most competitive baseline) in the following table. As the results show, our model can design proteins with higher binding affinity (by Gnina [2]) on samples that have lower similarity to training data.\n\n|Model| RMSD|Binding Affinity (kcal/mol)|\n|:-----|:----:|:----:|\n|PROTSEED|**2.4031**|-9.62|\n|NAEPro|2.6634|**-9.98**|\n\n[2] GNINA 1.0: Molecular docking with deep learning. A McNutt et al. Cheminformatics. 2021.\n\n---\n\n**Weakness4 & Q1: The authors do not provide results on standard benchmarks, such as the CATH dataset, for fair comparison on both sequence and structure design. Could you provide experimental results from the CATH dataset to compare with the original SMCDiff and FrameDiff results?**\n\nAns: We would like to clarify our task more clearly. Our task aims to co-design functional and novel protein sequence and structure. In our method, we use MSAs to automatically find motifs to guarantee the protein function, which is achieved within the same protein family. Instead, CATH is a protein dataset consisting of proteins from diverse families, making the motif extraction difficult. Besides, CATH is usually used to evaluate inverse folding task, say protein sequence design based on fixed backbone structure, which is not the goal of our paper. We hope we didn’t misunderstand the reviewer’s meaning by “the original SMCDiff and FrameDiff results” as SMCDiff and FrameDiff haven’t been evaluated on CATH dataset. Since SMCDiff and FrameDiff focus on protein structure design, CATH is usually used to evaluate protein sequence design based on fixed backbone structure, which may have some discrepancies on the design goals. \n\n**Q2: Could you provide head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design? Please follow the same setting.**\n\nAns: We provide protein sequence design given backbone structures on our two datasets in the following table. In this setting, we slightly modified our method to make it adaptable to the task as the reviewer asked. Specifically, we provided the whole backbone structure as the model input and designed the whole protein sequence. As the results show, even though we only provide  backbone structure without any motif residues, our model can still achieves higher AAR and pLDDT on myoglobin, and higher AAR on beta-lactamase.\n\n**myoglobin**\n\n|Model| AAR (%)|pLDDT |\n|:-----|:----:|:----:|\n|ProteinMPNN|81.37|81.9817|\n|ESMIF|76.49|78.0986|\n|PiFold|78.93|80.2912|\n|NAEPro|**85.56**|**82.3871**|\n\n$\\beta$-lactamase\n\n|Model| AAR (%)|pLDDT |\n|:-----|:----:|:----:|\n|ProteinMPNN|53.59|**70.6723**|\n|ESMIF|57.49|61.3928|\n|PiFold|63.38|64.9017|\n|NAEPro|**68.39**|65.8218|\n\n**Q3: Could you provide the code for checking the results?**\n\nAns: We have provided the code and data in the supplementary material. Due to the memory limitation, we couldn’t upload the model checkpoints. However, the training process is highly efficient, and the reviewer could train the model if he/she is interested.\n\n---\n\nWe thank the reviewer for the positive reviews as well as the suggestions for improvement. We have clarified all your concerns, added the experiments and updated the paper accordingly. Our responses to the reviewer’s concerns and questions are provided below:\n\n**Weakness 1: Both EGNN and ESM2 are existing models, and there are many existing works on antibody structure and sequence co-design. The combination may limit the novelty of this method.**\n\nAns: We would like to point out that our NAEPro is not a combination of ESM2 and EGNN. Instead our key innovations are (1) interleaving layers of sequence-level attention and local neighborhood equivariant sub-layer (2) accelerating the local sub-layer with the k-nearest neighbors.  On the contrary,  EGNN is a fully-connected graph and updates message, coordinates and atom features based on all other atoms in 3D space, which is not efficient for long proteins. Besides, EGNN+ESM2 is a sequential process and can not cross-condition on sequence and structure. We compare our method with EGNN+ESM2, NAEPro w/o ESM2 initialization on myoglobin in the Following Table (Table 2 in the paper).  As the results show, EGNN+ESM2 performs much worse than our NAEPro, and removing the ESM2 initialization will not influence the performance too much, demonstrating that our model is much more superior than EGNN+ESM2.\n\n|Model| AAR (%)|RMSD|pLDDT |TM-score |consistency|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|EGNN+ESM2|51.12|2.9891|77.3399|0.4656|4.9827|\n|NAEPro-w/o-ESM2 initialization|79.82|2.6398|76.3032|0.5159|4.7273|\n|NAEPro|**89.37**|**2.6307**|**81.9507**|**0.5692**|**4.3865**|\n\n**Weakness 2: It is likely that previous works can be readily applied to the motif-conditioned setting, and the authors can easily adapt their method to antibody design tasks. It would be beneficial if the authors further elaborate on the significance of their work**\n\nAns: We would like to clarify our goal more clearly. Our task is to co-design functional and novel protein sequence and structure. Previous work which is most similar to ours is Inpainting [1]. However, [1] provides the functional sites manually while we automatically mined the functional sites (plus the conserved sites as suggested by reviewer ****VH6U****) by MSAs. Previous methods considering only sequence design constrained by fitness value [2,3] or sequence design based on given backbone structure (inverse folding) [4] is not suitable for our task. For antibody design, antibody is always Y-shaped, while one of our goal is to design novel and diverse protein structure. For example, in Figure 4, the designed beta-lactamases have different fold categories and belong to three different sub-classes. In Figure 5 (b), the designed myoglobin has a RMSD of 3.943\\AA with the most similar one in PDB, but it can still bind the corresponding metallocofactor heme in the AlphaFill simulation [5]. From this aspect, we think antibody design models can not be directly applied to our task without any modification and vice versa. \n\n[1] Scaffolding protein functional sites using deep learning.  Jue wang. et al. Science. 2022.\n\n[2] Biological Sequence Design with GFlowNets. [Moksh Jain](https://arxiv.org/search/q-bio?searchtype=author&query=Jain,+M) et al. ICML 2022. \n\n[3] Proximal Exploration for Model-guided Protein Sequence Design. Zhizhou Ren et al. ICML 2022. \n\n[4] Robust deep learning based protein sequence design using ProteinMPNN. J. Dauparas et al. Science. 2022.\n\n[5] Alphafill: enriching alphafold models with ligands and cofactors. Maarten L Hekkelman et al. Nature methods. 2023.\n\n**Weakness 3:** **The authors do not provide code for checking the soundness of the methods.**\n\nAns: We have provided the code and data in the supplementary material. Due to the memory limitation, we couldn’t upload the model checkpoints. However, the training process is highly efficient, and the reviewer could train the model if he/she is interested.\n\n---\n\n**Q6: For a novel sequence, ESMFold’s prediction is worse than AlphaFold2. Consequently, evaluating novel sequences should use AlphaFold2 instead of ESMFold.**\n\nAns: Due to limited time and computing resources, we provide the AlphaFold2 results for ProtSeed, RFDiffusion+ProteinMPNN and our NAEPro. As the results show, even though ESMFold achieves lower scores than AlphaFold2, the tendencies (ranking) among different models are similar.  Particularly, our model achieves an average TM-score over 0.5 on beta-lactamase (ESMFold 0.4812) and pLDDT over 70 (ESMFold 66.8705), which gives stronger evidence that our model has the ability to design proteins which have stable structures.\n\n**myoglobin**\n\n|Model| pLDDT|TM-score|\n|:-----|:----:|:----:|\n|PROTSEED|77.9032|0.5621|\n|RFDiffusion+ProteinMPNN|88.2130|0.5103|\n|NAEPro|**91.4139**|**0.5912**|\n\n**$\\beta$-lactamase**\n\n|Model| pLDDT|TM-score|\n|:-----|:----:|:----:|\n|PROTSEED|73.3176|0.4901|\n|RFDiffusion+ProteinMPNN|**87.8092**|0.4806|\n|NAEPro|76.9550|**0.5009**|\n\n **Q7: \"Consistency - What is the designed structure and the predicted structure? Which is from NAEPro? Where does the other one come from?**\n\nAns: Consistency is calculated as the RMSD between the designed structure from NAEPro and the one predicted by ESMFold for the designed sequence from NAEPro.\n\n---\n\n**Weakness 4:  Results in Table 1 do not support the superiority of the proposed method. For instance, RMSD>1.7 \\AA means the two structures are different for myoglobin. For pLDDT, the prediction on lactamase is ~66%, which is much lower than 80%-90% which is conventionally believed reliable for folding predictions. The TM-score, if they are smaller than 0.5, then the two structures are believed different.** \n\nAns: We would like to clarify our goal more clearly.  Our task is to design the sequence and structure of proteins with effective functions.  We calculated TM-score by AlphaFold2 as suggested by the reviewer, and the two proteins designed by our model both achieves an average score over 0.5 (myoglobin **0.5912** and beta-lactamase **0.5009**). However, as found by previous method [7],  protein pairs with a TM-score >0.5 are most likely in the same fold while those with a TM-score <0.5 are mainly not in the same fold. Therefore, low TM-score can not indicate the designed protein doesn’t have the corresponding function. As claimed in previous work [9,10], pLDDT between 70~90 are classified to be confident. We calculated pLDDT by AlphaFold2 as suggested by the reviewer and the two protein families designed by our model are confident on an average aspect (myoglobin **91.4139** and beta-lactamase **76.9550**).  Similarly, as observed in previous work[11,12], pLDDT has a weak correlation with protein function. Therefore, a moderate TM-score and pLDDT can not indicate a protein expresses no or poor function. As shown in figure 5(b), the designed myoglobin has a RMSD of 3.943\\AA with the most similar one in PDB, but it can still bind the corresponding metallocofactor heme in the AlphaFill simulation [8]. To systematically illustrate the function of our designed proteins, we use docking method Gnina [13] to compute the binding affinity between the designed metalloproteins and the corresponding metallocofactors. The results in the following table shows that our model achieves the best binding affinity scores on both metalloproteins, demonstrating the proteins designed by our model are highly potential to actively exhibit biochemical functions.\n\n|Model|myoglobin (kcal/mol)|beta-lactamase (kcal/mol)|\n|:-----|:----:|:----:|\n|Hallucination|-7.23|-6.37|\n|Inpainting|-9.32|-7.63|\n|SMCDiff+ProteinMPNN|-8.79|-6.89|\n|PROTSEED|-10.07|-7.68|\n|FrameDiff+ProteinMPNN|-9.54|-7.21|\n|RFDiffusion+ProteinMPNN|-9.76|-7.59|\n|NAEPro|**-10.14**|**-7.80**|\n\n[7] How significant is a protein structure similarity with TM-score= 0.5? Jinrui Xu and Yang Zhang. *Bioinformatics. 2010.* \n\n[8] Alphafill: enriching alphafold models with ligands and cofactors. Maarten L Hekkelman et al. Nature methods. 2023.\n\n[9] pLDDT Values in AlphaFold2 Protein Models Are Unrelated to Globular Protein Local Flexibility. Oliviero Carugo. Crystals. 2023\n\n[10] Generating new protein sequences by using dense network and attention mechanism. Feng Wang et al. Mathematical Biosciences and Engineering. 2023.\n\n[11] Using AlphaFold to predict the impact of single mutations on protein stability and function. Marina A. Pak et al. PloS one. 2023.\n\n[12] Peptide binder design with inverse folding and protein structure prediction. Patrick Bryant et al. Nature Communications Chemistry. 2023.\n\n[13] GNINA 1.0: Molecular docking with deep learning. A McNutt et al. Cheminformatics. 2021.\n\n**Q1 & Q2: \"The use of separate models for sequence and structure cannot ensure the consistency between the generated sequence and structure.\" \"knowing the topology of a protein before design process is difficult and also cannot guarantee the designed proteins have the desired functions.\" I disagree with this statement.** **I disagree with this statement.** \n\nAns: Thanks for the suggestion! We agree with the reviewer and updated the statement in our paper. \n\n**Q3: what is the meaning of \"generally-encoded\" for 20 types of amino acids?** \n\nAns: It means the 20 common amino acids. We have updated the paper accordingly.\n\n**Q4: \"The selection of motif varies from setting to setting...\". Can the authors please provide a clearer explanation?**\n\nAns: We mean the motifs have different meanings in different tasks. For de novo enzyme design [14], it means the binding sites where enzyme binds the metallocofactor and substrate. For de novo binder design [15], it means the binding sites where the protein binder binds the protein targets.\n\n[14] De novo enzyme design using rosetta3. Florian Richter et al. PloS one. 2011.\n\n[15] De novo design of protein interactions with learned surface fingerprints. Pablo Gainza. Nature. 2023.\n\n---\n\nWe thank the reviewer for the insightful questions. We have clarified all your concerns, added the experiments and updated the paper accordingly. Answers to specific points are provided below:\n\n**Weakness 1 & weakness 2 & Q5: The identified problem or the motivation for designing such a method is not supported by existing literature.  Some designed modules counter the intuition in biology, such as \"motif mining\"**\n\nAns: We follow the design insight proposed by [1], which starts from the functional site and fills in additional sequence and structure to create a viable protein scaffold in a single forward pass. Different from [1], we provide the functional sites which are automatically mined from MSA as previous methods [2, 3] while [1] provides the functional sites manually. Besides, we design a new architecture called NAEPro to generate the whole protein sequence and structure based on the given partial protein fragments, while [1] achieves this goal by finetuning the RoseTTAFold. However, we agree with the reviewer’s opinion in question 5 that MSA discovers not only protein functional sites as addressed in [2,3] but also finds conversed sites which may not directly relate to the protein function. Therefore, we change all the term “motif/functional sites” in our paper to  “meaningful protein fragments”.\n\n[1] Scaffolding protein functional sites using deep learning.  Jue wang. et al. Science. 2022.\n\n[2] Emerging methods in protein co-evolution. David de Juan et al. Nature Review Genetics. 2013.\n\n[3] Evolutionary information for specifying a protein fold. Michael Socolich et al. Nature. 2005.\n\n**Weakness 3 & Q1: The method is not 'extensively evaluated' as claimed by the authors (they only selectively analyzed 2 proteins), and many results do not suggest that the proposed method is SOTA among baselines. Moreover, the evaluation does not include all relevant methods, which makes it too aggressive for the authors to claim in the Introduction that their method \"achieves the HIGHEST...among ALL competitors\" and \"is faster than the FASTEST method\". Empirically, It has been proven by several recent research that the de novo method can generate new sequences for fixed backbone or desired functions [4-6].**\n\nAns: Sorry for the confusion. Our method achieves the best performance on 4/5 metrics on myoglobin and 3/5 metrics on beta-lactamase among all the compared representative baselines in our paper. Our model is faster than all baselines compared in our paper. We have revised all these statements in our paper (highlighted in red). For other relevant methods as the reviewer suggested in reference [4-6], we have already compared with [4]. [5] was posted on Oct. 3, 2023, which is later than the ICLR submission ddl, and [3] was posted on Aug. 14, 2023, which is within 3 months before ICLR submission ddl. We appreciate the reviewer’s suggestion, and we tried to compare with [3], but they haven’t released their code now. We emailed the authors of [3] to ask for code and they said their work is under review now and refused to provide code currently. We are trying to reimplement the model now and will update the results in our revised version.\n\n[4] Watson et al., De novo design of protein structure and function with RFdiffusion (2023).\n\n[5] Sumida et al., Improving protein expression, stability, and function with ProteinMPNN (2023).\n\n[6] Zhou et al., Conditional Protein Denoising Diffusion Generates Programmable Endonuclease Sequences (2023).\n\n---\n\nWe thank all the reviewers' valuable suggestions.\n\nWe have uploaded a revised draft that incorporates feedback from reviewers VH6U and MgyU, with the updated portions highlighted in red. Additionally, in response to reviewer Wq6M's suggestion, we have included the code and data we used in our paper in the supplementary material (Due to the maximum memory limitation -100M, we couldn’t upload the model checkpoints).\n\nHere's an overview highlighting the principal modifications to our paper:\n\n1. The revised term from “motif/functional sites” to “meaningful fragments” in abstract, introduction third paragraph, method opening paragraph, method 3.1-3.4, experiments 4.1-4.2, analysis 5.1, Conclusion (Reviewer VH6U, weakness 1 & weakness 2 & Q5)\n2. The corrected claim in abstract and last paragraph in introduction (review VH6U, weakness3) \n3. The corrected claim in Introduction second paragraph (Reviewer VH6U, Q1)\n4. The corrected claim in Related Work second paragraph (Reviewer VH6U, Q2)\n5. The clarified explanation of 20 common amino acids in Method opening paragraph(Reviewer VH6U, Q3)\n6. The corrected theoretical claim in Method 3.5 and Appendix A. 1 (reviewer MgyU, weakness 5) \n7. The clarified task definition in Method opening paragraph (reviewer MgyU, Q2)\n\nWe look forward to any additional reading and feedback. If you have any further questions, we are very happy to continue discussion!", "author_response": "Thanks for the responses! Our answers to your follow-up questions are provided as follows:\n\n**1. About Weakness4 & Q1 Page 2, \"We carry out experiments on two metalloproteins, including β-lactamase and myoglobin.\" I would like to say that the experimental setting is the main concern to me. Since the authors have only evaluated the proposed method on two structures, I am not sure if the method can be extended to generalized situations. I know that you are doing protein sequence-structure inpaintning problem, in which case a comparison with a previous baselines [1,2,3] on stantard datasets may help to reveal the generalization capabilities of your algorithm.** \n\nAns: We agree with reviewer Wq6M and additionally compare our method with PROTSEED on B12 protein. Due to the approaching ddl and limited time, PROTSEED has only been trained for 100 epochs, the same as our method. (suggested training epoch is 2000 on their GitHub)\n\nThe results are reported as follows: \n\n**Binding affinity score for designed B12**\n\n|Model| top-5 (kcal/mol)|top-10 (kcal/mol)|top-30 (kcal/mol)| median|\n|:-----|:----:|:----:|:----:|:----:|\n|PROTSEED|-9.45$\\pm$ 0.03|-9.36$\\pm$0.09|-9.04$\\pm$0.27|-8.46|\n|NAEPro|**-11.06$\\pm$0.14**|**-10.96$\\pm$0.14**|**-10.61$\\pm$0.29**|**-9.11**|\n\n**2. About Q2 Your methods are not carefully designed for protein inverse folding. However, the presented results outperformed current SOTA by a large margin (curent SOTA is about 50%-60% AAR), which could not convince me. I doubt the veracity and correctness of the experimental results.** \n\nAns: Sorry for the confusion. We evaluate the inverse-folding task on our own two datasets instead of the original CATH. For our datasets, the proteins are from the same family, and thus they may have much overlap on the sequence level, which definitely will lead to high AAR. Besides, our model is initialized with ESM2 weights, which will also improve the AAR. \n\nAlthough the sequence prediction based a fixed backbone structure is not our objective, **we adapt our method to CATH setting by keeping all the CA coordinates and masking all residues. Due to the approaching ddl and limited time, our model has only been trained for 10 epochs, while usually the suggested training epoch would be 100 like GVP, PiFold, etc.** (See below: ESMIF, ProteinMPNN, PiFold results are quoted from PiFold paper). \n\nCATH 4.2 results\n\n|Model| AAR (%)|PPL |\n|:-----|:----:|:----:|\n|ProteinMPNN|45.96|4.61|\n|ESMIF|38.30|6.44|\n|PiFold|51.66|4.55|\n|NAEPro|9.16|15.26|\n\n**Again, we did not claim the superiority of our method on inverse folding task on CATH dataset.**\n\n**3. Page 1, \"Despite their great potential for novel structure design, such sequential design policy fails to cross-condition on sequence and structure, which might lead to inconsistent proteins and inefficient design process\". I can not agree with this statement. RFDiffusion's authors say that they also considered simultaneously designing structure and sequence within RFdiffusion, but combining ProteinMPNN with the diffusion of structure alone provides the excellent performance, as shown in your Table.1. Unfortunately, I observe that you deleted the consistency metric in Table.1, which seems like dishonest behavior to me.**\n\nAns: We do not intend to hide any information. As suggested by reviewer MgyU, the binding affinity is a better metric for functional protein design rather than the accuracy metrics (consistency, AAR, RMSD, etc.). Therefore, we replaced all the accuracy metrics in Table 1 and 2 in our paper according with the binding affinity scores calculated by Gnina. The full table with all scores are listed below:\n\n$\\beta$-lactamase\n\n|Model| AAR (\\%,$\\uparrow$) | RMSD ({ \\AA},$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination | $4.79$ | $--$ | $30.5511$ | $0.2918$ | $--$|\n|Inpainting | $16.73$ | $4.0599$ | $61.7679$ | $0.3790$ | $6.2578$|\n|SMCDiff+ProteinMPNN| $19.94$ | $10.3960$ | $42.0375$ | $0.3458$ |$10.2117$|\n| PROTSEED | $37.63$ | $3.0142$ | $64.3861$| $0.4637$ | $3.3748$ |\n| FrameDiff+IF |$26.20$ | $6.0151$ | $65.6445$ | $0.3657$ | $7.8703$|\n|RFDiffusion+IF | $22.93$ | $6.0438$ | **83.4058** | $0.3747$ |**0.5565**|\n|NAEPro|**65.71**| **2.9916**| 66.8705|**0.4812**| 7.3760|\n\n\n**Myoglobin**\n|Model| AAR (\\%,$\\uparrow$) | RMSD (\\AA,$\\downarrow$) | pLDDT ($\\uparrow$) | TM-score ($\\uparrow$) | Consistency (\\AA,$\\downarrow$)|\n|:-----|:----:|:----:|:----:|:----:|:----:|\n|Hallucination| $4.81$ | $--$ | $38.2817$ | $0.2754$ |--|\n|Inpainting |39.59|3.3751|67.0813|0.4391 |3.2108|\n|SMCDiff+IF | $12.47$ | $8.0067$ | $34.5914$ | $0.2235$ | $8.8754$ |\n| PROTSEED | $48.55$ | $2.8753$ | $61.5588$ | $0.5466$ | $0.9764$ |\n| FrameDiff+IF | $20.43$ | $5.9739$ | $61.3945$ | $0.3757$ |$3.5078$|\n|RFDiffusion+IF | $32.70$ | $3.9930$ | $78.9868$ | $0.4147$  | **0.4375** |\n|NAEPro | **89.37**| **2.6307**|**81.9507**|**0.5692**|4.3865|"}
{"claim": "The proposed combinatorial toy model is too simplistic and fails to capture implicit biases of deep learning that drive observed merging phenomena.", "claim_type": "subjective", "paper_id": "ze7DOLi394", "paper_title": "On the Joint Interaction of Models, Data, and Features", "paper_venue": "iclr2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "RUsLYJEhvF", "reviewer": "Reviewer_MLur", "review_text": "Summary: This paper introduces the interaction tensor, for empirically analyzing the interaction between data and model through features. Based on some observations using this tensor, they propose a very simple toy model (a combinatorial model) for feature learning. They show that this model also exhibits Generalization Disagreement Equality (GDE). Finally, the authors use their model to provide data distributions that break GDE in real world experiments.\n\nStrengths: - The problem of feature learning is quite important and there has been a lot of attempts for gaining a better theoretical understanding of it in recent years.\n\n- The authors are able to come up with a toy model that shows GDE which is a very important phenomenon (and it does need the explicit assumption of calibration).\n\n- The model that the authors propose is simple and it can be analyzed fully.\n\n- The paper is very well-written.\n\nWeaknesses: - I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of\nabstraction\". However, I think the model proposed in this paper is too simplistic. The implicit biases of deep learning are core to some of the merging phenomenon that we see these days and the models that the authors propose fails to capture that. I also think that a good toy model should leave the door open to generalizations and getting closer to real world practice (for example, for the random features model of deep learning, there is a very obvious way to move towards making it more realistic). But the models that the authors propose is too abstract and it is not clear what simplifications are made to the real problem to arrive at the proposed model.\n\n- Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\n- It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\n- The notations are a bit confusing (i, j, k, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations (p_d, p_r, etc.). It will also help explain the method.\n\n- The authors \"prove\" GDE in their model without the explicit assumption of calibration. But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\n- The theoretical understanding of feature learning is not as rudimentary as the authors claim. For example,\n\n[1] Alex Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent, 2022.\n\n[2] Zhichao Wang, Andrew Engel, Anand Sarwate, Ioana Dumitriu, and Tony Chiang. Spectral evolution and invariance in linear-width neural networks, 2022.\n\n[3] Eshaan Nichani, Alex Damian, and Jason D Lee. Provable guarantees for nonlinear feature learning in three-layer neural networks, 2023.\n\n\n[4] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. Learning two-layer neural networks, one (giant) step at a time, 2023.\n\n[5] Behrad Moniri, Donghwan Lee, Hamed Hassani, and Edgar Dobriban, A theory of non-linear feature learning with one gradient step in two-layer neural networks, 2023.\n\nand many more.\n\nQuestions: - Looking at figure 3 (a) and 3 (c), it seems that the observations made from them are not that significant. Am I missing something? How does the choice of the thresholds affect your observations?", "labeling_timestamp": "2026-01-11T16:29:41.086240", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors acknowledge the model's simplicity and many assumptions and concede it does not model the learning process or fully match practice, but they defend the choice as a tractable, useful abstraction and argue it can still yield insight; thus they partially endorse the reviewer's critique while disputing that this makes the work without value.", "evidence": "\"We fully agree that many assumptions were made. The way we see it is that deep learning's complexity necessitates some level of simplification for tractable analysis.\"; \"This is correct. The framework as it stands does not describe the learning process, which is an extremely complicated subject itself.\"", "combined_author_response": "Thank you for the continued engagement and dedication to the review process! We address your remaining concerns as follows:\n\n> 0\n\nApologies for missing the first part of your question earlier. In 3 (a), the observation we are relying on is that data points with a small number of features are much more likely to have very high confidence under the ensemble and vice versa. For a concrete example, it is much more likely for a data point with 0.8 confidence to have 25 features than a data point with 0.4 confidence to have 25 features. As such, we believe that it’s fair to say that data with fewer features tend to have higher confidence. For 3(c), we claim that the more features the models share, the lower bound on their shared errors becomes higher, which can be clearly observed in the figure. This *does not* mean that the fewer features the models share, their shared errors would necessarily be smaller. Instead, both models would make random guesses (recall that we make similar assumptions in the conceptual model). This means that a pair of models can have a high or low amount of shared errors due to randomness, but the lower bound on the shared error will become higher if they share more features. We believe that the figure supports this claim.\n\n> 1\n\nThank you for clarifying what you meant about the limitations. We agree with your assessment of NTK and shallow neural networks, and we absolutely agree that we should be transparent with the limitations of the framework. Appendix D.3 (previously D.4) is meant to discuss the limitations of the model but we see that it may not be as clear as we hoped. As such, we have expanded Appendix D.3 to be more explicit about these limitations. This was already referred to at the beginning of section 5 where we introduced the model and in the conclusion, but once again, due to space limits, we could not include them in the main text.\n\n> 2\n\nThank you for clarifying this and pointing out the notation issues. We have streamlined the notations in Section 3 for better clarity so the footnotes are no longer needed.\n\nSpecifically, we have changed to use $C$ for the number of classes instead of $K$ and use $K$ to denote the number of top principal components, so all capital letters refer to a fixed number (e.g., $N$ for the total number of data, $M$ for the number of models, etc). Lower case of $N,M,K,T$ will refer to a particular index for what the object the capital letters refer to, so $k$ will always refer to the feature, $m$ will always refer to the model and $n$ will always refer to data, etc.\n\nOne exception is the k-partite graph. Since this is the standard reference to the problem we use $\\mathtt{k}$ to differentiate it.\nAnother one is that $i,j, a, b$ are used instead of $m$ and $k$ to refer to a pair of models and features around equation 2.\nWe believe that this should resolve the ambiguity but if there are any concerns regarding the notation left please let us know.\n\nRegarding the table of notation, we fully agree that a table would be great to have. Unfortunately, so far we have not been able to find a way to squeeze it into the text. And we will definitely include it in the extended version.\n\n------------\n\nWe hope these revisions and clarifications address your concerns. Please let us know if you have any further questions.\n\n---\n\nThank you for your thoughtful feedback! We are glad that you still find our work original and creative.\n\n> many assumptions went into the combinatorial analysis.\n\nWe fully agree that many assumptions were made. The way we see it is that deep learning's complexity necessitates some level of simplification for tractable analysis. In this case, since the resulting framework is still useful for understanding empirical phenomena, we feel the assumptions are justified. \n\n> The framework is less useful for understanding the learning process of networks…\n\nThis is correct. The framework as it stands does not describe the learning process, which is an extremely complicated subject itself. However, we postulate that techniques similar to [1] could be used to study the optimization of a feature model like ours in the future.\n\n> typos\n\nThank you for the close reading and for catching these typos! We have fixed them in the revision.\n\n> have you considered modeling the rare-dominant variation as a spectrum instead of a binary?\n\nThis is an excellent question! We have indeed considered the possibility of modeling it as a spectrum, which we discussed in Appendix D.1 and D.4. In our preliminary investigation, we found that modeling the feature spectrum may require introducing more free parameters (e.g., Zipf’s distribution), and can drastically increase the complexity of analysis. Given that this is our first foray into the topic, we opted for the simpler binary model. This is an important problem and we plan to explore it in more detail in future works.\n\nWe hope these responses have addressed some of the questions and we are open to further discussion. Thank you again for your constructive feedback!\n\n**Reference**\n\n[1] Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. Allen-Zhu et al.\n\n---\n\n> The theoretical understanding of feature learning is not as rudimentary as the authors claim\n\nWe sincerely apologize for the characterization. Perhaps “rudimentary” is not the best description. We have revised the phrase to be “still in early development” to better convey the sentiment. However, we do feel that it is reasonable to say that we still have much to understand about feature learning. \n\nTo briefly address the specific works you mentioned: these models generally assume the input data are isotropic Gaussian and the function class is a 2 layer MLP. The feature is usually a linear function of the input and often special training algorithms are needed (e.g., layer-wise training). A notable exception is [3] which learns non-linear features but still requires layer-wise training. \nMost of these works are quite recent (and all came out after we started this project): [3,4,5] showed up in October 2023 and [2] was updated in November 2023. In general, it is hard to say how well they encapsulate the full complexity of deep learning, or how feasible it is to apply them to modern architectures and datasets. Nonetheless, the goal of this work is not to compare existing theoretical works on the topic or discuss their merits and shortcomings, but rather to identify what realistic (perhaps strong) modeling assumptions to make when studying feature learning. Given the recently increasing interest in the subject, we view these works and our approach as complementary components of a broader exploration into feature learning. We have added a discussion of these works to Appendix D.1, but would be happy to revise them if you have further suggestions.\n\nWe hope these responses address your concerns and we are open to further discussion. Thank you again for your constructive feedback!\n\n**Reference**\n\n[6] Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. Allen-Zhu et al.\n\n---\n\nThank you for your detailed review and thoughtful feedback. We are glad that you find the problem we study timely and important. Below are our responses to your concerns:\n\n> I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of abstraction\". However, I think the model proposed in this paper is too simplistic…\n\nWe definitely agree that our model is unconventional and more abstract than most existing works but similar assumptions have been made in prior work [6]. As is the case for all theoretical models, simplifications are required to keep analysis tractable. Despite the simplification, our model already exhibits some interesting behaviors and offers insight into empirical phenomena so we believe that it adds value to the current discussion on feature learning. \n\nIn the case of random feature models, we feel that it is a little unfair to say in hindsight that there was a very obvious way to move towards making it more realistic. When NTK first came out in 2018, it was thought to be very promising. Indeed, despite its simplification, it offers clear value in the analysis of some phenomena in deep networks. However, it only later became clear that there was a big gap between NTK and real deep learning (despite many efforts to close this gap). More recent works (many of which you already brought up) rely on quite distinct analysis tools than those used in NTK and, still, it’s not clear if any of them is close to real-world practice.  As such, while we agree (and have acknowledged in Section 5, Appendix D.1 and D.4) that there is a gap between our proposed framework and practice, we feel it’s a tall order for the current state of deep learning theory to have no strong assumptions.  Just like NTK analysis, we hope that our approach will ultimately prove valuable (albeit along different dimensions), despite its simplicity.\n\n> Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\nThe primary goal of our work is to investigate the effect of different features being learned by different models on GDE and the calibration of deep ensembles, both of which are robust empirical observations. While our model does not directly provide insights into training, understanding these feature interactions may indirectly inform training strategies in the future. For example, [6] shows potential avenues for how observations like ours may transfer to training.\n\n> It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\nAs detailed in Appendix F.10 (referred to at the end of Section 4), our conclusions are robust across a range of parameter settings so we believe that the settings used in our paper should be appropriate for most situations.\n\n> The notations are a bit confusing ($i, j, k$, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations ($p_d, p_r$, etc.). It will also help explain the method.\n\nWe apologize for the confusion. There is already a centralized list of notations in Appendix A which we could not include in the main text due to space constraints (this is referred to in the second paragraph of section 5). Beyond this, could you be more specific about where the $i,j,k$ are causing confusion so we could better address the confusion? $i,j,k$ are used to denote an arbitrary member of the ordered collection so we feel this is in accordance with the standard usage.\n\n> But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\nWe want to clarify that we *do not* claim that the assumptions we make are weaker than calibration (since neither is a clear superset of the other), but we do believe that it sheds more light on the underlying reasons behind GDE. Only stating the model is calibrated does not tell us much about the model or the data, but our assumptions do. In fact, our framework tells us how one can break our assumption (and calibration) through data distribution interventions. Of course, it is an important open question why so many natural distributions seem to satisfy these assumptions.\n\n---\n\nThank you for the support of our paper! We are glad that you find our work valuable.\n\nRegarding Figure 1, to better address your concerns, could you elaborate on which aspects of the figure you found unclear? This will help us make more targeted improvements. \n\nThe goal of Figure 1 is to demonstrate that our definition of features can capture images with redundant information and images that contain uncommon instances of a given class. To make this clearer, we have updated the figure in the paper with texts to better reflect the core idea. Does this resolve your concerns? If not, we’d be happy to make more adjustments.\n\n---\n\nThank you for your supportive feedback on our work. We are glad that you find our new perspective on feature learning interesting. Below are our responses to your questions:\n\n> how important are these hyperparameters? How to properly set them?\n\nIn Appendix F.10, we conducted an ablation study and found that our qualitative conclusions are robust to variations in these hyperparameters. Therefore, we believe the values used in our paper should be appropriate for most applications.\n\n> Currently theoretical framework analyzes binary classification, does the analysis extend to multi-class classification?\n\nWe discuss the necessary steps for the extension to multi-class classification in Appendix D.3. \n\n>Does the framework allow for introducing distribution shifts and etc?\n\nThis is a great question. We believe our framework can accommodate distribution shifts. For instance, the simplest form of covariate shift one could introduce is to add a new set of features that are not present in the training data with some probability at test time. Since the model has not seen these new features during training, we may assume that individual members of the ensemble will make random predictions (but the prediction of different models will be correlated) if the data points only contain the new features (the same assumption for features that the models did not learn). One can also introduce a covariate shift on existing features but that would require imposing a non-uniform distribution over the existing features. We leave the exploration of these directions to future works.\n\nWe hope these answers clarify your question, and we are open to further discussion. Thank you again for your constructive feedback.\n\n---\n\nWe thank all the reviewers for their valuable time. We are glad that the reviewers found the results original, interesting, and timely.\n\nWe have modified Figure 1 and made minor revisions, and added some related works to the paper which are highlighted with orange in the text.\n\nWe will respond to each reviewer’s comments individually.", "author_response": "Thank you for your detailed review and thoughtful feedback. We are glad that you find the problem we study timely and important. Below are our responses to your concerns:\n\n> I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of abstraction\". However, I think the model proposed in this paper is too simplistic…\n\nWe definitely agree that our model is unconventional and more abstract than most existing works but similar assumptions have been made in prior work [6]. As is the case for all theoretical models, simplifications are required to keep analysis tractable. Despite the simplification, our model already exhibits some interesting behaviors and offers insight into empirical phenomena so we believe that it adds value to the current discussion on feature learning. \n\nIn the case of random feature models, we feel that it is a little unfair to say in hindsight that there was a very obvious way to move towards making it more realistic. When NTK first came out in 2018, it was thought to be very promising. Indeed, despite its simplification, it offers clear value in the analysis of some phenomena in deep networks. However, it only later became clear that there was a big gap between NTK and real deep learning (despite many efforts to close this gap). More recent works (many of which you already brought up) rely on quite distinct analysis tools than those used in NTK and, still, it’s not clear if any of them is close to real-world practice.  As such, while we agree (and have acknowledged in Section 5, Appendix D.1 and D.4) that there is a gap between our proposed framework and practice, we feel it’s a tall order for the current state of deep learning theory to have no strong assumptions.  Just like NTK analysis, we hope that our approach will ultimately prove valuable (albeit along different dimensions), despite its simplicity.\n\n> Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\nThe primary goal of our work is to investigate the effect of different features being learned by different models on GDE and the calibration of deep ensembles, both of which are robust empirical observations. While our model does not directly provide insights into training, understanding these feature interactions may indirectly inform training strategies in the future. For example, [6] shows potential avenues for how observations like ours may transfer to training.\n\n> It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\nAs detailed in Appendix F.10 (referred to at the end of Section 4), our conclusions are robust across a range of parameter settings so we believe that the settings used in our paper should be appropriate for most situations.\n\n> The notations are a bit confusing ($i, j, k$, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations ($p_d, p_r$, etc.). It will also help explain the method.\n\nWe apologize for the confusion. There is already a centralized list of notations in Appendix A which we could not include in the main text due to space constraints (this is referred to in the second paragraph of section 5). Beyond this, could you be more specific about where the $i,j,k$ are causing confusion so we could better address the confusion? $i,j,k$ are used to denote an arbitrary member of the ordered collection so we feel this is in accordance with the standard usage.\n\n> But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\nWe want to clarify that we *do not* claim that the assumptions we make are weaker than calibration (since neither is a clear superset of the other), but we do believe that it sheds more light on the underlying reasons behind GDE. Only stating the model is calibrated does not tell us much about the model or the data, but our assumptions do. In fact, our framework tells us how one can break our assumption (and calibration) through data distribution interventions. Of course, it is an important open question why so many natural distributions seem to satisfy these assumptions."}
{"claim": "The proposed model does not leave an obvious path for generalizations toward more realistic models or practical deep learning setups.", "claim_type": "novelty", "paper_id": "ze7DOLi394", "paper_title": "On the Joint Interaction of Models, Data, and Features", "paper_venue": "iclr2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "RUsLYJEhvF", "reviewer": "Reviewer_MLur", "review_text": "Summary: This paper introduces the interaction tensor, for empirically analyzing the interaction between data and model through features. Based on some observations using this tensor, they propose a very simple toy model (a combinatorial model) for feature learning. They show that this model also exhibits Generalization Disagreement Equality (GDE). Finally, the authors use their model to provide data distributions that break GDE in real world experiments.\n\nStrengths: - The problem of feature learning is quite important and there has been a lot of attempts for gaining a better theoretical understanding of it in recent years.\n\n- The authors are able to come up with a toy model that shows GDE which is a very important phenomenon (and it does need the explicit assumption of calibration).\n\n- The model that the authors propose is simple and it can be analyzed fully.\n\n- The paper is very well-written.\n\nWeaknesses: - I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of\nabstraction\". However, I think the model proposed in this paper is too simplistic. The implicit biases of deep learning are core to some of the merging phenomenon that we see these days and the models that the authors propose fails to capture that. I also think that a good toy model should leave the door open to generalizations and getting closer to real world practice (for example, for the random features model of deep learning, there is a very obvious way to move towards making it more realistic). But the models that the authors propose is too abstract and it is not clear what simplifications are made to the real problem to arrive at the proposed model.\n\n- Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\n- It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\n- The notations are a bit confusing (i, j, k, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations (p_d, p_r, etc.). It will also help explain the method.\n\n- The authors \"prove\" GDE in their model without the explicit assumption of calibration. But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\n- The theoretical understanding of feature learning is not as rudimentary as the authors claim. For example,\n\n[1] Alex Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent, 2022.\n\n[2] Zhichao Wang, Andrew Engel, Anand Sarwate, Ioana Dumitriu, and Tony Chiang. Spectral evolution and invariance in linear-width neural networks, 2022.\n\n[3] Eshaan Nichani, Alex Damian, and Jason D Lee. Provable guarantees for nonlinear feature learning in three-layer neural networks, 2023.\n\n\n[4] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. Learning two-layer neural networks, one (giant) step at a time, 2023.\n\n[5] Behrad Moniri, Donghwan Lee, Hamed Hassani, and Edgar Dobriban, A theory of non-linear feature learning with one gradient step in two-layer neural networks, 2023.\n\nand many more.\n\nQuestions: - Looking at figure 3 (a) and 3 (c), it seems that the observations made from them are not that significant. Am I missing something? How does the choice of the thresholds affect your observations?", "labeling_timestamp": "2026-01-11T16:29:51.303931", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors acknowledge the gap between their abstract model and practical setups and explicitly describe limitations, but they also point to possible extensions (e.g., applying optimization techniques, extending to multi-class, and accommodating distribution shifts) and commit to exploring these directions in future work, so they partially address the reviewer's claim.", "evidence": "\"This is correct. The framework as it stands does not describe the learning process, which is an extremely complicated subject itself. However, we postulate that techniques similar to [1] could be used to study the optimization of a feature model like ours in the future.\" \n\n\"We believe our framework can accommodate distribution shifts. For instance, the simplest form of covariate shift one could introduce is to add a new set of features that are not present in the training data with some probability at test time.... One can also introduce a covariate shift on existing features but that would require imposing a non-uniform distribution over the existing features. We leave the exploration of these directions to future works.\"", "combined_author_response": "Thank you for the continued engagement and dedication to the review process! We address your remaining concerns as follows:\n\n> 0\n\nApologies for missing the first part of your question earlier. In 3 (a), the observation we are relying on is that data points with a small number of features are much more likely to have very high confidence under the ensemble and vice versa. For a concrete example, it is much more likely for a data point with 0.8 confidence to have 25 features than a data point with 0.4 confidence to have 25 features. As such, we believe that it’s fair to say that data with fewer features tend to have higher confidence. For 3(c), we claim that the more features the models share, the lower bound on their shared errors becomes higher, which can be clearly observed in the figure. This *does not* mean that the fewer features the models share, their shared errors would necessarily be smaller. Instead, both models would make random guesses (recall that we make similar assumptions in the conceptual model). This means that a pair of models can have a high or low amount of shared errors due to randomness, but the lower bound on the shared error will become higher if they share more features. We believe that the figure supports this claim.\n\n> 1\n\nThank you for clarifying what you meant about the limitations. We agree with your assessment of NTK and shallow neural networks, and we absolutely agree that we should be transparent with the limitations of the framework. Appendix D.3 (previously D.4) is meant to discuss the limitations of the model but we see that it may not be as clear as we hoped. As such, we have expanded Appendix D.3 to be more explicit about these limitations. This was already referred to at the beginning of section 5 where we introduced the model and in the conclusion, but once again, due to space limits, we could not include them in the main text.\n\n> 2\n\nThank you for clarifying this and pointing out the notation issues. We have streamlined the notations in Section 3 for better clarity so the footnotes are no longer needed.\n\nSpecifically, we have changed to use $C$ for the number of classes instead of $K$ and use $K$ to denote the number of top principal components, so all capital letters refer to a fixed number (e.g., $N$ for the total number of data, $M$ for the number of models, etc). Lower case of $N,M,K,T$ will refer to a particular index for what the object the capital letters refer to, so $k$ will always refer to the feature, $m$ will always refer to the model and $n$ will always refer to data, etc.\n\nOne exception is the k-partite graph. Since this is the standard reference to the problem we use $\\mathtt{k}$ to differentiate it.\nAnother one is that $i,j, a, b$ are used instead of $m$ and $k$ to refer to a pair of models and features around equation 2.\nWe believe that this should resolve the ambiguity but if there are any concerns regarding the notation left please let us know.\n\nRegarding the table of notation, we fully agree that a table would be great to have. Unfortunately, so far we have not been able to find a way to squeeze it into the text. And we will definitely include it in the extended version.\n\n------------\n\nWe hope these revisions and clarifications address your concerns. Please let us know if you have any further questions.\n\n---\n\nThank you for your thoughtful feedback! We are glad that you still find our work original and creative.\n\n> many assumptions went into the combinatorial analysis.\n\nWe fully agree that many assumptions were made. The way we see it is that deep learning's complexity necessitates some level of simplification for tractable analysis. In this case, since the resulting framework is still useful for understanding empirical phenomena, we feel the assumptions are justified. \n\n> The framework is less useful for understanding the learning process of networks…\n\nThis is correct. The framework as it stands does not describe the learning process, which is an extremely complicated subject itself. However, we postulate that techniques similar to [1] could be used to study the optimization of a feature model like ours in the future.\n\n> typos\n\nThank you for the close reading and for catching these typos! We have fixed them in the revision.\n\n> have you considered modeling the rare-dominant variation as a spectrum instead of a binary?\n\nThis is an excellent question! We have indeed considered the possibility of modeling it as a spectrum, which we discussed in Appendix D.1 and D.4. In our preliminary investigation, we found that modeling the feature spectrum may require introducing more free parameters (e.g., Zipf’s distribution), and can drastically increase the complexity of analysis. Given that this is our first foray into the topic, we opted for the simpler binary model. This is an important problem and we plan to explore it in more detail in future works.\n\nWe hope these responses have addressed some of the questions and we are open to further discussion. Thank you again for your constructive feedback!\n\n**Reference**\n\n[1] Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. Allen-Zhu et al.\n\n---\n\n> The theoretical understanding of feature learning is not as rudimentary as the authors claim\n\nWe sincerely apologize for the characterization. Perhaps “rudimentary” is not the best description. We have revised the phrase to be “still in early development” to better convey the sentiment. However, we do feel that it is reasonable to say that we still have much to understand about feature learning. \n\nTo briefly address the specific works you mentioned: these models generally assume the input data are isotropic Gaussian and the function class is a 2 layer MLP. The feature is usually a linear function of the input and often special training algorithms are needed (e.g., layer-wise training). A notable exception is [3] which learns non-linear features but still requires layer-wise training. \nMost of these works are quite recent (and all came out after we started this project): [3,4,5] showed up in October 2023 and [2] was updated in November 2023. In general, it is hard to say how well they encapsulate the full complexity of deep learning, or how feasible it is to apply them to modern architectures and datasets. Nonetheless, the goal of this work is not to compare existing theoretical works on the topic or discuss their merits and shortcomings, but rather to identify what realistic (perhaps strong) modeling assumptions to make when studying feature learning. Given the recently increasing interest in the subject, we view these works and our approach as complementary components of a broader exploration into feature learning. We have added a discussion of these works to Appendix D.1, but would be happy to revise them if you have further suggestions.\n\nWe hope these responses address your concerns and we are open to further discussion. Thank you again for your constructive feedback!\n\n**Reference**\n\n[6] Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. Allen-Zhu et al.\n\n---\n\nThank you for your detailed review and thoughtful feedback. We are glad that you find the problem we study timely and important. Below are our responses to your concerns:\n\n> I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of abstraction\". However, I think the model proposed in this paper is too simplistic…\n\nWe definitely agree that our model is unconventional and more abstract than most existing works but similar assumptions have been made in prior work [6]. As is the case for all theoretical models, simplifications are required to keep analysis tractable. Despite the simplification, our model already exhibits some interesting behaviors and offers insight into empirical phenomena so we believe that it adds value to the current discussion on feature learning. \n\nIn the case of random feature models, we feel that it is a little unfair to say in hindsight that there was a very obvious way to move towards making it more realistic. When NTK first came out in 2018, it was thought to be very promising. Indeed, despite its simplification, it offers clear value in the analysis of some phenomena in deep networks. However, it only later became clear that there was a big gap between NTK and real deep learning (despite many efforts to close this gap). More recent works (many of which you already brought up) rely on quite distinct analysis tools than those used in NTK and, still, it’s not clear if any of them is close to real-world practice.  As such, while we agree (and have acknowledged in Section 5, Appendix D.1 and D.4) that there is a gap between our proposed framework and practice, we feel it’s a tall order for the current state of deep learning theory to have no strong assumptions.  Just like NTK analysis, we hope that our approach will ultimately prove valuable (albeit along different dimensions), despite its simplicity.\n\n> Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\nThe primary goal of our work is to investigate the effect of different features being learned by different models on GDE and the calibration of deep ensembles, both of which are robust empirical observations. While our model does not directly provide insights into training, understanding these feature interactions may indirectly inform training strategies in the future. For example, [6] shows potential avenues for how observations like ours may transfer to training.\n\n> It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\nAs detailed in Appendix F.10 (referred to at the end of Section 4), our conclusions are robust across a range of parameter settings so we believe that the settings used in our paper should be appropriate for most situations.\n\n> The notations are a bit confusing ($i, j, k$, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations ($p_d, p_r$, etc.). It will also help explain the method.\n\nWe apologize for the confusion. There is already a centralized list of notations in Appendix A which we could not include in the main text due to space constraints (this is referred to in the second paragraph of section 5). Beyond this, could you be more specific about where the $i,j,k$ are causing confusion so we could better address the confusion? $i,j,k$ are used to denote an arbitrary member of the ordered collection so we feel this is in accordance with the standard usage.\n\n> But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\nWe want to clarify that we *do not* claim that the assumptions we make are weaker than calibration (since neither is a clear superset of the other), but we do believe that it sheds more light on the underlying reasons behind GDE. Only stating the model is calibrated does not tell us much about the model or the data, but our assumptions do. In fact, our framework tells us how one can break our assumption (and calibration) through data distribution interventions. Of course, it is an important open question why so many natural distributions seem to satisfy these assumptions.\n\n---\n\nThank you for the support of our paper! We are glad that you find our work valuable.\n\nRegarding Figure 1, to better address your concerns, could you elaborate on which aspects of the figure you found unclear? This will help us make more targeted improvements. \n\nThe goal of Figure 1 is to demonstrate that our definition of features can capture images with redundant information and images that contain uncommon instances of a given class. To make this clearer, we have updated the figure in the paper with texts to better reflect the core idea. Does this resolve your concerns? If not, we’d be happy to make more adjustments.\n\n---\n\nThank you for your supportive feedback on our work. We are glad that you find our new perspective on feature learning interesting. Below are our responses to your questions:\n\n> how important are these hyperparameters? How to properly set them?\n\nIn Appendix F.10, we conducted an ablation study and found that our qualitative conclusions are robust to variations in these hyperparameters. Therefore, we believe the values used in our paper should be appropriate for most applications.\n\n> Currently theoretical framework analyzes binary classification, does the analysis extend to multi-class classification?\n\nWe discuss the necessary steps for the extension to multi-class classification in Appendix D.3. \n\n>Does the framework allow for introducing distribution shifts and etc?\n\nThis is a great question. We believe our framework can accommodate distribution shifts. For instance, the simplest form of covariate shift one could introduce is to add a new set of features that are not present in the training data with some probability at test time. Since the model has not seen these new features during training, we may assume that individual members of the ensemble will make random predictions (but the prediction of different models will be correlated) if the data points only contain the new features (the same assumption for features that the models did not learn). One can also introduce a covariate shift on existing features but that would require imposing a non-uniform distribution over the existing features. We leave the exploration of these directions to future works.\n\nWe hope these answers clarify your question, and we are open to further discussion. Thank you again for your constructive feedback.\n\n---\n\nWe thank all the reviewers for their valuable time. We are glad that the reviewers found the results original, interesting, and timely.\n\nWe have modified Figure 1 and made minor revisions, and added some related works to the paper which are highlighted with orange in the text.\n\nWe will respond to each reviewer’s comments individually.", "author_response": "Thank you for your detailed review and thoughtful feedback. We are glad that you find the problem we study timely and important. Below are our responses to your concerns:\n\n> I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of abstraction\". However, I think the model proposed in this paper is too simplistic…\n\nWe definitely agree that our model is unconventional and more abstract than most existing works but similar assumptions have been made in prior work [6]. As is the case for all theoretical models, simplifications are required to keep analysis tractable. Despite the simplification, our model already exhibits some interesting behaviors and offers insight into empirical phenomena so we believe that it adds value to the current discussion on feature learning. \n\nIn the case of random feature models, we feel that it is a little unfair to say in hindsight that there was a very obvious way to move towards making it more realistic. When NTK first came out in 2018, it was thought to be very promising. Indeed, despite its simplification, it offers clear value in the analysis of some phenomena in deep networks. However, it only later became clear that there was a big gap between NTK and real deep learning (despite many efforts to close this gap). More recent works (many of which you already brought up) rely on quite distinct analysis tools than those used in NTK and, still, it’s not clear if any of them is close to real-world practice.  As such, while we agree (and have acknowledged in Section 5, Appendix D.1 and D.4) that there is a gap between our proposed framework and practice, we feel it’s a tall order for the current state of deep learning theory to have no strong assumptions.  Just like NTK analysis, we hope that our approach will ultimately prove valuable (albeit along different dimensions), despite its simplicity.\n\n> Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\nThe primary goal of our work is to investigate the effect of different features being learned by different models on GDE and the calibration of deep ensembles, both of which are robust empirical observations. While our model does not directly provide insights into training, understanding these feature interactions may indirectly inform training strategies in the future. For example, [6] shows potential avenues for how observations like ours may transfer to training.\n\n> It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\nAs detailed in Appendix F.10 (referred to at the end of Section 4), our conclusions are robust across a range of parameter settings so we believe that the settings used in our paper should be appropriate for most situations.\n\n> The notations are a bit confusing ($i, j, k$, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations ($p_d, p_r$, etc.). It will also help explain the method.\n\nWe apologize for the confusion. There is already a centralized list of notations in Appendix A which we could not include in the main text due to space constraints (this is referred to in the second paragraph of section 5). Beyond this, could you be more specific about where the $i,j,k$ are causing confusion so we could better address the confusion? $i,j,k$ are used to denote an arbitrary member of the ordered collection so we feel this is in accordance with the standard usage.\n\n> But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\nWe want to clarify that we *do not* claim that the assumptions we make are weaker than calibration (since neither is a clear superset of the other), but we do believe that it sheds more light on the underlying reasons behind GDE. Only stating the model is calibrated does not tell us much about the model or the data, but our assumptions do. In fact, our framework tells us how one can break our assumption (and calibration) through data distribution interventions. Of course, it is an important open question why so many natural distributions seem to satisfy these assumptions."}
{"claim": "The paper does not clearly state which simplifications connect the real problem to the proposed abstract model.", "claim_type": "methodology", "paper_id": "ze7DOLi394", "paper_title": "On the Joint Interaction of Models, Data, and Features", "paper_venue": "iclr2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "RUsLYJEhvF", "reviewer": "Reviewer_MLur", "review_text": "Summary: This paper introduces the interaction tensor, for empirically analyzing the interaction between data and model through features. Based on some observations using this tensor, they propose a very simple toy model (a combinatorial model) for feature learning. They show that this model also exhibits Generalization Disagreement Equality (GDE). Finally, the authors use their model to provide data distributions that break GDE in real world experiments.\n\nStrengths: - The problem of feature learning is quite important and there has been a lot of attempts for gaining a better theoretical understanding of it in recent years.\n\n- The authors are able to come up with a toy model that shows GDE which is a very important phenomenon (and it does need the explicit assumption of calibration).\n\n- The model that the authors propose is simple and it can be analyzed fully.\n\n- The paper is very well-written.\n\nWeaknesses: - I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of\nabstraction\". However, I think the model proposed in this paper is too simplistic. The implicit biases of deep learning are core to some of the merging phenomenon that we see these days and the models that the authors propose fails to capture that. I also think that a good toy model should leave the door open to generalizations and getting closer to real world practice (for example, for the random features model of deep learning, there is a very obvious way to move towards making it more realistic). But the models that the authors propose is too abstract and it is not clear what simplifications are made to the real problem to arrive at the proposed model.\n\n- Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\n- It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\n- The notations are a bit confusing (i, j, k, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations (p_d, p_r, etc.). It will also help explain the method.\n\n- The authors \"prove\" GDE in their model without the explicit assumption of calibration. But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\n- The theoretical understanding of feature learning is not as rudimentary as the authors claim. For example,\n\n[1] Alex Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent, 2022.\n\n[2] Zhichao Wang, Andrew Engel, Anand Sarwate, Ioana Dumitriu, and Tony Chiang. Spectral evolution and invariance in linear-width neural networks, 2022.\n\n[3] Eshaan Nichani, Alex Damian, and Jason D Lee. Provable guarantees for nonlinear feature learning in three-layer neural networks, 2023.\n\n\n[4] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. Learning two-layer neural networks, one (giant) step at a time, 2023.\n\n[5] Behrad Moniri, Donghwan Lee, Hamed Hassani, and Edgar Dobriban, A theory of non-linear feature learning with one gradient step in two-layer neural networks, 2023.\n\nand many more.\n\nQuestions: - Looking at figure 3 (a) and 3 (c), it seems that the observations made from them are not that significant. Am I missing something? How does the choice of the thresholds affect your observations?", "labeling_timestamp": "2026-01-11T16:29:42.794438", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors acknowledge the concern and state they have expanded the appendix and clarified the model's limitations, indicating they agree the paper needed clearer statements about simplifications connecting the real problem to the abstract model.", "evidence": "\"we absolutely agree that we should be transparent with the limitations of the framework. Appendix D.3 (previously D.4) is meant to discuss the limitations of the model but we see that it may not be as clear as we hoped. As such, we have expanded Appendix D.3 to be more explicit about these limitations.\"", "combined_author_response": "Thank you for the continued engagement and dedication to the review process! We address your remaining concerns as follows:\n\n> 0\n\nApologies for missing the first part of your question earlier. In 3 (a), the observation we are relying on is that data points with a small number of features are much more likely to have very high confidence under the ensemble and vice versa. For a concrete example, it is much more likely for a data point with 0.8 confidence to have 25 features than a data point with 0.4 confidence to have 25 features. As such, we believe that it’s fair to say that data with fewer features tend to have higher confidence. For 3(c), we claim that the more features the models share, the lower bound on their shared errors becomes higher, which can be clearly observed in the figure. This *does not* mean that the fewer features the models share, their shared errors would necessarily be smaller. Instead, both models would make random guesses (recall that we make similar assumptions in the conceptual model). This means that a pair of models can have a high or low amount of shared errors due to randomness, but the lower bound on the shared error will become higher if they share more features. We believe that the figure supports this claim.\n\n> 1\n\nThank you for clarifying what you meant about the limitations. We agree with your assessment of NTK and shallow neural networks, and we absolutely agree that we should be transparent with the limitations of the framework. Appendix D.3 (previously D.4) is meant to discuss the limitations of the model but we see that it may not be as clear as we hoped. As such, we have expanded Appendix D.3 to be more explicit about these limitations. This was already referred to at the beginning of section 5 where we introduced the model and in the conclusion, but once again, due to space limits, we could not include them in the main text.\n\n> 2\n\nThank you for clarifying this and pointing out the notation issues. We have streamlined the notations in Section 3 for better clarity so the footnotes are no longer needed.\n\nSpecifically, we have changed to use $C$ for the number of classes instead of $K$ and use $K$ to denote the number of top principal components, so all capital letters refer to a fixed number (e.g., $N$ for the total number of data, $M$ for the number of models, etc). Lower case of $N,M,K,T$ will refer to a particular index for what the object the capital letters refer to, so $k$ will always refer to the feature, $m$ will always refer to the model and $n$ will always refer to data, etc.\n\nOne exception is the k-partite graph. Since this is the standard reference to the problem we use $\\mathtt{k}$ to differentiate it.\nAnother one is that $i,j, a, b$ are used instead of $m$ and $k$ to refer to a pair of models and features around equation 2.\nWe believe that this should resolve the ambiguity but if there are any concerns regarding the notation left please let us know.\n\nRegarding the table of notation, we fully agree that a table would be great to have. Unfortunately, so far we have not been able to find a way to squeeze it into the text. And we will definitely include it in the extended version.\n\n------------\n\nWe hope these revisions and clarifications address your concerns. Please let us know if you have any further questions.\n\n---\n\nThank you for your thoughtful feedback! We are glad that you still find our work original and creative.\n\n> many assumptions went into the combinatorial analysis.\n\nWe fully agree that many assumptions were made. The way we see it is that deep learning's complexity necessitates some level of simplification for tractable analysis. In this case, since the resulting framework is still useful for understanding empirical phenomena, we feel the assumptions are justified. \n\n> The framework is less useful for understanding the learning process of networks…\n\nThis is correct. The framework as it stands does not describe the learning process, which is an extremely complicated subject itself. However, we postulate that techniques similar to [1] could be used to study the optimization of a feature model like ours in the future.\n\n> typos\n\nThank you for the close reading and for catching these typos! We have fixed them in the revision.\n\n> have you considered modeling the rare-dominant variation as a spectrum instead of a binary?\n\nThis is an excellent question! We have indeed considered the possibility of modeling it as a spectrum, which we discussed in Appendix D.1 and D.4. In our preliminary investigation, we found that modeling the feature spectrum may require introducing more free parameters (e.g., Zipf’s distribution), and can drastically increase the complexity of analysis. Given that this is our first foray into the topic, we opted for the simpler binary model. This is an important problem and we plan to explore it in more detail in future works.\n\nWe hope these responses have addressed some of the questions and we are open to further discussion. Thank you again for your constructive feedback!\n\n**Reference**\n\n[1] Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. Allen-Zhu et al.\n\n---\n\n> The theoretical understanding of feature learning is not as rudimentary as the authors claim\n\nWe sincerely apologize for the characterization. Perhaps “rudimentary” is not the best description. We have revised the phrase to be “still in early development” to better convey the sentiment. However, we do feel that it is reasonable to say that we still have much to understand about feature learning. \n\nTo briefly address the specific works you mentioned: these models generally assume the input data are isotropic Gaussian and the function class is a 2 layer MLP. The feature is usually a linear function of the input and often special training algorithms are needed (e.g., layer-wise training). A notable exception is [3] which learns non-linear features but still requires layer-wise training. \nMost of these works are quite recent (and all came out after we started this project): [3,4,5] showed up in October 2023 and [2] was updated in November 2023. In general, it is hard to say how well they encapsulate the full complexity of deep learning, or how feasible it is to apply them to modern architectures and datasets. Nonetheless, the goal of this work is not to compare existing theoretical works on the topic or discuss their merits and shortcomings, but rather to identify what realistic (perhaps strong) modeling assumptions to make when studying feature learning. Given the recently increasing interest in the subject, we view these works and our approach as complementary components of a broader exploration into feature learning. We have added a discussion of these works to Appendix D.1, but would be happy to revise them if you have further suggestions.\n\nWe hope these responses address your concerns and we are open to further discussion. Thank you again for your constructive feedback!\n\n**Reference**\n\n[6] Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. Allen-Zhu et al.\n\n---\n\nThank you for your detailed review and thoughtful feedback. We are glad that you find the problem we study timely and important. Below are our responses to your concerns:\n\n> I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of abstraction\". However, I think the model proposed in this paper is too simplistic…\n\nWe definitely agree that our model is unconventional and more abstract than most existing works but similar assumptions have been made in prior work [6]. As is the case for all theoretical models, simplifications are required to keep analysis tractable. Despite the simplification, our model already exhibits some interesting behaviors and offers insight into empirical phenomena so we believe that it adds value to the current discussion on feature learning. \n\nIn the case of random feature models, we feel that it is a little unfair to say in hindsight that there was a very obvious way to move towards making it more realistic. When NTK first came out in 2018, it was thought to be very promising. Indeed, despite its simplification, it offers clear value in the analysis of some phenomena in deep networks. However, it only later became clear that there was a big gap between NTK and real deep learning (despite many efforts to close this gap). More recent works (many of which you already brought up) rely on quite distinct analysis tools than those used in NTK and, still, it’s not clear if any of them is close to real-world practice.  As such, while we agree (and have acknowledged in Section 5, Appendix D.1 and D.4) that there is a gap between our proposed framework and practice, we feel it’s a tall order for the current state of deep learning theory to have no strong assumptions.  Just like NTK analysis, we hope that our approach will ultimately prove valuable (albeit along different dimensions), despite its simplicity.\n\n> Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\nThe primary goal of our work is to investigate the effect of different features being learned by different models on GDE and the calibration of deep ensembles, both of which are robust empirical observations. While our model does not directly provide insights into training, understanding these feature interactions may indirectly inform training strategies in the future. For example, [6] shows potential avenues for how observations like ours may transfer to training.\n\n> It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\nAs detailed in Appendix F.10 (referred to at the end of Section 4), our conclusions are robust across a range of parameter settings so we believe that the settings used in our paper should be appropriate for most situations.\n\n> The notations are a bit confusing ($i, j, k$, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations ($p_d, p_r$, etc.). It will also help explain the method.\n\nWe apologize for the confusion. There is already a centralized list of notations in Appendix A which we could not include in the main text due to space constraints (this is referred to in the second paragraph of section 5). Beyond this, could you be more specific about where the $i,j,k$ are causing confusion so we could better address the confusion? $i,j,k$ are used to denote an arbitrary member of the ordered collection so we feel this is in accordance with the standard usage.\n\n> But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\nWe want to clarify that we *do not* claim that the assumptions we make are weaker than calibration (since neither is a clear superset of the other), but we do believe that it sheds more light on the underlying reasons behind GDE. Only stating the model is calibrated does not tell us much about the model or the data, but our assumptions do. In fact, our framework tells us how one can break our assumption (and calibration) through data distribution interventions. Of course, it is an important open question why so many natural distributions seem to satisfy these assumptions.\n\n---\n\nThank you for the support of our paper! We are glad that you find our work valuable.\n\nRegarding Figure 1, to better address your concerns, could you elaborate on which aspects of the figure you found unclear? This will help us make more targeted improvements. \n\nThe goal of Figure 1 is to demonstrate that our definition of features can capture images with redundant information and images that contain uncommon instances of a given class. To make this clearer, we have updated the figure in the paper with texts to better reflect the core idea. Does this resolve your concerns? If not, we’d be happy to make more adjustments.\n\n---\n\nThank you for your supportive feedback on our work. We are glad that you find our new perspective on feature learning interesting. Below are our responses to your questions:\n\n> how important are these hyperparameters? How to properly set them?\n\nIn Appendix F.10, we conducted an ablation study and found that our qualitative conclusions are robust to variations in these hyperparameters. Therefore, we believe the values used in our paper should be appropriate for most applications.\n\n> Currently theoretical framework analyzes binary classification, does the analysis extend to multi-class classification?\n\nWe discuss the necessary steps for the extension to multi-class classification in Appendix D.3. \n\n>Does the framework allow for introducing distribution shifts and etc?\n\nThis is a great question. We believe our framework can accommodate distribution shifts. For instance, the simplest form of covariate shift one could introduce is to add a new set of features that are not present in the training data with some probability at test time. Since the model has not seen these new features during training, we may assume that individual members of the ensemble will make random predictions (but the prediction of different models will be correlated) if the data points only contain the new features (the same assumption for features that the models did not learn). One can also introduce a covariate shift on existing features but that would require imposing a non-uniform distribution over the existing features. We leave the exploration of these directions to future works.\n\nWe hope these answers clarify your question, and we are open to further discussion. Thank you again for your constructive feedback.\n\n---\n\nWe thank all the reviewers for their valuable time. We are glad that the reviewers found the results original, interesting, and timely.\n\nWe have modified Figure 1 and made minor revisions, and added some related works to the paper which are highlighted with orange in the text.\n\nWe will respond to each reviewer’s comments individually.", "author_response": "Thank you for your detailed review and thoughtful feedback. We are glad that you find the problem we study timely and important. Below are our responses to your concerns:\n\n> I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of abstraction\". However, I think the model proposed in this paper is too simplistic…\n\nWe definitely agree that our model is unconventional and more abstract than most existing works but similar assumptions have been made in prior work [6]. As is the case for all theoretical models, simplifications are required to keep analysis tractable. Despite the simplification, our model already exhibits some interesting behaviors and offers insight into empirical phenomena so we believe that it adds value to the current discussion on feature learning. \n\nIn the case of random feature models, we feel that it is a little unfair to say in hindsight that there was a very obvious way to move towards making it more realistic. When NTK first came out in 2018, it was thought to be very promising. Indeed, despite its simplification, it offers clear value in the analysis of some phenomena in deep networks. However, it only later became clear that there was a big gap between NTK and real deep learning (despite many efforts to close this gap). More recent works (many of which you already brought up) rely on quite distinct analysis tools than those used in NTK and, still, it’s not clear if any of them is close to real-world practice.  As such, while we agree (and have acknowledged in Section 5, Appendix D.1 and D.4) that there is a gap between our proposed framework and practice, we feel it’s a tall order for the current state of deep learning theory to have no strong assumptions.  Just like NTK analysis, we hope that our approach will ultimately prove valuable (albeit along different dimensions), despite its simplicity.\n\n> Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\nThe primary goal of our work is to investigate the effect of different features being learned by different models on GDE and the calibration of deep ensembles, both of which are robust empirical observations. While our model does not directly provide insights into training, understanding these feature interactions may indirectly inform training strategies in the future. For example, [6] shows potential avenues for how observations like ours may transfer to training.\n\n> It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\nAs detailed in Appendix F.10 (referred to at the end of Section 4), our conclusions are robust across a range of parameter settings so we believe that the settings used in our paper should be appropriate for most situations.\n\n> The notations are a bit confusing ($i, j, k$, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations ($p_d, p_r$, etc.). It will also help explain the method.\n\nWe apologize for the confusion. There is already a centralized list of notations in Appendix A which we could not include in the main text due to space constraints (this is referred to in the second paragraph of section 5). Beyond this, could you be more specific about where the $i,j,k$ are causing confusion so we could better address the confusion? $i,j,k$ are used to denote an arbitrary member of the ordered collection so we feel this is in accordance with the standard usage.\n\n> But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\nWe want to clarify that we *do not* claim that the assumptions we make are weaker than calibration (since neither is a clear superset of the other), but we do believe that it sheds more light on the underlying reasons behind GDE. Only stating the model is calibrated does not tell us much about the model or the data, but our assumptions do. In fact, our framework tells us how one can break our assumption (and calibration) through data distribution interventions. Of course, it is an important open question why so many natural distributions seem to satisfy these assumptions."}
{"claim": "The interaction-tensor-based model is not well motivated, and the paper does not explain how it relates to actual deep network training.", "claim_type": "subjective", "paper_id": "ze7DOLi394", "paper_title": "On the Joint Interaction of Models, Data, and Features", "paper_venue": "iclr2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "RUsLYJEhvF", "reviewer": "Reviewer_MLur", "review_text": "Summary: This paper introduces the interaction tensor, for empirically analyzing the interaction between data and model through features. Based on some observations using this tensor, they propose a very simple toy model (a combinatorial model) for feature learning. They show that this model also exhibits Generalization Disagreement Equality (GDE). Finally, the authors use their model to provide data distributions that break GDE in real world experiments.\n\nStrengths: - The problem of feature learning is quite important and there has been a lot of attempts for gaining a better theoretical understanding of it in recent years.\n\n- The authors are able to come up with a toy model that shows GDE which is a very important phenomenon (and it does need the explicit assumption of calibration).\n\n- The model that the authors propose is simple and it can be analyzed fully.\n\n- The paper is very well-written.\n\nWeaknesses: - I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of\nabstraction\". However, I think the model proposed in this paper is too simplistic. The implicit biases of deep learning are core to some of the merging phenomenon that we see these days and the models that the authors propose fails to capture that. I also think that a good toy model should leave the door open to generalizations and getting closer to real world practice (for example, for the random features model of deep learning, there is a very obvious way to move towards making it more realistic). But the models that the authors propose is too abstract and it is not clear what simplifications are made to the real problem to arrive at the proposed model.\n\n- Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\n- It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\n- The notations are a bit confusing (i, j, k, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations (p_d, p_r, etc.). It will also help explain the method.\n\n- The authors \"prove\" GDE in their model without the explicit assumption of calibration. But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\n- The theoretical understanding of feature learning is not as rudimentary as the authors claim. For example,\n\n[1] Alex Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent, 2022.\n\n[2] Zhichao Wang, Andrew Engel, Anand Sarwate, Ioana Dumitriu, and Tony Chiang. Spectral evolution and invariance in linear-width neural networks, 2022.\n\n[3] Eshaan Nichani, Alex Damian, and Jason D Lee. Provable guarantees for nonlinear feature learning in three-layer neural networks, 2023.\n\n\n[4] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. Learning two-layer neural networks, one (giant) step at a time, 2023.\n\n[5] Behrad Moniri, Donghwan Lee, Hamed Hassani, and Edgar Dobriban, A theory of non-linear feature learning with one gradient step in two-layer neural networks, 2023.\n\nand many more.\n\nQuestions: - Looking at figure 3 (a) and 3 (c), it seems that the observations made from them are not that significant. Am I missing something? How does the choice of the thresholds affect your observations?", "labeling_timestamp": "2026-01-11T16:29:51.866758", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors concede that the framework does not describe the learning/training process (agreeing with that part of the claim) but simultaneously defend the model's motivation and usefulness for explaining empirical phenomena and relate it to prior work, so they partially address the reviewer's concern rather than fully conceding or refuting it.", "evidence": "\"This is correct. The framework as it stands does not describe the learning process, which is an extremely complicated subject itself.\"\n\n\"The primary goal of our work is to investigate the effect of different features being learned by different models on GDE and the calibration of deep ensembles... While our model does not directly provide insights into training, understanding these feature interactions may indirectly inform training strategies in the future.\" \n\n\"We definitely agree that our model is unconventional and more abstract than most existing works but similar assumptions have been made in prior work [6]... Despite the simplification, our model already exhibits some interesting behaviors and offers insight into empirical phenomena so we believe that it adds value to the current discussion on feature learning.\"", "combined_author_response": "Thank you for the continued engagement and dedication to the review process! We address your remaining concerns as follows:\n\n> 0\n\nApologies for missing the first part of your question earlier. In 3 (a), the observation we are relying on is that data points with a small number of features are much more likely to have very high confidence under the ensemble and vice versa. For a concrete example, it is much more likely for a data point with 0.8 confidence to have 25 features than a data point with 0.4 confidence to have 25 features. As such, we believe that it’s fair to say that data with fewer features tend to have higher confidence. For 3(c), we claim that the more features the models share, the lower bound on their shared errors becomes higher, which can be clearly observed in the figure. This *does not* mean that the fewer features the models share, their shared errors would necessarily be smaller. Instead, both models would make random guesses (recall that we make similar assumptions in the conceptual model). This means that a pair of models can have a high or low amount of shared errors due to randomness, but the lower bound on the shared error will become higher if they share more features. We believe that the figure supports this claim.\n\n> 1\n\nThank you for clarifying what you meant about the limitations. We agree with your assessment of NTK and shallow neural networks, and we absolutely agree that we should be transparent with the limitations of the framework. Appendix D.3 (previously D.4) is meant to discuss the limitations of the model but we see that it may not be as clear as we hoped. As such, we have expanded Appendix D.3 to be more explicit about these limitations. This was already referred to at the beginning of section 5 where we introduced the model and in the conclusion, but once again, due to space limits, we could not include them in the main text.\n\n> 2\n\nThank you for clarifying this and pointing out the notation issues. We have streamlined the notations in Section 3 for better clarity so the footnotes are no longer needed.\n\nSpecifically, we have changed to use $C$ for the number of classes instead of $K$ and use $K$ to denote the number of top principal components, so all capital letters refer to a fixed number (e.g., $N$ for the total number of data, $M$ for the number of models, etc). Lower case of $N,M,K,T$ will refer to a particular index for what the object the capital letters refer to, so $k$ will always refer to the feature, $m$ will always refer to the model and $n$ will always refer to data, etc.\n\nOne exception is the k-partite graph. Since this is the standard reference to the problem we use $\\mathtt{k}$ to differentiate it.\nAnother one is that $i,j, a, b$ are used instead of $m$ and $k$ to refer to a pair of models and features around equation 2.\nWe believe that this should resolve the ambiguity but if there are any concerns regarding the notation left please let us know.\n\nRegarding the table of notation, we fully agree that a table would be great to have. Unfortunately, so far we have not been able to find a way to squeeze it into the text. And we will definitely include it in the extended version.\n\n------------\n\nWe hope these revisions and clarifications address your concerns. Please let us know if you have any further questions.\n\n---\n\nThank you for your thoughtful feedback! We are glad that you still find our work original and creative.\n\n> many assumptions went into the combinatorial analysis.\n\nWe fully agree that many assumptions were made. The way we see it is that deep learning's complexity necessitates some level of simplification for tractable analysis. In this case, since the resulting framework is still useful for understanding empirical phenomena, we feel the assumptions are justified. \n\n> The framework is less useful for understanding the learning process of networks…\n\nThis is correct. The framework as it stands does not describe the learning process, which is an extremely complicated subject itself. However, we postulate that techniques similar to [1] could be used to study the optimization of a feature model like ours in the future.\n\n> typos\n\nThank you for the close reading and for catching these typos! We have fixed them in the revision.\n\n> have you considered modeling the rare-dominant variation as a spectrum instead of a binary?\n\nThis is an excellent question! We have indeed considered the possibility of modeling it as a spectrum, which we discussed in Appendix D.1 and D.4. In our preliminary investigation, we found that modeling the feature spectrum may require introducing more free parameters (e.g., Zipf’s distribution), and can drastically increase the complexity of analysis. Given that this is our first foray into the topic, we opted for the simpler binary model. This is an important problem and we plan to explore it in more detail in future works.\n\nWe hope these responses have addressed some of the questions and we are open to further discussion. Thank you again for your constructive feedback!\n\n**Reference**\n\n[1] Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. Allen-Zhu et al.\n\n---\n\n> The theoretical understanding of feature learning is not as rudimentary as the authors claim\n\nWe sincerely apologize for the characterization. Perhaps “rudimentary” is not the best description. We have revised the phrase to be “still in early development” to better convey the sentiment. However, we do feel that it is reasonable to say that we still have much to understand about feature learning. \n\nTo briefly address the specific works you mentioned: these models generally assume the input data are isotropic Gaussian and the function class is a 2 layer MLP. The feature is usually a linear function of the input and often special training algorithms are needed (e.g., layer-wise training). A notable exception is [3] which learns non-linear features but still requires layer-wise training. \nMost of these works are quite recent (and all came out after we started this project): [3,4,5] showed up in October 2023 and [2] was updated in November 2023. In general, it is hard to say how well they encapsulate the full complexity of deep learning, or how feasible it is to apply them to modern architectures and datasets. Nonetheless, the goal of this work is not to compare existing theoretical works on the topic or discuss their merits and shortcomings, but rather to identify what realistic (perhaps strong) modeling assumptions to make when studying feature learning. Given the recently increasing interest in the subject, we view these works and our approach as complementary components of a broader exploration into feature learning. We have added a discussion of these works to Appendix D.1, but would be happy to revise them if you have further suggestions.\n\nWe hope these responses address your concerns and we are open to further discussion. Thank you again for your constructive feedback!\n\n**Reference**\n\n[6] Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. Allen-Zhu et al.\n\n---\n\nThank you for your detailed review and thoughtful feedback. We are glad that you find the problem we study timely and important. Below are our responses to your concerns:\n\n> I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of abstraction\". However, I think the model proposed in this paper is too simplistic…\n\nWe definitely agree that our model is unconventional and more abstract than most existing works but similar assumptions have been made in prior work [6]. As is the case for all theoretical models, simplifications are required to keep analysis tractable. Despite the simplification, our model already exhibits some interesting behaviors and offers insight into empirical phenomena so we believe that it adds value to the current discussion on feature learning. \n\nIn the case of random feature models, we feel that it is a little unfair to say in hindsight that there was a very obvious way to move towards making it more realistic. When NTK first came out in 2018, it was thought to be very promising. Indeed, despite its simplification, it offers clear value in the analysis of some phenomena in deep networks. However, it only later became clear that there was a big gap between NTK and real deep learning (despite many efforts to close this gap). More recent works (many of which you already brought up) rely on quite distinct analysis tools than those used in NTK and, still, it’s not clear if any of them is close to real-world practice.  As such, while we agree (and have acknowledged in Section 5, Appendix D.1 and D.4) that there is a gap between our proposed framework and practice, we feel it’s a tall order for the current state of deep learning theory to have no strong assumptions.  Just like NTK analysis, we hope that our approach will ultimately prove valuable (albeit along different dimensions), despite its simplicity.\n\n> Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\nThe primary goal of our work is to investigate the effect of different features being learned by different models on GDE and the calibration of deep ensembles, both of which are robust empirical observations. While our model does not directly provide insights into training, understanding these feature interactions may indirectly inform training strategies in the future. For example, [6] shows potential avenues for how observations like ours may transfer to training.\n\n> It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\nAs detailed in Appendix F.10 (referred to at the end of Section 4), our conclusions are robust across a range of parameter settings so we believe that the settings used in our paper should be appropriate for most situations.\n\n> The notations are a bit confusing ($i, j, k$, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations ($p_d, p_r$, etc.). It will also help explain the method.\n\nWe apologize for the confusion. There is already a centralized list of notations in Appendix A which we could not include in the main text due to space constraints (this is referred to in the second paragraph of section 5). Beyond this, could you be more specific about where the $i,j,k$ are causing confusion so we could better address the confusion? $i,j,k$ are used to denote an arbitrary member of the ordered collection so we feel this is in accordance with the standard usage.\n\n> But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\nWe want to clarify that we *do not* claim that the assumptions we make are weaker than calibration (since neither is a clear superset of the other), but we do believe that it sheds more light on the underlying reasons behind GDE. Only stating the model is calibrated does not tell us much about the model or the data, but our assumptions do. In fact, our framework tells us how one can break our assumption (and calibration) through data distribution interventions. Of course, it is an important open question why so many natural distributions seem to satisfy these assumptions.\n\n---\n\nThank you for the support of our paper! We are glad that you find our work valuable.\n\nRegarding Figure 1, to better address your concerns, could you elaborate on which aspects of the figure you found unclear? This will help us make more targeted improvements. \n\nThe goal of Figure 1 is to demonstrate that our definition of features can capture images with redundant information and images that contain uncommon instances of a given class. To make this clearer, we have updated the figure in the paper with texts to better reflect the core idea. Does this resolve your concerns? If not, we’d be happy to make more adjustments.\n\n---\n\nThank you for your supportive feedback on our work. We are glad that you find our new perspective on feature learning interesting. Below are our responses to your questions:\n\n> how important are these hyperparameters? How to properly set them?\n\nIn Appendix F.10, we conducted an ablation study and found that our qualitative conclusions are robust to variations in these hyperparameters. Therefore, we believe the values used in our paper should be appropriate for most applications.\n\n> Currently theoretical framework analyzes binary classification, does the analysis extend to multi-class classification?\n\nWe discuss the necessary steps for the extension to multi-class classification in Appendix D.3. \n\n>Does the framework allow for introducing distribution shifts and etc?\n\nThis is a great question. We believe our framework can accommodate distribution shifts. For instance, the simplest form of covariate shift one could introduce is to add a new set of features that are not present in the training data with some probability at test time. Since the model has not seen these new features during training, we may assume that individual members of the ensemble will make random predictions (but the prediction of different models will be correlated) if the data points only contain the new features (the same assumption for features that the models did not learn). One can also introduce a covariate shift on existing features but that would require imposing a non-uniform distribution over the existing features. We leave the exploration of these directions to future works.\n\nWe hope these answers clarify your question, and we are open to further discussion. Thank you again for your constructive feedback.\n\n---\n\nWe thank all the reviewers for their valuable time. We are glad that the reviewers found the results original, interesting, and timely.\n\nWe have modified Figure 1 and made minor revisions, and added some related works to the paper which are highlighted with orange in the text.\n\nWe will respond to each reviewer’s comments individually.", "author_response": "Thank you for your detailed review and thoughtful feedback. We are glad that you find the problem we study timely and important. Below are our responses to your concerns:\n\n> I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of abstraction\". However, I think the model proposed in this paper is too simplistic…\n\nWe definitely agree that our model is unconventional and more abstract than most existing works but similar assumptions have been made in prior work [6]. As is the case for all theoretical models, simplifications are required to keep analysis tractable. Despite the simplification, our model already exhibits some interesting behaviors and offers insight into empirical phenomena so we believe that it adds value to the current discussion on feature learning. \n\nIn the case of random feature models, we feel that it is a little unfair to say in hindsight that there was a very obvious way to move towards making it more realistic. When NTK first came out in 2018, it was thought to be very promising. Indeed, despite its simplification, it offers clear value in the analysis of some phenomena in deep networks. However, it only later became clear that there was a big gap between NTK and real deep learning (despite many efforts to close this gap). More recent works (many of which you already brought up) rely on quite distinct analysis tools than those used in NTK and, still, it’s not clear if any of them is close to real-world practice.  As such, while we agree (and have acknowledged in Section 5, Appendix D.1 and D.4) that there is a gap between our proposed framework and practice, we feel it’s a tall order for the current state of deep learning theory to have no strong assumptions.  Just like NTK analysis, we hope that our approach will ultimately prove valuable (albeit along different dimensions), despite its simplicity.\n\n> Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\nThe primary goal of our work is to investigate the effect of different features being learned by different models on GDE and the calibration of deep ensembles, both of which are robust empirical observations. While our model does not directly provide insights into training, understanding these feature interactions may indirectly inform training strategies in the future. For example, [6] shows potential avenues for how observations like ours may transfer to training.\n\n> It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\nAs detailed in Appendix F.10 (referred to at the end of Section 4), our conclusions are robust across a range of parameter settings so we believe that the settings used in our paper should be appropriate for most situations.\n\n> The notations are a bit confusing ($i, j, k$, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations ($p_d, p_r$, etc.). It will also help explain the method.\n\nWe apologize for the confusion. There is already a centralized list of notations in Appendix A which we could not include in the main text due to space constraints (this is referred to in the second paragraph of section 5). Beyond this, could you be more specific about where the $i,j,k$ are causing confusion so we could better address the confusion? $i,j,k$ are used to denote an arbitrary member of the ordered collection so we feel this is in accordance with the standard usage.\n\n> But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\nWe want to clarify that we *do not* claim that the assumptions we make are weaker than calibration (since neither is a clear superset of the other), but we do believe that it sheds more light on the underlying reasons behind GDE. Only stating the model is calibrated does not tell us much about the model or the data, but our assumptions do. In fact, our framework tells us how one can break our assumption (and calibration) through data distribution interventions. Of course, it is an important open question why so many natural distributions seem to satisfy these assumptions."}
{"claim": "The authors do not clearly specify how hyperparameters, such as thresholds, were set in their model or experiments.", "claim_type": "methodology", "paper_id": "ze7DOLi394", "paper_title": "On the Joint Interaction of Models, Data, and Features", "paper_venue": "iclr2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "RUsLYJEhvF", "reviewer": "Reviewer_MLur", "review_text": "Summary: This paper introduces the interaction tensor, for empirically analyzing the interaction between data and model through features. Based on some observations using this tensor, they propose a very simple toy model (a combinatorial model) for feature learning. They show that this model also exhibits Generalization Disagreement Equality (GDE). Finally, the authors use their model to provide data distributions that break GDE in real world experiments.\n\nStrengths: - The problem of feature learning is quite important and there has been a lot of attempts for gaining a better theoretical understanding of it in recent years.\n\n- The authors are able to come up with a toy model that shows GDE which is a very important phenomenon (and it does need the explicit assumption of calibration).\n\n- The model that the authors propose is simple and it can be analyzed fully.\n\n- The paper is very well-written.\n\nWeaknesses: - I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of\nabstraction\". However, I think the model proposed in this paper is too simplistic. The implicit biases of deep learning are core to some of the merging phenomenon that we see these days and the models that the authors propose fails to capture that. I also think that a good toy model should leave the door open to generalizations and getting closer to real world practice (for example, for the random features model of deep learning, there is a very obvious way to move towards making it more realistic). But the models that the authors propose is too abstract and it is not clear what simplifications are made to the real problem to arrive at the proposed model.\n\n- Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\n- It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\n- The notations are a bit confusing (i, j, k, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations (p_d, p_r, etc.). It will also help explain the method.\n\n- The authors \"prove\" GDE in their model without the explicit assumption of calibration. But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\n- The theoretical understanding of feature learning is not as rudimentary as the authors claim. For example,\n\n[1] Alex Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent, 2022.\n\n[2] Zhichao Wang, Andrew Engel, Anand Sarwate, Ioana Dumitriu, and Tony Chiang. Spectral evolution and invariance in linear-width neural networks, 2022.\n\n[3] Eshaan Nichani, Alex Damian, and Jason D Lee. Provable guarantees for nonlinear feature learning in three-layer neural networks, 2023.\n\n\n[4] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. Learning two-layer neural networks, one (giant) step at a time, 2023.\n\n[5] Behrad Moniri, Donghwan Lee, Hamed Hassani, and Edgar Dobriban, A theory of non-linear feature learning with one gradient step in two-layer neural networks, 2023.\n\nand many more.\n\nQuestions: - Looking at figure 3 (a) and 3 (c), it seems that the observations made from them are not that significant. Am I missing something? How does the choice of the thresholds affect your observations?", "labeling_timestamp": "2026-01-11T16:30:13.166280", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors point the reviewer to Appendix F.10 and state they performed an ablation study showing conclusions are robust across a range of hyperparameter settings, thereby addressing how hyperparameters (e.g., thresholds) were set and their impact.", "evidence": "\"As detailed in Appendix F.10 (referred at the end of Section 4), our conclusions are robust across a range of parameter settings so we believe that the settings used in our paper should be appropriate for most situations.\" / \"In Appendix F.10, we conducted an ablation study and found that our qualitative conclusions are robust to variations in these hyperparameters. Therefore, we believe the values used in our paper should be appropriate for most applications.\"", "combined_author_response": "Thank you for the continued engagement and dedication to the review process! We address your remaining concerns as follows:\n\n> 0\n\nApologies for missing the first part of your question earlier. In 3 (a), the observation we are relying on is that data points with a small number of features are much more likely to have very high confidence under the ensemble and vice versa. For a concrete example, it is much more likely for a data point with 0.8 confidence to have 25 features than a data point with 0.4 confidence to have 25 features. As such, we believe that it’s fair to say that data with fewer features tend to have higher confidence. For 3(c), we claim that the more features the models share, the lower bound on their shared errors becomes higher, which can be clearly observed in the figure. This *does not* mean that the fewer features the models share, their shared errors would necessarily be smaller. Instead, both models would make random guesses (recall that we make similar assumptions in the conceptual model). This means that a pair of models can have a high or low amount of shared errors due to randomness, but the lower bound on the shared error will become higher if they share more features. We believe that the figure supports this claim.\n\n> 1\n\nThank you for clarifying what you meant about the limitations. We agree with your assessment of NTK and shallow neural networks, and we absolutely agree that we should be transparent with the limitations of the framework. Appendix D.3 (previously D.4) is meant to discuss the limitations of the model but we see that it may not be as clear as we hoped. As such, we have expanded Appendix D.3 to be more explicit about these limitations. This was already referred to at the beginning of section 5 where we introduced the model and in the conclusion, but once again, due to space limits, we could not include them in the main text.\n\n> 2\n\nThank you for clarifying this and pointing out the notation issues. We have streamlined the notations in Section 3 for better clarity so the footnotes are no longer needed.\n\nSpecifically, we have changed to use $C$ for the number of classes instead of $K$ and use $K$ to denote the number of top principal components, so all capital letters refer to a fixed number (e.g., $N$ for the total number of data, $M$ for the number of models, etc). Lower case of $N,M,K,T$ will refer to a particular index for what the object the capital letters refer to, so $k$ will always refer to the feature, $m$ will always refer to the model and $n$ will always refer to data, etc.\n\nOne exception is the k-partite graph. Since this is the standard reference to the problem we use $\\mathtt{k}$ to differentiate it.\nAnother one is that $i,j, a, b$ are used instead of $m$ and $k$ to refer to a pair of models and features around equation 2.\nWe believe that this should resolve the ambiguity but if there are any concerns regarding the notation left please let us know.\n\nRegarding the table of notation, we fully agree that a table would be great to have. Unfortunately, so far we have not been able to find a way to squeeze it into the text. And we will definitely include it in the extended version.\n\n------------\n\nWe hope these revisions and clarifications address your concerns. Please let us know if you have any further questions.\n\n---\n\nThank you for your thoughtful feedback! We are glad that you still find our work original and creative.\n\n> many assumptions went into the combinatorial analysis.\n\nWe fully agree that many assumptions were made. The way we see it is that deep learning's complexity necessitates some level of simplification for tractable analysis. In this case, since the resulting framework is still useful for understanding empirical phenomena, we feel the assumptions are justified. \n\n> The framework is less useful for understanding the learning process of networks…\n\nThis is correct. The framework as it stands does not describe the learning process, which is an extremely complicated subject itself. However, we postulate that techniques similar to [1] could be used to study the optimization of a feature model like ours in the future.\n\n> typos\n\nThank you for the close reading and for catching these typos! We have fixed them in the revision.\n\n> have you considered modeling the rare-dominant variation as a spectrum instead of a binary?\n\nThis is an excellent question! We have indeed considered the possibility of modeling it as a spectrum, which we discussed in Appendix D.1 and D.4. In our preliminary investigation, we found that modeling the feature spectrum may require introducing more free parameters (e.g., Zipf’s distribution), and can drastically increase the complexity of analysis. Given that this is our first foray into the topic, we opted for the simpler binary model. This is an important problem and we plan to explore it in more detail in future works.\n\nWe hope these responses have addressed some of the questions and we are open to further discussion. Thank you again for your constructive feedback!\n\n**Reference**\n\n[1] Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. Allen-Zhu et al.\n\n---\n\n> The theoretical understanding of feature learning is not as rudimentary as the authors claim\n\nWe sincerely apologize for the characterization. Perhaps “rudimentary” is not the best description. We have revised the phrase to be “still in early development” to better convey the sentiment. However, we do feel that it is reasonable to say that we still have much to understand about feature learning. \n\nTo briefly address the specific works you mentioned: these models generally assume the input data are isotropic Gaussian and the function class is a 2 layer MLP. The feature is usually a linear function of the input and often special training algorithms are needed (e.g., layer-wise training). A notable exception is [3] which learns non-linear features but still requires layer-wise training. \nMost of these works are quite recent (and all came out after we started this project): [3,4,5] showed up in October 2023 and [2] was updated in November 2023. In general, it is hard to say how well they encapsulate the full complexity of deep learning, or how feasible it is to apply them to modern architectures and datasets. Nonetheless, the goal of this work is not to compare existing theoretical works on the topic or discuss their merits and shortcomings, but rather to identify what realistic (perhaps strong) modeling assumptions to make when studying feature learning. Given the recently increasing interest in the subject, we view these works and our approach as complementary components of a broader exploration into feature learning. We have added a discussion of these works to Appendix D.1, but would be happy to revise them if you have further suggestions.\n\nWe hope these responses address your concerns and we are open to further discussion. Thank you again for your constructive feedback!\n\n**Reference**\n\n[6] Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. Allen-Zhu et al.\n\n---\n\nThank you for your detailed review and thoughtful feedback. We are glad that you find the problem we study timely and important. Below are our responses to your concerns:\n\n> I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of abstraction\". However, I think the model proposed in this paper is too simplistic…\n\nWe definitely agree that our model is unconventional and more abstract than most existing works but similar assumptions have been made in prior work [6]. As is the case for all theoretical models, simplifications are required to keep analysis tractable. Despite the simplification, our model already exhibits some interesting behaviors and offers insight into empirical phenomena so we believe that it adds value to the current discussion on feature learning. \n\nIn the case of random feature models, we feel that it is a little unfair to say in hindsight that there was a very obvious way to move towards making it more realistic. When NTK first came out in 2018, it was thought to be very promising. Indeed, despite its simplification, it offers clear value in the analysis of some phenomena in deep networks. However, it only later became clear that there was a big gap between NTK and real deep learning (despite many efforts to close this gap). More recent works (many of which you already brought up) rely on quite distinct analysis tools than those used in NTK and, still, it’s not clear if any of them is close to real-world practice.  As such, while we agree (and have acknowledged in Section 5, Appendix D.1 and D.4) that there is a gap between our proposed framework and practice, we feel it’s a tall order for the current state of deep learning theory to have no strong assumptions.  Just like NTK analysis, we hope that our approach will ultimately prove valuable (albeit along different dimensions), despite its simplicity.\n\n> Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\nThe primary goal of our work is to investigate the effect of different features being learned by different models on GDE and the calibration of deep ensembles, both of which are robust empirical observations. While our model does not directly provide insights into training, understanding these feature interactions may indirectly inform training strategies in the future. For example, [6] shows potential avenues for how observations like ours may transfer to training.\n\n> It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\nAs detailed in Appendix F.10 (referred to at the end of Section 4), our conclusions are robust across a range of parameter settings so we believe that the settings used in our paper should be appropriate for most situations.\n\n> The notations are a bit confusing ($i, j, k$, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations ($p_d, p_r$, etc.). It will also help explain the method.\n\nWe apologize for the confusion. There is already a centralized list of notations in Appendix A which we could not include in the main text due to space constraints (this is referred to in the second paragraph of section 5). Beyond this, could you be more specific about where the $i,j,k$ are causing confusion so we could better address the confusion? $i,j,k$ are used to denote an arbitrary member of the ordered collection so we feel this is in accordance with the standard usage.\n\n> But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\nWe want to clarify that we *do not* claim that the assumptions we make are weaker than calibration (since neither is a clear superset of the other), but we do believe that it sheds more light on the underlying reasons behind GDE. Only stating the model is calibrated does not tell us much about the model or the data, but our assumptions do. In fact, our framework tells us how one can break our assumption (and calibration) through data distribution interventions. Of course, it is an important open question why so many natural distributions seem to satisfy these assumptions.\n\n---\n\nThank you for the support of our paper! We are glad that you find our work valuable.\n\nRegarding Figure 1, to better address your concerns, could you elaborate on which aspects of the figure you found unclear? This will help us make more targeted improvements. \n\nThe goal of Figure 1 is to demonstrate that our definition of features can capture images with redundant information and images that contain uncommon instances of a given class. To make this clearer, we have updated the figure in the paper with texts to better reflect the core idea. Does this resolve your concerns? If not, we’d be happy to make more adjustments.\n\n---\n\nThank you for your supportive feedback on our work. We are glad that you find our new perspective on feature learning interesting. Below are our responses to your questions:\n\n> how important are these hyperparameters? How to properly set them?\n\nIn Appendix F.10, we conducted an ablation study and found that our qualitative conclusions are robust to variations in these hyperparameters. Therefore, we believe the values used in our paper should be appropriate for most applications.\n\n> Currently theoretical framework analyzes binary classification, does the analysis extend to multi-class classification?\n\nWe discuss the necessary steps for the extension to multi-class classification in Appendix D.3. \n\n>Does the framework allow for introducing distribution shifts and etc?\n\nThis is a great question. We believe our framework can accommodate distribution shifts. For instance, the simplest form of covariate shift one could introduce is to add a new set of features that are not present in the training data with some probability at test time. Since the model has not seen these new features during training, we may assume that individual members of the ensemble will make random predictions (but the prediction of different models will be correlated) if the data points only contain the new features (the same assumption for features that the models did not learn). One can also introduce a covariate shift on existing features but that would require imposing a non-uniform distribution over the existing features. We leave the exploration of these directions to future works.\n\nWe hope these answers clarify your question, and we are open to further discussion. Thank you again for your constructive feedback.\n\n---\n\nWe thank all the reviewers for their valuable time. We are glad that the reviewers found the results original, interesting, and timely.\n\nWe have modified Figure 1 and made minor revisions, and added some related works to the paper which are highlighted with orange in the text.\n\nWe will respond to each reviewer’s comments individually.", "author_response": "Thank you for your detailed review and thoughtful feedback. We are glad that you find the problem we study timely and important. Below are our responses to your concerns:\n\n> I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of abstraction\". However, I think the model proposed in this paper is too simplistic…\n\nWe definitely agree that our model is unconventional and more abstract than most existing works but similar assumptions have been made in prior work [6]. As is the case for all theoretical models, simplifications are required to keep analysis tractable. Despite the simplification, our model already exhibits some interesting behaviors and offers insight into empirical phenomena so we believe that it adds value to the current discussion on feature learning. \n\nIn the case of random feature models, we feel that it is a little unfair to say in hindsight that there was a very obvious way to move towards making it more realistic. When NTK first came out in 2018, it was thought to be very promising. Indeed, despite its simplification, it offers clear value in the analysis of some phenomena in deep networks. However, it only later became clear that there was a big gap between NTK and real deep learning (despite many efforts to close this gap). More recent works (many of which you already brought up) rely on quite distinct analysis tools than those used in NTK and, still, it’s not clear if any of them is close to real-world practice.  As such, while we agree (and have acknowledged in Section 5, Appendix D.1 and D.4) that there is a gap between our proposed framework and practice, we feel it’s a tall order for the current state of deep learning theory to have no strong assumptions.  Just like NTK analysis, we hope that our approach will ultimately prove valuable (albeit along different dimensions), despite its simplicity.\n\n> Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\nThe primary goal of our work is to investigate the effect of different features being learned by different models on GDE and the calibration of deep ensembles, both of which are robust empirical observations. While our model does not directly provide insights into training, understanding these feature interactions may indirectly inform training strategies in the future. For example, [6] shows potential avenues for how observations like ours may transfer to training.\n\n> It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\nAs detailed in Appendix F.10 (referred to at the end of Section 4), our conclusions are robust across a range of parameter settings so we believe that the settings used in our paper should be appropriate for most situations.\n\n> The notations are a bit confusing ($i, j, k$, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations ($p_d, p_r$, etc.). It will also help explain the method.\n\nWe apologize for the confusion. There is already a centralized list of notations in Appendix A which we could not include in the main text due to space constraints (this is referred to in the second paragraph of section 5). Beyond this, could you be more specific about where the $i,j,k$ are causing confusion so we could better address the confusion? $i,j,k$ are used to denote an arbitrary member of the ordered collection so we feel this is in accordance with the standard usage.\n\n> But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\nWe want to clarify that we *do not* claim that the assumptions we make are weaker than calibration (since neither is a clear superset of the other), but we do believe that it sheds more light on the underlying reasons behind GDE. Only stating the model is calibrated does not tell us much about the model or the data, but our assumptions do. In fact, our framework tells us how one can break our assumption (and calibration) through data distribution interventions. Of course, it is an important open question why so many natural distributions seem to satisfy these assumptions."}
{"claim": "It is unclear how the choice of thresholds affects the reported observations and results.", "claim_type": "methodology", "paper_id": "ze7DOLi394", "paper_title": "On the Joint Interaction of Models, Data, and Features", "paper_venue": "iclr2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "RUsLYJEhvF", "reviewer": "Reviewer_MLur", "review_text": "Summary: This paper introduces the interaction tensor, for empirically analyzing the interaction between data and model through features. Based on some observations using this tensor, they propose a very simple toy model (a combinatorial model) for feature learning. They show that this model also exhibits Generalization Disagreement Equality (GDE). Finally, the authors use their model to provide data distributions that break GDE in real world experiments.\n\nStrengths: - The problem of feature learning is quite important and there has been a lot of attempts for gaining a better theoretical understanding of it in recent years.\n\n- The authors are able to come up with a toy model that shows GDE which is a very important phenomenon (and it does need the explicit assumption of calibration).\n\n- The model that the authors propose is simple and it can be analyzed fully.\n\n- The paper is very well-written.\n\nWeaknesses: - I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of\nabstraction\". However, I think the model proposed in this paper is too simplistic. The implicit biases of deep learning are core to some of the merging phenomenon that we see these days and the models that the authors propose fails to capture that. I also think that a good toy model should leave the door open to generalizations and getting closer to real world practice (for example, for the random features model of deep learning, there is a very obvious way to move towards making it more realistic). But the models that the authors propose is too abstract and it is not clear what simplifications are made to the real problem to arrive at the proposed model.\n\n- Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\n- It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\n- The notations are a bit confusing (i, j, k, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations (p_d, p_r, etc.). It will also help explain the method.\n\n- The authors \"prove\" GDE in their model without the explicit assumption of calibration. But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\n- The theoretical understanding of feature learning is not as rudimentary as the authors claim. For example,\n\n[1] Alex Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent, 2022.\n\n[2] Zhichao Wang, Andrew Engel, Anand Sarwate, Ioana Dumitriu, and Tony Chiang. Spectral evolution and invariance in linear-width neural networks, 2022.\n\n[3] Eshaan Nichani, Alex Damian, and Jason D Lee. Provable guarantees for nonlinear feature learning in three-layer neural networks, 2023.\n\n\n[4] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. Learning two-layer neural networks, one (giant) step at a time, 2023.\n\n[5] Behrad Moniri, Donghwan Lee, Hamed Hassani, and Edgar Dobriban, A theory of non-linear feature learning with one gradient step in two-layer neural networks, 2023.\n\nand many more.\n\nQuestions: - Looking at figure 3 (a) and 3 (c), it seems that the observations made from them are not that significant. Am I missing something? How does the choice of the thresholds affect your observations?", "labeling_timestamp": "2026-01-11T16:30:00.795233", "model": "gpt-5-mini", "label": "Supported", "justification": "Authors directly address the concern by pointing to an ablation study (Appendix F.10) and stating that conclusions are robust across a range of threshold/hyperparameter settings, thereby clarifying how thresholds affect results.", "evidence": "\"As detailed in Appendix F.10 (referred to at the end of Section 4), our conclusions are robust across a range of parameter settings so we believe that the settings used in our paper should be appropriate for most situations.\" / \"In Appendix F.10, we conducted an ablation study and found that our qualitative conclusions are robust to variations in these hyperparameters.\"", "combined_author_response": "Thank you for the continued engagement and dedication to the review process! We address your remaining concerns as follows:\n\n> 0\n\nApologies for missing the first part of your question earlier. In 3 (a), the observation we are relying on is that data points with a small number of features are much more likely to have very high confidence under the ensemble and vice versa. For a concrete example, it is much more likely for a data point with 0.8 confidence to have 25 features than a data point with 0.4 confidence to have 25 features. As such, we believe that it’s fair to say that data with fewer features tend to have higher confidence. For 3(c), we claim that the more features the models share, the lower bound on their shared errors becomes higher, which can be clearly observed in the figure. This *does not* mean that the fewer features the models share, their shared errors would necessarily be smaller. Instead, both models would make random guesses (recall that we make similar assumptions in the conceptual model). This means that a pair of models can have a high or low amount of shared errors due to randomness, but the lower bound on the shared error will become higher if they share more features. We believe that the figure supports this claim.\n\n> 1\n\nThank you for clarifying what you meant about the limitations. We agree with your assessment of NTK and shallow neural networks, and we absolutely agree that we should be transparent with the limitations of the framework. Appendix D.3 (previously D.4) is meant to discuss the limitations of the model but we see that it may not be as clear as we hoped. As such, we have expanded Appendix D.3 to be more explicit about these limitations. This was already referred to at the beginning of section 5 where we introduced the model and in the conclusion, but once again, due to space limits, we could not include them in the main text.\n\n> 2\n\nThank you for clarifying this and pointing out the notation issues. We have streamlined the notations in Section 3 for better clarity so the footnotes are no longer needed.\n\nSpecifically, we have changed to use $C$ for the number of classes instead of $K$ and use $K$ to denote the number of top principal components, so all capital letters refer to a fixed number (e.g., $N$ for the total number of data, $M$ for the number of models, etc). Lower case of $N,M,K,T$ will refer to a particular index for what the object the capital letters refer to, so $k$ will always refer to the feature, $m$ will always refer to the model and $n$ will always refer to data, etc.\n\nOne exception is the k-partite graph. Since this is the standard reference to the problem we use $\\mathtt{k}$ to differentiate it.\nAnother one is that $i,j, a, b$ are used instead of $m$ and $k$ to refer to a pair of models and features around equation 2.\nWe believe that this should resolve the ambiguity but if there are any concerns regarding the notation left please let us know.\n\nRegarding the table of notation, we fully agree that a table would be great to have. Unfortunately, so far we have not been able to find a way to squeeze it into the text. And we will definitely include it in the extended version.\n\n------------\n\nWe hope these revisions and clarifications address your concerns. Please let us know if you have any further questions.\n\n---\n\nThank you for your thoughtful feedback! We are glad that you still find our work original and creative.\n\n> many assumptions went into the combinatorial analysis.\n\nWe fully agree that many assumptions were made. The way we see it is that deep learning's complexity necessitates some level of simplification for tractable analysis. In this case, since the resulting framework is still useful for understanding empirical phenomena, we feel the assumptions are justified. \n\n> The framework is less useful for understanding the learning process of networks…\n\nThis is correct. The framework as it stands does not describe the learning process, which is an extremely complicated subject itself. However, we postulate that techniques similar to [1] could be used to study the optimization of a feature model like ours in the future.\n\n> typos\n\nThank you for the close reading and for catching these typos! We have fixed them in the revision.\n\n> have you considered modeling the rare-dominant variation as a spectrum instead of a binary?\n\nThis is an excellent question! We have indeed considered the possibility of modeling it as a spectrum, which we discussed in Appendix D.1 and D.4. In our preliminary investigation, we found that modeling the feature spectrum may require introducing more free parameters (e.g., Zipf’s distribution), and can drastically increase the complexity of analysis. Given that this is our first foray into the topic, we opted for the simpler binary model. This is an important problem and we plan to explore it in more detail in future works.\n\nWe hope these responses have addressed some of the questions and we are open to further discussion. Thank you again for your constructive feedback!\n\n**Reference**\n\n[1] Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. Allen-Zhu et al.\n\n---\n\n> The theoretical understanding of feature learning is not as rudimentary as the authors claim\n\nWe sincerely apologize for the characterization. Perhaps “rudimentary” is not the best description. We have revised the phrase to be “still in early development” to better convey the sentiment. However, we do feel that it is reasonable to say that we still have much to understand about feature learning. \n\nTo briefly address the specific works you mentioned: these models generally assume the input data are isotropic Gaussian and the function class is a 2 layer MLP. The feature is usually a linear function of the input and often special training algorithms are needed (e.g., layer-wise training). A notable exception is [3] which learns non-linear features but still requires layer-wise training. \nMost of these works are quite recent (and all came out after we started this project): [3,4,5] showed up in October 2023 and [2] was updated in November 2023. In general, it is hard to say how well they encapsulate the full complexity of deep learning, or how feasible it is to apply them to modern architectures and datasets. Nonetheless, the goal of this work is not to compare existing theoretical works on the topic or discuss their merits and shortcomings, but rather to identify what realistic (perhaps strong) modeling assumptions to make when studying feature learning. Given the recently increasing interest in the subject, we view these works and our approach as complementary components of a broader exploration into feature learning. We have added a discussion of these works to Appendix D.1, but would be happy to revise them if you have further suggestions.\n\nWe hope these responses address your concerns and we are open to further discussion. Thank you again for your constructive feedback!\n\n**Reference**\n\n[6] Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. Allen-Zhu et al.\n\n---\n\nThank you for your detailed review and thoughtful feedback. We are glad that you find the problem we study timely and important. Below are our responses to your concerns:\n\n> I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of abstraction\". However, I think the model proposed in this paper is too simplistic…\n\nWe definitely agree that our model is unconventional and more abstract than most existing works but similar assumptions have been made in prior work [6]. As is the case for all theoretical models, simplifications are required to keep analysis tractable. Despite the simplification, our model already exhibits some interesting behaviors and offers insight into empirical phenomena so we believe that it adds value to the current discussion on feature learning. \n\nIn the case of random feature models, we feel that it is a little unfair to say in hindsight that there was a very obvious way to move towards making it more realistic. When NTK first came out in 2018, it was thought to be very promising. Indeed, despite its simplification, it offers clear value in the analysis of some phenomena in deep networks. However, it only later became clear that there was a big gap between NTK and real deep learning (despite many efforts to close this gap). More recent works (many of which you already brought up) rely on quite distinct analysis tools than those used in NTK and, still, it’s not clear if any of them is close to real-world practice.  As such, while we agree (and have acknowledged in Section 5, Appendix D.1 and D.4) that there is a gap between our proposed framework and practice, we feel it’s a tall order for the current state of deep learning theory to have no strong assumptions.  Just like NTK analysis, we hope that our approach will ultimately prove valuable (albeit along different dimensions), despite its simplicity.\n\n> Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\nThe primary goal of our work is to investigate the effect of different features being learned by different models on GDE and the calibration of deep ensembles, both of which are robust empirical observations. While our model does not directly provide insights into training, understanding these feature interactions may indirectly inform training strategies in the future. For example, [6] shows potential avenues for how observations like ours may transfer to training.\n\n> It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\nAs detailed in Appendix F.10 (referred to at the end of Section 4), our conclusions are robust across a range of parameter settings so we believe that the settings used in our paper should be appropriate for most situations.\n\n> The notations are a bit confusing ($i, j, k$, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations ($p_d, p_r$, etc.). It will also help explain the method.\n\nWe apologize for the confusion. There is already a centralized list of notations in Appendix A which we could not include in the main text due to space constraints (this is referred to in the second paragraph of section 5). Beyond this, could you be more specific about where the $i,j,k$ are causing confusion so we could better address the confusion? $i,j,k$ are used to denote an arbitrary member of the ordered collection so we feel this is in accordance with the standard usage.\n\n> But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\nWe want to clarify that we *do not* claim that the assumptions we make are weaker than calibration (since neither is a clear superset of the other), but we do believe that it sheds more light on the underlying reasons behind GDE. Only stating the model is calibrated does not tell us much about the model or the data, but our assumptions do. In fact, our framework tells us how one can break our assumption (and calibration) through data distribution interventions. Of course, it is an important open question why so many natural distributions seem to satisfy these assumptions.\n\n---\n\nThank you for the support of our paper! We are glad that you find our work valuable.\n\nRegarding Figure 1, to better address your concerns, could you elaborate on which aspects of the figure you found unclear? This will help us make more targeted improvements. \n\nThe goal of Figure 1 is to demonstrate that our definition of features can capture images with redundant information and images that contain uncommon instances of a given class. To make this clearer, we have updated the figure in the paper with texts to better reflect the core idea. Does this resolve your concerns? If not, we’d be happy to make more adjustments.\n\n---\n\nThank you for your supportive feedback on our work. We are glad that you find our new perspective on feature learning interesting. Below are our responses to your questions:\n\n> how important are these hyperparameters? How to properly set them?\n\nIn Appendix F.10, we conducted an ablation study and found that our qualitative conclusions are robust to variations in these hyperparameters. Therefore, we believe the values used in our paper should be appropriate for most applications.\n\n> Currently theoretical framework analyzes binary classification, does the analysis extend to multi-class classification?\n\nWe discuss the necessary steps for the extension to multi-class classification in Appendix D.3. \n\n>Does the framework allow for introducing distribution shifts and etc?\n\nThis is a great question. We believe our framework can accommodate distribution shifts. For instance, the simplest form of covariate shift one could introduce is to add a new set of features that are not present in the training data with some probability at test time. Since the model has not seen these new features during training, we may assume that individual members of the ensemble will make random predictions (but the prediction of different models will be correlated) if the data points only contain the new features (the same assumption for features that the models did not learn). One can also introduce a covariate shift on existing features but that would require imposing a non-uniform distribution over the existing features. We leave the exploration of these directions to future works.\n\nWe hope these answers clarify your question, and we are open to further discussion. Thank you again for your constructive feedback.\n\n---\n\nWe thank all the reviewers for their valuable time. We are glad that the reviewers found the results original, interesting, and timely.\n\nWe have modified Figure 1 and made minor revisions, and added some related works to the paper which are highlighted with orange in the text.\n\nWe will respond to each reviewer’s comments individually.", "author_response": "Thank you for your detailed review and thoughtful feedback. We are glad that you find the problem we study timely and important. Below are our responses to your concerns:\n\n> I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of abstraction\". However, I think the model proposed in this paper is too simplistic…\n\nWe definitely agree that our model is unconventional and more abstract than most existing works but similar assumptions have been made in prior work [6]. As is the case for all theoretical models, simplifications are required to keep analysis tractable. Despite the simplification, our model already exhibits some interesting behaviors and offers insight into empirical phenomena so we believe that it adds value to the current discussion on feature learning. \n\nIn the case of random feature models, we feel that it is a little unfair to say in hindsight that there was a very obvious way to move towards making it more realistic. When NTK first came out in 2018, it was thought to be very promising. Indeed, despite its simplification, it offers clear value in the analysis of some phenomena in deep networks. However, it only later became clear that there was a big gap between NTK and real deep learning (despite many efforts to close this gap). More recent works (many of which you already brought up) rely on quite distinct analysis tools than those used in NTK and, still, it’s not clear if any of them is close to real-world practice.  As such, while we agree (and have acknowledged in Section 5, Appendix D.1 and D.4) that there is a gap between our proposed framework and practice, we feel it’s a tall order for the current state of deep learning theory to have no strong assumptions.  Just like NTK analysis, we hope that our approach will ultimately prove valuable (albeit along different dimensions), despite its simplicity.\n\n> Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\nThe primary goal of our work is to investigate the effect of different features being learned by different models on GDE and the calibration of deep ensembles, both of which are robust empirical observations. While our model does not directly provide insights into training, understanding these feature interactions may indirectly inform training strategies in the future. For example, [6] shows potential avenues for how observations like ours may transfer to training.\n\n> It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\nAs detailed in Appendix F.10 (referred to at the end of Section 4), our conclusions are robust across a range of parameter settings so we believe that the settings used in our paper should be appropriate for most situations.\n\n> The notations are a bit confusing ($i, j, k$, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations ($p_d, p_r$, etc.). It will also help explain the method.\n\nWe apologize for the confusion. There is already a centralized list of notations in Appendix A which we could not include in the main text due to space constraints (this is referred to in the second paragraph of section 5). Beyond this, could you be more specific about where the $i,j,k$ are causing confusion so we could better address the confusion? $i,j,k$ are used to denote an arbitrary member of the ordered collection so we feel this is in accordance with the standard usage.\n\n> But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\nWe want to clarify that we *do not* claim that the assumptions we make are weaker than calibration (since neither is a clear superset of the other), but we do believe that it sheds more light on the underlying reasons behind GDE. Only stating the model is calibrated does not tell us much about the model or the data, but our assumptions do. In fact, our framework tells us how one can break our assumption (and calibration) through data distribution interventions. Of course, it is an important open question why so many natural distributions seem to satisfy these assumptions."}
{"claim": "The notational choices (using generic letters like i, j, k) are confusing and the paper lacks a summary figure for symbols such as p_d and p_r.", "claim_type": "presentation", "paper_id": "ze7DOLi394", "paper_title": "On the Joint Interaction of Models, Data, and Features", "paper_venue": "iclr2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "RUsLYJEhvF", "reviewer": "Reviewer_MLur", "review_text": "Summary: This paper introduces the interaction tensor, for empirically analyzing the interaction between data and model through features. Based on some observations using this tensor, they propose a very simple toy model (a combinatorial model) for feature learning. They show that this model also exhibits Generalization Disagreement Equality (GDE). Finally, the authors use their model to provide data distributions that break GDE in real world experiments.\n\nStrengths: - The problem of feature learning is quite important and there has been a lot of attempts for gaining a better theoretical understanding of it in recent years.\n\n- The authors are able to come up with a toy model that shows GDE which is a very important phenomenon (and it does need the explicit assumption of calibration).\n\n- The model that the authors propose is simple and it can be analyzed fully.\n\n- The paper is very well-written.\n\nWeaknesses: - I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of\nabstraction\". However, I think the model proposed in this paper is too simplistic. The implicit biases of deep learning are core to some of the merging phenomenon that we see these days and the models that the authors propose fails to capture that. I also think that a good toy model should leave the door open to generalizations and getting closer to real world practice (for example, for the random features model of deep learning, there is a very obvious way to move towards making it more realistic). But the models that the authors propose is too abstract and it is not clear what simplifications are made to the real problem to arrive at the proposed model.\n\n- Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\n- It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\n- The notations are a bit confusing (i, j, k, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations (p_d, p_r, etc.). It will also help explain the method.\n\n- The authors \"prove\" GDE in their model without the explicit assumption of calibration. But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\n- The theoretical understanding of feature learning is not as rudimentary as the authors claim. For example,\n\n[1] Alex Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent, 2022.\n\n[2] Zhichao Wang, Andrew Engel, Anand Sarwate, Ioana Dumitriu, and Tony Chiang. Spectral evolution and invariance in linear-width neural networks, 2022.\n\n[3] Eshaan Nichani, Alex Damian, and Jason D Lee. Provable guarantees for nonlinear feature learning in three-layer neural networks, 2023.\n\n\n[4] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. Learning two-layer neural networks, one (giant) step at a time, 2023.\n\n[5] Behrad Moniri, Donghwan Lee, Hamed Hassani, and Edgar Dobriban, A theory of non-linear feature learning with one gradient step in two-layer neural networks, 2023.\n\nand many more.\n\nQuestions: - Looking at figure 3 (a) and 3 (c), it seems that the observations made from them are not that significant. Am I missing something? How does the choice of the thresholds affect your observations?", "labeling_timestamp": "2026-01-11T16:30:26.004174", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors acknowledge the notation issues and report concrete changes to streamline notation and clarify index conventions in Section 3. However, they did not add the requested summary table/figure to the main text, only noting a centralized notation list in the appendix and promising to include a table in an extended version.", "evidence": "\"We have streamlined the notations in Section 3 for better clarity so the footnotes are no longer needed.\" ... \"Regarding the table of notation, we fully agree that a table would be great to have. Unfortunately, so far we have not been able to find a way to squeeze it into the text. And we will definitely include it in the extended version.\"", "combined_author_response": "Thank you for the continued engagement and dedication to the review process! We address your remaining concerns as follows:\n\n> 0\n\nApologies for missing the first part of your question earlier. In 3 (a), the observation we are relying on is that data points with a small number of features are much more likely to have very high confidence under the ensemble and vice versa. For a concrete example, it is much more likely for a data point with 0.8 confidence to have 25 features than a data point with 0.4 confidence to have 25 features. As such, we believe that it’s fair to say that data with fewer features tend to have higher confidence. For 3(c), we claim that the more features the models share, the lower bound on their shared errors becomes higher, which can be clearly observed in the figure. This *does not* mean that the fewer features the models share, their shared errors would necessarily be smaller. Instead, both models would make random guesses (recall that we make similar assumptions in the conceptual model). This means that a pair of models can have a high or low amount of shared errors due to randomness, but the lower bound on the shared error will become higher if they share more features. We believe that the figure supports this claim.\n\n> 1\n\nThank you for clarifying what you meant about the limitations. We agree with your assessment of NTK and shallow neural networks, and we absolutely agree that we should be transparent with the limitations of the framework. Appendix D.3 (previously D.4) is meant to discuss the limitations of the model but we see that it may not be as clear as we hoped. As such, we have expanded Appendix D.3 to be more explicit about these limitations. This was already referred to at the beginning of section 5 where we introduced the model and in the conclusion, but once again, due to space limits, we could not include them in the main text.\n\n> 2\n\nThank you for clarifying this and pointing out the notation issues. We have streamlined the notations in Section 3 for better clarity so the footnotes are no longer needed.\n\nSpecifically, we have changed to use $C$ for the number of classes instead of $K$ and use $K$ to denote the number of top principal components, so all capital letters refer to a fixed number (e.g., $N$ for the total number of data, $M$ for the number of models, etc). Lower case of $N,M,K,T$ will refer to a particular index for what the object the capital letters refer to, so $k$ will always refer to the feature, $m$ will always refer to the model and $n$ will always refer to data, etc.\n\nOne exception is the k-partite graph. Since this is the standard reference to the problem we use $\\mathtt{k}$ to differentiate it.\nAnother one is that $i,j, a, b$ are used instead of $m$ and $k$ to refer to a pair of models and features around equation 2.\nWe believe that this should resolve the ambiguity but if there are any concerns regarding the notation left please let us know.\n\nRegarding the table of notation, we fully agree that a table would be great to have. Unfortunately, so far we have not been able to find a way to squeeze it into the text. And we will definitely include it in the extended version.\n\n------------\n\nWe hope these revisions and clarifications address your concerns. Please let us know if you have any further questions.\n\n---\n\nThank you for your thoughtful feedback! We are glad that you still find our work original and creative.\n\n> many assumptions went into the combinatorial analysis.\n\nWe fully agree that many assumptions were made. The way we see it is that deep learning's complexity necessitates some level of simplification for tractable analysis. In this case, since the resulting framework is still useful for understanding empirical phenomena, we feel the assumptions are justified. \n\n> The framework is less useful for understanding the learning process of networks…\n\nThis is correct. The framework as it stands does not describe the learning process, which is an extremely complicated subject itself. However, we postulate that techniques similar to [1] could be used to study the optimization of a feature model like ours in the future.\n\n> typos\n\nThank you for the close reading and for catching these typos! We have fixed them in the revision.\n\n> have you considered modeling the rare-dominant variation as a spectrum instead of a binary?\n\nThis is an excellent question! We have indeed considered the possibility of modeling it as a spectrum, which we discussed in Appendix D.1 and D.4. In our preliminary investigation, we found that modeling the feature spectrum may require introducing more free parameters (e.g., Zipf’s distribution), and can drastically increase the complexity of analysis. Given that this is our first foray into the topic, we opted for the simpler binary model. This is an important problem and we plan to explore it in more detail in future works.\n\nWe hope these responses have addressed some of the questions and we are open to further discussion. Thank you again for your constructive feedback!\n\n**Reference**\n\n[1] Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. Allen-Zhu et al.\n\n---\n\n> The theoretical understanding of feature learning is not as rudimentary as the authors claim\n\nWe sincerely apologize for the characterization. Perhaps “rudimentary” is not the best description. We have revised the phrase to be “still in early development” to better convey the sentiment. However, we do feel that it is reasonable to say that we still have much to understand about feature learning. \n\nTo briefly address the specific works you mentioned: these models generally assume the input data are isotropic Gaussian and the function class is a 2 layer MLP. The feature is usually a linear function of the input and often special training algorithms are needed (e.g., layer-wise training). A notable exception is [3] which learns non-linear features but still requires layer-wise training. \nMost of these works are quite recent (and all came out after we started this project): [3,4,5] showed up in October 2023 and [2] was updated in November 2023. In general, it is hard to say how well they encapsulate the full complexity of deep learning, or how feasible it is to apply them to modern architectures and datasets. Nonetheless, the goal of this work is not to compare existing theoretical works on the topic or discuss their merits and shortcomings, but rather to identify what realistic (perhaps strong) modeling assumptions to make when studying feature learning. Given the recently increasing interest in the subject, we view these works and our approach as complementary components of a broader exploration into feature learning. We have added a discussion of these works to Appendix D.1, but would be happy to revise them if you have further suggestions.\n\nWe hope these responses address your concerns and we are open to further discussion. Thank you again for your constructive feedback!\n\n**Reference**\n\n[6] Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. Allen-Zhu et al.\n\n---\n\nThank you for your detailed review and thoughtful feedback. We are glad that you find the problem we study timely and important. Below are our responses to your concerns:\n\n> I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of abstraction\". However, I think the model proposed in this paper is too simplistic…\n\nWe definitely agree that our model is unconventional and more abstract than most existing works but similar assumptions have been made in prior work [6]. As is the case for all theoretical models, simplifications are required to keep analysis tractable. Despite the simplification, our model already exhibits some interesting behaviors and offers insight into empirical phenomena so we believe that it adds value to the current discussion on feature learning. \n\nIn the case of random feature models, we feel that it is a little unfair to say in hindsight that there was a very obvious way to move towards making it more realistic. When NTK first came out in 2018, it was thought to be very promising. Indeed, despite its simplification, it offers clear value in the analysis of some phenomena in deep networks. However, it only later became clear that there was a big gap between NTK and real deep learning (despite many efforts to close this gap). More recent works (many of which you already brought up) rely on quite distinct analysis tools than those used in NTK and, still, it’s not clear if any of them is close to real-world practice.  As such, while we agree (and have acknowledged in Section 5, Appendix D.1 and D.4) that there is a gap between our proposed framework and practice, we feel it’s a tall order for the current state of deep learning theory to have no strong assumptions.  Just like NTK analysis, we hope that our approach will ultimately prove valuable (albeit along different dimensions), despite its simplicity.\n\n> Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\nThe primary goal of our work is to investigate the effect of different features being learned by different models on GDE and the calibration of deep ensembles, both of which are robust empirical observations. While our model does not directly provide insights into training, understanding these feature interactions may indirectly inform training strategies in the future. For example, [6] shows potential avenues for how observations like ours may transfer to training.\n\n> It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\nAs detailed in Appendix F.10 (referred to at the end of Section 4), our conclusions are robust across a range of parameter settings so we believe that the settings used in our paper should be appropriate for most situations.\n\n> The notations are a bit confusing ($i, j, k$, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations ($p_d, p_r$, etc.). It will also help explain the method.\n\nWe apologize for the confusion. There is already a centralized list of notations in Appendix A which we could not include in the main text due to space constraints (this is referred to in the second paragraph of section 5). Beyond this, could you be more specific about where the $i,j,k$ are causing confusion so we could better address the confusion? $i,j,k$ are used to denote an arbitrary member of the ordered collection so we feel this is in accordance with the standard usage.\n\n> But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\nWe want to clarify that we *do not* claim that the assumptions we make are weaker than calibration (since neither is a clear superset of the other), but we do believe that it sheds more light on the underlying reasons behind GDE. Only stating the model is calibrated does not tell us much about the model or the data, but our assumptions do. In fact, our framework tells us how one can break our assumption (and calibration) through data distribution interventions. Of course, it is an important open question why so many natural distributions seem to satisfy these assumptions.\n\n---\n\nThank you for the support of our paper! We are glad that you find our work valuable.\n\nRegarding Figure 1, to better address your concerns, could you elaborate on which aspects of the figure you found unclear? This will help us make more targeted improvements. \n\nThe goal of Figure 1 is to demonstrate that our definition of features can capture images with redundant information and images that contain uncommon instances of a given class. To make this clearer, we have updated the figure in the paper with texts to better reflect the core idea. Does this resolve your concerns? If not, we’d be happy to make more adjustments.\n\n---\n\nThank you for your supportive feedback on our work. We are glad that you find our new perspective on feature learning interesting. Below are our responses to your questions:\n\n> how important are these hyperparameters? How to properly set them?\n\nIn Appendix F.10, we conducted an ablation study and found that our qualitative conclusions are robust to variations in these hyperparameters. Therefore, we believe the values used in our paper should be appropriate for most applications.\n\n> Currently theoretical framework analyzes binary classification, does the analysis extend to multi-class classification?\n\nWe discuss the necessary steps for the extension to multi-class classification in Appendix D.3. \n\n>Does the framework allow for introducing distribution shifts and etc?\n\nThis is a great question. We believe our framework can accommodate distribution shifts. For instance, the simplest form of covariate shift one could introduce is to add a new set of features that are not present in the training data with some probability at test time. Since the model has not seen these new features during training, we may assume that individual members of the ensemble will make random predictions (but the prediction of different models will be correlated) if the data points only contain the new features (the same assumption for features that the models did not learn). One can also introduce a covariate shift on existing features but that would require imposing a non-uniform distribution over the existing features. We leave the exploration of these directions to future works.\n\nWe hope these answers clarify your question, and we are open to further discussion. Thank you again for your constructive feedback.\n\n---\n\nWe thank all the reviewers for their valuable time. We are glad that the reviewers found the results original, interesting, and timely.\n\nWe have modified Figure 1 and made minor revisions, and added some related works to the paper which are highlighted with orange in the text.\n\nWe will respond to each reviewer’s comments individually.", "author_response": "Thank you for your detailed review and thoughtful feedback. We are glad that you find the problem we study timely and important. Below are our responses to your concerns:\n\n> I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of abstraction\". However, I think the model proposed in this paper is too simplistic…\n\nWe definitely agree that our model is unconventional and more abstract than most existing works but similar assumptions have been made in prior work [6]. As is the case for all theoretical models, simplifications are required to keep analysis tractable. Despite the simplification, our model already exhibits some interesting behaviors and offers insight into empirical phenomena so we believe that it adds value to the current discussion on feature learning. \n\nIn the case of random feature models, we feel that it is a little unfair to say in hindsight that there was a very obvious way to move towards making it more realistic. When NTK first came out in 2018, it was thought to be very promising. Indeed, despite its simplification, it offers clear value in the analysis of some phenomena in deep networks. However, it only later became clear that there was a big gap between NTK and real deep learning (despite many efforts to close this gap). More recent works (many of which you already brought up) rely on quite distinct analysis tools than those used in NTK and, still, it’s not clear if any of them is close to real-world practice.  As such, while we agree (and have acknowledged in Section 5, Appendix D.1 and D.4) that there is a gap between our proposed framework and practice, we feel it’s a tall order for the current state of deep learning theory to have no strong assumptions.  Just like NTK analysis, we hope that our approach will ultimately prove valuable (albeit along different dimensions), despite its simplicity.\n\n> Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\nThe primary goal of our work is to investigate the effect of different features being learned by different models on GDE and the calibration of deep ensembles, both of which are robust empirical observations. While our model does not directly provide insights into training, understanding these feature interactions may indirectly inform training strategies in the future. For example, [6] shows potential avenues for how observations like ours may transfer to training.\n\n> It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\nAs detailed in Appendix F.10 (referred to at the end of Section 4), our conclusions are robust across a range of parameter settings so we believe that the settings used in our paper should be appropriate for most situations.\n\n> The notations are a bit confusing ($i, j, k$, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations ($p_d, p_r$, etc.). It will also help explain the method.\n\nWe apologize for the confusion. There is already a centralized list of notations in Appendix A which we could not include in the main text due to space constraints (this is referred to in the second paragraph of section 5). Beyond this, could you be more specific about where the $i,j,k$ are causing confusion so we could better address the confusion? $i,j,k$ are used to denote an arbitrary member of the ordered collection so we feel this is in accordance with the standard usage.\n\n> But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\nWe want to clarify that we *do not* claim that the assumptions we make are weaker than calibration (since neither is a clear superset of the other), but we do believe that it sheds more light on the underlying reasons behind GDE. Only stating the model is calibrated does not tell us much about the model or the data, but our assumptions do. In fact, our framework tells us how one can break our assumption (and calibration) through data distribution interventions. Of course, it is an important open question why so many natural distributions seem to satisfy these assumptions."}
{"claim": "The authors claim to prove GDE without explicitly assuming calibration, but they do not clarify whether their model assumptions are stronger or weaker than calibration.", "claim_type": "methodology", "paper_id": "ze7DOLi394", "paper_title": "On the Joint Interaction of Models, Data, and Features", "paper_venue": "iclr2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "RUsLYJEhvF", "reviewer": "Reviewer_MLur", "review_text": "Summary: This paper introduces the interaction tensor, for empirically analyzing the interaction between data and model through features. Based on some observations using this tensor, they propose a very simple toy model (a combinatorial model) for feature learning. They show that this model also exhibits Generalization Disagreement Equality (GDE). Finally, the authors use their model to provide data distributions that break GDE in real world experiments.\n\nStrengths: - The problem of feature learning is quite important and there has been a lot of attempts for gaining a better theoretical understanding of it in recent years.\n\n- The authors are able to come up with a toy model that shows GDE which is a very important phenomenon (and it does need the explicit assumption of calibration).\n\n- The model that the authors propose is simple and it can be analyzed fully.\n\n- The paper is very well-written.\n\nWeaknesses: - I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of\nabstraction\". However, I think the model proposed in this paper is too simplistic. The implicit biases of deep learning are core to some of the merging phenomenon that we see these days and the models that the authors propose fails to capture that. I also think that a good toy model should leave the door open to generalizations and getting closer to real world practice (for example, for the random features model of deep learning, there is a very obvious way to move towards making it more realistic). But the models that the authors propose is too abstract and it is not clear what simplifications are made to the real problem to arrive at the proposed model.\n\n- Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\n- It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\n- The notations are a bit confusing (i, j, k, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations (p_d, p_r, etc.). It will also help explain the method.\n\n- The authors \"prove\" GDE in their model without the explicit assumption of calibration. But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\n- The theoretical understanding of feature learning is not as rudimentary as the authors claim. For example,\n\n[1] Alex Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent, 2022.\n\n[2] Zhichao Wang, Andrew Engel, Anand Sarwate, Ioana Dumitriu, and Tony Chiang. Spectral evolution and invariance in linear-width neural networks, 2022.\n\n[3] Eshaan Nichani, Alex Damian, and Jason D Lee. Provable guarantees for nonlinear feature learning in three-layer neural networks, 2023.\n\n\n[4] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. Learning two-layer neural networks, one (giant) step at a time, 2023.\n\n[5] Behrad Moniri, Donghwan Lee, Hamed Hassani, and Edgar Dobriban, A theory of non-linear feature learning with one gradient step in two-layer neural networks, 2023.\n\nand many more.\n\nQuestions: - Looking at figure 3 (a) and 3 (c), it seems that the observations made from them are not that significant. Am I missing something? How does the choice of the thresholds affect your observations?", "labeling_timestamp": "2026-01-11T16:30:17.968770", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors explicitly clarify the relationship: they state they do not claim their assumptions are weaker than calibration and assert that neither set of assumptions is a clear superset of the other, thus directly addressing the reviewer's concern.", "evidence": "We want to clarify that we *do not* claim that the assumptions we make are weaker than calibration (since neither is a clear superset of the other), but we do believe that it sheds more light on the underlying reasons behind GDE. Only stating the model is calibrated does not tell us much about the model or the data, but our assumptions do. In fact, our framework tells us how one can break our assumption (and calibration) through data distribution interventions. Of course, it is an important open question why so many natural distributions seem to satisfy these assumptions.", "combined_author_response": "Thank you for the continued engagement and dedication to the review process! We address your remaining concerns as follows:\n\n> 0\n\nApologies for missing the first part of your question earlier. In 3 (a), the observation we are relying on is that data points with a small number of features are much more likely to have very high confidence under the ensemble and vice versa. For a concrete example, it is much more likely for a data point with 0.8 confidence to have 25 features than a data point with 0.4 confidence to have 25 features. As such, we believe that it’s fair to say that data with fewer features tend to have higher confidence. For 3(c), we claim that the more features the models share, the lower bound on their shared errors becomes higher, which can be clearly observed in the figure. This *does not* mean that the fewer features the models share, their shared errors would necessarily be smaller. Instead, both models would make random guesses (recall that we make similar assumptions in the conceptual model). This means that a pair of models can have a high or low amount of shared errors due to randomness, but the lower bound on the shared error will become higher if they share more features. We believe that the figure supports this claim.\n\n> 1\n\nThank you for clarifying what you meant about the limitations. We agree with your assessment of NTK and shallow neural networks, and we absolutely agree that we should be transparent with the limitations of the framework. Appendix D.3 (previously D.4) is meant to discuss the limitations of the model but we see that it may not be as clear as we hoped. As such, we have expanded Appendix D.3 to be more explicit about these limitations. This was already referred to at the beginning of section 5 where we introduced the model and in the conclusion, but once again, due to space limits, we could not include them in the main text.\n\n> 2\n\nThank you for clarifying this and pointing out the notation issues. We have streamlined the notations in Section 3 for better clarity so the footnotes are no longer needed.\n\nSpecifically, we have changed to use $C$ for the number of classes instead of $K$ and use $K$ to denote the number of top principal components, so all capital letters refer to a fixed number (e.g., $N$ for the total number of data, $M$ for the number of models, etc). Lower case of $N,M,K,T$ will refer to a particular index for what the object the capital letters refer to, so $k$ will always refer to the feature, $m$ will always refer to the model and $n$ will always refer to data, etc.\n\nOne exception is the k-partite graph. Since this is the standard reference to the problem we use $\\mathtt{k}$ to differentiate it.\nAnother one is that $i,j, a, b$ are used instead of $m$ and $k$ to refer to a pair of models and features around equation 2.\nWe believe that this should resolve the ambiguity but if there are any concerns regarding the notation left please let us know.\n\nRegarding the table of notation, we fully agree that a table would be great to have. Unfortunately, so far we have not been able to find a way to squeeze it into the text. And we will definitely include it in the extended version.\n\n------------\n\nWe hope these revisions and clarifications address your concerns. Please let us know if you have any further questions.\n\n---\n\nThank you for your thoughtful feedback! We are glad that you still find our work original and creative.\n\n> many assumptions went into the combinatorial analysis.\n\nWe fully agree that many assumptions were made. The way we see it is that deep learning's complexity necessitates some level of simplification for tractable analysis. In this case, since the resulting framework is still useful for understanding empirical phenomena, we feel the assumptions are justified. \n\n> The framework is less useful for understanding the learning process of networks…\n\nThis is correct. The framework as it stands does not describe the learning process, which is an extremely complicated subject itself. However, we postulate that techniques similar to [1] could be used to study the optimization of a feature model like ours in the future.\n\n> typos\n\nThank you for the close reading and for catching these typos! We have fixed them in the revision.\n\n> have you considered modeling the rare-dominant variation as a spectrum instead of a binary?\n\nThis is an excellent question! We have indeed considered the possibility of modeling it as a spectrum, which we discussed in Appendix D.1 and D.4. In our preliminary investigation, we found that modeling the feature spectrum may require introducing more free parameters (e.g., Zipf’s distribution), and can drastically increase the complexity of analysis. Given that this is our first foray into the topic, we opted for the simpler binary model. This is an important problem and we plan to explore it in more detail in future works.\n\nWe hope these responses have addressed some of the questions and we are open to further discussion. Thank you again for your constructive feedback!\n\n**Reference**\n\n[1] Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. Allen-Zhu et al.\n\n---\n\n> The theoretical understanding of feature learning is not as rudimentary as the authors claim\n\nWe sincerely apologize for the characterization. Perhaps “rudimentary” is not the best description. We have revised the phrase to be “still in early development” to better convey the sentiment. However, we do feel that it is reasonable to say that we still have much to understand about feature learning. \n\nTo briefly address the specific works you mentioned: these models generally assume the input data are isotropic Gaussian and the function class is a 2 layer MLP. The feature is usually a linear function of the input and often special training algorithms are needed (e.g., layer-wise training). A notable exception is [3] which learns non-linear features but still requires layer-wise training. \nMost of these works are quite recent (and all came out after we started this project): [3,4,5] showed up in October 2023 and [2] was updated in November 2023. In general, it is hard to say how well they encapsulate the full complexity of deep learning, or how feasible it is to apply them to modern architectures and datasets. Nonetheless, the goal of this work is not to compare existing theoretical works on the topic or discuss their merits and shortcomings, but rather to identify what realistic (perhaps strong) modeling assumptions to make when studying feature learning. Given the recently increasing interest in the subject, we view these works and our approach as complementary components of a broader exploration into feature learning. We have added a discussion of these works to Appendix D.1, but would be happy to revise them if you have further suggestions.\n\nWe hope these responses address your concerns and we are open to further discussion. Thank you again for your constructive feedback!\n\n**Reference**\n\n[6] Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. Allen-Zhu et al.\n\n---\n\nThank you for your detailed review and thoughtful feedback. We are glad that you find the problem we study timely and important. Below are our responses to your concerns:\n\n> I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of abstraction\". However, I think the model proposed in this paper is too simplistic…\n\nWe definitely agree that our model is unconventional and more abstract than most existing works but similar assumptions have been made in prior work [6]. As is the case for all theoretical models, simplifications are required to keep analysis tractable. Despite the simplification, our model already exhibits some interesting behaviors and offers insight into empirical phenomena so we believe that it adds value to the current discussion on feature learning. \n\nIn the case of random feature models, we feel that it is a little unfair to say in hindsight that there was a very obvious way to move towards making it more realistic. When NTK first came out in 2018, it was thought to be very promising. Indeed, despite its simplification, it offers clear value in the analysis of some phenomena in deep networks. However, it only later became clear that there was a big gap between NTK and real deep learning (despite many efforts to close this gap). More recent works (many of which you already brought up) rely on quite distinct analysis tools than those used in NTK and, still, it’s not clear if any of them is close to real-world practice.  As such, while we agree (and have acknowledged in Section 5, Appendix D.1 and D.4) that there is a gap between our proposed framework and practice, we feel it’s a tall order for the current state of deep learning theory to have no strong assumptions.  Just like NTK analysis, we hope that our approach will ultimately prove valuable (albeit along different dimensions), despite its simplicity.\n\n> Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\nThe primary goal of our work is to investigate the effect of different features being learned by different models on GDE and the calibration of deep ensembles, both of which are robust empirical observations. While our model does not directly provide insights into training, understanding these feature interactions may indirectly inform training strategies in the future. For example, [6] shows potential avenues for how observations like ours may transfer to training.\n\n> It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\nAs detailed in Appendix F.10 (referred to at the end of Section 4), our conclusions are robust across a range of parameter settings so we believe that the settings used in our paper should be appropriate for most situations.\n\n> The notations are a bit confusing ($i, j, k$, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations ($p_d, p_r$, etc.). It will also help explain the method.\n\nWe apologize for the confusion. There is already a centralized list of notations in Appendix A which we could not include in the main text due to space constraints (this is referred to in the second paragraph of section 5). Beyond this, could you be more specific about where the $i,j,k$ are causing confusion so we could better address the confusion? $i,j,k$ are used to denote an arbitrary member of the ordered collection so we feel this is in accordance with the standard usage.\n\n> But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\nWe want to clarify that we *do not* claim that the assumptions we make are weaker than calibration (since neither is a clear superset of the other), but we do believe that it sheds more light on the underlying reasons behind GDE. Only stating the model is calibrated does not tell us much about the model or the data, but our assumptions do. In fact, our framework tells us how one can break our assumption (and calibration) through data distribution interventions. Of course, it is an important open question why so many natural distributions seem to satisfy these assumptions.\n\n---\n\nThank you for the support of our paper! We are glad that you find our work valuable.\n\nRegarding Figure 1, to better address your concerns, could you elaborate on which aspects of the figure you found unclear? This will help us make more targeted improvements. \n\nThe goal of Figure 1 is to demonstrate that our definition of features can capture images with redundant information and images that contain uncommon instances of a given class. To make this clearer, we have updated the figure in the paper with texts to better reflect the core idea. Does this resolve your concerns? If not, we’d be happy to make more adjustments.\n\n---\n\nThank you for your supportive feedback on our work. We are glad that you find our new perspective on feature learning interesting. Below are our responses to your questions:\n\n> how important are these hyperparameters? How to properly set them?\n\nIn Appendix F.10, we conducted an ablation study and found that our qualitative conclusions are robust to variations in these hyperparameters. Therefore, we believe the values used in our paper should be appropriate for most applications.\n\n> Currently theoretical framework analyzes binary classification, does the analysis extend to multi-class classification?\n\nWe discuss the necessary steps for the extension to multi-class classification in Appendix D.3. \n\n>Does the framework allow for introducing distribution shifts and etc?\n\nThis is a great question. We believe our framework can accommodate distribution shifts. For instance, the simplest form of covariate shift one could introduce is to add a new set of features that are not present in the training data with some probability at test time. Since the model has not seen these new features during training, we may assume that individual members of the ensemble will make random predictions (but the prediction of different models will be correlated) if the data points only contain the new features (the same assumption for features that the models did not learn). One can also introduce a covariate shift on existing features but that would require imposing a non-uniform distribution over the existing features. We leave the exploration of these directions to future works.\n\nWe hope these answers clarify your question, and we are open to further discussion. Thank you again for your constructive feedback.\n\n---\n\nWe thank all the reviewers for their valuable time. We are glad that the reviewers found the results original, interesting, and timely.\n\nWe have modified Figure 1 and made minor revisions, and added some related works to the paper which are highlighted with orange in the text.\n\nWe will respond to each reviewer’s comments individually.", "author_response": "Thank you for your detailed review and thoughtful feedback. We are glad that you find the problem we study timely and important. Below are our responses to your concerns:\n\n> I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of abstraction\". However, I think the model proposed in this paper is too simplistic…\n\nWe definitely agree that our model is unconventional and more abstract than most existing works but similar assumptions have been made in prior work [6]. As is the case for all theoretical models, simplifications are required to keep analysis tractable. Despite the simplification, our model already exhibits some interesting behaviors and offers insight into empirical phenomena so we believe that it adds value to the current discussion on feature learning. \n\nIn the case of random feature models, we feel that it is a little unfair to say in hindsight that there was a very obvious way to move towards making it more realistic. When NTK first came out in 2018, it was thought to be very promising. Indeed, despite its simplification, it offers clear value in the analysis of some phenomena in deep networks. However, it only later became clear that there was a big gap between NTK and real deep learning (despite many efforts to close this gap). More recent works (many of which you already brought up) rely on quite distinct analysis tools than those used in NTK and, still, it’s not clear if any of them is close to real-world practice.  As such, while we agree (and have acknowledged in Section 5, Appendix D.1 and D.4) that there is a gap between our proposed framework and practice, we feel it’s a tall order for the current state of deep learning theory to have no strong assumptions.  Just like NTK analysis, we hope that our approach will ultimately prove valuable (albeit along different dimensions), despite its simplicity.\n\n> Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\nThe primary goal of our work is to investigate the effect of different features being learned by different models on GDE and the calibration of deep ensembles, both of which are robust empirical observations. While our model does not directly provide insights into training, understanding these feature interactions may indirectly inform training strategies in the future. For example, [6] shows potential avenues for how observations like ours may transfer to training.\n\n> It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\nAs detailed in Appendix F.10 (referred to at the end of Section 4), our conclusions are robust across a range of parameter settings so we believe that the settings used in our paper should be appropriate for most situations.\n\n> The notations are a bit confusing ($i, j, k$, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations ($p_d, p_r$, etc.). It will also help explain the method.\n\nWe apologize for the confusion. There is already a centralized list of notations in Appendix A which we could not include in the main text due to space constraints (this is referred to in the second paragraph of section 5). Beyond this, could you be more specific about where the $i,j,k$ are causing confusion so we could better address the confusion? $i,j,k$ are used to denote an arbitrary member of the ordered collection so we feel this is in accordance with the standard usage.\n\n> But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\nWe want to clarify that we *do not* claim that the assumptions we make are weaker than calibration (since neither is a clear superset of the other), but we do believe that it sheds more light on the underlying reasons behind GDE. Only stating the model is calibrated does not tell us much about the model or the data, but our assumptions do. In fact, our framework tells us how one can break our assumption (and calibration) through data distribution interventions. Of course, it is an important open question why so many natural distributions seem to satisfy these assumptions."}
{"claim": "The paper overlooks several recent theoretical works on provable feature learning, contradicting the claim that the theoretical understanding of feature learning is rudimentary.", "claim_type": "novelty", "paper_id": "ze7DOLi394", "paper_title": "On the Joint Interaction of Models, Data, and Features", "paper_venue": "iclr2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "RUsLYJEhvF", "reviewer": "Reviewer_MLur", "review_text": "Summary: This paper introduces the interaction tensor, for empirically analyzing the interaction between data and model through features. Based on some observations using this tensor, they propose a very simple toy model (a combinatorial model) for feature learning. They show that this model also exhibits Generalization Disagreement Equality (GDE). Finally, the authors use their model to provide data distributions that break GDE in real world experiments.\n\nStrengths: - The problem of feature learning is quite important and there has been a lot of attempts for gaining a better theoretical understanding of it in recent years.\n\n- The authors are able to come up with a toy model that shows GDE which is a very important phenomenon (and it does need the explicit assumption of calibration).\n\n- The model that the authors propose is simple and it can be analyzed fully.\n\n- The paper is very well-written.\n\nWeaknesses: - I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of\nabstraction\". However, I think the model proposed in this paper is too simplistic. The implicit biases of deep learning are core to some of the merging phenomenon that we see these days and the models that the authors propose fails to capture that. I also think that a good toy model should leave the door open to generalizations and getting closer to real world practice (for example, for the random features model of deep learning, there is a very obvious way to move towards making it more realistic). But the models that the authors propose is too abstract and it is not clear what simplifications are made to the real problem to arrive at the proposed model.\n\n- Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\n- It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\n- The notations are a bit confusing (i, j, k, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations (p_d, p_r, etc.). It will also help explain the method.\n\n- The authors \"prove\" GDE in their model without the explicit assumption of calibration. But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\n- The theoretical understanding of feature learning is not as rudimentary as the authors claim. For example,\n\n[1] Alex Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent, 2022.\n\n[2] Zhichao Wang, Andrew Engel, Anand Sarwate, Ioana Dumitriu, and Tony Chiang. Spectral evolution and invariance in linear-width neural networks, 2022.\n\n[3] Eshaan Nichani, Alex Damian, and Jason D Lee. Provable guarantees for nonlinear feature learning in three-layer neural networks, 2023.\n\n\n[4] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. Learning two-layer neural networks, one (giant) step at a time, 2023.\n\n[5] Behrad Moniri, Donghwan Lee, Hamed Hassani, and Edgar Dobriban, A theory of non-linear feature learning with one gradient step in two-layer neural networks, 2023.\n\nand many more.\n\nQuestions: - Looking at figure 3 (a) and 3 (c), it seems that the observations made from them are not that significant. Am I missing something? How does the choice of the thresholds affect your observations?", "labeling_timestamp": "2026-01-11T16:30:21.874625", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors acknowledge the reviewer’s point, apologize for the wording, revise the characterization, and state that they added discussion of the recent works to the appendix and related-works section—thus addressing the claim.", "evidence": "\"We sincerely apologize for the characterization. Perhaps “rudimentary” is not the best description. We have revised the phrase to be “still in early development”... To briefly address the specific works you mentioned: ... Most of these works are quite recent (and all came out after we started this project)... We have added a discussion of these works to Appendix D.1, but would be happy to revise them if you have further suggestions.\" \"We have modified Figure 1 and made minor revisions, and added some related works to the paper which are highlighted with orange in the text.\"", "combined_author_response": "Thank you for the continued engagement and dedication to the review process! We address your remaining concerns as follows:\n\n> 0\n\nApologies for missing the first part of your question earlier. In 3 (a), the observation we are relying on is that data points with a small number of features are much more likely to have very high confidence under the ensemble and vice versa. For a concrete example, it is much more likely for a data point with 0.8 confidence to have 25 features than a data point with 0.4 confidence to have 25 features. As such, we believe that it’s fair to say that data with fewer features tend to have higher confidence. For 3(c), we claim that the more features the models share, the lower bound on their shared errors becomes higher, which can be clearly observed in the figure. This *does not* mean that the fewer features the models share, their shared errors would necessarily be smaller. Instead, both models would make random guesses (recall that we make similar assumptions in the conceptual model). This means that a pair of models can have a high or low amount of shared errors due to randomness, but the lower bound on the shared error will become higher if they share more features. We believe that the figure supports this claim.\n\n> 1\n\nThank you for clarifying what you meant about the limitations. We agree with your assessment of NTK and shallow neural networks, and we absolutely agree that we should be transparent with the limitations of the framework. Appendix D.3 (previously D.4) is meant to discuss the limitations of the model but we see that it may not be as clear as we hoped. As such, we have expanded Appendix D.3 to be more explicit about these limitations. This was already referred to at the beginning of section 5 where we introduced the model and in the conclusion, but once again, due to space limits, we could not include them in the main text.\n\n> 2\n\nThank you for clarifying this and pointing out the notation issues. We have streamlined the notations in Section 3 for better clarity so the footnotes are no longer needed.\n\nSpecifically, we have changed to use $C$ for the number of classes instead of $K$ and use $K$ to denote the number of top principal components, so all capital letters refer to a fixed number (e.g., $N$ for the total number of data, $M$ for the number of models, etc). Lower case of $N,M,K,T$ will refer to a particular index for what the object the capital letters refer to, so $k$ will always refer to the feature, $m$ will always refer to the model and $n$ will always refer to data, etc.\n\nOne exception is the k-partite graph. Since this is the standard reference to the problem we use $\\mathtt{k}$ to differentiate it.\nAnother one is that $i,j, a, b$ are used instead of $m$ and $k$ to refer to a pair of models and features around equation 2.\nWe believe that this should resolve the ambiguity but if there are any concerns regarding the notation left please let us know.\n\nRegarding the table of notation, we fully agree that a table would be great to have. Unfortunately, so far we have not been able to find a way to squeeze it into the text. And we will definitely include it in the extended version.\n\n------------\n\nWe hope these revisions and clarifications address your concerns. Please let us know if you have any further questions.\n\n---\n\nThank you for your thoughtful feedback! We are glad that you still find our work original and creative.\n\n> many assumptions went into the combinatorial analysis.\n\nWe fully agree that many assumptions were made. The way we see it is that deep learning's complexity necessitates some level of simplification for tractable analysis. In this case, since the resulting framework is still useful for understanding empirical phenomena, we feel the assumptions are justified. \n\n> The framework is less useful for understanding the learning process of networks…\n\nThis is correct. The framework as it stands does not describe the learning process, which is an extremely complicated subject itself. However, we postulate that techniques similar to [1] could be used to study the optimization of a feature model like ours in the future.\n\n> typos\n\nThank you for the close reading and for catching these typos! We have fixed them in the revision.\n\n> have you considered modeling the rare-dominant variation as a spectrum instead of a binary?\n\nThis is an excellent question! We have indeed considered the possibility of modeling it as a spectrum, which we discussed in Appendix D.1 and D.4. In our preliminary investigation, we found that modeling the feature spectrum may require introducing more free parameters (e.g., Zipf’s distribution), and can drastically increase the complexity of analysis. Given that this is our first foray into the topic, we opted for the simpler binary model. This is an important problem and we plan to explore it in more detail in future works.\n\nWe hope these responses have addressed some of the questions and we are open to further discussion. Thank you again for your constructive feedback!\n\n**Reference**\n\n[1] Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. Allen-Zhu et al.\n\n---\n\n> The theoretical understanding of feature learning is not as rudimentary as the authors claim\n\nWe sincerely apologize for the characterization. Perhaps “rudimentary” is not the best description. We have revised the phrase to be “still in early development” to better convey the sentiment. However, we do feel that it is reasonable to say that we still have much to understand about feature learning. \n\nTo briefly address the specific works you mentioned: these models generally assume the input data are isotropic Gaussian and the function class is a 2 layer MLP. The feature is usually a linear function of the input and often special training algorithms are needed (e.g., layer-wise training). A notable exception is [3] which learns non-linear features but still requires layer-wise training. \nMost of these works are quite recent (and all came out after we started this project): [3,4,5] showed up in October 2023 and [2] was updated in November 2023. In general, it is hard to say how well they encapsulate the full complexity of deep learning, or how feasible it is to apply them to modern architectures and datasets. Nonetheless, the goal of this work is not to compare existing theoretical works on the topic or discuss their merits and shortcomings, but rather to identify what realistic (perhaps strong) modeling assumptions to make when studying feature learning. Given the recently increasing interest in the subject, we view these works and our approach as complementary components of a broader exploration into feature learning. We have added a discussion of these works to Appendix D.1, but would be happy to revise them if you have further suggestions.\n\nWe hope these responses address your concerns and we are open to further discussion. Thank you again for your constructive feedback!\n\n**Reference**\n\n[6] Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. Allen-Zhu et al.\n\n---\n\nThank you for your detailed review and thoughtful feedback. We are glad that you find the problem we study timely and important. Below are our responses to your concerns:\n\n> I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of abstraction\". However, I think the model proposed in this paper is too simplistic…\n\nWe definitely agree that our model is unconventional and more abstract than most existing works but similar assumptions have been made in prior work [6]. As is the case for all theoretical models, simplifications are required to keep analysis tractable. Despite the simplification, our model already exhibits some interesting behaviors and offers insight into empirical phenomena so we believe that it adds value to the current discussion on feature learning. \n\nIn the case of random feature models, we feel that it is a little unfair to say in hindsight that there was a very obvious way to move towards making it more realistic. When NTK first came out in 2018, it was thought to be very promising. Indeed, despite its simplification, it offers clear value in the analysis of some phenomena in deep networks. However, it only later became clear that there was a big gap between NTK and real deep learning (despite many efforts to close this gap). More recent works (many of which you already brought up) rely on quite distinct analysis tools than those used in NTK and, still, it’s not clear if any of them is close to real-world practice.  As such, while we agree (and have acknowledged in Section 5, Appendix D.1 and D.4) that there is a gap between our proposed framework and practice, we feel it’s a tall order for the current state of deep learning theory to have no strong assumptions.  Just like NTK analysis, we hope that our approach will ultimately prove valuable (albeit along different dimensions), despite its simplicity.\n\n> Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\nThe primary goal of our work is to investigate the effect of different features being learned by different models on GDE and the calibration of deep ensembles, both of which are robust empirical observations. While our model does not directly provide insights into training, understanding these feature interactions may indirectly inform training strategies in the future. For example, [6] shows potential avenues for how observations like ours may transfer to training.\n\n> It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\nAs detailed in Appendix F.10 (referred to at the end of Section 4), our conclusions are robust across a range of parameter settings so we believe that the settings used in our paper should be appropriate for most situations.\n\n> The notations are a bit confusing ($i, j, k$, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations ($p_d, p_r$, etc.). It will also help explain the method.\n\nWe apologize for the confusion. There is already a centralized list of notations in Appendix A which we could not include in the main text due to space constraints (this is referred to in the second paragraph of section 5). Beyond this, could you be more specific about where the $i,j,k$ are causing confusion so we could better address the confusion? $i,j,k$ are used to denote an arbitrary member of the ordered collection so we feel this is in accordance with the standard usage.\n\n> But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\nWe want to clarify that we *do not* claim that the assumptions we make are weaker than calibration (since neither is a clear superset of the other), but we do believe that it sheds more light on the underlying reasons behind GDE. Only stating the model is calibrated does not tell us much about the model or the data, but our assumptions do. In fact, our framework tells us how one can break our assumption (and calibration) through data distribution interventions. Of course, it is an important open question why so many natural distributions seem to satisfy these assumptions.\n\n---\n\nThank you for the support of our paper! We are glad that you find our work valuable.\n\nRegarding Figure 1, to better address your concerns, could you elaborate on which aspects of the figure you found unclear? This will help us make more targeted improvements. \n\nThe goal of Figure 1 is to demonstrate that our definition of features can capture images with redundant information and images that contain uncommon instances of a given class. To make this clearer, we have updated the figure in the paper with texts to better reflect the core idea. Does this resolve your concerns? If not, we’d be happy to make more adjustments.\n\n---\n\nThank you for your supportive feedback on our work. We are glad that you find our new perspective on feature learning interesting. Below are our responses to your questions:\n\n> how important are these hyperparameters? How to properly set them?\n\nIn Appendix F.10, we conducted an ablation study and found that our qualitative conclusions are robust to variations in these hyperparameters. Therefore, we believe the values used in our paper should be appropriate for most applications.\n\n> Currently theoretical framework analyzes binary classification, does the analysis extend to multi-class classification?\n\nWe discuss the necessary steps for the extension to multi-class classification in Appendix D.3. \n\n>Does the framework allow for introducing distribution shifts and etc?\n\nThis is a great question. We believe our framework can accommodate distribution shifts. For instance, the simplest form of covariate shift one could introduce is to add a new set of features that are not present in the training data with some probability at test time. Since the model has not seen these new features during training, we may assume that individual members of the ensemble will make random predictions (but the prediction of different models will be correlated) if the data points only contain the new features (the same assumption for features that the models did not learn). One can also introduce a covariate shift on existing features but that would require imposing a non-uniform distribution over the existing features. We leave the exploration of these directions to future works.\n\nWe hope these answers clarify your question, and we are open to further discussion. Thank you again for your constructive feedback.\n\n---\n\nWe thank all the reviewers for their valuable time. We are glad that the reviewers found the results original, interesting, and timely.\n\nWe have modified Figure 1 and made minor revisions, and added some related works to the paper which are highlighted with orange in the text.\n\nWe will respond to each reviewer’s comments individually.", "author_response": "Thank you for your detailed review and thoughtful feedback. We are glad that you find the problem we study timely and important. Below are our responses to your concerns:\n\n> I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of abstraction\". However, I think the model proposed in this paper is too simplistic…\n\nWe definitely agree that our model is unconventional and more abstract than most existing works but similar assumptions have been made in prior work [6]. As is the case for all theoretical models, simplifications are required to keep analysis tractable. Despite the simplification, our model already exhibits some interesting behaviors and offers insight into empirical phenomena so we believe that it adds value to the current discussion on feature learning. \n\nIn the case of random feature models, we feel that it is a little unfair to say in hindsight that there was a very obvious way to move towards making it more realistic. When NTK first came out in 2018, it was thought to be very promising. Indeed, despite its simplification, it offers clear value in the analysis of some phenomena in deep networks. However, it only later became clear that there was a big gap between NTK and real deep learning (despite many efforts to close this gap). More recent works (many of which you already brought up) rely on quite distinct analysis tools than those used in NTK and, still, it’s not clear if any of them is close to real-world practice.  As such, while we agree (and have acknowledged in Section 5, Appendix D.1 and D.4) that there is a gap between our proposed framework and practice, we feel it’s a tall order for the current state of deep learning theory to have no strong assumptions.  Just like NTK analysis, we hope that our approach will ultimately prove valuable (albeit along different dimensions), despite its simplicity.\n\n> Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\nThe primary goal of our work is to investigate the effect of different features being learned by different models on GDE and the calibration of deep ensembles, both of which are robust empirical observations. While our model does not directly provide insights into training, understanding these feature interactions may indirectly inform training strategies in the future. For example, [6] shows potential avenues for how observations like ours may transfer to training.\n\n> It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\nAs detailed in Appendix F.10 (referred to at the end of Section 4), our conclusions are robust across a range of parameter settings so we believe that the settings used in our paper should be appropriate for most situations.\n\n> The notations are a bit confusing ($i, j, k$, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations ($p_d, p_r$, etc.). It will also help explain the method.\n\nWe apologize for the confusion. There is already a centralized list of notations in Appendix A which we could not include in the main text due to space constraints (this is referred to in the second paragraph of section 5). Beyond this, could you be more specific about where the $i,j,k$ are causing confusion so we could better address the confusion? $i,j,k$ are used to denote an arbitrary member of the ordered collection so we feel this is in accordance with the standard usage.\n\n> But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\nWe want to clarify that we *do not* claim that the assumptions we make are weaker than calibration (since neither is a clear superset of the other), but we do believe that it sheds more light on the underlying reasons behind GDE. Only stating the model is calibrated does not tell us much about the model or the data, but our assumptions do. In fact, our framework tells us how one can break our assumption (and calibration) through data distribution interventions. Of course, it is an important open question why so many natural distributions seem to satisfy these assumptions."}
{"claim": "The observations shown in Figures 3(a) and 3(c) appear not statistically significant or convincing based on the plotted results.", "claim_type": "experimental", "paper_id": "ze7DOLi394", "paper_title": "On the Joint Interaction of Models, Data, and Features", "paper_venue": "iclr2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "RUsLYJEhvF", "reviewer": "Reviewer_MLur", "review_text": "Summary: This paper introduces the interaction tensor, for empirically analyzing the interaction between data and model through features. Based on some observations using this tensor, they propose a very simple toy model (a combinatorial model) for feature learning. They show that this model also exhibits Generalization Disagreement Equality (GDE). Finally, the authors use their model to provide data distributions that break GDE in real world experiments.\n\nStrengths: - The problem of feature learning is quite important and there has been a lot of attempts for gaining a better theoretical understanding of it in recent years.\n\n- The authors are able to come up with a toy model that shows GDE which is a very important phenomenon (and it does need the explicit assumption of calibration).\n\n- The model that the authors propose is simple and it can be analyzed fully.\n\n- The paper is very well-written.\n\nWeaknesses: - I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of\nabstraction\". However, I think the model proposed in this paper is too simplistic. The implicit biases of deep learning are core to some of the merging phenomenon that we see these days and the models that the authors propose fails to capture that. I also think that a good toy model should leave the door open to generalizations and getting closer to real world practice (for example, for the random features model of deep learning, there is a very obvious way to move towards making it more realistic). But the models that the authors propose is too abstract and it is not clear what simplifications are made to the real problem to arrive at the proposed model.\n\n- Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\n- It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\n- The notations are a bit confusing (i, j, k, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations (p_d, p_r, etc.). It will also help explain the method.\n\n- The authors \"prove\" GDE in their model without the explicit assumption of calibration. But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\n- The theoretical understanding of feature learning is not as rudimentary as the authors claim. For example,\n\n[1] Alex Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent, 2022.\n\n[2] Zhichao Wang, Andrew Engel, Anand Sarwate, Ioana Dumitriu, and Tony Chiang. Spectral evolution and invariance in linear-width neural networks, 2022.\n\n[3] Eshaan Nichani, Alex Damian, and Jason D Lee. Provable guarantees for nonlinear feature learning in three-layer neural networks, 2023.\n\n\n[4] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. Learning two-layer neural networks, one (giant) step at a time, 2023.\n\n[5] Behrad Moniri, Donghwan Lee, Hamed Hassani, and Edgar Dobriban, A theory of non-linear feature learning with one gradient step in two-layer neural networks, 2023.\n\nand many more.\n\nQuestions: - Looking at figure 3 (a) and 3 (c), it seems that the observations made from them are not that significant. Am I missing something? How does the choice of the thresholds affect your observations?", "labeling_timestamp": "2026-01-11T16:30:28.011924", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The authors directly defend the figures and their interpretation, clarifying the observed patterns in 3(a) and 3(c) and asserting that the figures do support the claims, thereby disagreeing with the reviewer's assessment that they are not convincing.", "evidence": "\"As such, we believe that it’s fair to say that data with fewer features tend to have higher confidence.\" | \"For 3(c), we claim that the more features the models share, the lower bound on their shared errors becomes higher, which can be clearly observed in the figure... We believe that the figure supports this claim.\"", "combined_author_response": "Thank you for the continued engagement and dedication to the review process! We address your remaining concerns as follows:\n\n> 0\n\nApologies for missing the first part of your question earlier. In 3 (a), the observation we are relying on is that data points with a small number of features are much more likely to have very high confidence under the ensemble and vice versa. For a concrete example, it is much more likely for a data point with 0.8 confidence to have 25 features than a data point with 0.4 confidence to have 25 features. As such, we believe that it’s fair to say that data with fewer features tend to have higher confidence. For 3(c), we claim that the more features the models share, the lower bound on their shared errors becomes higher, which can be clearly observed in the figure. This *does not* mean that the fewer features the models share, their shared errors would necessarily be smaller. Instead, both models would make random guesses (recall that we make similar assumptions in the conceptual model). This means that a pair of models can have a high or low amount of shared errors due to randomness, but the lower bound on the shared error will become higher if they share more features. We believe that the figure supports this claim.\n\n> 1\n\nThank you for clarifying what you meant about the limitations. We agree with your assessment of NTK and shallow neural networks, and we absolutely agree that we should be transparent with the limitations of the framework. Appendix D.3 (previously D.4) is meant to discuss the limitations of the model but we see that it may not be as clear as we hoped. As such, we have expanded Appendix D.3 to be more explicit about these limitations. This was already referred to at the beginning of section 5 where we introduced the model and in the conclusion, but once again, due to space limits, we could not include them in the main text.\n\n> 2\n\nThank you for clarifying this and pointing out the notation issues. We have streamlined the notations in Section 3 for better clarity so the footnotes are no longer needed.\n\nSpecifically, we have changed to use $C$ for the number of classes instead of $K$ and use $K$ to denote the number of top principal components, so all capital letters refer to a fixed number (e.g., $N$ for the total number of data, $M$ for the number of models, etc). Lower case of $N,M,K,T$ will refer to a particular index for what the object the capital letters refer to, so $k$ will always refer to the feature, $m$ will always refer to the model and $n$ will always refer to data, etc.\n\nOne exception is the k-partite graph. Since this is the standard reference to the problem we use $\\mathtt{k}$ to differentiate it.\nAnother one is that $i,j, a, b$ are used instead of $m$ and $k$ to refer to a pair of models and features around equation 2.\nWe believe that this should resolve the ambiguity but if there are any concerns regarding the notation left please let us know.\n\nRegarding the table of notation, we fully agree that a table would be great to have. Unfortunately, so far we have not been able to find a way to squeeze it into the text. And we will definitely include it in the extended version.\n\n------------\n\nWe hope these revisions and clarifications address your concerns. Please let us know if you have any further questions.\n\n---\n\nThank you for your thoughtful feedback! We are glad that you still find our work original and creative.\n\n> many assumptions went into the combinatorial analysis.\n\nWe fully agree that many assumptions were made. The way we see it is that deep learning's complexity necessitates some level of simplification for tractable analysis. In this case, since the resulting framework is still useful for understanding empirical phenomena, we feel the assumptions are justified. \n\n> The framework is less useful for understanding the learning process of networks…\n\nThis is correct. The framework as it stands does not describe the learning process, which is an extremely complicated subject itself. However, we postulate that techniques similar to [1] could be used to study the optimization of a feature model like ours in the future.\n\n> typos\n\nThank you for the close reading and for catching these typos! We have fixed them in the revision.\n\n> have you considered modeling the rare-dominant variation as a spectrum instead of a binary?\n\nThis is an excellent question! We have indeed considered the possibility of modeling it as a spectrum, which we discussed in Appendix D.1 and D.4. In our preliminary investigation, we found that modeling the feature spectrum may require introducing more free parameters (e.g., Zipf’s distribution), and can drastically increase the complexity of analysis. Given that this is our first foray into the topic, we opted for the simpler binary model. This is an important problem and we plan to explore it in more detail in future works.\n\nWe hope these responses have addressed some of the questions and we are open to further discussion. Thank you again for your constructive feedback!\n\n**Reference**\n\n[1] Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. Allen-Zhu et al.\n\n---\n\n> The theoretical understanding of feature learning is not as rudimentary as the authors claim\n\nWe sincerely apologize for the characterization. Perhaps “rudimentary” is not the best description. We have revised the phrase to be “still in early development” to better convey the sentiment. However, we do feel that it is reasonable to say that we still have much to understand about feature learning. \n\nTo briefly address the specific works you mentioned: these models generally assume the input data are isotropic Gaussian and the function class is a 2 layer MLP. The feature is usually a linear function of the input and often special training algorithms are needed (e.g., layer-wise training). A notable exception is [3] which learns non-linear features but still requires layer-wise training. \nMost of these works are quite recent (and all came out after we started this project): [3,4,5] showed up in October 2023 and [2] was updated in November 2023. In general, it is hard to say how well they encapsulate the full complexity of deep learning, or how feasible it is to apply them to modern architectures and datasets. Nonetheless, the goal of this work is not to compare existing theoretical works on the topic or discuss their merits and shortcomings, but rather to identify what realistic (perhaps strong) modeling assumptions to make when studying feature learning. Given the recently increasing interest in the subject, we view these works and our approach as complementary components of a broader exploration into feature learning. We have added a discussion of these works to Appendix D.1, but would be happy to revise them if you have further suggestions.\n\nWe hope these responses address your concerns and we are open to further discussion. Thank you again for your constructive feedback!\n\n**Reference**\n\n[6] Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. Allen-Zhu et al.\n\n---\n\nThank you for your detailed review and thoughtful feedback. We are glad that you find the problem we study timely and important. Below are our responses to your concerns:\n\n> I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of abstraction\". However, I think the model proposed in this paper is too simplistic…\n\nWe definitely agree that our model is unconventional and more abstract than most existing works but similar assumptions have been made in prior work [6]. As is the case for all theoretical models, simplifications are required to keep analysis tractable. Despite the simplification, our model already exhibits some interesting behaviors and offers insight into empirical phenomena so we believe that it adds value to the current discussion on feature learning. \n\nIn the case of random feature models, we feel that it is a little unfair to say in hindsight that there was a very obvious way to move towards making it more realistic. When NTK first came out in 2018, it was thought to be very promising. Indeed, despite its simplification, it offers clear value in the analysis of some phenomena in deep networks. However, it only later became clear that there was a big gap between NTK and real deep learning (despite many efforts to close this gap). More recent works (many of which you already brought up) rely on quite distinct analysis tools than those used in NTK and, still, it’s not clear if any of them is close to real-world practice.  As such, while we agree (and have acknowledged in Section 5, Appendix D.1 and D.4) that there is a gap between our proposed framework and practice, we feel it’s a tall order for the current state of deep learning theory to have no strong assumptions.  Just like NTK analysis, we hope that our approach will ultimately prove valuable (albeit along different dimensions), despite its simplicity.\n\n> Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\nThe primary goal of our work is to investigate the effect of different features being learned by different models on GDE and the calibration of deep ensembles, both of which are robust empirical observations. While our model does not directly provide insights into training, understanding these feature interactions may indirectly inform training strategies in the future. For example, [6] shows potential avenues for how observations like ours may transfer to training.\n\n> It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\nAs detailed in Appendix F.10 (referred to at the end of Section 4), our conclusions are robust across a range of parameter settings so we believe that the settings used in our paper should be appropriate for most situations.\n\n> The notations are a bit confusing ($i, j, k$, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations ($p_d, p_r$, etc.). It will also help explain the method.\n\nWe apologize for the confusion. There is already a centralized list of notations in Appendix A which we could not include in the main text due to space constraints (this is referred to in the second paragraph of section 5). Beyond this, could you be more specific about where the $i,j,k$ are causing confusion so we could better address the confusion? $i,j,k$ are used to denote an arbitrary member of the ordered collection so we feel this is in accordance with the standard usage.\n\n> But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\nWe want to clarify that we *do not* claim that the assumptions we make are weaker than calibration (since neither is a clear superset of the other), but we do believe that it sheds more light on the underlying reasons behind GDE. Only stating the model is calibrated does not tell us much about the model or the data, but our assumptions do. In fact, our framework tells us how one can break our assumption (and calibration) through data distribution interventions. Of course, it is an important open question why so many natural distributions seem to satisfy these assumptions.\n\n---\n\nThank you for the support of our paper! We are glad that you find our work valuable.\n\nRegarding Figure 1, to better address your concerns, could you elaborate on which aspects of the figure you found unclear? This will help us make more targeted improvements. \n\nThe goal of Figure 1 is to demonstrate that our definition of features can capture images with redundant information and images that contain uncommon instances of a given class. To make this clearer, we have updated the figure in the paper with texts to better reflect the core idea. Does this resolve your concerns? If not, we’d be happy to make more adjustments.\n\n---\n\nThank you for your supportive feedback on our work. We are glad that you find our new perspective on feature learning interesting. Below are our responses to your questions:\n\n> how important are these hyperparameters? How to properly set them?\n\nIn Appendix F.10, we conducted an ablation study and found that our qualitative conclusions are robust to variations in these hyperparameters. Therefore, we believe the values used in our paper should be appropriate for most applications.\n\n> Currently theoretical framework analyzes binary classification, does the analysis extend to multi-class classification?\n\nWe discuss the necessary steps for the extension to multi-class classification in Appendix D.3. \n\n>Does the framework allow for introducing distribution shifts and etc?\n\nThis is a great question. We believe our framework can accommodate distribution shifts. For instance, the simplest form of covariate shift one could introduce is to add a new set of features that are not present in the training data with some probability at test time. Since the model has not seen these new features during training, we may assume that individual members of the ensemble will make random predictions (but the prediction of different models will be correlated) if the data points only contain the new features (the same assumption for features that the models did not learn). One can also introduce a covariate shift on existing features but that would require imposing a non-uniform distribution over the existing features. We leave the exploration of these directions to future works.\n\nWe hope these answers clarify your question, and we are open to further discussion. Thank you again for your constructive feedback.\n\n---\n\nWe thank all the reviewers for their valuable time. We are glad that the reviewers found the results original, interesting, and timely.\n\nWe have modified Figure 1 and made minor revisions, and added some related works to the paper which are highlighted with orange in the text.\n\nWe will respond to each reviewer’s comments individually.", "author_response": "Thank you for your detailed review and thoughtful feedback. We are glad that you find the problem we study timely and important. Below are our responses to your concerns:\n\n> I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of abstraction\". However, I think the model proposed in this paper is too simplistic…\n\nWe definitely agree that our model is unconventional and more abstract than most existing works but similar assumptions have been made in prior work [6]. As is the case for all theoretical models, simplifications are required to keep analysis tractable. Despite the simplification, our model already exhibits some interesting behaviors and offers insight into empirical phenomena so we believe that it adds value to the current discussion on feature learning. \n\nIn the case of random feature models, we feel that it is a little unfair to say in hindsight that there was a very obvious way to move towards making it more realistic. When NTK first came out in 2018, it was thought to be very promising. Indeed, despite its simplification, it offers clear value in the analysis of some phenomena in deep networks. However, it only later became clear that there was a big gap between NTK and real deep learning (despite many efforts to close this gap). More recent works (many of which you already brought up) rely on quite distinct analysis tools than those used in NTK and, still, it’s not clear if any of them is close to real-world practice.  As such, while we agree (and have acknowledged in Section 5, Appendix D.1 and D.4) that there is a gap between our proposed framework and practice, we feel it’s a tall order for the current state of deep learning theory to have no strong assumptions.  Just like NTK analysis, we hope that our approach will ultimately prove valuable (albeit along different dimensions), despite its simplicity.\n\n> Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\nThe primary goal of our work is to investigate the effect of different features being learned by different models on GDE and the calibration of deep ensembles, both of which are robust empirical observations. While our model does not directly provide insights into training, understanding these feature interactions may indirectly inform training strategies in the future. For example, [6] shows potential avenues for how observations like ours may transfer to training.\n\n> It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\nAs detailed in Appendix F.10 (referred to at the end of Section 4), our conclusions are robust across a range of parameter settings so we believe that the settings used in our paper should be appropriate for most situations.\n\n> The notations are a bit confusing ($i, j, k$, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations ($p_d, p_r$, etc.). It will also help explain the method.\n\nWe apologize for the confusion. There is already a centralized list of notations in Appendix A which we could not include in the main text due to space constraints (this is referred to in the second paragraph of section 5). Beyond this, could you be more specific about where the $i,j,k$ are causing confusion so we could better address the confusion? $i,j,k$ are used to denote an arbitrary member of the ordered collection so we feel this is in accordance with the standard usage.\n\n> But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\nWe want to clarify that we *do not* claim that the assumptions we make are weaker than calibration (since neither is a clear superset of the other), but we do believe that it sheds more light on the underlying reasons behind GDE. Only stating the model is calibrated does not tell us much about the model or the data, but our assumptions do. In fact, our framework tells us how one can break our assumption (and calibration) through data distribution interventions. Of course, it is an important open question why so many natural distributions seem to satisfy these assumptions."}
{"claim": "The paper does not explore obvious generalizations (for example from random features models) to make the toy model closer to practical deep learning.", "claim_type": "methodology", "paper_id": "ze7DOLi394", "paper_title": "On the Joint Interaction of Models, Data, and Features", "paper_venue": "iclr2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "RUsLYJEhvF", "reviewer": "Reviewer_MLur", "review_text": "Summary: This paper introduces the interaction tensor, for empirically analyzing the interaction between data and model through features. Based on some observations using this tensor, they propose a very simple toy model (a combinatorial model) for feature learning. They show that this model also exhibits Generalization Disagreement Equality (GDE). Finally, the authors use their model to provide data distributions that break GDE in real world experiments.\n\nStrengths: - The problem of feature learning is quite important and there has been a lot of attempts for gaining a better theoretical understanding of it in recent years.\n\n- The authors are able to come up with a toy model that shows GDE which is a very important phenomenon (and it does need the explicit assumption of calibration).\n\n- The model that the authors propose is simple and it can be analyzed fully.\n\n- The paper is very well-written.\n\nWeaknesses: - I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of\nabstraction\". However, I think the model proposed in this paper is too simplistic. The implicit biases of deep learning are core to some of the merging phenomenon that we see these days and the models that the authors propose fails to capture that. I also think that a good toy model should leave the door open to generalizations and getting closer to real world practice (for example, for the random features model of deep learning, there is a very obvious way to move towards making it more realistic). But the models that the authors propose is too abstract and it is not clear what simplifications are made to the real problem to arrive at the proposed model.\n\n- Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\n- It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\n- The notations are a bit confusing (i, j, k, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations (p_d, p_r, etc.). It will also help explain the method.\n\n- The authors \"prove\" GDE in their model without the explicit assumption of calibration. But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\n- The theoretical understanding of feature learning is not as rudimentary as the authors claim. For example,\n\n[1] Alex Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent, 2022.\n\n[2] Zhichao Wang, Andrew Engel, Anand Sarwate, Ioana Dumitriu, and Tony Chiang. Spectral evolution and invariance in linear-width neural networks, 2022.\n\n[3] Eshaan Nichani, Alex Damian, and Jason D Lee. Provable guarantees for nonlinear feature learning in three-layer neural networks, 2023.\n\n\n[4] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. Learning two-layer neural networks, one (giant) step at a time, 2023.\n\n[5] Behrad Moniri, Donghwan Lee, Hamed Hassani, and Edgar Dobriban, A theory of non-linear feature learning with one gradient step in two-layer neural networks, 2023.\n\nand many more.\n\nQuestions: - Looking at figure 3 (a) and 3 (c), it seems that the observations made from them are not that significant. Am I missing something? How does the choice of the thresholds affect your observations?", "labeling_timestamp": "2026-01-11T16:30:40.445873", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors acknowledge the gap between their toy model and practical deep learning, agree that limitations exist, and say they considered some generalizations (e.g., feature spectra) and expanded the discussion in appendices. However, they did not incorporate those generalizations into the current paper and justify keeping the simpler model, leaving broader generalizations to future work.", "evidence": "“We have indeed considered the possibility of modeling it as a spectrum... we opted for the simpler binary model. This is an important problem and we plan to explore it in more detail in future works.”", "combined_author_response": "Thank you for the continued engagement and dedication to the review process! We address your remaining concerns as follows:\n\n> 0\n\nApologies for missing the first part of your question earlier. In 3 (a), the observation we are relying on is that data points with a small number of features are much more likely to have very high confidence under the ensemble and vice versa. For a concrete example, it is much more likely for a data point with 0.8 confidence to have 25 features than a data point with 0.4 confidence to have 25 features. As such, we believe that it’s fair to say that data with fewer features tend to have higher confidence. For 3(c), we claim that the more features the models share, the lower bound on their shared errors becomes higher, which can be clearly observed in the figure. This *does not* mean that the fewer features the models share, their shared errors would necessarily be smaller. Instead, both models would make random guesses (recall that we make similar assumptions in the conceptual model). This means that a pair of models can have a high or low amount of shared errors due to randomness, but the lower bound on the shared error will become higher if they share more features. We believe that the figure supports this claim.\n\n> 1\n\nThank you for clarifying what you meant about the limitations. We agree with your assessment of NTK and shallow neural networks, and we absolutely agree that we should be transparent with the limitations of the framework. Appendix D.3 (previously D.4) is meant to discuss the limitations of the model but we see that it may not be as clear as we hoped. As such, we have expanded Appendix D.3 to be more explicit about these limitations. This was already referred to at the beginning of section 5 where we introduced the model and in the conclusion, but once again, due to space limits, we could not include them in the main text.\n\n> 2\n\nThank you for clarifying this and pointing out the notation issues. We have streamlined the notations in Section 3 for better clarity so the footnotes are no longer needed.\n\nSpecifically, we have changed to use $C$ for the number of classes instead of $K$ and use $K$ to denote the number of top principal components, so all capital letters refer to a fixed number (e.g., $N$ for the total number of data, $M$ for the number of models, etc). Lower case of $N,M,K,T$ will refer to a particular index for what the object the capital letters refer to, so $k$ will always refer to the feature, $m$ will always refer to the model and $n$ will always refer to data, etc.\n\nOne exception is the k-partite graph. Since this is the standard reference to the problem we use $\\mathtt{k}$ to differentiate it.\nAnother one is that $i,j, a, b$ are used instead of $m$ and $k$ to refer to a pair of models and features around equation 2.\nWe believe that this should resolve the ambiguity but if there are any concerns regarding the notation left please let us know.\n\nRegarding the table of notation, we fully agree that a table would be great to have. Unfortunately, so far we have not been able to find a way to squeeze it into the text. And we will definitely include it in the extended version.\n\n------------\n\nWe hope these revisions and clarifications address your concerns. Please let us know if you have any further questions.\n\n---\n\nThank you for your thoughtful feedback! We are glad that you still find our work original and creative.\n\n> many assumptions went into the combinatorial analysis.\n\nWe fully agree that many assumptions were made. The way we see it is that deep learning's complexity necessitates some level of simplification for tractable analysis. In this case, since the resulting framework is still useful for understanding empirical phenomena, we feel the assumptions are justified. \n\n> The framework is less useful for understanding the learning process of networks…\n\nThis is correct. The framework as it stands does not describe the learning process, which is an extremely complicated subject itself. However, we postulate that techniques similar to [1] could be used to study the optimization of a feature model like ours in the future.\n\n> typos\n\nThank you for the close reading and for catching these typos! We have fixed them in the revision.\n\n> have you considered modeling the rare-dominant variation as a spectrum instead of a binary?\n\nThis is an excellent question! We have indeed considered the possibility of modeling it as a spectrum, which we discussed in Appendix D.1 and D.4. In our preliminary investigation, we found that modeling the feature spectrum may require introducing more free parameters (e.g., Zipf’s distribution), and can drastically increase the complexity of analysis. Given that this is our first foray into the topic, we opted for the simpler binary model. This is an important problem and we plan to explore it in more detail in future works.\n\nWe hope these responses have addressed some of the questions and we are open to further discussion. Thank you again for your constructive feedback!\n\n**Reference**\n\n[1] Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. Allen-Zhu et al.\n\n---\n\n> The theoretical understanding of feature learning is not as rudimentary as the authors claim\n\nWe sincerely apologize for the characterization. Perhaps “rudimentary” is not the best description. We have revised the phrase to be “still in early development” to better convey the sentiment. However, we do feel that it is reasonable to say that we still have much to understand about feature learning. \n\nTo briefly address the specific works you mentioned: these models generally assume the input data are isotropic Gaussian and the function class is a 2 layer MLP. The feature is usually a linear function of the input and often special training algorithms are needed (e.g., layer-wise training). A notable exception is [3] which learns non-linear features but still requires layer-wise training. \nMost of these works are quite recent (and all came out after we started this project): [3,4,5] showed up in October 2023 and [2] was updated in November 2023. In general, it is hard to say how well they encapsulate the full complexity of deep learning, or how feasible it is to apply them to modern architectures and datasets. Nonetheless, the goal of this work is not to compare existing theoretical works on the topic or discuss their merits and shortcomings, but rather to identify what realistic (perhaps strong) modeling assumptions to make when studying feature learning. Given the recently increasing interest in the subject, we view these works and our approach as complementary components of a broader exploration into feature learning. We have added a discussion of these works to Appendix D.1, but would be happy to revise them if you have further suggestions.\n\nWe hope these responses address your concerns and we are open to further discussion. Thank you again for your constructive feedback!\n\n**Reference**\n\n[6] Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. Allen-Zhu et al.\n\n---\n\nThank you for your detailed review and thoughtful feedback. We are glad that you find the problem we study timely and important. Below are our responses to your concerns:\n\n> I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of abstraction\". However, I think the model proposed in this paper is too simplistic…\n\nWe definitely agree that our model is unconventional and more abstract than most existing works but similar assumptions have been made in prior work [6]. As is the case for all theoretical models, simplifications are required to keep analysis tractable. Despite the simplification, our model already exhibits some interesting behaviors and offers insight into empirical phenomena so we believe that it adds value to the current discussion on feature learning. \n\nIn the case of random feature models, we feel that it is a little unfair to say in hindsight that there was a very obvious way to move towards making it more realistic. When NTK first came out in 2018, it was thought to be very promising. Indeed, despite its simplification, it offers clear value in the analysis of some phenomena in deep networks. However, it only later became clear that there was a big gap between NTK and real deep learning (despite many efforts to close this gap). More recent works (many of which you already brought up) rely on quite distinct analysis tools than those used in NTK and, still, it’s not clear if any of them is close to real-world practice.  As such, while we agree (and have acknowledged in Section 5, Appendix D.1 and D.4) that there is a gap between our proposed framework and practice, we feel it’s a tall order for the current state of deep learning theory to have no strong assumptions.  Just like NTK analysis, we hope that our approach will ultimately prove valuable (albeit along different dimensions), despite its simplicity.\n\n> Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\nThe primary goal of our work is to investigate the effect of different features being learned by different models on GDE and the calibration of deep ensembles, both of which are robust empirical observations. While our model does not directly provide insights into training, understanding these feature interactions may indirectly inform training strategies in the future. For example, [6] shows potential avenues for how observations like ours may transfer to training.\n\n> It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\nAs detailed in Appendix F.10 (referred to at the end of Section 4), our conclusions are robust across a range of parameter settings so we believe that the settings used in our paper should be appropriate for most situations.\n\n> The notations are a bit confusing ($i, j, k$, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations ($p_d, p_r$, etc.). It will also help explain the method.\n\nWe apologize for the confusion. There is already a centralized list of notations in Appendix A which we could not include in the main text due to space constraints (this is referred to in the second paragraph of section 5). Beyond this, could you be more specific about where the $i,j,k$ are causing confusion so we could better address the confusion? $i,j,k$ are used to denote an arbitrary member of the ordered collection so we feel this is in accordance with the standard usage.\n\n> But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\nWe want to clarify that we *do not* claim that the assumptions we make are weaker than calibration (since neither is a clear superset of the other), but we do believe that it sheds more light on the underlying reasons behind GDE. Only stating the model is calibrated does not tell us much about the model or the data, but our assumptions do. In fact, our framework tells us how one can break our assumption (and calibration) through data distribution interventions. Of course, it is an important open question why so many natural distributions seem to satisfy these assumptions.\n\n---\n\nThank you for the support of our paper! We are glad that you find our work valuable.\n\nRegarding Figure 1, to better address your concerns, could you elaborate on which aspects of the figure you found unclear? This will help us make more targeted improvements. \n\nThe goal of Figure 1 is to demonstrate that our definition of features can capture images with redundant information and images that contain uncommon instances of a given class. To make this clearer, we have updated the figure in the paper with texts to better reflect the core idea. Does this resolve your concerns? If not, we’d be happy to make more adjustments.\n\n---\n\nThank you for your supportive feedback on our work. We are glad that you find our new perspective on feature learning interesting. Below are our responses to your questions:\n\n> how important are these hyperparameters? How to properly set them?\n\nIn Appendix F.10, we conducted an ablation study and found that our qualitative conclusions are robust to variations in these hyperparameters. Therefore, we believe the values used in our paper should be appropriate for most applications.\n\n> Currently theoretical framework analyzes binary classification, does the analysis extend to multi-class classification?\n\nWe discuss the necessary steps for the extension to multi-class classification in Appendix D.3. \n\n>Does the framework allow for introducing distribution shifts and etc?\n\nThis is a great question. We believe our framework can accommodate distribution shifts. For instance, the simplest form of covariate shift one could introduce is to add a new set of features that are not present in the training data with some probability at test time. Since the model has not seen these new features during training, we may assume that individual members of the ensemble will make random predictions (but the prediction of different models will be correlated) if the data points only contain the new features (the same assumption for features that the models did not learn). One can also introduce a covariate shift on existing features but that would require imposing a non-uniform distribution over the existing features. We leave the exploration of these directions to future works.\n\nWe hope these answers clarify your question, and we are open to further discussion. Thank you again for your constructive feedback.\n\n---\n\nWe thank all the reviewers for their valuable time. We are glad that the reviewers found the results original, interesting, and timely.\n\nWe have modified Figure 1 and made minor revisions, and added some related works to the paper which are highlighted with orange in the text.\n\nWe will respond to each reviewer’s comments individually.", "author_response": "Thank you for your detailed review and thoughtful feedback. We are glad that you find the problem we study timely and important. Below are our responses to your concerns:\n\n> I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of abstraction\". However, I think the model proposed in this paper is too simplistic…\n\nWe definitely agree that our model is unconventional and more abstract than most existing works but similar assumptions have been made in prior work [6]. As is the case for all theoretical models, simplifications are required to keep analysis tractable. Despite the simplification, our model already exhibits some interesting behaviors and offers insight into empirical phenomena so we believe that it adds value to the current discussion on feature learning. \n\nIn the case of random feature models, we feel that it is a little unfair to say in hindsight that there was a very obvious way to move towards making it more realistic. When NTK first came out in 2018, it was thought to be very promising. Indeed, despite its simplification, it offers clear value in the analysis of some phenomena in deep networks. However, it only later became clear that there was a big gap between NTK and real deep learning (despite many efforts to close this gap). More recent works (many of which you already brought up) rely on quite distinct analysis tools than those used in NTK and, still, it’s not clear if any of them is close to real-world practice.  As such, while we agree (and have acknowledged in Section 5, Appendix D.1 and D.4) that there is a gap between our proposed framework and practice, we feel it’s a tall order for the current state of deep learning theory to have no strong assumptions.  Just like NTK analysis, we hope that our approach will ultimately prove valuable (albeit along different dimensions), despite its simplicity.\n\n> Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\nThe primary goal of our work is to investigate the effect of different features being learned by different models on GDE and the calibration of deep ensembles, both of which are robust empirical observations. While our model does not directly provide insights into training, understanding these feature interactions may indirectly inform training strategies in the future. For example, [6] shows potential avenues for how observations like ours may transfer to training.\n\n> It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\nAs detailed in Appendix F.10 (referred to at the end of Section 4), our conclusions are robust across a range of parameter settings so we believe that the settings used in our paper should be appropriate for most situations.\n\n> The notations are a bit confusing ($i, j, k$, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations ($p_d, p_r$, etc.). It will also help explain the method.\n\nWe apologize for the confusion. There is already a centralized list of notations in Appendix A which we could not include in the main text due to space constraints (this is referred to in the second paragraph of section 5). Beyond this, could you be more specific about where the $i,j,k$ are causing confusion so we could better address the confusion? $i,j,k$ are used to denote an arbitrary member of the ordered collection so we feel this is in accordance with the standard usage.\n\n> But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\nWe want to clarify that we *do not* claim that the assumptions we make are weaker than calibration (since neither is a clear superset of the other), but we do believe that it sheds more light on the underlying reasons behind GDE. Only stating the model is calibrated does not tell us much about the model or the data, but our assumptions do. In fact, our framework tells us how one can break our assumption (and calibration) through data distribution interventions. Of course, it is an important open question why so many natural distributions seem to satisfy these assumptions."}
{"claim": "The proposed method lacks necessary motivation and the paper fails to convincingly justify why this approach is needed.", "claim_type": "subjective", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ppMpJyIYRZ", "reviewer": "Reviewer_m8JS", "review_text": "Summary: The paper introduces a method to enhance the out-of-distribution performance of graph neural networks (GNN) by learning to understand distribution shifts instead of addressing the assumed ones. To achieve this, the Mixture of Experts architecture is integrated into the GNN, supplemented by an alignment procedure to recognize the shift. Empirical experiments are conducted to validate the theoretical assertion.\n\nOn the whole, I believe the proposed method lacks the necessary motivation and its novelty isn't substantial enough to meet the standard.\n\nStrengths: - The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\n- Real-world datasets back the claims through experiments.\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\n\nWeaknesses: - The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \"previous research has concentrated on addressing specific types of distribution shifts.\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \"environment generators\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\n\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\n\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\n\nQuestions: Please check Weaknesses.", "labeling_timestamp": "2026-01-11T16:30:43.473166", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The authors explicitly defend and justify the motivation, argue that heterogeneous shifts are important and previously overlooked, provide theoretical and empirical support, and state they do not agree that lack of motivation should cause rejection.", "evidence": "\"We understand the reviewer's clarification concerns, however, we don't agree that they, based on our justificaton, are the cause of rejection. We believe nuanced heterogeneous shifts are common, important, yet being typically ignored in the research domain of distribution shifts, we made these argument clear and also promise to refine.\"", "combined_author_response": "Dear reviewer NLg1,\n\nAs the discussion period is closing soon, we hope to engage with you and check if your concerns are addressed.\n\nWe sincerely appreciate your time and attention!\n\nBest,\n\nAuthors of Paper 1560\n\n---\n\nThanks for reading our response! We would also appreciate your patience for reading the two points below \n\n---\n\n**Regarding point #1:**\n\nYes, we agree that the concept of heterogeneous shifts is relatively new for the current studies on distribution shifts, however, this is not new for studies on network patterns (Newman, 2003; Leskovec et al., 2005; 2007; Peel et al., 2017). In fact, the ignorance of such nuanced heterogeneous shifts in the previous studies instead emphasises our motivation and the potential impact of this work. \n\nFrom a causality perspective, distribution shifts can naturally happen in the instance level when (1) additional causal variables, beyond environmental factors and randomized noise, influence these shifts, or (2) multiple causal variables simultaneously affect the shifts with different strengths. Without modeling these explicitly, the mitigation of distribution shifts can easily fail. \n\nIn terms of our presentation, we have illustrated these cases of interest in abstract and introduction, we made further explanations in Section 3.1, we also present the specific results of distribution types in Figure 3 (b). \n\n\n**Regarding point #2:**\n\nWe think we can all agree that, if the ground truth of the instance shifts is available on the real-world datasets, it would be crystal clear to see where the improvement comes from since we can conduct case study to compare our method and the baselines on instances with nuanced distribution shifts, to see the influence of the modeling these heterogeneous shifts. \n\nHowever, with such ground truth not available, we had to seek other seemingly less intuitive but also in-depth way to illustrate the insights (esp Section 4.3) as mentioned above in our previous response. This is also why we designed the synthetic experiments at the first place. We think we did try hard to explain the underlying rationale with the ground truth being absent.\n\n----\n\n**Refinement**: We can of course add a causal graph in the our assumption section to make the concept more clear. And we can illustrate more if you could let us know the specific obstable to understand our mechanism, which will be extremely helpful.\n\n----\n\n### **Summary/TL;DR**\nWe understand the reviewer's clarification concerns, however, we don't agree that they, based on our justificaton, are the cause of rejection. We believe nuanced heterogeneous shifts are common, important, yet being typically ignored in the research domain of distribution shifts, we made these argument clear and also promise to refine. While explaining the underlying rationale is hindered by the lack of ground truth, we did try hard to dissection it from the gating model, the invariant representations generated, and more ablations in the appendix. We respect the reviewer's current opinion. Still, reconsideration will be greatly appreciated.\n\n---\n\nWe thank the reviewer once again! We also enjoyed the process of making our work more sound from your suggestions. And your approval surely means a lot to us.\n\nWe do apologize for not making our solutions towards the limitations clear enough. We summarize them into a short table, hopefully could alleviate your concerns a bit more.\n\n| Limitation| Potential solutions | Location of discussion/action |\n|:---|:---|:---|\n|**Coverage of the transformations** | 1) Extend the coverage by adding representative transform functions. 2) Or include tranform functions based on domain knowledge or a few samples from target distribution | Appendix F, paragraph #3|\n|**Complexity of the transformations** | Make experts dedicated to different hyperparameters for the same type of transformation | Argument options are available in our codebase. Will make it more detailed in the experimental settings. |\n|**Label distributional shift**|Integrate the objective of the existing methods studying labe distributional shifts into our framework. |Appendix F, paragraph #4 |\n\n----\n\nWe completely agree on the existence of these limitations and we will move some limitations (esp #1) to the main paper in our final version. While they could be important in the practice, they are, in our perspective, fair \"side effects\" considering the benefits (i.e., mitigating multiple and nuanced distribution shifts and better interpretability), and may not be the central part of our novelty and main contribution (i.e., the proposal of an equivalent mixture, the concept of referential invariant representations, as well as the training framework). We are also eager to further improve other aspects.\n\nOnce again thank you so much for your support!!\n\n---\n\nDear Reviewer m8JS,\n\nA gentle nudge that we would like to know if our response adequately addresses your concerns. \n\nYour time and feedback is greatly appreciated!\n\nThank you sincerely,\n\nAuthors of Paper 1560\n\n---\n\nWe deeply appreciate your approval. Your suggestions definitely inspired us a lot and have greatly improved our work.\n\nA few additional notes for the further comments:\n\n- Regarding **point #2**: In node classification tasks, the BCE objective already considers node embeddings other than the graph embedding of the k-hop subgraph. What we described previously refers to the computational graph (i.e., k-hop message passing), which generates node embeddings. We apologize for any confusion.\n\n- Regarding **point #3**: Yes it would be quite interesting to see how graph pretraining methods perform on the current OOD benchmarks. Graph pretraining methods like GCC and GPT-GNN also consider graph extrapolation to some extent, e.g., through subgraph extraction and masked attributes/structures. The key difference between graph pretraining and generalization may lie in their different focuses on **expressiveness** and **invariance**. While these two aspects do not always conflict, ensuring invariance w.r.t. a certain type of extrapolation might affect expressiveness (if the change is relevant to labels), and vice versa. To seek a balance in between, one might need prior knowledge in which types of transformations may or may not be sensitive to the labels (and perhaps build experts with different goals to make ensure invariance or expressivess). We believe there is still a lot to explore in this domain.\n\nOverall, we are grateful for your positive stand on this work. We believe that our [current version](https://openreview.net/pdf?id=QQ5eVDIMu4), incorporating opinions from you and other reviewers, is sound and well-refined. We are also committed to further improving it. We would appreciate your support based on our current version! \n\nThank you!\n\n---\n\nDear Reviewers of Paper 1560,\n\nWe hope this message finds you well.\n\nAs the discussion phase approaches its end, we hope you find our responses useful. We would like to ask if the issues have been addressed.\n\nWe understand that the discussion time is short, and some of you might be enjoying holidays at the moment. We apologize for posting our responses a bit late as we aimed to address your concerns clearly.\n\nWe sincerely appreciate your time and attention!\n\nBest regards,\n\nAuthors of Paper 1560\n\n---\n\n**Comment 4: Related works on graph transfer learning.**\n\nYes! We agree that graph transfer learning is a relevant topic. Thank you for providing the additional related work! We added the discussion on these works in our revision, and we repeat it here for your convenience:\n\n```It is also worth mentioning that graph domain adaptation (Zhang et al., 2019; Wu et al., 2020), different from the problem studied in this work, commonly relies on limited labeled samples from target datasets for improved transferability. For instance, to generate domain adaptive network embedding, DANE (Zhang et al., 2019) uses shared weight graph convolutional networks and adversarial learning regularization, while UDA-GCN (Wu et al., 2020) employs attention mechanisms to merge global and local consistencies.```\n\nMoreover, we believe this is actually relevant to `Comment & Question 1` regarding the generality of the distribution shifts covered by the transform functions. In our future works, we added the following discussion:\n\n```...Leveraging a few samples from target distribution (i.e., domain adaptation). Specifically, we can leverage the samples from the target distribution to inform the selection or construction of transform functions, which can better guarantee the distribution shifts are covered by the transform functions.  For example, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space. We believe this would be an interesting future direction.```\n\n---\n\n# Summary\n\nOnce again thank you very much for helping us position our work and reflect the literature better. We hope all of your concerns are solved and we are happy to engage further if there are any other points we missed!\n\n---\n\n### **Reference**\n\n[1] Unsupervised Domain Adaptive Graph Convolutional Networks. Wu et al. WWW 2020\n\n[2] DANE: Domain adaptive network embedding. Zhang et al. IJCAI 2019.\n\n[3] Tong Zhao, Yozen Liu, Leonardo Neves, Oliver J. Woodford, Meng Jiang, and Neil Shah. Data augmentation for graph neural networks. AAAI, 2021.\n\n---\n\nWe are grateful for your positive feedback and detailed suggestions! We provide responses below to address your remaining concerns. \n\n---\n\n### **Comment & Question 1: Generality of the distribution shifts covered by the transform functions.**\n\nGood question! We provide the response from two angles:\n\n\n**(a) For general domain**: In our experiments, we mainly use the five stochastic transform functions, which are universal graph augmentations as listed in Zhao et al., (2021) [3]. In our code implementation, we have also included additional transform functions as shown in Appendix B. We believe these transform functions, while not exhaustive, still cover a wide range of distribution shifts observing from our experimental results.\n\nNevertheless, we agree that the real graph distribution shifts can go beyond any possible combinations of the predefined transform functions. In that case, the assumption may not hold, meaning that GraphMETRO may not capture and precisely mitigate the unknown distribution shift. This scenario could always possibly exist due to the lack of information about the testing distribution or its domain knowledge. We include it as a limitation in Appendix 5, while we further discuss how we could alleviate the problem with additional information.\n\n**(b) For specific domains where additional knowledge is available**: In fact, knowing the tendency of the distribution shifts, such as increasing malicious users in a trading system, would be very helpful in constructing the transform functions that can cover the target distribution shifts well. We believe that such knowledge can come from two sources: \n- **Domain knowledge**, e.g., on molecular datasets, the transform function could be adding additional carbon structure to a molecule (while preserving its functional groups). Or, in a particular social network, transform functions can be defined from known user behaviors. \n- **Leveraging a few samples from target distribution**. This is in fact in line with the reviewer’s Comment #4 regarding graph transfer learning. Specifically, with the guide from a few target samples, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space.\n\n We hope this response can alleviate your concern about our applicability.\n\n---\n\n### **Comment & Question 2: The mechanism of GaphMETRO in handling heterogeneous shifts within one graph.**\n\nGood question! For simplicity, suppose we have two transform functions, i.e., adding edges and dropping edges. Given a node classification task and the objective in Eq. (3), the model is trained on the extrapolated datasets based on the transform functions. After that, given an unseen graph with imbalanced distribution shifts, the gating model outputs scores to identify nodes likely to experience increasing or decreasing degrees, while others might adhere to the original distribution. Then, for each node, each expert takes its multihop subgraph, and outputs its referential invariant representation w.r.t. the correposing transform function. These expert outputs and gating model results are then aggregated to form the final representation. Assuming accurate predictions by the gating function, nodes in denser/sparser subgraphs are represented by expert models corresponding to adding/dropping edges. As each expert is trained to create invariant representations, the final node representations remain unaffected by their individual distribution shifts.\n\n---\n\n### **Comment 3: How to leverage graph pretraining.**\n\nIf we understand correctly, the reviewer was asking how to leverage a pretrained model to further aid the training of GraphMETRO. Please let us know if otherwise. \n\nThis is in fact an interesting point! We believe a model pretrained on a wide variety of data can be very helpful to initialize the gating model, which is required to output the mixture of a node or graph (after it is finetuned on the extrapolated dataset). By enhancing the gating model's predictive capability regarding mixtures, GraphMETRO's final representation should become more resilient. This becomes particularly advantageous when dealing with graphs not previously encountered in the extrapolated dataset. Moreover, it is possible that the pretrained model will also benefit the expert models, while one minor concern would be that the expert model may tend to be similar instead of dedicating to generating invariant representation w.r.t. their corresponding transform function. We added the above discussion to Appendix F as a future work. Thanks again for this comment!\n\n---\n\n### **Question 4: Why does the test accuracy vary across different transformations?**\n\nGreat question! Here we summarize three possible reasons:\n\n- **Information Preservation in Transformations:** Certain transformations retain more informative features than others. For instance, in the REDDIT-BINARY (graph classification task), the random subgraph transformation may retain more graph label-related information compared to dropping edges, as the latter tends to lose more global information. This discrepancy in testing performance, where dropping edges outperforms random subgraph extraction, could be due to the preservation of crucial information. However, conclusions may vary across datasets or tasks depending on how information influences final predictions. For CiteSeer (a node classification task), a random subgraph might preserve more local node information, potentially explaining why its testing performance surpasses dropping edges in this specific task.\n\n- **Complexity of transformation:** Certain transformations inherently generate more diverse graphs than others. If the model lacks the expressive capacity to capture such diversity, it may lead to a decline in testing performance.\n\n- **Model Sensitivity:** Certain transformations may be easier for a model to learn due to compatibility with specific model architectures. This extends beyond transformation complexity and emphasizes how different model architectures may prefer learning particular distributions from one of the extrapolated datasets, which can also contribute to the difference in the testing performance.\n\nWe included the above discussion to Appendix F: Open Discussions. We hope this response can answer your question and improve the soundness of our work.\n\n---\n\n# Summary \n\nWe are grateful for your time and insightful suggestions! \n\nWe would like to highlight that our main contribution is framing the graph generalization problem on top of an equivalent mixture, a simple yet novel and tractable \"middle ground\", as well as proposing the training framework which effectively guarantees the generalization. While our method relies on a set of predefined transform functions, we believe they cover a wide range of distribution shifts based on our empirical results. Also, we agree that there could be some scenarios where the transform functions may not cover complex distributions, and we discuss two future directions and include them into our future works. Moreover, while selecting the hyperparameters for the transform functions introduce extra complexity, the issue could be minor in practice and we also conduct more experiments to justify the applicability better. Finally, we address several questions about clarification and presentation, as well as including more future works. \n\nLastly, we prudently ask you to reevaluate our work given the clarification in our responses, which we also updated our paper correspondingly. Overall, we believe our work makes good contributions to the field of graph distribution learning by proposing a novel and effective solution, and we would appreciate your reconsideration on this point. Thank you for your efforts again!\n\n---\n\n### **Reference**\n\n[1] Menon, Aditya Krishna, et al. \"Long-tail learning via logit adjustment.\" International Conference on Learning Representations. 2020.\n\n[2] Cao, Kaidi, et al. \"Learning imbalanced datasets with label-distribution-aware margin loss.\" Advances in neural information processing systems 32 (2019).\n\n[3] DARTS: Differentiable Architecture Search. Hanxiao Liu, Karen Simonyan, Yiming Yang. 2018.\n\n---\n\n**(3) What is the complexity of the transform functions and how does it affect generalization?** \n\nInteresting question! In fact, our implementation and framework could easily avoid selecting hyperparameters on the transform functions. Specifically, we can make multiple transform functions of the same type with different ranges of hyperparameters. Specifically, GraphMETRO allows three edge dropping transform functions, $\\tau_1^{\\alpha_1}, \\tau_2^{\\alpha_2}, \\tau_3^{\\alpha_3}$, where $\\alpha_i$ (i=1, 2, 3) are three different ranges of edge dropping probabilities, e.g., [0.1,0.3], [0.3, 0.6], [0.6,0.9], representing different transform extents. Thus, given an input from the validation dataset, the gating model will highlight the transform function which simultaneously selects the corresponding hyperparameter that matches the distribution of the validation set. Interestingly, this idea is in the same spirit as how DARTS [3]  proposes to perform architecture search by formulating the task in a differentiable manner. \n\nIn our previous experiments, we did try this scheme where we replaced a single edge dropping transform function with the ratio range [0.3, 0.5] to three transform functions as mentioned above. While we didn’t see a significantly different performance in that case, we believe this would be a flexible solution which avoids the need to conduct hyperparameter selection. \n\nWe include the above discussion in our open discussion and future works (Appendix F). We hope this response can alleviate your concern about the applicability of our method. \n\n---\n\n### **Comment 4: Limitations of our work**\n\nGreat point! We think the issue of label distributional shift, while important, is orthogonal and complementary to the focus of our current study. To elaborate, label distributional shifts exert analogous impacts across various modalities, such as graphs or images. Moreover, existing methods [1,2] designed to tackle label distributional shifts can be seamlessly integrated into our proposed framework. Such integration would necessitate minimal adjustments, potentially involving modifications to the loss function or the training pipeline. We added this as a future work in Appendix F.\n\n---\n\n### **Question 1: Statistical significance of the results on Table 1**\n\nThanks for the question! We compute the p-value of our method against the best baselines method as follows:\n\n| | WebKB | Twitch | Twitter | SST2 | \n|:--|:--|:--|:--|:--|\n|p-value|< 0.001| 0.023 | 0.042| 0.081|\n\nGiven the cut-off threshold as 0.05, we believe the performances of GraphMETRO are statistically significant on WekGB, Twitch, and Twitter datasets, while on the SST2, we see relatively weak evidence. We added the p-value results to our revision and hope our response can alleviate your concern on our improvements.\n\n---\n\n###  **Question 2: Can we say we use ERM for the node classification even if nodes and their labels are not i.i.d.?** \n\nIf we understand correctly, the reviewer is asking for clarification on the 2nd term of our objective. Please let us know if otherwise. Here, our thinking is that the cross-entropy loss for node classification already assumes node labels are conditionally independent given the model (the negative log-likelihood is a sum over the labeled nodes in training). Then, we use the same assumption of cross-entropy on Empirical Risk Minimization (ERM). That is, for a given model we must also minimize the error variance across nodes. The task is then to find the model with the best performance and small variance.\n\n---\n\n### **Question 3: Numerical results on Figure 2**\n\nThanks for pointing it out! We included all of the numerical results of Figure 2 in Appendix E, while showing the results on DBLP below.\n\n| |i.i.d. (0)|noisy feature (1)|add edge (2)|drop edge (3)|drop node (4)|random subgraph (5)|\n|:--|:--|:--|:--|:--|:--|:--|\n|ERM | 85.71 | 84.48 | 71.08 | 79.69 | 83.41 | 76.9|\n|ERM-Aug | 85.66 | 85.29 | 74.85 | 82.34 | 84.44 | 72.81|\n|GraphMETRO | 85.92 | 85.78 | 76.61 | 82.95 | 84.98 | 81.32|\n\n| |(4, 5)|(3, 5)|(2, 5)|(1, 5)|(2, 4)|(1, 4)|     (2, 3)|(1, 3)|\n|:--|:--|:--|:--|:--|:--|:--|:--|:--|\n|ERM | 70.4 | 77.63 | 81.99 | 79.69 | 70.55 | 71.52 | 77.73 | 79.59|\n|ERM-Aug | 74.16 | 81.04 | 83.65 | 68.62 | 74.01 | 68.27 | 81.13 | 84.49|\n|GraphMETRO | 76.18 | 81.71 | 84.26 | 80.31 | 75.1 | 71.05 | 81.85 | 87.14|\n\nAcross all of the synthetic environments, GraphMETRO averagely outperforms ERM and ERM-Aug by 3.20% and 2.45%, respectively.\n\n---\n\nWe appreciate your comments! To address your concerns, below we prudently justify the assumption of our method, the predefined transformation functions, as well as their complexity, and clarify our presented results.\n\n---\n\n### **Comment 1-3: Applicability and our assumptions**\n\nThanks for these great comments! Here we provide response in three folds:\n\n**(1) How does GraphMETRO identify all distribution shifts from transform functions if they are treated independently when combined together?** \n\nIf we understand correctly, by “treated independently”, the reviewer is referring to the first term in our objective $\\text{BCE}(\\phi(\\tau^{(k)}(\\mathcal{G})), Y (\\tau^{(k)}))$, where we formulate predicting the distribution shifts types of a jointly transformed graph as a binary multiclass classification problem. We believe the difficulty of this task comes from both the property of transform functions and the expressiveness of the gating model. \n\n- Firstly, some transform functions are **inherently disentangled**, e.g., adding nodes feature noise and random subgraph extraction. In this case, there will be certain distinction between any pair from these three data distributions, i.e., (graphs with node noise, random subgraph graphs, random subgraphs with node noise), which the gating model can easily tell. \n- While some transform functions can be **essentially similar**, e.g., drop path and drop edges, this won’t affect the performance of our method as long as each expert outputs the corresponding invariant representation. \n- Lastly, indeed, there could be more **complex combinations of the transform functions**, which poses challenges to the gating model’s expressiveness in identifying the combinations. However, this challenge may be minor in the practice. Specifically, we observe fairly high accuracy performances of the gating model, which are above 85% and 73% averagely on extrapolated datasets with one transformation and multiple transformations, respectively.\n\nWe added the above discussion to Appendix F to enable a more comprehensive view towards our methodology. We hope this can alleviate your concern on our gating model’s performance in identifying the distribution shift types.\n\n**(2) How does the predefined transform functions cover complex distributions causing the distribution shift?**\n\nThis is also a great question! We believe there are two angles for this question. \n\n**(a) For general domain**: In our experiments, we mainly use the five stochastic transform functions, which are universal graph augmentations as listed in Zhao et al., (2021) [3]. In our code implementation, we have also included additional transform functions as shown in Appendix B. We believe these transform functions, while not exhaustive, still cover a wide range of distribution shifts observing from our experimental results.\n\nNevertheless, we agree that the real graph distribution shifts can go beyond any possible combinations of the predefined transform functions. In that case, the assumption may not hold, meaning that GraphMETRO may not capture and precisely mitigate the unknown distribution shift. This scenario could always possibly exist due to the lack of information about the testing distribution or its domain knowledge. We include it as a limitation in Appendix 5, while we further discuss how we could alleviate the problem with additional information.\n\n**(b) For specific domains where additional knowledge is available**: In fact, knowing the tendency of the distribution shifts, such as increasing malicious users in a trading system, would be very helpful in constructing the transform functions that can cover the target distribution shifts well. We believe that such knowledge can come from two sources: \n- **Domain knowledge**, e.g., on molecular datasets, the transform function could be adding additional carbon structure to a molecule (while preserving its functional groups). Or, in a particular social network, transform functions can be defined from known user behaviors. \n- **Leveraging a few samples from target distribution**. Specifically, with the guide from a few target samples, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space.\n\n---\n\n[1] Sui et al. Unleashing the Power of Graph Data Augmentation on Covariate Distribution Shift. NeurIPS 2023.\n\n[2] Nianzu Yang et al. Learning substructure invariance for out-of-distribution molecular representations, NeurIPS 2022\n\n[3] Yongduo Sui et al. Causal Attention for Interpretable and Generalizable Graph Classification, KDD 2022.\n\n[4] Jiaqi Ma et al. Subgroup Generalization and Fairness of Graph Neural Networks, NeurIPS 2022.\n\n---\n\n### **Comment 2: Our assumption**\n\nThanks for this comment! In Section 3.1, we have discussed the assumptions in detail with specifications. We repeat part of it here for your convenience:\n\n```Assumption 1 essentially states that the distribution shifts (whatever they are) can be decomposed into several mixture components of stochastic graph transformations. For example, on a social network dataset, each mixture component can represent different patterns of user behavior or network dynamics shifts. Specifically, one mixture component might correspond to increased user activity, while another could signify a particular trend of interaction within a certain group of users. Such a mixture pattern is common and well-studied in the real-world network datasets (Newman, 2003; Leskovec et al., 2005; 2007; Peel et al., 2017).```\n\nWe hope the analysis provides detailed illustration to the assumption. We are happy to add more discussion if you think anything  is still unclear!\n\n---\n\n### **Comment 2 & 3: Implementation of our method**\n\n- **Model architectures**: \nPlease see Tables 2 and 3 in Appendix A for detailed model architecture information. Specifically, we use the same encoders and classifiers from GOOD benchmarkfor real-world datasets to ensure fair comparisons. We employ backbones based on the best ERM performance for synthetic datasets. Moreover, GraphMETRO is model-agnostic, which consistently improves performance across varied model architectures.\n\n- **Stochastic transformation**: Please see Appendix B where we include the introduction of the stochastic transform functions. \n\n- **Optimization process**: Please see Table 2 and 3 (Appendix A), where we included hyperparameters on each dataset. In the second paragraph after Eq. (3), we also described our training pipeline: `“...we set apart the other loss terms from backpropagating to it to avoid interference with the training of the gating model… We optimize the objective via stochastic gradient descent”`. \n\n- **For a more intricate optimization strategy:** This is a great catch! In fact, we did try to pretrain the gating model for several epochs as warm up before training the whole model in an end-to-end fashion via the objective Eq. in (3). However, we didn’t notice a statistically significant difference in their performance, which can be due to that expert models take more time to convergence (since they need to align with the base model during the training) compared to the gating model.\n\nDue to the space limitation, we had to include most of the implementation details in the appendix while we added pointers in the main paper. However, we will try to make it more detailed and feel free to let us know if anything is missing! \n\n---\n\n### **Comment 2 (Cont.): Understand the functionality, underlying mechanism, and performance gain**\n\nThanks for the comment. For clarity, we provide the following pointers to the paper:\n\n- In Section 4.3, we provide a study to reveal the underlying **mechanism** of GraphMETRO, i.e., each expert excels in generating invariant representations concerning a stochastic transform function, which provide a solid foundation in generating referential invariant representations w.r.t. the specific transformations and further guarantee the generalization.\n\n- In Appendix C, we study the **impact of the MoE architecture** on model performance, which shows that the model performance may decline if the expressiveness of the expert model decreases. \n\n- In Appendix D, we study the **impact of the stochastic transform function** on model performance, where we also provide a detailed discussion of the modeling mechanism.\n\nWe believe the above studies and discussion in our paper provides an in-depth view, highlighting the roles of our objective, architecture, and stochastic transform function.  Please let us know if any of these perspectives is still unclear, and we can further improve our experimental study.\n\n-----\n\n# Summary\n\nWe hope our answers can address all of the concerns. We are happy to follow up if you have any further questions.\n\nWe also prudently ask you to reevaluate our work. To highlight, our motivation is supported by the common fine-grained graph shift heterogeneity, and the fact that most of the previous works could not model such nuanced distribution shifts in an effective and flexible manner. Moreover, we added more related works and modified our statement to position our work better. In general, we believe GraphMETRO is a more general and flexible solution that can mitigate a wider range of distribution shifts, which is backed by the experimental results. Detailed justification on our assumption and implementation details are also available in our paper. \n\nThus, we believe our work makes important contributions and provides a clear presentation. We are happy to discuss more and revise our paper if any concern remains. Thank you for your efforts and we are looking forward to your reply!!\n\n---\n\nWe appreciate your comments! To address your concerns, below we prudently justify the motivation of our proposed method, clarify our assumptions, and provide details regarding our experiment implementation.\n\n---------------------\n\n### **Comment 1: Motivation of this work**\n\nThank you for this comment! We believe there might be a bit of misunderstanding due to our different definitions of *“graph shift heterogeneity”*. We firstly discuss the related works mentioned and then justify our statement:\n\n**(1) Related works**\n\nPlease see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nMoreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you! \n\nSpecifically, Sui et al. [1] proposed a graph data augmentation strategy that alleviates covariate shift by generating diverse and invariant causal features. However, the trainable augmenter they used may not distill diverse augmentations or construct unseen perturbations. Moreover, Sui et al. [1] test its method only on graph classification tasks, while GraphMETRO can be applied to both node and graph classification tasks. Besides, we have discussed graph augmentation and attention-based methods in our related works, and we added more recent works on graph OOD [2,3,4], and we hope our response clears your concern on the related work discussion.\n\n\n**(2) The definition of graph shift heterogeneity**\n\nIn this work, we refer to **“heterogeneous shifts”** as multiple and different levels of shifts which vary across different instances (nodes or graphs), as illustrated in the example in the abstract. While we agree that the existing invariant learning approches can accommodate multiple distribution shifts, it could be hard for them to tackle nuanced distribution shifts for individual instances (nodes or graphs) since the distribution shifts are inferred from variance across multiple data environments. If GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering **a mixture of transformations as a proxy** for the target distribution shifts rather than invariance to whole-graph environment shifts. This is the type of heterogeneity we are interested in our paper. \n\n**(3) Regarding our original motivation statement**\n\nWhile the statement pointed out by the reviewer serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category. And we have provided detailed discussion about three lines of research in the related work section.\n\nHowever, we agree that we could make this statement border to cover the previous invariant learning methods. To improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to \"**previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments…**”. We also modified our introduction correspondingly, we hope this will better position our work.\n\n**(4) The choice of our MoE design**\n\n- The choice of our approach comes as a consequence of our motivation to model the graph/instance shift heterogeneity. As mentioned, mitigating multiple and nuanced distribution shifts simply goes beyond certain distribution shift types or environment construction as seen in the previous methods. \n- Thus, GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts. This enables the prediction of multiple different distribution shifts and the flexibility to model fine-grained heterogeneity since the mixture can be varied across different instances. We then tackled the proxy to mitigate the target distribution shifts. Intuitively, this solution provides a **“middle ground”** to deem graph generalization as an equivalent mixture, which, we believe, is a more tractable solution.\n\nWe updated our paper to make the above point more clear. We genuinely hope our answer can justify the motivation and solve your concern.\n\n---\n\n### **Question 2: How does the computational cost of GraphMETRO compare to other methods?**\n\nPlease see the last paragraph of Section 3.4, where we analyze the computation complexity of GraphMETRO. We repeat part of it here for your convenience:\n\n```Consider the scenario where we use an individual encoder for each expert. The forward process of $f$ involves $O(K)$ forward times using the weighted sum aggregation (or $O(1)$ if using the maximum selection). Since we extend the dataset to $(K + 1)$ times larger than the original data, the computation complexity is $O(K^2 |D_s|)$, where |Ds| is the size of the source dataset.```\n\nThus, the computation cost is about $K^2$ or $K$ times (if using the maximum selection) than an ERM model, where $K=5$ in our experiments. Compared to DIR, as they extract $B$ spurious subgraphs from each batch to conduct the intervention, their computation cost is $B$ times compared to ERM, where $B$ could be 32. Thus, we believe the computation cost of GraphMETRO is fair for the gains we get, considering $K$ is usually small.\n\n----------------------\n\n### **Question 3: Can GraphMETRO handle multiple different types of distribution shifts that simultaneously exist in data?**\n\nYes! The distribution shift types corresponding to the gating outputs with high scores will be tackled during training. That is, if the gating output highlights multiple mixture components, their corresponding distribution shift types will be handled jointly.\n\n----------------------\n\n### **Question 4: Can GraphMETRO  tackle distribution shifts on molecular graphs?**\n\nThat is a great idea! Yes, GraphMETRO can be applied to molecular datasets if one designs transform functions to cover typical molecular variants. For instance, a transform function may add carbon structures to the molecules. These domain-specific transform functions are outside the scope of our work, however, we believe these would be interesting future work directions!\n\n----------------------\n\n# Summary \n\nWe thank the reviewer for the time and insightful suggestions! We hope our answers can address your concerns well.\n\nWe also prudently ask you to reconsider our work if the concerns are addressed. To highlight, our novelty comes from the formulation of an equivalent mixture for graph OOD and the training framework to effectively realize generalization. We also provide an in-depth analysis on our originality compared to some previous invariant learning methods. While we discussed and compared with previous works, we added more related works and modified our statement to position our work better. Finally, our method achieves great improvements on both node and graph classification tasks, and is a more general solution to mitigate multiple and nuanced distribution shifts. \n\nOverall, we believe our work proposes a new paradigm and novel training framework and makes good contributions in the fields of graph generalization, and we would appreciate your reconsideration on this point. Thank you for your efforts again!\n\n----------------------\n\n**Reference**\n\n\n[1-5] The same as listed by the reviewer\n\n\n[6] Beatrice Bevilacqua, Yangze Zhou, and Bruno Ribeiro. Size-invariant graph representations for graph classification extrapolations. In ICML, 2021.\n\n[7] Davide Buffelli, Pietro Li´o, and Fabio Vandin. Sizeshiftreg: a regularization method for improving size-generalization in graph neural networks. In NeurIPS, 2022.\n\n[8] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In NeurIPS, 2019.\n\n[9] Mucong Ding, Kezhi Kong, Jiuhai Chen, John Kirchenbauer, Micah Goldblum, David Wipf, Furong Huang, and Tom Goldstein. A closer look at distribution shifts and out-of-distribution generalization on graphs. In NeurIPS DistShift, 2021.\n\n---\n\n### **Comment 3 & Question 1: Related works on invariant learning and clarification on our statement.**\n\nThanks for pointing it out! Below we clarify the statement and clear potential misunderstanding:\n\n**(1) How does GraphMETRO compare with invariant learning methods like DIR and EERM?** \n\nPlease see our related work section where we discussed these two papers, i.e., DIR (Wu et al., 2022c) and EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on environmental patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nBesides, we also provide a more in-depth comparison in our point (3) below to highlight our key technical originality.\n\n**(2) Regarding our statement about previous works:** \n\nWhile the statement serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category, and we have provided detailed discussion about three lines of research in the related work section.\n\nTo improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to **\"previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments** (which is highly limited when confronted with nuanced distribution shifts)”. We also modified our introduction correspondingly. Thanks for letting us know our statement could be misinterpreted.\n\n**(3) Why do we say our method could be more broad than the existing invariant learning approaches?** \n\n- Invariant subgraph learning approaches, e.g., [1,2], consider variance of constructed data environments, which are designed very differently compared to our work. \nWhile they can accommodate multiple distribution shifts (as in multiple environments), these focus on patterns within each environment and ignore the variety across instances (e.g., shifts at the resolution of nodes), which may not be well-captured by the environment assignments. \n- GraphMETRO considers that specific parts of the test graph may have different shifts. Particularly, our goal is to make the generalization to unknown testing distribution more adaptive and broad, as opposed to limiting the distribution shifts to being invariant to specific types of subgraphs. \n\nIn other words, if GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering a mixture of transformations as a proxy for the target distribution shifts rather than invariance to environment shifts.\n\n**(4) Key technical originality compared to invariance learning (going more deeply)**\n\nAnother interesting view to see the innovation of GraphMETRO is that it breaks the typical invariant learning formulation, which assumes the data is manipulated by the environment variables (and then can be “decoded” into multiple environments). Instead, GraphMETRO sees the distribution shifts on an instance as a mixture, which is represented by the score vector output by the gating function over the basis of the transform functions. In other words, GraphMETRO can produce infinite environments as the elements in the score vector are continuous. One can see that once we limit the output domain of the gating function into, e.g., binary {0, 1}, GraphMETRO can also produce a limited number of environments (if we categorize the instances based on the score vector), which covers the environment construction in invariant learning. Moreover, as mentioned, we propose the concept of referential invariant representation with a base model $\\xi_0$, which is also different from previous works on invariant learning. We added the above discussion to Appendix F to improve the depth of our analysis.\n\n---\n\nWe appreciate your efforts and insightful comments! To address your concerns, we provide point-to-point responses below.\n\n---------------------\n\n### **Comment 1: Regarding the novelty of GraphMETRO.**\n\nThanks for the comment! We believe our novelty comes from the proposal of an equivalent mixture for graph OOD and the construction of our training framework, as detailed below:\n\n- **An equivalent mixture for graph OOD**: The key challenge we faced to mitigate multiple and nuanced distribution shifts is the intrinsic complexity and heterogeneity of graph distribution shifts, which simply goes beyond certain distribution shift types [6,7,8,9] or environment construction as seen in the previous methods [1,2,3]. GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts, where the mixture can be varied across different instances, and then tackled the proxy to mitigate the target distribution shifts. We believe the high-level idea is succinct, nevertheless, novel, in the sense that it provides a “middle ground” to deem graph generalization as an equivalent mixture that is more tractable.\n\n- **Training framework**: With the guide of our formulation, the training framework is still non-trivial due to two problems, i.e., “how to provide supervision for predicting the mixture” and “how to ensure the experts corresponding to mixture components are compatible when working as a whole”. Specifically, GraphMETRO solves the first problem by conducting graph extrapolation. This is somewhat similar to the spirit of graph pretraining in the sense that we inject heterogeneity to promote the expressiveness of the gating model in recognizing the mixture components. For the second problem, we introduce the concept of Referential Invariant Representation, along with the novel objective in Eq. (3) to enforce the invariance and compatibility. It is worth mentioning that the model performance is much worse than the reported numbers (e.g., 2.7\\% lower on Twitch dataset) without the compatibility constraint, indicating the proposed referential invariance concept is indispensable. \n\nWe added more justification in the introduction (updated in the revision). We genuinely hope our responses can solve your concerns about the novelty of our work.\n\n------------------------------\n\n### **Comment 2: Comparison with recent methods [3,4,5].**\n\nIn compacting the paper to fit in the page limit we mistakenly did not include these relevant references, we apologize. We added discussion in the revised version. Here we summarize these works and point out their key differences with our method: \n\n- In particular, **Yang et al. [3]** explore molecule representation learning in out-of-distribution (OOD) scenarios. They achieve this by directing the molecule encoder to utilize stable and environment-invariant substructures relevant to the labels without the need for environmental labels. \n- Similarly, **Sui et al. [4]** introduces causal attention modules to identify key invariant subgraph features that can be described as causing the graph label. The type of OOD task that Sui et al. [4] considers assumes the graph label is caused by a subgraph, which is quite different from ours. Moreover, both Yang et al. [3] and Sui et al. [4] consider tasks where the graph label is caused by a subgraph. \n- **Ma et al.[5]** is an interesting theoretical work which studies GNN generalization and examines their fairness, showing that the test subgroup's distance from the training set impacts GNN performance. Ma et al.[5], as far as we could assess, does not propose any specific architecture to solve the type of OOD tasks we consider in our work.\n\nOverall, the goal of GraphMETRO is to be invariant to a mixture of selected stochastic transform functions (and the mixture can vary across different instances), which is a more flexible and general solution. \nWe added a discussion of these works to our revision, hopefully providing a more comprehensive comparison and literature overview. We hope our responses can solve your concerns about the related work.\n\n---\n\nWe sincerely appreciate all reviewers' time, efforts, and valuable suggestions in reviewing our paper. We are glad that most of the reviewers reached a positive consensus on our work's motivation and experimental results. Here is a summary of our responses:\n\n- **Clarification**: We clarify our key assumptions (`m8JS, FURX`), novelty (`m8JS, NLg1`), and the experimental settings (`m8JS`).\n\n- **Related work**: : We add more related works on recent graph OOD learning (`NLg1, m8JS`) and graph transfer learning (`FURX`)\n\n- **Limitation**: We add discussion about the limitations of GraphMETRO when confronted with unknown distribution shifts that surpass our key assumptions (`FURX`, `C9mM`).\n\nWe hope our responses can clarify your confusion and alleviate concerns and we updated our paper **(highlighted in green)** corespondingly. We thank all reviewers again, and look forward to your reply!", "author_response": "We appreciate your comments! To address your concerns, below we prudently justify the motivation of our proposed method, clarify our assumptions, and provide details regarding our experiment implementation.\n\n---------------------\n\n### **Comment 1: Motivation of this work**\n\nThank you for this comment! We believe there might be a bit of misunderstanding due to our different definitions of *“graph shift heterogeneity”*. We firstly discuss the related works mentioned and then justify our statement:\n\n**(1) Related works**\n\nPlease see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nMoreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you! \n\nSpecifically, Sui et al. [1] proposed a graph data augmentation strategy that alleviates covariate shift by generating diverse and invariant causal features. However, the trainable augmenter they used may not distill diverse augmentations or construct unseen perturbations. Moreover, Sui et al. [1] test its method only on graph classification tasks, while GraphMETRO can be applied to both node and graph classification tasks. Besides, we have discussed graph augmentation and attention-based methods in our related works, and we added more recent works on graph OOD [2,3,4], and we hope our response clears your concern on the related work discussion.\n\n\n**(2) The definition of graph shift heterogeneity**\n\nIn this work, we refer to **“heterogeneous shifts”** as multiple and different levels of shifts which vary across different instances (nodes or graphs), as illustrated in the example in the abstract. While we agree that the existing invariant learning approches can accommodate multiple distribution shifts, it could be hard for them to tackle nuanced distribution shifts for individual instances (nodes or graphs) since the distribution shifts are inferred from variance across multiple data environments. If GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering **a mixture of transformations as a proxy** for the target distribution shifts rather than invariance to whole-graph environment shifts. This is the type of heterogeneity we are interested in our paper. \n\n**(3) Regarding our original motivation statement**\n\nWhile the statement pointed out by the reviewer serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category. And we have provided detailed discussion about three lines of research in the related work section.\n\nHowever, we agree that we could make this statement border to cover the previous invariant learning methods. To improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to \"**previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments…**”. We also modified our introduction correspondingly, we hope this will better position our work.\n\n**(4) The choice of our MoE design**\n\n- The choice of our approach comes as a consequence of our motivation to model the graph/instance shift heterogeneity. As mentioned, mitigating multiple and nuanced distribution shifts simply goes beyond certain distribution shift types or environment construction as seen in the previous methods. \n- Thus, GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts. This enables the prediction of multiple different distribution shifts and the flexibility to model fine-grained heterogeneity since the mixture can be varied across different instances. We then tackled the proxy to mitigate the target distribution shifts. Intuitively, this solution provides a **“middle ground”** to deem graph generalization as an equivalent mixture, which, we believe, is a more tractable solution.\n\nWe updated our paper to make the above point more clear. We genuinely hope our answer can justify the motivation and solve your concern."}
{"claim": "The novelty of the proposed method is insufficient and does not meet the standards expected for significant research contributions.", "claim_type": "novelty", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ppMpJyIYRZ", "reviewer": "Reviewer_m8JS", "review_text": "Summary: The paper introduces a method to enhance the out-of-distribution performance of graph neural networks (GNN) by learning to understand distribution shifts instead of addressing the assumed ones. To achieve this, the Mixture of Experts architecture is integrated into the GNN, supplemented by an alignment procedure to recognize the shift. Empirical experiments are conducted to validate the theoretical assertion.\n\nOn the whole, I believe the proposed method lacks the necessary motivation and its novelty isn't substantial enough to meet the standard.\n\nStrengths: - The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\n- Real-world datasets back the claims through experiments.\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\n\nWeaknesses: - The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \"previous research has concentrated on addressing specific types of distribution shifts.\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \"environment generators\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\n\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\n\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\n\nQuestions: Please check Weaknesses.", "labeling_timestamp": "2026-01-11T16:30:43.367085", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The authors firmly disagree with the reviewer's claim and defend the paper's novelty, outlining specific original contributions (equivalent mixture formulation, referential invariant representations, training framework), adding comparisons to prior work, and promising revisions to clarify these points.", "evidence": "\"we don't agree that they, based on our justificaton, are the cause of rejection. We believe nuanced heterogeneous shifts are common, important, yet being typically ignored in the research domain of distribution shifts...\"; \"We believe our work makes good contributions to the field of graph distribution learning by proposing a novel and effective solution...\"; \"We believe our novelty comes from the formulation of an equivalent mixture for graph OOD and the construction of our training framework...\"", "combined_author_response": "Dear reviewer NLg1,\n\nAs the discussion period is closing soon, we hope to engage with you and check if your concerns are addressed.\n\nWe sincerely appreciate your time and attention!\n\nBest,\n\nAuthors of Paper 1560\n\n---\n\nThanks for reading our response! We would also appreciate your patience for reading the two points below \n\n---\n\n**Regarding point #1:**\n\nYes, we agree that the concept of heterogeneous shifts is relatively new for the current studies on distribution shifts, however, this is not new for studies on network patterns (Newman, 2003; Leskovec et al., 2005; 2007; Peel et al., 2017). In fact, the ignorance of such nuanced heterogeneous shifts in the previous studies instead emphasises our motivation and the potential impact of this work. \n\nFrom a causality perspective, distribution shifts can naturally happen in the instance level when (1) additional causal variables, beyond environmental factors and randomized noise, influence these shifts, or (2) multiple causal variables simultaneously affect the shifts with different strengths. Without modeling these explicitly, the mitigation of distribution shifts can easily fail. \n\nIn terms of our presentation, we have illustrated these cases of interest in abstract and introduction, we made further explanations in Section 3.1, we also present the specific results of distribution types in Figure 3 (b). \n\n\n**Regarding point #2:**\n\nWe think we can all agree that, if the ground truth of the instance shifts is available on the real-world datasets, it would be crystal clear to see where the improvement comes from since we can conduct case study to compare our method and the baselines on instances with nuanced distribution shifts, to see the influence of the modeling these heterogeneous shifts. \n\nHowever, with such ground truth not available, we had to seek other seemingly less intuitive but also in-depth way to illustrate the insights (esp Section 4.3) as mentioned above in our previous response. This is also why we designed the synthetic experiments at the first place. We think we did try hard to explain the underlying rationale with the ground truth being absent.\n\n----\n\n**Refinement**: We can of course add a causal graph in the our assumption section to make the concept more clear. And we can illustrate more if you could let us know the specific obstable to understand our mechanism, which will be extremely helpful.\n\n----\n\n### **Summary/TL;DR**\nWe understand the reviewer's clarification concerns, however, we don't agree that they, based on our justificaton, are the cause of rejection. We believe nuanced heterogeneous shifts are common, important, yet being typically ignored in the research domain of distribution shifts, we made these argument clear and also promise to refine. While explaining the underlying rationale is hindered by the lack of ground truth, we did try hard to dissection it from the gating model, the invariant representations generated, and more ablations in the appendix. We respect the reviewer's current opinion. Still, reconsideration will be greatly appreciated.\n\n---\n\nWe thank the reviewer once again! We also enjoyed the process of making our work more sound from your suggestions. And your approval surely means a lot to us.\n\nWe do apologize for not making our solutions towards the limitations clear enough. We summarize them into a short table, hopefully could alleviate your concerns a bit more.\n\n| Limitation| Potential solutions | Location of discussion/action |\n|:---|:---|:---|\n|**Coverage of the transformations** | 1) Extend the coverage by adding representative transform functions. 2) Or include tranform functions based on domain knowledge or a few samples from target distribution | Appendix F, paragraph #3|\n|**Complexity of the transformations** | Make experts dedicated to different hyperparameters for the same type of transformation | Argument options are available in our codebase. Will make it more detailed in the experimental settings. |\n|**Label distributional shift**|Integrate the objective of the existing methods studying labe distributional shifts into our framework. |Appendix F, paragraph #4 |\n\n----\n\nWe completely agree on the existence of these limitations and we will move some limitations (esp #1) to the main paper in our final version. While they could be important in the practice, they are, in our perspective, fair \"side effects\" considering the benefits (i.e., mitigating multiple and nuanced distribution shifts and better interpretability), and may not be the central part of our novelty and main contribution (i.e., the proposal of an equivalent mixture, the concept of referential invariant representations, as well as the training framework). We are also eager to further improve other aspects.\n\nOnce again thank you so much for your support!!\n\n---\n\nDear Reviewer m8JS,\n\nA gentle nudge that we would like to know if our response adequately addresses your concerns. \n\nYour time and feedback is greatly appreciated!\n\nThank you sincerely,\n\nAuthors of Paper 1560\n\n---\n\nWe deeply appreciate your approval. Your suggestions definitely inspired us a lot and have greatly improved our work.\n\nA few additional notes for the further comments:\n\n- Regarding **point #2**: In node classification tasks, the BCE objective already considers node embeddings other than the graph embedding of the k-hop subgraph. What we described previously refers to the computational graph (i.e., k-hop message passing), which generates node embeddings. We apologize for any confusion.\n\n- Regarding **point #3**: Yes it would be quite interesting to see how graph pretraining methods perform on the current OOD benchmarks. Graph pretraining methods like GCC and GPT-GNN also consider graph extrapolation to some extent, e.g., through subgraph extraction and masked attributes/structures. The key difference between graph pretraining and generalization may lie in their different focuses on **expressiveness** and **invariance**. While these two aspects do not always conflict, ensuring invariance w.r.t. a certain type of extrapolation might affect expressiveness (if the change is relevant to labels), and vice versa. To seek a balance in between, one might need prior knowledge in which types of transformations may or may not be sensitive to the labels (and perhaps build experts with different goals to make ensure invariance or expressivess). We believe there is still a lot to explore in this domain.\n\nOverall, we are grateful for your positive stand on this work. We believe that our [current version](https://openreview.net/pdf?id=QQ5eVDIMu4), incorporating opinions from you and other reviewers, is sound and well-refined. We are also committed to further improving it. We would appreciate your support based on our current version! \n\nThank you!\n\n---\n\nDear Reviewers of Paper 1560,\n\nWe hope this message finds you well.\n\nAs the discussion phase approaches its end, we hope you find our responses useful. We would like to ask if the issues have been addressed.\n\nWe understand that the discussion time is short, and some of you might be enjoying holidays at the moment. We apologize for posting our responses a bit late as we aimed to address your concerns clearly.\n\nWe sincerely appreciate your time and attention!\n\nBest regards,\n\nAuthors of Paper 1560\n\n---\n\n**Comment 4: Related works on graph transfer learning.**\n\nYes! We agree that graph transfer learning is a relevant topic. Thank you for providing the additional related work! We added the discussion on these works in our revision, and we repeat it here for your convenience:\n\n```It is also worth mentioning that graph domain adaptation (Zhang et al., 2019; Wu et al., 2020), different from the problem studied in this work, commonly relies on limited labeled samples from target datasets for improved transferability. For instance, to generate domain adaptive network embedding, DANE (Zhang et al., 2019) uses shared weight graph convolutional networks and adversarial learning regularization, while UDA-GCN (Wu et al., 2020) employs attention mechanisms to merge global and local consistencies.```\n\nMoreover, we believe this is actually relevant to `Comment & Question 1` regarding the generality of the distribution shifts covered by the transform functions. In our future works, we added the following discussion:\n\n```...Leveraging a few samples from target distribution (i.e., domain adaptation). Specifically, we can leverage the samples from the target distribution to inform the selection or construction of transform functions, which can better guarantee the distribution shifts are covered by the transform functions.  For example, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space. We believe this would be an interesting future direction.```\n\n---\n\n# Summary\n\nOnce again thank you very much for helping us position our work and reflect the literature better. We hope all of your concerns are solved and we are happy to engage further if there are any other points we missed!\n\n---\n\n### **Reference**\n\n[1] Unsupervised Domain Adaptive Graph Convolutional Networks. Wu et al. WWW 2020\n\n[2] DANE: Domain adaptive network embedding. Zhang et al. IJCAI 2019.\n\n[3] Tong Zhao, Yozen Liu, Leonardo Neves, Oliver J. Woodford, Meng Jiang, and Neil Shah. Data augmentation for graph neural networks. AAAI, 2021.\n\n---\n\nWe are grateful for your positive feedback and detailed suggestions! We provide responses below to address your remaining concerns. \n\n---\n\n### **Comment & Question 1: Generality of the distribution shifts covered by the transform functions.**\n\nGood question! We provide the response from two angles:\n\n\n**(a) For general domain**: In our experiments, we mainly use the five stochastic transform functions, which are universal graph augmentations as listed in Zhao et al., (2021) [3]. In our code implementation, we have also included additional transform functions as shown in Appendix B. We believe these transform functions, while not exhaustive, still cover a wide range of distribution shifts observing from our experimental results.\n\nNevertheless, we agree that the real graph distribution shifts can go beyond any possible combinations of the predefined transform functions. In that case, the assumption may not hold, meaning that GraphMETRO may not capture and precisely mitigate the unknown distribution shift. This scenario could always possibly exist due to the lack of information about the testing distribution or its domain knowledge. We include it as a limitation in Appendix 5, while we further discuss how we could alleviate the problem with additional information.\n\n**(b) For specific domains where additional knowledge is available**: In fact, knowing the tendency of the distribution shifts, such as increasing malicious users in a trading system, would be very helpful in constructing the transform functions that can cover the target distribution shifts well. We believe that such knowledge can come from two sources: \n- **Domain knowledge**, e.g., on molecular datasets, the transform function could be adding additional carbon structure to a molecule (while preserving its functional groups). Or, in a particular social network, transform functions can be defined from known user behaviors. \n- **Leveraging a few samples from target distribution**. This is in fact in line with the reviewer’s Comment #4 regarding graph transfer learning. Specifically, with the guide from a few target samples, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space.\n\n We hope this response can alleviate your concern about our applicability.\n\n---\n\n### **Comment & Question 2: The mechanism of GaphMETRO in handling heterogeneous shifts within one graph.**\n\nGood question! For simplicity, suppose we have two transform functions, i.e., adding edges and dropping edges. Given a node classification task and the objective in Eq. (3), the model is trained on the extrapolated datasets based on the transform functions. After that, given an unseen graph with imbalanced distribution shifts, the gating model outputs scores to identify nodes likely to experience increasing or decreasing degrees, while others might adhere to the original distribution. Then, for each node, each expert takes its multihop subgraph, and outputs its referential invariant representation w.r.t. the correposing transform function. These expert outputs and gating model results are then aggregated to form the final representation. Assuming accurate predictions by the gating function, nodes in denser/sparser subgraphs are represented by expert models corresponding to adding/dropping edges. As each expert is trained to create invariant representations, the final node representations remain unaffected by their individual distribution shifts.\n\n---\n\n### **Comment 3: How to leverage graph pretraining.**\n\nIf we understand correctly, the reviewer was asking how to leverage a pretrained model to further aid the training of GraphMETRO. Please let us know if otherwise. \n\nThis is in fact an interesting point! We believe a model pretrained on a wide variety of data can be very helpful to initialize the gating model, which is required to output the mixture of a node or graph (after it is finetuned on the extrapolated dataset). By enhancing the gating model's predictive capability regarding mixtures, GraphMETRO's final representation should become more resilient. This becomes particularly advantageous when dealing with graphs not previously encountered in the extrapolated dataset. Moreover, it is possible that the pretrained model will also benefit the expert models, while one minor concern would be that the expert model may tend to be similar instead of dedicating to generating invariant representation w.r.t. their corresponding transform function. We added the above discussion to Appendix F as a future work. Thanks again for this comment!\n\n---\n\n### **Question 4: Why does the test accuracy vary across different transformations?**\n\nGreat question! Here we summarize three possible reasons:\n\n- **Information Preservation in Transformations:** Certain transformations retain more informative features than others. For instance, in the REDDIT-BINARY (graph classification task), the random subgraph transformation may retain more graph label-related information compared to dropping edges, as the latter tends to lose more global information. This discrepancy in testing performance, where dropping edges outperforms random subgraph extraction, could be due to the preservation of crucial information. However, conclusions may vary across datasets or tasks depending on how information influences final predictions. For CiteSeer (a node classification task), a random subgraph might preserve more local node information, potentially explaining why its testing performance surpasses dropping edges in this specific task.\n\n- **Complexity of transformation:** Certain transformations inherently generate more diverse graphs than others. If the model lacks the expressive capacity to capture such diversity, it may lead to a decline in testing performance.\n\n- **Model Sensitivity:** Certain transformations may be easier for a model to learn due to compatibility with specific model architectures. This extends beyond transformation complexity and emphasizes how different model architectures may prefer learning particular distributions from one of the extrapolated datasets, which can also contribute to the difference in the testing performance.\n\nWe included the above discussion to Appendix F: Open Discussions. We hope this response can answer your question and improve the soundness of our work.\n\n---\n\n# Summary \n\nWe are grateful for your time and insightful suggestions! \n\nWe would like to highlight that our main contribution is framing the graph generalization problem on top of an equivalent mixture, a simple yet novel and tractable \"middle ground\", as well as proposing the training framework which effectively guarantees the generalization. While our method relies on a set of predefined transform functions, we believe they cover a wide range of distribution shifts based on our empirical results. Also, we agree that there could be some scenarios where the transform functions may not cover complex distributions, and we discuss two future directions and include them into our future works. Moreover, while selecting the hyperparameters for the transform functions introduce extra complexity, the issue could be minor in practice and we also conduct more experiments to justify the applicability better. Finally, we address several questions about clarification and presentation, as well as including more future works. \n\nLastly, we prudently ask you to reevaluate our work given the clarification in our responses, which we also updated our paper correspondingly. Overall, we believe our work makes good contributions to the field of graph distribution learning by proposing a novel and effective solution, and we would appreciate your reconsideration on this point. Thank you for your efforts again!\n\n---\n\n### **Reference**\n\n[1] Menon, Aditya Krishna, et al. \"Long-tail learning via logit adjustment.\" International Conference on Learning Representations. 2020.\n\n[2] Cao, Kaidi, et al. \"Learning imbalanced datasets with label-distribution-aware margin loss.\" Advances in neural information processing systems 32 (2019).\n\n[3] DARTS: Differentiable Architecture Search. Hanxiao Liu, Karen Simonyan, Yiming Yang. 2018.\n\n---\n\n**(3) What is the complexity of the transform functions and how does it affect generalization?** \n\nInteresting question! In fact, our implementation and framework could easily avoid selecting hyperparameters on the transform functions. Specifically, we can make multiple transform functions of the same type with different ranges of hyperparameters. Specifically, GraphMETRO allows three edge dropping transform functions, $\\tau_1^{\\alpha_1}, \\tau_2^{\\alpha_2}, \\tau_3^{\\alpha_3}$, where $\\alpha_i$ (i=1, 2, 3) are three different ranges of edge dropping probabilities, e.g., [0.1,0.3], [0.3, 0.6], [0.6,0.9], representing different transform extents. Thus, given an input from the validation dataset, the gating model will highlight the transform function which simultaneously selects the corresponding hyperparameter that matches the distribution of the validation set. Interestingly, this idea is in the same spirit as how DARTS [3]  proposes to perform architecture search by formulating the task in a differentiable manner. \n\nIn our previous experiments, we did try this scheme where we replaced a single edge dropping transform function with the ratio range [0.3, 0.5] to three transform functions as mentioned above. While we didn’t see a significantly different performance in that case, we believe this would be a flexible solution which avoids the need to conduct hyperparameter selection. \n\nWe include the above discussion in our open discussion and future works (Appendix F). We hope this response can alleviate your concern about the applicability of our method. \n\n---\n\n### **Comment 4: Limitations of our work**\n\nGreat point! We think the issue of label distributional shift, while important, is orthogonal and complementary to the focus of our current study. To elaborate, label distributional shifts exert analogous impacts across various modalities, such as graphs or images. Moreover, existing methods [1,2] designed to tackle label distributional shifts can be seamlessly integrated into our proposed framework. Such integration would necessitate minimal adjustments, potentially involving modifications to the loss function or the training pipeline. We added this as a future work in Appendix F.\n\n---\n\n### **Question 1: Statistical significance of the results on Table 1**\n\nThanks for the question! We compute the p-value of our method against the best baselines method as follows:\n\n| | WebKB | Twitch | Twitter | SST2 | \n|:--|:--|:--|:--|:--|\n|p-value|< 0.001| 0.023 | 0.042| 0.081|\n\nGiven the cut-off threshold as 0.05, we believe the performances of GraphMETRO are statistically significant on WekGB, Twitch, and Twitter datasets, while on the SST2, we see relatively weak evidence. We added the p-value results to our revision and hope our response can alleviate your concern on our improvements.\n\n---\n\n###  **Question 2: Can we say we use ERM for the node classification even if nodes and their labels are not i.i.d.?** \n\nIf we understand correctly, the reviewer is asking for clarification on the 2nd term of our objective. Please let us know if otherwise. Here, our thinking is that the cross-entropy loss for node classification already assumes node labels are conditionally independent given the model (the negative log-likelihood is a sum over the labeled nodes in training). Then, we use the same assumption of cross-entropy on Empirical Risk Minimization (ERM). That is, for a given model we must also minimize the error variance across nodes. The task is then to find the model with the best performance and small variance.\n\n---\n\n### **Question 3: Numerical results on Figure 2**\n\nThanks for pointing it out! We included all of the numerical results of Figure 2 in Appendix E, while showing the results on DBLP below.\n\n| |i.i.d. (0)|noisy feature (1)|add edge (2)|drop edge (3)|drop node (4)|random subgraph (5)|\n|:--|:--|:--|:--|:--|:--|:--|\n|ERM | 85.71 | 84.48 | 71.08 | 79.69 | 83.41 | 76.9|\n|ERM-Aug | 85.66 | 85.29 | 74.85 | 82.34 | 84.44 | 72.81|\n|GraphMETRO | 85.92 | 85.78 | 76.61 | 82.95 | 84.98 | 81.32|\n\n| |(4, 5)|(3, 5)|(2, 5)|(1, 5)|(2, 4)|(1, 4)|     (2, 3)|(1, 3)|\n|:--|:--|:--|:--|:--|:--|:--|:--|:--|\n|ERM | 70.4 | 77.63 | 81.99 | 79.69 | 70.55 | 71.52 | 77.73 | 79.59|\n|ERM-Aug | 74.16 | 81.04 | 83.65 | 68.62 | 74.01 | 68.27 | 81.13 | 84.49|\n|GraphMETRO | 76.18 | 81.71 | 84.26 | 80.31 | 75.1 | 71.05 | 81.85 | 87.14|\n\nAcross all of the synthetic environments, GraphMETRO averagely outperforms ERM and ERM-Aug by 3.20% and 2.45%, respectively.\n\n---\n\nWe appreciate your comments! To address your concerns, below we prudently justify the assumption of our method, the predefined transformation functions, as well as their complexity, and clarify our presented results.\n\n---\n\n### **Comment 1-3: Applicability and our assumptions**\n\nThanks for these great comments! Here we provide response in three folds:\n\n**(1) How does GraphMETRO identify all distribution shifts from transform functions if they are treated independently when combined together?** \n\nIf we understand correctly, by “treated independently”, the reviewer is referring to the first term in our objective $\\text{BCE}(\\phi(\\tau^{(k)}(\\mathcal{G})), Y (\\tau^{(k)}))$, where we formulate predicting the distribution shifts types of a jointly transformed graph as a binary multiclass classification problem. We believe the difficulty of this task comes from both the property of transform functions and the expressiveness of the gating model. \n\n- Firstly, some transform functions are **inherently disentangled**, e.g., adding nodes feature noise and random subgraph extraction. In this case, there will be certain distinction between any pair from these three data distributions, i.e., (graphs with node noise, random subgraph graphs, random subgraphs with node noise), which the gating model can easily tell. \n- While some transform functions can be **essentially similar**, e.g., drop path and drop edges, this won’t affect the performance of our method as long as each expert outputs the corresponding invariant representation. \n- Lastly, indeed, there could be more **complex combinations of the transform functions**, which poses challenges to the gating model’s expressiveness in identifying the combinations. However, this challenge may be minor in the practice. Specifically, we observe fairly high accuracy performances of the gating model, which are above 85% and 73% averagely on extrapolated datasets with one transformation and multiple transformations, respectively.\n\nWe added the above discussion to Appendix F to enable a more comprehensive view towards our methodology. We hope this can alleviate your concern on our gating model’s performance in identifying the distribution shift types.\n\n**(2) How does the predefined transform functions cover complex distributions causing the distribution shift?**\n\nThis is also a great question! We believe there are two angles for this question. \n\n**(a) For general domain**: In our experiments, we mainly use the five stochastic transform functions, which are universal graph augmentations as listed in Zhao et al., (2021) [3]. In our code implementation, we have also included additional transform functions as shown in Appendix B. We believe these transform functions, while not exhaustive, still cover a wide range of distribution shifts observing from our experimental results.\n\nNevertheless, we agree that the real graph distribution shifts can go beyond any possible combinations of the predefined transform functions. In that case, the assumption may not hold, meaning that GraphMETRO may not capture and precisely mitigate the unknown distribution shift. This scenario could always possibly exist due to the lack of information about the testing distribution or its domain knowledge. We include it as a limitation in Appendix 5, while we further discuss how we could alleviate the problem with additional information.\n\n**(b) For specific domains where additional knowledge is available**: In fact, knowing the tendency of the distribution shifts, such as increasing malicious users in a trading system, would be very helpful in constructing the transform functions that can cover the target distribution shifts well. We believe that such knowledge can come from two sources: \n- **Domain knowledge**, e.g., on molecular datasets, the transform function could be adding additional carbon structure to a molecule (while preserving its functional groups). Or, in a particular social network, transform functions can be defined from known user behaviors. \n- **Leveraging a few samples from target distribution**. Specifically, with the guide from a few target samples, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space.\n\n---\n\n[1] Sui et al. Unleashing the Power of Graph Data Augmentation on Covariate Distribution Shift. NeurIPS 2023.\n\n[2] Nianzu Yang et al. Learning substructure invariance for out-of-distribution molecular representations, NeurIPS 2022\n\n[3] Yongduo Sui et al. Causal Attention for Interpretable and Generalizable Graph Classification, KDD 2022.\n\n[4] Jiaqi Ma et al. Subgroup Generalization and Fairness of Graph Neural Networks, NeurIPS 2022.\n\n---\n\n### **Comment 2: Our assumption**\n\nThanks for this comment! In Section 3.1, we have discussed the assumptions in detail with specifications. We repeat part of it here for your convenience:\n\n```Assumption 1 essentially states that the distribution shifts (whatever they are) can be decomposed into several mixture components of stochastic graph transformations. For example, on a social network dataset, each mixture component can represent different patterns of user behavior or network dynamics shifts. Specifically, one mixture component might correspond to increased user activity, while another could signify a particular trend of interaction within a certain group of users. Such a mixture pattern is common and well-studied in the real-world network datasets (Newman, 2003; Leskovec et al., 2005; 2007; Peel et al., 2017).```\n\nWe hope the analysis provides detailed illustration to the assumption. We are happy to add more discussion if you think anything  is still unclear!\n\n---\n\n### **Comment 2 & 3: Implementation of our method**\n\n- **Model architectures**: \nPlease see Tables 2 and 3 in Appendix A for detailed model architecture information. Specifically, we use the same encoders and classifiers from GOOD benchmarkfor real-world datasets to ensure fair comparisons. We employ backbones based on the best ERM performance for synthetic datasets. Moreover, GraphMETRO is model-agnostic, which consistently improves performance across varied model architectures.\n\n- **Stochastic transformation**: Please see Appendix B where we include the introduction of the stochastic transform functions. \n\n- **Optimization process**: Please see Table 2 and 3 (Appendix A), where we included hyperparameters on each dataset. In the second paragraph after Eq. (3), we also described our training pipeline: `“...we set apart the other loss terms from backpropagating to it to avoid interference with the training of the gating model… We optimize the objective via stochastic gradient descent”`. \n\n- **For a more intricate optimization strategy:** This is a great catch! In fact, we did try to pretrain the gating model for several epochs as warm up before training the whole model in an end-to-end fashion via the objective Eq. in (3). However, we didn’t notice a statistically significant difference in their performance, which can be due to that expert models take more time to convergence (since they need to align with the base model during the training) compared to the gating model.\n\nDue to the space limitation, we had to include most of the implementation details in the appendix while we added pointers in the main paper. However, we will try to make it more detailed and feel free to let us know if anything is missing! \n\n---\n\n### **Comment 2 (Cont.): Understand the functionality, underlying mechanism, and performance gain**\n\nThanks for the comment. For clarity, we provide the following pointers to the paper:\n\n- In Section 4.3, we provide a study to reveal the underlying **mechanism** of GraphMETRO, i.e., each expert excels in generating invariant representations concerning a stochastic transform function, which provide a solid foundation in generating referential invariant representations w.r.t. the specific transformations and further guarantee the generalization.\n\n- In Appendix C, we study the **impact of the MoE architecture** on model performance, which shows that the model performance may decline if the expressiveness of the expert model decreases. \n\n- In Appendix D, we study the **impact of the stochastic transform function** on model performance, where we also provide a detailed discussion of the modeling mechanism.\n\nWe believe the above studies and discussion in our paper provides an in-depth view, highlighting the roles of our objective, architecture, and stochastic transform function.  Please let us know if any of these perspectives is still unclear, and we can further improve our experimental study.\n\n-----\n\n# Summary\n\nWe hope our answers can address all of the concerns. We are happy to follow up if you have any further questions.\n\nWe also prudently ask you to reevaluate our work. To highlight, our motivation is supported by the common fine-grained graph shift heterogeneity, and the fact that most of the previous works could not model such nuanced distribution shifts in an effective and flexible manner. Moreover, we added more related works and modified our statement to position our work better. In general, we believe GraphMETRO is a more general and flexible solution that can mitigate a wider range of distribution shifts, which is backed by the experimental results. Detailed justification on our assumption and implementation details are also available in our paper. \n\nThus, we believe our work makes important contributions and provides a clear presentation. We are happy to discuss more and revise our paper if any concern remains. Thank you for your efforts and we are looking forward to your reply!!\n\n---\n\nWe appreciate your comments! To address your concerns, below we prudently justify the motivation of our proposed method, clarify our assumptions, and provide details regarding our experiment implementation.\n\n---------------------\n\n### **Comment 1: Motivation of this work**\n\nThank you for this comment! We believe there might be a bit of misunderstanding due to our different definitions of *“graph shift heterogeneity”*. We firstly discuss the related works mentioned and then justify our statement:\n\n**(1) Related works**\n\nPlease see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nMoreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you! \n\nSpecifically, Sui et al. [1] proposed a graph data augmentation strategy that alleviates covariate shift by generating diverse and invariant causal features. However, the trainable augmenter they used may not distill diverse augmentations or construct unseen perturbations. Moreover, Sui et al. [1] test its method only on graph classification tasks, while GraphMETRO can be applied to both node and graph classification tasks. Besides, we have discussed graph augmentation and attention-based methods in our related works, and we added more recent works on graph OOD [2,3,4], and we hope our response clears your concern on the related work discussion.\n\n\n**(2) The definition of graph shift heterogeneity**\n\nIn this work, we refer to **“heterogeneous shifts”** as multiple and different levels of shifts which vary across different instances (nodes or graphs), as illustrated in the example in the abstract. While we agree that the existing invariant learning approches can accommodate multiple distribution shifts, it could be hard for them to tackle nuanced distribution shifts for individual instances (nodes or graphs) since the distribution shifts are inferred from variance across multiple data environments. If GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering **a mixture of transformations as a proxy** for the target distribution shifts rather than invariance to whole-graph environment shifts. This is the type of heterogeneity we are interested in our paper. \n\n**(3) Regarding our original motivation statement**\n\nWhile the statement pointed out by the reviewer serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category. And we have provided detailed discussion about three lines of research in the related work section.\n\nHowever, we agree that we could make this statement border to cover the previous invariant learning methods. To improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to \"**previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments…**”. We also modified our introduction correspondingly, we hope this will better position our work.\n\n**(4) The choice of our MoE design**\n\n- The choice of our approach comes as a consequence of our motivation to model the graph/instance shift heterogeneity. As mentioned, mitigating multiple and nuanced distribution shifts simply goes beyond certain distribution shift types or environment construction as seen in the previous methods. \n- Thus, GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts. This enables the prediction of multiple different distribution shifts and the flexibility to model fine-grained heterogeneity since the mixture can be varied across different instances. We then tackled the proxy to mitigate the target distribution shifts. Intuitively, this solution provides a **“middle ground”** to deem graph generalization as an equivalent mixture, which, we believe, is a more tractable solution.\n\nWe updated our paper to make the above point more clear. We genuinely hope our answer can justify the motivation and solve your concern.\n\n---\n\n### **Question 2: How does the computational cost of GraphMETRO compare to other methods?**\n\nPlease see the last paragraph of Section 3.4, where we analyze the computation complexity of GraphMETRO. We repeat part of it here for your convenience:\n\n```Consider the scenario where we use an individual encoder for each expert. The forward process of $f$ involves $O(K)$ forward times using the weighted sum aggregation (or $O(1)$ if using the maximum selection). Since we extend the dataset to $(K + 1)$ times larger than the original data, the computation complexity is $O(K^2 |D_s|)$, where |Ds| is the size of the source dataset.```\n\nThus, the computation cost is about $K^2$ or $K$ times (if using the maximum selection) than an ERM model, where $K=5$ in our experiments. Compared to DIR, as they extract $B$ spurious subgraphs from each batch to conduct the intervention, their computation cost is $B$ times compared to ERM, where $B$ could be 32. Thus, we believe the computation cost of GraphMETRO is fair for the gains we get, considering $K$ is usually small.\n\n----------------------\n\n### **Question 3: Can GraphMETRO handle multiple different types of distribution shifts that simultaneously exist in data?**\n\nYes! The distribution shift types corresponding to the gating outputs with high scores will be tackled during training. That is, if the gating output highlights multiple mixture components, their corresponding distribution shift types will be handled jointly.\n\n----------------------\n\n### **Question 4: Can GraphMETRO  tackle distribution shifts on molecular graphs?**\n\nThat is a great idea! Yes, GraphMETRO can be applied to molecular datasets if one designs transform functions to cover typical molecular variants. For instance, a transform function may add carbon structures to the molecules. These domain-specific transform functions are outside the scope of our work, however, we believe these would be interesting future work directions!\n\n----------------------\n\n# Summary \n\nWe thank the reviewer for the time and insightful suggestions! We hope our answers can address your concerns well.\n\nWe also prudently ask you to reconsider our work if the concerns are addressed. To highlight, our novelty comes from the formulation of an equivalent mixture for graph OOD and the training framework to effectively realize generalization. We also provide an in-depth analysis on our originality compared to some previous invariant learning methods. While we discussed and compared with previous works, we added more related works and modified our statement to position our work better. Finally, our method achieves great improvements on both node and graph classification tasks, and is a more general solution to mitigate multiple and nuanced distribution shifts. \n\nOverall, we believe our work proposes a new paradigm and novel training framework and makes good contributions in the fields of graph generalization, and we would appreciate your reconsideration on this point. Thank you for your efforts again!\n\n----------------------\n\n**Reference**\n\n\n[1-5] The same as listed by the reviewer\n\n\n[6] Beatrice Bevilacqua, Yangze Zhou, and Bruno Ribeiro. Size-invariant graph representations for graph classification extrapolations. In ICML, 2021.\n\n[7] Davide Buffelli, Pietro Li´o, and Fabio Vandin. Sizeshiftreg: a regularization method for improving size-generalization in graph neural networks. In NeurIPS, 2022.\n\n[8] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In NeurIPS, 2019.\n\n[9] Mucong Ding, Kezhi Kong, Jiuhai Chen, John Kirchenbauer, Micah Goldblum, David Wipf, Furong Huang, and Tom Goldstein. A closer look at distribution shifts and out-of-distribution generalization on graphs. In NeurIPS DistShift, 2021.\n\n---\n\n### **Comment 3 & Question 1: Related works on invariant learning and clarification on our statement.**\n\nThanks for pointing it out! Below we clarify the statement and clear potential misunderstanding:\n\n**(1) How does GraphMETRO compare with invariant learning methods like DIR and EERM?** \n\nPlease see our related work section where we discussed these two papers, i.e., DIR (Wu et al., 2022c) and EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on environmental patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nBesides, we also provide a more in-depth comparison in our point (3) below to highlight our key technical originality.\n\n**(2) Regarding our statement about previous works:** \n\nWhile the statement serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category, and we have provided detailed discussion about three lines of research in the related work section.\n\nTo improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to **\"previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments** (which is highly limited when confronted with nuanced distribution shifts)”. We also modified our introduction correspondingly. Thanks for letting us know our statement could be misinterpreted.\n\n**(3) Why do we say our method could be more broad than the existing invariant learning approaches?** \n\n- Invariant subgraph learning approaches, e.g., [1,2], consider variance of constructed data environments, which are designed very differently compared to our work. \nWhile they can accommodate multiple distribution shifts (as in multiple environments), these focus on patterns within each environment and ignore the variety across instances (e.g., shifts at the resolution of nodes), which may not be well-captured by the environment assignments. \n- GraphMETRO considers that specific parts of the test graph may have different shifts. Particularly, our goal is to make the generalization to unknown testing distribution more adaptive and broad, as opposed to limiting the distribution shifts to being invariant to specific types of subgraphs. \n\nIn other words, if GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering a mixture of transformations as a proxy for the target distribution shifts rather than invariance to environment shifts.\n\n**(4) Key technical originality compared to invariance learning (going more deeply)**\n\nAnother interesting view to see the innovation of GraphMETRO is that it breaks the typical invariant learning formulation, which assumes the data is manipulated by the environment variables (and then can be “decoded” into multiple environments). Instead, GraphMETRO sees the distribution shifts on an instance as a mixture, which is represented by the score vector output by the gating function over the basis of the transform functions. In other words, GraphMETRO can produce infinite environments as the elements in the score vector are continuous. One can see that once we limit the output domain of the gating function into, e.g., binary {0, 1}, GraphMETRO can also produce a limited number of environments (if we categorize the instances based on the score vector), which covers the environment construction in invariant learning. Moreover, as mentioned, we propose the concept of referential invariant representation with a base model $\\xi_0$, which is also different from previous works on invariant learning. We added the above discussion to Appendix F to improve the depth of our analysis.\n\n---\n\nWe appreciate your efforts and insightful comments! To address your concerns, we provide point-to-point responses below.\n\n---------------------\n\n### **Comment 1: Regarding the novelty of GraphMETRO.**\n\nThanks for the comment! We believe our novelty comes from the proposal of an equivalent mixture for graph OOD and the construction of our training framework, as detailed below:\n\n- **An equivalent mixture for graph OOD**: The key challenge we faced to mitigate multiple and nuanced distribution shifts is the intrinsic complexity and heterogeneity of graph distribution shifts, which simply goes beyond certain distribution shift types [6,7,8,9] or environment construction as seen in the previous methods [1,2,3]. GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts, where the mixture can be varied across different instances, and then tackled the proxy to mitigate the target distribution shifts. We believe the high-level idea is succinct, nevertheless, novel, in the sense that it provides a “middle ground” to deem graph generalization as an equivalent mixture that is more tractable.\n\n- **Training framework**: With the guide of our formulation, the training framework is still non-trivial due to two problems, i.e., “how to provide supervision for predicting the mixture” and “how to ensure the experts corresponding to mixture components are compatible when working as a whole”. Specifically, GraphMETRO solves the first problem by conducting graph extrapolation. This is somewhat similar to the spirit of graph pretraining in the sense that we inject heterogeneity to promote the expressiveness of the gating model in recognizing the mixture components. For the second problem, we introduce the concept of Referential Invariant Representation, along with the novel objective in Eq. (3) to enforce the invariance and compatibility. It is worth mentioning that the model performance is much worse than the reported numbers (e.g., 2.7\\% lower on Twitch dataset) without the compatibility constraint, indicating the proposed referential invariance concept is indispensable. \n\nWe added more justification in the introduction (updated in the revision). We genuinely hope our responses can solve your concerns about the novelty of our work.\n\n------------------------------\n\n### **Comment 2: Comparison with recent methods [3,4,5].**\n\nIn compacting the paper to fit in the page limit we mistakenly did not include these relevant references, we apologize. We added discussion in the revised version. Here we summarize these works and point out their key differences with our method: \n\n- In particular, **Yang et al. [3]** explore molecule representation learning in out-of-distribution (OOD) scenarios. They achieve this by directing the molecule encoder to utilize stable and environment-invariant substructures relevant to the labels without the need for environmental labels. \n- Similarly, **Sui et al. [4]** introduces causal attention modules to identify key invariant subgraph features that can be described as causing the graph label. The type of OOD task that Sui et al. [4] considers assumes the graph label is caused by a subgraph, which is quite different from ours. Moreover, both Yang et al. [3] and Sui et al. [4] consider tasks where the graph label is caused by a subgraph. \n- **Ma et al.[5]** is an interesting theoretical work which studies GNN generalization and examines their fairness, showing that the test subgroup's distance from the training set impacts GNN performance. Ma et al.[5], as far as we could assess, does not propose any specific architecture to solve the type of OOD tasks we consider in our work.\n\nOverall, the goal of GraphMETRO is to be invariant to a mixture of selected stochastic transform functions (and the mixture can vary across different instances), which is a more flexible and general solution. \nWe added a discussion of these works to our revision, hopefully providing a more comprehensive comparison and literature overview. We hope our responses can solve your concerns about the related work.\n\n---\n\nWe sincerely appreciate all reviewers' time, efforts, and valuable suggestions in reviewing our paper. We are glad that most of the reviewers reached a positive consensus on our work's motivation and experimental results. Here is a summary of our responses:\n\n- **Clarification**: We clarify our key assumptions (`m8JS, FURX`), novelty (`m8JS, NLg1`), and the experimental settings (`m8JS`).\n\n- **Related work**: : We add more related works on recent graph OOD learning (`NLg1, m8JS`) and graph transfer learning (`FURX`)\n\n- **Limitation**: We add discussion about the limitations of GraphMETRO when confronted with unknown distribution shifts that surpass our key assumptions (`FURX`, `C9mM`).\n\nWe hope our responses can clarify your confusion and alleviate concerns and we updated our paper **(highlighted in green)** corespondingly. We thank all reviewers again, and look forward to your reply!", "author_response": "We appreciate your comments! To address your concerns, below we prudently justify the motivation of our proposed method, clarify our assumptions, and provide details regarding our experiment implementation.\n\n---------------------\n\n### **Comment 1: Motivation of this work**\n\nThank you for this comment! We believe there might be a bit of misunderstanding due to our different definitions of *“graph shift heterogeneity”*. We firstly discuss the related works mentioned and then justify our statement:\n\n**(1) Related works**\n\nPlease see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nMoreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you! \n\nSpecifically, Sui et al. [1] proposed a graph data augmentation strategy that alleviates covariate shift by generating diverse and invariant causal features. However, the trainable augmenter they used may not distill diverse augmentations or construct unseen perturbations. Moreover, Sui et al. [1] test its method only on graph classification tasks, while GraphMETRO can be applied to both node and graph classification tasks. Besides, we have discussed graph augmentation and attention-based methods in our related works, and we added more recent works on graph OOD [2,3,4], and we hope our response clears your concern on the related work discussion.\n\n\n**(2) The definition of graph shift heterogeneity**\n\nIn this work, we refer to **“heterogeneous shifts”** as multiple and different levels of shifts which vary across different instances (nodes or graphs), as illustrated in the example in the abstract. While we agree that the existing invariant learning approches can accommodate multiple distribution shifts, it could be hard for them to tackle nuanced distribution shifts for individual instances (nodes or graphs) since the distribution shifts are inferred from variance across multiple data environments. If GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering **a mixture of transformations as a proxy** for the target distribution shifts rather than invariance to whole-graph environment shifts. This is the type of heterogeneity we are interested in our paper. \n\n**(3) Regarding our original motivation statement**\n\nWhile the statement pointed out by the reviewer serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category. And we have provided detailed discussion about three lines of research in the related work section.\n\nHowever, we agree that we could make this statement border to cover the previous invariant learning methods. To improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to \"**previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments…**”. We also modified our introduction correspondingly, we hope this will better position our work.\n\n**(4) The choice of our MoE design**\n\n- The choice of our approach comes as a consequence of our motivation to model the graph/instance shift heterogeneity. As mentioned, mitigating multiple and nuanced distribution shifts simply goes beyond certain distribution shift types or environment construction as seen in the previous methods. \n- Thus, GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts. This enables the prediction of multiple different distribution shifts and the flexibility to model fine-grained heterogeneity since the mixture can be varied across different instances. We then tackled the proxy to mitigate the target distribution shifts. Intuitively, this solution provides a **“middle ground”** to deem graph generalization as an equivalent mixture, which, we believe, is a more tractable solution.\n\nWe updated our paper to make the above point more clear. We genuinely hope our answer can justify the motivation and solve your concern."}
{"claim": "The paper's motivation is inadequately substantiated and mischaracterizes prior work on distribution shifts in graph neural networks.", "claim_type": "subjective", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ppMpJyIYRZ", "reviewer": "Reviewer_m8JS", "review_text": "Summary: The paper introduces a method to enhance the out-of-distribution performance of graph neural networks (GNN) by learning to understand distribution shifts instead of addressing the assumed ones. To achieve this, the Mixture of Experts architecture is integrated into the GNN, supplemented by an alignment procedure to recognize the shift. Empirical experiments are conducted to validate the theoretical assertion.\n\nOn the whole, I believe the proposed method lacks the necessary motivation and its novelty isn't substantial enough to meet the standard.\n\nStrengths: - The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\n- Real-world datasets back the claims through experiments.\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\n\nWeaknesses: - The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \"previous research has concentrated on addressing specific types of distribution shifts.\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \"environment generators\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\n\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\n\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\n\nQuestions: Please check Weaknesses.", "labeling_timestamp": "2026-01-11T16:30:56.563516", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors acknowledge issues in presentation and related-work positioning, add missing citations and revise their wording to avoid overstating prior work limitations, but they continue to defend their original motivation and argue it remains valid. Thus they partially address the reviewer's concern rather than fully conceding or fully contradicting it.", "evidence": "To improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to \"**previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments…**”. We also modified our introduction correspondingly.", "combined_author_response": "Dear reviewer NLg1,\n\nAs the discussion period is closing soon, we hope to engage with you and check if your concerns are addressed.\n\nWe sincerely appreciate your time and attention!\n\nBest,\n\nAuthors of Paper 1560\n\n---\n\nThanks for reading our response! We would also appreciate your patience for reading the two points below \n\n---\n\n**Regarding point #1:**\n\nYes, we agree that the concept of heterogeneous shifts is relatively new for the current studies on distribution shifts, however, this is not new for studies on network patterns (Newman, 2003; Leskovec et al., 2005; 2007; Peel et al., 2017). In fact, the ignorance of such nuanced heterogeneous shifts in the previous studies instead emphasises our motivation and the potential impact of this work. \n\nFrom a causality perspective, distribution shifts can naturally happen in the instance level when (1) additional causal variables, beyond environmental factors and randomized noise, influence these shifts, or (2) multiple causal variables simultaneously affect the shifts with different strengths. Without modeling these explicitly, the mitigation of distribution shifts can easily fail. \n\nIn terms of our presentation, we have illustrated these cases of interest in abstract and introduction, we made further explanations in Section 3.1, we also present the specific results of distribution types in Figure 3 (b). \n\n\n**Regarding point #2:**\n\nWe think we can all agree that, if the ground truth of the instance shifts is available on the real-world datasets, it would be crystal clear to see where the improvement comes from since we can conduct case study to compare our method and the baselines on instances with nuanced distribution shifts, to see the influence of the modeling these heterogeneous shifts. \n\nHowever, with such ground truth not available, we had to seek other seemingly less intuitive but also in-depth way to illustrate the insights (esp Section 4.3) as mentioned above in our previous response. This is also why we designed the synthetic experiments at the first place. We think we did try hard to explain the underlying rationale with the ground truth being absent.\n\n----\n\n**Refinement**: We can of course add a causal graph in the our assumption section to make the concept more clear. And we can illustrate more if you could let us know the specific obstable to understand our mechanism, which will be extremely helpful.\n\n----\n\n### **Summary/TL;DR**\nWe understand the reviewer's clarification concerns, however, we don't agree that they, based on our justificaton, are the cause of rejection. We believe nuanced heterogeneous shifts are common, important, yet being typically ignored in the research domain of distribution shifts, we made these argument clear and also promise to refine. While explaining the underlying rationale is hindered by the lack of ground truth, we did try hard to dissection it from the gating model, the invariant representations generated, and more ablations in the appendix. We respect the reviewer's current opinion. Still, reconsideration will be greatly appreciated.\n\n---\n\nWe thank the reviewer once again! We also enjoyed the process of making our work more sound from your suggestions. And your approval surely means a lot to us.\n\nWe do apologize for not making our solutions towards the limitations clear enough. We summarize them into a short table, hopefully could alleviate your concerns a bit more.\n\n| Limitation| Potential solutions | Location of discussion/action |\n|:---|:---|:---|\n|**Coverage of the transformations** | 1) Extend the coverage by adding representative transform functions. 2) Or include tranform functions based on domain knowledge or a few samples from target distribution | Appendix F, paragraph #3|\n|**Complexity of the transformations** | Make experts dedicated to different hyperparameters for the same type of transformation | Argument options are available in our codebase. Will make it more detailed in the experimental settings. |\n|**Label distributional shift**|Integrate the objective of the existing methods studying labe distributional shifts into our framework. |Appendix F, paragraph #4 |\n\n----\n\nWe completely agree on the existence of these limitations and we will move some limitations (esp #1) to the main paper in our final version. While they could be important in the practice, they are, in our perspective, fair \"side effects\" considering the benefits (i.e., mitigating multiple and nuanced distribution shifts and better interpretability), and may not be the central part of our novelty and main contribution (i.e., the proposal of an equivalent mixture, the concept of referential invariant representations, as well as the training framework). We are also eager to further improve other aspects.\n\nOnce again thank you so much for your support!!\n\n---\n\nDear Reviewer m8JS,\n\nA gentle nudge that we would like to know if our response adequately addresses your concerns. \n\nYour time and feedback is greatly appreciated!\n\nThank you sincerely,\n\nAuthors of Paper 1560\n\n---\n\nWe deeply appreciate your approval. Your suggestions definitely inspired us a lot and have greatly improved our work.\n\nA few additional notes for the further comments:\n\n- Regarding **point #2**: In node classification tasks, the BCE objective already considers node embeddings other than the graph embedding of the k-hop subgraph. What we described previously refers to the computational graph (i.e., k-hop message passing), which generates node embeddings. We apologize for any confusion.\n\n- Regarding **point #3**: Yes it would be quite interesting to see how graph pretraining methods perform on the current OOD benchmarks. Graph pretraining methods like GCC and GPT-GNN also consider graph extrapolation to some extent, e.g., through subgraph extraction and masked attributes/structures. The key difference between graph pretraining and generalization may lie in their different focuses on **expressiveness** and **invariance**. While these two aspects do not always conflict, ensuring invariance w.r.t. a certain type of extrapolation might affect expressiveness (if the change is relevant to labels), and vice versa. To seek a balance in between, one might need prior knowledge in which types of transformations may or may not be sensitive to the labels (and perhaps build experts with different goals to make ensure invariance or expressivess). We believe there is still a lot to explore in this domain.\n\nOverall, we are grateful for your positive stand on this work. We believe that our [current version](https://openreview.net/pdf?id=QQ5eVDIMu4), incorporating opinions from you and other reviewers, is sound and well-refined. We are also committed to further improving it. We would appreciate your support based on our current version! \n\nThank you!\n\n---\n\nDear Reviewers of Paper 1560,\n\nWe hope this message finds you well.\n\nAs the discussion phase approaches its end, we hope you find our responses useful. We would like to ask if the issues have been addressed.\n\nWe understand that the discussion time is short, and some of you might be enjoying holidays at the moment. We apologize for posting our responses a bit late as we aimed to address your concerns clearly.\n\nWe sincerely appreciate your time and attention!\n\nBest regards,\n\nAuthors of Paper 1560\n\n---\n\n**Comment 4: Related works on graph transfer learning.**\n\nYes! We agree that graph transfer learning is a relevant topic. Thank you for providing the additional related work! We added the discussion on these works in our revision, and we repeat it here for your convenience:\n\n```It is also worth mentioning that graph domain adaptation (Zhang et al., 2019; Wu et al., 2020), different from the problem studied in this work, commonly relies on limited labeled samples from target datasets for improved transferability. For instance, to generate domain adaptive network embedding, DANE (Zhang et al., 2019) uses shared weight graph convolutional networks and adversarial learning regularization, while UDA-GCN (Wu et al., 2020) employs attention mechanisms to merge global and local consistencies.```\n\nMoreover, we believe this is actually relevant to `Comment & Question 1` regarding the generality of the distribution shifts covered by the transform functions. In our future works, we added the following discussion:\n\n```...Leveraging a few samples from target distribution (i.e., domain adaptation). Specifically, we can leverage the samples from the target distribution to inform the selection or construction of transform functions, which can better guarantee the distribution shifts are covered by the transform functions.  For example, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space. We believe this would be an interesting future direction.```\n\n---\n\n# Summary\n\nOnce again thank you very much for helping us position our work and reflect the literature better. We hope all of your concerns are solved and we are happy to engage further if there are any other points we missed!\n\n---\n\n### **Reference**\n\n[1] Unsupervised Domain Adaptive Graph Convolutional Networks. Wu et al. WWW 2020\n\n[2] DANE: Domain adaptive network embedding. Zhang et al. IJCAI 2019.\n\n[3] Tong Zhao, Yozen Liu, Leonardo Neves, Oliver J. Woodford, Meng Jiang, and Neil Shah. Data augmentation for graph neural networks. AAAI, 2021.\n\n---\n\nWe are grateful for your positive feedback and detailed suggestions! We provide responses below to address your remaining concerns. \n\n---\n\n### **Comment & Question 1: Generality of the distribution shifts covered by the transform functions.**\n\nGood question! We provide the response from two angles:\n\n\n**(a) For general domain**: In our experiments, we mainly use the five stochastic transform functions, which are universal graph augmentations as listed in Zhao et al., (2021) [3]. In our code implementation, we have also included additional transform functions as shown in Appendix B. We believe these transform functions, while not exhaustive, still cover a wide range of distribution shifts observing from our experimental results.\n\nNevertheless, we agree that the real graph distribution shifts can go beyond any possible combinations of the predefined transform functions. In that case, the assumption may not hold, meaning that GraphMETRO may not capture and precisely mitigate the unknown distribution shift. This scenario could always possibly exist due to the lack of information about the testing distribution or its domain knowledge. We include it as a limitation in Appendix 5, while we further discuss how we could alleviate the problem with additional information.\n\n**(b) For specific domains where additional knowledge is available**: In fact, knowing the tendency of the distribution shifts, such as increasing malicious users in a trading system, would be very helpful in constructing the transform functions that can cover the target distribution shifts well. We believe that such knowledge can come from two sources: \n- **Domain knowledge**, e.g., on molecular datasets, the transform function could be adding additional carbon structure to a molecule (while preserving its functional groups). Or, in a particular social network, transform functions can be defined from known user behaviors. \n- **Leveraging a few samples from target distribution**. This is in fact in line with the reviewer’s Comment #4 regarding graph transfer learning. Specifically, with the guide from a few target samples, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space.\n\n We hope this response can alleviate your concern about our applicability.\n\n---\n\n### **Comment & Question 2: The mechanism of GaphMETRO in handling heterogeneous shifts within one graph.**\n\nGood question! For simplicity, suppose we have two transform functions, i.e., adding edges and dropping edges. Given a node classification task and the objective in Eq. (3), the model is trained on the extrapolated datasets based on the transform functions. After that, given an unseen graph with imbalanced distribution shifts, the gating model outputs scores to identify nodes likely to experience increasing or decreasing degrees, while others might adhere to the original distribution. Then, for each node, each expert takes its multihop subgraph, and outputs its referential invariant representation w.r.t. the correposing transform function. These expert outputs and gating model results are then aggregated to form the final representation. Assuming accurate predictions by the gating function, nodes in denser/sparser subgraphs are represented by expert models corresponding to adding/dropping edges. As each expert is trained to create invariant representations, the final node representations remain unaffected by their individual distribution shifts.\n\n---\n\n### **Comment 3: How to leverage graph pretraining.**\n\nIf we understand correctly, the reviewer was asking how to leverage a pretrained model to further aid the training of GraphMETRO. Please let us know if otherwise. \n\nThis is in fact an interesting point! We believe a model pretrained on a wide variety of data can be very helpful to initialize the gating model, which is required to output the mixture of a node or graph (after it is finetuned on the extrapolated dataset). By enhancing the gating model's predictive capability regarding mixtures, GraphMETRO's final representation should become more resilient. This becomes particularly advantageous when dealing with graphs not previously encountered in the extrapolated dataset. Moreover, it is possible that the pretrained model will also benefit the expert models, while one minor concern would be that the expert model may tend to be similar instead of dedicating to generating invariant representation w.r.t. their corresponding transform function. We added the above discussion to Appendix F as a future work. Thanks again for this comment!\n\n---\n\n### **Question 4: Why does the test accuracy vary across different transformations?**\n\nGreat question! Here we summarize three possible reasons:\n\n- **Information Preservation in Transformations:** Certain transformations retain more informative features than others. For instance, in the REDDIT-BINARY (graph classification task), the random subgraph transformation may retain more graph label-related information compared to dropping edges, as the latter tends to lose more global information. This discrepancy in testing performance, where dropping edges outperforms random subgraph extraction, could be due to the preservation of crucial information. However, conclusions may vary across datasets or tasks depending on how information influences final predictions. For CiteSeer (a node classification task), a random subgraph might preserve more local node information, potentially explaining why its testing performance surpasses dropping edges in this specific task.\n\n- **Complexity of transformation:** Certain transformations inherently generate more diverse graphs than others. If the model lacks the expressive capacity to capture such diversity, it may lead to a decline in testing performance.\n\n- **Model Sensitivity:** Certain transformations may be easier for a model to learn due to compatibility with specific model architectures. This extends beyond transformation complexity and emphasizes how different model architectures may prefer learning particular distributions from one of the extrapolated datasets, which can also contribute to the difference in the testing performance.\n\nWe included the above discussion to Appendix F: Open Discussions. We hope this response can answer your question and improve the soundness of our work.\n\n---\n\n# Summary \n\nWe are grateful for your time and insightful suggestions! \n\nWe would like to highlight that our main contribution is framing the graph generalization problem on top of an equivalent mixture, a simple yet novel and tractable \"middle ground\", as well as proposing the training framework which effectively guarantees the generalization. While our method relies on a set of predefined transform functions, we believe they cover a wide range of distribution shifts based on our empirical results. Also, we agree that there could be some scenarios where the transform functions may not cover complex distributions, and we discuss two future directions and include them into our future works. Moreover, while selecting the hyperparameters for the transform functions introduce extra complexity, the issue could be minor in practice and we also conduct more experiments to justify the applicability better. Finally, we address several questions about clarification and presentation, as well as including more future works. \n\nLastly, we prudently ask you to reevaluate our work given the clarification in our responses, which we also updated our paper correspondingly. Overall, we believe our work makes good contributions to the field of graph distribution learning by proposing a novel and effective solution, and we would appreciate your reconsideration on this point. Thank you for your efforts again!\n\n---\n\n### **Reference**\n\n[1] Menon, Aditya Krishna, et al. \"Long-tail learning via logit adjustment.\" International Conference on Learning Representations. 2020.\n\n[2] Cao, Kaidi, et al. \"Learning imbalanced datasets with label-distribution-aware margin loss.\" Advances in neural information processing systems 32 (2019).\n\n[3] DARTS: Differentiable Architecture Search. Hanxiao Liu, Karen Simonyan, Yiming Yang. 2018.\n\n---\n\n**(3) What is the complexity of the transform functions and how does it affect generalization?** \n\nInteresting question! In fact, our implementation and framework could easily avoid selecting hyperparameters on the transform functions. Specifically, we can make multiple transform functions of the same type with different ranges of hyperparameters. Specifically, GraphMETRO allows three edge dropping transform functions, $\\tau_1^{\\alpha_1}, \\tau_2^{\\alpha_2}, \\tau_3^{\\alpha_3}$, where $\\alpha_i$ (i=1, 2, 3) are three different ranges of edge dropping probabilities, e.g., [0.1,0.3], [0.3, 0.6], [0.6,0.9], representing different transform extents. Thus, given an input from the validation dataset, the gating model will highlight the transform function which simultaneously selects the corresponding hyperparameter that matches the distribution of the validation set. Interestingly, this idea is in the same spirit as how DARTS [3]  proposes to perform architecture search by formulating the task in a differentiable manner. \n\nIn our previous experiments, we did try this scheme where we replaced a single edge dropping transform function with the ratio range [0.3, 0.5] to three transform functions as mentioned above. While we didn’t see a significantly different performance in that case, we believe this would be a flexible solution which avoids the need to conduct hyperparameter selection. \n\nWe include the above discussion in our open discussion and future works (Appendix F). We hope this response can alleviate your concern about the applicability of our method. \n\n---\n\n### **Comment 4: Limitations of our work**\n\nGreat point! We think the issue of label distributional shift, while important, is orthogonal and complementary to the focus of our current study. To elaborate, label distributional shifts exert analogous impacts across various modalities, such as graphs or images. Moreover, existing methods [1,2] designed to tackle label distributional shifts can be seamlessly integrated into our proposed framework. Such integration would necessitate minimal adjustments, potentially involving modifications to the loss function or the training pipeline. We added this as a future work in Appendix F.\n\n---\n\n### **Question 1: Statistical significance of the results on Table 1**\n\nThanks for the question! We compute the p-value of our method against the best baselines method as follows:\n\n| | WebKB | Twitch | Twitter | SST2 | \n|:--|:--|:--|:--|:--|\n|p-value|< 0.001| 0.023 | 0.042| 0.081|\n\nGiven the cut-off threshold as 0.05, we believe the performances of GraphMETRO are statistically significant on WekGB, Twitch, and Twitter datasets, while on the SST2, we see relatively weak evidence. We added the p-value results to our revision and hope our response can alleviate your concern on our improvements.\n\n---\n\n###  **Question 2: Can we say we use ERM for the node classification even if nodes and their labels are not i.i.d.?** \n\nIf we understand correctly, the reviewer is asking for clarification on the 2nd term of our objective. Please let us know if otherwise. Here, our thinking is that the cross-entropy loss for node classification already assumes node labels are conditionally independent given the model (the negative log-likelihood is a sum over the labeled nodes in training). Then, we use the same assumption of cross-entropy on Empirical Risk Minimization (ERM). That is, for a given model we must also minimize the error variance across nodes. The task is then to find the model with the best performance and small variance.\n\n---\n\n### **Question 3: Numerical results on Figure 2**\n\nThanks for pointing it out! We included all of the numerical results of Figure 2 in Appendix E, while showing the results on DBLP below.\n\n| |i.i.d. (0)|noisy feature (1)|add edge (2)|drop edge (3)|drop node (4)|random subgraph (5)|\n|:--|:--|:--|:--|:--|:--|:--|\n|ERM | 85.71 | 84.48 | 71.08 | 79.69 | 83.41 | 76.9|\n|ERM-Aug | 85.66 | 85.29 | 74.85 | 82.34 | 84.44 | 72.81|\n|GraphMETRO | 85.92 | 85.78 | 76.61 | 82.95 | 84.98 | 81.32|\n\n| |(4, 5)|(3, 5)|(2, 5)|(1, 5)|(2, 4)|(1, 4)|     (2, 3)|(1, 3)|\n|:--|:--|:--|:--|:--|:--|:--|:--|:--|\n|ERM | 70.4 | 77.63 | 81.99 | 79.69 | 70.55 | 71.52 | 77.73 | 79.59|\n|ERM-Aug | 74.16 | 81.04 | 83.65 | 68.62 | 74.01 | 68.27 | 81.13 | 84.49|\n|GraphMETRO | 76.18 | 81.71 | 84.26 | 80.31 | 75.1 | 71.05 | 81.85 | 87.14|\n\nAcross all of the synthetic environments, GraphMETRO averagely outperforms ERM and ERM-Aug by 3.20% and 2.45%, respectively.\n\n---\n\nWe appreciate your comments! To address your concerns, below we prudently justify the assumption of our method, the predefined transformation functions, as well as their complexity, and clarify our presented results.\n\n---\n\n### **Comment 1-3: Applicability and our assumptions**\n\nThanks for these great comments! Here we provide response in three folds:\n\n**(1) How does GraphMETRO identify all distribution shifts from transform functions if they are treated independently when combined together?** \n\nIf we understand correctly, by “treated independently”, the reviewer is referring to the first term in our objective $\\text{BCE}(\\phi(\\tau^{(k)}(\\mathcal{G})), Y (\\tau^{(k)}))$, where we formulate predicting the distribution shifts types of a jointly transformed graph as a binary multiclass classification problem. We believe the difficulty of this task comes from both the property of transform functions and the expressiveness of the gating model. \n\n- Firstly, some transform functions are **inherently disentangled**, e.g., adding nodes feature noise and random subgraph extraction. In this case, there will be certain distinction between any pair from these three data distributions, i.e., (graphs with node noise, random subgraph graphs, random subgraphs with node noise), which the gating model can easily tell. \n- While some transform functions can be **essentially similar**, e.g., drop path and drop edges, this won’t affect the performance of our method as long as each expert outputs the corresponding invariant representation. \n- Lastly, indeed, there could be more **complex combinations of the transform functions**, which poses challenges to the gating model’s expressiveness in identifying the combinations. However, this challenge may be minor in the practice. Specifically, we observe fairly high accuracy performances of the gating model, which are above 85% and 73% averagely on extrapolated datasets with one transformation and multiple transformations, respectively.\n\nWe added the above discussion to Appendix F to enable a more comprehensive view towards our methodology. We hope this can alleviate your concern on our gating model’s performance in identifying the distribution shift types.\n\n**(2) How does the predefined transform functions cover complex distributions causing the distribution shift?**\n\nThis is also a great question! We believe there are two angles for this question. \n\n**(a) For general domain**: In our experiments, we mainly use the five stochastic transform functions, which are universal graph augmentations as listed in Zhao et al., (2021) [3]. In our code implementation, we have also included additional transform functions as shown in Appendix B. We believe these transform functions, while not exhaustive, still cover a wide range of distribution shifts observing from our experimental results.\n\nNevertheless, we agree that the real graph distribution shifts can go beyond any possible combinations of the predefined transform functions. In that case, the assumption may not hold, meaning that GraphMETRO may not capture and precisely mitigate the unknown distribution shift. This scenario could always possibly exist due to the lack of information about the testing distribution or its domain knowledge. We include it as a limitation in Appendix 5, while we further discuss how we could alleviate the problem with additional information.\n\n**(b) For specific domains where additional knowledge is available**: In fact, knowing the tendency of the distribution shifts, such as increasing malicious users in a trading system, would be very helpful in constructing the transform functions that can cover the target distribution shifts well. We believe that such knowledge can come from two sources: \n- **Domain knowledge**, e.g., on molecular datasets, the transform function could be adding additional carbon structure to a molecule (while preserving its functional groups). Or, in a particular social network, transform functions can be defined from known user behaviors. \n- **Leveraging a few samples from target distribution**. Specifically, with the guide from a few target samples, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space.\n\n---\n\n[1] Sui et al. Unleashing the Power of Graph Data Augmentation on Covariate Distribution Shift. NeurIPS 2023.\n\n[2] Nianzu Yang et al. Learning substructure invariance for out-of-distribution molecular representations, NeurIPS 2022\n\n[3] Yongduo Sui et al. Causal Attention for Interpretable and Generalizable Graph Classification, KDD 2022.\n\n[4] Jiaqi Ma et al. Subgroup Generalization and Fairness of Graph Neural Networks, NeurIPS 2022.\n\n---\n\n### **Comment 2: Our assumption**\n\nThanks for this comment! In Section 3.1, we have discussed the assumptions in detail with specifications. We repeat part of it here for your convenience:\n\n```Assumption 1 essentially states that the distribution shifts (whatever they are) can be decomposed into several mixture components of stochastic graph transformations. For example, on a social network dataset, each mixture component can represent different patterns of user behavior or network dynamics shifts. Specifically, one mixture component might correspond to increased user activity, while another could signify a particular trend of interaction within a certain group of users. Such a mixture pattern is common and well-studied in the real-world network datasets (Newman, 2003; Leskovec et al., 2005; 2007; Peel et al., 2017).```\n\nWe hope the analysis provides detailed illustration to the assumption. We are happy to add more discussion if you think anything  is still unclear!\n\n---\n\n### **Comment 2 & 3: Implementation of our method**\n\n- **Model architectures**: \nPlease see Tables 2 and 3 in Appendix A for detailed model architecture information. Specifically, we use the same encoders and classifiers from GOOD benchmarkfor real-world datasets to ensure fair comparisons. We employ backbones based on the best ERM performance for synthetic datasets. Moreover, GraphMETRO is model-agnostic, which consistently improves performance across varied model architectures.\n\n- **Stochastic transformation**: Please see Appendix B where we include the introduction of the stochastic transform functions. \n\n- **Optimization process**: Please see Table 2 and 3 (Appendix A), where we included hyperparameters on each dataset. In the second paragraph after Eq. (3), we also described our training pipeline: `“...we set apart the other loss terms from backpropagating to it to avoid interference with the training of the gating model… We optimize the objective via stochastic gradient descent”`. \n\n- **For a more intricate optimization strategy:** This is a great catch! In fact, we did try to pretrain the gating model for several epochs as warm up before training the whole model in an end-to-end fashion via the objective Eq. in (3). However, we didn’t notice a statistically significant difference in their performance, which can be due to that expert models take more time to convergence (since they need to align with the base model during the training) compared to the gating model.\n\nDue to the space limitation, we had to include most of the implementation details in the appendix while we added pointers in the main paper. However, we will try to make it more detailed and feel free to let us know if anything is missing! \n\n---\n\n### **Comment 2 (Cont.): Understand the functionality, underlying mechanism, and performance gain**\n\nThanks for the comment. For clarity, we provide the following pointers to the paper:\n\n- In Section 4.3, we provide a study to reveal the underlying **mechanism** of GraphMETRO, i.e., each expert excels in generating invariant representations concerning a stochastic transform function, which provide a solid foundation in generating referential invariant representations w.r.t. the specific transformations and further guarantee the generalization.\n\n- In Appendix C, we study the **impact of the MoE architecture** on model performance, which shows that the model performance may decline if the expressiveness of the expert model decreases. \n\n- In Appendix D, we study the **impact of the stochastic transform function** on model performance, where we also provide a detailed discussion of the modeling mechanism.\n\nWe believe the above studies and discussion in our paper provides an in-depth view, highlighting the roles of our objective, architecture, and stochastic transform function.  Please let us know if any of these perspectives is still unclear, and we can further improve our experimental study.\n\n-----\n\n# Summary\n\nWe hope our answers can address all of the concerns. We are happy to follow up if you have any further questions.\n\nWe also prudently ask you to reevaluate our work. To highlight, our motivation is supported by the common fine-grained graph shift heterogeneity, and the fact that most of the previous works could not model such nuanced distribution shifts in an effective and flexible manner. Moreover, we added more related works and modified our statement to position our work better. In general, we believe GraphMETRO is a more general and flexible solution that can mitigate a wider range of distribution shifts, which is backed by the experimental results. Detailed justification on our assumption and implementation details are also available in our paper. \n\nThus, we believe our work makes important contributions and provides a clear presentation. We are happy to discuss more and revise our paper if any concern remains. Thank you for your efforts and we are looking forward to your reply!!\n\n---\n\nWe appreciate your comments! To address your concerns, below we prudently justify the motivation of our proposed method, clarify our assumptions, and provide details regarding our experiment implementation.\n\n---------------------\n\n### **Comment 1: Motivation of this work**\n\nThank you for this comment! We believe there might be a bit of misunderstanding due to our different definitions of *“graph shift heterogeneity”*. We firstly discuss the related works mentioned and then justify our statement:\n\n**(1) Related works**\n\nPlease see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nMoreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you! \n\nSpecifically, Sui et al. [1] proposed a graph data augmentation strategy that alleviates covariate shift by generating diverse and invariant causal features. However, the trainable augmenter they used may not distill diverse augmentations or construct unseen perturbations. Moreover, Sui et al. [1] test its method only on graph classification tasks, while GraphMETRO can be applied to both node and graph classification tasks. Besides, we have discussed graph augmentation and attention-based methods in our related works, and we added more recent works on graph OOD [2,3,4], and we hope our response clears your concern on the related work discussion.\n\n\n**(2) The definition of graph shift heterogeneity**\n\nIn this work, we refer to **“heterogeneous shifts”** as multiple and different levels of shifts which vary across different instances (nodes or graphs), as illustrated in the example in the abstract. While we agree that the existing invariant learning approches can accommodate multiple distribution shifts, it could be hard for them to tackle nuanced distribution shifts for individual instances (nodes or graphs) since the distribution shifts are inferred from variance across multiple data environments. If GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering **a mixture of transformations as a proxy** for the target distribution shifts rather than invariance to whole-graph environment shifts. This is the type of heterogeneity we are interested in our paper. \n\n**(3) Regarding our original motivation statement**\n\nWhile the statement pointed out by the reviewer serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category. And we have provided detailed discussion about three lines of research in the related work section.\n\nHowever, we agree that we could make this statement border to cover the previous invariant learning methods. To improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to \"**previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments…**”. We also modified our introduction correspondingly, we hope this will better position our work.\n\n**(4) The choice of our MoE design**\n\n- The choice of our approach comes as a consequence of our motivation to model the graph/instance shift heterogeneity. As mentioned, mitigating multiple and nuanced distribution shifts simply goes beyond certain distribution shift types or environment construction as seen in the previous methods. \n- Thus, GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts. This enables the prediction of multiple different distribution shifts and the flexibility to model fine-grained heterogeneity since the mixture can be varied across different instances. We then tackled the proxy to mitigate the target distribution shifts. Intuitively, this solution provides a **“middle ground”** to deem graph generalization as an equivalent mixture, which, we believe, is a more tractable solution.\n\nWe updated our paper to make the above point more clear. We genuinely hope our answer can justify the motivation and solve your concern.\n\n---\n\n### **Question 2: How does the computational cost of GraphMETRO compare to other methods?**\n\nPlease see the last paragraph of Section 3.4, where we analyze the computation complexity of GraphMETRO. We repeat part of it here for your convenience:\n\n```Consider the scenario where we use an individual encoder for each expert. The forward process of $f$ involves $O(K)$ forward times using the weighted sum aggregation (or $O(1)$ if using the maximum selection). Since we extend the dataset to $(K + 1)$ times larger than the original data, the computation complexity is $O(K^2 |D_s|)$, where |Ds| is the size of the source dataset.```\n\nThus, the computation cost is about $K^2$ or $K$ times (if using the maximum selection) than an ERM model, where $K=5$ in our experiments. Compared to DIR, as they extract $B$ spurious subgraphs from each batch to conduct the intervention, their computation cost is $B$ times compared to ERM, where $B$ could be 32. Thus, we believe the computation cost of GraphMETRO is fair for the gains we get, considering $K$ is usually small.\n\n----------------------\n\n### **Question 3: Can GraphMETRO handle multiple different types of distribution shifts that simultaneously exist in data?**\n\nYes! The distribution shift types corresponding to the gating outputs with high scores will be tackled during training. That is, if the gating output highlights multiple mixture components, their corresponding distribution shift types will be handled jointly.\n\n----------------------\n\n### **Question 4: Can GraphMETRO  tackle distribution shifts on molecular graphs?**\n\nThat is a great idea! Yes, GraphMETRO can be applied to molecular datasets if one designs transform functions to cover typical molecular variants. For instance, a transform function may add carbon structures to the molecules. These domain-specific transform functions are outside the scope of our work, however, we believe these would be interesting future work directions!\n\n----------------------\n\n# Summary \n\nWe thank the reviewer for the time and insightful suggestions! We hope our answers can address your concerns well.\n\nWe also prudently ask you to reconsider our work if the concerns are addressed. To highlight, our novelty comes from the formulation of an equivalent mixture for graph OOD and the training framework to effectively realize generalization. We also provide an in-depth analysis on our originality compared to some previous invariant learning methods. While we discussed and compared with previous works, we added more related works and modified our statement to position our work better. Finally, our method achieves great improvements on both node and graph classification tasks, and is a more general solution to mitigate multiple and nuanced distribution shifts. \n\nOverall, we believe our work proposes a new paradigm and novel training framework and makes good contributions in the fields of graph generalization, and we would appreciate your reconsideration on this point. Thank you for your efforts again!\n\n----------------------\n\n**Reference**\n\n\n[1-5] The same as listed by the reviewer\n\n\n[6] Beatrice Bevilacqua, Yangze Zhou, and Bruno Ribeiro. Size-invariant graph representations for graph classification extrapolations. In ICML, 2021.\n\n[7] Davide Buffelli, Pietro Li´o, and Fabio Vandin. Sizeshiftreg: a regularization method for improving size-generalization in graph neural networks. In NeurIPS, 2022.\n\n[8] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In NeurIPS, 2019.\n\n[9] Mucong Ding, Kezhi Kong, Jiuhai Chen, John Kirchenbauer, Micah Goldblum, David Wipf, Furong Huang, and Tom Goldstein. A closer look at distribution shifts and out-of-distribution generalization on graphs. In NeurIPS DistShift, 2021.\n\n---\n\n### **Comment 3 & Question 1: Related works on invariant learning and clarification on our statement.**\n\nThanks for pointing it out! Below we clarify the statement and clear potential misunderstanding:\n\n**(1) How does GraphMETRO compare with invariant learning methods like DIR and EERM?** \n\nPlease see our related work section where we discussed these two papers, i.e., DIR (Wu et al., 2022c) and EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on environmental patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nBesides, we also provide a more in-depth comparison in our point (3) below to highlight our key technical originality.\n\n**(2) Regarding our statement about previous works:** \n\nWhile the statement serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category, and we have provided detailed discussion about three lines of research in the related work section.\n\nTo improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to **\"previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments** (which is highly limited when confronted with nuanced distribution shifts)”. We also modified our introduction correspondingly. Thanks for letting us know our statement could be misinterpreted.\n\n**(3) Why do we say our method could be more broad than the existing invariant learning approaches?** \n\n- Invariant subgraph learning approaches, e.g., [1,2], consider variance of constructed data environments, which are designed very differently compared to our work. \nWhile they can accommodate multiple distribution shifts (as in multiple environments), these focus on patterns within each environment and ignore the variety across instances (e.g., shifts at the resolution of nodes), which may not be well-captured by the environment assignments. \n- GraphMETRO considers that specific parts of the test graph may have different shifts. Particularly, our goal is to make the generalization to unknown testing distribution more adaptive and broad, as opposed to limiting the distribution shifts to being invariant to specific types of subgraphs. \n\nIn other words, if GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering a mixture of transformations as a proxy for the target distribution shifts rather than invariance to environment shifts.\n\n**(4) Key technical originality compared to invariance learning (going more deeply)**\n\nAnother interesting view to see the innovation of GraphMETRO is that it breaks the typical invariant learning formulation, which assumes the data is manipulated by the environment variables (and then can be “decoded” into multiple environments). Instead, GraphMETRO sees the distribution shifts on an instance as a mixture, which is represented by the score vector output by the gating function over the basis of the transform functions. In other words, GraphMETRO can produce infinite environments as the elements in the score vector are continuous. One can see that once we limit the output domain of the gating function into, e.g., binary {0, 1}, GraphMETRO can also produce a limited number of environments (if we categorize the instances based on the score vector), which covers the environment construction in invariant learning. Moreover, as mentioned, we propose the concept of referential invariant representation with a base model $\\xi_0$, which is also different from previous works on invariant learning. We added the above discussion to Appendix F to improve the depth of our analysis.\n\n---\n\nWe appreciate your efforts and insightful comments! To address your concerns, we provide point-to-point responses below.\n\n---------------------\n\n### **Comment 1: Regarding the novelty of GraphMETRO.**\n\nThanks for the comment! We believe our novelty comes from the proposal of an equivalent mixture for graph OOD and the construction of our training framework, as detailed below:\n\n- **An equivalent mixture for graph OOD**: The key challenge we faced to mitigate multiple and nuanced distribution shifts is the intrinsic complexity and heterogeneity of graph distribution shifts, which simply goes beyond certain distribution shift types [6,7,8,9] or environment construction as seen in the previous methods [1,2,3]. GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts, where the mixture can be varied across different instances, and then tackled the proxy to mitigate the target distribution shifts. We believe the high-level idea is succinct, nevertheless, novel, in the sense that it provides a “middle ground” to deem graph generalization as an equivalent mixture that is more tractable.\n\n- **Training framework**: With the guide of our formulation, the training framework is still non-trivial due to two problems, i.e., “how to provide supervision for predicting the mixture” and “how to ensure the experts corresponding to mixture components are compatible when working as a whole”. Specifically, GraphMETRO solves the first problem by conducting graph extrapolation. This is somewhat similar to the spirit of graph pretraining in the sense that we inject heterogeneity to promote the expressiveness of the gating model in recognizing the mixture components. For the second problem, we introduce the concept of Referential Invariant Representation, along with the novel objective in Eq. (3) to enforce the invariance and compatibility. It is worth mentioning that the model performance is much worse than the reported numbers (e.g., 2.7\\% lower on Twitch dataset) without the compatibility constraint, indicating the proposed referential invariance concept is indispensable. \n\nWe added more justification in the introduction (updated in the revision). We genuinely hope our responses can solve your concerns about the novelty of our work.\n\n------------------------------\n\n### **Comment 2: Comparison with recent methods [3,4,5].**\n\nIn compacting the paper to fit in the page limit we mistakenly did not include these relevant references, we apologize. We added discussion in the revised version. Here we summarize these works and point out their key differences with our method: \n\n- In particular, **Yang et al. [3]** explore molecule representation learning in out-of-distribution (OOD) scenarios. They achieve this by directing the molecule encoder to utilize stable and environment-invariant substructures relevant to the labels without the need for environmental labels. \n- Similarly, **Sui et al. [4]** introduces causal attention modules to identify key invariant subgraph features that can be described as causing the graph label. The type of OOD task that Sui et al. [4] considers assumes the graph label is caused by a subgraph, which is quite different from ours. Moreover, both Yang et al. [3] and Sui et al. [4] consider tasks where the graph label is caused by a subgraph. \n- **Ma et al.[5]** is an interesting theoretical work which studies GNN generalization and examines their fairness, showing that the test subgroup's distance from the training set impacts GNN performance. Ma et al.[5], as far as we could assess, does not propose any specific architecture to solve the type of OOD tasks we consider in our work.\n\nOverall, the goal of GraphMETRO is to be invariant to a mixture of selected stochastic transform functions (and the mixture can vary across different instances), which is a more flexible and general solution. \nWe added a discussion of these works to our revision, hopefully providing a more comprehensive comparison and literature overview. We hope our responses can solve your concerns about the related work.\n\n---\n\nWe sincerely appreciate all reviewers' time, efforts, and valuable suggestions in reviewing our paper. We are glad that most of the reviewers reached a positive consensus on our work's motivation and experimental results. Here is a summary of our responses:\n\n- **Clarification**: We clarify our key assumptions (`m8JS, FURX`), novelty (`m8JS, NLg1`), and the experimental settings (`m8JS`).\n\n- **Related work**: : We add more related works on recent graph OOD learning (`NLg1, m8JS`) and graph transfer learning (`FURX`)\n\n- **Limitation**: We add discussion about the limitations of GraphMETRO when confronted with unknown distribution shifts that surpass our key assumptions (`FURX`, `C9mM`).\n\nWe hope our responses can clarify your confusion and alleviate concerns and we updated our paper **(highlighted in green)** corespondingly. We thank all reviewers again, and look forward to your reply!", "author_response": "We appreciate your comments! To address your concerns, below we prudently justify the motivation of our proposed method, clarify our assumptions, and provide details regarding our experiment implementation.\n\n---------------------\n\n### **Comment 1: Motivation of this work**\n\nThank you for this comment! We believe there might be a bit of misunderstanding due to our different definitions of *“graph shift heterogeneity”*. We firstly discuss the related works mentioned and then justify our statement:\n\n**(1) Related works**\n\nPlease see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nMoreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you! \n\nSpecifically, Sui et al. [1] proposed a graph data augmentation strategy that alleviates covariate shift by generating diverse and invariant causal features. However, the trainable augmenter they used may not distill diverse augmentations or construct unseen perturbations. Moreover, Sui et al. [1] test its method only on graph classification tasks, while GraphMETRO can be applied to both node and graph classification tasks. Besides, we have discussed graph augmentation and attention-based methods in our related works, and we added more recent works on graph OOD [2,3,4], and we hope our response clears your concern on the related work discussion.\n\n\n**(2) The definition of graph shift heterogeneity**\n\nIn this work, we refer to **“heterogeneous shifts”** as multiple and different levels of shifts which vary across different instances (nodes or graphs), as illustrated in the example in the abstract. While we agree that the existing invariant learning approches can accommodate multiple distribution shifts, it could be hard for them to tackle nuanced distribution shifts for individual instances (nodes or graphs) since the distribution shifts are inferred from variance across multiple data environments. If GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering **a mixture of transformations as a proxy** for the target distribution shifts rather than invariance to whole-graph environment shifts. This is the type of heterogeneity we are interested in our paper. \n\n**(3) Regarding our original motivation statement**\n\nWhile the statement pointed out by the reviewer serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category. And we have provided detailed discussion about three lines of research in the related work section.\n\nHowever, we agree that we could make this statement border to cover the previous invariant learning methods. To improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to \"**previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments…**”. We also modified our introduction correspondingly, we hope this will better position our work.\n\n**(4) The choice of our MoE design**\n\n- The choice of our approach comes as a consequence of our motivation to model the graph/instance shift heterogeneity. As mentioned, mitigating multiple and nuanced distribution shifts simply goes beyond certain distribution shift types or environment construction as seen in the previous methods. \n- Thus, GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts. This enables the prediction of multiple different distribution shifts and the flexibility to model fine-grained heterogeneity since the mixture can be varied across different instances. We then tackled the proxy to mitigate the target distribution shifts. Intuitively, this solution provides a **“middle ground”** to deem graph generalization as an equivalent mixture, which, we believe, is a more tractable solution.\n\nWe updated our paper to make the above point more clear. We genuinely hope our answer can justify the motivation and solve your concern."}
{"claim": "The paper overlooks prior works that learn environment generators for GNNs to detect graph shifts (e.g., arXiv:2202.02466).", "claim_type": "subjective", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ppMpJyIYRZ", "reviewer": "Reviewer_m8JS", "review_text": "Summary: The paper introduces a method to enhance the out-of-distribution performance of graph neural networks (GNN) by learning to understand distribution shifts instead of addressing the assumed ones. To achieve this, the Mixture of Experts architecture is integrated into the GNN, supplemented by an alignment procedure to recognize the shift. Empirical experiments are conducted to validate the theoretical assertion.\n\nOn the whole, I believe the proposed method lacks the necessary motivation and its novelty isn't substantial enough to meet the standard.\n\nStrengths: - The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\n- Real-world datasets back the claims through experiments.\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\n\nWeaknesses: - The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \"previous research has concentrated on addressing specific types of distribution shifts.\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \"environment generators\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\n\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\n\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\n\nQuestions: Please check Weaknesses.", "labeling_timestamp": "2026-01-11T16:31:02.630483", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors acknowledge related prior work, state they have added discussion of relevant papers (including the one the reviewer mentioned), apologize for missing a recent reference, and updated the manuscript to include these works—therefore agreeing that related literature needed to be addressed and amending the paper accordingly.", "evidence": "\"Please see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a).\"; \"Moreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you!\"; \"We add more related works and modified our statement to position our work better.\"", "combined_author_response": "Dear reviewer NLg1,\n\nAs the discussion period is closing soon, we hope to engage with you and check if your concerns are addressed.\n\nWe sincerely appreciate your time and attention!\n\nBest,\n\nAuthors of Paper 1560\n\n---\n\nThanks for reading our response! We would also appreciate your patience for reading the two points below \n\n---\n\n**Regarding point #1:**\n\nYes, we agree that the concept of heterogeneous shifts is relatively new for the current studies on distribution shifts, however, this is not new for studies on network patterns (Newman, 2003; Leskovec et al., 2005; 2007; Peel et al., 2017). In fact, the ignorance of such nuanced heterogeneous shifts in the previous studies instead emphasises our motivation and the potential impact of this work. \n\nFrom a causality perspective, distribution shifts can naturally happen in the instance level when (1) additional causal variables, beyond environmental factors and randomized noise, influence these shifts, or (2) multiple causal variables simultaneously affect the shifts with different strengths. Without modeling these explicitly, the mitigation of distribution shifts can easily fail. \n\nIn terms of our presentation, we have illustrated these cases of interest in abstract and introduction, we made further explanations in Section 3.1, we also present the specific results of distribution types in Figure 3 (b). \n\n\n**Regarding point #2:**\n\nWe think we can all agree that, if the ground truth of the instance shifts is available on the real-world datasets, it would be crystal clear to see where the improvement comes from since we can conduct case study to compare our method and the baselines on instances with nuanced distribution shifts, to see the influence of the modeling these heterogeneous shifts. \n\nHowever, with such ground truth not available, we had to seek other seemingly less intuitive but also in-depth way to illustrate the insights (esp Section 4.3) as mentioned above in our previous response. This is also why we designed the synthetic experiments at the first place. We think we did try hard to explain the underlying rationale with the ground truth being absent.\n\n----\n\n**Refinement**: We can of course add a causal graph in the our assumption section to make the concept more clear. And we can illustrate more if you could let us know the specific obstable to understand our mechanism, which will be extremely helpful.\n\n----\n\n### **Summary/TL;DR**\nWe understand the reviewer's clarification concerns, however, we don't agree that they, based on our justificaton, are the cause of rejection. We believe nuanced heterogeneous shifts are common, important, yet being typically ignored in the research domain of distribution shifts, we made these argument clear and also promise to refine. While explaining the underlying rationale is hindered by the lack of ground truth, we did try hard to dissection it from the gating model, the invariant representations generated, and more ablations in the appendix. We respect the reviewer's current opinion. Still, reconsideration will be greatly appreciated.\n\n---\n\nWe thank the reviewer once again! We also enjoyed the process of making our work more sound from your suggestions. And your approval surely means a lot to us.\n\nWe do apologize for not making our solutions towards the limitations clear enough. We summarize them into a short table, hopefully could alleviate your concerns a bit more.\n\n| Limitation| Potential solutions | Location of discussion/action |\n|:---|:---|:---|\n|**Coverage of the transformations** | 1) Extend the coverage by adding representative transform functions. 2) Or include tranform functions based on domain knowledge or a few samples from target distribution | Appendix F, paragraph #3|\n|**Complexity of the transformations** | Make experts dedicated to different hyperparameters for the same type of transformation | Argument options are available in our codebase. Will make it more detailed in the experimental settings. |\n|**Label distributional shift**|Integrate the objective of the existing methods studying labe distributional shifts into our framework. |Appendix F, paragraph #4 |\n\n----\n\nWe completely agree on the existence of these limitations and we will move some limitations (esp #1) to the main paper in our final version. While they could be important in the practice, they are, in our perspective, fair \"side effects\" considering the benefits (i.e., mitigating multiple and nuanced distribution shifts and better interpretability), and may not be the central part of our novelty and main contribution (i.e., the proposal of an equivalent mixture, the concept of referential invariant representations, as well as the training framework). We are also eager to further improve other aspects.\n\nOnce again thank you so much for your support!!\n\n---\n\nDear Reviewer m8JS,\n\nA gentle nudge that we would like to know if our response adequately addresses your concerns. \n\nYour time and feedback is greatly appreciated!\n\nThank you sincerely,\n\nAuthors of Paper 1560\n\n---\n\nWe deeply appreciate your approval. Your suggestions definitely inspired us a lot and have greatly improved our work.\n\nA few additional notes for the further comments:\n\n- Regarding **point #2**: In node classification tasks, the BCE objective already considers node embeddings other than the graph embedding of the k-hop subgraph. What we described previously refers to the computational graph (i.e., k-hop message passing), which generates node embeddings. We apologize for any confusion.\n\n- Regarding **point #3**: Yes it would be quite interesting to see how graph pretraining methods perform on the current OOD benchmarks. Graph pretraining methods like GCC and GPT-GNN also consider graph extrapolation to some extent, e.g., through subgraph extraction and masked attributes/structures. The key difference between graph pretraining and generalization may lie in their different focuses on **expressiveness** and **invariance**. While these two aspects do not always conflict, ensuring invariance w.r.t. a certain type of extrapolation might affect expressiveness (if the change is relevant to labels), and vice versa. To seek a balance in between, one might need prior knowledge in which types of transformations may or may not be sensitive to the labels (and perhaps build experts with different goals to make ensure invariance or expressivess). We believe there is still a lot to explore in this domain.\n\nOverall, we are grateful for your positive stand on this work. We believe that our [current version](https://openreview.net/pdf?id=QQ5eVDIMu4), incorporating opinions from you and other reviewers, is sound and well-refined. We are also committed to further improving it. We would appreciate your support based on our current version! \n\nThank you!\n\n---\n\nDear Reviewers of Paper 1560,\n\nWe hope this message finds you well.\n\nAs the discussion phase approaches its end, we hope you find our responses useful. We would like to ask if the issues have been addressed.\n\nWe understand that the discussion time is short, and some of you might be enjoying holidays at the moment. We apologize for posting our responses a bit late as we aimed to address your concerns clearly.\n\nWe sincerely appreciate your time and attention!\n\nBest regards,\n\nAuthors of Paper 1560\n\n---\n\n**Comment 4: Related works on graph transfer learning.**\n\nYes! We agree that graph transfer learning is a relevant topic. Thank you for providing the additional related work! We added the discussion on these works in our revision, and we repeat it here for your convenience:\n\n```It is also worth mentioning that graph domain adaptation (Zhang et al., 2019; Wu et al., 2020), different from the problem studied in this work, commonly relies on limited labeled samples from target datasets for improved transferability. For instance, to generate domain adaptive network embedding, DANE (Zhang et al., 2019) uses shared weight graph convolutional networks and adversarial learning regularization, while UDA-GCN (Wu et al., 2020) employs attention mechanisms to merge global and local consistencies.```\n\nMoreover, we believe this is actually relevant to `Comment & Question 1` regarding the generality of the distribution shifts covered by the transform functions. In our future works, we added the following discussion:\n\n```...Leveraging a few samples from target distribution (i.e., domain adaptation). Specifically, we can leverage the samples from the target distribution to inform the selection or construction of transform functions, which can better guarantee the distribution shifts are covered by the transform functions.  For example, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space. We believe this would be an interesting future direction.```\n\n---\n\n# Summary\n\nOnce again thank you very much for helping us position our work and reflect the literature better. We hope all of your concerns are solved and we are happy to engage further if there are any other points we missed!\n\n---\n\n### **Reference**\n\n[1] Unsupervised Domain Adaptive Graph Convolutional Networks. Wu et al. WWW 2020\n\n[2] DANE: Domain adaptive network embedding. Zhang et al. IJCAI 2019.\n\n[3] Tong Zhao, Yozen Liu, Leonardo Neves, Oliver J. Woodford, Meng Jiang, and Neil Shah. Data augmentation for graph neural networks. AAAI, 2021.\n\n---\n\nWe are grateful for your positive feedback and detailed suggestions! We provide responses below to address your remaining concerns. \n\n---\n\n### **Comment & Question 1: Generality of the distribution shifts covered by the transform functions.**\n\nGood question! We provide the response from two angles:\n\n\n**(a) For general domain**: In our experiments, we mainly use the five stochastic transform functions, which are universal graph augmentations as listed in Zhao et al., (2021) [3]. In our code implementation, we have also included additional transform functions as shown in Appendix B. We believe these transform functions, while not exhaustive, still cover a wide range of distribution shifts observing from our experimental results.\n\nNevertheless, we agree that the real graph distribution shifts can go beyond any possible combinations of the predefined transform functions. In that case, the assumption may not hold, meaning that GraphMETRO may not capture and precisely mitigate the unknown distribution shift. This scenario could always possibly exist due to the lack of information about the testing distribution or its domain knowledge. We include it as a limitation in Appendix 5, while we further discuss how we could alleviate the problem with additional information.\n\n**(b) For specific domains where additional knowledge is available**: In fact, knowing the tendency of the distribution shifts, such as increasing malicious users in a trading system, would be very helpful in constructing the transform functions that can cover the target distribution shifts well. We believe that such knowledge can come from two sources: \n- **Domain knowledge**, e.g., on molecular datasets, the transform function could be adding additional carbon structure to a molecule (while preserving its functional groups). Or, in a particular social network, transform functions can be defined from known user behaviors. \n- **Leveraging a few samples from target distribution**. This is in fact in line with the reviewer’s Comment #4 regarding graph transfer learning. Specifically, with the guide from a few target samples, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space.\n\n We hope this response can alleviate your concern about our applicability.\n\n---\n\n### **Comment & Question 2: The mechanism of GaphMETRO in handling heterogeneous shifts within one graph.**\n\nGood question! For simplicity, suppose we have two transform functions, i.e., adding edges and dropping edges. Given a node classification task and the objective in Eq. (3), the model is trained on the extrapolated datasets based on the transform functions. After that, given an unseen graph with imbalanced distribution shifts, the gating model outputs scores to identify nodes likely to experience increasing or decreasing degrees, while others might adhere to the original distribution. Then, for each node, each expert takes its multihop subgraph, and outputs its referential invariant representation w.r.t. the correposing transform function. These expert outputs and gating model results are then aggregated to form the final representation. Assuming accurate predictions by the gating function, nodes in denser/sparser subgraphs are represented by expert models corresponding to adding/dropping edges. As each expert is trained to create invariant representations, the final node representations remain unaffected by their individual distribution shifts.\n\n---\n\n### **Comment 3: How to leverage graph pretraining.**\n\nIf we understand correctly, the reviewer was asking how to leverage a pretrained model to further aid the training of GraphMETRO. Please let us know if otherwise. \n\nThis is in fact an interesting point! We believe a model pretrained on a wide variety of data can be very helpful to initialize the gating model, which is required to output the mixture of a node or graph (after it is finetuned on the extrapolated dataset). By enhancing the gating model's predictive capability regarding mixtures, GraphMETRO's final representation should become more resilient. This becomes particularly advantageous when dealing with graphs not previously encountered in the extrapolated dataset. Moreover, it is possible that the pretrained model will also benefit the expert models, while one minor concern would be that the expert model may tend to be similar instead of dedicating to generating invariant representation w.r.t. their corresponding transform function. We added the above discussion to Appendix F as a future work. Thanks again for this comment!\n\n---\n\n### **Question 4: Why does the test accuracy vary across different transformations?**\n\nGreat question! Here we summarize three possible reasons:\n\n- **Information Preservation in Transformations:** Certain transformations retain more informative features than others. For instance, in the REDDIT-BINARY (graph classification task), the random subgraph transformation may retain more graph label-related information compared to dropping edges, as the latter tends to lose more global information. This discrepancy in testing performance, where dropping edges outperforms random subgraph extraction, could be due to the preservation of crucial information. However, conclusions may vary across datasets or tasks depending on how information influences final predictions. For CiteSeer (a node classification task), a random subgraph might preserve more local node information, potentially explaining why its testing performance surpasses dropping edges in this specific task.\n\n- **Complexity of transformation:** Certain transformations inherently generate more diverse graphs than others. If the model lacks the expressive capacity to capture such diversity, it may lead to a decline in testing performance.\n\n- **Model Sensitivity:** Certain transformations may be easier for a model to learn due to compatibility with specific model architectures. This extends beyond transformation complexity and emphasizes how different model architectures may prefer learning particular distributions from one of the extrapolated datasets, which can also contribute to the difference in the testing performance.\n\nWe included the above discussion to Appendix F: Open Discussions. We hope this response can answer your question and improve the soundness of our work.\n\n---\n\n# Summary \n\nWe are grateful for your time and insightful suggestions! \n\nWe would like to highlight that our main contribution is framing the graph generalization problem on top of an equivalent mixture, a simple yet novel and tractable \"middle ground\", as well as proposing the training framework which effectively guarantees the generalization. While our method relies on a set of predefined transform functions, we believe they cover a wide range of distribution shifts based on our empirical results. Also, we agree that there could be some scenarios where the transform functions may not cover complex distributions, and we discuss two future directions and include them into our future works. Moreover, while selecting the hyperparameters for the transform functions introduce extra complexity, the issue could be minor in practice and we also conduct more experiments to justify the applicability better. Finally, we address several questions about clarification and presentation, as well as including more future works. \n\nLastly, we prudently ask you to reevaluate our work given the clarification in our responses, which we also updated our paper correspondingly. Overall, we believe our work makes good contributions to the field of graph distribution learning by proposing a novel and effective solution, and we would appreciate your reconsideration on this point. Thank you for your efforts again!\n\n---\n\n### **Reference**\n\n[1] Menon, Aditya Krishna, et al. \"Long-tail learning via logit adjustment.\" International Conference on Learning Representations. 2020.\n\n[2] Cao, Kaidi, et al. \"Learning imbalanced datasets with label-distribution-aware margin loss.\" Advances in neural information processing systems 32 (2019).\n\n[3] DARTS: Differentiable Architecture Search. Hanxiao Liu, Karen Simonyan, Yiming Yang. 2018.\n\n---\n\n**(3) What is the complexity of the transform functions and how does it affect generalization?** \n\nInteresting question! In fact, our implementation and framework could easily avoid selecting hyperparameters on the transform functions. Specifically, we can make multiple transform functions of the same type with different ranges of hyperparameters. Specifically, GraphMETRO allows three edge dropping transform functions, $\\tau_1^{\\alpha_1}, \\tau_2^{\\alpha_2}, \\tau_3^{\\alpha_3}$, where $\\alpha_i$ (i=1, 2, 3) are three different ranges of edge dropping probabilities, e.g., [0.1,0.3], [0.3, 0.6], [0.6,0.9], representing different transform extents. Thus, given an input from the validation dataset, the gating model will highlight the transform function which simultaneously selects the corresponding hyperparameter that matches the distribution of the validation set. Interestingly, this idea is in the same spirit as how DARTS [3]  proposes to perform architecture search by formulating the task in a differentiable manner. \n\nIn our previous experiments, we did try this scheme where we replaced a single edge dropping transform function with the ratio range [0.3, 0.5] to three transform functions as mentioned above. While we didn’t see a significantly different performance in that case, we believe this would be a flexible solution which avoids the need to conduct hyperparameter selection. \n\nWe include the above discussion in our open discussion and future works (Appendix F). We hope this response can alleviate your concern about the applicability of our method. \n\n---\n\n### **Comment 4: Limitations of our work**\n\nGreat point! We think the issue of label distributional shift, while important, is orthogonal and complementary to the focus of our current study. To elaborate, label distributional shifts exert analogous impacts across various modalities, such as graphs or images. Moreover, existing methods [1,2] designed to tackle label distributional shifts can be seamlessly integrated into our proposed framework. Such integration would necessitate minimal adjustments, potentially involving modifications to the loss function or the training pipeline. We added this as a future work in Appendix F.\n\n---\n\n### **Question 1: Statistical significance of the results on Table 1**\n\nThanks for the question! We compute the p-value of our method against the best baselines method as follows:\n\n| | WebKB | Twitch | Twitter | SST2 | \n|:--|:--|:--|:--|:--|\n|p-value|< 0.001| 0.023 | 0.042| 0.081|\n\nGiven the cut-off threshold as 0.05, we believe the performances of GraphMETRO are statistically significant on WekGB, Twitch, and Twitter datasets, while on the SST2, we see relatively weak evidence. We added the p-value results to our revision and hope our response can alleviate your concern on our improvements.\n\n---\n\n###  **Question 2: Can we say we use ERM for the node classification even if nodes and their labels are not i.i.d.?** \n\nIf we understand correctly, the reviewer is asking for clarification on the 2nd term of our objective. Please let us know if otherwise. Here, our thinking is that the cross-entropy loss for node classification already assumes node labels are conditionally independent given the model (the negative log-likelihood is a sum over the labeled nodes in training). Then, we use the same assumption of cross-entropy on Empirical Risk Minimization (ERM). That is, for a given model we must also minimize the error variance across nodes. The task is then to find the model with the best performance and small variance.\n\n---\n\n### **Question 3: Numerical results on Figure 2**\n\nThanks for pointing it out! We included all of the numerical results of Figure 2 in Appendix E, while showing the results on DBLP below.\n\n| |i.i.d. (0)|noisy feature (1)|add edge (2)|drop edge (3)|drop node (4)|random subgraph (5)|\n|:--|:--|:--|:--|:--|:--|:--|\n|ERM | 85.71 | 84.48 | 71.08 | 79.69 | 83.41 | 76.9|\n|ERM-Aug | 85.66 | 85.29 | 74.85 | 82.34 | 84.44 | 72.81|\n|GraphMETRO | 85.92 | 85.78 | 76.61 | 82.95 | 84.98 | 81.32|\n\n| |(4, 5)|(3, 5)|(2, 5)|(1, 5)|(2, 4)|(1, 4)|     (2, 3)|(1, 3)|\n|:--|:--|:--|:--|:--|:--|:--|:--|:--|\n|ERM | 70.4 | 77.63 | 81.99 | 79.69 | 70.55 | 71.52 | 77.73 | 79.59|\n|ERM-Aug | 74.16 | 81.04 | 83.65 | 68.62 | 74.01 | 68.27 | 81.13 | 84.49|\n|GraphMETRO | 76.18 | 81.71 | 84.26 | 80.31 | 75.1 | 71.05 | 81.85 | 87.14|\n\nAcross all of the synthetic environments, GraphMETRO averagely outperforms ERM and ERM-Aug by 3.20% and 2.45%, respectively.\n\n---\n\nWe appreciate your comments! To address your concerns, below we prudently justify the assumption of our method, the predefined transformation functions, as well as their complexity, and clarify our presented results.\n\n---\n\n### **Comment 1-3: Applicability and our assumptions**\n\nThanks for these great comments! Here we provide response in three folds:\n\n**(1) How does GraphMETRO identify all distribution shifts from transform functions if they are treated independently when combined together?** \n\nIf we understand correctly, by “treated independently”, the reviewer is referring to the first term in our objective $\\text{BCE}(\\phi(\\tau^{(k)}(\\mathcal{G})), Y (\\tau^{(k)}))$, where we formulate predicting the distribution shifts types of a jointly transformed graph as a binary multiclass classification problem. We believe the difficulty of this task comes from both the property of transform functions and the expressiveness of the gating model. \n\n- Firstly, some transform functions are **inherently disentangled**, e.g., adding nodes feature noise and random subgraph extraction. In this case, there will be certain distinction between any pair from these three data distributions, i.e., (graphs with node noise, random subgraph graphs, random subgraphs with node noise), which the gating model can easily tell. \n- While some transform functions can be **essentially similar**, e.g., drop path and drop edges, this won’t affect the performance of our method as long as each expert outputs the corresponding invariant representation. \n- Lastly, indeed, there could be more **complex combinations of the transform functions**, which poses challenges to the gating model’s expressiveness in identifying the combinations. However, this challenge may be minor in the practice. Specifically, we observe fairly high accuracy performances of the gating model, which are above 85% and 73% averagely on extrapolated datasets with one transformation and multiple transformations, respectively.\n\nWe added the above discussion to Appendix F to enable a more comprehensive view towards our methodology. We hope this can alleviate your concern on our gating model’s performance in identifying the distribution shift types.\n\n**(2) How does the predefined transform functions cover complex distributions causing the distribution shift?**\n\nThis is also a great question! We believe there are two angles for this question. \n\n**(a) For general domain**: In our experiments, we mainly use the five stochastic transform functions, which are universal graph augmentations as listed in Zhao et al., (2021) [3]. In our code implementation, we have also included additional transform functions as shown in Appendix B. We believe these transform functions, while not exhaustive, still cover a wide range of distribution shifts observing from our experimental results.\n\nNevertheless, we agree that the real graph distribution shifts can go beyond any possible combinations of the predefined transform functions. In that case, the assumption may not hold, meaning that GraphMETRO may not capture and precisely mitigate the unknown distribution shift. This scenario could always possibly exist due to the lack of information about the testing distribution or its domain knowledge. We include it as a limitation in Appendix 5, while we further discuss how we could alleviate the problem with additional information.\n\n**(b) For specific domains where additional knowledge is available**: In fact, knowing the tendency of the distribution shifts, such as increasing malicious users in a trading system, would be very helpful in constructing the transform functions that can cover the target distribution shifts well. We believe that such knowledge can come from two sources: \n- **Domain knowledge**, e.g., on molecular datasets, the transform function could be adding additional carbon structure to a molecule (while preserving its functional groups). Or, in a particular social network, transform functions can be defined from known user behaviors. \n- **Leveraging a few samples from target distribution**. Specifically, with the guide from a few target samples, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space.\n\n---\n\n[1] Sui et al. Unleashing the Power of Graph Data Augmentation on Covariate Distribution Shift. NeurIPS 2023.\n\n[2] Nianzu Yang et al. Learning substructure invariance for out-of-distribution molecular representations, NeurIPS 2022\n\n[3] Yongduo Sui et al. Causal Attention for Interpretable and Generalizable Graph Classification, KDD 2022.\n\n[4] Jiaqi Ma et al. Subgroup Generalization and Fairness of Graph Neural Networks, NeurIPS 2022.\n\n---\n\n### **Comment 2: Our assumption**\n\nThanks for this comment! In Section 3.1, we have discussed the assumptions in detail with specifications. We repeat part of it here for your convenience:\n\n```Assumption 1 essentially states that the distribution shifts (whatever they are) can be decomposed into several mixture components of stochastic graph transformations. For example, on a social network dataset, each mixture component can represent different patterns of user behavior or network dynamics shifts. Specifically, one mixture component might correspond to increased user activity, while another could signify a particular trend of interaction within a certain group of users. Such a mixture pattern is common and well-studied in the real-world network datasets (Newman, 2003; Leskovec et al., 2005; 2007; Peel et al., 2017).```\n\nWe hope the analysis provides detailed illustration to the assumption. We are happy to add more discussion if you think anything  is still unclear!\n\n---\n\n### **Comment 2 & 3: Implementation of our method**\n\n- **Model architectures**: \nPlease see Tables 2 and 3 in Appendix A for detailed model architecture information. Specifically, we use the same encoders and classifiers from GOOD benchmarkfor real-world datasets to ensure fair comparisons. We employ backbones based on the best ERM performance for synthetic datasets. Moreover, GraphMETRO is model-agnostic, which consistently improves performance across varied model architectures.\n\n- **Stochastic transformation**: Please see Appendix B where we include the introduction of the stochastic transform functions. \n\n- **Optimization process**: Please see Table 2 and 3 (Appendix A), where we included hyperparameters on each dataset. In the second paragraph after Eq. (3), we also described our training pipeline: `“...we set apart the other loss terms from backpropagating to it to avoid interference with the training of the gating model… We optimize the objective via stochastic gradient descent”`. \n\n- **For a more intricate optimization strategy:** This is a great catch! In fact, we did try to pretrain the gating model for several epochs as warm up before training the whole model in an end-to-end fashion via the objective Eq. in (3). However, we didn’t notice a statistically significant difference in their performance, which can be due to that expert models take more time to convergence (since they need to align with the base model during the training) compared to the gating model.\n\nDue to the space limitation, we had to include most of the implementation details in the appendix while we added pointers in the main paper. However, we will try to make it more detailed and feel free to let us know if anything is missing! \n\n---\n\n### **Comment 2 (Cont.): Understand the functionality, underlying mechanism, and performance gain**\n\nThanks for the comment. For clarity, we provide the following pointers to the paper:\n\n- In Section 4.3, we provide a study to reveal the underlying **mechanism** of GraphMETRO, i.e., each expert excels in generating invariant representations concerning a stochastic transform function, which provide a solid foundation in generating referential invariant representations w.r.t. the specific transformations and further guarantee the generalization.\n\n- In Appendix C, we study the **impact of the MoE architecture** on model performance, which shows that the model performance may decline if the expressiveness of the expert model decreases. \n\n- In Appendix D, we study the **impact of the stochastic transform function** on model performance, where we also provide a detailed discussion of the modeling mechanism.\n\nWe believe the above studies and discussion in our paper provides an in-depth view, highlighting the roles of our objective, architecture, and stochastic transform function.  Please let us know if any of these perspectives is still unclear, and we can further improve our experimental study.\n\n-----\n\n# Summary\n\nWe hope our answers can address all of the concerns. We are happy to follow up if you have any further questions.\n\nWe also prudently ask you to reevaluate our work. To highlight, our motivation is supported by the common fine-grained graph shift heterogeneity, and the fact that most of the previous works could not model such nuanced distribution shifts in an effective and flexible manner. Moreover, we added more related works and modified our statement to position our work better. In general, we believe GraphMETRO is a more general and flexible solution that can mitigate a wider range of distribution shifts, which is backed by the experimental results. Detailed justification on our assumption and implementation details are also available in our paper. \n\nThus, we believe our work makes important contributions and provides a clear presentation. We are happy to discuss more and revise our paper if any concern remains. Thank you for your efforts and we are looking forward to your reply!!\n\n---\n\nWe appreciate your comments! To address your concerns, below we prudently justify the motivation of our proposed method, clarify our assumptions, and provide details regarding our experiment implementation.\n\n---------------------\n\n### **Comment 1: Motivation of this work**\n\nThank you for this comment! We believe there might be a bit of misunderstanding due to our different definitions of *“graph shift heterogeneity”*. We firstly discuss the related works mentioned and then justify our statement:\n\n**(1) Related works**\n\nPlease see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nMoreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you! \n\nSpecifically, Sui et al. [1] proposed a graph data augmentation strategy that alleviates covariate shift by generating diverse and invariant causal features. However, the trainable augmenter they used may not distill diverse augmentations or construct unseen perturbations. Moreover, Sui et al. [1] test its method only on graph classification tasks, while GraphMETRO can be applied to both node and graph classification tasks. Besides, we have discussed graph augmentation and attention-based methods in our related works, and we added more recent works on graph OOD [2,3,4], and we hope our response clears your concern on the related work discussion.\n\n\n**(2) The definition of graph shift heterogeneity**\n\nIn this work, we refer to **“heterogeneous shifts”** as multiple and different levels of shifts which vary across different instances (nodes or graphs), as illustrated in the example in the abstract. While we agree that the existing invariant learning approches can accommodate multiple distribution shifts, it could be hard for them to tackle nuanced distribution shifts for individual instances (nodes or graphs) since the distribution shifts are inferred from variance across multiple data environments. If GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering **a mixture of transformations as a proxy** for the target distribution shifts rather than invariance to whole-graph environment shifts. This is the type of heterogeneity we are interested in our paper. \n\n**(3) Regarding our original motivation statement**\n\nWhile the statement pointed out by the reviewer serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category. And we have provided detailed discussion about three lines of research in the related work section.\n\nHowever, we agree that we could make this statement border to cover the previous invariant learning methods. To improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to \"**previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments…**”. We also modified our introduction correspondingly, we hope this will better position our work.\n\n**(4) The choice of our MoE design**\n\n- The choice of our approach comes as a consequence of our motivation to model the graph/instance shift heterogeneity. As mentioned, mitigating multiple and nuanced distribution shifts simply goes beyond certain distribution shift types or environment construction as seen in the previous methods. \n- Thus, GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts. This enables the prediction of multiple different distribution shifts and the flexibility to model fine-grained heterogeneity since the mixture can be varied across different instances. We then tackled the proxy to mitigate the target distribution shifts. Intuitively, this solution provides a **“middle ground”** to deem graph generalization as an equivalent mixture, which, we believe, is a more tractable solution.\n\nWe updated our paper to make the above point more clear. We genuinely hope our answer can justify the motivation and solve your concern.\n\n---\n\n### **Question 2: How does the computational cost of GraphMETRO compare to other methods?**\n\nPlease see the last paragraph of Section 3.4, where we analyze the computation complexity of GraphMETRO. We repeat part of it here for your convenience:\n\n```Consider the scenario where we use an individual encoder for each expert. The forward process of $f$ involves $O(K)$ forward times using the weighted sum aggregation (or $O(1)$ if using the maximum selection). Since we extend the dataset to $(K + 1)$ times larger than the original data, the computation complexity is $O(K^2 |D_s|)$, where |Ds| is the size of the source dataset.```\n\nThus, the computation cost is about $K^2$ or $K$ times (if using the maximum selection) than an ERM model, where $K=5$ in our experiments. Compared to DIR, as they extract $B$ spurious subgraphs from each batch to conduct the intervention, their computation cost is $B$ times compared to ERM, where $B$ could be 32. Thus, we believe the computation cost of GraphMETRO is fair for the gains we get, considering $K$ is usually small.\n\n----------------------\n\n### **Question 3: Can GraphMETRO handle multiple different types of distribution shifts that simultaneously exist in data?**\n\nYes! The distribution shift types corresponding to the gating outputs with high scores will be tackled during training. That is, if the gating output highlights multiple mixture components, their corresponding distribution shift types will be handled jointly.\n\n----------------------\n\n### **Question 4: Can GraphMETRO  tackle distribution shifts on molecular graphs?**\n\nThat is a great idea! Yes, GraphMETRO can be applied to molecular datasets if one designs transform functions to cover typical molecular variants. For instance, a transform function may add carbon structures to the molecules. These domain-specific transform functions are outside the scope of our work, however, we believe these would be interesting future work directions!\n\n----------------------\n\n# Summary \n\nWe thank the reviewer for the time and insightful suggestions! We hope our answers can address your concerns well.\n\nWe also prudently ask you to reconsider our work if the concerns are addressed. To highlight, our novelty comes from the formulation of an equivalent mixture for graph OOD and the training framework to effectively realize generalization. We also provide an in-depth analysis on our originality compared to some previous invariant learning methods. While we discussed and compared with previous works, we added more related works and modified our statement to position our work better. Finally, our method achieves great improvements on both node and graph classification tasks, and is a more general solution to mitigate multiple and nuanced distribution shifts. \n\nOverall, we believe our work proposes a new paradigm and novel training framework and makes good contributions in the fields of graph generalization, and we would appreciate your reconsideration on this point. Thank you for your efforts again!\n\n----------------------\n\n**Reference**\n\n\n[1-5] The same as listed by the reviewer\n\n\n[6] Beatrice Bevilacqua, Yangze Zhou, and Bruno Ribeiro. Size-invariant graph representations for graph classification extrapolations. In ICML, 2021.\n\n[7] Davide Buffelli, Pietro Li´o, and Fabio Vandin. Sizeshiftreg: a regularization method for improving size-generalization in graph neural networks. In NeurIPS, 2022.\n\n[8] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In NeurIPS, 2019.\n\n[9] Mucong Ding, Kezhi Kong, Jiuhai Chen, John Kirchenbauer, Micah Goldblum, David Wipf, Furong Huang, and Tom Goldstein. A closer look at distribution shifts and out-of-distribution generalization on graphs. In NeurIPS DistShift, 2021.\n\n---\n\n### **Comment 3 & Question 1: Related works on invariant learning and clarification on our statement.**\n\nThanks for pointing it out! Below we clarify the statement and clear potential misunderstanding:\n\n**(1) How does GraphMETRO compare with invariant learning methods like DIR and EERM?** \n\nPlease see our related work section where we discussed these two papers, i.e., DIR (Wu et al., 2022c) and EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on environmental patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nBesides, we also provide a more in-depth comparison in our point (3) below to highlight our key technical originality.\n\n**(2) Regarding our statement about previous works:** \n\nWhile the statement serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category, and we have provided detailed discussion about three lines of research in the related work section.\n\nTo improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to **\"previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments** (which is highly limited when confronted with nuanced distribution shifts)”. We also modified our introduction correspondingly. Thanks for letting us know our statement could be misinterpreted.\n\n**(3) Why do we say our method could be more broad than the existing invariant learning approaches?** \n\n- Invariant subgraph learning approaches, e.g., [1,2], consider variance of constructed data environments, which are designed very differently compared to our work. \nWhile they can accommodate multiple distribution shifts (as in multiple environments), these focus on patterns within each environment and ignore the variety across instances (e.g., shifts at the resolution of nodes), which may not be well-captured by the environment assignments. \n- GraphMETRO considers that specific parts of the test graph may have different shifts. Particularly, our goal is to make the generalization to unknown testing distribution more adaptive and broad, as opposed to limiting the distribution shifts to being invariant to specific types of subgraphs. \n\nIn other words, if GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering a mixture of transformations as a proxy for the target distribution shifts rather than invariance to environment shifts.\n\n**(4) Key technical originality compared to invariance learning (going more deeply)**\n\nAnother interesting view to see the innovation of GraphMETRO is that it breaks the typical invariant learning formulation, which assumes the data is manipulated by the environment variables (and then can be “decoded” into multiple environments). Instead, GraphMETRO sees the distribution shifts on an instance as a mixture, which is represented by the score vector output by the gating function over the basis of the transform functions. In other words, GraphMETRO can produce infinite environments as the elements in the score vector are continuous. One can see that once we limit the output domain of the gating function into, e.g., binary {0, 1}, GraphMETRO can also produce a limited number of environments (if we categorize the instances based on the score vector), which covers the environment construction in invariant learning. Moreover, as mentioned, we propose the concept of referential invariant representation with a base model $\\xi_0$, which is also different from previous works on invariant learning. We added the above discussion to Appendix F to improve the depth of our analysis.\n\n---\n\nWe appreciate your efforts and insightful comments! To address your concerns, we provide point-to-point responses below.\n\n---------------------\n\n### **Comment 1: Regarding the novelty of GraphMETRO.**\n\nThanks for the comment! We believe our novelty comes from the proposal of an equivalent mixture for graph OOD and the construction of our training framework, as detailed below:\n\n- **An equivalent mixture for graph OOD**: The key challenge we faced to mitigate multiple and nuanced distribution shifts is the intrinsic complexity and heterogeneity of graph distribution shifts, which simply goes beyond certain distribution shift types [6,7,8,9] or environment construction as seen in the previous methods [1,2,3]. GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts, where the mixture can be varied across different instances, and then tackled the proxy to mitigate the target distribution shifts. We believe the high-level idea is succinct, nevertheless, novel, in the sense that it provides a “middle ground” to deem graph generalization as an equivalent mixture that is more tractable.\n\n- **Training framework**: With the guide of our formulation, the training framework is still non-trivial due to two problems, i.e., “how to provide supervision for predicting the mixture” and “how to ensure the experts corresponding to mixture components are compatible when working as a whole”. Specifically, GraphMETRO solves the first problem by conducting graph extrapolation. This is somewhat similar to the spirit of graph pretraining in the sense that we inject heterogeneity to promote the expressiveness of the gating model in recognizing the mixture components. For the second problem, we introduce the concept of Referential Invariant Representation, along with the novel objective in Eq. (3) to enforce the invariance and compatibility. It is worth mentioning that the model performance is much worse than the reported numbers (e.g., 2.7\\% lower on Twitch dataset) without the compatibility constraint, indicating the proposed referential invariance concept is indispensable. \n\nWe added more justification in the introduction (updated in the revision). We genuinely hope our responses can solve your concerns about the novelty of our work.\n\n------------------------------\n\n### **Comment 2: Comparison with recent methods [3,4,5].**\n\nIn compacting the paper to fit in the page limit we mistakenly did not include these relevant references, we apologize. We added discussion in the revised version. Here we summarize these works and point out their key differences with our method: \n\n- In particular, **Yang et al. [3]** explore molecule representation learning in out-of-distribution (OOD) scenarios. They achieve this by directing the molecule encoder to utilize stable and environment-invariant substructures relevant to the labels without the need for environmental labels. \n- Similarly, **Sui et al. [4]** introduces causal attention modules to identify key invariant subgraph features that can be described as causing the graph label. The type of OOD task that Sui et al. [4] considers assumes the graph label is caused by a subgraph, which is quite different from ours. Moreover, both Yang et al. [3] and Sui et al. [4] consider tasks where the graph label is caused by a subgraph. \n- **Ma et al.[5]** is an interesting theoretical work which studies GNN generalization and examines their fairness, showing that the test subgroup's distance from the training set impacts GNN performance. Ma et al.[5], as far as we could assess, does not propose any specific architecture to solve the type of OOD tasks we consider in our work.\n\nOverall, the goal of GraphMETRO is to be invariant to a mixture of selected stochastic transform functions (and the mixture can vary across different instances), which is a more flexible and general solution. \nWe added a discussion of these works to our revision, hopefully providing a more comprehensive comparison and literature overview. We hope our responses can solve your concerns about the related work.\n\n---\n\nWe sincerely appreciate all reviewers' time, efforts, and valuable suggestions in reviewing our paper. We are glad that most of the reviewers reached a positive consensus on our work's motivation and experimental results. Here is a summary of our responses:\n\n- **Clarification**: We clarify our key assumptions (`m8JS, FURX`), novelty (`m8JS, NLg1`), and the experimental settings (`m8JS`).\n\n- **Related work**: : We add more related works on recent graph OOD learning (`NLg1, m8JS`) and graph transfer learning (`FURX`)\n\n- **Limitation**: We add discussion about the limitations of GraphMETRO when confronted with unknown distribution shifts that surpass our key assumptions (`FURX`, `C9mM`).\n\nWe hope our responses can clarify your confusion and alleviate concerns and we updated our paper **(highlighted in green)** corespondingly. We thank all reviewers again, and look forward to your reply!", "author_response": "We appreciate your comments! To address your concerns, below we prudently justify the motivation of our proposed method, clarify our assumptions, and provide details regarding our experiment implementation.\n\n---------------------\n\n### **Comment 1: Motivation of this work**\n\nThank you for this comment! We believe there might be a bit of misunderstanding due to our different definitions of *“graph shift heterogeneity”*. We firstly discuss the related works mentioned and then justify our statement:\n\n**(1) Related works**\n\nPlease see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nMoreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you! \n\nSpecifically, Sui et al. [1] proposed a graph data augmentation strategy that alleviates covariate shift by generating diverse and invariant causal features. However, the trainable augmenter they used may not distill diverse augmentations or construct unseen perturbations. Moreover, Sui et al. [1] test its method only on graph classification tasks, while GraphMETRO can be applied to both node and graph classification tasks. Besides, we have discussed graph augmentation and attention-based methods in our related works, and we added more recent works on graph OOD [2,3,4], and we hope our response clears your concern on the related work discussion.\n\n\n**(2) The definition of graph shift heterogeneity**\n\nIn this work, we refer to **“heterogeneous shifts”** as multiple and different levels of shifts which vary across different instances (nodes or graphs), as illustrated in the example in the abstract. While we agree that the existing invariant learning approches can accommodate multiple distribution shifts, it could be hard for them to tackle nuanced distribution shifts for individual instances (nodes or graphs) since the distribution shifts are inferred from variance across multiple data environments. If GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering **a mixture of transformations as a proxy** for the target distribution shifts rather than invariance to whole-graph environment shifts. This is the type of heterogeneity we are interested in our paper. \n\n**(3) Regarding our original motivation statement**\n\nWhile the statement pointed out by the reviewer serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category. And we have provided detailed discussion about three lines of research in the related work section.\n\nHowever, we agree that we could make this statement border to cover the previous invariant learning methods. To improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to \"**previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments…**”. We also modified our introduction correspondingly, we hope this will better position our work.\n\n**(4) The choice of our MoE design**\n\n- The choice of our approach comes as a consequence of our motivation to model the graph/instance shift heterogeneity. As mentioned, mitigating multiple and nuanced distribution shifts simply goes beyond certain distribution shift types or environment construction as seen in the previous methods. \n- Thus, GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts. This enables the prediction of multiple different distribution shifts and the flexibility to model fine-grained heterogeneity since the mixture can be varied across different instances. We then tackled the proxy to mitigate the target distribution shifts. Intuitively, this solution provides a **“middle ground”** to deem graph generalization as an equivalent mixture, which, we believe, is a more tractable solution.\n\nWe updated our paper to make the above point more clear. We genuinely hope our answer can justify the motivation and solve your concern."}
{"claim": "Existing works focus on learning shift-specific graph transformations (e.g., arXiv:2211.02843), but the paper fails to acknowledge these approaches.", "claim_type": "subjective", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ppMpJyIYRZ", "reviewer": "Reviewer_m8JS", "review_text": "Summary: The paper introduces a method to enhance the out-of-distribution performance of graph neural networks (GNN) by learning to understand distribution shifts instead of addressing the assumed ones. To achieve this, the Mixture of Experts architecture is integrated into the GNN, supplemented by an alignment procedure to recognize the shift. Empirical experiments are conducted to validate the theoretical assertion.\n\nOn the whole, I believe the proposed method lacks the necessary motivation and its novelty isn't substantial enough to meet the standard.\n\nStrengths: - The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\n- Real-world datasets back the claims through experiments.\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\n\nWeaknesses: - The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \"previous research has concentrated on addressing specific types of distribution shifts.\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \"environment generators\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\n\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\n\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\n\nQuestions: Please check Weaknesses.", "labeling_timestamp": "2026-01-11T16:31:13.121024", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors acknowledge the related works, apologize for omissions, state they have added the missing references and discussions (including the specific papers mentioned), and explain how their work differs — i.e., they both agree the approaches are relevant and have updated the manuscript to include them.", "evidence": "“Please see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a). ... Moreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you!”; and “In compacting the paper to fit in the page limit we mistakenly did not include these relevant references, we apologize. We added discussion in the revised version.”", "combined_author_response": "Dear reviewer NLg1,\n\nAs the discussion period is closing soon, we hope to engage with you and check if your concerns are addressed.\n\nWe sincerely appreciate your time and attention!\n\nBest,\n\nAuthors of Paper 1560\n\n---\n\nThanks for reading our response! We would also appreciate your patience for reading the two points below \n\n---\n\n**Regarding point #1:**\n\nYes, we agree that the concept of heterogeneous shifts is relatively new for the current studies on distribution shifts, however, this is not new for studies on network patterns (Newman, 2003; Leskovec et al., 2005; 2007; Peel et al., 2017). In fact, the ignorance of such nuanced heterogeneous shifts in the previous studies instead emphasises our motivation and the potential impact of this work. \n\nFrom a causality perspective, distribution shifts can naturally happen in the instance level when (1) additional causal variables, beyond environmental factors and randomized noise, influence these shifts, or (2) multiple causal variables simultaneously affect the shifts with different strengths. Without modeling these explicitly, the mitigation of distribution shifts can easily fail. \n\nIn terms of our presentation, we have illustrated these cases of interest in abstract and introduction, we made further explanations in Section 3.1, we also present the specific results of distribution types in Figure 3 (b). \n\n\n**Regarding point #2:**\n\nWe think we can all agree that, if the ground truth of the instance shifts is available on the real-world datasets, it would be crystal clear to see where the improvement comes from since we can conduct case study to compare our method and the baselines on instances with nuanced distribution shifts, to see the influence of the modeling these heterogeneous shifts. \n\nHowever, with such ground truth not available, we had to seek other seemingly less intuitive but also in-depth way to illustrate the insights (esp Section 4.3) as mentioned above in our previous response. This is also why we designed the synthetic experiments at the first place. We think we did try hard to explain the underlying rationale with the ground truth being absent.\n\n----\n\n**Refinement**: We can of course add a causal graph in the our assumption section to make the concept more clear. And we can illustrate more if you could let us know the specific obstable to understand our mechanism, which will be extremely helpful.\n\n----\n\n### **Summary/TL;DR**\nWe understand the reviewer's clarification concerns, however, we don't agree that they, based on our justificaton, are the cause of rejection. We believe nuanced heterogeneous shifts are common, important, yet being typically ignored in the research domain of distribution shifts, we made these argument clear and also promise to refine. While explaining the underlying rationale is hindered by the lack of ground truth, we did try hard to dissection it from the gating model, the invariant representations generated, and more ablations in the appendix. We respect the reviewer's current opinion. Still, reconsideration will be greatly appreciated.\n\n---\n\nWe thank the reviewer once again! We also enjoyed the process of making our work more sound from your suggestions. And your approval surely means a lot to us.\n\nWe do apologize for not making our solutions towards the limitations clear enough. We summarize them into a short table, hopefully could alleviate your concerns a bit more.\n\n| Limitation| Potential solutions | Location of discussion/action |\n|:---|:---|:---|\n|**Coverage of the transformations** | 1) Extend the coverage by adding representative transform functions. 2) Or include tranform functions based on domain knowledge or a few samples from target distribution | Appendix F, paragraph #3|\n|**Complexity of the transformations** | Make experts dedicated to different hyperparameters for the same type of transformation | Argument options are available in our codebase. Will make it more detailed in the experimental settings. |\n|**Label distributional shift**|Integrate the objective of the existing methods studying labe distributional shifts into our framework. |Appendix F, paragraph #4 |\n\n----\n\nWe completely agree on the existence of these limitations and we will move some limitations (esp #1) to the main paper in our final version. While they could be important in the practice, they are, in our perspective, fair \"side effects\" considering the benefits (i.e., mitigating multiple and nuanced distribution shifts and better interpretability), and may not be the central part of our novelty and main contribution (i.e., the proposal of an equivalent mixture, the concept of referential invariant representations, as well as the training framework). We are also eager to further improve other aspects.\n\nOnce again thank you so much for your support!!\n\n---\n\nDear Reviewer m8JS,\n\nA gentle nudge that we would like to know if our response adequately addresses your concerns. \n\nYour time and feedback is greatly appreciated!\n\nThank you sincerely,\n\nAuthors of Paper 1560\n\n---\n\nWe deeply appreciate your approval. Your suggestions definitely inspired us a lot and have greatly improved our work.\n\nA few additional notes for the further comments:\n\n- Regarding **point #2**: In node classification tasks, the BCE objective already considers node embeddings other than the graph embedding of the k-hop subgraph. What we described previously refers to the computational graph (i.e., k-hop message passing), which generates node embeddings. We apologize for any confusion.\n\n- Regarding **point #3**: Yes it would be quite interesting to see how graph pretraining methods perform on the current OOD benchmarks. Graph pretraining methods like GCC and GPT-GNN also consider graph extrapolation to some extent, e.g., through subgraph extraction and masked attributes/structures. The key difference between graph pretraining and generalization may lie in their different focuses on **expressiveness** and **invariance**. While these two aspects do not always conflict, ensuring invariance w.r.t. a certain type of extrapolation might affect expressiveness (if the change is relevant to labels), and vice versa. To seek a balance in between, one might need prior knowledge in which types of transformations may or may not be sensitive to the labels (and perhaps build experts with different goals to make ensure invariance or expressivess). We believe there is still a lot to explore in this domain.\n\nOverall, we are grateful for your positive stand on this work. We believe that our [current version](https://openreview.net/pdf?id=QQ5eVDIMu4), incorporating opinions from you and other reviewers, is sound and well-refined. We are also committed to further improving it. We would appreciate your support based on our current version! \n\nThank you!\n\n---\n\nDear Reviewers of Paper 1560,\n\nWe hope this message finds you well.\n\nAs the discussion phase approaches its end, we hope you find our responses useful. We would like to ask if the issues have been addressed.\n\nWe understand that the discussion time is short, and some of you might be enjoying holidays at the moment. We apologize for posting our responses a bit late as we aimed to address your concerns clearly.\n\nWe sincerely appreciate your time and attention!\n\nBest regards,\n\nAuthors of Paper 1560\n\n---\n\n**Comment 4: Related works on graph transfer learning.**\n\nYes! We agree that graph transfer learning is a relevant topic. Thank you for providing the additional related work! We added the discussion on these works in our revision, and we repeat it here for your convenience:\n\n```It is also worth mentioning that graph domain adaptation (Zhang et al., 2019; Wu et al., 2020), different from the problem studied in this work, commonly relies on limited labeled samples from target datasets for improved transferability. For instance, to generate domain adaptive network embedding, DANE (Zhang et al., 2019) uses shared weight graph convolutional networks and adversarial learning regularization, while UDA-GCN (Wu et al., 2020) employs attention mechanisms to merge global and local consistencies.```\n\nMoreover, we believe this is actually relevant to `Comment & Question 1` regarding the generality of the distribution shifts covered by the transform functions. In our future works, we added the following discussion:\n\n```...Leveraging a few samples from target distribution (i.e., domain adaptation). Specifically, we can leverage the samples from the target distribution to inform the selection or construction of transform functions, which can better guarantee the distribution shifts are covered by the transform functions.  For example, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space. We believe this would be an interesting future direction.```\n\n---\n\n# Summary\n\nOnce again thank you very much for helping us position our work and reflect the literature better. We hope all of your concerns are solved and we are happy to engage further if there are any other points we missed!\n\n---\n\n### **Reference**\n\n[1] Unsupervised Domain Adaptive Graph Convolutional Networks. Wu et al. WWW 2020\n\n[2] DANE: Domain adaptive network embedding. Zhang et al. IJCAI 2019.\n\n[3] Tong Zhao, Yozen Liu, Leonardo Neves, Oliver J. Woodford, Meng Jiang, and Neil Shah. Data augmentation for graph neural networks. AAAI, 2021.\n\n---\n\nWe are grateful for your positive feedback and detailed suggestions! We provide responses below to address your remaining concerns. \n\n---\n\n### **Comment & Question 1: Generality of the distribution shifts covered by the transform functions.**\n\nGood question! We provide the response from two angles:\n\n\n**(a) For general domain**: In our experiments, we mainly use the five stochastic transform functions, which are universal graph augmentations as listed in Zhao et al., (2021) [3]. In our code implementation, we have also included additional transform functions as shown in Appendix B. We believe these transform functions, while not exhaustive, still cover a wide range of distribution shifts observing from our experimental results.\n\nNevertheless, we agree that the real graph distribution shifts can go beyond any possible combinations of the predefined transform functions. In that case, the assumption may not hold, meaning that GraphMETRO may not capture and precisely mitigate the unknown distribution shift. This scenario could always possibly exist due to the lack of information about the testing distribution or its domain knowledge. We include it as a limitation in Appendix 5, while we further discuss how we could alleviate the problem with additional information.\n\n**(b) For specific domains where additional knowledge is available**: In fact, knowing the tendency of the distribution shifts, such as increasing malicious users in a trading system, would be very helpful in constructing the transform functions that can cover the target distribution shifts well. We believe that such knowledge can come from two sources: \n- **Domain knowledge**, e.g., on molecular datasets, the transform function could be adding additional carbon structure to a molecule (while preserving its functional groups). Or, in a particular social network, transform functions can be defined from known user behaviors. \n- **Leveraging a few samples from target distribution**. This is in fact in line with the reviewer’s Comment #4 regarding graph transfer learning. Specifically, with the guide from a few target samples, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space.\n\n We hope this response can alleviate your concern about our applicability.\n\n---\n\n### **Comment & Question 2: The mechanism of GaphMETRO in handling heterogeneous shifts within one graph.**\n\nGood question! For simplicity, suppose we have two transform functions, i.e., adding edges and dropping edges. Given a node classification task and the objective in Eq. (3), the model is trained on the extrapolated datasets based on the transform functions. After that, given an unseen graph with imbalanced distribution shifts, the gating model outputs scores to identify nodes likely to experience increasing or decreasing degrees, while others might adhere to the original distribution. Then, for each node, each expert takes its multihop subgraph, and outputs its referential invariant representation w.r.t. the correposing transform function. These expert outputs and gating model results are then aggregated to form the final representation. Assuming accurate predictions by the gating function, nodes in denser/sparser subgraphs are represented by expert models corresponding to adding/dropping edges. As each expert is trained to create invariant representations, the final node representations remain unaffected by their individual distribution shifts.\n\n---\n\n### **Comment 3: How to leverage graph pretraining.**\n\nIf we understand correctly, the reviewer was asking how to leverage a pretrained model to further aid the training of GraphMETRO. Please let us know if otherwise. \n\nThis is in fact an interesting point! We believe a model pretrained on a wide variety of data can be very helpful to initialize the gating model, which is required to output the mixture of a node or graph (after it is finetuned on the extrapolated dataset). By enhancing the gating model's predictive capability regarding mixtures, GraphMETRO's final representation should become more resilient. This becomes particularly advantageous when dealing with graphs not previously encountered in the extrapolated dataset. Moreover, it is possible that the pretrained model will also benefit the expert models, while one minor concern would be that the expert model may tend to be similar instead of dedicating to generating invariant representation w.r.t. their corresponding transform function. We added the above discussion to Appendix F as a future work. Thanks again for this comment!\n\n---\n\n### **Question 4: Why does the test accuracy vary across different transformations?**\n\nGreat question! Here we summarize three possible reasons:\n\n- **Information Preservation in Transformations:** Certain transformations retain more informative features than others. For instance, in the REDDIT-BINARY (graph classification task), the random subgraph transformation may retain more graph label-related information compared to dropping edges, as the latter tends to lose more global information. This discrepancy in testing performance, where dropping edges outperforms random subgraph extraction, could be due to the preservation of crucial information. However, conclusions may vary across datasets or tasks depending on how information influences final predictions. For CiteSeer (a node classification task), a random subgraph might preserve more local node information, potentially explaining why its testing performance surpasses dropping edges in this specific task.\n\n- **Complexity of transformation:** Certain transformations inherently generate more diverse graphs than others. If the model lacks the expressive capacity to capture such diversity, it may lead to a decline in testing performance.\n\n- **Model Sensitivity:** Certain transformations may be easier for a model to learn due to compatibility with specific model architectures. This extends beyond transformation complexity and emphasizes how different model architectures may prefer learning particular distributions from one of the extrapolated datasets, which can also contribute to the difference in the testing performance.\n\nWe included the above discussion to Appendix F: Open Discussions. We hope this response can answer your question and improve the soundness of our work.\n\n---\n\n# Summary \n\nWe are grateful for your time and insightful suggestions! \n\nWe would like to highlight that our main contribution is framing the graph generalization problem on top of an equivalent mixture, a simple yet novel and tractable \"middle ground\", as well as proposing the training framework which effectively guarantees the generalization. While our method relies on a set of predefined transform functions, we believe they cover a wide range of distribution shifts based on our empirical results. Also, we agree that there could be some scenarios where the transform functions may not cover complex distributions, and we discuss two future directions and include them into our future works. Moreover, while selecting the hyperparameters for the transform functions introduce extra complexity, the issue could be minor in practice and we also conduct more experiments to justify the applicability better. Finally, we address several questions about clarification and presentation, as well as including more future works. \n\nLastly, we prudently ask you to reevaluate our work given the clarification in our responses, which we also updated our paper correspondingly. Overall, we believe our work makes good contributions to the field of graph distribution learning by proposing a novel and effective solution, and we would appreciate your reconsideration on this point. Thank you for your efforts again!\n\n---\n\n### **Reference**\n\n[1] Menon, Aditya Krishna, et al. \"Long-tail learning via logit adjustment.\" International Conference on Learning Representations. 2020.\n\n[2] Cao, Kaidi, et al. \"Learning imbalanced datasets with label-distribution-aware margin loss.\" Advances in neural information processing systems 32 (2019).\n\n[3] DARTS: Differentiable Architecture Search. Hanxiao Liu, Karen Simonyan, Yiming Yang. 2018.\n\n---\n\n**(3) What is the complexity of the transform functions and how does it affect generalization?** \n\nInteresting question! In fact, our implementation and framework could easily avoid selecting hyperparameters on the transform functions. Specifically, we can make multiple transform functions of the same type with different ranges of hyperparameters. Specifically, GraphMETRO allows three edge dropping transform functions, $\\tau_1^{\\alpha_1}, \\tau_2^{\\alpha_2}, \\tau_3^{\\alpha_3}$, where $\\alpha_i$ (i=1, 2, 3) are three different ranges of edge dropping probabilities, e.g., [0.1,0.3], [0.3, 0.6], [0.6,0.9], representing different transform extents. Thus, given an input from the validation dataset, the gating model will highlight the transform function which simultaneously selects the corresponding hyperparameter that matches the distribution of the validation set. Interestingly, this idea is in the same spirit as how DARTS [3]  proposes to perform architecture search by formulating the task in a differentiable manner. \n\nIn our previous experiments, we did try this scheme where we replaced a single edge dropping transform function with the ratio range [0.3, 0.5] to three transform functions as mentioned above. While we didn’t see a significantly different performance in that case, we believe this would be a flexible solution which avoids the need to conduct hyperparameter selection. \n\nWe include the above discussion in our open discussion and future works (Appendix F). We hope this response can alleviate your concern about the applicability of our method. \n\n---\n\n### **Comment 4: Limitations of our work**\n\nGreat point! We think the issue of label distributional shift, while important, is orthogonal and complementary to the focus of our current study. To elaborate, label distributional shifts exert analogous impacts across various modalities, such as graphs or images. Moreover, existing methods [1,2] designed to tackle label distributional shifts can be seamlessly integrated into our proposed framework. Such integration would necessitate minimal adjustments, potentially involving modifications to the loss function or the training pipeline. We added this as a future work in Appendix F.\n\n---\n\n### **Question 1: Statistical significance of the results on Table 1**\n\nThanks for the question! We compute the p-value of our method against the best baselines method as follows:\n\n| | WebKB | Twitch | Twitter | SST2 | \n|:--|:--|:--|:--|:--|\n|p-value|< 0.001| 0.023 | 0.042| 0.081|\n\nGiven the cut-off threshold as 0.05, we believe the performances of GraphMETRO are statistically significant on WekGB, Twitch, and Twitter datasets, while on the SST2, we see relatively weak evidence. We added the p-value results to our revision and hope our response can alleviate your concern on our improvements.\n\n---\n\n###  **Question 2: Can we say we use ERM for the node classification even if nodes and their labels are not i.i.d.?** \n\nIf we understand correctly, the reviewer is asking for clarification on the 2nd term of our objective. Please let us know if otherwise. Here, our thinking is that the cross-entropy loss for node classification already assumes node labels are conditionally independent given the model (the negative log-likelihood is a sum over the labeled nodes in training). Then, we use the same assumption of cross-entropy on Empirical Risk Minimization (ERM). That is, for a given model we must also minimize the error variance across nodes. The task is then to find the model with the best performance and small variance.\n\n---\n\n### **Question 3: Numerical results on Figure 2**\n\nThanks for pointing it out! We included all of the numerical results of Figure 2 in Appendix E, while showing the results on DBLP below.\n\n| |i.i.d. (0)|noisy feature (1)|add edge (2)|drop edge (3)|drop node (4)|random subgraph (5)|\n|:--|:--|:--|:--|:--|:--|:--|\n|ERM | 85.71 | 84.48 | 71.08 | 79.69 | 83.41 | 76.9|\n|ERM-Aug | 85.66 | 85.29 | 74.85 | 82.34 | 84.44 | 72.81|\n|GraphMETRO | 85.92 | 85.78 | 76.61 | 82.95 | 84.98 | 81.32|\n\n| |(4, 5)|(3, 5)|(2, 5)|(1, 5)|(2, 4)|(1, 4)|     (2, 3)|(1, 3)|\n|:--|:--|:--|:--|:--|:--|:--|:--|:--|\n|ERM | 70.4 | 77.63 | 81.99 | 79.69 | 70.55 | 71.52 | 77.73 | 79.59|\n|ERM-Aug | 74.16 | 81.04 | 83.65 | 68.62 | 74.01 | 68.27 | 81.13 | 84.49|\n|GraphMETRO | 76.18 | 81.71 | 84.26 | 80.31 | 75.1 | 71.05 | 81.85 | 87.14|\n\nAcross all of the synthetic environments, GraphMETRO averagely outperforms ERM and ERM-Aug by 3.20% and 2.45%, respectively.\n\n---\n\nWe appreciate your comments! To address your concerns, below we prudently justify the assumption of our method, the predefined transformation functions, as well as their complexity, and clarify our presented results.\n\n---\n\n### **Comment 1-3: Applicability and our assumptions**\n\nThanks for these great comments! Here we provide response in three folds:\n\n**(1) How does GraphMETRO identify all distribution shifts from transform functions if they are treated independently when combined together?** \n\nIf we understand correctly, by “treated independently”, the reviewer is referring to the first term in our objective $\\text{BCE}(\\phi(\\tau^{(k)}(\\mathcal{G})), Y (\\tau^{(k)}))$, where we formulate predicting the distribution shifts types of a jointly transformed graph as a binary multiclass classification problem. We believe the difficulty of this task comes from both the property of transform functions and the expressiveness of the gating model. \n\n- Firstly, some transform functions are **inherently disentangled**, e.g., adding nodes feature noise and random subgraph extraction. In this case, there will be certain distinction between any pair from these three data distributions, i.e., (graphs with node noise, random subgraph graphs, random subgraphs with node noise), which the gating model can easily tell. \n- While some transform functions can be **essentially similar**, e.g., drop path and drop edges, this won’t affect the performance of our method as long as each expert outputs the corresponding invariant representation. \n- Lastly, indeed, there could be more **complex combinations of the transform functions**, which poses challenges to the gating model’s expressiveness in identifying the combinations. However, this challenge may be minor in the practice. Specifically, we observe fairly high accuracy performances of the gating model, which are above 85% and 73% averagely on extrapolated datasets with one transformation and multiple transformations, respectively.\n\nWe added the above discussion to Appendix F to enable a more comprehensive view towards our methodology. We hope this can alleviate your concern on our gating model’s performance in identifying the distribution shift types.\n\n**(2) How does the predefined transform functions cover complex distributions causing the distribution shift?**\n\nThis is also a great question! We believe there are two angles for this question. \n\n**(a) For general domain**: In our experiments, we mainly use the five stochastic transform functions, which are universal graph augmentations as listed in Zhao et al., (2021) [3]. In our code implementation, we have also included additional transform functions as shown in Appendix B. We believe these transform functions, while not exhaustive, still cover a wide range of distribution shifts observing from our experimental results.\n\nNevertheless, we agree that the real graph distribution shifts can go beyond any possible combinations of the predefined transform functions. In that case, the assumption may not hold, meaning that GraphMETRO may not capture and precisely mitigate the unknown distribution shift. This scenario could always possibly exist due to the lack of information about the testing distribution or its domain knowledge. We include it as a limitation in Appendix 5, while we further discuss how we could alleviate the problem with additional information.\n\n**(b) For specific domains where additional knowledge is available**: In fact, knowing the tendency of the distribution shifts, such as increasing malicious users in a trading system, would be very helpful in constructing the transform functions that can cover the target distribution shifts well. We believe that such knowledge can come from two sources: \n- **Domain knowledge**, e.g., on molecular datasets, the transform function could be adding additional carbon structure to a molecule (while preserving its functional groups). Or, in a particular social network, transform functions can be defined from known user behaviors. \n- **Leveraging a few samples from target distribution**. Specifically, with the guide from a few target samples, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space.\n\n---\n\n[1] Sui et al. Unleashing the Power of Graph Data Augmentation on Covariate Distribution Shift. NeurIPS 2023.\n\n[2] Nianzu Yang et al. Learning substructure invariance for out-of-distribution molecular representations, NeurIPS 2022\n\n[3] Yongduo Sui et al. Causal Attention for Interpretable and Generalizable Graph Classification, KDD 2022.\n\n[4] Jiaqi Ma et al. Subgroup Generalization and Fairness of Graph Neural Networks, NeurIPS 2022.\n\n---\n\n### **Comment 2: Our assumption**\n\nThanks for this comment! In Section 3.1, we have discussed the assumptions in detail with specifications. We repeat part of it here for your convenience:\n\n```Assumption 1 essentially states that the distribution shifts (whatever they are) can be decomposed into several mixture components of stochastic graph transformations. For example, on a social network dataset, each mixture component can represent different patterns of user behavior or network dynamics shifts. Specifically, one mixture component might correspond to increased user activity, while another could signify a particular trend of interaction within a certain group of users. Such a mixture pattern is common and well-studied in the real-world network datasets (Newman, 2003; Leskovec et al., 2005; 2007; Peel et al., 2017).```\n\nWe hope the analysis provides detailed illustration to the assumption. We are happy to add more discussion if you think anything  is still unclear!\n\n---\n\n### **Comment 2 & 3: Implementation of our method**\n\n- **Model architectures**: \nPlease see Tables 2 and 3 in Appendix A for detailed model architecture information. Specifically, we use the same encoders and classifiers from GOOD benchmarkfor real-world datasets to ensure fair comparisons. We employ backbones based on the best ERM performance for synthetic datasets. Moreover, GraphMETRO is model-agnostic, which consistently improves performance across varied model architectures.\n\n- **Stochastic transformation**: Please see Appendix B where we include the introduction of the stochastic transform functions. \n\n- **Optimization process**: Please see Table 2 and 3 (Appendix A), where we included hyperparameters on each dataset. In the second paragraph after Eq. (3), we also described our training pipeline: `“...we set apart the other loss terms from backpropagating to it to avoid interference with the training of the gating model… We optimize the objective via stochastic gradient descent”`. \n\n- **For a more intricate optimization strategy:** This is a great catch! In fact, we did try to pretrain the gating model for several epochs as warm up before training the whole model in an end-to-end fashion via the objective Eq. in (3). However, we didn’t notice a statistically significant difference in their performance, which can be due to that expert models take more time to convergence (since they need to align with the base model during the training) compared to the gating model.\n\nDue to the space limitation, we had to include most of the implementation details in the appendix while we added pointers in the main paper. However, we will try to make it more detailed and feel free to let us know if anything is missing! \n\n---\n\n### **Comment 2 (Cont.): Understand the functionality, underlying mechanism, and performance gain**\n\nThanks for the comment. For clarity, we provide the following pointers to the paper:\n\n- In Section 4.3, we provide a study to reveal the underlying **mechanism** of GraphMETRO, i.e., each expert excels in generating invariant representations concerning a stochastic transform function, which provide a solid foundation in generating referential invariant representations w.r.t. the specific transformations and further guarantee the generalization.\n\n- In Appendix C, we study the **impact of the MoE architecture** on model performance, which shows that the model performance may decline if the expressiveness of the expert model decreases. \n\n- In Appendix D, we study the **impact of the stochastic transform function** on model performance, where we also provide a detailed discussion of the modeling mechanism.\n\nWe believe the above studies and discussion in our paper provides an in-depth view, highlighting the roles of our objective, architecture, and stochastic transform function.  Please let us know if any of these perspectives is still unclear, and we can further improve our experimental study.\n\n-----\n\n# Summary\n\nWe hope our answers can address all of the concerns. We are happy to follow up if you have any further questions.\n\nWe also prudently ask you to reevaluate our work. To highlight, our motivation is supported by the common fine-grained graph shift heterogeneity, and the fact that most of the previous works could not model such nuanced distribution shifts in an effective and flexible manner. Moreover, we added more related works and modified our statement to position our work better. In general, we believe GraphMETRO is a more general and flexible solution that can mitigate a wider range of distribution shifts, which is backed by the experimental results. Detailed justification on our assumption and implementation details are also available in our paper. \n\nThus, we believe our work makes important contributions and provides a clear presentation. We are happy to discuss more and revise our paper if any concern remains. Thank you for your efforts and we are looking forward to your reply!!\n\n---\n\nWe appreciate your comments! To address your concerns, below we prudently justify the motivation of our proposed method, clarify our assumptions, and provide details regarding our experiment implementation.\n\n---------------------\n\n### **Comment 1: Motivation of this work**\n\nThank you for this comment! We believe there might be a bit of misunderstanding due to our different definitions of *“graph shift heterogeneity”*. We firstly discuss the related works mentioned and then justify our statement:\n\n**(1) Related works**\n\nPlease see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nMoreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you! \n\nSpecifically, Sui et al. [1] proposed a graph data augmentation strategy that alleviates covariate shift by generating diverse and invariant causal features. However, the trainable augmenter they used may not distill diverse augmentations or construct unseen perturbations. Moreover, Sui et al. [1] test its method only on graph classification tasks, while GraphMETRO can be applied to both node and graph classification tasks. Besides, we have discussed graph augmentation and attention-based methods in our related works, and we added more recent works on graph OOD [2,3,4], and we hope our response clears your concern on the related work discussion.\n\n\n**(2) The definition of graph shift heterogeneity**\n\nIn this work, we refer to **“heterogeneous shifts”** as multiple and different levels of shifts which vary across different instances (nodes or graphs), as illustrated in the example in the abstract. While we agree that the existing invariant learning approches can accommodate multiple distribution shifts, it could be hard for them to tackle nuanced distribution shifts for individual instances (nodes or graphs) since the distribution shifts are inferred from variance across multiple data environments. If GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering **a mixture of transformations as a proxy** for the target distribution shifts rather than invariance to whole-graph environment shifts. This is the type of heterogeneity we are interested in our paper. \n\n**(3) Regarding our original motivation statement**\n\nWhile the statement pointed out by the reviewer serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category. And we have provided detailed discussion about three lines of research in the related work section.\n\nHowever, we agree that we could make this statement border to cover the previous invariant learning methods. To improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to \"**previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments…**”. We also modified our introduction correspondingly, we hope this will better position our work.\n\n**(4) The choice of our MoE design**\n\n- The choice of our approach comes as a consequence of our motivation to model the graph/instance shift heterogeneity. As mentioned, mitigating multiple and nuanced distribution shifts simply goes beyond certain distribution shift types or environment construction as seen in the previous methods. \n- Thus, GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts. This enables the prediction of multiple different distribution shifts and the flexibility to model fine-grained heterogeneity since the mixture can be varied across different instances. We then tackled the proxy to mitigate the target distribution shifts. Intuitively, this solution provides a **“middle ground”** to deem graph generalization as an equivalent mixture, which, we believe, is a more tractable solution.\n\nWe updated our paper to make the above point more clear. We genuinely hope our answer can justify the motivation and solve your concern.\n\n---\n\n### **Question 2: How does the computational cost of GraphMETRO compare to other methods?**\n\nPlease see the last paragraph of Section 3.4, where we analyze the computation complexity of GraphMETRO. We repeat part of it here for your convenience:\n\n```Consider the scenario where we use an individual encoder for each expert. The forward process of $f$ involves $O(K)$ forward times using the weighted sum aggregation (or $O(1)$ if using the maximum selection). Since we extend the dataset to $(K + 1)$ times larger than the original data, the computation complexity is $O(K^2 |D_s|)$, where |Ds| is the size of the source dataset.```\n\nThus, the computation cost is about $K^2$ or $K$ times (if using the maximum selection) than an ERM model, where $K=5$ in our experiments. Compared to DIR, as they extract $B$ spurious subgraphs from each batch to conduct the intervention, their computation cost is $B$ times compared to ERM, where $B$ could be 32. Thus, we believe the computation cost of GraphMETRO is fair for the gains we get, considering $K$ is usually small.\n\n----------------------\n\n### **Question 3: Can GraphMETRO handle multiple different types of distribution shifts that simultaneously exist in data?**\n\nYes! The distribution shift types corresponding to the gating outputs with high scores will be tackled during training. That is, if the gating output highlights multiple mixture components, their corresponding distribution shift types will be handled jointly.\n\n----------------------\n\n### **Question 4: Can GraphMETRO  tackle distribution shifts on molecular graphs?**\n\nThat is a great idea! Yes, GraphMETRO can be applied to molecular datasets if one designs transform functions to cover typical molecular variants. For instance, a transform function may add carbon structures to the molecules. These domain-specific transform functions are outside the scope of our work, however, we believe these would be interesting future work directions!\n\n----------------------\n\n# Summary \n\nWe thank the reviewer for the time and insightful suggestions! We hope our answers can address your concerns well.\n\nWe also prudently ask you to reconsider our work if the concerns are addressed. To highlight, our novelty comes from the formulation of an equivalent mixture for graph OOD and the training framework to effectively realize generalization. We also provide an in-depth analysis on our originality compared to some previous invariant learning methods. While we discussed and compared with previous works, we added more related works and modified our statement to position our work better. Finally, our method achieves great improvements on both node and graph classification tasks, and is a more general solution to mitigate multiple and nuanced distribution shifts. \n\nOverall, we believe our work proposes a new paradigm and novel training framework and makes good contributions in the fields of graph generalization, and we would appreciate your reconsideration on this point. Thank you for your efforts again!\n\n----------------------\n\n**Reference**\n\n\n[1-5] The same as listed by the reviewer\n\n\n[6] Beatrice Bevilacqua, Yangze Zhou, and Bruno Ribeiro. Size-invariant graph representations for graph classification extrapolations. In ICML, 2021.\n\n[7] Davide Buffelli, Pietro Li´o, and Fabio Vandin. Sizeshiftreg: a regularization method for improving size-generalization in graph neural networks. In NeurIPS, 2022.\n\n[8] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In NeurIPS, 2019.\n\n[9] Mucong Ding, Kezhi Kong, Jiuhai Chen, John Kirchenbauer, Micah Goldblum, David Wipf, Furong Huang, and Tom Goldstein. A closer look at distribution shifts and out-of-distribution generalization on graphs. In NeurIPS DistShift, 2021.\n\n---\n\n### **Comment 3 & Question 1: Related works on invariant learning and clarification on our statement.**\n\nThanks for pointing it out! Below we clarify the statement and clear potential misunderstanding:\n\n**(1) How does GraphMETRO compare with invariant learning methods like DIR and EERM?** \n\nPlease see our related work section where we discussed these two papers, i.e., DIR (Wu et al., 2022c) and EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on environmental patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nBesides, we also provide a more in-depth comparison in our point (3) below to highlight our key technical originality.\n\n**(2) Regarding our statement about previous works:** \n\nWhile the statement serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category, and we have provided detailed discussion about three lines of research in the related work section.\n\nTo improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to **\"previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments** (which is highly limited when confronted with nuanced distribution shifts)”. We also modified our introduction correspondingly. Thanks for letting us know our statement could be misinterpreted.\n\n**(3) Why do we say our method could be more broad than the existing invariant learning approaches?** \n\n- Invariant subgraph learning approaches, e.g., [1,2], consider variance of constructed data environments, which are designed very differently compared to our work. \nWhile they can accommodate multiple distribution shifts (as in multiple environments), these focus on patterns within each environment and ignore the variety across instances (e.g., shifts at the resolution of nodes), which may not be well-captured by the environment assignments. \n- GraphMETRO considers that specific parts of the test graph may have different shifts. Particularly, our goal is to make the generalization to unknown testing distribution more adaptive and broad, as opposed to limiting the distribution shifts to being invariant to specific types of subgraphs. \n\nIn other words, if GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering a mixture of transformations as a proxy for the target distribution shifts rather than invariance to environment shifts.\n\n**(4) Key technical originality compared to invariance learning (going more deeply)**\n\nAnother interesting view to see the innovation of GraphMETRO is that it breaks the typical invariant learning formulation, which assumes the data is manipulated by the environment variables (and then can be “decoded” into multiple environments). Instead, GraphMETRO sees the distribution shifts on an instance as a mixture, which is represented by the score vector output by the gating function over the basis of the transform functions. In other words, GraphMETRO can produce infinite environments as the elements in the score vector are continuous. One can see that once we limit the output domain of the gating function into, e.g., binary {0, 1}, GraphMETRO can also produce a limited number of environments (if we categorize the instances based on the score vector), which covers the environment construction in invariant learning. Moreover, as mentioned, we propose the concept of referential invariant representation with a base model $\\xi_0$, which is also different from previous works on invariant learning. We added the above discussion to Appendix F to improve the depth of our analysis.\n\n---\n\nWe appreciate your efforts and insightful comments! To address your concerns, we provide point-to-point responses below.\n\n---------------------\n\n### **Comment 1: Regarding the novelty of GraphMETRO.**\n\nThanks for the comment! We believe our novelty comes from the proposal of an equivalent mixture for graph OOD and the construction of our training framework, as detailed below:\n\n- **An equivalent mixture for graph OOD**: The key challenge we faced to mitigate multiple and nuanced distribution shifts is the intrinsic complexity and heterogeneity of graph distribution shifts, which simply goes beyond certain distribution shift types [6,7,8,9] or environment construction as seen in the previous methods [1,2,3]. GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts, where the mixture can be varied across different instances, and then tackled the proxy to mitigate the target distribution shifts. We believe the high-level idea is succinct, nevertheless, novel, in the sense that it provides a “middle ground” to deem graph generalization as an equivalent mixture that is more tractable.\n\n- **Training framework**: With the guide of our formulation, the training framework is still non-trivial due to two problems, i.e., “how to provide supervision for predicting the mixture” and “how to ensure the experts corresponding to mixture components are compatible when working as a whole”. Specifically, GraphMETRO solves the first problem by conducting graph extrapolation. This is somewhat similar to the spirit of graph pretraining in the sense that we inject heterogeneity to promote the expressiveness of the gating model in recognizing the mixture components. For the second problem, we introduce the concept of Referential Invariant Representation, along with the novel objective in Eq. (3) to enforce the invariance and compatibility. It is worth mentioning that the model performance is much worse than the reported numbers (e.g., 2.7\\% lower on Twitch dataset) without the compatibility constraint, indicating the proposed referential invariance concept is indispensable. \n\nWe added more justification in the introduction (updated in the revision). We genuinely hope our responses can solve your concerns about the novelty of our work.\n\n------------------------------\n\n### **Comment 2: Comparison with recent methods [3,4,5].**\n\nIn compacting the paper to fit in the page limit we mistakenly did not include these relevant references, we apologize. We added discussion in the revised version. Here we summarize these works and point out their key differences with our method: \n\n- In particular, **Yang et al. [3]** explore molecule representation learning in out-of-distribution (OOD) scenarios. They achieve this by directing the molecule encoder to utilize stable and environment-invariant substructures relevant to the labels without the need for environmental labels. \n- Similarly, **Sui et al. [4]** introduces causal attention modules to identify key invariant subgraph features that can be described as causing the graph label. The type of OOD task that Sui et al. [4] considers assumes the graph label is caused by a subgraph, which is quite different from ours. Moreover, both Yang et al. [3] and Sui et al. [4] consider tasks where the graph label is caused by a subgraph. \n- **Ma et al.[5]** is an interesting theoretical work which studies GNN generalization and examines their fairness, showing that the test subgroup's distance from the training set impacts GNN performance. Ma et al.[5], as far as we could assess, does not propose any specific architecture to solve the type of OOD tasks we consider in our work.\n\nOverall, the goal of GraphMETRO is to be invariant to a mixture of selected stochastic transform functions (and the mixture can vary across different instances), which is a more flexible and general solution. \nWe added a discussion of these works to our revision, hopefully providing a more comprehensive comparison and literature overview. We hope our responses can solve your concerns about the related work.\n\n---\n\nWe sincerely appreciate all reviewers' time, efforts, and valuable suggestions in reviewing our paper. We are glad that most of the reviewers reached a positive consensus on our work's motivation and experimental results. Here is a summary of our responses:\n\n- **Clarification**: We clarify our key assumptions (`m8JS, FURX`), novelty (`m8JS, NLg1`), and the experimental settings (`m8JS`).\n\n- **Related work**: : We add more related works on recent graph OOD learning (`NLg1, m8JS`) and graph transfer learning (`FURX`)\n\n- **Limitation**: We add discussion about the limitations of GraphMETRO when confronted with unknown distribution shifts that surpass our key assumptions (`FURX`, `C9mM`).\n\nWe hope our responses can clarify your confusion and alleviate concerns and we updated our paper **(highlighted in green)** corespondingly. We thank all reviewers again, and look forward to your reply!", "author_response": "We appreciate your comments! To address your concerns, below we prudently justify the motivation of our proposed method, clarify our assumptions, and provide details regarding our experiment implementation.\n\n---------------------\n\n### **Comment 1: Motivation of this work**\n\nThank you for this comment! We believe there might be a bit of misunderstanding due to our different definitions of *“graph shift heterogeneity”*. We firstly discuss the related works mentioned and then justify our statement:\n\n**(1) Related works**\n\nPlease see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nMoreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you! \n\nSpecifically, Sui et al. [1] proposed a graph data augmentation strategy that alleviates covariate shift by generating diverse and invariant causal features. However, the trainable augmenter they used may not distill diverse augmentations or construct unseen perturbations. Moreover, Sui et al. [1] test its method only on graph classification tasks, while GraphMETRO can be applied to both node and graph classification tasks. Besides, we have discussed graph augmentation and attention-based methods in our related works, and we added more recent works on graph OOD [2,3,4], and we hope our response clears your concern on the related work discussion.\n\n\n**(2) The definition of graph shift heterogeneity**\n\nIn this work, we refer to **“heterogeneous shifts”** as multiple and different levels of shifts which vary across different instances (nodes or graphs), as illustrated in the example in the abstract. While we agree that the existing invariant learning approches can accommodate multiple distribution shifts, it could be hard for them to tackle nuanced distribution shifts for individual instances (nodes or graphs) since the distribution shifts are inferred from variance across multiple data environments. If GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering **a mixture of transformations as a proxy** for the target distribution shifts rather than invariance to whole-graph environment shifts. This is the type of heterogeneity we are interested in our paper. \n\n**(3) Regarding our original motivation statement**\n\nWhile the statement pointed out by the reviewer serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category. And we have provided detailed discussion about three lines of research in the related work section.\n\nHowever, we agree that we could make this statement border to cover the previous invariant learning methods. To improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to \"**previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments…**”. We also modified our introduction correspondingly, we hope this will better position our work.\n\n**(4) The choice of our MoE design**\n\n- The choice of our approach comes as a consequence of our motivation to model the graph/instance shift heterogeneity. As mentioned, mitigating multiple and nuanced distribution shifts simply goes beyond certain distribution shift types or environment construction as seen in the previous methods. \n- Thus, GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts. This enables the prediction of multiple different distribution shifts and the flexibility to model fine-grained heterogeneity since the mixture can be varied across different instances. We then tackled the proxy to mitigate the target distribution shifts. Intuitively, this solution provides a **“middle ground”** to deem graph generalization as an equivalent mixture, which, we believe, is a more tractable solution.\n\nWe updated our paper to make the above point more clear. We genuinely hope our answer can justify the motivation and solve your concern."}
{"claim": "The authors do not provide a detailed or robust justification for choosing Mixture of Experts (MOE) over alternative approaches.", "claim_type": "subjective", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ppMpJyIYRZ", "reviewer": "Reviewer_m8JS", "review_text": "Summary: The paper introduces a method to enhance the out-of-distribution performance of graph neural networks (GNN) by learning to understand distribution shifts instead of addressing the assumed ones. To achieve this, the Mixture of Experts architecture is integrated into the GNN, supplemented by an alignment procedure to recognize the shift. Empirical experiments are conducted to validate the theoretical assertion.\n\nOn the whole, I believe the proposed method lacks the necessary motivation and its novelty isn't substantial enough to meet the standard.\n\nStrengths: - The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\n- Real-world datasets back the claims through experiments.\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\n\nWeaknesses: - The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \"previous research has concentrated on addressing specific types of distribution shifts.\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \"environment generators\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\n\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\n\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\n\nQuestions: Please check Weaknesses.", "labeling_timestamp": "2026-01-11T16:31:25.448669", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors provide a conceptual justification for using an MoE (modeling instance-level heterogeneous shifts via a mixture of transformations) and point to empirical/ablation analyses (Appendix C, compatibility constraint) that support the design, but they do not fully present direct, detailed comparisons to alternative architectures or exhaustive empirical evidence demonstrating MoE superiority over other approaches.", "evidence": "“Thus, GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts. This enables the prediction of multiple different distribution shifts and the flexibility to model fine-grained heterogeneity since the mixture can be varied across different instances. We then tackled the proxy to mitigate the target distribution shifts. Intuitively, this solution provides a ‘middle ground’ to deem graph generalization as an equivalent mixture, which, we believe, is a more tractable solution.” | “In Appendix C, we study the **impact of the MoE architecture** on model performance, which shows that the model performance may decline if the expressiveness of the expert model decreases.” | “It is worth mentioning that the model performance is much worse than the reported numbers (e.g., 2.7% lower on Twitch dataset) without the compatibility constraint, indicating the proposed referential invariance concept is indispensable.”", "combined_author_response": "Dear reviewer NLg1,\n\nAs the discussion period is closing soon, we hope to engage with you and check if your concerns are addressed.\n\nWe sincerely appreciate your time and attention!\n\nBest,\n\nAuthors of Paper 1560\n\n---\n\nThanks for reading our response! We would also appreciate your patience for reading the two points below \n\n---\n\n**Regarding point #1:**\n\nYes, we agree that the concept of heterogeneous shifts is relatively new for the current studies on distribution shifts, however, this is not new for studies on network patterns (Newman, 2003; Leskovec et al., 2005; 2007; Peel et al., 2017). In fact, the ignorance of such nuanced heterogeneous shifts in the previous studies instead emphasises our motivation and the potential impact of this work. \n\nFrom a causality perspective, distribution shifts can naturally happen in the instance level when (1) additional causal variables, beyond environmental factors and randomized noise, influence these shifts, or (2) multiple causal variables simultaneously affect the shifts with different strengths. Without modeling these explicitly, the mitigation of distribution shifts can easily fail. \n\nIn terms of our presentation, we have illustrated these cases of interest in abstract and introduction, we made further explanations in Section 3.1, we also present the specific results of distribution types in Figure 3 (b). \n\n\n**Regarding point #2:**\n\nWe think we can all agree that, if the ground truth of the instance shifts is available on the real-world datasets, it would be crystal clear to see where the improvement comes from since we can conduct case study to compare our method and the baselines on instances with nuanced distribution shifts, to see the influence of the modeling these heterogeneous shifts. \n\nHowever, with such ground truth not available, we had to seek other seemingly less intuitive but also in-depth way to illustrate the insights (esp Section 4.3) as mentioned above in our previous response. This is also why we designed the synthetic experiments at the first place. We think we did try hard to explain the underlying rationale with the ground truth being absent.\n\n----\n\n**Refinement**: We can of course add a causal graph in the our assumption section to make the concept more clear. And we can illustrate more if you could let us know the specific obstable to understand our mechanism, which will be extremely helpful.\n\n----\n\n### **Summary/TL;DR**\nWe understand the reviewer's clarification concerns, however, we don't agree that they, based on our justificaton, are the cause of rejection. We believe nuanced heterogeneous shifts are common, important, yet being typically ignored in the research domain of distribution shifts, we made these argument clear and also promise to refine. While explaining the underlying rationale is hindered by the lack of ground truth, we did try hard to dissection it from the gating model, the invariant representations generated, and more ablations in the appendix. We respect the reviewer's current opinion. Still, reconsideration will be greatly appreciated.\n\n---\n\nWe thank the reviewer once again! We also enjoyed the process of making our work more sound from your suggestions. And your approval surely means a lot to us.\n\nWe do apologize for not making our solutions towards the limitations clear enough. We summarize them into a short table, hopefully could alleviate your concerns a bit more.\n\n| Limitation| Potential solutions | Location of discussion/action |\n|:---|:---|:---|\n|**Coverage of the transformations** | 1) Extend the coverage by adding representative transform functions. 2) Or include tranform functions based on domain knowledge or a few samples from target distribution | Appendix F, paragraph #3|\n|**Complexity of the transformations** | Make experts dedicated to different hyperparameters for the same type of transformation | Argument options are available in our codebase. Will make it more detailed in the experimental settings. |\n|**Label distributional shift**|Integrate the objective of the existing methods studying labe distributional shifts into our framework. |Appendix F, paragraph #4 |\n\n----\n\nWe completely agree on the existence of these limitations and we will move some limitations (esp #1) to the main paper in our final version. While they could be important in the practice, they are, in our perspective, fair \"side effects\" considering the benefits (i.e., mitigating multiple and nuanced distribution shifts and better interpretability), and may not be the central part of our novelty and main contribution (i.e., the proposal of an equivalent mixture, the concept of referential invariant representations, as well as the training framework). We are also eager to further improve other aspects.\n\nOnce again thank you so much for your support!!\n\n---\n\nDear Reviewer m8JS,\n\nA gentle nudge that we would like to know if our response adequately addresses your concerns. \n\nYour time and feedback is greatly appreciated!\n\nThank you sincerely,\n\nAuthors of Paper 1560\n\n---\n\nWe deeply appreciate your approval. Your suggestions definitely inspired us a lot and have greatly improved our work.\n\nA few additional notes for the further comments:\n\n- Regarding **point #2**: In node classification tasks, the BCE objective already considers node embeddings other than the graph embedding of the k-hop subgraph. What we described previously refers to the computational graph (i.e., k-hop message passing), which generates node embeddings. We apologize for any confusion.\n\n- Regarding **point #3**: Yes it would be quite interesting to see how graph pretraining methods perform on the current OOD benchmarks. Graph pretraining methods like GCC and GPT-GNN also consider graph extrapolation to some extent, e.g., through subgraph extraction and masked attributes/structures. The key difference between graph pretraining and generalization may lie in their different focuses on **expressiveness** and **invariance**. While these two aspects do not always conflict, ensuring invariance w.r.t. a certain type of extrapolation might affect expressiveness (if the change is relevant to labels), and vice versa. To seek a balance in between, one might need prior knowledge in which types of transformations may or may not be sensitive to the labels (and perhaps build experts with different goals to make ensure invariance or expressivess). We believe there is still a lot to explore in this domain.\n\nOverall, we are grateful for your positive stand on this work. We believe that our [current version](https://openreview.net/pdf?id=QQ5eVDIMu4), incorporating opinions from you and other reviewers, is sound and well-refined. We are also committed to further improving it. We would appreciate your support based on our current version! \n\nThank you!\n\n---\n\nDear Reviewers of Paper 1560,\n\nWe hope this message finds you well.\n\nAs the discussion phase approaches its end, we hope you find our responses useful. We would like to ask if the issues have been addressed.\n\nWe understand that the discussion time is short, and some of you might be enjoying holidays at the moment. We apologize for posting our responses a bit late as we aimed to address your concerns clearly.\n\nWe sincerely appreciate your time and attention!\n\nBest regards,\n\nAuthors of Paper 1560\n\n---\n\n**Comment 4: Related works on graph transfer learning.**\n\nYes! We agree that graph transfer learning is a relevant topic. Thank you for providing the additional related work! We added the discussion on these works in our revision, and we repeat it here for your convenience:\n\n```It is also worth mentioning that graph domain adaptation (Zhang et al., 2019; Wu et al., 2020), different from the problem studied in this work, commonly relies on limited labeled samples from target datasets for improved transferability. For instance, to generate domain adaptive network embedding, DANE (Zhang et al., 2019) uses shared weight graph convolutional networks and adversarial learning regularization, while UDA-GCN (Wu et al., 2020) employs attention mechanisms to merge global and local consistencies.```\n\nMoreover, we believe this is actually relevant to `Comment & Question 1` regarding the generality of the distribution shifts covered by the transform functions. In our future works, we added the following discussion:\n\n```...Leveraging a few samples from target distribution (i.e., domain adaptation). Specifically, we can leverage the samples from the target distribution to inform the selection or construction of transform functions, which can better guarantee the distribution shifts are covered by the transform functions.  For example, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space. We believe this would be an interesting future direction.```\n\n---\n\n# Summary\n\nOnce again thank you very much for helping us position our work and reflect the literature better. We hope all of your concerns are solved and we are happy to engage further if there are any other points we missed!\n\n---\n\n### **Reference**\n\n[1] Unsupervised Domain Adaptive Graph Convolutional Networks. Wu et al. WWW 2020\n\n[2] DANE: Domain adaptive network embedding. Zhang et al. IJCAI 2019.\n\n[3] Tong Zhao, Yozen Liu, Leonardo Neves, Oliver J. Woodford, Meng Jiang, and Neil Shah. Data augmentation for graph neural networks. AAAI, 2021.\n\n---\n\nWe are grateful for your positive feedback and detailed suggestions! We provide responses below to address your remaining concerns. \n\n---\n\n### **Comment & Question 1: Generality of the distribution shifts covered by the transform functions.**\n\nGood question! We provide the response from two angles:\n\n\n**(a) For general domain**: In our experiments, we mainly use the five stochastic transform functions, which are universal graph augmentations as listed in Zhao et al., (2021) [3]. In our code implementation, we have also included additional transform functions as shown in Appendix B. We believe these transform functions, while not exhaustive, still cover a wide range of distribution shifts observing from our experimental results.\n\nNevertheless, we agree that the real graph distribution shifts can go beyond any possible combinations of the predefined transform functions. In that case, the assumption may not hold, meaning that GraphMETRO may not capture and precisely mitigate the unknown distribution shift. This scenario could always possibly exist due to the lack of information about the testing distribution or its domain knowledge. We include it as a limitation in Appendix 5, while we further discuss how we could alleviate the problem with additional information.\n\n**(b) For specific domains where additional knowledge is available**: In fact, knowing the tendency of the distribution shifts, such as increasing malicious users in a trading system, would be very helpful in constructing the transform functions that can cover the target distribution shifts well. We believe that such knowledge can come from two sources: \n- **Domain knowledge**, e.g., on molecular datasets, the transform function could be adding additional carbon structure to a molecule (while preserving its functional groups). Or, in a particular social network, transform functions can be defined from known user behaviors. \n- **Leveraging a few samples from target distribution**. This is in fact in line with the reviewer’s Comment #4 regarding graph transfer learning. Specifically, with the guide from a few target samples, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space.\n\n We hope this response can alleviate your concern about our applicability.\n\n---\n\n### **Comment & Question 2: The mechanism of GaphMETRO in handling heterogeneous shifts within one graph.**\n\nGood question! For simplicity, suppose we have two transform functions, i.e., adding edges and dropping edges. Given a node classification task and the objective in Eq. (3), the model is trained on the extrapolated datasets based on the transform functions. After that, given an unseen graph with imbalanced distribution shifts, the gating model outputs scores to identify nodes likely to experience increasing or decreasing degrees, while others might adhere to the original distribution. Then, for each node, each expert takes its multihop subgraph, and outputs its referential invariant representation w.r.t. the correposing transform function. These expert outputs and gating model results are then aggregated to form the final representation. Assuming accurate predictions by the gating function, nodes in denser/sparser subgraphs are represented by expert models corresponding to adding/dropping edges. As each expert is trained to create invariant representations, the final node representations remain unaffected by their individual distribution shifts.\n\n---\n\n### **Comment 3: How to leverage graph pretraining.**\n\nIf we understand correctly, the reviewer was asking how to leverage a pretrained model to further aid the training of GraphMETRO. Please let us know if otherwise. \n\nThis is in fact an interesting point! We believe a model pretrained on a wide variety of data can be very helpful to initialize the gating model, which is required to output the mixture of a node or graph (after it is finetuned on the extrapolated dataset). By enhancing the gating model's predictive capability regarding mixtures, GraphMETRO's final representation should become more resilient. This becomes particularly advantageous when dealing with graphs not previously encountered in the extrapolated dataset. Moreover, it is possible that the pretrained model will also benefit the expert models, while one minor concern would be that the expert model may tend to be similar instead of dedicating to generating invariant representation w.r.t. their corresponding transform function. We added the above discussion to Appendix F as a future work. Thanks again for this comment!\n\n---\n\n### **Question 4: Why does the test accuracy vary across different transformations?**\n\nGreat question! Here we summarize three possible reasons:\n\n- **Information Preservation in Transformations:** Certain transformations retain more informative features than others. For instance, in the REDDIT-BINARY (graph classification task), the random subgraph transformation may retain more graph label-related information compared to dropping edges, as the latter tends to lose more global information. This discrepancy in testing performance, where dropping edges outperforms random subgraph extraction, could be due to the preservation of crucial information. However, conclusions may vary across datasets or tasks depending on how information influences final predictions. For CiteSeer (a node classification task), a random subgraph might preserve more local node information, potentially explaining why its testing performance surpasses dropping edges in this specific task.\n\n- **Complexity of transformation:** Certain transformations inherently generate more diverse graphs than others. If the model lacks the expressive capacity to capture such diversity, it may lead to a decline in testing performance.\n\n- **Model Sensitivity:** Certain transformations may be easier for a model to learn due to compatibility with specific model architectures. This extends beyond transformation complexity and emphasizes how different model architectures may prefer learning particular distributions from one of the extrapolated datasets, which can also contribute to the difference in the testing performance.\n\nWe included the above discussion to Appendix F: Open Discussions. We hope this response can answer your question and improve the soundness of our work.\n\n---\n\n# Summary \n\nWe are grateful for your time and insightful suggestions! \n\nWe would like to highlight that our main contribution is framing the graph generalization problem on top of an equivalent mixture, a simple yet novel and tractable \"middle ground\", as well as proposing the training framework which effectively guarantees the generalization. While our method relies on a set of predefined transform functions, we believe they cover a wide range of distribution shifts based on our empirical results. Also, we agree that there could be some scenarios where the transform functions may not cover complex distributions, and we discuss two future directions and include them into our future works. Moreover, while selecting the hyperparameters for the transform functions introduce extra complexity, the issue could be minor in practice and we also conduct more experiments to justify the applicability better. Finally, we address several questions about clarification and presentation, as well as including more future works. \n\nLastly, we prudently ask you to reevaluate our work given the clarification in our responses, which we also updated our paper correspondingly. Overall, we believe our work makes good contributions to the field of graph distribution learning by proposing a novel and effective solution, and we would appreciate your reconsideration on this point. Thank you for your efforts again!\n\n---\n\n### **Reference**\n\n[1] Menon, Aditya Krishna, et al. \"Long-tail learning via logit adjustment.\" International Conference on Learning Representations. 2020.\n\n[2] Cao, Kaidi, et al. \"Learning imbalanced datasets with label-distribution-aware margin loss.\" Advances in neural information processing systems 32 (2019).\n\n[3] DARTS: Differentiable Architecture Search. Hanxiao Liu, Karen Simonyan, Yiming Yang. 2018.\n\n---\n\n**(3) What is the complexity of the transform functions and how does it affect generalization?** \n\nInteresting question! In fact, our implementation and framework could easily avoid selecting hyperparameters on the transform functions. Specifically, we can make multiple transform functions of the same type with different ranges of hyperparameters. Specifically, GraphMETRO allows three edge dropping transform functions, $\\tau_1^{\\alpha_1}, \\tau_2^{\\alpha_2}, \\tau_3^{\\alpha_3}$, where $\\alpha_i$ (i=1, 2, 3) are three different ranges of edge dropping probabilities, e.g., [0.1,0.3], [0.3, 0.6], [0.6,0.9], representing different transform extents. Thus, given an input from the validation dataset, the gating model will highlight the transform function which simultaneously selects the corresponding hyperparameter that matches the distribution of the validation set. Interestingly, this idea is in the same spirit as how DARTS [3]  proposes to perform architecture search by formulating the task in a differentiable manner. \n\nIn our previous experiments, we did try this scheme where we replaced a single edge dropping transform function with the ratio range [0.3, 0.5] to three transform functions as mentioned above. While we didn’t see a significantly different performance in that case, we believe this would be a flexible solution which avoids the need to conduct hyperparameter selection. \n\nWe include the above discussion in our open discussion and future works (Appendix F). We hope this response can alleviate your concern about the applicability of our method. \n\n---\n\n### **Comment 4: Limitations of our work**\n\nGreat point! We think the issue of label distributional shift, while important, is orthogonal and complementary to the focus of our current study. To elaborate, label distributional shifts exert analogous impacts across various modalities, such as graphs or images. Moreover, existing methods [1,2] designed to tackle label distributional shifts can be seamlessly integrated into our proposed framework. Such integration would necessitate minimal adjustments, potentially involving modifications to the loss function or the training pipeline. We added this as a future work in Appendix F.\n\n---\n\n### **Question 1: Statistical significance of the results on Table 1**\n\nThanks for the question! We compute the p-value of our method against the best baselines method as follows:\n\n| | WebKB | Twitch | Twitter | SST2 | \n|:--|:--|:--|:--|:--|\n|p-value|< 0.001| 0.023 | 0.042| 0.081|\n\nGiven the cut-off threshold as 0.05, we believe the performances of GraphMETRO are statistically significant on WekGB, Twitch, and Twitter datasets, while on the SST2, we see relatively weak evidence. We added the p-value results to our revision and hope our response can alleviate your concern on our improvements.\n\n---\n\n###  **Question 2: Can we say we use ERM for the node classification even if nodes and their labels are not i.i.d.?** \n\nIf we understand correctly, the reviewer is asking for clarification on the 2nd term of our objective. Please let us know if otherwise. Here, our thinking is that the cross-entropy loss for node classification already assumes node labels are conditionally independent given the model (the negative log-likelihood is a sum over the labeled nodes in training). Then, we use the same assumption of cross-entropy on Empirical Risk Minimization (ERM). That is, for a given model we must also minimize the error variance across nodes. The task is then to find the model with the best performance and small variance.\n\n---\n\n### **Question 3: Numerical results on Figure 2**\n\nThanks for pointing it out! We included all of the numerical results of Figure 2 in Appendix E, while showing the results on DBLP below.\n\n| |i.i.d. (0)|noisy feature (1)|add edge (2)|drop edge (3)|drop node (4)|random subgraph (5)|\n|:--|:--|:--|:--|:--|:--|:--|\n|ERM | 85.71 | 84.48 | 71.08 | 79.69 | 83.41 | 76.9|\n|ERM-Aug | 85.66 | 85.29 | 74.85 | 82.34 | 84.44 | 72.81|\n|GraphMETRO | 85.92 | 85.78 | 76.61 | 82.95 | 84.98 | 81.32|\n\n| |(4, 5)|(3, 5)|(2, 5)|(1, 5)|(2, 4)|(1, 4)|     (2, 3)|(1, 3)|\n|:--|:--|:--|:--|:--|:--|:--|:--|:--|\n|ERM | 70.4 | 77.63 | 81.99 | 79.69 | 70.55 | 71.52 | 77.73 | 79.59|\n|ERM-Aug | 74.16 | 81.04 | 83.65 | 68.62 | 74.01 | 68.27 | 81.13 | 84.49|\n|GraphMETRO | 76.18 | 81.71 | 84.26 | 80.31 | 75.1 | 71.05 | 81.85 | 87.14|\n\nAcross all of the synthetic environments, GraphMETRO averagely outperforms ERM and ERM-Aug by 3.20% and 2.45%, respectively.\n\n---\n\nWe appreciate your comments! To address your concerns, below we prudently justify the assumption of our method, the predefined transformation functions, as well as their complexity, and clarify our presented results.\n\n---\n\n### **Comment 1-3: Applicability and our assumptions**\n\nThanks for these great comments! Here we provide response in three folds:\n\n**(1) How does GraphMETRO identify all distribution shifts from transform functions if they are treated independently when combined together?** \n\nIf we understand correctly, by “treated independently”, the reviewer is referring to the first term in our objective $\\text{BCE}(\\phi(\\tau^{(k)}(\\mathcal{G})), Y (\\tau^{(k)}))$, where we formulate predicting the distribution shifts types of a jointly transformed graph as a binary multiclass classification problem. We believe the difficulty of this task comes from both the property of transform functions and the expressiveness of the gating model. \n\n- Firstly, some transform functions are **inherently disentangled**, e.g., adding nodes feature noise and random subgraph extraction. In this case, there will be certain distinction between any pair from these three data distributions, i.e., (graphs with node noise, random subgraph graphs, random subgraphs with node noise), which the gating model can easily tell. \n- While some transform functions can be **essentially similar**, e.g., drop path and drop edges, this won’t affect the performance of our method as long as each expert outputs the corresponding invariant representation. \n- Lastly, indeed, there could be more **complex combinations of the transform functions**, which poses challenges to the gating model’s expressiveness in identifying the combinations. However, this challenge may be minor in the practice. Specifically, we observe fairly high accuracy performances of the gating model, which are above 85% and 73% averagely on extrapolated datasets with one transformation and multiple transformations, respectively.\n\nWe added the above discussion to Appendix F to enable a more comprehensive view towards our methodology. We hope this can alleviate your concern on our gating model’s performance in identifying the distribution shift types.\n\n**(2) How does the predefined transform functions cover complex distributions causing the distribution shift?**\n\nThis is also a great question! We believe there are two angles for this question. \n\n**(a) For general domain**: In our experiments, we mainly use the five stochastic transform functions, which are universal graph augmentations as listed in Zhao et al., (2021) [3]. In our code implementation, we have also included additional transform functions as shown in Appendix B. We believe these transform functions, while not exhaustive, still cover a wide range of distribution shifts observing from our experimental results.\n\nNevertheless, we agree that the real graph distribution shifts can go beyond any possible combinations of the predefined transform functions. In that case, the assumption may not hold, meaning that GraphMETRO may not capture and precisely mitigate the unknown distribution shift. This scenario could always possibly exist due to the lack of information about the testing distribution or its domain knowledge. We include it as a limitation in Appendix 5, while we further discuss how we could alleviate the problem with additional information.\n\n**(b) For specific domains where additional knowledge is available**: In fact, knowing the tendency of the distribution shifts, such as increasing malicious users in a trading system, would be very helpful in constructing the transform functions that can cover the target distribution shifts well. We believe that such knowledge can come from two sources: \n- **Domain knowledge**, e.g., on molecular datasets, the transform function could be adding additional carbon structure to a molecule (while preserving its functional groups). Or, in a particular social network, transform functions can be defined from known user behaviors. \n- **Leveraging a few samples from target distribution**. Specifically, with the guide from a few target samples, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space.\n\n---\n\n[1] Sui et al. Unleashing the Power of Graph Data Augmentation on Covariate Distribution Shift. NeurIPS 2023.\n\n[2] Nianzu Yang et al. Learning substructure invariance for out-of-distribution molecular representations, NeurIPS 2022\n\n[3] Yongduo Sui et al. Causal Attention for Interpretable and Generalizable Graph Classification, KDD 2022.\n\n[4] Jiaqi Ma et al. Subgroup Generalization and Fairness of Graph Neural Networks, NeurIPS 2022.\n\n---\n\n### **Comment 2: Our assumption**\n\nThanks for this comment! In Section 3.1, we have discussed the assumptions in detail with specifications. We repeat part of it here for your convenience:\n\n```Assumption 1 essentially states that the distribution shifts (whatever they are) can be decomposed into several mixture components of stochastic graph transformations. For example, on a social network dataset, each mixture component can represent different patterns of user behavior or network dynamics shifts. Specifically, one mixture component might correspond to increased user activity, while another could signify a particular trend of interaction within a certain group of users. Such a mixture pattern is common and well-studied in the real-world network datasets (Newman, 2003; Leskovec et al., 2005; 2007; Peel et al., 2017).```\n\nWe hope the analysis provides detailed illustration to the assumption. We are happy to add more discussion if you think anything  is still unclear!\n\n---\n\n### **Comment 2 & 3: Implementation of our method**\n\n- **Model architectures**: \nPlease see Tables 2 and 3 in Appendix A for detailed model architecture information. Specifically, we use the same encoders and classifiers from GOOD benchmarkfor real-world datasets to ensure fair comparisons. We employ backbones based on the best ERM performance for synthetic datasets. Moreover, GraphMETRO is model-agnostic, which consistently improves performance across varied model architectures.\n\n- **Stochastic transformation**: Please see Appendix B where we include the introduction of the stochastic transform functions. \n\n- **Optimization process**: Please see Table 2 and 3 (Appendix A), where we included hyperparameters on each dataset. In the second paragraph after Eq. (3), we also described our training pipeline: `“...we set apart the other loss terms from backpropagating to it to avoid interference with the training of the gating model… We optimize the objective via stochastic gradient descent”`. \n\n- **For a more intricate optimization strategy:** This is a great catch! In fact, we did try to pretrain the gating model for several epochs as warm up before training the whole model in an end-to-end fashion via the objective Eq. in (3). However, we didn’t notice a statistically significant difference in their performance, which can be due to that expert models take more time to convergence (since they need to align with the base model during the training) compared to the gating model.\n\nDue to the space limitation, we had to include most of the implementation details in the appendix while we added pointers in the main paper. However, we will try to make it more detailed and feel free to let us know if anything is missing! \n\n---\n\n### **Comment 2 (Cont.): Understand the functionality, underlying mechanism, and performance gain**\n\nThanks for the comment. For clarity, we provide the following pointers to the paper:\n\n- In Section 4.3, we provide a study to reveal the underlying **mechanism** of GraphMETRO, i.e., each expert excels in generating invariant representations concerning a stochastic transform function, which provide a solid foundation in generating referential invariant representations w.r.t. the specific transformations and further guarantee the generalization.\n\n- In Appendix C, we study the **impact of the MoE architecture** on model performance, which shows that the model performance may decline if the expressiveness of the expert model decreases. \n\n- In Appendix D, we study the **impact of the stochastic transform function** on model performance, where we also provide a detailed discussion of the modeling mechanism.\n\nWe believe the above studies and discussion in our paper provides an in-depth view, highlighting the roles of our objective, architecture, and stochastic transform function.  Please let us know if any of these perspectives is still unclear, and we can further improve our experimental study.\n\n-----\n\n# Summary\n\nWe hope our answers can address all of the concerns. We are happy to follow up if you have any further questions.\n\nWe also prudently ask you to reevaluate our work. To highlight, our motivation is supported by the common fine-grained graph shift heterogeneity, and the fact that most of the previous works could not model such nuanced distribution shifts in an effective and flexible manner. Moreover, we added more related works and modified our statement to position our work better. In general, we believe GraphMETRO is a more general and flexible solution that can mitigate a wider range of distribution shifts, which is backed by the experimental results. Detailed justification on our assumption and implementation details are also available in our paper. \n\nThus, we believe our work makes important contributions and provides a clear presentation. We are happy to discuss more and revise our paper if any concern remains. Thank you for your efforts and we are looking forward to your reply!!\n\n---\n\nWe appreciate your comments! To address your concerns, below we prudently justify the motivation of our proposed method, clarify our assumptions, and provide details regarding our experiment implementation.\n\n---------------------\n\n### **Comment 1: Motivation of this work**\n\nThank you for this comment! We believe there might be a bit of misunderstanding due to our different definitions of *“graph shift heterogeneity”*. We firstly discuss the related works mentioned and then justify our statement:\n\n**(1) Related works**\n\nPlease see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nMoreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you! \n\nSpecifically, Sui et al. [1] proposed a graph data augmentation strategy that alleviates covariate shift by generating diverse and invariant causal features. However, the trainable augmenter they used may not distill diverse augmentations or construct unseen perturbations. Moreover, Sui et al. [1] test its method only on graph classification tasks, while GraphMETRO can be applied to both node and graph classification tasks. Besides, we have discussed graph augmentation and attention-based methods in our related works, and we added more recent works on graph OOD [2,3,4], and we hope our response clears your concern on the related work discussion.\n\n\n**(2) The definition of graph shift heterogeneity**\n\nIn this work, we refer to **“heterogeneous shifts”** as multiple and different levels of shifts which vary across different instances (nodes or graphs), as illustrated in the example in the abstract. While we agree that the existing invariant learning approches can accommodate multiple distribution shifts, it could be hard for them to tackle nuanced distribution shifts for individual instances (nodes or graphs) since the distribution shifts are inferred from variance across multiple data environments. If GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering **a mixture of transformations as a proxy** for the target distribution shifts rather than invariance to whole-graph environment shifts. This is the type of heterogeneity we are interested in our paper. \n\n**(3) Regarding our original motivation statement**\n\nWhile the statement pointed out by the reviewer serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category. And we have provided detailed discussion about three lines of research in the related work section.\n\nHowever, we agree that we could make this statement border to cover the previous invariant learning methods. To improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to \"**previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments…**”. We also modified our introduction correspondingly, we hope this will better position our work.\n\n**(4) The choice of our MoE design**\n\n- The choice of our approach comes as a consequence of our motivation to model the graph/instance shift heterogeneity. As mentioned, mitigating multiple and nuanced distribution shifts simply goes beyond certain distribution shift types or environment construction as seen in the previous methods. \n- Thus, GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts. This enables the prediction of multiple different distribution shifts and the flexibility to model fine-grained heterogeneity since the mixture can be varied across different instances. We then tackled the proxy to mitigate the target distribution shifts. Intuitively, this solution provides a **“middle ground”** to deem graph generalization as an equivalent mixture, which, we believe, is a more tractable solution.\n\nWe updated our paper to make the above point more clear. We genuinely hope our answer can justify the motivation and solve your concern.\n\n---\n\n### **Question 2: How does the computational cost of GraphMETRO compare to other methods?**\n\nPlease see the last paragraph of Section 3.4, where we analyze the computation complexity of GraphMETRO. We repeat part of it here for your convenience:\n\n```Consider the scenario where we use an individual encoder for each expert. The forward process of $f$ involves $O(K)$ forward times using the weighted sum aggregation (or $O(1)$ if using the maximum selection). Since we extend the dataset to $(K + 1)$ times larger than the original data, the computation complexity is $O(K^2 |D_s|)$, where |Ds| is the size of the source dataset.```\n\nThus, the computation cost is about $K^2$ or $K$ times (if using the maximum selection) than an ERM model, where $K=5$ in our experiments. Compared to DIR, as they extract $B$ spurious subgraphs from each batch to conduct the intervention, their computation cost is $B$ times compared to ERM, where $B$ could be 32. Thus, we believe the computation cost of GraphMETRO is fair for the gains we get, considering $K$ is usually small.\n\n----------------------\n\n### **Question 3: Can GraphMETRO handle multiple different types of distribution shifts that simultaneously exist in data?**\n\nYes! The distribution shift types corresponding to the gating outputs with high scores will be tackled during training. That is, if the gating output highlights multiple mixture components, their corresponding distribution shift types will be handled jointly.\n\n----------------------\n\n### **Question 4: Can GraphMETRO  tackle distribution shifts on molecular graphs?**\n\nThat is a great idea! Yes, GraphMETRO can be applied to molecular datasets if one designs transform functions to cover typical molecular variants. For instance, a transform function may add carbon structures to the molecules. These domain-specific transform functions are outside the scope of our work, however, we believe these would be interesting future work directions!\n\n----------------------\n\n# Summary \n\nWe thank the reviewer for the time and insightful suggestions! We hope our answers can address your concerns well.\n\nWe also prudently ask you to reconsider our work if the concerns are addressed. To highlight, our novelty comes from the formulation of an equivalent mixture for graph OOD and the training framework to effectively realize generalization. We also provide an in-depth analysis on our originality compared to some previous invariant learning methods. While we discussed and compared with previous works, we added more related works and modified our statement to position our work better. Finally, our method achieves great improvements on both node and graph classification tasks, and is a more general solution to mitigate multiple and nuanced distribution shifts. \n\nOverall, we believe our work proposes a new paradigm and novel training framework and makes good contributions in the fields of graph generalization, and we would appreciate your reconsideration on this point. Thank you for your efforts again!\n\n----------------------\n\n**Reference**\n\n\n[1-5] The same as listed by the reviewer\n\n\n[6] Beatrice Bevilacqua, Yangze Zhou, and Bruno Ribeiro. Size-invariant graph representations for graph classification extrapolations. In ICML, 2021.\n\n[7] Davide Buffelli, Pietro Li´o, and Fabio Vandin. Sizeshiftreg: a regularization method for improving size-generalization in graph neural networks. In NeurIPS, 2022.\n\n[8] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In NeurIPS, 2019.\n\n[9] Mucong Ding, Kezhi Kong, Jiuhai Chen, John Kirchenbauer, Micah Goldblum, David Wipf, Furong Huang, and Tom Goldstein. A closer look at distribution shifts and out-of-distribution generalization on graphs. In NeurIPS DistShift, 2021.\n\n---\n\n### **Comment 3 & Question 1: Related works on invariant learning and clarification on our statement.**\n\nThanks for pointing it out! Below we clarify the statement and clear potential misunderstanding:\n\n**(1) How does GraphMETRO compare with invariant learning methods like DIR and EERM?** \n\nPlease see our related work section where we discussed these two papers, i.e., DIR (Wu et al., 2022c) and EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on environmental patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nBesides, we also provide a more in-depth comparison in our point (3) below to highlight our key technical originality.\n\n**(2) Regarding our statement about previous works:** \n\nWhile the statement serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category, and we have provided detailed discussion about three lines of research in the related work section.\n\nTo improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to **\"previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments** (which is highly limited when confronted with nuanced distribution shifts)”. We also modified our introduction correspondingly. Thanks for letting us know our statement could be misinterpreted.\n\n**(3) Why do we say our method could be more broad than the existing invariant learning approaches?** \n\n- Invariant subgraph learning approaches, e.g., [1,2], consider variance of constructed data environments, which are designed very differently compared to our work. \nWhile they can accommodate multiple distribution shifts (as in multiple environments), these focus on patterns within each environment and ignore the variety across instances (e.g., shifts at the resolution of nodes), which may not be well-captured by the environment assignments. \n- GraphMETRO considers that specific parts of the test graph may have different shifts. Particularly, our goal is to make the generalization to unknown testing distribution more adaptive and broad, as opposed to limiting the distribution shifts to being invariant to specific types of subgraphs. \n\nIn other words, if GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering a mixture of transformations as a proxy for the target distribution shifts rather than invariance to environment shifts.\n\n**(4) Key technical originality compared to invariance learning (going more deeply)**\n\nAnother interesting view to see the innovation of GraphMETRO is that it breaks the typical invariant learning formulation, which assumes the data is manipulated by the environment variables (and then can be “decoded” into multiple environments). Instead, GraphMETRO sees the distribution shifts on an instance as a mixture, which is represented by the score vector output by the gating function over the basis of the transform functions. In other words, GraphMETRO can produce infinite environments as the elements in the score vector are continuous. One can see that once we limit the output domain of the gating function into, e.g., binary {0, 1}, GraphMETRO can also produce a limited number of environments (if we categorize the instances based on the score vector), which covers the environment construction in invariant learning. Moreover, as mentioned, we propose the concept of referential invariant representation with a base model $\\xi_0$, which is also different from previous works on invariant learning. We added the above discussion to Appendix F to improve the depth of our analysis.\n\n---\n\nWe appreciate your efforts and insightful comments! To address your concerns, we provide point-to-point responses below.\n\n---------------------\n\n### **Comment 1: Regarding the novelty of GraphMETRO.**\n\nThanks for the comment! We believe our novelty comes from the proposal of an equivalent mixture for graph OOD and the construction of our training framework, as detailed below:\n\n- **An equivalent mixture for graph OOD**: The key challenge we faced to mitigate multiple and nuanced distribution shifts is the intrinsic complexity and heterogeneity of graph distribution shifts, which simply goes beyond certain distribution shift types [6,7,8,9] or environment construction as seen in the previous methods [1,2,3]. GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts, where the mixture can be varied across different instances, and then tackled the proxy to mitigate the target distribution shifts. We believe the high-level idea is succinct, nevertheless, novel, in the sense that it provides a “middle ground” to deem graph generalization as an equivalent mixture that is more tractable.\n\n- **Training framework**: With the guide of our formulation, the training framework is still non-trivial due to two problems, i.e., “how to provide supervision for predicting the mixture” and “how to ensure the experts corresponding to mixture components are compatible when working as a whole”. Specifically, GraphMETRO solves the first problem by conducting graph extrapolation. This is somewhat similar to the spirit of graph pretraining in the sense that we inject heterogeneity to promote the expressiveness of the gating model in recognizing the mixture components. For the second problem, we introduce the concept of Referential Invariant Representation, along with the novel objective in Eq. (3) to enforce the invariance and compatibility. It is worth mentioning that the model performance is much worse than the reported numbers (e.g., 2.7\\% lower on Twitch dataset) without the compatibility constraint, indicating the proposed referential invariance concept is indispensable. \n\nWe added more justification in the introduction (updated in the revision). We genuinely hope our responses can solve your concerns about the novelty of our work.\n\n------------------------------\n\n### **Comment 2: Comparison with recent methods [3,4,5].**\n\nIn compacting the paper to fit in the page limit we mistakenly did not include these relevant references, we apologize. We added discussion in the revised version. Here we summarize these works and point out their key differences with our method: \n\n- In particular, **Yang et al. [3]** explore molecule representation learning in out-of-distribution (OOD) scenarios. They achieve this by directing the molecule encoder to utilize stable and environment-invariant substructures relevant to the labels without the need for environmental labels. \n- Similarly, **Sui et al. [4]** introduces causal attention modules to identify key invariant subgraph features that can be described as causing the graph label. The type of OOD task that Sui et al. [4] considers assumes the graph label is caused by a subgraph, which is quite different from ours. Moreover, both Yang et al. [3] and Sui et al. [4] consider tasks where the graph label is caused by a subgraph. \n- **Ma et al.[5]** is an interesting theoretical work which studies GNN generalization and examines their fairness, showing that the test subgroup's distance from the training set impacts GNN performance. Ma et al.[5], as far as we could assess, does not propose any specific architecture to solve the type of OOD tasks we consider in our work.\n\nOverall, the goal of GraphMETRO is to be invariant to a mixture of selected stochastic transform functions (and the mixture can vary across different instances), which is a more flexible and general solution. \nWe added a discussion of these works to our revision, hopefully providing a more comprehensive comparison and literature overview. We hope our responses can solve your concerns about the related work.\n\n---\n\nWe sincerely appreciate all reviewers' time, efforts, and valuable suggestions in reviewing our paper. We are glad that most of the reviewers reached a positive consensus on our work's motivation and experimental results. Here is a summary of our responses:\n\n- **Clarification**: We clarify our key assumptions (`m8JS, FURX`), novelty (`m8JS, NLg1`), and the experimental settings (`m8JS`).\n\n- **Related work**: : We add more related works on recent graph OOD learning (`NLg1, m8JS`) and graph transfer learning (`FURX`)\n\n- **Limitation**: We add discussion about the limitations of GraphMETRO when confronted with unknown distribution shifts that surpass our key assumptions (`FURX`, `C9mM`).\n\nWe hope our responses can clarify your confusion and alleviate concerns and we updated our paper **(highlighted in green)** corespondingly. We thank all reviewers again, and look forward to your reply!", "author_response": "We appreciate your comments! To address your concerns, below we prudently justify the motivation of our proposed method, clarify our assumptions, and provide details regarding our experiment implementation.\n\n---------------------\n\n### **Comment 1: Motivation of this work**\n\nThank you for this comment! We believe there might be a bit of misunderstanding due to our different definitions of *“graph shift heterogeneity”*. We firstly discuss the related works mentioned and then justify our statement:\n\n**(1) Related works**\n\nPlease see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nMoreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you! \n\nSpecifically, Sui et al. [1] proposed a graph data augmentation strategy that alleviates covariate shift by generating diverse and invariant causal features. However, the trainable augmenter they used may not distill diverse augmentations or construct unseen perturbations. Moreover, Sui et al. [1] test its method only on graph classification tasks, while GraphMETRO can be applied to both node and graph classification tasks. Besides, we have discussed graph augmentation and attention-based methods in our related works, and we added more recent works on graph OOD [2,3,4], and we hope our response clears your concern on the related work discussion.\n\n\n**(2) The definition of graph shift heterogeneity**\n\nIn this work, we refer to **“heterogeneous shifts”** as multiple and different levels of shifts which vary across different instances (nodes or graphs), as illustrated in the example in the abstract. While we agree that the existing invariant learning approches can accommodate multiple distribution shifts, it could be hard for them to tackle nuanced distribution shifts for individual instances (nodes or graphs) since the distribution shifts are inferred from variance across multiple data environments. If GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering **a mixture of transformations as a proxy** for the target distribution shifts rather than invariance to whole-graph environment shifts. This is the type of heterogeneity we are interested in our paper. \n\n**(3) Regarding our original motivation statement**\n\nWhile the statement pointed out by the reviewer serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category. And we have provided detailed discussion about three lines of research in the related work section.\n\nHowever, we agree that we could make this statement border to cover the previous invariant learning methods. To improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to \"**previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments…**”. We also modified our introduction correspondingly, we hope this will better position our work.\n\n**(4) The choice of our MoE design**\n\n- The choice of our approach comes as a consequence of our motivation to model the graph/instance shift heterogeneity. As mentioned, mitigating multiple and nuanced distribution shifts simply goes beyond certain distribution shift types or environment construction as seen in the previous methods. \n- Thus, GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts. This enables the prediction of multiple different distribution shifts and the flexibility to model fine-grained heterogeneity since the mixture can be varied across different instances. We then tackled the proxy to mitigate the target distribution shifts. Intuitively, this solution provides a **“middle ground”** to deem graph generalization as an equivalent mixture, which, we believe, is a more tractable solution.\n\nWe updated our paper to make the above point more clear. We genuinely hope our answer can justify the motivation and solve your concern."}
{"claim": "The theoretical assumption about distribution shifts is presented in overly broad terms and lacks sufficient specificity for validation.", "claim_type": "methodology", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ppMpJyIYRZ", "reviewer": "Reviewer_m8JS", "review_text": "Summary: The paper introduces a method to enhance the out-of-distribution performance of graph neural networks (GNN) by learning to understand distribution shifts instead of addressing the assumed ones. To achieve this, the Mixture of Experts architecture is integrated into the GNN, supplemented by an alignment procedure to recognize the shift. Empirical experiments are conducted to validate the theoretical assertion.\n\nOn the whole, I believe the proposed method lacks the necessary motivation and its novelty isn't substantial enough to meet the standard.\n\nStrengths: - The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\n- Real-world datasets back the claims through experiments.\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\n\nWeaknesses: - The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \"previous research has concentrated on addressing specific types of distribution shifts.\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \"environment generators\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\n\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\n\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\n\nQuestions: Please check Weaknesses.", "labeling_timestamp": "2026-01-11T16:31:21.071501", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors acknowledge the reviewer's concern and provide clarifications, references, and concrete places in the paper (Section 3.1, figures, appendices) where the assumption is explained; they also admit limitations and promise refinements (e.g., adding a causal graph). However, they defend the assumption's plausibility and do not fully concede that it is overly broad or invalid, so the response only partially addresses the claim.", "evidence": "Yes, we agree that the concept of heterogeneous shifts is relatively new for the current studies on distribution shifts, however, this is not new for studies on network patterns (Newman, 2003; Leskovec et al., 2005; 2007; Peel et al., 2017). In fact, the ignorance of such nuanced heterogeneous shifts in the previous studies instead emphasises our motivation and the potential impact of this work.", "combined_author_response": "Dear reviewer NLg1,\n\nAs the discussion period is closing soon, we hope to engage with you and check if your concerns are addressed.\n\nWe sincerely appreciate your time and attention!\n\nBest,\n\nAuthors of Paper 1560\n\n---\n\nThanks for reading our response! We would also appreciate your patience for reading the two points below \n\n---\n\n**Regarding point #1:**\n\nYes, we agree that the concept of heterogeneous shifts is relatively new for the current studies on distribution shifts, however, this is not new for studies on network patterns (Newman, 2003; Leskovec et al., 2005; 2007; Peel et al., 2017). In fact, the ignorance of such nuanced heterogeneous shifts in the previous studies instead emphasises our motivation and the potential impact of this work. \n\nFrom a causality perspective, distribution shifts can naturally happen in the instance level when (1) additional causal variables, beyond environmental factors and randomized noise, influence these shifts, or (2) multiple causal variables simultaneously affect the shifts with different strengths. Without modeling these explicitly, the mitigation of distribution shifts can easily fail. \n\nIn terms of our presentation, we have illustrated these cases of interest in abstract and introduction, we made further explanations in Section 3.1, we also present the specific results of distribution types in Figure 3 (b). \n\n\n**Regarding point #2:**\n\nWe think we can all agree that, if the ground truth of the instance shifts is available on the real-world datasets, it would be crystal clear to see where the improvement comes from since we can conduct case study to compare our method and the baselines on instances with nuanced distribution shifts, to see the influence of the modeling these heterogeneous shifts. \n\nHowever, with such ground truth not available, we had to seek other seemingly less intuitive but also in-depth way to illustrate the insights (esp Section 4.3) as mentioned above in our previous response. This is also why we designed the synthetic experiments at the first place. We think we did try hard to explain the underlying rationale with the ground truth being absent.\n\n----\n\n**Refinement**: We can of course add a causal graph in the our assumption section to make the concept more clear. And we can illustrate more if you could let us know the specific obstable to understand our mechanism, which will be extremely helpful.\n\n----\n\n### **Summary/TL;DR**\nWe understand the reviewer's clarification concerns, however, we don't agree that they, based on our justificaton, are the cause of rejection. We believe nuanced heterogeneous shifts are common, important, yet being typically ignored in the research domain of distribution shifts, we made these argument clear and also promise to refine. While explaining the underlying rationale is hindered by the lack of ground truth, we did try hard to dissection it from the gating model, the invariant representations generated, and more ablations in the appendix. We respect the reviewer's current opinion. Still, reconsideration will be greatly appreciated.\n\n---\n\nWe thank the reviewer once again! We also enjoyed the process of making our work more sound from your suggestions. And your approval surely means a lot to us.\n\nWe do apologize for not making our solutions towards the limitations clear enough. We summarize them into a short table, hopefully could alleviate your concerns a bit more.\n\n| Limitation| Potential solutions | Location of discussion/action |\n|:---|:---|:---|\n|**Coverage of the transformations** | 1) Extend the coverage by adding representative transform functions. 2) Or include tranform functions based on domain knowledge or a few samples from target distribution | Appendix F, paragraph #3|\n|**Complexity of the transformations** | Make experts dedicated to different hyperparameters for the same type of transformation | Argument options are available in our codebase. Will make it more detailed in the experimental settings. |\n|**Label distributional shift**|Integrate the objective of the existing methods studying labe distributional shifts into our framework. |Appendix F, paragraph #4 |\n\n----\n\nWe completely agree on the existence of these limitations and we will move some limitations (esp #1) to the main paper in our final version. While they could be important in the practice, they are, in our perspective, fair \"side effects\" considering the benefits (i.e., mitigating multiple and nuanced distribution shifts and better interpretability), and may not be the central part of our novelty and main contribution (i.e., the proposal of an equivalent mixture, the concept of referential invariant representations, as well as the training framework). We are also eager to further improve other aspects.\n\nOnce again thank you so much for your support!!\n\n---\n\nDear Reviewer m8JS,\n\nA gentle nudge that we would like to know if our response adequately addresses your concerns. \n\nYour time and feedback is greatly appreciated!\n\nThank you sincerely,\n\nAuthors of Paper 1560\n\n---\n\nWe deeply appreciate your approval. Your suggestions definitely inspired us a lot and have greatly improved our work.\n\nA few additional notes for the further comments:\n\n- Regarding **point #2**: In node classification tasks, the BCE objective already considers node embeddings other than the graph embedding of the k-hop subgraph. What we described previously refers to the computational graph (i.e., k-hop message passing), which generates node embeddings. We apologize for any confusion.\n\n- Regarding **point #3**: Yes it would be quite interesting to see how graph pretraining methods perform on the current OOD benchmarks. Graph pretraining methods like GCC and GPT-GNN also consider graph extrapolation to some extent, e.g., through subgraph extraction and masked attributes/structures. The key difference between graph pretraining and generalization may lie in their different focuses on **expressiveness** and **invariance**. While these two aspects do not always conflict, ensuring invariance w.r.t. a certain type of extrapolation might affect expressiveness (if the change is relevant to labels), and vice versa. To seek a balance in between, one might need prior knowledge in which types of transformations may or may not be sensitive to the labels (and perhaps build experts with different goals to make ensure invariance or expressivess). We believe there is still a lot to explore in this domain.\n\nOverall, we are grateful for your positive stand on this work. We believe that our [current version](https://openreview.net/pdf?id=QQ5eVDIMu4), incorporating opinions from you and other reviewers, is sound and well-refined. We are also committed to further improving it. We would appreciate your support based on our current version! \n\nThank you!\n\n---\n\nDear Reviewers of Paper 1560,\n\nWe hope this message finds you well.\n\nAs the discussion phase approaches its end, we hope you find our responses useful. We would like to ask if the issues have been addressed.\n\nWe understand that the discussion time is short, and some of you might be enjoying holidays at the moment. We apologize for posting our responses a bit late as we aimed to address your concerns clearly.\n\nWe sincerely appreciate your time and attention!\n\nBest regards,\n\nAuthors of Paper 1560\n\n---\n\n**Comment 4: Related works on graph transfer learning.**\n\nYes! We agree that graph transfer learning is a relevant topic. Thank you for providing the additional related work! We added the discussion on these works in our revision, and we repeat it here for your convenience:\n\n```It is also worth mentioning that graph domain adaptation (Zhang et al., 2019; Wu et al., 2020), different from the problem studied in this work, commonly relies on limited labeled samples from target datasets for improved transferability. For instance, to generate domain adaptive network embedding, DANE (Zhang et al., 2019) uses shared weight graph convolutional networks and adversarial learning regularization, while UDA-GCN (Wu et al., 2020) employs attention mechanisms to merge global and local consistencies.```\n\nMoreover, we believe this is actually relevant to `Comment & Question 1` regarding the generality of the distribution shifts covered by the transform functions. In our future works, we added the following discussion:\n\n```...Leveraging a few samples from target distribution (i.e., domain adaptation). Specifically, we can leverage the samples from the target distribution to inform the selection or construction of transform functions, which can better guarantee the distribution shifts are covered by the transform functions.  For example, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space. We believe this would be an interesting future direction.```\n\n---\n\n# Summary\n\nOnce again thank you very much for helping us position our work and reflect the literature better. We hope all of your concerns are solved and we are happy to engage further if there are any other points we missed!\n\n---\n\n### **Reference**\n\n[1] Unsupervised Domain Adaptive Graph Convolutional Networks. Wu et al. WWW 2020\n\n[2] DANE: Domain adaptive network embedding. Zhang et al. IJCAI 2019.\n\n[3] Tong Zhao, Yozen Liu, Leonardo Neves, Oliver J. Woodford, Meng Jiang, and Neil Shah. Data augmentation for graph neural networks. AAAI, 2021.\n\n---\n\nWe are grateful for your positive feedback and detailed suggestions! We provide responses below to address your remaining concerns. \n\n---\n\n### **Comment & Question 1: Generality of the distribution shifts covered by the transform functions.**\n\nGood question! We provide the response from two angles:\n\n\n**(a) For general domain**: In our experiments, we mainly use the five stochastic transform functions, which are universal graph augmentations as listed in Zhao et al., (2021) [3]. In our code implementation, we have also included additional transform functions as shown in Appendix B. We believe these transform functions, while not exhaustive, still cover a wide range of distribution shifts observing from our experimental results.\n\nNevertheless, we agree that the real graph distribution shifts can go beyond any possible combinations of the predefined transform functions. In that case, the assumption may not hold, meaning that GraphMETRO may not capture and precisely mitigate the unknown distribution shift. This scenario could always possibly exist due to the lack of information about the testing distribution or its domain knowledge. We include it as a limitation in Appendix 5, while we further discuss how we could alleviate the problem with additional information.\n\n**(b) For specific domains where additional knowledge is available**: In fact, knowing the tendency of the distribution shifts, such as increasing malicious users in a trading system, would be very helpful in constructing the transform functions that can cover the target distribution shifts well. We believe that such knowledge can come from two sources: \n- **Domain knowledge**, e.g., on molecular datasets, the transform function could be adding additional carbon structure to a molecule (while preserving its functional groups). Or, in a particular social network, transform functions can be defined from known user behaviors. \n- **Leveraging a few samples from target distribution**. This is in fact in line with the reviewer’s Comment #4 regarding graph transfer learning. Specifically, with the guide from a few target samples, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space.\n\n We hope this response can alleviate your concern about our applicability.\n\n---\n\n### **Comment & Question 2: The mechanism of GaphMETRO in handling heterogeneous shifts within one graph.**\n\nGood question! For simplicity, suppose we have two transform functions, i.e., adding edges and dropping edges. Given a node classification task and the objective in Eq. (3), the model is trained on the extrapolated datasets based on the transform functions. After that, given an unseen graph with imbalanced distribution shifts, the gating model outputs scores to identify nodes likely to experience increasing or decreasing degrees, while others might adhere to the original distribution. Then, for each node, each expert takes its multihop subgraph, and outputs its referential invariant representation w.r.t. the correposing transform function. These expert outputs and gating model results are then aggregated to form the final representation. Assuming accurate predictions by the gating function, nodes in denser/sparser subgraphs are represented by expert models corresponding to adding/dropping edges. As each expert is trained to create invariant representations, the final node representations remain unaffected by their individual distribution shifts.\n\n---\n\n### **Comment 3: How to leverage graph pretraining.**\n\nIf we understand correctly, the reviewer was asking how to leverage a pretrained model to further aid the training of GraphMETRO. Please let us know if otherwise. \n\nThis is in fact an interesting point! We believe a model pretrained on a wide variety of data can be very helpful to initialize the gating model, which is required to output the mixture of a node or graph (after it is finetuned on the extrapolated dataset). By enhancing the gating model's predictive capability regarding mixtures, GraphMETRO's final representation should become more resilient. This becomes particularly advantageous when dealing with graphs not previously encountered in the extrapolated dataset. Moreover, it is possible that the pretrained model will also benefit the expert models, while one minor concern would be that the expert model may tend to be similar instead of dedicating to generating invariant representation w.r.t. their corresponding transform function. We added the above discussion to Appendix F as a future work. Thanks again for this comment!\n\n---\n\n### **Question 4: Why does the test accuracy vary across different transformations?**\n\nGreat question! Here we summarize three possible reasons:\n\n- **Information Preservation in Transformations:** Certain transformations retain more informative features than others. For instance, in the REDDIT-BINARY (graph classification task), the random subgraph transformation may retain more graph label-related information compared to dropping edges, as the latter tends to lose more global information. This discrepancy in testing performance, where dropping edges outperforms random subgraph extraction, could be due to the preservation of crucial information. However, conclusions may vary across datasets or tasks depending on how information influences final predictions. For CiteSeer (a node classification task), a random subgraph might preserve more local node information, potentially explaining why its testing performance surpasses dropping edges in this specific task.\n\n- **Complexity of transformation:** Certain transformations inherently generate more diverse graphs than others. If the model lacks the expressive capacity to capture such diversity, it may lead to a decline in testing performance.\n\n- **Model Sensitivity:** Certain transformations may be easier for a model to learn due to compatibility with specific model architectures. This extends beyond transformation complexity and emphasizes how different model architectures may prefer learning particular distributions from one of the extrapolated datasets, which can also contribute to the difference in the testing performance.\n\nWe included the above discussion to Appendix F: Open Discussions. We hope this response can answer your question and improve the soundness of our work.\n\n---\n\n# Summary \n\nWe are grateful for your time and insightful suggestions! \n\nWe would like to highlight that our main contribution is framing the graph generalization problem on top of an equivalent mixture, a simple yet novel and tractable \"middle ground\", as well as proposing the training framework which effectively guarantees the generalization. While our method relies on a set of predefined transform functions, we believe they cover a wide range of distribution shifts based on our empirical results. Also, we agree that there could be some scenarios where the transform functions may not cover complex distributions, and we discuss two future directions and include them into our future works. Moreover, while selecting the hyperparameters for the transform functions introduce extra complexity, the issue could be minor in practice and we also conduct more experiments to justify the applicability better. Finally, we address several questions about clarification and presentation, as well as including more future works. \n\nLastly, we prudently ask you to reevaluate our work given the clarification in our responses, which we also updated our paper correspondingly. Overall, we believe our work makes good contributions to the field of graph distribution learning by proposing a novel and effective solution, and we would appreciate your reconsideration on this point. Thank you for your efforts again!\n\n---\n\n### **Reference**\n\n[1] Menon, Aditya Krishna, et al. \"Long-tail learning via logit adjustment.\" International Conference on Learning Representations. 2020.\n\n[2] Cao, Kaidi, et al. \"Learning imbalanced datasets with label-distribution-aware margin loss.\" Advances in neural information processing systems 32 (2019).\n\n[3] DARTS: Differentiable Architecture Search. Hanxiao Liu, Karen Simonyan, Yiming Yang. 2018.\n\n---\n\n**(3) What is the complexity of the transform functions and how does it affect generalization?** \n\nInteresting question! In fact, our implementation and framework could easily avoid selecting hyperparameters on the transform functions. Specifically, we can make multiple transform functions of the same type with different ranges of hyperparameters. Specifically, GraphMETRO allows three edge dropping transform functions, $\\tau_1^{\\alpha_1}, \\tau_2^{\\alpha_2}, \\tau_3^{\\alpha_3}$, where $\\alpha_i$ (i=1, 2, 3) are three different ranges of edge dropping probabilities, e.g., [0.1,0.3], [0.3, 0.6], [0.6,0.9], representing different transform extents. Thus, given an input from the validation dataset, the gating model will highlight the transform function which simultaneously selects the corresponding hyperparameter that matches the distribution of the validation set. Interestingly, this idea is in the same spirit as how DARTS [3]  proposes to perform architecture search by formulating the task in a differentiable manner. \n\nIn our previous experiments, we did try this scheme where we replaced a single edge dropping transform function with the ratio range [0.3, 0.5] to three transform functions as mentioned above. While we didn’t see a significantly different performance in that case, we believe this would be a flexible solution which avoids the need to conduct hyperparameter selection. \n\nWe include the above discussion in our open discussion and future works (Appendix F). We hope this response can alleviate your concern about the applicability of our method. \n\n---\n\n### **Comment 4: Limitations of our work**\n\nGreat point! We think the issue of label distributional shift, while important, is orthogonal and complementary to the focus of our current study. To elaborate, label distributional shifts exert analogous impacts across various modalities, such as graphs or images. Moreover, existing methods [1,2] designed to tackle label distributional shifts can be seamlessly integrated into our proposed framework. Such integration would necessitate minimal adjustments, potentially involving modifications to the loss function or the training pipeline. We added this as a future work in Appendix F.\n\n---\n\n### **Question 1: Statistical significance of the results on Table 1**\n\nThanks for the question! We compute the p-value of our method against the best baselines method as follows:\n\n| | WebKB | Twitch | Twitter | SST2 | \n|:--|:--|:--|:--|:--|\n|p-value|< 0.001| 0.023 | 0.042| 0.081|\n\nGiven the cut-off threshold as 0.05, we believe the performances of GraphMETRO are statistically significant on WekGB, Twitch, and Twitter datasets, while on the SST2, we see relatively weak evidence. We added the p-value results to our revision and hope our response can alleviate your concern on our improvements.\n\n---\n\n###  **Question 2: Can we say we use ERM for the node classification even if nodes and their labels are not i.i.d.?** \n\nIf we understand correctly, the reviewer is asking for clarification on the 2nd term of our objective. Please let us know if otherwise. Here, our thinking is that the cross-entropy loss for node classification already assumes node labels are conditionally independent given the model (the negative log-likelihood is a sum over the labeled nodes in training). Then, we use the same assumption of cross-entropy on Empirical Risk Minimization (ERM). That is, for a given model we must also minimize the error variance across nodes. The task is then to find the model with the best performance and small variance.\n\n---\n\n### **Question 3: Numerical results on Figure 2**\n\nThanks for pointing it out! We included all of the numerical results of Figure 2 in Appendix E, while showing the results on DBLP below.\n\n| |i.i.d. (0)|noisy feature (1)|add edge (2)|drop edge (3)|drop node (4)|random subgraph (5)|\n|:--|:--|:--|:--|:--|:--|:--|\n|ERM | 85.71 | 84.48 | 71.08 | 79.69 | 83.41 | 76.9|\n|ERM-Aug | 85.66 | 85.29 | 74.85 | 82.34 | 84.44 | 72.81|\n|GraphMETRO | 85.92 | 85.78 | 76.61 | 82.95 | 84.98 | 81.32|\n\n| |(4, 5)|(3, 5)|(2, 5)|(1, 5)|(2, 4)|(1, 4)|     (2, 3)|(1, 3)|\n|:--|:--|:--|:--|:--|:--|:--|:--|:--|\n|ERM | 70.4 | 77.63 | 81.99 | 79.69 | 70.55 | 71.52 | 77.73 | 79.59|\n|ERM-Aug | 74.16 | 81.04 | 83.65 | 68.62 | 74.01 | 68.27 | 81.13 | 84.49|\n|GraphMETRO | 76.18 | 81.71 | 84.26 | 80.31 | 75.1 | 71.05 | 81.85 | 87.14|\n\nAcross all of the synthetic environments, GraphMETRO averagely outperforms ERM and ERM-Aug by 3.20% and 2.45%, respectively.\n\n---\n\nWe appreciate your comments! To address your concerns, below we prudently justify the assumption of our method, the predefined transformation functions, as well as their complexity, and clarify our presented results.\n\n---\n\n### **Comment 1-3: Applicability and our assumptions**\n\nThanks for these great comments! Here we provide response in three folds:\n\n**(1) How does GraphMETRO identify all distribution shifts from transform functions if they are treated independently when combined together?** \n\nIf we understand correctly, by “treated independently”, the reviewer is referring to the first term in our objective $\\text{BCE}(\\phi(\\tau^{(k)}(\\mathcal{G})), Y (\\tau^{(k)}))$, where we formulate predicting the distribution shifts types of a jointly transformed graph as a binary multiclass classification problem. We believe the difficulty of this task comes from both the property of transform functions and the expressiveness of the gating model. \n\n- Firstly, some transform functions are **inherently disentangled**, e.g., adding nodes feature noise and random subgraph extraction. In this case, there will be certain distinction between any pair from these three data distributions, i.e., (graphs with node noise, random subgraph graphs, random subgraphs with node noise), which the gating model can easily tell. \n- While some transform functions can be **essentially similar**, e.g., drop path and drop edges, this won’t affect the performance of our method as long as each expert outputs the corresponding invariant representation. \n- Lastly, indeed, there could be more **complex combinations of the transform functions**, which poses challenges to the gating model’s expressiveness in identifying the combinations. However, this challenge may be minor in the practice. Specifically, we observe fairly high accuracy performances of the gating model, which are above 85% and 73% averagely on extrapolated datasets with one transformation and multiple transformations, respectively.\n\nWe added the above discussion to Appendix F to enable a more comprehensive view towards our methodology. We hope this can alleviate your concern on our gating model’s performance in identifying the distribution shift types.\n\n**(2) How does the predefined transform functions cover complex distributions causing the distribution shift?**\n\nThis is also a great question! We believe there are two angles for this question. \n\n**(a) For general domain**: In our experiments, we mainly use the five stochastic transform functions, which are universal graph augmentations as listed in Zhao et al., (2021) [3]. In our code implementation, we have also included additional transform functions as shown in Appendix B. We believe these transform functions, while not exhaustive, still cover a wide range of distribution shifts observing from our experimental results.\n\nNevertheless, we agree that the real graph distribution shifts can go beyond any possible combinations of the predefined transform functions. In that case, the assumption may not hold, meaning that GraphMETRO may not capture and precisely mitigate the unknown distribution shift. This scenario could always possibly exist due to the lack of information about the testing distribution or its domain knowledge. We include it as a limitation in Appendix 5, while we further discuss how we could alleviate the problem with additional information.\n\n**(b) For specific domains where additional knowledge is available**: In fact, knowing the tendency of the distribution shifts, such as increasing malicious users in a trading system, would be very helpful in constructing the transform functions that can cover the target distribution shifts well. We believe that such knowledge can come from two sources: \n- **Domain knowledge**, e.g., on molecular datasets, the transform function could be adding additional carbon structure to a molecule (while preserving its functional groups). Or, in a particular social network, transform functions can be defined from known user behaviors. \n- **Leveraging a few samples from target distribution**. Specifically, with the guide from a few target samples, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space.\n\n---\n\n[1] Sui et al. Unleashing the Power of Graph Data Augmentation on Covariate Distribution Shift. NeurIPS 2023.\n\n[2] Nianzu Yang et al. Learning substructure invariance for out-of-distribution molecular representations, NeurIPS 2022\n\n[3] Yongduo Sui et al. Causal Attention for Interpretable and Generalizable Graph Classification, KDD 2022.\n\n[4] Jiaqi Ma et al. Subgroup Generalization and Fairness of Graph Neural Networks, NeurIPS 2022.\n\n---\n\n### **Comment 2: Our assumption**\n\nThanks for this comment! In Section 3.1, we have discussed the assumptions in detail with specifications. We repeat part of it here for your convenience:\n\n```Assumption 1 essentially states that the distribution shifts (whatever they are) can be decomposed into several mixture components of stochastic graph transformations. For example, on a social network dataset, each mixture component can represent different patterns of user behavior or network dynamics shifts. Specifically, one mixture component might correspond to increased user activity, while another could signify a particular trend of interaction within a certain group of users. Such a mixture pattern is common and well-studied in the real-world network datasets (Newman, 2003; Leskovec et al., 2005; 2007; Peel et al., 2017).```\n\nWe hope the analysis provides detailed illustration to the assumption. We are happy to add more discussion if you think anything  is still unclear!\n\n---\n\n### **Comment 2 & 3: Implementation of our method**\n\n- **Model architectures**: \nPlease see Tables 2 and 3 in Appendix A for detailed model architecture information. Specifically, we use the same encoders and classifiers from GOOD benchmarkfor real-world datasets to ensure fair comparisons. We employ backbones based on the best ERM performance for synthetic datasets. Moreover, GraphMETRO is model-agnostic, which consistently improves performance across varied model architectures.\n\n- **Stochastic transformation**: Please see Appendix B where we include the introduction of the stochastic transform functions. \n\n- **Optimization process**: Please see Table 2 and 3 (Appendix A), where we included hyperparameters on each dataset. In the second paragraph after Eq. (3), we also described our training pipeline: `“...we set apart the other loss terms from backpropagating to it to avoid interference with the training of the gating model… We optimize the objective via stochastic gradient descent”`. \n\n- **For a more intricate optimization strategy:** This is a great catch! In fact, we did try to pretrain the gating model for several epochs as warm up before training the whole model in an end-to-end fashion via the objective Eq. in (3). However, we didn’t notice a statistically significant difference in their performance, which can be due to that expert models take more time to convergence (since they need to align with the base model during the training) compared to the gating model.\n\nDue to the space limitation, we had to include most of the implementation details in the appendix while we added pointers in the main paper. However, we will try to make it more detailed and feel free to let us know if anything is missing! \n\n---\n\n### **Comment 2 (Cont.): Understand the functionality, underlying mechanism, and performance gain**\n\nThanks for the comment. For clarity, we provide the following pointers to the paper:\n\n- In Section 4.3, we provide a study to reveal the underlying **mechanism** of GraphMETRO, i.e., each expert excels in generating invariant representations concerning a stochastic transform function, which provide a solid foundation in generating referential invariant representations w.r.t. the specific transformations and further guarantee the generalization.\n\n- In Appendix C, we study the **impact of the MoE architecture** on model performance, which shows that the model performance may decline if the expressiveness of the expert model decreases. \n\n- In Appendix D, we study the **impact of the stochastic transform function** on model performance, where we also provide a detailed discussion of the modeling mechanism.\n\nWe believe the above studies and discussion in our paper provides an in-depth view, highlighting the roles of our objective, architecture, and stochastic transform function.  Please let us know if any of these perspectives is still unclear, and we can further improve our experimental study.\n\n-----\n\n# Summary\n\nWe hope our answers can address all of the concerns. We are happy to follow up if you have any further questions.\n\nWe also prudently ask you to reevaluate our work. To highlight, our motivation is supported by the common fine-grained graph shift heterogeneity, and the fact that most of the previous works could not model such nuanced distribution shifts in an effective and flexible manner. Moreover, we added more related works and modified our statement to position our work better. In general, we believe GraphMETRO is a more general and flexible solution that can mitigate a wider range of distribution shifts, which is backed by the experimental results. Detailed justification on our assumption and implementation details are also available in our paper. \n\nThus, we believe our work makes important contributions and provides a clear presentation. We are happy to discuss more and revise our paper if any concern remains. Thank you for your efforts and we are looking forward to your reply!!\n\n---\n\nWe appreciate your comments! To address your concerns, below we prudently justify the motivation of our proposed method, clarify our assumptions, and provide details regarding our experiment implementation.\n\n---------------------\n\n### **Comment 1: Motivation of this work**\n\nThank you for this comment! We believe there might be a bit of misunderstanding due to our different definitions of *“graph shift heterogeneity”*. We firstly discuss the related works mentioned and then justify our statement:\n\n**(1) Related works**\n\nPlease see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nMoreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you! \n\nSpecifically, Sui et al. [1] proposed a graph data augmentation strategy that alleviates covariate shift by generating diverse and invariant causal features. However, the trainable augmenter they used may not distill diverse augmentations or construct unseen perturbations. Moreover, Sui et al. [1] test its method only on graph classification tasks, while GraphMETRO can be applied to both node and graph classification tasks. Besides, we have discussed graph augmentation and attention-based methods in our related works, and we added more recent works on graph OOD [2,3,4], and we hope our response clears your concern on the related work discussion.\n\n\n**(2) The definition of graph shift heterogeneity**\n\nIn this work, we refer to **“heterogeneous shifts”** as multiple and different levels of shifts which vary across different instances (nodes or graphs), as illustrated in the example in the abstract. While we agree that the existing invariant learning approches can accommodate multiple distribution shifts, it could be hard for them to tackle nuanced distribution shifts for individual instances (nodes or graphs) since the distribution shifts are inferred from variance across multiple data environments. If GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering **a mixture of transformations as a proxy** for the target distribution shifts rather than invariance to whole-graph environment shifts. This is the type of heterogeneity we are interested in our paper. \n\n**(3) Regarding our original motivation statement**\n\nWhile the statement pointed out by the reviewer serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category. And we have provided detailed discussion about three lines of research in the related work section.\n\nHowever, we agree that we could make this statement border to cover the previous invariant learning methods. To improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to \"**previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments…**”. We also modified our introduction correspondingly, we hope this will better position our work.\n\n**(4) The choice of our MoE design**\n\n- The choice of our approach comes as a consequence of our motivation to model the graph/instance shift heterogeneity. As mentioned, mitigating multiple and nuanced distribution shifts simply goes beyond certain distribution shift types or environment construction as seen in the previous methods. \n- Thus, GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts. This enables the prediction of multiple different distribution shifts and the flexibility to model fine-grained heterogeneity since the mixture can be varied across different instances. We then tackled the proxy to mitigate the target distribution shifts. Intuitively, this solution provides a **“middle ground”** to deem graph generalization as an equivalent mixture, which, we believe, is a more tractable solution.\n\nWe updated our paper to make the above point more clear. We genuinely hope our answer can justify the motivation and solve your concern.\n\n---\n\n### **Question 2: How does the computational cost of GraphMETRO compare to other methods?**\n\nPlease see the last paragraph of Section 3.4, where we analyze the computation complexity of GraphMETRO. We repeat part of it here for your convenience:\n\n```Consider the scenario where we use an individual encoder for each expert. The forward process of $f$ involves $O(K)$ forward times using the weighted sum aggregation (or $O(1)$ if using the maximum selection). Since we extend the dataset to $(K + 1)$ times larger than the original data, the computation complexity is $O(K^2 |D_s|)$, where |Ds| is the size of the source dataset.```\n\nThus, the computation cost is about $K^2$ or $K$ times (if using the maximum selection) than an ERM model, where $K=5$ in our experiments. Compared to DIR, as they extract $B$ spurious subgraphs from each batch to conduct the intervention, their computation cost is $B$ times compared to ERM, where $B$ could be 32. Thus, we believe the computation cost of GraphMETRO is fair for the gains we get, considering $K$ is usually small.\n\n----------------------\n\n### **Question 3: Can GraphMETRO handle multiple different types of distribution shifts that simultaneously exist in data?**\n\nYes! The distribution shift types corresponding to the gating outputs with high scores will be tackled during training. That is, if the gating output highlights multiple mixture components, their corresponding distribution shift types will be handled jointly.\n\n----------------------\n\n### **Question 4: Can GraphMETRO  tackle distribution shifts on molecular graphs?**\n\nThat is a great idea! Yes, GraphMETRO can be applied to molecular datasets if one designs transform functions to cover typical molecular variants. For instance, a transform function may add carbon structures to the molecules. These domain-specific transform functions are outside the scope of our work, however, we believe these would be interesting future work directions!\n\n----------------------\n\n# Summary \n\nWe thank the reviewer for the time and insightful suggestions! We hope our answers can address your concerns well.\n\nWe also prudently ask you to reconsider our work if the concerns are addressed. To highlight, our novelty comes from the formulation of an equivalent mixture for graph OOD and the training framework to effectively realize generalization. We also provide an in-depth analysis on our originality compared to some previous invariant learning methods. While we discussed and compared with previous works, we added more related works and modified our statement to position our work better. Finally, our method achieves great improvements on both node and graph classification tasks, and is a more general solution to mitigate multiple and nuanced distribution shifts. \n\nOverall, we believe our work proposes a new paradigm and novel training framework and makes good contributions in the fields of graph generalization, and we would appreciate your reconsideration on this point. Thank you for your efforts again!\n\n----------------------\n\n**Reference**\n\n\n[1-5] The same as listed by the reviewer\n\n\n[6] Beatrice Bevilacqua, Yangze Zhou, and Bruno Ribeiro. Size-invariant graph representations for graph classification extrapolations. In ICML, 2021.\n\n[7] Davide Buffelli, Pietro Li´o, and Fabio Vandin. Sizeshiftreg: a regularization method for improving size-generalization in graph neural networks. In NeurIPS, 2022.\n\n[8] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In NeurIPS, 2019.\n\n[9] Mucong Ding, Kezhi Kong, Jiuhai Chen, John Kirchenbauer, Micah Goldblum, David Wipf, Furong Huang, and Tom Goldstein. A closer look at distribution shifts and out-of-distribution generalization on graphs. In NeurIPS DistShift, 2021.\n\n---\n\n### **Comment 3 & Question 1: Related works on invariant learning and clarification on our statement.**\n\nThanks for pointing it out! Below we clarify the statement and clear potential misunderstanding:\n\n**(1) How does GraphMETRO compare with invariant learning methods like DIR and EERM?** \n\nPlease see our related work section where we discussed these two papers, i.e., DIR (Wu et al., 2022c) and EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on environmental patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nBesides, we also provide a more in-depth comparison in our point (3) below to highlight our key technical originality.\n\n**(2) Regarding our statement about previous works:** \n\nWhile the statement serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category, and we have provided detailed discussion about three lines of research in the related work section.\n\nTo improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to **\"previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments** (which is highly limited when confronted with nuanced distribution shifts)”. We also modified our introduction correspondingly. Thanks for letting us know our statement could be misinterpreted.\n\n**(3) Why do we say our method could be more broad than the existing invariant learning approaches?** \n\n- Invariant subgraph learning approaches, e.g., [1,2], consider variance of constructed data environments, which are designed very differently compared to our work. \nWhile they can accommodate multiple distribution shifts (as in multiple environments), these focus on patterns within each environment and ignore the variety across instances (e.g., shifts at the resolution of nodes), which may not be well-captured by the environment assignments. \n- GraphMETRO considers that specific parts of the test graph may have different shifts. Particularly, our goal is to make the generalization to unknown testing distribution more adaptive and broad, as opposed to limiting the distribution shifts to being invariant to specific types of subgraphs. \n\nIn other words, if GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering a mixture of transformations as a proxy for the target distribution shifts rather than invariance to environment shifts.\n\n**(4) Key technical originality compared to invariance learning (going more deeply)**\n\nAnother interesting view to see the innovation of GraphMETRO is that it breaks the typical invariant learning formulation, which assumes the data is manipulated by the environment variables (and then can be “decoded” into multiple environments). Instead, GraphMETRO sees the distribution shifts on an instance as a mixture, which is represented by the score vector output by the gating function over the basis of the transform functions. In other words, GraphMETRO can produce infinite environments as the elements in the score vector are continuous. One can see that once we limit the output domain of the gating function into, e.g., binary {0, 1}, GraphMETRO can also produce a limited number of environments (if we categorize the instances based on the score vector), which covers the environment construction in invariant learning. Moreover, as mentioned, we propose the concept of referential invariant representation with a base model $\\xi_0$, which is also different from previous works on invariant learning. We added the above discussion to Appendix F to improve the depth of our analysis.\n\n---\n\nWe appreciate your efforts and insightful comments! To address your concerns, we provide point-to-point responses below.\n\n---------------------\n\n### **Comment 1: Regarding the novelty of GraphMETRO.**\n\nThanks for the comment! We believe our novelty comes from the proposal of an equivalent mixture for graph OOD and the construction of our training framework, as detailed below:\n\n- **An equivalent mixture for graph OOD**: The key challenge we faced to mitigate multiple and nuanced distribution shifts is the intrinsic complexity and heterogeneity of graph distribution shifts, which simply goes beyond certain distribution shift types [6,7,8,9] or environment construction as seen in the previous methods [1,2,3]. GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts, where the mixture can be varied across different instances, and then tackled the proxy to mitigate the target distribution shifts. We believe the high-level idea is succinct, nevertheless, novel, in the sense that it provides a “middle ground” to deem graph generalization as an equivalent mixture that is more tractable.\n\n- **Training framework**: With the guide of our formulation, the training framework is still non-trivial due to two problems, i.e., “how to provide supervision for predicting the mixture” and “how to ensure the experts corresponding to mixture components are compatible when working as a whole”. Specifically, GraphMETRO solves the first problem by conducting graph extrapolation. This is somewhat similar to the spirit of graph pretraining in the sense that we inject heterogeneity to promote the expressiveness of the gating model in recognizing the mixture components. For the second problem, we introduce the concept of Referential Invariant Representation, along with the novel objective in Eq. (3) to enforce the invariance and compatibility. It is worth mentioning that the model performance is much worse than the reported numbers (e.g., 2.7\\% lower on Twitch dataset) without the compatibility constraint, indicating the proposed referential invariance concept is indispensable. \n\nWe added more justification in the introduction (updated in the revision). We genuinely hope our responses can solve your concerns about the novelty of our work.\n\n------------------------------\n\n### **Comment 2: Comparison with recent methods [3,4,5].**\n\nIn compacting the paper to fit in the page limit we mistakenly did not include these relevant references, we apologize. We added discussion in the revised version. Here we summarize these works and point out their key differences with our method: \n\n- In particular, **Yang et al. [3]** explore molecule representation learning in out-of-distribution (OOD) scenarios. They achieve this by directing the molecule encoder to utilize stable and environment-invariant substructures relevant to the labels without the need for environmental labels. \n- Similarly, **Sui et al. [4]** introduces causal attention modules to identify key invariant subgraph features that can be described as causing the graph label. The type of OOD task that Sui et al. [4] considers assumes the graph label is caused by a subgraph, which is quite different from ours. Moreover, both Yang et al. [3] and Sui et al. [4] consider tasks where the graph label is caused by a subgraph. \n- **Ma et al.[5]** is an interesting theoretical work which studies GNN generalization and examines their fairness, showing that the test subgroup's distance from the training set impacts GNN performance. Ma et al.[5], as far as we could assess, does not propose any specific architecture to solve the type of OOD tasks we consider in our work.\n\nOverall, the goal of GraphMETRO is to be invariant to a mixture of selected stochastic transform functions (and the mixture can vary across different instances), which is a more flexible and general solution. \nWe added a discussion of these works to our revision, hopefully providing a more comprehensive comparison and literature overview. We hope our responses can solve your concerns about the related work.\n\n---\n\nWe sincerely appreciate all reviewers' time, efforts, and valuable suggestions in reviewing our paper. We are glad that most of the reviewers reached a positive consensus on our work's motivation and experimental results. Here is a summary of our responses:\n\n- **Clarification**: We clarify our key assumptions (`m8JS, FURX`), novelty (`m8JS, NLg1`), and the experimental settings (`m8JS`).\n\n- **Related work**: : We add more related works on recent graph OOD learning (`NLg1, m8JS`) and graph transfer learning (`FURX`)\n\n- **Limitation**: We add discussion about the limitations of GraphMETRO when confronted with unknown distribution shifts that surpass our key assumptions (`FURX`, `C9mM`).\n\nWe hope our responses can clarify your confusion and alleviate concerns and we updated our paper **(highlighted in green)** corespondingly. We thank all reviewers again, and look forward to your reply!", "author_response": "We appreciate your comments! To address your concerns, below we prudently justify the motivation of our proposed method, clarify our assumptions, and provide details regarding our experiment implementation.\n\n---------------------\n\n### **Comment 1: Motivation of this work**\n\nThank you for this comment! We believe there might be a bit of misunderstanding due to our different definitions of *“graph shift heterogeneity”*. We firstly discuss the related works mentioned and then justify our statement:\n\n**(1) Related works**\n\nPlease see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nMoreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you! \n\nSpecifically, Sui et al. [1] proposed a graph data augmentation strategy that alleviates covariate shift by generating diverse and invariant causal features. However, the trainable augmenter they used may not distill diverse augmentations or construct unseen perturbations. Moreover, Sui et al. [1] test its method only on graph classification tasks, while GraphMETRO can be applied to both node and graph classification tasks. Besides, we have discussed graph augmentation and attention-based methods in our related works, and we added more recent works on graph OOD [2,3,4], and we hope our response clears your concern on the related work discussion.\n\n\n**(2) The definition of graph shift heterogeneity**\n\nIn this work, we refer to **“heterogeneous shifts”** as multiple and different levels of shifts which vary across different instances (nodes or graphs), as illustrated in the example in the abstract. While we agree that the existing invariant learning approches can accommodate multiple distribution shifts, it could be hard for them to tackle nuanced distribution shifts for individual instances (nodes or graphs) since the distribution shifts are inferred from variance across multiple data environments. If GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering **a mixture of transformations as a proxy** for the target distribution shifts rather than invariance to whole-graph environment shifts. This is the type of heterogeneity we are interested in our paper. \n\n**(3) Regarding our original motivation statement**\n\nWhile the statement pointed out by the reviewer serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category. And we have provided detailed discussion about three lines of research in the related work section.\n\nHowever, we agree that we could make this statement border to cover the previous invariant learning methods. To improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to \"**previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments…**”. We also modified our introduction correspondingly, we hope this will better position our work.\n\n**(4) The choice of our MoE design**\n\n- The choice of our approach comes as a consequence of our motivation to model the graph/instance shift heterogeneity. As mentioned, mitigating multiple and nuanced distribution shifts simply goes beyond certain distribution shift types or environment construction as seen in the previous methods. \n- Thus, GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts. This enables the prediction of multiple different distribution shifts and the flexibility to model fine-grained heterogeneity since the mixture can be varied across different instances. We then tackled the proxy to mitigate the target distribution shifts. Intuitively, this solution provides a **“middle ground”** to deem graph generalization as an equivalent mixture, which, we believe, is a more tractable solution.\n\nWe updated our paper to make the above point more clear. We genuinely hope our answer can justify the motivation and solve your concern."}
{"claim": "Key architectural design choices are presented without rationale, making the model architecture appear arbitrary to readers.", "claim_type": "methodology", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ppMpJyIYRZ", "reviewer": "Reviewer_m8JS", "review_text": "Summary: The paper introduces a method to enhance the out-of-distribution performance of graph neural networks (GNN) by learning to understand distribution shifts instead of addressing the assumed ones. To achieve this, the Mixture of Experts architecture is integrated into the GNN, supplemented by an alignment procedure to recognize the shift. Empirical experiments are conducted to validate the theoretical assertion.\n\nOn the whole, I believe the proposed method lacks the necessary motivation and its novelty isn't substantial enough to meet the standard.\n\nStrengths: - The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\n- Real-world datasets back the claims through experiments.\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\n\nWeaknesses: - The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \"previous research has concentrated on addressing specific types of distribution shifts.\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \"environment generators\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\n\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\n\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\n\nQuestions: Please check Weaknesses.", "labeling_timestamp": "2026-01-11T16:31:30.164919", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors acknowledge shortcomings in clarity and provide rationale for architectural choices (especially the MoE design), point to detailed architecture tables and ablation studies in the appendix, and say they updated the paper — but they do not explicitly concede that the original presentation made the design appear arbitrary nor claim they fully remedied the reviewer's concern in the current submission.", "evidence": "“We do apologize for not making our solutions towards the limitations clear enough.”; “The choice of our MoE design - The choice of our approach comes as a consequence of our motivation to model the graph/instance shift heterogeneity. As mentioned, mitigating multiple and nuanced distribution shifts simply goes beyond certain distribution shift types or environment construction as seen in the previous methods.”; “Model architectures: Please see Tables 2 and 3 in Appendix A for detailed model architecture information.”", "combined_author_response": "Dear reviewer NLg1,\n\nAs the discussion period is closing soon, we hope to engage with you and check if your concerns are addressed.\n\nWe sincerely appreciate your time and attention!\n\nBest,\n\nAuthors of Paper 1560\n\n---\n\nThanks for reading our response! We would also appreciate your patience for reading the two points below \n\n---\n\n**Regarding point #1:**\n\nYes, we agree that the concept of heterogeneous shifts is relatively new for the current studies on distribution shifts, however, this is not new for studies on network patterns (Newman, 2003; Leskovec et al., 2005; 2007; Peel et al., 2017). In fact, the ignorance of such nuanced heterogeneous shifts in the previous studies instead emphasises our motivation and the potential impact of this work. \n\nFrom a causality perspective, distribution shifts can naturally happen in the instance level when (1) additional causal variables, beyond environmental factors and randomized noise, influence these shifts, or (2) multiple causal variables simultaneously affect the shifts with different strengths. Without modeling these explicitly, the mitigation of distribution shifts can easily fail. \n\nIn terms of our presentation, we have illustrated these cases of interest in abstract and introduction, we made further explanations in Section 3.1, we also present the specific results of distribution types in Figure 3 (b). \n\n\n**Regarding point #2:**\n\nWe think we can all agree that, if the ground truth of the instance shifts is available on the real-world datasets, it would be crystal clear to see where the improvement comes from since we can conduct case study to compare our method and the baselines on instances with nuanced distribution shifts, to see the influence of the modeling these heterogeneous shifts. \n\nHowever, with such ground truth not available, we had to seek other seemingly less intuitive but also in-depth way to illustrate the insights (esp Section 4.3) as mentioned above in our previous response. This is also why we designed the synthetic experiments at the first place. We think we did try hard to explain the underlying rationale with the ground truth being absent.\n\n----\n\n**Refinement**: We can of course add a causal graph in the our assumption section to make the concept more clear. And we can illustrate more if you could let us know the specific obstable to understand our mechanism, which will be extremely helpful.\n\n----\n\n### **Summary/TL;DR**\nWe understand the reviewer's clarification concerns, however, we don't agree that they, based on our justificaton, are the cause of rejection. We believe nuanced heterogeneous shifts are common, important, yet being typically ignored in the research domain of distribution shifts, we made these argument clear and also promise to refine. While explaining the underlying rationale is hindered by the lack of ground truth, we did try hard to dissection it from the gating model, the invariant representations generated, and more ablations in the appendix. We respect the reviewer's current opinion. Still, reconsideration will be greatly appreciated.\n\n---\n\nWe thank the reviewer once again! We also enjoyed the process of making our work more sound from your suggestions. And your approval surely means a lot to us.\n\nWe do apologize for not making our solutions towards the limitations clear enough. We summarize them into a short table, hopefully could alleviate your concerns a bit more.\n\n| Limitation| Potential solutions | Location of discussion/action |\n|:---|:---|:---|\n|**Coverage of the transformations** | 1) Extend the coverage by adding representative transform functions. 2) Or include tranform functions based on domain knowledge or a few samples from target distribution | Appendix F, paragraph #3|\n|**Complexity of the transformations** | Make experts dedicated to different hyperparameters for the same type of transformation | Argument options are available in our codebase. Will make it more detailed in the experimental settings. |\n|**Label distributional shift**|Integrate the objective of the existing methods studying labe distributional shifts into our framework. |Appendix F, paragraph #4 |\n\n----\n\nWe completely agree on the existence of these limitations and we will move some limitations (esp #1) to the main paper in our final version. While they could be important in the practice, they are, in our perspective, fair \"side effects\" considering the benefits (i.e., mitigating multiple and nuanced distribution shifts and better interpretability), and may not be the central part of our novelty and main contribution (i.e., the proposal of an equivalent mixture, the concept of referential invariant representations, as well as the training framework). We are also eager to further improve other aspects.\n\nOnce again thank you so much for your support!!\n\n---\n\nDear Reviewer m8JS,\n\nA gentle nudge that we would like to know if our response adequately addresses your concerns. \n\nYour time and feedback is greatly appreciated!\n\nThank you sincerely,\n\nAuthors of Paper 1560\n\n---\n\nWe deeply appreciate your approval. Your suggestions definitely inspired us a lot and have greatly improved our work.\n\nA few additional notes for the further comments:\n\n- Regarding **point #2**: In node classification tasks, the BCE objective already considers node embeddings other than the graph embedding of the k-hop subgraph. What we described previously refers to the computational graph (i.e., k-hop message passing), which generates node embeddings. We apologize for any confusion.\n\n- Regarding **point #3**: Yes it would be quite interesting to see how graph pretraining methods perform on the current OOD benchmarks. Graph pretraining methods like GCC and GPT-GNN also consider graph extrapolation to some extent, e.g., through subgraph extraction and masked attributes/structures. The key difference between graph pretraining and generalization may lie in their different focuses on **expressiveness** and **invariance**. While these two aspects do not always conflict, ensuring invariance w.r.t. a certain type of extrapolation might affect expressiveness (if the change is relevant to labels), and vice versa. To seek a balance in between, one might need prior knowledge in which types of transformations may or may not be sensitive to the labels (and perhaps build experts with different goals to make ensure invariance or expressivess). We believe there is still a lot to explore in this domain.\n\nOverall, we are grateful for your positive stand on this work. We believe that our [current version](https://openreview.net/pdf?id=QQ5eVDIMu4), incorporating opinions from you and other reviewers, is sound and well-refined. We are also committed to further improving it. We would appreciate your support based on our current version! \n\nThank you!\n\n---\n\nDear Reviewers of Paper 1560,\n\nWe hope this message finds you well.\n\nAs the discussion phase approaches its end, we hope you find our responses useful. We would like to ask if the issues have been addressed.\n\nWe understand that the discussion time is short, and some of you might be enjoying holidays at the moment. We apologize for posting our responses a bit late as we aimed to address your concerns clearly.\n\nWe sincerely appreciate your time and attention!\n\nBest regards,\n\nAuthors of Paper 1560\n\n---\n\n**Comment 4: Related works on graph transfer learning.**\n\nYes! We agree that graph transfer learning is a relevant topic. Thank you for providing the additional related work! We added the discussion on these works in our revision, and we repeat it here for your convenience:\n\n```It is also worth mentioning that graph domain adaptation (Zhang et al., 2019; Wu et al., 2020), different from the problem studied in this work, commonly relies on limited labeled samples from target datasets for improved transferability. For instance, to generate domain adaptive network embedding, DANE (Zhang et al., 2019) uses shared weight graph convolutional networks and adversarial learning regularization, while UDA-GCN (Wu et al., 2020) employs attention mechanisms to merge global and local consistencies.```\n\nMoreover, we believe this is actually relevant to `Comment & Question 1` regarding the generality of the distribution shifts covered by the transform functions. In our future works, we added the following discussion:\n\n```...Leveraging a few samples from target distribution (i.e., domain adaptation). Specifically, we can leverage the samples from the target distribution to inform the selection or construction of transform functions, which can better guarantee the distribution shifts are covered by the transform functions.  For example, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space. We believe this would be an interesting future direction.```\n\n---\n\n# Summary\n\nOnce again thank you very much for helping us position our work and reflect the literature better. We hope all of your concerns are solved and we are happy to engage further if there are any other points we missed!\n\n---\n\n### **Reference**\n\n[1] Unsupervised Domain Adaptive Graph Convolutional Networks. Wu et al. WWW 2020\n\n[2] DANE: Domain adaptive network embedding. Zhang et al. IJCAI 2019.\n\n[3] Tong Zhao, Yozen Liu, Leonardo Neves, Oliver J. Woodford, Meng Jiang, and Neil Shah. Data augmentation for graph neural networks. AAAI, 2021.\n\n---\n\nWe are grateful for your positive feedback and detailed suggestions! We provide responses below to address your remaining concerns. \n\n---\n\n### **Comment & Question 1: Generality of the distribution shifts covered by the transform functions.**\n\nGood question! We provide the response from two angles:\n\n\n**(a) For general domain**: In our experiments, we mainly use the five stochastic transform functions, which are universal graph augmentations as listed in Zhao et al., (2021) [3]. In our code implementation, we have also included additional transform functions as shown in Appendix B. We believe these transform functions, while not exhaustive, still cover a wide range of distribution shifts observing from our experimental results.\n\nNevertheless, we agree that the real graph distribution shifts can go beyond any possible combinations of the predefined transform functions. In that case, the assumption may not hold, meaning that GraphMETRO may not capture and precisely mitigate the unknown distribution shift. This scenario could always possibly exist due to the lack of information about the testing distribution or its domain knowledge. We include it as a limitation in Appendix 5, while we further discuss how we could alleviate the problem with additional information.\n\n**(b) For specific domains where additional knowledge is available**: In fact, knowing the tendency of the distribution shifts, such as increasing malicious users in a trading system, would be very helpful in constructing the transform functions that can cover the target distribution shifts well. We believe that such knowledge can come from two sources: \n- **Domain knowledge**, e.g., on molecular datasets, the transform function could be adding additional carbon structure to a molecule (while preserving its functional groups). Or, in a particular social network, transform functions can be defined from known user behaviors. \n- **Leveraging a few samples from target distribution**. This is in fact in line with the reviewer’s Comment #4 regarding graph transfer learning. Specifically, with the guide from a few target samples, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space.\n\n We hope this response can alleviate your concern about our applicability.\n\n---\n\n### **Comment & Question 2: The mechanism of GaphMETRO in handling heterogeneous shifts within one graph.**\n\nGood question! For simplicity, suppose we have two transform functions, i.e., adding edges and dropping edges. Given a node classification task and the objective in Eq. (3), the model is trained on the extrapolated datasets based on the transform functions. After that, given an unseen graph with imbalanced distribution shifts, the gating model outputs scores to identify nodes likely to experience increasing or decreasing degrees, while others might adhere to the original distribution. Then, for each node, each expert takes its multihop subgraph, and outputs its referential invariant representation w.r.t. the correposing transform function. These expert outputs and gating model results are then aggregated to form the final representation. Assuming accurate predictions by the gating function, nodes in denser/sparser subgraphs are represented by expert models corresponding to adding/dropping edges. As each expert is trained to create invariant representations, the final node representations remain unaffected by their individual distribution shifts.\n\n---\n\n### **Comment 3: How to leverage graph pretraining.**\n\nIf we understand correctly, the reviewer was asking how to leverage a pretrained model to further aid the training of GraphMETRO. Please let us know if otherwise. \n\nThis is in fact an interesting point! We believe a model pretrained on a wide variety of data can be very helpful to initialize the gating model, which is required to output the mixture of a node or graph (after it is finetuned on the extrapolated dataset). By enhancing the gating model's predictive capability regarding mixtures, GraphMETRO's final representation should become more resilient. This becomes particularly advantageous when dealing with graphs not previously encountered in the extrapolated dataset. Moreover, it is possible that the pretrained model will also benefit the expert models, while one minor concern would be that the expert model may tend to be similar instead of dedicating to generating invariant representation w.r.t. their corresponding transform function. We added the above discussion to Appendix F as a future work. Thanks again for this comment!\n\n---\n\n### **Question 4: Why does the test accuracy vary across different transformations?**\n\nGreat question! Here we summarize three possible reasons:\n\n- **Information Preservation in Transformations:** Certain transformations retain more informative features than others. For instance, in the REDDIT-BINARY (graph classification task), the random subgraph transformation may retain more graph label-related information compared to dropping edges, as the latter tends to lose more global information. This discrepancy in testing performance, where dropping edges outperforms random subgraph extraction, could be due to the preservation of crucial information. However, conclusions may vary across datasets or tasks depending on how information influences final predictions. For CiteSeer (a node classification task), a random subgraph might preserve more local node information, potentially explaining why its testing performance surpasses dropping edges in this specific task.\n\n- **Complexity of transformation:** Certain transformations inherently generate more diverse graphs than others. If the model lacks the expressive capacity to capture such diversity, it may lead to a decline in testing performance.\n\n- **Model Sensitivity:** Certain transformations may be easier for a model to learn due to compatibility with specific model architectures. This extends beyond transformation complexity and emphasizes how different model architectures may prefer learning particular distributions from one of the extrapolated datasets, which can also contribute to the difference in the testing performance.\n\nWe included the above discussion to Appendix F: Open Discussions. We hope this response can answer your question and improve the soundness of our work.\n\n---\n\n# Summary \n\nWe are grateful for your time and insightful suggestions! \n\nWe would like to highlight that our main contribution is framing the graph generalization problem on top of an equivalent mixture, a simple yet novel and tractable \"middle ground\", as well as proposing the training framework which effectively guarantees the generalization. While our method relies on a set of predefined transform functions, we believe they cover a wide range of distribution shifts based on our empirical results. Also, we agree that there could be some scenarios where the transform functions may not cover complex distributions, and we discuss two future directions and include them into our future works. Moreover, while selecting the hyperparameters for the transform functions introduce extra complexity, the issue could be minor in practice and we also conduct more experiments to justify the applicability better. Finally, we address several questions about clarification and presentation, as well as including more future works. \n\nLastly, we prudently ask you to reevaluate our work given the clarification in our responses, which we also updated our paper correspondingly. Overall, we believe our work makes good contributions to the field of graph distribution learning by proposing a novel and effective solution, and we would appreciate your reconsideration on this point. Thank you for your efforts again!\n\n---\n\n### **Reference**\n\n[1] Menon, Aditya Krishna, et al. \"Long-tail learning via logit adjustment.\" International Conference on Learning Representations. 2020.\n\n[2] Cao, Kaidi, et al. \"Learning imbalanced datasets with label-distribution-aware margin loss.\" Advances in neural information processing systems 32 (2019).\n\n[3] DARTS: Differentiable Architecture Search. Hanxiao Liu, Karen Simonyan, Yiming Yang. 2018.\n\n---\n\n**(3) What is the complexity of the transform functions and how does it affect generalization?** \n\nInteresting question! In fact, our implementation and framework could easily avoid selecting hyperparameters on the transform functions. Specifically, we can make multiple transform functions of the same type with different ranges of hyperparameters. Specifically, GraphMETRO allows three edge dropping transform functions, $\\tau_1^{\\alpha_1}, \\tau_2^{\\alpha_2}, \\tau_3^{\\alpha_3}$, where $\\alpha_i$ (i=1, 2, 3) are three different ranges of edge dropping probabilities, e.g., [0.1,0.3], [0.3, 0.6], [0.6,0.9], representing different transform extents. Thus, given an input from the validation dataset, the gating model will highlight the transform function which simultaneously selects the corresponding hyperparameter that matches the distribution of the validation set. Interestingly, this idea is in the same spirit as how DARTS [3]  proposes to perform architecture search by formulating the task in a differentiable manner. \n\nIn our previous experiments, we did try this scheme where we replaced a single edge dropping transform function with the ratio range [0.3, 0.5] to three transform functions as mentioned above. While we didn’t see a significantly different performance in that case, we believe this would be a flexible solution which avoids the need to conduct hyperparameter selection. \n\nWe include the above discussion in our open discussion and future works (Appendix F). We hope this response can alleviate your concern about the applicability of our method. \n\n---\n\n### **Comment 4: Limitations of our work**\n\nGreat point! We think the issue of label distributional shift, while important, is orthogonal and complementary to the focus of our current study. To elaborate, label distributional shifts exert analogous impacts across various modalities, such as graphs or images. Moreover, existing methods [1,2] designed to tackle label distributional shifts can be seamlessly integrated into our proposed framework. Such integration would necessitate minimal adjustments, potentially involving modifications to the loss function or the training pipeline. We added this as a future work in Appendix F.\n\n---\n\n### **Question 1: Statistical significance of the results on Table 1**\n\nThanks for the question! We compute the p-value of our method against the best baselines method as follows:\n\n| | WebKB | Twitch | Twitter | SST2 | \n|:--|:--|:--|:--|:--|\n|p-value|< 0.001| 0.023 | 0.042| 0.081|\n\nGiven the cut-off threshold as 0.05, we believe the performances of GraphMETRO are statistically significant on WekGB, Twitch, and Twitter datasets, while on the SST2, we see relatively weak evidence. We added the p-value results to our revision and hope our response can alleviate your concern on our improvements.\n\n---\n\n###  **Question 2: Can we say we use ERM for the node classification even if nodes and their labels are not i.i.d.?** \n\nIf we understand correctly, the reviewer is asking for clarification on the 2nd term of our objective. Please let us know if otherwise. Here, our thinking is that the cross-entropy loss for node classification already assumes node labels are conditionally independent given the model (the negative log-likelihood is a sum over the labeled nodes in training). Then, we use the same assumption of cross-entropy on Empirical Risk Minimization (ERM). That is, for a given model we must also minimize the error variance across nodes. The task is then to find the model with the best performance and small variance.\n\n---\n\n### **Question 3: Numerical results on Figure 2**\n\nThanks for pointing it out! We included all of the numerical results of Figure 2 in Appendix E, while showing the results on DBLP below.\n\n| |i.i.d. (0)|noisy feature (1)|add edge (2)|drop edge (3)|drop node (4)|random subgraph (5)|\n|:--|:--|:--|:--|:--|:--|:--|\n|ERM | 85.71 | 84.48 | 71.08 | 79.69 | 83.41 | 76.9|\n|ERM-Aug | 85.66 | 85.29 | 74.85 | 82.34 | 84.44 | 72.81|\n|GraphMETRO | 85.92 | 85.78 | 76.61 | 82.95 | 84.98 | 81.32|\n\n| |(4, 5)|(3, 5)|(2, 5)|(1, 5)|(2, 4)|(1, 4)|     (2, 3)|(1, 3)|\n|:--|:--|:--|:--|:--|:--|:--|:--|:--|\n|ERM | 70.4 | 77.63 | 81.99 | 79.69 | 70.55 | 71.52 | 77.73 | 79.59|\n|ERM-Aug | 74.16 | 81.04 | 83.65 | 68.62 | 74.01 | 68.27 | 81.13 | 84.49|\n|GraphMETRO | 76.18 | 81.71 | 84.26 | 80.31 | 75.1 | 71.05 | 81.85 | 87.14|\n\nAcross all of the synthetic environments, GraphMETRO averagely outperforms ERM and ERM-Aug by 3.20% and 2.45%, respectively.\n\n---\n\nWe appreciate your comments! To address your concerns, below we prudently justify the assumption of our method, the predefined transformation functions, as well as their complexity, and clarify our presented results.\n\n---\n\n### **Comment 1-3: Applicability and our assumptions**\n\nThanks for these great comments! Here we provide response in three folds:\n\n**(1) How does GraphMETRO identify all distribution shifts from transform functions if they are treated independently when combined together?** \n\nIf we understand correctly, by “treated independently”, the reviewer is referring to the first term in our objective $\\text{BCE}(\\phi(\\tau^{(k)}(\\mathcal{G})), Y (\\tau^{(k)}))$, where we formulate predicting the distribution shifts types of a jointly transformed graph as a binary multiclass classification problem. We believe the difficulty of this task comes from both the property of transform functions and the expressiveness of the gating model. \n\n- Firstly, some transform functions are **inherently disentangled**, e.g., adding nodes feature noise and random subgraph extraction. In this case, there will be certain distinction between any pair from these three data distributions, i.e., (graphs with node noise, random subgraph graphs, random subgraphs with node noise), which the gating model can easily tell. \n- While some transform functions can be **essentially similar**, e.g., drop path and drop edges, this won’t affect the performance of our method as long as each expert outputs the corresponding invariant representation. \n- Lastly, indeed, there could be more **complex combinations of the transform functions**, which poses challenges to the gating model’s expressiveness in identifying the combinations. However, this challenge may be minor in the practice. Specifically, we observe fairly high accuracy performances of the gating model, which are above 85% and 73% averagely on extrapolated datasets with one transformation and multiple transformations, respectively.\n\nWe added the above discussion to Appendix F to enable a more comprehensive view towards our methodology. We hope this can alleviate your concern on our gating model’s performance in identifying the distribution shift types.\n\n**(2) How does the predefined transform functions cover complex distributions causing the distribution shift?**\n\nThis is also a great question! We believe there are two angles for this question. \n\n**(a) For general domain**: In our experiments, we mainly use the five stochastic transform functions, which are universal graph augmentations as listed in Zhao et al., (2021) [3]. In our code implementation, we have also included additional transform functions as shown in Appendix B. We believe these transform functions, while not exhaustive, still cover a wide range of distribution shifts observing from our experimental results.\n\nNevertheless, we agree that the real graph distribution shifts can go beyond any possible combinations of the predefined transform functions. In that case, the assumption may not hold, meaning that GraphMETRO may not capture and precisely mitigate the unknown distribution shift. This scenario could always possibly exist due to the lack of information about the testing distribution or its domain knowledge. We include it as a limitation in Appendix 5, while we further discuss how we could alleviate the problem with additional information.\n\n**(b) For specific domains where additional knowledge is available**: In fact, knowing the tendency of the distribution shifts, such as increasing malicious users in a trading system, would be very helpful in constructing the transform functions that can cover the target distribution shifts well. We believe that such knowledge can come from two sources: \n- **Domain knowledge**, e.g., on molecular datasets, the transform function could be adding additional carbon structure to a molecule (while preserving its functional groups). Or, in a particular social network, transform functions can be defined from known user behaviors. \n- **Leveraging a few samples from target distribution**. Specifically, with the guide from a few target samples, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space.\n\n---\n\n[1] Sui et al. Unleashing the Power of Graph Data Augmentation on Covariate Distribution Shift. NeurIPS 2023.\n\n[2] Nianzu Yang et al. Learning substructure invariance for out-of-distribution molecular representations, NeurIPS 2022\n\n[3] Yongduo Sui et al. Causal Attention for Interpretable and Generalizable Graph Classification, KDD 2022.\n\n[4] Jiaqi Ma et al. Subgroup Generalization and Fairness of Graph Neural Networks, NeurIPS 2022.\n\n---\n\n### **Comment 2: Our assumption**\n\nThanks for this comment! In Section 3.1, we have discussed the assumptions in detail with specifications. We repeat part of it here for your convenience:\n\n```Assumption 1 essentially states that the distribution shifts (whatever they are) can be decomposed into several mixture components of stochastic graph transformations. For example, on a social network dataset, each mixture component can represent different patterns of user behavior or network dynamics shifts. Specifically, one mixture component might correspond to increased user activity, while another could signify a particular trend of interaction within a certain group of users. Such a mixture pattern is common and well-studied in the real-world network datasets (Newman, 2003; Leskovec et al., 2005; 2007; Peel et al., 2017).```\n\nWe hope the analysis provides detailed illustration to the assumption. We are happy to add more discussion if you think anything  is still unclear!\n\n---\n\n### **Comment 2 & 3: Implementation of our method**\n\n- **Model architectures**: \nPlease see Tables 2 and 3 in Appendix A for detailed model architecture information. Specifically, we use the same encoders and classifiers from GOOD benchmarkfor real-world datasets to ensure fair comparisons. We employ backbones based on the best ERM performance for synthetic datasets. Moreover, GraphMETRO is model-agnostic, which consistently improves performance across varied model architectures.\n\n- **Stochastic transformation**: Please see Appendix B where we include the introduction of the stochastic transform functions. \n\n- **Optimization process**: Please see Table 2 and 3 (Appendix A), where we included hyperparameters on each dataset. In the second paragraph after Eq. (3), we also described our training pipeline: `“...we set apart the other loss terms from backpropagating to it to avoid interference with the training of the gating model… We optimize the objective via stochastic gradient descent”`. \n\n- **For a more intricate optimization strategy:** This is a great catch! In fact, we did try to pretrain the gating model for several epochs as warm up before training the whole model in an end-to-end fashion via the objective Eq. in (3). However, we didn’t notice a statistically significant difference in their performance, which can be due to that expert models take more time to convergence (since they need to align with the base model during the training) compared to the gating model.\n\nDue to the space limitation, we had to include most of the implementation details in the appendix while we added pointers in the main paper. However, we will try to make it more detailed and feel free to let us know if anything is missing! \n\n---\n\n### **Comment 2 (Cont.): Understand the functionality, underlying mechanism, and performance gain**\n\nThanks for the comment. For clarity, we provide the following pointers to the paper:\n\n- In Section 4.3, we provide a study to reveal the underlying **mechanism** of GraphMETRO, i.e., each expert excels in generating invariant representations concerning a stochastic transform function, which provide a solid foundation in generating referential invariant representations w.r.t. the specific transformations and further guarantee the generalization.\n\n- In Appendix C, we study the **impact of the MoE architecture** on model performance, which shows that the model performance may decline if the expressiveness of the expert model decreases. \n\n- In Appendix D, we study the **impact of the stochastic transform function** on model performance, where we also provide a detailed discussion of the modeling mechanism.\n\nWe believe the above studies and discussion in our paper provides an in-depth view, highlighting the roles of our objective, architecture, and stochastic transform function.  Please let us know if any of these perspectives is still unclear, and we can further improve our experimental study.\n\n-----\n\n# Summary\n\nWe hope our answers can address all of the concerns. We are happy to follow up if you have any further questions.\n\nWe also prudently ask you to reevaluate our work. To highlight, our motivation is supported by the common fine-grained graph shift heterogeneity, and the fact that most of the previous works could not model such nuanced distribution shifts in an effective and flexible manner. Moreover, we added more related works and modified our statement to position our work better. In general, we believe GraphMETRO is a more general and flexible solution that can mitigate a wider range of distribution shifts, which is backed by the experimental results. Detailed justification on our assumption and implementation details are also available in our paper. \n\nThus, we believe our work makes important contributions and provides a clear presentation. We are happy to discuss more and revise our paper if any concern remains. Thank you for your efforts and we are looking forward to your reply!!\n\n---\n\nWe appreciate your comments! To address your concerns, below we prudently justify the motivation of our proposed method, clarify our assumptions, and provide details regarding our experiment implementation.\n\n---------------------\n\n### **Comment 1: Motivation of this work**\n\nThank you for this comment! We believe there might be a bit of misunderstanding due to our different definitions of *“graph shift heterogeneity”*. We firstly discuss the related works mentioned and then justify our statement:\n\n**(1) Related works**\n\nPlease see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nMoreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you! \n\nSpecifically, Sui et al. [1] proposed a graph data augmentation strategy that alleviates covariate shift by generating diverse and invariant causal features. However, the trainable augmenter they used may not distill diverse augmentations or construct unseen perturbations. Moreover, Sui et al. [1] test its method only on graph classification tasks, while GraphMETRO can be applied to both node and graph classification tasks. Besides, we have discussed graph augmentation and attention-based methods in our related works, and we added more recent works on graph OOD [2,3,4], and we hope our response clears your concern on the related work discussion.\n\n\n**(2) The definition of graph shift heterogeneity**\n\nIn this work, we refer to **“heterogeneous shifts”** as multiple and different levels of shifts which vary across different instances (nodes or graphs), as illustrated in the example in the abstract. While we agree that the existing invariant learning approches can accommodate multiple distribution shifts, it could be hard for them to tackle nuanced distribution shifts for individual instances (nodes or graphs) since the distribution shifts are inferred from variance across multiple data environments. If GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering **a mixture of transformations as a proxy** for the target distribution shifts rather than invariance to whole-graph environment shifts. This is the type of heterogeneity we are interested in our paper. \n\n**(3) Regarding our original motivation statement**\n\nWhile the statement pointed out by the reviewer serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category. And we have provided detailed discussion about three lines of research in the related work section.\n\nHowever, we agree that we could make this statement border to cover the previous invariant learning methods. To improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to \"**previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments…**”. We also modified our introduction correspondingly, we hope this will better position our work.\n\n**(4) The choice of our MoE design**\n\n- The choice of our approach comes as a consequence of our motivation to model the graph/instance shift heterogeneity. As mentioned, mitigating multiple and nuanced distribution shifts simply goes beyond certain distribution shift types or environment construction as seen in the previous methods. \n- Thus, GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts. This enables the prediction of multiple different distribution shifts and the flexibility to model fine-grained heterogeneity since the mixture can be varied across different instances. We then tackled the proxy to mitigate the target distribution shifts. Intuitively, this solution provides a **“middle ground”** to deem graph generalization as an equivalent mixture, which, we believe, is a more tractable solution.\n\nWe updated our paper to make the above point more clear. We genuinely hope our answer can justify the motivation and solve your concern.\n\n---\n\n### **Question 2: How does the computational cost of GraphMETRO compare to other methods?**\n\nPlease see the last paragraph of Section 3.4, where we analyze the computation complexity of GraphMETRO. We repeat part of it here for your convenience:\n\n```Consider the scenario where we use an individual encoder for each expert. The forward process of $f$ involves $O(K)$ forward times using the weighted sum aggregation (or $O(1)$ if using the maximum selection). Since we extend the dataset to $(K + 1)$ times larger than the original data, the computation complexity is $O(K^2 |D_s|)$, where |Ds| is the size of the source dataset.```\n\nThus, the computation cost is about $K^2$ or $K$ times (if using the maximum selection) than an ERM model, where $K=5$ in our experiments. Compared to DIR, as they extract $B$ spurious subgraphs from each batch to conduct the intervention, their computation cost is $B$ times compared to ERM, where $B$ could be 32. Thus, we believe the computation cost of GraphMETRO is fair for the gains we get, considering $K$ is usually small.\n\n----------------------\n\n### **Question 3: Can GraphMETRO handle multiple different types of distribution shifts that simultaneously exist in data?**\n\nYes! The distribution shift types corresponding to the gating outputs with high scores will be tackled during training. That is, if the gating output highlights multiple mixture components, their corresponding distribution shift types will be handled jointly.\n\n----------------------\n\n### **Question 4: Can GraphMETRO  tackle distribution shifts on molecular graphs?**\n\nThat is a great idea! Yes, GraphMETRO can be applied to molecular datasets if one designs transform functions to cover typical molecular variants. For instance, a transform function may add carbon structures to the molecules. These domain-specific transform functions are outside the scope of our work, however, we believe these would be interesting future work directions!\n\n----------------------\n\n# Summary \n\nWe thank the reviewer for the time and insightful suggestions! We hope our answers can address your concerns well.\n\nWe also prudently ask you to reconsider our work if the concerns are addressed. To highlight, our novelty comes from the formulation of an equivalent mixture for graph OOD and the training framework to effectively realize generalization. We also provide an in-depth analysis on our originality compared to some previous invariant learning methods. While we discussed and compared with previous works, we added more related works and modified our statement to position our work better. Finally, our method achieves great improvements on both node and graph classification tasks, and is a more general solution to mitigate multiple and nuanced distribution shifts. \n\nOverall, we believe our work proposes a new paradigm and novel training framework and makes good contributions in the fields of graph generalization, and we would appreciate your reconsideration on this point. Thank you for your efforts again!\n\n----------------------\n\n**Reference**\n\n\n[1-5] The same as listed by the reviewer\n\n\n[6] Beatrice Bevilacqua, Yangze Zhou, and Bruno Ribeiro. Size-invariant graph representations for graph classification extrapolations. In ICML, 2021.\n\n[7] Davide Buffelli, Pietro Li´o, and Fabio Vandin. Sizeshiftreg: a regularization method for improving size-generalization in graph neural networks. In NeurIPS, 2022.\n\n[8] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In NeurIPS, 2019.\n\n[9] Mucong Ding, Kezhi Kong, Jiuhai Chen, John Kirchenbauer, Micah Goldblum, David Wipf, Furong Huang, and Tom Goldstein. A closer look at distribution shifts and out-of-distribution generalization on graphs. In NeurIPS DistShift, 2021.\n\n---\n\n### **Comment 3 & Question 1: Related works on invariant learning and clarification on our statement.**\n\nThanks for pointing it out! Below we clarify the statement and clear potential misunderstanding:\n\n**(1) How does GraphMETRO compare with invariant learning methods like DIR and EERM?** \n\nPlease see our related work section where we discussed these two papers, i.e., DIR (Wu et al., 2022c) and EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on environmental patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nBesides, we also provide a more in-depth comparison in our point (3) below to highlight our key technical originality.\n\n**(2) Regarding our statement about previous works:** \n\nWhile the statement serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category, and we have provided detailed discussion about three lines of research in the related work section.\n\nTo improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to **\"previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments** (which is highly limited when confronted with nuanced distribution shifts)”. We also modified our introduction correspondingly. Thanks for letting us know our statement could be misinterpreted.\n\n**(3) Why do we say our method could be more broad than the existing invariant learning approaches?** \n\n- Invariant subgraph learning approaches, e.g., [1,2], consider variance of constructed data environments, which are designed very differently compared to our work. \nWhile they can accommodate multiple distribution shifts (as in multiple environments), these focus on patterns within each environment and ignore the variety across instances (e.g., shifts at the resolution of nodes), which may not be well-captured by the environment assignments. \n- GraphMETRO considers that specific parts of the test graph may have different shifts. Particularly, our goal is to make the generalization to unknown testing distribution more adaptive and broad, as opposed to limiting the distribution shifts to being invariant to specific types of subgraphs. \n\nIn other words, if GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering a mixture of transformations as a proxy for the target distribution shifts rather than invariance to environment shifts.\n\n**(4) Key technical originality compared to invariance learning (going more deeply)**\n\nAnother interesting view to see the innovation of GraphMETRO is that it breaks the typical invariant learning formulation, which assumes the data is manipulated by the environment variables (and then can be “decoded” into multiple environments). Instead, GraphMETRO sees the distribution shifts on an instance as a mixture, which is represented by the score vector output by the gating function over the basis of the transform functions. In other words, GraphMETRO can produce infinite environments as the elements in the score vector are continuous. One can see that once we limit the output domain of the gating function into, e.g., binary {0, 1}, GraphMETRO can also produce a limited number of environments (if we categorize the instances based on the score vector), which covers the environment construction in invariant learning. Moreover, as mentioned, we propose the concept of referential invariant representation with a base model $\\xi_0$, which is also different from previous works on invariant learning. We added the above discussion to Appendix F to improve the depth of our analysis.\n\n---\n\nWe appreciate your efforts and insightful comments! To address your concerns, we provide point-to-point responses below.\n\n---------------------\n\n### **Comment 1: Regarding the novelty of GraphMETRO.**\n\nThanks for the comment! We believe our novelty comes from the proposal of an equivalent mixture for graph OOD and the construction of our training framework, as detailed below:\n\n- **An equivalent mixture for graph OOD**: The key challenge we faced to mitigate multiple and nuanced distribution shifts is the intrinsic complexity and heterogeneity of graph distribution shifts, which simply goes beyond certain distribution shift types [6,7,8,9] or environment construction as seen in the previous methods [1,2,3]. GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts, where the mixture can be varied across different instances, and then tackled the proxy to mitigate the target distribution shifts. We believe the high-level idea is succinct, nevertheless, novel, in the sense that it provides a “middle ground” to deem graph generalization as an equivalent mixture that is more tractable.\n\n- **Training framework**: With the guide of our formulation, the training framework is still non-trivial due to two problems, i.e., “how to provide supervision for predicting the mixture” and “how to ensure the experts corresponding to mixture components are compatible when working as a whole”. Specifically, GraphMETRO solves the first problem by conducting graph extrapolation. This is somewhat similar to the spirit of graph pretraining in the sense that we inject heterogeneity to promote the expressiveness of the gating model in recognizing the mixture components. For the second problem, we introduce the concept of Referential Invariant Representation, along with the novel objective in Eq. (3) to enforce the invariance and compatibility. It is worth mentioning that the model performance is much worse than the reported numbers (e.g., 2.7\\% lower on Twitch dataset) without the compatibility constraint, indicating the proposed referential invariance concept is indispensable. \n\nWe added more justification in the introduction (updated in the revision). We genuinely hope our responses can solve your concerns about the novelty of our work.\n\n------------------------------\n\n### **Comment 2: Comparison with recent methods [3,4,5].**\n\nIn compacting the paper to fit in the page limit we mistakenly did not include these relevant references, we apologize. We added discussion in the revised version. Here we summarize these works and point out their key differences with our method: \n\n- In particular, **Yang et al. [3]** explore molecule representation learning in out-of-distribution (OOD) scenarios. They achieve this by directing the molecule encoder to utilize stable and environment-invariant substructures relevant to the labels without the need for environmental labels. \n- Similarly, **Sui et al. [4]** introduces causal attention modules to identify key invariant subgraph features that can be described as causing the graph label. The type of OOD task that Sui et al. [4] considers assumes the graph label is caused by a subgraph, which is quite different from ours. Moreover, both Yang et al. [3] and Sui et al. [4] consider tasks where the graph label is caused by a subgraph. \n- **Ma et al.[5]** is an interesting theoretical work which studies GNN generalization and examines their fairness, showing that the test subgroup's distance from the training set impacts GNN performance. Ma et al.[5], as far as we could assess, does not propose any specific architecture to solve the type of OOD tasks we consider in our work.\n\nOverall, the goal of GraphMETRO is to be invariant to a mixture of selected stochastic transform functions (and the mixture can vary across different instances), which is a more flexible and general solution. \nWe added a discussion of these works to our revision, hopefully providing a more comprehensive comparison and literature overview. We hope our responses can solve your concerns about the related work.\n\n---\n\nWe sincerely appreciate all reviewers' time, efforts, and valuable suggestions in reviewing our paper. We are glad that most of the reviewers reached a positive consensus on our work's motivation and experimental results. Here is a summary of our responses:\n\n- **Clarification**: We clarify our key assumptions (`m8JS, FURX`), novelty (`m8JS, NLg1`), and the experimental settings (`m8JS`).\n\n- **Related work**: : We add more related works on recent graph OOD learning (`NLg1, m8JS`) and graph transfer learning (`FURX`)\n\n- **Limitation**: We add discussion about the limitations of GraphMETRO when confronted with unknown distribution shifts that surpass our key assumptions (`FURX`, `C9mM`).\n\nWe hope our responses can clarify your confusion and alleviate concerns and we updated our paper **(highlighted in green)** corespondingly. We thank all reviewers again, and look forward to your reply!", "author_response": "We appreciate your comments! To address your concerns, below we prudently justify the motivation of our proposed method, clarify our assumptions, and provide details regarding our experiment implementation.\n\n---------------------\n\n### **Comment 1: Motivation of this work**\n\nThank you for this comment! We believe there might be a bit of misunderstanding due to our different definitions of *“graph shift heterogeneity”*. We firstly discuss the related works mentioned and then justify our statement:\n\n**(1) Related works**\n\nPlease see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nMoreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you! \n\nSpecifically, Sui et al. [1] proposed a graph data augmentation strategy that alleviates covariate shift by generating diverse and invariant causal features. However, the trainable augmenter they used may not distill diverse augmentations or construct unseen perturbations. Moreover, Sui et al. [1] test its method only on graph classification tasks, while GraphMETRO can be applied to both node and graph classification tasks. Besides, we have discussed graph augmentation and attention-based methods in our related works, and we added more recent works on graph OOD [2,3,4], and we hope our response clears your concern on the related work discussion.\n\n\n**(2) The definition of graph shift heterogeneity**\n\nIn this work, we refer to **“heterogeneous shifts”** as multiple and different levels of shifts which vary across different instances (nodes or graphs), as illustrated in the example in the abstract. While we agree that the existing invariant learning approches can accommodate multiple distribution shifts, it could be hard for them to tackle nuanced distribution shifts for individual instances (nodes or graphs) since the distribution shifts are inferred from variance across multiple data environments. If GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering **a mixture of transformations as a proxy** for the target distribution shifts rather than invariance to whole-graph environment shifts. This is the type of heterogeneity we are interested in our paper. \n\n**(3) Regarding our original motivation statement**\n\nWhile the statement pointed out by the reviewer serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category. And we have provided detailed discussion about three lines of research in the related work section.\n\nHowever, we agree that we could make this statement border to cover the previous invariant learning methods. To improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to \"**previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments…**”. We also modified our introduction correspondingly, we hope this will better position our work.\n\n**(4) The choice of our MoE design**\n\n- The choice of our approach comes as a consequence of our motivation to model the graph/instance shift heterogeneity. As mentioned, mitigating multiple and nuanced distribution shifts simply goes beyond certain distribution shift types or environment construction as seen in the previous methods. \n- Thus, GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts. This enables the prediction of multiple different distribution shifts and the flexibility to model fine-grained heterogeneity since the mixture can be varied across different instances. We then tackled the proxy to mitigate the target distribution shifts. Intuitively, this solution provides a **“middle ground”** to deem graph generalization as an equivalent mixture, which, we believe, is a more tractable solution.\n\nWe updated our paper to make the above point more clear. We genuinely hope our answer can justify the motivation and solve your concern."}
{"claim": "It is challenging to discern the model's functionality, underlying mechanism, or how it improves upon existing methods from the paper.", "claim_type": "presentation", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ppMpJyIYRZ", "reviewer": "Reviewer_m8JS", "review_text": "Summary: The paper introduces a method to enhance the out-of-distribution performance of graph neural networks (GNN) by learning to understand distribution shifts instead of addressing the assumed ones. To achieve this, the Mixture of Experts architecture is integrated into the GNN, supplemented by an alignment procedure to recognize the shift. Empirical experiments are conducted to validate the theoretical assertion.\n\nOn the whole, I believe the proposed method lacks the necessary motivation and its novelty isn't substantial enough to meet the standard.\n\nStrengths: - The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\n- Real-world datasets back the claims through experiments.\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\n\nWeaknesses: - The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \"previous research has concentrated on addressing specific types of distribution shifts.\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \"environment generators\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\n\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\n\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\n\nQuestions: Please check Weaknesses.", "labeling_timestamp": "2026-01-11T16:31:34.420819", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors acknowledge shortcomings in presentation and explicitly provide detailed explanations of the model's functionality and mechanism (pointing to sections, appendices, experiments), offer to add clarifying material (e.g., a causal graph), and describe how their method improves over baselines—therefore they address the reviewer’s concern.", "evidence": "\"We do apologize for not making our solutions towards the limitations clear enough.\" \"For clarity, we provide the following pointers to the paper: - In Section 4.3, we provide a study to reveal the underlying **mechanism** of GraphMETRO, i.e., each expert excels in generating invariant representations concerning a stochastic transform function...\"", "combined_author_response": "Dear reviewer NLg1,\n\nAs the discussion period is closing soon, we hope to engage with you and check if your concerns are addressed.\n\nWe sincerely appreciate your time and attention!\n\nBest,\n\nAuthors of Paper 1560\n\n---\n\nThanks for reading our response! We would also appreciate your patience for reading the two points below \n\n---\n\n**Regarding point #1:**\n\nYes, we agree that the concept of heterogeneous shifts is relatively new for the current studies on distribution shifts, however, this is not new for studies on network patterns (Newman, 2003; Leskovec et al., 2005; 2007; Peel et al., 2017). In fact, the ignorance of such nuanced heterogeneous shifts in the previous studies instead emphasises our motivation and the potential impact of this work. \n\nFrom a causality perspective, distribution shifts can naturally happen in the instance level when (1) additional causal variables, beyond environmental factors and randomized noise, influence these shifts, or (2) multiple causal variables simultaneously affect the shifts with different strengths. Without modeling these explicitly, the mitigation of distribution shifts can easily fail. \n\nIn terms of our presentation, we have illustrated these cases of interest in abstract and introduction, we made further explanations in Section 3.1, we also present the specific results of distribution types in Figure 3 (b). \n\n\n**Regarding point #2:**\n\nWe think we can all agree that, if the ground truth of the instance shifts is available on the real-world datasets, it would be crystal clear to see where the improvement comes from since we can conduct case study to compare our method and the baselines on instances with nuanced distribution shifts, to see the influence of the modeling these heterogeneous shifts. \n\nHowever, with such ground truth not available, we had to seek other seemingly less intuitive but also in-depth way to illustrate the insights (esp Section 4.3) as mentioned above in our previous response. This is also why we designed the synthetic experiments at the first place. We think we did try hard to explain the underlying rationale with the ground truth being absent.\n\n----\n\n**Refinement**: We can of course add a causal graph in the our assumption section to make the concept more clear. And we can illustrate more if you could let us know the specific obstable to understand our mechanism, which will be extremely helpful.\n\n----\n\n### **Summary/TL;DR**\nWe understand the reviewer's clarification concerns, however, we don't agree that they, based on our justificaton, are the cause of rejection. We believe nuanced heterogeneous shifts are common, important, yet being typically ignored in the research domain of distribution shifts, we made these argument clear and also promise to refine. While explaining the underlying rationale is hindered by the lack of ground truth, we did try hard to dissection it from the gating model, the invariant representations generated, and more ablations in the appendix. We respect the reviewer's current opinion. Still, reconsideration will be greatly appreciated.\n\n---\n\nWe thank the reviewer once again! We also enjoyed the process of making our work more sound from your suggestions. And your approval surely means a lot to us.\n\nWe do apologize for not making our solutions towards the limitations clear enough. We summarize them into a short table, hopefully could alleviate your concerns a bit more.\n\n| Limitation| Potential solutions | Location of discussion/action |\n|:---|:---|:---|\n|**Coverage of the transformations** | 1) Extend the coverage by adding representative transform functions. 2) Or include tranform functions based on domain knowledge or a few samples from target distribution | Appendix F, paragraph #3|\n|**Complexity of the transformations** | Make experts dedicated to different hyperparameters for the same type of transformation | Argument options are available in our codebase. Will make it more detailed in the experimental settings. |\n|**Label distributional shift**|Integrate the objective of the existing methods studying labe distributional shifts into our framework. |Appendix F, paragraph #4 |\n\n----\n\nWe completely agree on the existence of these limitations and we will move some limitations (esp #1) to the main paper in our final version. While they could be important in the practice, they are, in our perspective, fair \"side effects\" considering the benefits (i.e., mitigating multiple and nuanced distribution shifts and better interpretability), and may not be the central part of our novelty and main contribution (i.e., the proposal of an equivalent mixture, the concept of referential invariant representations, as well as the training framework). We are also eager to further improve other aspects.\n\nOnce again thank you so much for your support!!\n\n---\n\nDear Reviewer m8JS,\n\nA gentle nudge that we would like to know if our response adequately addresses your concerns. \n\nYour time and feedback is greatly appreciated!\n\nThank you sincerely,\n\nAuthors of Paper 1560\n\n---\n\nWe deeply appreciate your approval. Your suggestions definitely inspired us a lot and have greatly improved our work.\n\nA few additional notes for the further comments:\n\n- Regarding **point #2**: In node classification tasks, the BCE objective already considers node embeddings other than the graph embedding of the k-hop subgraph. What we described previously refers to the computational graph (i.e., k-hop message passing), which generates node embeddings. We apologize for any confusion.\n\n- Regarding **point #3**: Yes it would be quite interesting to see how graph pretraining methods perform on the current OOD benchmarks. Graph pretraining methods like GCC and GPT-GNN also consider graph extrapolation to some extent, e.g., through subgraph extraction and masked attributes/structures. The key difference between graph pretraining and generalization may lie in their different focuses on **expressiveness** and **invariance**. While these two aspects do not always conflict, ensuring invariance w.r.t. a certain type of extrapolation might affect expressiveness (if the change is relevant to labels), and vice versa. To seek a balance in between, one might need prior knowledge in which types of transformations may or may not be sensitive to the labels (and perhaps build experts with different goals to make ensure invariance or expressivess). We believe there is still a lot to explore in this domain.\n\nOverall, we are grateful for your positive stand on this work. We believe that our [current version](https://openreview.net/pdf?id=QQ5eVDIMu4), incorporating opinions from you and other reviewers, is sound and well-refined. We are also committed to further improving it. We would appreciate your support based on our current version! \n\nThank you!\n\n---\n\nDear Reviewers of Paper 1560,\n\nWe hope this message finds you well.\n\nAs the discussion phase approaches its end, we hope you find our responses useful. We would like to ask if the issues have been addressed.\n\nWe understand that the discussion time is short, and some of you might be enjoying holidays at the moment. We apologize for posting our responses a bit late as we aimed to address your concerns clearly.\n\nWe sincerely appreciate your time and attention!\n\nBest regards,\n\nAuthors of Paper 1560\n\n---\n\n**Comment 4: Related works on graph transfer learning.**\n\nYes! We agree that graph transfer learning is a relevant topic. Thank you for providing the additional related work! We added the discussion on these works in our revision, and we repeat it here for your convenience:\n\n```It is also worth mentioning that graph domain adaptation (Zhang et al., 2019; Wu et al., 2020), different from the problem studied in this work, commonly relies on limited labeled samples from target datasets for improved transferability. For instance, to generate domain adaptive network embedding, DANE (Zhang et al., 2019) uses shared weight graph convolutional networks and adversarial learning regularization, while UDA-GCN (Wu et al., 2020) employs attention mechanisms to merge global and local consistencies.```\n\nMoreover, we believe this is actually relevant to `Comment & Question 1` regarding the generality of the distribution shifts covered by the transform functions. In our future works, we added the following discussion:\n\n```...Leveraging a few samples from target distribution (i.e., domain adaptation). Specifically, we can leverage the samples from the target distribution to inform the selection or construction of transform functions, which can better guarantee the distribution shifts are covered by the transform functions.  For example, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space. We believe this would be an interesting future direction.```\n\n---\n\n# Summary\n\nOnce again thank you very much for helping us position our work and reflect the literature better. We hope all of your concerns are solved and we are happy to engage further if there are any other points we missed!\n\n---\n\n### **Reference**\n\n[1] Unsupervised Domain Adaptive Graph Convolutional Networks. Wu et al. WWW 2020\n\n[2] DANE: Domain adaptive network embedding. Zhang et al. IJCAI 2019.\n\n[3] Tong Zhao, Yozen Liu, Leonardo Neves, Oliver J. Woodford, Meng Jiang, and Neil Shah. Data augmentation for graph neural networks. AAAI, 2021.\n\n---\n\nWe are grateful for your positive feedback and detailed suggestions! We provide responses below to address your remaining concerns. \n\n---\n\n### **Comment & Question 1: Generality of the distribution shifts covered by the transform functions.**\n\nGood question! We provide the response from two angles:\n\n\n**(a) For general domain**: In our experiments, we mainly use the five stochastic transform functions, which are universal graph augmentations as listed in Zhao et al., (2021) [3]. In our code implementation, we have also included additional transform functions as shown in Appendix B. We believe these transform functions, while not exhaustive, still cover a wide range of distribution shifts observing from our experimental results.\n\nNevertheless, we agree that the real graph distribution shifts can go beyond any possible combinations of the predefined transform functions. In that case, the assumption may not hold, meaning that GraphMETRO may not capture and precisely mitigate the unknown distribution shift. This scenario could always possibly exist due to the lack of information about the testing distribution or its domain knowledge. We include it as a limitation in Appendix 5, while we further discuss how we could alleviate the problem with additional information.\n\n**(b) For specific domains where additional knowledge is available**: In fact, knowing the tendency of the distribution shifts, such as increasing malicious users in a trading system, would be very helpful in constructing the transform functions that can cover the target distribution shifts well. We believe that such knowledge can come from two sources: \n- **Domain knowledge**, e.g., on molecular datasets, the transform function could be adding additional carbon structure to a molecule (while preserving its functional groups). Or, in a particular social network, transform functions can be defined from known user behaviors. \n- **Leveraging a few samples from target distribution**. This is in fact in line with the reviewer’s Comment #4 regarding graph transfer learning. Specifically, with the guide from a few target samples, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space.\n\n We hope this response can alleviate your concern about our applicability.\n\n---\n\n### **Comment & Question 2: The mechanism of GaphMETRO in handling heterogeneous shifts within one graph.**\n\nGood question! For simplicity, suppose we have two transform functions, i.e., adding edges and dropping edges. Given a node classification task and the objective in Eq. (3), the model is trained on the extrapolated datasets based on the transform functions. After that, given an unseen graph with imbalanced distribution shifts, the gating model outputs scores to identify nodes likely to experience increasing or decreasing degrees, while others might adhere to the original distribution. Then, for each node, each expert takes its multihop subgraph, and outputs its referential invariant representation w.r.t. the correposing transform function. These expert outputs and gating model results are then aggregated to form the final representation. Assuming accurate predictions by the gating function, nodes in denser/sparser subgraphs are represented by expert models corresponding to adding/dropping edges. As each expert is trained to create invariant representations, the final node representations remain unaffected by their individual distribution shifts.\n\n---\n\n### **Comment 3: How to leverage graph pretraining.**\n\nIf we understand correctly, the reviewer was asking how to leverage a pretrained model to further aid the training of GraphMETRO. Please let us know if otherwise. \n\nThis is in fact an interesting point! We believe a model pretrained on a wide variety of data can be very helpful to initialize the gating model, which is required to output the mixture of a node or graph (after it is finetuned on the extrapolated dataset). By enhancing the gating model's predictive capability regarding mixtures, GraphMETRO's final representation should become more resilient. This becomes particularly advantageous when dealing with graphs not previously encountered in the extrapolated dataset. Moreover, it is possible that the pretrained model will also benefit the expert models, while one minor concern would be that the expert model may tend to be similar instead of dedicating to generating invariant representation w.r.t. their corresponding transform function. We added the above discussion to Appendix F as a future work. Thanks again for this comment!\n\n---\n\n### **Question 4: Why does the test accuracy vary across different transformations?**\n\nGreat question! Here we summarize three possible reasons:\n\n- **Information Preservation in Transformations:** Certain transformations retain more informative features than others. For instance, in the REDDIT-BINARY (graph classification task), the random subgraph transformation may retain more graph label-related information compared to dropping edges, as the latter tends to lose more global information. This discrepancy in testing performance, where dropping edges outperforms random subgraph extraction, could be due to the preservation of crucial information. However, conclusions may vary across datasets or tasks depending on how information influences final predictions. For CiteSeer (a node classification task), a random subgraph might preserve more local node information, potentially explaining why its testing performance surpasses dropping edges in this specific task.\n\n- **Complexity of transformation:** Certain transformations inherently generate more diverse graphs than others. If the model lacks the expressive capacity to capture such diversity, it may lead to a decline in testing performance.\n\n- **Model Sensitivity:** Certain transformations may be easier for a model to learn due to compatibility with specific model architectures. This extends beyond transformation complexity and emphasizes how different model architectures may prefer learning particular distributions from one of the extrapolated datasets, which can also contribute to the difference in the testing performance.\n\nWe included the above discussion to Appendix F: Open Discussions. We hope this response can answer your question and improve the soundness of our work.\n\n---\n\n# Summary \n\nWe are grateful for your time and insightful suggestions! \n\nWe would like to highlight that our main contribution is framing the graph generalization problem on top of an equivalent mixture, a simple yet novel and tractable \"middle ground\", as well as proposing the training framework which effectively guarantees the generalization. While our method relies on a set of predefined transform functions, we believe they cover a wide range of distribution shifts based on our empirical results. Also, we agree that there could be some scenarios where the transform functions may not cover complex distributions, and we discuss two future directions and include them into our future works. Moreover, while selecting the hyperparameters for the transform functions introduce extra complexity, the issue could be minor in practice and we also conduct more experiments to justify the applicability better. Finally, we address several questions about clarification and presentation, as well as including more future works. \n\nLastly, we prudently ask you to reevaluate our work given the clarification in our responses, which we also updated our paper correspondingly. Overall, we believe our work makes good contributions to the field of graph distribution learning by proposing a novel and effective solution, and we would appreciate your reconsideration on this point. Thank you for your efforts again!\n\n---\n\n### **Reference**\n\n[1] Menon, Aditya Krishna, et al. \"Long-tail learning via logit adjustment.\" International Conference on Learning Representations. 2020.\n\n[2] Cao, Kaidi, et al. \"Learning imbalanced datasets with label-distribution-aware margin loss.\" Advances in neural information processing systems 32 (2019).\n\n[3] DARTS: Differentiable Architecture Search. Hanxiao Liu, Karen Simonyan, Yiming Yang. 2018.\n\n---\n\n**(3) What is the complexity of the transform functions and how does it affect generalization?** \n\nInteresting question! In fact, our implementation and framework could easily avoid selecting hyperparameters on the transform functions. Specifically, we can make multiple transform functions of the same type with different ranges of hyperparameters. Specifically, GraphMETRO allows three edge dropping transform functions, $\\tau_1^{\\alpha_1}, \\tau_2^{\\alpha_2}, \\tau_3^{\\alpha_3}$, where $\\alpha_i$ (i=1, 2, 3) are three different ranges of edge dropping probabilities, e.g., [0.1,0.3], [0.3, 0.6], [0.6,0.9], representing different transform extents. Thus, given an input from the validation dataset, the gating model will highlight the transform function which simultaneously selects the corresponding hyperparameter that matches the distribution of the validation set. Interestingly, this idea is in the same spirit as how DARTS [3]  proposes to perform architecture search by formulating the task in a differentiable manner. \n\nIn our previous experiments, we did try this scheme where we replaced a single edge dropping transform function with the ratio range [0.3, 0.5] to three transform functions as mentioned above. While we didn’t see a significantly different performance in that case, we believe this would be a flexible solution which avoids the need to conduct hyperparameter selection. \n\nWe include the above discussion in our open discussion and future works (Appendix F). We hope this response can alleviate your concern about the applicability of our method. \n\n---\n\n### **Comment 4: Limitations of our work**\n\nGreat point! We think the issue of label distributional shift, while important, is orthogonal and complementary to the focus of our current study. To elaborate, label distributional shifts exert analogous impacts across various modalities, such as graphs or images. Moreover, existing methods [1,2] designed to tackle label distributional shifts can be seamlessly integrated into our proposed framework. Such integration would necessitate minimal adjustments, potentially involving modifications to the loss function or the training pipeline. We added this as a future work in Appendix F.\n\n---\n\n### **Question 1: Statistical significance of the results on Table 1**\n\nThanks for the question! We compute the p-value of our method against the best baselines method as follows:\n\n| | WebKB | Twitch | Twitter | SST2 | \n|:--|:--|:--|:--|:--|\n|p-value|< 0.001| 0.023 | 0.042| 0.081|\n\nGiven the cut-off threshold as 0.05, we believe the performances of GraphMETRO are statistically significant on WekGB, Twitch, and Twitter datasets, while on the SST2, we see relatively weak evidence. We added the p-value results to our revision and hope our response can alleviate your concern on our improvements.\n\n---\n\n###  **Question 2: Can we say we use ERM for the node classification even if nodes and their labels are not i.i.d.?** \n\nIf we understand correctly, the reviewer is asking for clarification on the 2nd term of our objective. Please let us know if otherwise. Here, our thinking is that the cross-entropy loss for node classification already assumes node labels are conditionally independent given the model (the negative log-likelihood is a sum over the labeled nodes in training). Then, we use the same assumption of cross-entropy on Empirical Risk Minimization (ERM). That is, for a given model we must also minimize the error variance across nodes. The task is then to find the model with the best performance and small variance.\n\n---\n\n### **Question 3: Numerical results on Figure 2**\n\nThanks for pointing it out! We included all of the numerical results of Figure 2 in Appendix E, while showing the results on DBLP below.\n\n| |i.i.d. (0)|noisy feature (1)|add edge (2)|drop edge (3)|drop node (4)|random subgraph (5)|\n|:--|:--|:--|:--|:--|:--|:--|\n|ERM | 85.71 | 84.48 | 71.08 | 79.69 | 83.41 | 76.9|\n|ERM-Aug | 85.66 | 85.29 | 74.85 | 82.34 | 84.44 | 72.81|\n|GraphMETRO | 85.92 | 85.78 | 76.61 | 82.95 | 84.98 | 81.32|\n\n| |(4, 5)|(3, 5)|(2, 5)|(1, 5)|(2, 4)|(1, 4)|     (2, 3)|(1, 3)|\n|:--|:--|:--|:--|:--|:--|:--|:--|:--|\n|ERM | 70.4 | 77.63 | 81.99 | 79.69 | 70.55 | 71.52 | 77.73 | 79.59|\n|ERM-Aug | 74.16 | 81.04 | 83.65 | 68.62 | 74.01 | 68.27 | 81.13 | 84.49|\n|GraphMETRO | 76.18 | 81.71 | 84.26 | 80.31 | 75.1 | 71.05 | 81.85 | 87.14|\n\nAcross all of the synthetic environments, GraphMETRO averagely outperforms ERM and ERM-Aug by 3.20% and 2.45%, respectively.\n\n---\n\nWe appreciate your comments! To address your concerns, below we prudently justify the assumption of our method, the predefined transformation functions, as well as their complexity, and clarify our presented results.\n\n---\n\n### **Comment 1-3: Applicability and our assumptions**\n\nThanks for these great comments! Here we provide response in three folds:\n\n**(1) How does GraphMETRO identify all distribution shifts from transform functions if they are treated independently when combined together?** \n\nIf we understand correctly, by “treated independently”, the reviewer is referring to the first term in our objective $\\text{BCE}(\\phi(\\tau^{(k)}(\\mathcal{G})), Y (\\tau^{(k)}))$, where we formulate predicting the distribution shifts types of a jointly transformed graph as a binary multiclass classification problem. We believe the difficulty of this task comes from both the property of transform functions and the expressiveness of the gating model. \n\n- Firstly, some transform functions are **inherently disentangled**, e.g., adding nodes feature noise and random subgraph extraction. In this case, there will be certain distinction between any pair from these three data distributions, i.e., (graphs with node noise, random subgraph graphs, random subgraphs with node noise), which the gating model can easily tell. \n- While some transform functions can be **essentially similar**, e.g., drop path and drop edges, this won’t affect the performance of our method as long as each expert outputs the corresponding invariant representation. \n- Lastly, indeed, there could be more **complex combinations of the transform functions**, which poses challenges to the gating model’s expressiveness in identifying the combinations. However, this challenge may be minor in the practice. Specifically, we observe fairly high accuracy performances of the gating model, which are above 85% and 73% averagely on extrapolated datasets with one transformation and multiple transformations, respectively.\n\nWe added the above discussion to Appendix F to enable a more comprehensive view towards our methodology. We hope this can alleviate your concern on our gating model’s performance in identifying the distribution shift types.\n\n**(2) How does the predefined transform functions cover complex distributions causing the distribution shift?**\n\nThis is also a great question! We believe there are two angles for this question. \n\n**(a) For general domain**: In our experiments, we mainly use the five stochastic transform functions, which are universal graph augmentations as listed in Zhao et al., (2021) [3]. In our code implementation, we have also included additional transform functions as shown in Appendix B. We believe these transform functions, while not exhaustive, still cover a wide range of distribution shifts observing from our experimental results.\n\nNevertheless, we agree that the real graph distribution shifts can go beyond any possible combinations of the predefined transform functions. In that case, the assumption may not hold, meaning that GraphMETRO may not capture and precisely mitigate the unknown distribution shift. This scenario could always possibly exist due to the lack of information about the testing distribution or its domain knowledge. We include it as a limitation in Appendix 5, while we further discuss how we could alleviate the problem with additional information.\n\n**(b) For specific domains where additional knowledge is available**: In fact, knowing the tendency of the distribution shifts, such as increasing malicious users in a trading system, would be very helpful in constructing the transform functions that can cover the target distribution shifts well. We believe that such knowledge can come from two sources: \n- **Domain knowledge**, e.g., on molecular datasets, the transform function could be adding additional carbon structure to a molecule (while preserving its functional groups). Or, in a particular social network, transform functions can be defined from known user behaviors. \n- **Leveraging a few samples from target distribution**. Specifically, with the guide from a few target samples, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space.\n\n---\n\n[1] Sui et al. Unleashing the Power of Graph Data Augmentation on Covariate Distribution Shift. NeurIPS 2023.\n\n[2] Nianzu Yang et al. Learning substructure invariance for out-of-distribution molecular representations, NeurIPS 2022\n\n[3] Yongduo Sui et al. Causal Attention for Interpretable and Generalizable Graph Classification, KDD 2022.\n\n[4] Jiaqi Ma et al. Subgroup Generalization and Fairness of Graph Neural Networks, NeurIPS 2022.\n\n---\n\n### **Comment 2: Our assumption**\n\nThanks for this comment! In Section 3.1, we have discussed the assumptions in detail with specifications. We repeat part of it here for your convenience:\n\n```Assumption 1 essentially states that the distribution shifts (whatever they are) can be decomposed into several mixture components of stochastic graph transformations. For example, on a social network dataset, each mixture component can represent different patterns of user behavior or network dynamics shifts. Specifically, one mixture component might correspond to increased user activity, while another could signify a particular trend of interaction within a certain group of users. Such a mixture pattern is common and well-studied in the real-world network datasets (Newman, 2003; Leskovec et al., 2005; 2007; Peel et al., 2017).```\n\nWe hope the analysis provides detailed illustration to the assumption. We are happy to add more discussion if you think anything  is still unclear!\n\n---\n\n### **Comment 2 & 3: Implementation of our method**\n\n- **Model architectures**: \nPlease see Tables 2 and 3 in Appendix A for detailed model architecture information. Specifically, we use the same encoders and classifiers from GOOD benchmarkfor real-world datasets to ensure fair comparisons. We employ backbones based on the best ERM performance for synthetic datasets. Moreover, GraphMETRO is model-agnostic, which consistently improves performance across varied model architectures.\n\n- **Stochastic transformation**: Please see Appendix B where we include the introduction of the stochastic transform functions. \n\n- **Optimization process**: Please see Table 2 and 3 (Appendix A), where we included hyperparameters on each dataset. In the second paragraph after Eq. (3), we also described our training pipeline: `“...we set apart the other loss terms from backpropagating to it to avoid interference with the training of the gating model… We optimize the objective via stochastic gradient descent”`. \n\n- **For a more intricate optimization strategy:** This is a great catch! In fact, we did try to pretrain the gating model for several epochs as warm up before training the whole model in an end-to-end fashion via the objective Eq. in (3). However, we didn’t notice a statistically significant difference in their performance, which can be due to that expert models take more time to convergence (since they need to align with the base model during the training) compared to the gating model.\n\nDue to the space limitation, we had to include most of the implementation details in the appendix while we added pointers in the main paper. However, we will try to make it more detailed and feel free to let us know if anything is missing! \n\n---\n\n### **Comment 2 (Cont.): Understand the functionality, underlying mechanism, and performance gain**\n\nThanks for the comment. For clarity, we provide the following pointers to the paper:\n\n- In Section 4.3, we provide a study to reveal the underlying **mechanism** of GraphMETRO, i.e., each expert excels in generating invariant representations concerning a stochastic transform function, which provide a solid foundation in generating referential invariant representations w.r.t. the specific transformations and further guarantee the generalization.\n\n- In Appendix C, we study the **impact of the MoE architecture** on model performance, which shows that the model performance may decline if the expressiveness of the expert model decreases. \n\n- In Appendix D, we study the **impact of the stochastic transform function** on model performance, where we also provide a detailed discussion of the modeling mechanism.\n\nWe believe the above studies and discussion in our paper provides an in-depth view, highlighting the roles of our objective, architecture, and stochastic transform function.  Please let us know if any of these perspectives is still unclear, and we can further improve our experimental study.\n\n-----\n\n# Summary\n\nWe hope our answers can address all of the concerns. We are happy to follow up if you have any further questions.\n\nWe also prudently ask you to reevaluate our work. To highlight, our motivation is supported by the common fine-grained graph shift heterogeneity, and the fact that most of the previous works could not model such nuanced distribution shifts in an effective and flexible manner. Moreover, we added more related works and modified our statement to position our work better. In general, we believe GraphMETRO is a more general and flexible solution that can mitigate a wider range of distribution shifts, which is backed by the experimental results. Detailed justification on our assumption and implementation details are also available in our paper. \n\nThus, we believe our work makes important contributions and provides a clear presentation. We are happy to discuss more and revise our paper if any concern remains. Thank you for your efforts and we are looking forward to your reply!!\n\n---\n\nWe appreciate your comments! To address your concerns, below we prudently justify the motivation of our proposed method, clarify our assumptions, and provide details regarding our experiment implementation.\n\n---------------------\n\n### **Comment 1: Motivation of this work**\n\nThank you for this comment! We believe there might be a bit of misunderstanding due to our different definitions of *“graph shift heterogeneity”*. We firstly discuss the related works mentioned and then justify our statement:\n\n**(1) Related works**\n\nPlease see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nMoreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you! \n\nSpecifically, Sui et al. [1] proposed a graph data augmentation strategy that alleviates covariate shift by generating diverse and invariant causal features. However, the trainable augmenter they used may not distill diverse augmentations or construct unseen perturbations. Moreover, Sui et al. [1] test its method only on graph classification tasks, while GraphMETRO can be applied to both node and graph classification tasks. Besides, we have discussed graph augmentation and attention-based methods in our related works, and we added more recent works on graph OOD [2,3,4], and we hope our response clears your concern on the related work discussion.\n\n\n**(2) The definition of graph shift heterogeneity**\n\nIn this work, we refer to **“heterogeneous shifts”** as multiple and different levels of shifts which vary across different instances (nodes or graphs), as illustrated in the example in the abstract. While we agree that the existing invariant learning approches can accommodate multiple distribution shifts, it could be hard for them to tackle nuanced distribution shifts for individual instances (nodes or graphs) since the distribution shifts are inferred from variance across multiple data environments. If GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering **a mixture of transformations as a proxy** for the target distribution shifts rather than invariance to whole-graph environment shifts. This is the type of heterogeneity we are interested in our paper. \n\n**(3) Regarding our original motivation statement**\n\nWhile the statement pointed out by the reviewer serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category. And we have provided detailed discussion about three lines of research in the related work section.\n\nHowever, we agree that we could make this statement border to cover the previous invariant learning methods. To improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to \"**previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments…**”. We also modified our introduction correspondingly, we hope this will better position our work.\n\n**(4) The choice of our MoE design**\n\n- The choice of our approach comes as a consequence of our motivation to model the graph/instance shift heterogeneity. As mentioned, mitigating multiple and nuanced distribution shifts simply goes beyond certain distribution shift types or environment construction as seen in the previous methods. \n- Thus, GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts. This enables the prediction of multiple different distribution shifts and the flexibility to model fine-grained heterogeneity since the mixture can be varied across different instances. We then tackled the proxy to mitigate the target distribution shifts. Intuitively, this solution provides a **“middle ground”** to deem graph generalization as an equivalent mixture, which, we believe, is a more tractable solution.\n\nWe updated our paper to make the above point more clear. We genuinely hope our answer can justify the motivation and solve your concern.\n\n---\n\n### **Question 2: How does the computational cost of GraphMETRO compare to other methods?**\n\nPlease see the last paragraph of Section 3.4, where we analyze the computation complexity of GraphMETRO. We repeat part of it here for your convenience:\n\n```Consider the scenario where we use an individual encoder for each expert. The forward process of $f$ involves $O(K)$ forward times using the weighted sum aggregation (or $O(1)$ if using the maximum selection). Since we extend the dataset to $(K + 1)$ times larger than the original data, the computation complexity is $O(K^2 |D_s|)$, where |Ds| is the size of the source dataset.```\n\nThus, the computation cost is about $K^2$ or $K$ times (if using the maximum selection) than an ERM model, where $K=5$ in our experiments. Compared to DIR, as they extract $B$ spurious subgraphs from each batch to conduct the intervention, their computation cost is $B$ times compared to ERM, where $B$ could be 32. Thus, we believe the computation cost of GraphMETRO is fair for the gains we get, considering $K$ is usually small.\n\n----------------------\n\n### **Question 3: Can GraphMETRO handle multiple different types of distribution shifts that simultaneously exist in data?**\n\nYes! The distribution shift types corresponding to the gating outputs with high scores will be tackled during training. That is, if the gating output highlights multiple mixture components, their corresponding distribution shift types will be handled jointly.\n\n----------------------\n\n### **Question 4: Can GraphMETRO  tackle distribution shifts on molecular graphs?**\n\nThat is a great idea! Yes, GraphMETRO can be applied to molecular datasets if one designs transform functions to cover typical molecular variants. For instance, a transform function may add carbon structures to the molecules. These domain-specific transform functions are outside the scope of our work, however, we believe these would be interesting future work directions!\n\n----------------------\n\n# Summary \n\nWe thank the reviewer for the time and insightful suggestions! We hope our answers can address your concerns well.\n\nWe also prudently ask you to reconsider our work if the concerns are addressed. To highlight, our novelty comes from the formulation of an equivalent mixture for graph OOD and the training framework to effectively realize generalization. We also provide an in-depth analysis on our originality compared to some previous invariant learning methods. While we discussed and compared with previous works, we added more related works and modified our statement to position our work better. Finally, our method achieves great improvements on both node and graph classification tasks, and is a more general solution to mitigate multiple and nuanced distribution shifts. \n\nOverall, we believe our work proposes a new paradigm and novel training framework and makes good contributions in the fields of graph generalization, and we would appreciate your reconsideration on this point. Thank you for your efforts again!\n\n----------------------\n\n**Reference**\n\n\n[1-5] The same as listed by the reviewer\n\n\n[6] Beatrice Bevilacqua, Yangze Zhou, and Bruno Ribeiro. Size-invariant graph representations for graph classification extrapolations. In ICML, 2021.\n\n[7] Davide Buffelli, Pietro Li´o, and Fabio Vandin. Sizeshiftreg: a regularization method for improving size-generalization in graph neural networks. In NeurIPS, 2022.\n\n[8] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In NeurIPS, 2019.\n\n[9] Mucong Ding, Kezhi Kong, Jiuhai Chen, John Kirchenbauer, Micah Goldblum, David Wipf, Furong Huang, and Tom Goldstein. A closer look at distribution shifts and out-of-distribution generalization on graphs. In NeurIPS DistShift, 2021.\n\n---\n\n### **Comment 3 & Question 1: Related works on invariant learning and clarification on our statement.**\n\nThanks for pointing it out! Below we clarify the statement and clear potential misunderstanding:\n\n**(1) How does GraphMETRO compare with invariant learning methods like DIR and EERM?** \n\nPlease see our related work section where we discussed these two papers, i.e., DIR (Wu et al., 2022c) and EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on environmental patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nBesides, we also provide a more in-depth comparison in our point (3) below to highlight our key technical originality.\n\n**(2) Regarding our statement about previous works:** \n\nWhile the statement serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category, and we have provided detailed discussion about three lines of research in the related work section.\n\nTo improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to **\"previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments** (which is highly limited when confronted with nuanced distribution shifts)”. We also modified our introduction correspondingly. Thanks for letting us know our statement could be misinterpreted.\n\n**(3) Why do we say our method could be more broad than the existing invariant learning approaches?** \n\n- Invariant subgraph learning approaches, e.g., [1,2], consider variance of constructed data environments, which are designed very differently compared to our work. \nWhile they can accommodate multiple distribution shifts (as in multiple environments), these focus on patterns within each environment and ignore the variety across instances (e.g., shifts at the resolution of nodes), which may not be well-captured by the environment assignments. \n- GraphMETRO considers that specific parts of the test graph may have different shifts. Particularly, our goal is to make the generalization to unknown testing distribution more adaptive and broad, as opposed to limiting the distribution shifts to being invariant to specific types of subgraphs. \n\nIn other words, if GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering a mixture of transformations as a proxy for the target distribution shifts rather than invariance to environment shifts.\n\n**(4) Key technical originality compared to invariance learning (going more deeply)**\n\nAnother interesting view to see the innovation of GraphMETRO is that it breaks the typical invariant learning formulation, which assumes the data is manipulated by the environment variables (and then can be “decoded” into multiple environments). Instead, GraphMETRO sees the distribution shifts on an instance as a mixture, which is represented by the score vector output by the gating function over the basis of the transform functions. In other words, GraphMETRO can produce infinite environments as the elements in the score vector are continuous. One can see that once we limit the output domain of the gating function into, e.g., binary {0, 1}, GraphMETRO can also produce a limited number of environments (if we categorize the instances based on the score vector), which covers the environment construction in invariant learning. Moreover, as mentioned, we propose the concept of referential invariant representation with a base model $\\xi_0$, which is also different from previous works on invariant learning. We added the above discussion to Appendix F to improve the depth of our analysis.\n\n---\n\nWe appreciate your efforts and insightful comments! To address your concerns, we provide point-to-point responses below.\n\n---------------------\n\n### **Comment 1: Regarding the novelty of GraphMETRO.**\n\nThanks for the comment! We believe our novelty comes from the proposal of an equivalent mixture for graph OOD and the construction of our training framework, as detailed below:\n\n- **An equivalent mixture for graph OOD**: The key challenge we faced to mitigate multiple and nuanced distribution shifts is the intrinsic complexity and heterogeneity of graph distribution shifts, which simply goes beyond certain distribution shift types [6,7,8,9] or environment construction as seen in the previous methods [1,2,3]. GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts, where the mixture can be varied across different instances, and then tackled the proxy to mitigate the target distribution shifts. We believe the high-level idea is succinct, nevertheless, novel, in the sense that it provides a “middle ground” to deem graph generalization as an equivalent mixture that is more tractable.\n\n- **Training framework**: With the guide of our formulation, the training framework is still non-trivial due to two problems, i.e., “how to provide supervision for predicting the mixture” and “how to ensure the experts corresponding to mixture components are compatible when working as a whole”. Specifically, GraphMETRO solves the first problem by conducting graph extrapolation. This is somewhat similar to the spirit of graph pretraining in the sense that we inject heterogeneity to promote the expressiveness of the gating model in recognizing the mixture components. For the second problem, we introduce the concept of Referential Invariant Representation, along with the novel objective in Eq. (3) to enforce the invariance and compatibility. It is worth mentioning that the model performance is much worse than the reported numbers (e.g., 2.7\\% lower on Twitch dataset) without the compatibility constraint, indicating the proposed referential invariance concept is indispensable. \n\nWe added more justification in the introduction (updated in the revision). We genuinely hope our responses can solve your concerns about the novelty of our work.\n\n------------------------------\n\n### **Comment 2: Comparison with recent methods [3,4,5].**\n\nIn compacting the paper to fit in the page limit we mistakenly did not include these relevant references, we apologize. We added discussion in the revised version. Here we summarize these works and point out their key differences with our method: \n\n- In particular, **Yang et al. [3]** explore molecule representation learning in out-of-distribution (OOD) scenarios. They achieve this by directing the molecule encoder to utilize stable and environment-invariant substructures relevant to the labels without the need for environmental labels. \n- Similarly, **Sui et al. [4]** introduces causal attention modules to identify key invariant subgraph features that can be described as causing the graph label. The type of OOD task that Sui et al. [4] considers assumes the graph label is caused by a subgraph, which is quite different from ours. Moreover, both Yang et al. [3] and Sui et al. [4] consider tasks where the graph label is caused by a subgraph. \n- **Ma et al.[5]** is an interesting theoretical work which studies GNN generalization and examines their fairness, showing that the test subgroup's distance from the training set impacts GNN performance. Ma et al.[5], as far as we could assess, does not propose any specific architecture to solve the type of OOD tasks we consider in our work.\n\nOverall, the goal of GraphMETRO is to be invariant to a mixture of selected stochastic transform functions (and the mixture can vary across different instances), which is a more flexible and general solution. \nWe added a discussion of these works to our revision, hopefully providing a more comprehensive comparison and literature overview. We hope our responses can solve your concerns about the related work.\n\n---\n\nWe sincerely appreciate all reviewers' time, efforts, and valuable suggestions in reviewing our paper. We are glad that most of the reviewers reached a positive consensus on our work's motivation and experimental results. Here is a summary of our responses:\n\n- **Clarification**: We clarify our key assumptions (`m8JS, FURX`), novelty (`m8JS, NLg1`), and the experimental settings (`m8JS`).\n\n- **Related work**: : We add more related works on recent graph OOD learning (`NLg1, m8JS`) and graph transfer learning (`FURX`)\n\n- **Limitation**: We add discussion about the limitations of GraphMETRO when confronted with unknown distribution shifts that surpass our key assumptions (`FURX`, `C9mM`).\n\nWe hope our responses can clarify your confusion and alleviate concerns and we updated our paper **(highlighted in green)** corespondingly. We thank all reviewers again, and look forward to your reply!", "author_response": "We appreciate your comments! To address your concerns, below we prudently justify the motivation of our proposed method, clarify our assumptions, and provide details regarding our experiment implementation.\n\n---------------------\n\n### **Comment 1: Motivation of this work**\n\nThank you for this comment! We believe there might be a bit of misunderstanding due to our different definitions of *“graph shift heterogeneity”*. We firstly discuss the related works mentioned and then justify our statement:\n\n**(1) Related works**\n\nPlease see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nMoreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you! \n\nSpecifically, Sui et al. [1] proposed a graph data augmentation strategy that alleviates covariate shift by generating diverse and invariant causal features. However, the trainable augmenter they used may not distill diverse augmentations or construct unseen perturbations. Moreover, Sui et al. [1] test its method only on graph classification tasks, while GraphMETRO can be applied to both node and graph classification tasks. Besides, we have discussed graph augmentation and attention-based methods in our related works, and we added more recent works on graph OOD [2,3,4], and we hope our response clears your concern on the related work discussion.\n\n\n**(2) The definition of graph shift heterogeneity**\n\nIn this work, we refer to **“heterogeneous shifts”** as multiple and different levels of shifts which vary across different instances (nodes or graphs), as illustrated in the example in the abstract. While we agree that the existing invariant learning approches can accommodate multiple distribution shifts, it could be hard for them to tackle nuanced distribution shifts for individual instances (nodes or graphs) since the distribution shifts are inferred from variance across multiple data environments. If GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering **a mixture of transformations as a proxy** for the target distribution shifts rather than invariance to whole-graph environment shifts. This is the type of heterogeneity we are interested in our paper. \n\n**(3) Regarding our original motivation statement**\n\nWhile the statement pointed out by the reviewer serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category. And we have provided detailed discussion about three lines of research in the related work section.\n\nHowever, we agree that we could make this statement border to cover the previous invariant learning methods. To improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to \"**previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments…**”. We also modified our introduction correspondingly, we hope this will better position our work.\n\n**(4) The choice of our MoE design**\n\n- The choice of our approach comes as a consequence of our motivation to model the graph/instance shift heterogeneity. As mentioned, mitigating multiple and nuanced distribution shifts simply goes beyond certain distribution shift types or environment construction as seen in the previous methods. \n- Thus, GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts. This enables the prediction of multiple different distribution shifts and the flexibility to model fine-grained heterogeneity since the mixture can be varied across different instances. We then tackled the proxy to mitigate the target distribution shifts. Intuitively, this solution provides a **“middle ground”** to deem graph generalization as an equivalent mixture, which, we believe, is a more tractable solution.\n\nWe updated our paper to make the above point more clear. We genuinely hope our answer can justify the motivation and solve your concern."}
{"claim": "The manuscript provides insufficient implementation details for the specific model architectures used, hindering reproducibility.", "claim_type": "methodology", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ppMpJyIYRZ", "reviewer": "Reviewer_m8JS", "review_text": "Summary: The paper introduces a method to enhance the out-of-distribution performance of graph neural networks (GNN) by learning to understand distribution shifts instead of addressing the assumed ones. To achieve this, the Mixture of Experts architecture is integrated into the GNN, supplemented by an alignment procedure to recognize the shift. Empirical experiments are conducted to validate the theoretical assertion.\n\nOn the whole, I believe the proposed method lacks the necessary motivation and its novelty isn't substantial enough to meet the standard.\n\nStrengths: - The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\n- Real-world datasets back the claims through experiments.\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\n\nWeaknesses: - The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \"previous research has concentrated on addressing specific types of distribution shifts.\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \"environment generators\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\n\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\n\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\n\nQuestions: Please check Weaknesses.", "labeling_timestamp": "2026-01-11T16:31:44.034051", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors point to implementation details in the appendix (model architectures, hyperparameters, optimization) and state they will expand them, thus addressing the reviewer’s concern but not fully conceding that details were originally insufficient. They provide counter-evidence while promising further clarification.", "evidence": "\"Model architectures: Please see Tables 2 and 3 in Appendix A for detailed model architecture information... Optimization process: Please see Table 2 and 3 (Appendix A), where we included hyperparameters on each dataset... Due to the space limitation, we had to include most of the implementation details in the appendix while we added pointers in the main paper. However, we will try to make it more detailed and feel free to let us know if anything is missing!\"", "combined_author_response": "Dear reviewer NLg1,\n\nAs the discussion period is closing soon, we hope to engage with you and check if your concerns are addressed.\n\nWe sincerely appreciate your time and attention!\n\nBest,\n\nAuthors of Paper 1560\n\n---\n\nThanks for reading our response! We would also appreciate your patience for reading the two points below \n\n---\n\n**Regarding point #1:**\n\nYes, we agree that the concept of heterogeneous shifts is relatively new for the current studies on distribution shifts, however, this is not new for studies on network patterns (Newman, 2003; Leskovec et al., 2005; 2007; Peel et al., 2017). In fact, the ignorance of such nuanced heterogeneous shifts in the previous studies instead emphasises our motivation and the potential impact of this work. \n\nFrom a causality perspective, distribution shifts can naturally happen in the instance level when (1) additional causal variables, beyond environmental factors and randomized noise, influence these shifts, or (2) multiple causal variables simultaneously affect the shifts with different strengths. Without modeling these explicitly, the mitigation of distribution shifts can easily fail. \n\nIn terms of our presentation, we have illustrated these cases of interest in abstract and introduction, we made further explanations in Section 3.1, we also present the specific results of distribution types in Figure 3 (b). \n\n\n**Regarding point #2:**\n\nWe think we can all agree that, if the ground truth of the instance shifts is available on the real-world datasets, it would be crystal clear to see where the improvement comes from since we can conduct case study to compare our method and the baselines on instances with nuanced distribution shifts, to see the influence of the modeling these heterogeneous shifts. \n\nHowever, with such ground truth not available, we had to seek other seemingly less intuitive but also in-depth way to illustrate the insights (esp Section 4.3) as mentioned above in our previous response. This is also why we designed the synthetic experiments at the first place. We think we did try hard to explain the underlying rationale with the ground truth being absent.\n\n----\n\n**Refinement**: We can of course add a causal graph in the our assumption section to make the concept more clear. And we can illustrate more if you could let us know the specific obstable to understand our mechanism, which will be extremely helpful.\n\n----\n\n### **Summary/TL;DR**\nWe understand the reviewer's clarification concerns, however, we don't agree that they, based on our justificaton, are the cause of rejection. We believe nuanced heterogeneous shifts are common, important, yet being typically ignored in the research domain of distribution shifts, we made these argument clear and also promise to refine. While explaining the underlying rationale is hindered by the lack of ground truth, we did try hard to dissection it from the gating model, the invariant representations generated, and more ablations in the appendix. We respect the reviewer's current opinion. Still, reconsideration will be greatly appreciated.\n\n---\n\nWe thank the reviewer once again! We also enjoyed the process of making our work more sound from your suggestions. And your approval surely means a lot to us.\n\nWe do apologize for not making our solutions towards the limitations clear enough. We summarize them into a short table, hopefully could alleviate your concerns a bit more.\n\n| Limitation| Potential solutions | Location of discussion/action |\n|:---|:---|:---|\n|**Coverage of the transformations** | 1) Extend the coverage by adding representative transform functions. 2) Or include tranform functions based on domain knowledge or a few samples from target distribution | Appendix F, paragraph #3|\n|**Complexity of the transformations** | Make experts dedicated to different hyperparameters for the same type of transformation | Argument options are available in our codebase. Will make it more detailed in the experimental settings. |\n|**Label distributional shift**|Integrate the objective of the existing methods studying labe distributional shifts into our framework. |Appendix F, paragraph #4 |\n\n----\n\nWe completely agree on the existence of these limitations and we will move some limitations (esp #1) to the main paper in our final version. While they could be important in the practice, they are, in our perspective, fair \"side effects\" considering the benefits (i.e., mitigating multiple and nuanced distribution shifts and better interpretability), and may not be the central part of our novelty and main contribution (i.e., the proposal of an equivalent mixture, the concept of referential invariant representations, as well as the training framework). We are also eager to further improve other aspects.\n\nOnce again thank you so much for your support!!\n\n---\n\nDear Reviewer m8JS,\n\nA gentle nudge that we would like to know if our response adequately addresses your concerns. \n\nYour time and feedback is greatly appreciated!\n\nThank you sincerely,\n\nAuthors of Paper 1560\n\n---\n\nWe deeply appreciate your approval. Your suggestions definitely inspired us a lot and have greatly improved our work.\n\nA few additional notes for the further comments:\n\n- Regarding **point #2**: In node classification tasks, the BCE objective already considers node embeddings other than the graph embedding of the k-hop subgraph. What we described previously refers to the computational graph (i.e., k-hop message passing), which generates node embeddings. We apologize for any confusion.\n\n- Regarding **point #3**: Yes it would be quite interesting to see how graph pretraining methods perform on the current OOD benchmarks. Graph pretraining methods like GCC and GPT-GNN also consider graph extrapolation to some extent, e.g., through subgraph extraction and masked attributes/structures. The key difference between graph pretraining and generalization may lie in their different focuses on **expressiveness** and **invariance**. While these two aspects do not always conflict, ensuring invariance w.r.t. a certain type of extrapolation might affect expressiveness (if the change is relevant to labels), and vice versa. To seek a balance in between, one might need prior knowledge in which types of transformations may or may not be sensitive to the labels (and perhaps build experts with different goals to make ensure invariance or expressivess). We believe there is still a lot to explore in this domain.\n\nOverall, we are grateful for your positive stand on this work. We believe that our [current version](https://openreview.net/pdf?id=QQ5eVDIMu4), incorporating opinions from you and other reviewers, is sound and well-refined. We are also committed to further improving it. We would appreciate your support based on our current version! \n\nThank you!\n\n---\n\nDear Reviewers of Paper 1560,\n\nWe hope this message finds you well.\n\nAs the discussion phase approaches its end, we hope you find our responses useful. We would like to ask if the issues have been addressed.\n\nWe understand that the discussion time is short, and some of you might be enjoying holidays at the moment. We apologize for posting our responses a bit late as we aimed to address your concerns clearly.\n\nWe sincerely appreciate your time and attention!\n\nBest regards,\n\nAuthors of Paper 1560\n\n---\n\n**Comment 4: Related works on graph transfer learning.**\n\nYes! We agree that graph transfer learning is a relevant topic. Thank you for providing the additional related work! We added the discussion on these works in our revision, and we repeat it here for your convenience:\n\n```It is also worth mentioning that graph domain adaptation (Zhang et al., 2019; Wu et al., 2020), different from the problem studied in this work, commonly relies on limited labeled samples from target datasets for improved transferability. For instance, to generate domain adaptive network embedding, DANE (Zhang et al., 2019) uses shared weight graph convolutional networks and adversarial learning regularization, while UDA-GCN (Wu et al., 2020) employs attention mechanisms to merge global and local consistencies.```\n\nMoreover, we believe this is actually relevant to `Comment & Question 1` regarding the generality of the distribution shifts covered by the transform functions. In our future works, we added the following discussion:\n\n```...Leveraging a few samples from target distribution (i.e., domain adaptation). Specifically, we can leverage the samples from the target distribution to inform the selection or construction of transform functions, which can better guarantee the distribution shifts are covered by the transform functions.  For example, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space. We believe this would be an interesting future direction.```\n\n---\n\n# Summary\n\nOnce again thank you very much for helping us position our work and reflect the literature better. We hope all of your concerns are solved and we are happy to engage further if there are any other points we missed!\n\n---\n\n### **Reference**\n\n[1] Unsupervised Domain Adaptive Graph Convolutional Networks. Wu et al. WWW 2020\n\n[2] DANE: Domain adaptive network embedding. Zhang et al. IJCAI 2019.\n\n[3] Tong Zhao, Yozen Liu, Leonardo Neves, Oliver J. Woodford, Meng Jiang, and Neil Shah. Data augmentation for graph neural networks. AAAI, 2021.\n\n---\n\nWe are grateful for your positive feedback and detailed suggestions! We provide responses below to address your remaining concerns. \n\n---\n\n### **Comment & Question 1: Generality of the distribution shifts covered by the transform functions.**\n\nGood question! We provide the response from two angles:\n\n\n**(a) For general domain**: In our experiments, we mainly use the five stochastic transform functions, which are universal graph augmentations as listed in Zhao et al., (2021) [3]. In our code implementation, we have also included additional transform functions as shown in Appendix B. We believe these transform functions, while not exhaustive, still cover a wide range of distribution shifts observing from our experimental results.\n\nNevertheless, we agree that the real graph distribution shifts can go beyond any possible combinations of the predefined transform functions. In that case, the assumption may not hold, meaning that GraphMETRO may not capture and precisely mitigate the unknown distribution shift. This scenario could always possibly exist due to the lack of information about the testing distribution or its domain knowledge. We include it as a limitation in Appendix 5, while we further discuss how we could alleviate the problem with additional information.\n\n**(b) For specific domains where additional knowledge is available**: In fact, knowing the tendency of the distribution shifts, such as increasing malicious users in a trading system, would be very helpful in constructing the transform functions that can cover the target distribution shifts well. We believe that such knowledge can come from two sources: \n- **Domain knowledge**, e.g., on molecular datasets, the transform function could be adding additional carbon structure to a molecule (while preserving its functional groups). Or, in a particular social network, transform functions can be defined from known user behaviors. \n- **Leveraging a few samples from target distribution**. This is in fact in line with the reviewer’s Comment #4 regarding graph transfer learning. Specifically, with the guide from a few target samples, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space.\n\n We hope this response can alleviate your concern about our applicability.\n\n---\n\n### **Comment & Question 2: The mechanism of GaphMETRO in handling heterogeneous shifts within one graph.**\n\nGood question! For simplicity, suppose we have two transform functions, i.e., adding edges and dropping edges. Given a node classification task and the objective in Eq. (3), the model is trained on the extrapolated datasets based on the transform functions. After that, given an unseen graph with imbalanced distribution shifts, the gating model outputs scores to identify nodes likely to experience increasing or decreasing degrees, while others might adhere to the original distribution. Then, for each node, each expert takes its multihop subgraph, and outputs its referential invariant representation w.r.t. the correposing transform function. These expert outputs and gating model results are then aggregated to form the final representation. Assuming accurate predictions by the gating function, nodes in denser/sparser subgraphs are represented by expert models corresponding to adding/dropping edges. As each expert is trained to create invariant representations, the final node representations remain unaffected by their individual distribution shifts.\n\n---\n\n### **Comment 3: How to leverage graph pretraining.**\n\nIf we understand correctly, the reviewer was asking how to leverage a pretrained model to further aid the training of GraphMETRO. Please let us know if otherwise. \n\nThis is in fact an interesting point! We believe a model pretrained on a wide variety of data can be very helpful to initialize the gating model, which is required to output the mixture of a node or graph (after it is finetuned on the extrapolated dataset). By enhancing the gating model's predictive capability regarding mixtures, GraphMETRO's final representation should become more resilient. This becomes particularly advantageous when dealing with graphs not previously encountered in the extrapolated dataset. Moreover, it is possible that the pretrained model will also benefit the expert models, while one minor concern would be that the expert model may tend to be similar instead of dedicating to generating invariant representation w.r.t. their corresponding transform function. We added the above discussion to Appendix F as a future work. Thanks again for this comment!\n\n---\n\n### **Question 4: Why does the test accuracy vary across different transformations?**\n\nGreat question! Here we summarize three possible reasons:\n\n- **Information Preservation in Transformations:** Certain transformations retain more informative features than others. For instance, in the REDDIT-BINARY (graph classification task), the random subgraph transformation may retain more graph label-related information compared to dropping edges, as the latter tends to lose more global information. This discrepancy in testing performance, where dropping edges outperforms random subgraph extraction, could be due to the preservation of crucial information. However, conclusions may vary across datasets or tasks depending on how information influences final predictions. For CiteSeer (a node classification task), a random subgraph might preserve more local node information, potentially explaining why its testing performance surpasses dropping edges in this specific task.\n\n- **Complexity of transformation:** Certain transformations inherently generate more diverse graphs than others. If the model lacks the expressive capacity to capture such diversity, it may lead to a decline in testing performance.\n\n- **Model Sensitivity:** Certain transformations may be easier for a model to learn due to compatibility with specific model architectures. This extends beyond transformation complexity and emphasizes how different model architectures may prefer learning particular distributions from one of the extrapolated datasets, which can also contribute to the difference in the testing performance.\n\nWe included the above discussion to Appendix F: Open Discussions. We hope this response can answer your question and improve the soundness of our work.\n\n---\n\n# Summary \n\nWe are grateful for your time and insightful suggestions! \n\nWe would like to highlight that our main contribution is framing the graph generalization problem on top of an equivalent mixture, a simple yet novel and tractable \"middle ground\", as well as proposing the training framework which effectively guarantees the generalization. While our method relies on a set of predefined transform functions, we believe they cover a wide range of distribution shifts based on our empirical results. Also, we agree that there could be some scenarios where the transform functions may not cover complex distributions, and we discuss two future directions and include them into our future works. Moreover, while selecting the hyperparameters for the transform functions introduce extra complexity, the issue could be minor in practice and we also conduct more experiments to justify the applicability better. Finally, we address several questions about clarification and presentation, as well as including more future works. \n\nLastly, we prudently ask you to reevaluate our work given the clarification in our responses, which we also updated our paper correspondingly. Overall, we believe our work makes good contributions to the field of graph distribution learning by proposing a novel and effective solution, and we would appreciate your reconsideration on this point. Thank you for your efforts again!\n\n---\n\n### **Reference**\n\n[1] Menon, Aditya Krishna, et al. \"Long-tail learning via logit adjustment.\" International Conference on Learning Representations. 2020.\n\n[2] Cao, Kaidi, et al. \"Learning imbalanced datasets with label-distribution-aware margin loss.\" Advances in neural information processing systems 32 (2019).\n\n[3] DARTS: Differentiable Architecture Search. Hanxiao Liu, Karen Simonyan, Yiming Yang. 2018.\n\n---\n\n**(3) What is the complexity of the transform functions and how does it affect generalization?** \n\nInteresting question! In fact, our implementation and framework could easily avoid selecting hyperparameters on the transform functions. Specifically, we can make multiple transform functions of the same type with different ranges of hyperparameters. Specifically, GraphMETRO allows three edge dropping transform functions, $\\tau_1^{\\alpha_1}, \\tau_2^{\\alpha_2}, \\tau_3^{\\alpha_3}$, where $\\alpha_i$ (i=1, 2, 3) are three different ranges of edge dropping probabilities, e.g., [0.1,0.3], [0.3, 0.6], [0.6,0.9], representing different transform extents. Thus, given an input from the validation dataset, the gating model will highlight the transform function which simultaneously selects the corresponding hyperparameter that matches the distribution of the validation set. Interestingly, this idea is in the same spirit as how DARTS [3]  proposes to perform architecture search by formulating the task in a differentiable manner. \n\nIn our previous experiments, we did try this scheme where we replaced a single edge dropping transform function with the ratio range [0.3, 0.5] to three transform functions as mentioned above. While we didn’t see a significantly different performance in that case, we believe this would be a flexible solution which avoids the need to conduct hyperparameter selection. \n\nWe include the above discussion in our open discussion and future works (Appendix F). We hope this response can alleviate your concern about the applicability of our method. \n\n---\n\n### **Comment 4: Limitations of our work**\n\nGreat point! We think the issue of label distributional shift, while important, is orthogonal and complementary to the focus of our current study. To elaborate, label distributional shifts exert analogous impacts across various modalities, such as graphs or images. Moreover, existing methods [1,2] designed to tackle label distributional shifts can be seamlessly integrated into our proposed framework. Such integration would necessitate minimal adjustments, potentially involving modifications to the loss function or the training pipeline. We added this as a future work in Appendix F.\n\n---\n\n### **Question 1: Statistical significance of the results on Table 1**\n\nThanks for the question! We compute the p-value of our method against the best baselines method as follows:\n\n| | WebKB | Twitch | Twitter | SST2 | \n|:--|:--|:--|:--|:--|\n|p-value|< 0.001| 0.023 | 0.042| 0.081|\n\nGiven the cut-off threshold as 0.05, we believe the performances of GraphMETRO are statistically significant on WekGB, Twitch, and Twitter datasets, while on the SST2, we see relatively weak evidence. We added the p-value results to our revision and hope our response can alleviate your concern on our improvements.\n\n---\n\n###  **Question 2: Can we say we use ERM for the node classification even if nodes and their labels are not i.i.d.?** \n\nIf we understand correctly, the reviewer is asking for clarification on the 2nd term of our objective. Please let us know if otherwise. Here, our thinking is that the cross-entropy loss for node classification already assumes node labels are conditionally independent given the model (the negative log-likelihood is a sum over the labeled nodes in training). Then, we use the same assumption of cross-entropy on Empirical Risk Minimization (ERM). That is, for a given model we must also minimize the error variance across nodes. The task is then to find the model with the best performance and small variance.\n\n---\n\n### **Question 3: Numerical results on Figure 2**\n\nThanks for pointing it out! We included all of the numerical results of Figure 2 in Appendix E, while showing the results on DBLP below.\n\n| |i.i.d. (0)|noisy feature (1)|add edge (2)|drop edge (3)|drop node (4)|random subgraph (5)|\n|:--|:--|:--|:--|:--|:--|:--|\n|ERM | 85.71 | 84.48 | 71.08 | 79.69 | 83.41 | 76.9|\n|ERM-Aug | 85.66 | 85.29 | 74.85 | 82.34 | 84.44 | 72.81|\n|GraphMETRO | 85.92 | 85.78 | 76.61 | 82.95 | 84.98 | 81.32|\n\n| |(4, 5)|(3, 5)|(2, 5)|(1, 5)|(2, 4)|(1, 4)|     (2, 3)|(1, 3)|\n|:--|:--|:--|:--|:--|:--|:--|:--|:--|\n|ERM | 70.4 | 77.63 | 81.99 | 79.69 | 70.55 | 71.52 | 77.73 | 79.59|\n|ERM-Aug | 74.16 | 81.04 | 83.65 | 68.62 | 74.01 | 68.27 | 81.13 | 84.49|\n|GraphMETRO | 76.18 | 81.71 | 84.26 | 80.31 | 75.1 | 71.05 | 81.85 | 87.14|\n\nAcross all of the synthetic environments, GraphMETRO averagely outperforms ERM and ERM-Aug by 3.20% and 2.45%, respectively.\n\n---\n\nWe appreciate your comments! To address your concerns, below we prudently justify the assumption of our method, the predefined transformation functions, as well as their complexity, and clarify our presented results.\n\n---\n\n### **Comment 1-3: Applicability and our assumptions**\n\nThanks for these great comments! Here we provide response in three folds:\n\n**(1) How does GraphMETRO identify all distribution shifts from transform functions if they are treated independently when combined together?** \n\nIf we understand correctly, by “treated independently”, the reviewer is referring to the first term in our objective $\\text{BCE}(\\phi(\\tau^{(k)}(\\mathcal{G})), Y (\\tau^{(k)}))$, where we formulate predicting the distribution shifts types of a jointly transformed graph as a binary multiclass classification problem. We believe the difficulty of this task comes from both the property of transform functions and the expressiveness of the gating model. \n\n- Firstly, some transform functions are **inherently disentangled**, e.g., adding nodes feature noise and random subgraph extraction. In this case, there will be certain distinction between any pair from these three data distributions, i.e., (graphs with node noise, random subgraph graphs, random subgraphs with node noise), which the gating model can easily tell. \n- While some transform functions can be **essentially similar**, e.g., drop path and drop edges, this won’t affect the performance of our method as long as each expert outputs the corresponding invariant representation. \n- Lastly, indeed, there could be more **complex combinations of the transform functions**, which poses challenges to the gating model’s expressiveness in identifying the combinations. However, this challenge may be minor in the practice. Specifically, we observe fairly high accuracy performances of the gating model, which are above 85% and 73% averagely on extrapolated datasets with one transformation and multiple transformations, respectively.\n\nWe added the above discussion to Appendix F to enable a more comprehensive view towards our methodology. We hope this can alleviate your concern on our gating model’s performance in identifying the distribution shift types.\n\n**(2) How does the predefined transform functions cover complex distributions causing the distribution shift?**\n\nThis is also a great question! We believe there are two angles for this question. \n\n**(a) For general domain**: In our experiments, we mainly use the five stochastic transform functions, which are universal graph augmentations as listed in Zhao et al., (2021) [3]. In our code implementation, we have also included additional transform functions as shown in Appendix B. We believe these transform functions, while not exhaustive, still cover a wide range of distribution shifts observing from our experimental results.\n\nNevertheless, we agree that the real graph distribution shifts can go beyond any possible combinations of the predefined transform functions. In that case, the assumption may not hold, meaning that GraphMETRO may not capture and precisely mitigate the unknown distribution shift. This scenario could always possibly exist due to the lack of information about the testing distribution or its domain knowledge. We include it as a limitation in Appendix 5, while we further discuss how we could alleviate the problem with additional information.\n\n**(b) For specific domains where additional knowledge is available**: In fact, knowing the tendency of the distribution shifts, such as increasing malicious users in a trading system, would be very helpful in constructing the transform functions that can cover the target distribution shifts well. We believe that such knowledge can come from two sources: \n- **Domain knowledge**, e.g., on molecular datasets, the transform function could be adding additional carbon structure to a molecule (while preserving its functional groups). Or, in a particular social network, transform functions can be defined from known user behaviors. \n- **Leveraging a few samples from target distribution**. Specifically, with the guide from a few target samples, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space.\n\n---\n\n[1] Sui et al. Unleashing the Power of Graph Data Augmentation on Covariate Distribution Shift. NeurIPS 2023.\n\n[2] Nianzu Yang et al. Learning substructure invariance for out-of-distribution molecular representations, NeurIPS 2022\n\n[3] Yongduo Sui et al. Causal Attention for Interpretable and Generalizable Graph Classification, KDD 2022.\n\n[4] Jiaqi Ma et al. Subgroup Generalization and Fairness of Graph Neural Networks, NeurIPS 2022.\n\n---\n\n### **Comment 2: Our assumption**\n\nThanks for this comment! In Section 3.1, we have discussed the assumptions in detail with specifications. We repeat part of it here for your convenience:\n\n```Assumption 1 essentially states that the distribution shifts (whatever they are) can be decomposed into several mixture components of stochastic graph transformations. For example, on a social network dataset, each mixture component can represent different patterns of user behavior or network dynamics shifts. Specifically, one mixture component might correspond to increased user activity, while another could signify a particular trend of interaction within a certain group of users. Such a mixture pattern is common and well-studied in the real-world network datasets (Newman, 2003; Leskovec et al., 2005; 2007; Peel et al., 2017).```\n\nWe hope the analysis provides detailed illustration to the assumption. We are happy to add more discussion if you think anything  is still unclear!\n\n---\n\n### **Comment 2 & 3: Implementation of our method**\n\n- **Model architectures**: \nPlease see Tables 2 and 3 in Appendix A for detailed model architecture information. Specifically, we use the same encoders and classifiers from GOOD benchmarkfor real-world datasets to ensure fair comparisons. We employ backbones based on the best ERM performance for synthetic datasets. Moreover, GraphMETRO is model-agnostic, which consistently improves performance across varied model architectures.\n\n- **Stochastic transformation**: Please see Appendix B where we include the introduction of the stochastic transform functions. \n\n- **Optimization process**: Please see Table 2 and 3 (Appendix A), where we included hyperparameters on each dataset. In the second paragraph after Eq. (3), we also described our training pipeline: `“...we set apart the other loss terms from backpropagating to it to avoid interference with the training of the gating model… We optimize the objective via stochastic gradient descent”`. \n\n- **For a more intricate optimization strategy:** This is a great catch! In fact, we did try to pretrain the gating model for several epochs as warm up before training the whole model in an end-to-end fashion via the objective Eq. in (3). However, we didn’t notice a statistically significant difference in their performance, which can be due to that expert models take more time to convergence (since they need to align with the base model during the training) compared to the gating model.\n\nDue to the space limitation, we had to include most of the implementation details in the appendix while we added pointers in the main paper. However, we will try to make it more detailed and feel free to let us know if anything is missing! \n\n---\n\n### **Comment 2 (Cont.): Understand the functionality, underlying mechanism, and performance gain**\n\nThanks for the comment. For clarity, we provide the following pointers to the paper:\n\n- In Section 4.3, we provide a study to reveal the underlying **mechanism** of GraphMETRO, i.e., each expert excels in generating invariant representations concerning a stochastic transform function, which provide a solid foundation in generating referential invariant representations w.r.t. the specific transformations and further guarantee the generalization.\n\n- In Appendix C, we study the **impact of the MoE architecture** on model performance, which shows that the model performance may decline if the expressiveness of the expert model decreases. \n\n- In Appendix D, we study the **impact of the stochastic transform function** on model performance, where we also provide a detailed discussion of the modeling mechanism.\n\nWe believe the above studies and discussion in our paper provides an in-depth view, highlighting the roles of our objective, architecture, and stochastic transform function.  Please let us know if any of these perspectives is still unclear, and we can further improve our experimental study.\n\n-----\n\n# Summary\n\nWe hope our answers can address all of the concerns. We are happy to follow up if you have any further questions.\n\nWe also prudently ask you to reevaluate our work. To highlight, our motivation is supported by the common fine-grained graph shift heterogeneity, and the fact that most of the previous works could not model such nuanced distribution shifts in an effective and flexible manner. Moreover, we added more related works and modified our statement to position our work better. In general, we believe GraphMETRO is a more general and flexible solution that can mitigate a wider range of distribution shifts, which is backed by the experimental results. Detailed justification on our assumption and implementation details are also available in our paper. \n\nThus, we believe our work makes important contributions and provides a clear presentation. We are happy to discuss more and revise our paper if any concern remains. Thank you for your efforts and we are looking forward to your reply!!\n\n---\n\nWe appreciate your comments! To address your concerns, below we prudently justify the motivation of our proposed method, clarify our assumptions, and provide details regarding our experiment implementation.\n\n---------------------\n\n### **Comment 1: Motivation of this work**\n\nThank you for this comment! We believe there might be a bit of misunderstanding due to our different definitions of *“graph shift heterogeneity”*. We firstly discuss the related works mentioned and then justify our statement:\n\n**(1) Related works**\n\nPlease see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nMoreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you! \n\nSpecifically, Sui et al. [1] proposed a graph data augmentation strategy that alleviates covariate shift by generating diverse and invariant causal features. However, the trainable augmenter they used may not distill diverse augmentations or construct unseen perturbations. Moreover, Sui et al. [1] test its method only on graph classification tasks, while GraphMETRO can be applied to both node and graph classification tasks. Besides, we have discussed graph augmentation and attention-based methods in our related works, and we added more recent works on graph OOD [2,3,4], and we hope our response clears your concern on the related work discussion.\n\n\n**(2) The definition of graph shift heterogeneity**\n\nIn this work, we refer to **“heterogeneous shifts”** as multiple and different levels of shifts which vary across different instances (nodes or graphs), as illustrated in the example in the abstract. While we agree that the existing invariant learning approches can accommodate multiple distribution shifts, it could be hard for them to tackle nuanced distribution shifts for individual instances (nodes or graphs) since the distribution shifts are inferred from variance across multiple data environments. If GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering **a mixture of transformations as a proxy** for the target distribution shifts rather than invariance to whole-graph environment shifts. This is the type of heterogeneity we are interested in our paper. \n\n**(3) Regarding our original motivation statement**\n\nWhile the statement pointed out by the reviewer serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category. And we have provided detailed discussion about three lines of research in the related work section.\n\nHowever, we agree that we could make this statement border to cover the previous invariant learning methods. To improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to \"**previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments…**”. We also modified our introduction correspondingly, we hope this will better position our work.\n\n**(4) The choice of our MoE design**\n\n- The choice of our approach comes as a consequence of our motivation to model the graph/instance shift heterogeneity. As mentioned, mitigating multiple and nuanced distribution shifts simply goes beyond certain distribution shift types or environment construction as seen in the previous methods. \n- Thus, GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts. This enables the prediction of multiple different distribution shifts and the flexibility to model fine-grained heterogeneity since the mixture can be varied across different instances. We then tackled the proxy to mitigate the target distribution shifts. Intuitively, this solution provides a **“middle ground”** to deem graph generalization as an equivalent mixture, which, we believe, is a more tractable solution.\n\nWe updated our paper to make the above point more clear. We genuinely hope our answer can justify the motivation and solve your concern.\n\n---\n\n### **Question 2: How does the computational cost of GraphMETRO compare to other methods?**\n\nPlease see the last paragraph of Section 3.4, where we analyze the computation complexity of GraphMETRO. We repeat part of it here for your convenience:\n\n```Consider the scenario where we use an individual encoder for each expert. The forward process of $f$ involves $O(K)$ forward times using the weighted sum aggregation (or $O(1)$ if using the maximum selection). Since we extend the dataset to $(K + 1)$ times larger than the original data, the computation complexity is $O(K^2 |D_s|)$, where |Ds| is the size of the source dataset.```\n\nThus, the computation cost is about $K^2$ or $K$ times (if using the maximum selection) than an ERM model, where $K=5$ in our experiments. Compared to DIR, as they extract $B$ spurious subgraphs from each batch to conduct the intervention, their computation cost is $B$ times compared to ERM, where $B$ could be 32. Thus, we believe the computation cost of GraphMETRO is fair for the gains we get, considering $K$ is usually small.\n\n----------------------\n\n### **Question 3: Can GraphMETRO handle multiple different types of distribution shifts that simultaneously exist in data?**\n\nYes! The distribution shift types corresponding to the gating outputs with high scores will be tackled during training. That is, if the gating output highlights multiple mixture components, their corresponding distribution shift types will be handled jointly.\n\n----------------------\n\n### **Question 4: Can GraphMETRO  tackle distribution shifts on molecular graphs?**\n\nThat is a great idea! Yes, GraphMETRO can be applied to molecular datasets if one designs transform functions to cover typical molecular variants. For instance, a transform function may add carbon structures to the molecules. These domain-specific transform functions are outside the scope of our work, however, we believe these would be interesting future work directions!\n\n----------------------\n\n# Summary \n\nWe thank the reviewer for the time and insightful suggestions! We hope our answers can address your concerns well.\n\nWe also prudently ask you to reconsider our work if the concerns are addressed. To highlight, our novelty comes from the formulation of an equivalent mixture for graph OOD and the training framework to effectively realize generalization. We also provide an in-depth analysis on our originality compared to some previous invariant learning methods. While we discussed and compared with previous works, we added more related works and modified our statement to position our work better. Finally, our method achieves great improvements on both node and graph classification tasks, and is a more general solution to mitigate multiple and nuanced distribution shifts. \n\nOverall, we believe our work proposes a new paradigm and novel training framework and makes good contributions in the fields of graph generalization, and we would appreciate your reconsideration on this point. Thank you for your efforts again!\n\n----------------------\n\n**Reference**\n\n\n[1-5] The same as listed by the reviewer\n\n\n[6] Beatrice Bevilacqua, Yangze Zhou, and Bruno Ribeiro. Size-invariant graph representations for graph classification extrapolations. In ICML, 2021.\n\n[7] Davide Buffelli, Pietro Li´o, and Fabio Vandin. Sizeshiftreg: a regularization method for improving size-generalization in graph neural networks. In NeurIPS, 2022.\n\n[8] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In NeurIPS, 2019.\n\n[9] Mucong Ding, Kezhi Kong, Jiuhai Chen, John Kirchenbauer, Micah Goldblum, David Wipf, Furong Huang, and Tom Goldstein. A closer look at distribution shifts and out-of-distribution generalization on graphs. In NeurIPS DistShift, 2021.\n\n---\n\n### **Comment 3 & Question 1: Related works on invariant learning and clarification on our statement.**\n\nThanks for pointing it out! Below we clarify the statement and clear potential misunderstanding:\n\n**(1) How does GraphMETRO compare with invariant learning methods like DIR and EERM?** \n\nPlease see our related work section where we discussed these two papers, i.e., DIR (Wu et al., 2022c) and EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on environmental patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nBesides, we also provide a more in-depth comparison in our point (3) below to highlight our key technical originality.\n\n**(2) Regarding our statement about previous works:** \n\nWhile the statement serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category, and we have provided detailed discussion about three lines of research in the related work section.\n\nTo improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to **\"previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments** (which is highly limited when confronted with nuanced distribution shifts)”. We also modified our introduction correspondingly. Thanks for letting us know our statement could be misinterpreted.\n\n**(3) Why do we say our method could be more broad than the existing invariant learning approaches?** \n\n- Invariant subgraph learning approaches, e.g., [1,2], consider variance of constructed data environments, which are designed very differently compared to our work. \nWhile they can accommodate multiple distribution shifts (as in multiple environments), these focus on patterns within each environment and ignore the variety across instances (e.g., shifts at the resolution of nodes), which may not be well-captured by the environment assignments. \n- GraphMETRO considers that specific parts of the test graph may have different shifts. Particularly, our goal is to make the generalization to unknown testing distribution more adaptive and broad, as opposed to limiting the distribution shifts to being invariant to specific types of subgraphs. \n\nIn other words, if GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering a mixture of transformations as a proxy for the target distribution shifts rather than invariance to environment shifts.\n\n**(4) Key technical originality compared to invariance learning (going more deeply)**\n\nAnother interesting view to see the innovation of GraphMETRO is that it breaks the typical invariant learning formulation, which assumes the data is manipulated by the environment variables (and then can be “decoded” into multiple environments). Instead, GraphMETRO sees the distribution shifts on an instance as a mixture, which is represented by the score vector output by the gating function over the basis of the transform functions. In other words, GraphMETRO can produce infinite environments as the elements in the score vector are continuous. One can see that once we limit the output domain of the gating function into, e.g., binary {0, 1}, GraphMETRO can also produce a limited number of environments (if we categorize the instances based on the score vector), which covers the environment construction in invariant learning. Moreover, as mentioned, we propose the concept of referential invariant representation with a base model $\\xi_0$, which is also different from previous works on invariant learning. We added the above discussion to Appendix F to improve the depth of our analysis.\n\n---\n\nWe appreciate your efforts and insightful comments! To address your concerns, we provide point-to-point responses below.\n\n---------------------\n\n### **Comment 1: Regarding the novelty of GraphMETRO.**\n\nThanks for the comment! We believe our novelty comes from the proposal of an equivalent mixture for graph OOD and the construction of our training framework, as detailed below:\n\n- **An equivalent mixture for graph OOD**: The key challenge we faced to mitigate multiple and nuanced distribution shifts is the intrinsic complexity and heterogeneity of graph distribution shifts, which simply goes beyond certain distribution shift types [6,7,8,9] or environment construction as seen in the previous methods [1,2,3]. GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts, where the mixture can be varied across different instances, and then tackled the proxy to mitigate the target distribution shifts. We believe the high-level idea is succinct, nevertheless, novel, in the sense that it provides a “middle ground” to deem graph generalization as an equivalent mixture that is more tractable.\n\n- **Training framework**: With the guide of our formulation, the training framework is still non-trivial due to two problems, i.e., “how to provide supervision for predicting the mixture” and “how to ensure the experts corresponding to mixture components are compatible when working as a whole”. Specifically, GraphMETRO solves the first problem by conducting graph extrapolation. This is somewhat similar to the spirit of graph pretraining in the sense that we inject heterogeneity to promote the expressiveness of the gating model in recognizing the mixture components. For the second problem, we introduce the concept of Referential Invariant Representation, along with the novel objective in Eq. (3) to enforce the invariance and compatibility. It is worth mentioning that the model performance is much worse than the reported numbers (e.g., 2.7\\% lower on Twitch dataset) without the compatibility constraint, indicating the proposed referential invariance concept is indispensable. \n\nWe added more justification in the introduction (updated in the revision). We genuinely hope our responses can solve your concerns about the novelty of our work.\n\n------------------------------\n\n### **Comment 2: Comparison with recent methods [3,4,5].**\n\nIn compacting the paper to fit in the page limit we mistakenly did not include these relevant references, we apologize. We added discussion in the revised version. Here we summarize these works and point out their key differences with our method: \n\n- In particular, **Yang et al. [3]** explore molecule representation learning in out-of-distribution (OOD) scenarios. They achieve this by directing the molecule encoder to utilize stable and environment-invariant substructures relevant to the labels without the need for environmental labels. \n- Similarly, **Sui et al. [4]** introduces causal attention modules to identify key invariant subgraph features that can be described as causing the graph label. The type of OOD task that Sui et al. [4] considers assumes the graph label is caused by a subgraph, which is quite different from ours. Moreover, both Yang et al. [3] and Sui et al. [4] consider tasks where the graph label is caused by a subgraph. \n- **Ma et al.[5]** is an interesting theoretical work which studies GNN generalization and examines their fairness, showing that the test subgroup's distance from the training set impacts GNN performance. Ma et al.[5], as far as we could assess, does not propose any specific architecture to solve the type of OOD tasks we consider in our work.\n\nOverall, the goal of GraphMETRO is to be invariant to a mixture of selected stochastic transform functions (and the mixture can vary across different instances), which is a more flexible and general solution. \nWe added a discussion of these works to our revision, hopefully providing a more comprehensive comparison and literature overview. We hope our responses can solve your concerns about the related work.\n\n---\n\nWe sincerely appreciate all reviewers' time, efforts, and valuable suggestions in reviewing our paper. We are glad that most of the reviewers reached a positive consensus on our work's motivation and experimental results. Here is a summary of our responses:\n\n- **Clarification**: We clarify our key assumptions (`m8JS, FURX`), novelty (`m8JS, NLg1`), and the experimental settings (`m8JS`).\n\n- **Related work**: : We add more related works on recent graph OOD learning (`NLg1, m8JS`) and graph transfer learning (`FURX`)\n\n- **Limitation**: We add discussion about the limitations of GraphMETRO when confronted with unknown distribution shifts that surpass our key assumptions (`FURX`, `C9mM`).\n\nWe hope our responses can clarify your confusion and alleviate concerns and we updated our paper **(highlighted in green)** corespondingly. We thank all reviewers again, and look forward to your reply!", "author_response": "We appreciate your comments! To address your concerns, below we prudently justify the motivation of our proposed method, clarify our assumptions, and provide details regarding our experiment implementation.\n\n---------------------\n\n### **Comment 1: Motivation of this work**\n\nThank you for this comment! We believe there might be a bit of misunderstanding due to our different definitions of *“graph shift heterogeneity”*. We firstly discuss the related works mentioned and then justify our statement:\n\n**(1) Related works**\n\nPlease see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nMoreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you! \n\nSpecifically, Sui et al. [1] proposed a graph data augmentation strategy that alleviates covariate shift by generating diverse and invariant causal features. However, the trainable augmenter they used may not distill diverse augmentations or construct unseen perturbations. Moreover, Sui et al. [1] test its method only on graph classification tasks, while GraphMETRO can be applied to both node and graph classification tasks. Besides, we have discussed graph augmentation and attention-based methods in our related works, and we added more recent works on graph OOD [2,3,4], and we hope our response clears your concern on the related work discussion.\n\n\n**(2) The definition of graph shift heterogeneity**\n\nIn this work, we refer to **“heterogeneous shifts”** as multiple and different levels of shifts which vary across different instances (nodes or graphs), as illustrated in the example in the abstract. While we agree that the existing invariant learning approches can accommodate multiple distribution shifts, it could be hard for them to tackle nuanced distribution shifts for individual instances (nodes or graphs) since the distribution shifts are inferred from variance across multiple data environments. If GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering **a mixture of transformations as a proxy** for the target distribution shifts rather than invariance to whole-graph environment shifts. This is the type of heterogeneity we are interested in our paper. \n\n**(3) Regarding our original motivation statement**\n\nWhile the statement pointed out by the reviewer serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category. And we have provided detailed discussion about three lines of research in the related work section.\n\nHowever, we agree that we could make this statement border to cover the previous invariant learning methods. To improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to \"**previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments…**”. We also modified our introduction correspondingly, we hope this will better position our work.\n\n**(4) The choice of our MoE design**\n\n- The choice of our approach comes as a consequence of our motivation to model the graph/instance shift heterogeneity. As mentioned, mitigating multiple and nuanced distribution shifts simply goes beyond certain distribution shift types or environment construction as seen in the previous methods. \n- Thus, GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts. This enables the prediction of multiple different distribution shifts and the flexibility to model fine-grained heterogeneity since the mixture can be varied across different instances. We then tackled the proxy to mitigate the target distribution shifts. Intuitively, this solution provides a **“middle ground”** to deem graph generalization as an equivalent mixture, which, we believe, is a more tractable solution.\n\nWe updated our paper to make the above point more clear. We genuinely hope our answer can justify the motivation and solve your concern."}
{"claim": "The paper lacks sufficient details on how stochastic transformations are implemented within the proposed method.", "claim_type": "methodology", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ppMpJyIYRZ", "reviewer": "Reviewer_m8JS", "review_text": "Summary: The paper introduces a method to enhance the out-of-distribution performance of graph neural networks (GNN) by learning to understand distribution shifts instead of addressing the assumed ones. To achieve this, the Mixture of Experts architecture is integrated into the GNN, supplemented by an alignment procedure to recognize the shift. Empirical experiments are conducted to validate the theoretical assertion.\n\nOn the whole, I believe the proposed method lacks the necessary motivation and its novelty isn't substantial enough to meet the standard.\n\nStrengths: - The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\n- Real-world datasets back the claims through experiments.\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\n\nWeaknesses: - The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \"previous research has concentrated on addressing specific types of distribution shifts.\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \"environment generators\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\n\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\n\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\n\nQuestions: Please check Weaknesses.", "labeling_timestamp": "2026-01-11T16:32:09.120466", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors point to implementation details in the appendix and code, describe which stochastic transform functions were used and how hyperparameter ranges can be handled, and acknowledge limitations and willingness to add more details — but they do not fully present full implementation specifics in the main text and admit some details were left to appendices due to space, so the reviewer's concern is only partially addressed.", "evidence": "“Please see Appendix B where we include the introduction of the stochastic transform functions.” | “In our experiments, we mainly use the five stochastic transform functions, which are universal graph augmentations as listed in Zhao et al., (2021) [3]. In our code implementation, we have also included additional transform functions as shown in Appendix B.” | “Due to the space limitation, we had to include most of the implementation details in the appendix while we added pointers in the main paper. However, we will try to make it more detailed and feel free to let us know if anything is missing!”", "combined_author_response": "Dear reviewer NLg1,\n\nAs the discussion period is closing soon, we hope to engage with you and check if your concerns are addressed.\n\nWe sincerely appreciate your time and attention!\n\nBest,\n\nAuthors of Paper 1560\n\n---\n\nThanks for reading our response! We would also appreciate your patience for reading the two points below \n\n---\n\n**Regarding point #1:**\n\nYes, we agree that the concept of heterogeneous shifts is relatively new for the current studies on distribution shifts, however, this is not new for studies on network patterns (Newman, 2003; Leskovec et al., 2005; 2007; Peel et al., 2017). In fact, the ignorance of such nuanced heterogeneous shifts in the previous studies instead emphasises our motivation and the potential impact of this work. \n\nFrom a causality perspective, distribution shifts can naturally happen in the instance level when (1) additional causal variables, beyond environmental factors and randomized noise, influence these shifts, or (2) multiple causal variables simultaneously affect the shifts with different strengths. Without modeling these explicitly, the mitigation of distribution shifts can easily fail. \n\nIn terms of our presentation, we have illustrated these cases of interest in abstract and introduction, we made further explanations in Section 3.1, we also present the specific results of distribution types in Figure 3 (b). \n\n\n**Regarding point #2:**\n\nWe think we can all agree that, if the ground truth of the instance shifts is available on the real-world datasets, it would be crystal clear to see where the improvement comes from since we can conduct case study to compare our method and the baselines on instances with nuanced distribution shifts, to see the influence of the modeling these heterogeneous shifts. \n\nHowever, with such ground truth not available, we had to seek other seemingly less intuitive but also in-depth way to illustrate the insights (esp Section 4.3) as mentioned above in our previous response. This is also why we designed the synthetic experiments at the first place. We think we did try hard to explain the underlying rationale with the ground truth being absent.\n\n----\n\n**Refinement**: We can of course add a causal graph in the our assumption section to make the concept more clear. And we can illustrate more if you could let us know the specific obstable to understand our mechanism, which will be extremely helpful.\n\n----\n\n### **Summary/TL;DR**\nWe understand the reviewer's clarification concerns, however, we don't agree that they, based on our justificaton, are the cause of rejection. We believe nuanced heterogeneous shifts are common, important, yet being typically ignored in the research domain of distribution shifts, we made these argument clear and also promise to refine. While explaining the underlying rationale is hindered by the lack of ground truth, we did try hard to dissection it from the gating model, the invariant representations generated, and more ablations in the appendix. We respect the reviewer's current opinion. Still, reconsideration will be greatly appreciated.\n\n---\n\nWe thank the reviewer once again! We also enjoyed the process of making our work more sound from your suggestions. And your approval surely means a lot to us.\n\nWe do apologize for not making our solutions towards the limitations clear enough. We summarize them into a short table, hopefully could alleviate your concerns a bit more.\n\n| Limitation| Potential solutions | Location of discussion/action |\n|:---|:---|:---|\n|**Coverage of the transformations** | 1) Extend the coverage by adding representative transform functions. 2) Or include tranform functions based on domain knowledge or a few samples from target distribution | Appendix F, paragraph #3|\n|**Complexity of the transformations** | Make experts dedicated to different hyperparameters for the same type of transformation | Argument options are available in our codebase. Will make it more detailed in the experimental settings. |\n|**Label distributional shift**|Integrate the objective of the existing methods studying labe distributional shifts into our framework. |Appendix F, paragraph #4 |\n\n----\n\nWe completely agree on the existence of these limitations and we will move some limitations (esp #1) to the main paper in our final version. While they could be important in the practice, they are, in our perspective, fair \"side effects\" considering the benefits (i.e., mitigating multiple and nuanced distribution shifts and better interpretability), and may not be the central part of our novelty and main contribution (i.e., the proposal of an equivalent mixture, the concept of referential invariant representations, as well as the training framework). We are also eager to further improve other aspects.\n\nOnce again thank you so much for your support!!\n\n---\n\nDear Reviewer m8JS,\n\nA gentle nudge that we would like to know if our response adequately addresses your concerns. \n\nYour time and feedback is greatly appreciated!\n\nThank you sincerely,\n\nAuthors of Paper 1560\n\n---\n\nWe deeply appreciate your approval. Your suggestions definitely inspired us a lot and have greatly improved our work.\n\nA few additional notes for the further comments:\n\n- Regarding **point #2**: In node classification tasks, the BCE objective already considers node embeddings other than the graph embedding of the k-hop subgraph. What we described previously refers to the computational graph (i.e., k-hop message passing), which generates node embeddings. We apologize for any confusion.\n\n- Regarding **point #3**: Yes it would be quite interesting to see how graph pretraining methods perform on the current OOD benchmarks. Graph pretraining methods like GCC and GPT-GNN also consider graph extrapolation to some extent, e.g., through subgraph extraction and masked attributes/structures. The key difference between graph pretraining and generalization may lie in their different focuses on **expressiveness** and **invariance**. While these two aspects do not always conflict, ensuring invariance w.r.t. a certain type of extrapolation might affect expressiveness (if the change is relevant to labels), and vice versa. To seek a balance in between, one might need prior knowledge in which types of transformations may or may not be sensitive to the labels (and perhaps build experts with different goals to make ensure invariance or expressivess). We believe there is still a lot to explore in this domain.\n\nOverall, we are grateful for your positive stand on this work. We believe that our [current version](https://openreview.net/pdf?id=QQ5eVDIMu4), incorporating opinions from you and other reviewers, is sound and well-refined. We are also committed to further improving it. We would appreciate your support based on our current version! \n\nThank you!\n\n---\n\nDear Reviewers of Paper 1560,\n\nWe hope this message finds you well.\n\nAs the discussion phase approaches its end, we hope you find our responses useful. We would like to ask if the issues have been addressed.\n\nWe understand that the discussion time is short, and some of you might be enjoying holidays at the moment. We apologize for posting our responses a bit late as we aimed to address your concerns clearly.\n\nWe sincerely appreciate your time and attention!\n\nBest regards,\n\nAuthors of Paper 1560\n\n---\n\n**Comment 4: Related works on graph transfer learning.**\n\nYes! We agree that graph transfer learning is a relevant topic. Thank you for providing the additional related work! We added the discussion on these works in our revision, and we repeat it here for your convenience:\n\n```It is also worth mentioning that graph domain adaptation (Zhang et al., 2019; Wu et al., 2020), different from the problem studied in this work, commonly relies on limited labeled samples from target datasets for improved transferability. For instance, to generate domain adaptive network embedding, DANE (Zhang et al., 2019) uses shared weight graph convolutional networks and adversarial learning regularization, while UDA-GCN (Wu et al., 2020) employs attention mechanisms to merge global and local consistencies.```\n\nMoreover, we believe this is actually relevant to `Comment & Question 1` regarding the generality of the distribution shifts covered by the transform functions. In our future works, we added the following discussion:\n\n```...Leveraging a few samples from target distribution (i.e., domain adaptation). Specifically, we can leverage the samples from the target distribution to inform the selection or construction of transform functions, which can better guarantee the distribution shifts are covered by the transform functions.  For example, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space. We believe this would be an interesting future direction.```\n\n---\n\n# Summary\n\nOnce again thank you very much for helping us position our work and reflect the literature better. We hope all of your concerns are solved and we are happy to engage further if there are any other points we missed!\n\n---\n\n### **Reference**\n\n[1] Unsupervised Domain Adaptive Graph Convolutional Networks. Wu et al. WWW 2020\n\n[2] DANE: Domain adaptive network embedding. Zhang et al. IJCAI 2019.\n\n[3] Tong Zhao, Yozen Liu, Leonardo Neves, Oliver J. Woodford, Meng Jiang, and Neil Shah. Data augmentation for graph neural networks. AAAI, 2021.\n\n---\n\nWe are grateful for your positive feedback and detailed suggestions! We provide responses below to address your remaining concerns. \n\n---\n\n### **Comment & Question 1: Generality of the distribution shifts covered by the transform functions.**\n\nGood question! We provide the response from two angles:\n\n\n**(a) For general domain**: In our experiments, we mainly use the five stochastic transform functions, which are universal graph augmentations as listed in Zhao et al., (2021) [3]. In our code implementation, we have also included additional transform functions as shown in Appendix B. We believe these transform functions, while not exhaustive, still cover a wide range of distribution shifts observing from our experimental results.\n\nNevertheless, we agree that the real graph distribution shifts can go beyond any possible combinations of the predefined transform functions. In that case, the assumption may not hold, meaning that GraphMETRO may not capture and precisely mitigate the unknown distribution shift. This scenario could always possibly exist due to the lack of information about the testing distribution or its domain knowledge. We include it as a limitation in Appendix 5, while we further discuss how we could alleviate the problem with additional information.\n\n**(b) For specific domains where additional knowledge is available**: In fact, knowing the tendency of the distribution shifts, such as increasing malicious users in a trading system, would be very helpful in constructing the transform functions that can cover the target distribution shifts well. We believe that such knowledge can come from two sources: \n- **Domain knowledge**, e.g., on molecular datasets, the transform function could be adding additional carbon structure to a molecule (while preserving its functional groups). Or, in a particular social network, transform functions can be defined from known user behaviors. \n- **Leveraging a few samples from target distribution**. This is in fact in line with the reviewer’s Comment #4 regarding graph transfer learning. Specifically, with the guide from a few target samples, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space.\n\n We hope this response can alleviate your concern about our applicability.\n\n---\n\n### **Comment & Question 2: The mechanism of GaphMETRO in handling heterogeneous shifts within one graph.**\n\nGood question! For simplicity, suppose we have two transform functions, i.e., adding edges and dropping edges. Given a node classification task and the objective in Eq. (3), the model is trained on the extrapolated datasets based on the transform functions. After that, given an unseen graph with imbalanced distribution shifts, the gating model outputs scores to identify nodes likely to experience increasing or decreasing degrees, while others might adhere to the original distribution. Then, for each node, each expert takes its multihop subgraph, and outputs its referential invariant representation w.r.t. the correposing transform function. These expert outputs and gating model results are then aggregated to form the final representation. Assuming accurate predictions by the gating function, nodes in denser/sparser subgraphs are represented by expert models corresponding to adding/dropping edges. As each expert is trained to create invariant representations, the final node representations remain unaffected by their individual distribution shifts.\n\n---\n\n### **Comment 3: How to leverage graph pretraining.**\n\nIf we understand correctly, the reviewer was asking how to leverage a pretrained model to further aid the training of GraphMETRO. Please let us know if otherwise. \n\nThis is in fact an interesting point! We believe a model pretrained on a wide variety of data can be very helpful to initialize the gating model, which is required to output the mixture of a node or graph (after it is finetuned on the extrapolated dataset). By enhancing the gating model's predictive capability regarding mixtures, GraphMETRO's final representation should become more resilient. This becomes particularly advantageous when dealing with graphs not previously encountered in the extrapolated dataset. Moreover, it is possible that the pretrained model will also benefit the expert models, while one minor concern would be that the expert model may tend to be similar instead of dedicating to generating invariant representation w.r.t. their corresponding transform function. We added the above discussion to Appendix F as a future work. Thanks again for this comment!\n\n---\n\n### **Question 4: Why does the test accuracy vary across different transformations?**\n\nGreat question! Here we summarize three possible reasons:\n\n- **Information Preservation in Transformations:** Certain transformations retain more informative features than others. For instance, in the REDDIT-BINARY (graph classification task), the random subgraph transformation may retain more graph label-related information compared to dropping edges, as the latter tends to lose more global information. This discrepancy in testing performance, where dropping edges outperforms random subgraph extraction, could be due to the preservation of crucial information. However, conclusions may vary across datasets or tasks depending on how information influences final predictions. For CiteSeer (a node classification task), a random subgraph might preserve more local node information, potentially explaining why its testing performance surpasses dropping edges in this specific task.\n\n- **Complexity of transformation:** Certain transformations inherently generate more diverse graphs than others. If the model lacks the expressive capacity to capture such diversity, it may lead to a decline in testing performance.\n\n- **Model Sensitivity:** Certain transformations may be easier for a model to learn due to compatibility with specific model architectures. This extends beyond transformation complexity and emphasizes how different model architectures may prefer learning particular distributions from one of the extrapolated datasets, which can also contribute to the difference in the testing performance.\n\nWe included the above discussion to Appendix F: Open Discussions. We hope this response can answer your question and improve the soundness of our work.\n\n---\n\n# Summary \n\nWe are grateful for your time and insightful suggestions! \n\nWe would like to highlight that our main contribution is framing the graph generalization problem on top of an equivalent mixture, a simple yet novel and tractable \"middle ground\", as well as proposing the training framework which effectively guarantees the generalization. While our method relies on a set of predefined transform functions, we believe they cover a wide range of distribution shifts based on our empirical results. Also, we agree that there could be some scenarios where the transform functions may not cover complex distributions, and we discuss two future directions and include them into our future works. Moreover, while selecting the hyperparameters for the transform functions introduce extra complexity, the issue could be minor in practice and we also conduct more experiments to justify the applicability better. Finally, we address several questions about clarification and presentation, as well as including more future works. \n\nLastly, we prudently ask you to reevaluate our work given the clarification in our responses, which we also updated our paper correspondingly. Overall, we believe our work makes good contributions to the field of graph distribution learning by proposing a novel and effective solution, and we would appreciate your reconsideration on this point. Thank you for your efforts again!\n\n---\n\n### **Reference**\n\n[1] Menon, Aditya Krishna, et al. \"Long-tail learning via logit adjustment.\" International Conference on Learning Representations. 2020.\n\n[2] Cao, Kaidi, et al. \"Learning imbalanced datasets with label-distribution-aware margin loss.\" Advances in neural information processing systems 32 (2019).\n\n[3] DARTS: Differentiable Architecture Search. Hanxiao Liu, Karen Simonyan, Yiming Yang. 2018.\n\n---\n\n**(3) What is the complexity of the transform functions and how does it affect generalization?** \n\nInteresting question! In fact, our implementation and framework could easily avoid selecting hyperparameters on the transform functions. Specifically, we can make multiple transform functions of the same type with different ranges of hyperparameters. Specifically, GraphMETRO allows three edge dropping transform functions, $\\tau_1^{\\alpha_1}, \\tau_2^{\\alpha_2}, \\tau_3^{\\alpha_3}$, where $\\alpha_i$ (i=1, 2, 3) are three different ranges of edge dropping probabilities, e.g., [0.1,0.3], [0.3, 0.6], [0.6,0.9], representing different transform extents. Thus, given an input from the validation dataset, the gating model will highlight the transform function which simultaneously selects the corresponding hyperparameter that matches the distribution of the validation set. Interestingly, this idea is in the same spirit as how DARTS [3]  proposes to perform architecture search by formulating the task in a differentiable manner. \n\nIn our previous experiments, we did try this scheme where we replaced a single edge dropping transform function with the ratio range [0.3, 0.5] to three transform functions as mentioned above. While we didn’t see a significantly different performance in that case, we believe this would be a flexible solution which avoids the need to conduct hyperparameter selection. \n\nWe include the above discussion in our open discussion and future works (Appendix F). We hope this response can alleviate your concern about the applicability of our method. \n\n---\n\n### **Comment 4: Limitations of our work**\n\nGreat point! We think the issue of label distributional shift, while important, is orthogonal and complementary to the focus of our current study. To elaborate, label distributional shifts exert analogous impacts across various modalities, such as graphs or images. Moreover, existing methods [1,2] designed to tackle label distributional shifts can be seamlessly integrated into our proposed framework. Such integration would necessitate minimal adjustments, potentially involving modifications to the loss function or the training pipeline. We added this as a future work in Appendix F.\n\n---\n\n### **Question 1: Statistical significance of the results on Table 1**\n\nThanks for the question! We compute the p-value of our method against the best baselines method as follows:\n\n| | WebKB | Twitch | Twitter | SST2 | \n|:--|:--|:--|:--|:--|\n|p-value|< 0.001| 0.023 | 0.042| 0.081|\n\nGiven the cut-off threshold as 0.05, we believe the performances of GraphMETRO are statistically significant on WekGB, Twitch, and Twitter datasets, while on the SST2, we see relatively weak evidence. We added the p-value results to our revision and hope our response can alleviate your concern on our improvements.\n\n---\n\n###  **Question 2: Can we say we use ERM for the node classification even if nodes and their labels are not i.i.d.?** \n\nIf we understand correctly, the reviewer is asking for clarification on the 2nd term of our objective. Please let us know if otherwise. Here, our thinking is that the cross-entropy loss for node classification already assumes node labels are conditionally independent given the model (the negative log-likelihood is a sum over the labeled nodes in training). Then, we use the same assumption of cross-entropy on Empirical Risk Minimization (ERM). That is, for a given model we must also minimize the error variance across nodes. The task is then to find the model with the best performance and small variance.\n\n---\n\n### **Question 3: Numerical results on Figure 2**\n\nThanks for pointing it out! We included all of the numerical results of Figure 2 in Appendix E, while showing the results on DBLP below.\n\n| |i.i.d. (0)|noisy feature (1)|add edge (2)|drop edge (3)|drop node (4)|random subgraph (5)|\n|:--|:--|:--|:--|:--|:--|:--|\n|ERM | 85.71 | 84.48 | 71.08 | 79.69 | 83.41 | 76.9|\n|ERM-Aug | 85.66 | 85.29 | 74.85 | 82.34 | 84.44 | 72.81|\n|GraphMETRO | 85.92 | 85.78 | 76.61 | 82.95 | 84.98 | 81.32|\n\n| |(4, 5)|(3, 5)|(2, 5)|(1, 5)|(2, 4)|(1, 4)|     (2, 3)|(1, 3)|\n|:--|:--|:--|:--|:--|:--|:--|:--|:--|\n|ERM | 70.4 | 77.63 | 81.99 | 79.69 | 70.55 | 71.52 | 77.73 | 79.59|\n|ERM-Aug | 74.16 | 81.04 | 83.65 | 68.62 | 74.01 | 68.27 | 81.13 | 84.49|\n|GraphMETRO | 76.18 | 81.71 | 84.26 | 80.31 | 75.1 | 71.05 | 81.85 | 87.14|\n\nAcross all of the synthetic environments, GraphMETRO averagely outperforms ERM and ERM-Aug by 3.20% and 2.45%, respectively.\n\n---\n\nWe appreciate your comments! To address your concerns, below we prudently justify the assumption of our method, the predefined transformation functions, as well as their complexity, and clarify our presented results.\n\n---\n\n### **Comment 1-3: Applicability and our assumptions**\n\nThanks for these great comments! Here we provide response in three folds:\n\n**(1) How does GraphMETRO identify all distribution shifts from transform functions if they are treated independently when combined together?** \n\nIf we understand correctly, by “treated independently”, the reviewer is referring to the first term in our objective $\\text{BCE}(\\phi(\\tau^{(k)}(\\mathcal{G})), Y (\\tau^{(k)}))$, where we formulate predicting the distribution shifts types of a jointly transformed graph as a binary multiclass classification problem. We believe the difficulty of this task comes from both the property of transform functions and the expressiveness of the gating model. \n\n- Firstly, some transform functions are **inherently disentangled**, e.g., adding nodes feature noise and random subgraph extraction. In this case, there will be certain distinction between any pair from these three data distributions, i.e., (graphs with node noise, random subgraph graphs, random subgraphs with node noise), which the gating model can easily tell. \n- While some transform functions can be **essentially similar**, e.g., drop path and drop edges, this won’t affect the performance of our method as long as each expert outputs the corresponding invariant representation. \n- Lastly, indeed, there could be more **complex combinations of the transform functions**, which poses challenges to the gating model’s expressiveness in identifying the combinations. However, this challenge may be minor in the practice. Specifically, we observe fairly high accuracy performances of the gating model, which are above 85% and 73% averagely on extrapolated datasets with one transformation and multiple transformations, respectively.\n\nWe added the above discussion to Appendix F to enable a more comprehensive view towards our methodology. We hope this can alleviate your concern on our gating model’s performance in identifying the distribution shift types.\n\n**(2) How does the predefined transform functions cover complex distributions causing the distribution shift?**\n\nThis is also a great question! We believe there are two angles for this question. \n\n**(a) For general domain**: In our experiments, we mainly use the five stochastic transform functions, which are universal graph augmentations as listed in Zhao et al., (2021) [3]. In our code implementation, we have also included additional transform functions as shown in Appendix B. We believe these transform functions, while not exhaustive, still cover a wide range of distribution shifts observing from our experimental results.\n\nNevertheless, we agree that the real graph distribution shifts can go beyond any possible combinations of the predefined transform functions. In that case, the assumption may not hold, meaning that GraphMETRO may not capture and precisely mitigate the unknown distribution shift. This scenario could always possibly exist due to the lack of information about the testing distribution or its domain knowledge. We include it as a limitation in Appendix 5, while we further discuss how we could alleviate the problem with additional information.\n\n**(b) For specific domains where additional knowledge is available**: In fact, knowing the tendency of the distribution shifts, such as increasing malicious users in a trading system, would be very helpful in constructing the transform functions that can cover the target distribution shifts well. We believe that such knowledge can come from two sources: \n- **Domain knowledge**, e.g., on molecular datasets, the transform function could be adding additional carbon structure to a molecule (while preserving its functional groups). Or, in a particular social network, transform functions can be defined from known user behaviors. \n- **Leveraging a few samples from target distribution**. Specifically, with the guide from a few target samples, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space.\n\n---\n\n[1] Sui et al. Unleashing the Power of Graph Data Augmentation on Covariate Distribution Shift. NeurIPS 2023.\n\n[2] Nianzu Yang et al. Learning substructure invariance for out-of-distribution molecular representations, NeurIPS 2022\n\n[3] Yongduo Sui et al. Causal Attention for Interpretable and Generalizable Graph Classification, KDD 2022.\n\n[4] Jiaqi Ma et al. Subgroup Generalization and Fairness of Graph Neural Networks, NeurIPS 2022.\n\n---\n\n### **Comment 2: Our assumption**\n\nThanks for this comment! In Section 3.1, we have discussed the assumptions in detail with specifications. We repeat part of it here for your convenience:\n\n```Assumption 1 essentially states that the distribution shifts (whatever they are) can be decomposed into several mixture components of stochastic graph transformations. For example, on a social network dataset, each mixture component can represent different patterns of user behavior or network dynamics shifts. Specifically, one mixture component might correspond to increased user activity, while another could signify a particular trend of interaction within a certain group of users. Such a mixture pattern is common and well-studied in the real-world network datasets (Newman, 2003; Leskovec et al., 2005; 2007; Peel et al., 2017).```\n\nWe hope the analysis provides detailed illustration to the assumption. We are happy to add more discussion if you think anything  is still unclear!\n\n---\n\n### **Comment 2 & 3: Implementation of our method**\n\n- **Model architectures**: \nPlease see Tables 2 and 3 in Appendix A for detailed model architecture information. Specifically, we use the same encoders and classifiers from GOOD benchmarkfor real-world datasets to ensure fair comparisons. We employ backbones based on the best ERM performance for synthetic datasets. Moreover, GraphMETRO is model-agnostic, which consistently improves performance across varied model architectures.\n\n- **Stochastic transformation**: Please see Appendix B where we include the introduction of the stochastic transform functions. \n\n- **Optimization process**: Please see Table 2 and 3 (Appendix A), where we included hyperparameters on each dataset. In the second paragraph after Eq. (3), we also described our training pipeline: `“...we set apart the other loss terms from backpropagating to it to avoid interference with the training of the gating model… We optimize the objective via stochastic gradient descent”`. \n\n- **For a more intricate optimization strategy:** This is a great catch! In fact, we did try to pretrain the gating model for several epochs as warm up before training the whole model in an end-to-end fashion via the objective Eq. in (3). However, we didn’t notice a statistically significant difference in their performance, which can be due to that expert models take more time to convergence (since they need to align with the base model during the training) compared to the gating model.\n\nDue to the space limitation, we had to include most of the implementation details in the appendix while we added pointers in the main paper. However, we will try to make it more detailed and feel free to let us know if anything is missing! \n\n---\n\n### **Comment 2 (Cont.): Understand the functionality, underlying mechanism, and performance gain**\n\nThanks for the comment. For clarity, we provide the following pointers to the paper:\n\n- In Section 4.3, we provide a study to reveal the underlying **mechanism** of GraphMETRO, i.e., each expert excels in generating invariant representations concerning a stochastic transform function, which provide a solid foundation in generating referential invariant representations w.r.t. the specific transformations and further guarantee the generalization.\n\n- In Appendix C, we study the **impact of the MoE architecture** on model performance, which shows that the model performance may decline if the expressiveness of the expert model decreases. \n\n- In Appendix D, we study the **impact of the stochastic transform function** on model performance, where we also provide a detailed discussion of the modeling mechanism.\n\nWe believe the above studies and discussion in our paper provides an in-depth view, highlighting the roles of our objective, architecture, and stochastic transform function.  Please let us know if any of these perspectives is still unclear, and we can further improve our experimental study.\n\n-----\n\n# Summary\n\nWe hope our answers can address all of the concerns. We are happy to follow up if you have any further questions.\n\nWe also prudently ask you to reevaluate our work. To highlight, our motivation is supported by the common fine-grained graph shift heterogeneity, and the fact that most of the previous works could not model such nuanced distribution shifts in an effective and flexible manner. Moreover, we added more related works and modified our statement to position our work better. In general, we believe GraphMETRO is a more general and flexible solution that can mitigate a wider range of distribution shifts, which is backed by the experimental results. Detailed justification on our assumption and implementation details are also available in our paper. \n\nThus, we believe our work makes important contributions and provides a clear presentation. We are happy to discuss more and revise our paper if any concern remains. Thank you for your efforts and we are looking forward to your reply!!\n\n---\n\nWe appreciate your comments! To address your concerns, below we prudently justify the motivation of our proposed method, clarify our assumptions, and provide details regarding our experiment implementation.\n\n---------------------\n\n### **Comment 1: Motivation of this work**\n\nThank you for this comment! We believe there might be a bit of misunderstanding due to our different definitions of *“graph shift heterogeneity”*. We firstly discuss the related works mentioned and then justify our statement:\n\n**(1) Related works**\n\nPlease see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nMoreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you! \n\nSpecifically, Sui et al. [1] proposed a graph data augmentation strategy that alleviates covariate shift by generating diverse and invariant causal features. However, the trainable augmenter they used may not distill diverse augmentations or construct unseen perturbations. Moreover, Sui et al. [1] test its method only on graph classification tasks, while GraphMETRO can be applied to both node and graph classification tasks. Besides, we have discussed graph augmentation and attention-based methods in our related works, and we added more recent works on graph OOD [2,3,4], and we hope our response clears your concern on the related work discussion.\n\n\n**(2) The definition of graph shift heterogeneity**\n\nIn this work, we refer to **“heterogeneous shifts”** as multiple and different levels of shifts which vary across different instances (nodes or graphs), as illustrated in the example in the abstract. While we agree that the existing invariant learning approches can accommodate multiple distribution shifts, it could be hard for them to tackle nuanced distribution shifts for individual instances (nodes or graphs) since the distribution shifts are inferred from variance across multiple data environments. If GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering **a mixture of transformations as a proxy** for the target distribution shifts rather than invariance to whole-graph environment shifts. This is the type of heterogeneity we are interested in our paper. \n\n**(3) Regarding our original motivation statement**\n\nWhile the statement pointed out by the reviewer serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category. And we have provided detailed discussion about three lines of research in the related work section.\n\nHowever, we agree that we could make this statement border to cover the previous invariant learning methods. To improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to \"**previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments…**”. We also modified our introduction correspondingly, we hope this will better position our work.\n\n**(4) The choice of our MoE design**\n\n- The choice of our approach comes as a consequence of our motivation to model the graph/instance shift heterogeneity. As mentioned, mitigating multiple and nuanced distribution shifts simply goes beyond certain distribution shift types or environment construction as seen in the previous methods. \n- Thus, GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts. This enables the prediction of multiple different distribution shifts and the flexibility to model fine-grained heterogeneity since the mixture can be varied across different instances. We then tackled the proxy to mitigate the target distribution shifts. Intuitively, this solution provides a **“middle ground”** to deem graph generalization as an equivalent mixture, which, we believe, is a more tractable solution.\n\nWe updated our paper to make the above point more clear. We genuinely hope our answer can justify the motivation and solve your concern.\n\n---\n\n### **Question 2: How does the computational cost of GraphMETRO compare to other methods?**\n\nPlease see the last paragraph of Section 3.4, where we analyze the computation complexity of GraphMETRO. We repeat part of it here for your convenience:\n\n```Consider the scenario where we use an individual encoder for each expert. The forward process of $f$ involves $O(K)$ forward times using the weighted sum aggregation (or $O(1)$ if using the maximum selection). Since we extend the dataset to $(K + 1)$ times larger than the original data, the computation complexity is $O(K^2 |D_s|)$, where |Ds| is the size of the source dataset.```\n\nThus, the computation cost is about $K^2$ or $K$ times (if using the maximum selection) than an ERM model, where $K=5$ in our experiments. Compared to DIR, as they extract $B$ spurious subgraphs from each batch to conduct the intervention, their computation cost is $B$ times compared to ERM, where $B$ could be 32. Thus, we believe the computation cost of GraphMETRO is fair for the gains we get, considering $K$ is usually small.\n\n----------------------\n\n### **Question 3: Can GraphMETRO handle multiple different types of distribution shifts that simultaneously exist in data?**\n\nYes! The distribution shift types corresponding to the gating outputs with high scores will be tackled during training. That is, if the gating output highlights multiple mixture components, their corresponding distribution shift types will be handled jointly.\n\n----------------------\n\n### **Question 4: Can GraphMETRO  tackle distribution shifts on molecular graphs?**\n\nThat is a great idea! Yes, GraphMETRO can be applied to molecular datasets if one designs transform functions to cover typical molecular variants. For instance, a transform function may add carbon structures to the molecules. These domain-specific transform functions are outside the scope of our work, however, we believe these would be interesting future work directions!\n\n----------------------\n\n# Summary \n\nWe thank the reviewer for the time and insightful suggestions! We hope our answers can address your concerns well.\n\nWe also prudently ask you to reconsider our work if the concerns are addressed. To highlight, our novelty comes from the formulation of an equivalent mixture for graph OOD and the training framework to effectively realize generalization. We also provide an in-depth analysis on our originality compared to some previous invariant learning methods. While we discussed and compared with previous works, we added more related works and modified our statement to position our work better. Finally, our method achieves great improvements on both node and graph classification tasks, and is a more general solution to mitigate multiple and nuanced distribution shifts. \n\nOverall, we believe our work proposes a new paradigm and novel training framework and makes good contributions in the fields of graph generalization, and we would appreciate your reconsideration on this point. Thank you for your efforts again!\n\n----------------------\n\n**Reference**\n\n\n[1-5] The same as listed by the reviewer\n\n\n[6] Beatrice Bevilacqua, Yangze Zhou, and Bruno Ribeiro. Size-invariant graph representations for graph classification extrapolations. In ICML, 2021.\n\n[7] Davide Buffelli, Pietro Li´o, and Fabio Vandin. Sizeshiftreg: a regularization method for improving size-generalization in graph neural networks. In NeurIPS, 2022.\n\n[8] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In NeurIPS, 2019.\n\n[9] Mucong Ding, Kezhi Kong, Jiuhai Chen, John Kirchenbauer, Micah Goldblum, David Wipf, Furong Huang, and Tom Goldstein. A closer look at distribution shifts and out-of-distribution generalization on graphs. In NeurIPS DistShift, 2021.\n\n---\n\n### **Comment 3 & Question 1: Related works on invariant learning and clarification on our statement.**\n\nThanks for pointing it out! Below we clarify the statement and clear potential misunderstanding:\n\n**(1) How does GraphMETRO compare with invariant learning methods like DIR and EERM?** \n\nPlease see our related work section where we discussed these two papers, i.e., DIR (Wu et al., 2022c) and EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on environmental patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nBesides, we also provide a more in-depth comparison in our point (3) below to highlight our key technical originality.\n\n**(2) Regarding our statement about previous works:** \n\nWhile the statement serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category, and we have provided detailed discussion about three lines of research in the related work section.\n\nTo improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to **\"previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments** (which is highly limited when confronted with nuanced distribution shifts)”. We also modified our introduction correspondingly. Thanks for letting us know our statement could be misinterpreted.\n\n**(3) Why do we say our method could be more broad than the existing invariant learning approaches?** \n\n- Invariant subgraph learning approaches, e.g., [1,2], consider variance of constructed data environments, which are designed very differently compared to our work. \nWhile they can accommodate multiple distribution shifts (as in multiple environments), these focus on patterns within each environment and ignore the variety across instances (e.g., shifts at the resolution of nodes), which may not be well-captured by the environment assignments. \n- GraphMETRO considers that specific parts of the test graph may have different shifts. Particularly, our goal is to make the generalization to unknown testing distribution more adaptive and broad, as opposed to limiting the distribution shifts to being invariant to specific types of subgraphs. \n\nIn other words, if GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering a mixture of transformations as a proxy for the target distribution shifts rather than invariance to environment shifts.\n\n**(4) Key technical originality compared to invariance learning (going more deeply)**\n\nAnother interesting view to see the innovation of GraphMETRO is that it breaks the typical invariant learning formulation, which assumes the data is manipulated by the environment variables (and then can be “decoded” into multiple environments). Instead, GraphMETRO sees the distribution shifts on an instance as a mixture, which is represented by the score vector output by the gating function over the basis of the transform functions. In other words, GraphMETRO can produce infinite environments as the elements in the score vector are continuous. One can see that once we limit the output domain of the gating function into, e.g., binary {0, 1}, GraphMETRO can also produce a limited number of environments (if we categorize the instances based on the score vector), which covers the environment construction in invariant learning. Moreover, as mentioned, we propose the concept of referential invariant representation with a base model $\\xi_0$, which is also different from previous works on invariant learning. We added the above discussion to Appendix F to improve the depth of our analysis.\n\n---\n\nWe appreciate your efforts and insightful comments! To address your concerns, we provide point-to-point responses below.\n\n---------------------\n\n### **Comment 1: Regarding the novelty of GraphMETRO.**\n\nThanks for the comment! We believe our novelty comes from the proposal of an equivalent mixture for graph OOD and the construction of our training framework, as detailed below:\n\n- **An equivalent mixture for graph OOD**: The key challenge we faced to mitigate multiple and nuanced distribution shifts is the intrinsic complexity and heterogeneity of graph distribution shifts, which simply goes beyond certain distribution shift types [6,7,8,9] or environment construction as seen in the previous methods [1,2,3]. GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts, where the mixture can be varied across different instances, and then tackled the proxy to mitigate the target distribution shifts. We believe the high-level idea is succinct, nevertheless, novel, in the sense that it provides a “middle ground” to deem graph generalization as an equivalent mixture that is more tractable.\n\n- **Training framework**: With the guide of our formulation, the training framework is still non-trivial due to two problems, i.e., “how to provide supervision for predicting the mixture” and “how to ensure the experts corresponding to mixture components are compatible when working as a whole”. Specifically, GraphMETRO solves the first problem by conducting graph extrapolation. This is somewhat similar to the spirit of graph pretraining in the sense that we inject heterogeneity to promote the expressiveness of the gating model in recognizing the mixture components. For the second problem, we introduce the concept of Referential Invariant Representation, along with the novel objective in Eq. (3) to enforce the invariance and compatibility. It is worth mentioning that the model performance is much worse than the reported numbers (e.g., 2.7\\% lower on Twitch dataset) without the compatibility constraint, indicating the proposed referential invariance concept is indispensable. \n\nWe added more justification in the introduction (updated in the revision). We genuinely hope our responses can solve your concerns about the novelty of our work.\n\n------------------------------\n\n### **Comment 2: Comparison with recent methods [3,4,5].**\n\nIn compacting the paper to fit in the page limit we mistakenly did not include these relevant references, we apologize. We added discussion in the revised version. Here we summarize these works and point out their key differences with our method: \n\n- In particular, **Yang et al. [3]** explore molecule representation learning in out-of-distribution (OOD) scenarios. They achieve this by directing the molecule encoder to utilize stable and environment-invariant substructures relevant to the labels without the need for environmental labels. \n- Similarly, **Sui et al. [4]** introduces causal attention modules to identify key invariant subgraph features that can be described as causing the graph label. The type of OOD task that Sui et al. [4] considers assumes the graph label is caused by a subgraph, which is quite different from ours. Moreover, both Yang et al. [3] and Sui et al. [4] consider tasks where the graph label is caused by a subgraph. \n- **Ma et al.[5]** is an interesting theoretical work which studies GNN generalization and examines their fairness, showing that the test subgroup's distance from the training set impacts GNN performance. Ma et al.[5], as far as we could assess, does not propose any specific architecture to solve the type of OOD tasks we consider in our work.\n\nOverall, the goal of GraphMETRO is to be invariant to a mixture of selected stochastic transform functions (and the mixture can vary across different instances), which is a more flexible and general solution. \nWe added a discussion of these works to our revision, hopefully providing a more comprehensive comparison and literature overview. We hope our responses can solve your concerns about the related work.\n\n---\n\nWe sincerely appreciate all reviewers' time, efforts, and valuable suggestions in reviewing our paper. We are glad that most of the reviewers reached a positive consensus on our work's motivation and experimental results. Here is a summary of our responses:\n\n- **Clarification**: We clarify our key assumptions (`m8JS, FURX`), novelty (`m8JS, NLg1`), and the experimental settings (`m8JS`).\n\n- **Related work**: : We add more related works on recent graph OOD learning (`NLg1, m8JS`) and graph transfer learning (`FURX`)\n\n- **Limitation**: We add discussion about the limitations of GraphMETRO when confronted with unknown distribution shifts that surpass our key assumptions (`FURX`, `C9mM`).\n\nWe hope our responses can clarify your confusion and alleviate concerns and we updated our paper **(highlighted in green)** corespondingly. We thank all reviewers again, and look forward to your reply!", "author_response": "We appreciate your comments! To address your concerns, below we prudently justify the motivation of our proposed method, clarify our assumptions, and provide details regarding our experiment implementation.\n\n---------------------\n\n### **Comment 1: Motivation of this work**\n\nThank you for this comment! We believe there might be a bit of misunderstanding due to our different definitions of *“graph shift heterogeneity”*. We firstly discuss the related works mentioned and then justify our statement:\n\n**(1) Related works**\n\nPlease see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nMoreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you! \n\nSpecifically, Sui et al. [1] proposed a graph data augmentation strategy that alleviates covariate shift by generating diverse and invariant causal features. However, the trainable augmenter they used may not distill diverse augmentations or construct unseen perturbations. Moreover, Sui et al. [1] test its method only on graph classification tasks, while GraphMETRO can be applied to both node and graph classification tasks. Besides, we have discussed graph augmentation and attention-based methods in our related works, and we added more recent works on graph OOD [2,3,4], and we hope our response clears your concern on the related work discussion.\n\n\n**(2) The definition of graph shift heterogeneity**\n\nIn this work, we refer to **“heterogeneous shifts”** as multiple and different levels of shifts which vary across different instances (nodes or graphs), as illustrated in the example in the abstract. While we agree that the existing invariant learning approches can accommodate multiple distribution shifts, it could be hard for them to tackle nuanced distribution shifts for individual instances (nodes or graphs) since the distribution shifts are inferred from variance across multiple data environments. If GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering **a mixture of transformations as a proxy** for the target distribution shifts rather than invariance to whole-graph environment shifts. This is the type of heterogeneity we are interested in our paper. \n\n**(3) Regarding our original motivation statement**\n\nWhile the statement pointed out by the reviewer serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category. And we have provided detailed discussion about three lines of research in the related work section.\n\nHowever, we agree that we could make this statement border to cover the previous invariant learning methods. To improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to \"**previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments…**”. We also modified our introduction correspondingly, we hope this will better position our work.\n\n**(4) The choice of our MoE design**\n\n- The choice of our approach comes as a consequence of our motivation to model the graph/instance shift heterogeneity. As mentioned, mitigating multiple and nuanced distribution shifts simply goes beyond certain distribution shift types or environment construction as seen in the previous methods. \n- Thus, GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts. This enables the prediction of multiple different distribution shifts and the flexibility to model fine-grained heterogeneity since the mixture can be varied across different instances. We then tackled the proxy to mitigate the target distribution shifts. Intuitively, this solution provides a **“middle ground”** to deem graph generalization as an equivalent mixture, which, we believe, is a more tractable solution.\n\nWe updated our paper to make the above point more clear. We genuinely hope our answer can justify the motivation and solve your concern."}
{"claim": "The paper omits important details about the optimization process used to train the model, such as specialized schedules or objectives.", "claim_type": "methodology", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ppMpJyIYRZ", "reviewer": "Reviewer_m8JS", "review_text": "Summary: The paper introduces a method to enhance the out-of-distribution performance of graph neural networks (GNN) by learning to understand distribution shifts instead of addressing the assumed ones. To achieve this, the Mixture of Experts architecture is integrated into the GNN, supplemented by an alignment procedure to recognize the shift. Empirical experiments are conducted to validate the theoretical assertion.\n\nOn the whole, I believe the proposed method lacks the necessary motivation and its novelty isn't substantial enough to meet the standard.\n\nStrengths: - The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\n- Real-world datasets back the claims through experiments.\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\n\nWeaknesses: - The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \"previous research has concentrated on addressing specific types of distribution shifts.\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \"environment generators\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\n\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\n\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\n\nQuestions: Please check Weaknesses.", "labeling_timestamp": "2026-01-11T16:31:57.537719", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors explicitly address the reviewer's concern by pointing to optimization details in the appendix (hyperparameters, training pipeline) and by describing key aspects of the optimization (use of SGD, isolating loss terms from backpropagation, and an attempted gating warm-up). However, they do not provide extensive specialized schedules or full optimization protocols in the main response, relying on appendix pointers and brief descriptions, so the concern is only partially resolved.", "evidence": "\"Optimization process: Please see Table 2 and 3 (Appendix A), where we included hyperparameters on each dataset. In the second paragraph after Eq. (3), we also described our training pipeline: '...we set apart the other loss terms from backpropagating to it to avoid interference with the training of the gating model… We optimize the objective via stochastic gradient descent'.\" \"For a more intricate optimization strategy: This is a great catch! In fact, we did try to pretrain the gating model for several epochs as warm up before training the whole model in an end-to-end fashion via the objective Eq. in (3). However, we didn’t notice a statistically significant difference in their performance...\" \"Due to the space limitation, we had to include most of the implementation details in the appendix while we added pointers in the main paper. However, we will try to make it more detailed and feel free to let us know if anything is missing!\"", "combined_author_response": "Dear reviewer NLg1,\n\nAs the discussion period is closing soon, we hope to engage with you and check if your concerns are addressed.\n\nWe sincerely appreciate your time and attention!\n\nBest,\n\nAuthors of Paper 1560\n\n---\n\nThanks for reading our response! We would also appreciate your patience for reading the two points below \n\n---\n\n**Regarding point #1:**\n\nYes, we agree that the concept of heterogeneous shifts is relatively new for the current studies on distribution shifts, however, this is not new for studies on network patterns (Newman, 2003; Leskovec et al., 2005; 2007; Peel et al., 2017). In fact, the ignorance of such nuanced heterogeneous shifts in the previous studies instead emphasises our motivation and the potential impact of this work. \n\nFrom a causality perspective, distribution shifts can naturally happen in the instance level when (1) additional causal variables, beyond environmental factors and randomized noise, influence these shifts, or (2) multiple causal variables simultaneously affect the shifts with different strengths. Without modeling these explicitly, the mitigation of distribution shifts can easily fail. \n\nIn terms of our presentation, we have illustrated these cases of interest in abstract and introduction, we made further explanations in Section 3.1, we also present the specific results of distribution types in Figure 3 (b). \n\n\n**Regarding point #2:**\n\nWe think we can all agree that, if the ground truth of the instance shifts is available on the real-world datasets, it would be crystal clear to see where the improvement comes from since we can conduct case study to compare our method and the baselines on instances with nuanced distribution shifts, to see the influence of the modeling these heterogeneous shifts. \n\nHowever, with such ground truth not available, we had to seek other seemingly less intuitive but also in-depth way to illustrate the insights (esp Section 4.3) as mentioned above in our previous response. This is also why we designed the synthetic experiments at the first place. We think we did try hard to explain the underlying rationale with the ground truth being absent.\n\n----\n\n**Refinement**: We can of course add a causal graph in the our assumption section to make the concept more clear. And we can illustrate more if you could let us know the specific obstable to understand our mechanism, which will be extremely helpful.\n\n----\n\n### **Summary/TL;DR**\nWe understand the reviewer's clarification concerns, however, we don't agree that they, based on our justificaton, are the cause of rejection. We believe nuanced heterogeneous shifts are common, important, yet being typically ignored in the research domain of distribution shifts, we made these argument clear and also promise to refine. While explaining the underlying rationale is hindered by the lack of ground truth, we did try hard to dissection it from the gating model, the invariant representations generated, and more ablations in the appendix. We respect the reviewer's current opinion. Still, reconsideration will be greatly appreciated.\n\n---\n\nWe thank the reviewer once again! We also enjoyed the process of making our work more sound from your suggestions. And your approval surely means a lot to us.\n\nWe do apologize for not making our solutions towards the limitations clear enough. We summarize them into a short table, hopefully could alleviate your concerns a bit more.\n\n| Limitation| Potential solutions | Location of discussion/action |\n|:---|:---|:---|\n|**Coverage of the transformations** | 1) Extend the coverage by adding representative transform functions. 2) Or include tranform functions based on domain knowledge or a few samples from target distribution | Appendix F, paragraph #3|\n|**Complexity of the transformations** | Make experts dedicated to different hyperparameters for the same type of transformation | Argument options are available in our codebase. Will make it more detailed in the experimental settings. |\n|**Label distributional shift**|Integrate the objective of the existing methods studying labe distributional shifts into our framework. |Appendix F, paragraph #4 |\n\n----\n\nWe completely agree on the existence of these limitations and we will move some limitations (esp #1) to the main paper in our final version. While they could be important in the practice, they are, in our perspective, fair \"side effects\" considering the benefits (i.e., mitigating multiple and nuanced distribution shifts and better interpretability), and may not be the central part of our novelty and main contribution (i.e., the proposal of an equivalent mixture, the concept of referential invariant representations, as well as the training framework). We are also eager to further improve other aspects.\n\nOnce again thank you so much for your support!!\n\n---\n\nDear Reviewer m8JS,\n\nA gentle nudge that we would like to know if our response adequately addresses your concerns. \n\nYour time and feedback is greatly appreciated!\n\nThank you sincerely,\n\nAuthors of Paper 1560\n\n---\n\nWe deeply appreciate your approval. Your suggestions definitely inspired us a lot and have greatly improved our work.\n\nA few additional notes for the further comments:\n\n- Regarding **point #2**: In node classification tasks, the BCE objective already considers node embeddings other than the graph embedding of the k-hop subgraph. What we described previously refers to the computational graph (i.e., k-hop message passing), which generates node embeddings. We apologize for any confusion.\n\n- Regarding **point #3**: Yes it would be quite interesting to see how graph pretraining methods perform on the current OOD benchmarks. Graph pretraining methods like GCC and GPT-GNN also consider graph extrapolation to some extent, e.g., through subgraph extraction and masked attributes/structures. The key difference between graph pretraining and generalization may lie in their different focuses on **expressiveness** and **invariance**. While these two aspects do not always conflict, ensuring invariance w.r.t. a certain type of extrapolation might affect expressiveness (if the change is relevant to labels), and vice versa. To seek a balance in between, one might need prior knowledge in which types of transformations may or may not be sensitive to the labels (and perhaps build experts with different goals to make ensure invariance or expressivess). We believe there is still a lot to explore in this domain.\n\nOverall, we are grateful for your positive stand on this work. We believe that our [current version](https://openreview.net/pdf?id=QQ5eVDIMu4), incorporating opinions from you and other reviewers, is sound and well-refined. We are also committed to further improving it. We would appreciate your support based on our current version! \n\nThank you!\n\n---\n\nDear Reviewers of Paper 1560,\n\nWe hope this message finds you well.\n\nAs the discussion phase approaches its end, we hope you find our responses useful. We would like to ask if the issues have been addressed.\n\nWe understand that the discussion time is short, and some of you might be enjoying holidays at the moment. We apologize for posting our responses a bit late as we aimed to address your concerns clearly.\n\nWe sincerely appreciate your time and attention!\n\nBest regards,\n\nAuthors of Paper 1560\n\n---\n\n**Comment 4: Related works on graph transfer learning.**\n\nYes! We agree that graph transfer learning is a relevant topic. Thank you for providing the additional related work! We added the discussion on these works in our revision, and we repeat it here for your convenience:\n\n```It is also worth mentioning that graph domain adaptation (Zhang et al., 2019; Wu et al., 2020), different from the problem studied in this work, commonly relies on limited labeled samples from target datasets for improved transferability. For instance, to generate domain adaptive network embedding, DANE (Zhang et al., 2019) uses shared weight graph convolutional networks and adversarial learning regularization, while UDA-GCN (Wu et al., 2020) employs attention mechanisms to merge global and local consistencies.```\n\nMoreover, we believe this is actually relevant to `Comment & Question 1` regarding the generality of the distribution shifts covered by the transform functions. In our future works, we added the following discussion:\n\n```...Leveraging a few samples from target distribution (i.e., domain adaptation). Specifically, we can leverage the samples from the target distribution to inform the selection or construction of transform functions, which can better guarantee the distribution shifts are covered by the transform functions.  For example, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space. We believe this would be an interesting future direction.```\n\n---\n\n# Summary\n\nOnce again thank you very much for helping us position our work and reflect the literature better. We hope all of your concerns are solved and we are happy to engage further if there are any other points we missed!\n\n---\n\n### **Reference**\n\n[1] Unsupervised Domain Adaptive Graph Convolutional Networks. Wu et al. WWW 2020\n\n[2] DANE: Domain adaptive network embedding. Zhang et al. IJCAI 2019.\n\n[3] Tong Zhao, Yozen Liu, Leonardo Neves, Oliver J. Woodford, Meng Jiang, and Neil Shah. Data augmentation for graph neural networks. AAAI, 2021.\n\n---\n\nWe are grateful for your positive feedback and detailed suggestions! We provide responses below to address your remaining concerns. \n\n---\n\n### **Comment & Question 1: Generality of the distribution shifts covered by the transform functions.**\n\nGood question! We provide the response from two angles:\n\n\n**(a) For general domain**: In our experiments, we mainly use the five stochastic transform functions, which are universal graph augmentations as listed in Zhao et al., (2021) [3]. In our code implementation, we have also included additional transform functions as shown in Appendix B. We believe these transform functions, while not exhaustive, still cover a wide range of distribution shifts observing from our experimental results.\n\nNevertheless, we agree that the real graph distribution shifts can go beyond any possible combinations of the predefined transform functions. In that case, the assumption may not hold, meaning that GraphMETRO may not capture and precisely mitigate the unknown distribution shift. This scenario could always possibly exist due to the lack of information about the testing distribution or its domain knowledge. We include it as a limitation in Appendix 5, while we further discuss how we could alleviate the problem with additional information.\n\n**(b) For specific domains where additional knowledge is available**: In fact, knowing the tendency of the distribution shifts, such as increasing malicious users in a trading system, would be very helpful in constructing the transform functions that can cover the target distribution shifts well. We believe that such knowledge can come from two sources: \n- **Domain knowledge**, e.g., on molecular datasets, the transform function could be adding additional carbon structure to a molecule (while preserving its functional groups). Or, in a particular social network, transform functions can be defined from known user behaviors. \n- **Leveraging a few samples from target distribution**. This is in fact in line with the reviewer’s Comment #4 regarding graph transfer learning. Specifically, with the guide from a few target samples, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space.\n\n We hope this response can alleviate your concern about our applicability.\n\n---\n\n### **Comment & Question 2: The mechanism of GaphMETRO in handling heterogeneous shifts within one graph.**\n\nGood question! For simplicity, suppose we have two transform functions, i.e., adding edges and dropping edges. Given a node classification task and the objective in Eq. (3), the model is trained on the extrapolated datasets based on the transform functions. After that, given an unseen graph with imbalanced distribution shifts, the gating model outputs scores to identify nodes likely to experience increasing or decreasing degrees, while others might adhere to the original distribution. Then, for each node, each expert takes its multihop subgraph, and outputs its referential invariant representation w.r.t. the correposing transform function. These expert outputs and gating model results are then aggregated to form the final representation. Assuming accurate predictions by the gating function, nodes in denser/sparser subgraphs are represented by expert models corresponding to adding/dropping edges. As each expert is trained to create invariant representations, the final node representations remain unaffected by their individual distribution shifts.\n\n---\n\n### **Comment 3: How to leverage graph pretraining.**\n\nIf we understand correctly, the reviewer was asking how to leverage a pretrained model to further aid the training of GraphMETRO. Please let us know if otherwise. \n\nThis is in fact an interesting point! We believe a model pretrained on a wide variety of data can be very helpful to initialize the gating model, which is required to output the mixture of a node or graph (after it is finetuned on the extrapolated dataset). By enhancing the gating model's predictive capability regarding mixtures, GraphMETRO's final representation should become more resilient. This becomes particularly advantageous when dealing with graphs not previously encountered in the extrapolated dataset. Moreover, it is possible that the pretrained model will also benefit the expert models, while one minor concern would be that the expert model may tend to be similar instead of dedicating to generating invariant representation w.r.t. their corresponding transform function. We added the above discussion to Appendix F as a future work. Thanks again for this comment!\n\n---\n\n### **Question 4: Why does the test accuracy vary across different transformations?**\n\nGreat question! Here we summarize three possible reasons:\n\n- **Information Preservation in Transformations:** Certain transformations retain more informative features than others. For instance, in the REDDIT-BINARY (graph classification task), the random subgraph transformation may retain more graph label-related information compared to dropping edges, as the latter tends to lose more global information. This discrepancy in testing performance, where dropping edges outperforms random subgraph extraction, could be due to the preservation of crucial information. However, conclusions may vary across datasets or tasks depending on how information influences final predictions. For CiteSeer (a node classification task), a random subgraph might preserve more local node information, potentially explaining why its testing performance surpasses dropping edges in this specific task.\n\n- **Complexity of transformation:** Certain transformations inherently generate more diverse graphs than others. If the model lacks the expressive capacity to capture such diversity, it may lead to a decline in testing performance.\n\n- **Model Sensitivity:** Certain transformations may be easier for a model to learn due to compatibility with specific model architectures. This extends beyond transformation complexity and emphasizes how different model architectures may prefer learning particular distributions from one of the extrapolated datasets, which can also contribute to the difference in the testing performance.\n\nWe included the above discussion to Appendix F: Open Discussions. We hope this response can answer your question and improve the soundness of our work.\n\n---\n\n# Summary \n\nWe are grateful for your time and insightful suggestions! \n\nWe would like to highlight that our main contribution is framing the graph generalization problem on top of an equivalent mixture, a simple yet novel and tractable \"middle ground\", as well as proposing the training framework which effectively guarantees the generalization. While our method relies on a set of predefined transform functions, we believe they cover a wide range of distribution shifts based on our empirical results. Also, we agree that there could be some scenarios where the transform functions may not cover complex distributions, and we discuss two future directions and include them into our future works. Moreover, while selecting the hyperparameters for the transform functions introduce extra complexity, the issue could be minor in practice and we also conduct more experiments to justify the applicability better. Finally, we address several questions about clarification and presentation, as well as including more future works. \n\nLastly, we prudently ask you to reevaluate our work given the clarification in our responses, which we also updated our paper correspondingly. Overall, we believe our work makes good contributions to the field of graph distribution learning by proposing a novel and effective solution, and we would appreciate your reconsideration on this point. Thank you for your efforts again!\n\n---\n\n### **Reference**\n\n[1] Menon, Aditya Krishna, et al. \"Long-tail learning via logit adjustment.\" International Conference on Learning Representations. 2020.\n\n[2] Cao, Kaidi, et al. \"Learning imbalanced datasets with label-distribution-aware margin loss.\" Advances in neural information processing systems 32 (2019).\n\n[3] DARTS: Differentiable Architecture Search. Hanxiao Liu, Karen Simonyan, Yiming Yang. 2018.\n\n---\n\n**(3) What is the complexity of the transform functions and how does it affect generalization?** \n\nInteresting question! In fact, our implementation and framework could easily avoid selecting hyperparameters on the transform functions. Specifically, we can make multiple transform functions of the same type with different ranges of hyperparameters. Specifically, GraphMETRO allows three edge dropping transform functions, $\\tau_1^{\\alpha_1}, \\tau_2^{\\alpha_2}, \\tau_3^{\\alpha_3}$, where $\\alpha_i$ (i=1, 2, 3) are three different ranges of edge dropping probabilities, e.g., [0.1,0.3], [0.3, 0.6], [0.6,0.9], representing different transform extents. Thus, given an input from the validation dataset, the gating model will highlight the transform function which simultaneously selects the corresponding hyperparameter that matches the distribution of the validation set. Interestingly, this idea is in the same spirit as how DARTS [3]  proposes to perform architecture search by formulating the task in a differentiable manner. \n\nIn our previous experiments, we did try this scheme where we replaced a single edge dropping transform function with the ratio range [0.3, 0.5] to three transform functions as mentioned above. While we didn’t see a significantly different performance in that case, we believe this would be a flexible solution which avoids the need to conduct hyperparameter selection. \n\nWe include the above discussion in our open discussion and future works (Appendix F). We hope this response can alleviate your concern about the applicability of our method. \n\n---\n\n### **Comment 4: Limitations of our work**\n\nGreat point! We think the issue of label distributional shift, while important, is orthogonal and complementary to the focus of our current study. To elaborate, label distributional shifts exert analogous impacts across various modalities, such as graphs or images. Moreover, existing methods [1,2] designed to tackle label distributional shifts can be seamlessly integrated into our proposed framework. Such integration would necessitate minimal adjustments, potentially involving modifications to the loss function or the training pipeline. We added this as a future work in Appendix F.\n\n---\n\n### **Question 1: Statistical significance of the results on Table 1**\n\nThanks for the question! We compute the p-value of our method against the best baselines method as follows:\n\n| | WebKB | Twitch | Twitter | SST2 | \n|:--|:--|:--|:--|:--|\n|p-value|< 0.001| 0.023 | 0.042| 0.081|\n\nGiven the cut-off threshold as 0.05, we believe the performances of GraphMETRO are statistically significant on WekGB, Twitch, and Twitter datasets, while on the SST2, we see relatively weak evidence. We added the p-value results to our revision and hope our response can alleviate your concern on our improvements.\n\n---\n\n###  **Question 2: Can we say we use ERM for the node classification even if nodes and their labels are not i.i.d.?** \n\nIf we understand correctly, the reviewer is asking for clarification on the 2nd term of our objective. Please let us know if otherwise. Here, our thinking is that the cross-entropy loss for node classification already assumes node labels are conditionally independent given the model (the negative log-likelihood is a sum over the labeled nodes in training). Then, we use the same assumption of cross-entropy on Empirical Risk Minimization (ERM). That is, for a given model we must also minimize the error variance across nodes. The task is then to find the model with the best performance and small variance.\n\n---\n\n### **Question 3: Numerical results on Figure 2**\n\nThanks for pointing it out! We included all of the numerical results of Figure 2 in Appendix E, while showing the results on DBLP below.\n\n| |i.i.d. (0)|noisy feature (1)|add edge (2)|drop edge (3)|drop node (4)|random subgraph (5)|\n|:--|:--|:--|:--|:--|:--|:--|\n|ERM | 85.71 | 84.48 | 71.08 | 79.69 | 83.41 | 76.9|\n|ERM-Aug | 85.66 | 85.29 | 74.85 | 82.34 | 84.44 | 72.81|\n|GraphMETRO | 85.92 | 85.78 | 76.61 | 82.95 | 84.98 | 81.32|\n\n| |(4, 5)|(3, 5)|(2, 5)|(1, 5)|(2, 4)|(1, 4)|     (2, 3)|(1, 3)|\n|:--|:--|:--|:--|:--|:--|:--|:--|:--|\n|ERM | 70.4 | 77.63 | 81.99 | 79.69 | 70.55 | 71.52 | 77.73 | 79.59|\n|ERM-Aug | 74.16 | 81.04 | 83.65 | 68.62 | 74.01 | 68.27 | 81.13 | 84.49|\n|GraphMETRO | 76.18 | 81.71 | 84.26 | 80.31 | 75.1 | 71.05 | 81.85 | 87.14|\n\nAcross all of the synthetic environments, GraphMETRO averagely outperforms ERM and ERM-Aug by 3.20% and 2.45%, respectively.\n\n---\n\nWe appreciate your comments! To address your concerns, below we prudently justify the assumption of our method, the predefined transformation functions, as well as their complexity, and clarify our presented results.\n\n---\n\n### **Comment 1-3: Applicability and our assumptions**\n\nThanks for these great comments! Here we provide response in three folds:\n\n**(1) How does GraphMETRO identify all distribution shifts from transform functions if they are treated independently when combined together?** \n\nIf we understand correctly, by “treated independently”, the reviewer is referring to the first term in our objective $\\text{BCE}(\\phi(\\tau^{(k)}(\\mathcal{G})), Y (\\tau^{(k)}))$, where we formulate predicting the distribution shifts types of a jointly transformed graph as a binary multiclass classification problem. We believe the difficulty of this task comes from both the property of transform functions and the expressiveness of the gating model. \n\n- Firstly, some transform functions are **inherently disentangled**, e.g., adding nodes feature noise and random subgraph extraction. In this case, there will be certain distinction between any pair from these three data distributions, i.e., (graphs with node noise, random subgraph graphs, random subgraphs with node noise), which the gating model can easily tell. \n- While some transform functions can be **essentially similar**, e.g., drop path and drop edges, this won’t affect the performance of our method as long as each expert outputs the corresponding invariant representation. \n- Lastly, indeed, there could be more **complex combinations of the transform functions**, which poses challenges to the gating model’s expressiveness in identifying the combinations. However, this challenge may be minor in the practice. Specifically, we observe fairly high accuracy performances of the gating model, which are above 85% and 73% averagely on extrapolated datasets with one transformation and multiple transformations, respectively.\n\nWe added the above discussion to Appendix F to enable a more comprehensive view towards our methodology. We hope this can alleviate your concern on our gating model’s performance in identifying the distribution shift types.\n\n**(2) How does the predefined transform functions cover complex distributions causing the distribution shift?**\n\nThis is also a great question! We believe there are two angles for this question. \n\n**(a) For general domain**: In our experiments, we mainly use the five stochastic transform functions, which are universal graph augmentations as listed in Zhao et al., (2021) [3]. In our code implementation, we have also included additional transform functions as shown in Appendix B. We believe these transform functions, while not exhaustive, still cover a wide range of distribution shifts observing from our experimental results.\n\nNevertheless, we agree that the real graph distribution shifts can go beyond any possible combinations of the predefined transform functions. In that case, the assumption may not hold, meaning that GraphMETRO may not capture and precisely mitigate the unknown distribution shift. This scenario could always possibly exist due to the lack of information about the testing distribution or its domain knowledge. We include it as a limitation in Appendix 5, while we further discuss how we could alleviate the problem with additional information.\n\n**(b) For specific domains where additional knowledge is available**: In fact, knowing the tendency of the distribution shifts, such as increasing malicious users in a trading system, would be very helpful in constructing the transform functions that can cover the target distribution shifts well. We believe that such knowledge can come from two sources: \n- **Domain knowledge**, e.g., on molecular datasets, the transform function could be adding additional carbon structure to a molecule (while preserving its functional groups). Or, in a particular social network, transform functions can be defined from known user behaviors. \n- **Leveraging a few samples from target distribution**. Specifically, with the guide from a few target samples, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space.\n\n---\n\n[1] Sui et al. Unleashing the Power of Graph Data Augmentation on Covariate Distribution Shift. NeurIPS 2023.\n\n[2] Nianzu Yang et al. Learning substructure invariance for out-of-distribution molecular representations, NeurIPS 2022\n\n[3] Yongduo Sui et al. Causal Attention for Interpretable and Generalizable Graph Classification, KDD 2022.\n\n[4] Jiaqi Ma et al. Subgroup Generalization and Fairness of Graph Neural Networks, NeurIPS 2022.\n\n---\n\n### **Comment 2: Our assumption**\n\nThanks for this comment! In Section 3.1, we have discussed the assumptions in detail with specifications. We repeat part of it here for your convenience:\n\n```Assumption 1 essentially states that the distribution shifts (whatever they are) can be decomposed into several mixture components of stochastic graph transformations. For example, on a social network dataset, each mixture component can represent different patterns of user behavior or network dynamics shifts. Specifically, one mixture component might correspond to increased user activity, while another could signify a particular trend of interaction within a certain group of users. Such a mixture pattern is common and well-studied in the real-world network datasets (Newman, 2003; Leskovec et al., 2005; 2007; Peel et al., 2017).```\n\nWe hope the analysis provides detailed illustration to the assumption. We are happy to add more discussion if you think anything  is still unclear!\n\n---\n\n### **Comment 2 & 3: Implementation of our method**\n\n- **Model architectures**: \nPlease see Tables 2 and 3 in Appendix A for detailed model architecture information. Specifically, we use the same encoders and classifiers from GOOD benchmarkfor real-world datasets to ensure fair comparisons. We employ backbones based on the best ERM performance for synthetic datasets. Moreover, GraphMETRO is model-agnostic, which consistently improves performance across varied model architectures.\n\n- **Stochastic transformation**: Please see Appendix B where we include the introduction of the stochastic transform functions. \n\n- **Optimization process**: Please see Table 2 and 3 (Appendix A), where we included hyperparameters on each dataset. In the second paragraph after Eq. (3), we also described our training pipeline: `“...we set apart the other loss terms from backpropagating to it to avoid interference with the training of the gating model… We optimize the objective via stochastic gradient descent”`. \n\n- **For a more intricate optimization strategy:** This is a great catch! In fact, we did try to pretrain the gating model for several epochs as warm up before training the whole model in an end-to-end fashion via the objective Eq. in (3). However, we didn’t notice a statistically significant difference in their performance, which can be due to that expert models take more time to convergence (since they need to align with the base model during the training) compared to the gating model.\n\nDue to the space limitation, we had to include most of the implementation details in the appendix while we added pointers in the main paper. However, we will try to make it more detailed and feel free to let us know if anything is missing! \n\n---\n\n### **Comment 2 (Cont.): Understand the functionality, underlying mechanism, and performance gain**\n\nThanks for the comment. For clarity, we provide the following pointers to the paper:\n\n- In Section 4.3, we provide a study to reveal the underlying **mechanism** of GraphMETRO, i.e., each expert excels in generating invariant representations concerning a stochastic transform function, which provide a solid foundation in generating referential invariant representations w.r.t. the specific transformations and further guarantee the generalization.\n\n- In Appendix C, we study the **impact of the MoE architecture** on model performance, which shows that the model performance may decline if the expressiveness of the expert model decreases. \n\n- In Appendix D, we study the **impact of the stochastic transform function** on model performance, where we also provide a detailed discussion of the modeling mechanism.\n\nWe believe the above studies and discussion in our paper provides an in-depth view, highlighting the roles of our objective, architecture, and stochastic transform function.  Please let us know if any of these perspectives is still unclear, and we can further improve our experimental study.\n\n-----\n\n# Summary\n\nWe hope our answers can address all of the concerns. We are happy to follow up if you have any further questions.\n\nWe also prudently ask you to reevaluate our work. To highlight, our motivation is supported by the common fine-grained graph shift heterogeneity, and the fact that most of the previous works could not model such nuanced distribution shifts in an effective and flexible manner. Moreover, we added more related works and modified our statement to position our work better. In general, we believe GraphMETRO is a more general and flexible solution that can mitigate a wider range of distribution shifts, which is backed by the experimental results. Detailed justification on our assumption and implementation details are also available in our paper. \n\nThus, we believe our work makes important contributions and provides a clear presentation. We are happy to discuss more and revise our paper if any concern remains. Thank you for your efforts and we are looking forward to your reply!!\n\n---\n\nWe appreciate your comments! To address your concerns, below we prudently justify the motivation of our proposed method, clarify our assumptions, and provide details regarding our experiment implementation.\n\n---------------------\n\n### **Comment 1: Motivation of this work**\n\nThank you for this comment! We believe there might be a bit of misunderstanding due to our different definitions of *“graph shift heterogeneity”*. We firstly discuss the related works mentioned and then justify our statement:\n\n**(1) Related works**\n\nPlease see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nMoreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you! \n\nSpecifically, Sui et al. [1] proposed a graph data augmentation strategy that alleviates covariate shift by generating diverse and invariant causal features. However, the trainable augmenter they used may not distill diverse augmentations or construct unseen perturbations. Moreover, Sui et al. [1] test its method only on graph classification tasks, while GraphMETRO can be applied to both node and graph classification tasks. Besides, we have discussed graph augmentation and attention-based methods in our related works, and we added more recent works on graph OOD [2,3,4], and we hope our response clears your concern on the related work discussion.\n\n\n**(2) The definition of graph shift heterogeneity**\n\nIn this work, we refer to **“heterogeneous shifts”** as multiple and different levels of shifts which vary across different instances (nodes or graphs), as illustrated in the example in the abstract. While we agree that the existing invariant learning approches can accommodate multiple distribution shifts, it could be hard for them to tackle nuanced distribution shifts for individual instances (nodes or graphs) since the distribution shifts are inferred from variance across multiple data environments. If GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering **a mixture of transformations as a proxy** for the target distribution shifts rather than invariance to whole-graph environment shifts. This is the type of heterogeneity we are interested in our paper. \n\n**(3) Regarding our original motivation statement**\n\nWhile the statement pointed out by the reviewer serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category. And we have provided detailed discussion about three lines of research in the related work section.\n\nHowever, we agree that we could make this statement border to cover the previous invariant learning methods. To improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to \"**previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments…**”. We also modified our introduction correspondingly, we hope this will better position our work.\n\n**(4) The choice of our MoE design**\n\n- The choice of our approach comes as a consequence of our motivation to model the graph/instance shift heterogeneity. As mentioned, mitigating multiple and nuanced distribution shifts simply goes beyond certain distribution shift types or environment construction as seen in the previous methods. \n- Thus, GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts. This enables the prediction of multiple different distribution shifts and the flexibility to model fine-grained heterogeneity since the mixture can be varied across different instances. We then tackled the proxy to mitigate the target distribution shifts. Intuitively, this solution provides a **“middle ground”** to deem graph generalization as an equivalent mixture, which, we believe, is a more tractable solution.\n\nWe updated our paper to make the above point more clear. We genuinely hope our answer can justify the motivation and solve your concern.\n\n---\n\n### **Question 2: How does the computational cost of GraphMETRO compare to other methods?**\n\nPlease see the last paragraph of Section 3.4, where we analyze the computation complexity of GraphMETRO. We repeat part of it here for your convenience:\n\n```Consider the scenario where we use an individual encoder for each expert. The forward process of $f$ involves $O(K)$ forward times using the weighted sum aggregation (or $O(1)$ if using the maximum selection). Since we extend the dataset to $(K + 1)$ times larger than the original data, the computation complexity is $O(K^2 |D_s|)$, where |Ds| is the size of the source dataset.```\n\nThus, the computation cost is about $K^2$ or $K$ times (if using the maximum selection) than an ERM model, where $K=5$ in our experiments. Compared to DIR, as they extract $B$ spurious subgraphs from each batch to conduct the intervention, their computation cost is $B$ times compared to ERM, where $B$ could be 32. Thus, we believe the computation cost of GraphMETRO is fair for the gains we get, considering $K$ is usually small.\n\n----------------------\n\n### **Question 3: Can GraphMETRO handle multiple different types of distribution shifts that simultaneously exist in data?**\n\nYes! The distribution shift types corresponding to the gating outputs with high scores will be tackled during training. That is, if the gating output highlights multiple mixture components, their corresponding distribution shift types will be handled jointly.\n\n----------------------\n\n### **Question 4: Can GraphMETRO  tackle distribution shifts on molecular graphs?**\n\nThat is a great idea! Yes, GraphMETRO can be applied to molecular datasets if one designs transform functions to cover typical molecular variants. For instance, a transform function may add carbon structures to the molecules. These domain-specific transform functions are outside the scope of our work, however, we believe these would be interesting future work directions!\n\n----------------------\n\n# Summary \n\nWe thank the reviewer for the time and insightful suggestions! We hope our answers can address your concerns well.\n\nWe also prudently ask you to reconsider our work if the concerns are addressed. To highlight, our novelty comes from the formulation of an equivalent mixture for graph OOD and the training framework to effectively realize generalization. We also provide an in-depth analysis on our originality compared to some previous invariant learning methods. While we discussed and compared with previous works, we added more related works and modified our statement to position our work better. Finally, our method achieves great improvements on both node and graph classification tasks, and is a more general solution to mitigate multiple and nuanced distribution shifts. \n\nOverall, we believe our work proposes a new paradigm and novel training framework and makes good contributions in the fields of graph generalization, and we would appreciate your reconsideration on this point. Thank you for your efforts again!\n\n----------------------\n\n**Reference**\n\n\n[1-5] The same as listed by the reviewer\n\n\n[6] Beatrice Bevilacqua, Yangze Zhou, and Bruno Ribeiro. Size-invariant graph representations for graph classification extrapolations. In ICML, 2021.\n\n[7] Davide Buffelli, Pietro Li´o, and Fabio Vandin. Sizeshiftreg: a regularization method for improving size-generalization in graph neural networks. In NeurIPS, 2022.\n\n[8] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In NeurIPS, 2019.\n\n[9] Mucong Ding, Kezhi Kong, Jiuhai Chen, John Kirchenbauer, Micah Goldblum, David Wipf, Furong Huang, and Tom Goldstein. A closer look at distribution shifts and out-of-distribution generalization on graphs. In NeurIPS DistShift, 2021.\n\n---\n\n### **Comment 3 & Question 1: Related works on invariant learning and clarification on our statement.**\n\nThanks for pointing it out! Below we clarify the statement and clear potential misunderstanding:\n\n**(1) How does GraphMETRO compare with invariant learning methods like DIR and EERM?** \n\nPlease see our related work section where we discussed these two papers, i.e., DIR (Wu et al., 2022c) and EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on environmental patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nBesides, we also provide a more in-depth comparison in our point (3) below to highlight our key technical originality.\n\n**(2) Regarding our statement about previous works:** \n\nWhile the statement serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category, and we have provided detailed discussion about three lines of research in the related work section.\n\nTo improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to **\"previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments** (which is highly limited when confronted with nuanced distribution shifts)”. We also modified our introduction correspondingly. Thanks for letting us know our statement could be misinterpreted.\n\n**(3) Why do we say our method could be more broad than the existing invariant learning approaches?** \n\n- Invariant subgraph learning approaches, e.g., [1,2], consider variance of constructed data environments, which are designed very differently compared to our work. \nWhile they can accommodate multiple distribution shifts (as in multiple environments), these focus on patterns within each environment and ignore the variety across instances (e.g., shifts at the resolution of nodes), which may not be well-captured by the environment assignments. \n- GraphMETRO considers that specific parts of the test graph may have different shifts. Particularly, our goal is to make the generalization to unknown testing distribution more adaptive and broad, as opposed to limiting the distribution shifts to being invariant to specific types of subgraphs. \n\nIn other words, if GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering a mixture of transformations as a proxy for the target distribution shifts rather than invariance to environment shifts.\n\n**(4) Key technical originality compared to invariance learning (going more deeply)**\n\nAnother interesting view to see the innovation of GraphMETRO is that it breaks the typical invariant learning formulation, which assumes the data is manipulated by the environment variables (and then can be “decoded” into multiple environments). Instead, GraphMETRO sees the distribution shifts on an instance as a mixture, which is represented by the score vector output by the gating function over the basis of the transform functions. In other words, GraphMETRO can produce infinite environments as the elements in the score vector are continuous. One can see that once we limit the output domain of the gating function into, e.g., binary {0, 1}, GraphMETRO can also produce a limited number of environments (if we categorize the instances based on the score vector), which covers the environment construction in invariant learning. Moreover, as mentioned, we propose the concept of referential invariant representation with a base model $\\xi_0$, which is also different from previous works on invariant learning. We added the above discussion to Appendix F to improve the depth of our analysis.\n\n---\n\nWe appreciate your efforts and insightful comments! To address your concerns, we provide point-to-point responses below.\n\n---------------------\n\n### **Comment 1: Regarding the novelty of GraphMETRO.**\n\nThanks for the comment! We believe our novelty comes from the proposal of an equivalent mixture for graph OOD and the construction of our training framework, as detailed below:\n\n- **An equivalent mixture for graph OOD**: The key challenge we faced to mitigate multiple and nuanced distribution shifts is the intrinsic complexity and heterogeneity of graph distribution shifts, which simply goes beyond certain distribution shift types [6,7,8,9] or environment construction as seen in the previous methods [1,2,3]. GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts, where the mixture can be varied across different instances, and then tackled the proxy to mitigate the target distribution shifts. We believe the high-level idea is succinct, nevertheless, novel, in the sense that it provides a “middle ground” to deem graph generalization as an equivalent mixture that is more tractable.\n\n- **Training framework**: With the guide of our formulation, the training framework is still non-trivial due to two problems, i.e., “how to provide supervision for predicting the mixture” and “how to ensure the experts corresponding to mixture components are compatible when working as a whole”. Specifically, GraphMETRO solves the first problem by conducting graph extrapolation. This is somewhat similar to the spirit of graph pretraining in the sense that we inject heterogeneity to promote the expressiveness of the gating model in recognizing the mixture components. For the second problem, we introduce the concept of Referential Invariant Representation, along with the novel objective in Eq. (3) to enforce the invariance and compatibility. It is worth mentioning that the model performance is much worse than the reported numbers (e.g., 2.7\\% lower on Twitch dataset) without the compatibility constraint, indicating the proposed referential invariance concept is indispensable. \n\nWe added more justification in the introduction (updated in the revision). We genuinely hope our responses can solve your concerns about the novelty of our work.\n\n------------------------------\n\n### **Comment 2: Comparison with recent methods [3,4,5].**\n\nIn compacting the paper to fit in the page limit we mistakenly did not include these relevant references, we apologize. We added discussion in the revised version. Here we summarize these works and point out their key differences with our method: \n\n- In particular, **Yang et al. [3]** explore molecule representation learning in out-of-distribution (OOD) scenarios. They achieve this by directing the molecule encoder to utilize stable and environment-invariant substructures relevant to the labels without the need for environmental labels. \n- Similarly, **Sui et al. [4]** introduces causal attention modules to identify key invariant subgraph features that can be described as causing the graph label. The type of OOD task that Sui et al. [4] considers assumes the graph label is caused by a subgraph, which is quite different from ours. Moreover, both Yang et al. [3] and Sui et al. [4] consider tasks where the graph label is caused by a subgraph. \n- **Ma et al.[5]** is an interesting theoretical work which studies GNN generalization and examines their fairness, showing that the test subgroup's distance from the training set impacts GNN performance. Ma et al.[5], as far as we could assess, does not propose any specific architecture to solve the type of OOD tasks we consider in our work.\n\nOverall, the goal of GraphMETRO is to be invariant to a mixture of selected stochastic transform functions (and the mixture can vary across different instances), which is a more flexible and general solution. \nWe added a discussion of these works to our revision, hopefully providing a more comprehensive comparison and literature overview. We hope our responses can solve your concerns about the related work.\n\n---\n\nWe sincerely appreciate all reviewers' time, efforts, and valuable suggestions in reviewing our paper. We are glad that most of the reviewers reached a positive consensus on our work's motivation and experimental results. Here is a summary of our responses:\n\n- **Clarification**: We clarify our key assumptions (`m8JS, FURX`), novelty (`m8JS, NLg1`), and the experimental settings (`m8JS`).\n\n- **Related work**: : We add more related works on recent graph OOD learning (`NLg1, m8JS`) and graph transfer learning (`FURX`)\n\n- **Limitation**: We add discussion about the limitations of GraphMETRO when confronted with unknown distribution shifts that surpass our key assumptions (`FURX`, `C9mM`).\n\nWe hope our responses can clarify your confusion and alleviate concerns and we updated our paper **(highlighted in green)** corespondingly. We thank all reviewers again, and look forward to your reply!", "author_response": "We appreciate your comments! To address your concerns, below we prudently justify the motivation of our proposed method, clarify our assumptions, and provide details regarding our experiment implementation.\n\n---------------------\n\n### **Comment 1: Motivation of this work**\n\nThank you for this comment! We believe there might be a bit of misunderstanding due to our different definitions of *“graph shift heterogeneity”*. We firstly discuss the related works mentioned and then justify our statement:\n\n**(1) Related works**\n\nPlease see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nMoreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you! \n\nSpecifically, Sui et al. [1] proposed a graph data augmentation strategy that alleviates covariate shift by generating diverse and invariant causal features. However, the trainable augmenter they used may not distill diverse augmentations or construct unseen perturbations. Moreover, Sui et al. [1] test its method only on graph classification tasks, while GraphMETRO can be applied to both node and graph classification tasks. Besides, we have discussed graph augmentation and attention-based methods in our related works, and we added more recent works on graph OOD [2,3,4], and we hope our response clears your concern on the related work discussion.\n\n\n**(2) The definition of graph shift heterogeneity**\n\nIn this work, we refer to **“heterogeneous shifts”** as multiple and different levels of shifts which vary across different instances (nodes or graphs), as illustrated in the example in the abstract. While we agree that the existing invariant learning approches can accommodate multiple distribution shifts, it could be hard for them to tackle nuanced distribution shifts for individual instances (nodes or graphs) since the distribution shifts are inferred from variance across multiple data environments. If GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering **a mixture of transformations as a proxy** for the target distribution shifts rather than invariance to whole-graph environment shifts. This is the type of heterogeneity we are interested in our paper. \n\n**(3) Regarding our original motivation statement**\n\nWhile the statement pointed out by the reviewer serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category. And we have provided detailed discussion about three lines of research in the related work section.\n\nHowever, we agree that we could make this statement border to cover the previous invariant learning methods. To improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to \"**previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments…**”. We also modified our introduction correspondingly, we hope this will better position our work.\n\n**(4) The choice of our MoE design**\n\n- The choice of our approach comes as a consequence of our motivation to model the graph/instance shift heterogeneity. As mentioned, mitigating multiple and nuanced distribution shifts simply goes beyond certain distribution shift types or environment construction as seen in the previous methods. \n- Thus, GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts. This enables the prediction of multiple different distribution shifts and the flexibility to model fine-grained heterogeneity since the mixture can be varied across different instances. We then tackled the proxy to mitigate the target distribution shifts. Intuitively, this solution provides a **“middle ground”** to deem graph generalization as an equivalent mixture, which, we believe, is a more tractable solution.\n\nWe updated our paper to make the above point more clear. We genuinely hope our answer can justify the motivation and solve your concern."}
{"claim": "Despite including shift learning and data augmentation components, the paper appears to rely on standard optimization routines without explaining why.", "claim_type": "methodology", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ppMpJyIYRZ", "reviewer": "Reviewer_m8JS", "review_text": "Summary: The paper introduces a method to enhance the out-of-distribution performance of graph neural networks (GNN) by learning to understand distribution shifts instead of addressing the assumed ones. To achieve this, the Mixture of Experts architecture is integrated into the GNN, supplemented by an alignment procedure to recognize the shift. Empirical experiments are conducted to validate the theoretical assertion.\n\nOn the whole, I believe the proposed method lacks the necessary motivation and its novelty isn't substantial enough to meet the standard.\n\nStrengths: - The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\n- Real-world datasets back the claims through experiments.\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\n\nWeaknesses: - The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \"previous research has concentrated on addressing specific types of distribution shifts.\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \"environment generators\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\n\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\n\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\n\nQuestions: Please check Weaknesses.", "labeling_timestamp": "2026-01-11T16:31:54.896528", "model": "gpt-5-mini", "label": "Supported", "justification": "Authors acknowledge and clarify their optimization choices: they state they use SGD, explain training pipeline details (e.g., isolating loss terms to avoid interference), point to appendix hyperparameters, and report having tried a more intricate strategy (pretraining the gating model) with no significant difference.", "evidence": "\"we set apart the other loss terms from backpropagating to it to avoid interference with the training of the gating model… We optimize the objective via stochastic gradient descent\"... \"we did try to pretrain the gating model for several epochs as warm up before training the whole model in an end-to-end fashion via the objective Eq. in (3). However, we didn’t notice a statistically significant difference in their performance\"", "combined_author_response": "Dear reviewer NLg1,\n\nAs the discussion period is closing soon, we hope to engage with you and check if your concerns are addressed.\n\nWe sincerely appreciate your time and attention!\n\nBest,\n\nAuthors of Paper 1560\n\n---\n\nThanks for reading our response! We would also appreciate your patience for reading the two points below \n\n---\n\n**Regarding point #1:**\n\nYes, we agree that the concept of heterogeneous shifts is relatively new for the current studies on distribution shifts, however, this is not new for studies on network patterns (Newman, 2003; Leskovec et al., 2005; 2007; Peel et al., 2017). In fact, the ignorance of such nuanced heterogeneous shifts in the previous studies instead emphasises our motivation and the potential impact of this work. \n\nFrom a causality perspective, distribution shifts can naturally happen in the instance level when (1) additional causal variables, beyond environmental factors and randomized noise, influence these shifts, or (2) multiple causal variables simultaneously affect the shifts with different strengths. Without modeling these explicitly, the mitigation of distribution shifts can easily fail. \n\nIn terms of our presentation, we have illustrated these cases of interest in abstract and introduction, we made further explanations in Section 3.1, we also present the specific results of distribution types in Figure 3 (b). \n\n\n**Regarding point #2:**\n\nWe think we can all agree that, if the ground truth of the instance shifts is available on the real-world datasets, it would be crystal clear to see where the improvement comes from since we can conduct case study to compare our method and the baselines on instances with nuanced distribution shifts, to see the influence of the modeling these heterogeneous shifts. \n\nHowever, with such ground truth not available, we had to seek other seemingly less intuitive but also in-depth way to illustrate the insights (esp Section 4.3) as mentioned above in our previous response. This is also why we designed the synthetic experiments at the first place. We think we did try hard to explain the underlying rationale with the ground truth being absent.\n\n----\n\n**Refinement**: We can of course add a causal graph in the our assumption section to make the concept more clear. And we can illustrate more if you could let us know the specific obstable to understand our mechanism, which will be extremely helpful.\n\n----\n\n### **Summary/TL;DR**\nWe understand the reviewer's clarification concerns, however, we don't agree that they, based on our justificaton, are the cause of rejection. We believe nuanced heterogeneous shifts are common, important, yet being typically ignored in the research domain of distribution shifts, we made these argument clear and also promise to refine. While explaining the underlying rationale is hindered by the lack of ground truth, we did try hard to dissection it from the gating model, the invariant representations generated, and more ablations in the appendix. We respect the reviewer's current opinion. Still, reconsideration will be greatly appreciated.\n\n---\n\nWe thank the reviewer once again! We also enjoyed the process of making our work more sound from your suggestions. And your approval surely means a lot to us.\n\nWe do apologize for not making our solutions towards the limitations clear enough. We summarize them into a short table, hopefully could alleviate your concerns a bit more.\n\n| Limitation| Potential solutions | Location of discussion/action |\n|:---|:---|:---|\n|**Coverage of the transformations** | 1) Extend the coverage by adding representative transform functions. 2) Or include tranform functions based on domain knowledge or a few samples from target distribution | Appendix F, paragraph #3|\n|**Complexity of the transformations** | Make experts dedicated to different hyperparameters for the same type of transformation | Argument options are available in our codebase. Will make it more detailed in the experimental settings. |\n|**Label distributional shift**|Integrate the objective of the existing methods studying labe distributional shifts into our framework. |Appendix F, paragraph #4 |\n\n----\n\nWe completely agree on the existence of these limitations and we will move some limitations (esp #1) to the main paper in our final version. While they could be important in the practice, they are, in our perspective, fair \"side effects\" considering the benefits (i.e., mitigating multiple and nuanced distribution shifts and better interpretability), and may not be the central part of our novelty and main contribution (i.e., the proposal of an equivalent mixture, the concept of referential invariant representations, as well as the training framework). We are also eager to further improve other aspects.\n\nOnce again thank you so much for your support!!\n\n---\n\nDear Reviewer m8JS,\n\nA gentle nudge that we would like to know if our response adequately addresses your concerns. \n\nYour time and feedback is greatly appreciated!\n\nThank you sincerely,\n\nAuthors of Paper 1560\n\n---\n\nWe deeply appreciate your approval. Your suggestions definitely inspired us a lot and have greatly improved our work.\n\nA few additional notes for the further comments:\n\n- Regarding **point #2**: In node classification tasks, the BCE objective already considers node embeddings other than the graph embedding of the k-hop subgraph. What we described previously refers to the computational graph (i.e., k-hop message passing), which generates node embeddings. We apologize for any confusion.\n\n- Regarding **point #3**: Yes it would be quite interesting to see how graph pretraining methods perform on the current OOD benchmarks. Graph pretraining methods like GCC and GPT-GNN also consider graph extrapolation to some extent, e.g., through subgraph extraction and masked attributes/structures. The key difference between graph pretraining and generalization may lie in their different focuses on **expressiveness** and **invariance**. While these two aspects do not always conflict, ensuring invariance w.r.t. a certain type of extrapolation might affect expressiveness (if the change is relevant to labels), and vice versa. To seek a balance in between, one might need prior knowledge in which types of transformations may or may not be sensitive to the labels (and perhaps build experts with different goals to make ensure invariance or expressivess). We believe there is still a lot to explore in this domain.\n\nOverall, we are grateful for your positive stand on this work. We believe that our [current version](https://openreview.net/pdf?id=QQ5eVDIMu4), incorporating opinions from you and other reviewers, is sound and well-refined. We are also committed to further improving it. We would appreciate your support based on our current version! \n\nThank you!\n\n---\n\nDear Reviewers of Paper 1560,\n\nWe hope this message finds you well.\n\nAs the discussion phase approaches its end, we hope you find our responses useful. We would like to ask if the issues have been addressed.\n\nWe understand that the discussion time is short, and some of you might be enjoying holidays at the moment. We apologize for posting our responses a bit late as we aimed to address your concerns clearly.\n\nWe sincerely appreciate your time and attention!\n\nBest regards,\n\nAuthors of Paper 1560\n\n---\n\n**Comment 4: Related works on graph transfer learning.**\n\nYes! We agree that graph transfer learning is a relevant topic. Thank you for providing the additional related work! We added the discussion on these works in our revision, and we repeat it here for your convenience:\n\n```It is also worth mentioning that graph domain adaptation (Zhang et al., 2019; Wu et al., 2020), different from the problem studied in this work, commonly relies on limited labeled samples from target datasets for improved transferability. For instance, to generate domain adaptive network embedding, DANE (Zhang et al., 2019) uses shared weight graph convolutional networks and adversarial learning regularization, while UDA-GCN (Wu et al., 2020) employs attention mechanisms to merge global and local consistencies.```\n\nMoreover, we believe this is actually relevant to `Comment & Question 1` regarding the generality of the distribution shifts covered by the transform functions. In our future works, we added the following discussion:\n\n```...Leveraging a few samples from target distribution (i.e., domain adaptation). Specifically, we can leverage the samples from the target distribution to inform the selection or construction of transform functions, which can better guarantee the distribution shifts are covered by the transform functions.  For example, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space. We believe this would be an interesting future direction.```\n\n---\n\n# Summary\n\nOnce again thank you very much for helping us position our work and reflect the literature better. We hope all of your concerns are solved and we are happy to engage further if there are any other points we missed!\n\n---\n\n### **Reference**\n\n[1] Unsupervised Domain Adaptive Graph Convolutional Networks. Wu et al. WWW 2020\n\n[2] DANE: Domain adaptive network embedding. Zhang et al. IJCAI 2019.\n\n[3] Tong Zhao, Yozen Liu, Leonardo Neves, Oliver J. Woodford, Meng Jiang, and Neil Shah. Data augmentation for graph neural networks. AAAI, 2021.\n\n---\n\nWe are grateful for your positive feedback and detailed suggestions! We provide responses below to address your remaining concerns. \n\n---\n\n### **Comment & Question 1: Generality of the distribution shifts covered by the transform functions.**\n\nGood question! We provide the response from two angles:\n\n\n**(a) For general domain**: In our experiments, we mainly use the five stochastic transform functions, which are universal graph augmentations as listed in Zhao et al., (2021) [3]. In our code implementation, we have also included additional transform functions as shown in Appendix B. We believe these transform functions, while not exhaustive, still cover a wide range of distribution shifts observing from our experimental results.\n\nNevertheless, we agree that the real graph distribution shifts can go beyond any possible combinations of the predefined transform functions. In that case, the assumption may not hold, meaning that GraphMETRO may not capture and precisely mitigate the unknown distribution shift. This scenario could always possibly exist due to the lack of information about the testing distribution or its domain knowledge. We include it as a limitation in Appendix 5, while we further discuss how we could alleviate the problem with additional information.\n\n**(b) For specific domains where additional knowledge is available**: In fact, knowing the tendency of the distribution shifts, such as increasing malicious users in a trading system, would be very helpful in constructing the transform functions that can cover the target distribution shifts well. We believe that such knowledge can come from two sources: \n- **Domain knowledge**, e.g., on molecular datasets, the transform function could be adding additional carbon structure to a molecule (while preserving its functional groups). Or, in a particular social network, transform functions can be defined from known user behaviors. \n- **Leveraging a few samples from target distribution**. This is in fact in line with the reviewer’s Comment #4 regarding graph transfer learning. Specifically, with the guide from a few target samples, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space.\n\n We hope this response can alleviate your concern about our applicability.\n\n---\n\n### **Comment & Question 2: The mechanism of GaphMETRO in handling heterogeneous shifts within one graph.**\n\nGood question! For simplicity, suppose we have two transform functions, i.e., adding edges and dropping edges. Given a node classification task and the objective in Eq. (3), the model is trained on the extrapolated datasets based on the transform functions. After that, given an unseen graph with imbalanced distribution shifts, the gating model outputs scores to identify nodes likely to experience increasing or decreasing degrees, while others might adhere to the original distribution. Then, for each node, each expert takes its multihop subgraph, and outputs its referential invariant representation w.r.t. the correposing transform function. These expert outputs and gating model results are then aggregated to form the final representation. Assuming accurate predictions by the gating function, nodes in denser/sparser subgraphs are represented by expert models corresponding to adding/dropping edges. As each expert is trained to create invariant representations, the final node representations remain unaffected by their individual distribution shifts.\n\n---\n\n### **Comment 3: How to leverage graph pretraining.**\n\nIf we understand correctly, the reviewer was asking how to leverage a pretrained model to further aid the training of GraphMETRO. Please let us know if otherwise. \n\nThis is in fact an interesting point! We believe a model pretrained on a wide variety of data can be very helpful to initialize the gating model, which is required to output the mixture of a node or graph (after it is finetuned on the extrapolated dataset). By enhancing the gating model's predictive capability regarding mixtures, GraphMETRO's final representation should become more resilient. This becomes particularly advantageous when dealing with graphs not previously encountered in the extrapolated dataset. Moreover, it is possible that the pretrained model will also benefit the expert models, while one minor concern would be that the expert model may tend to be similar instead of dedicating to generating invariant representation w.r.t. their corresponding transform function. We added the above discussion to Appendix F as a future work. Thanks again for this comment!\n\n---\n\n### **Question 4: Why does the test accuracy vary across different transformations?**\n\nGreat question! Here we summarize three possible reasons:\n\n- **Information Preservation in Transformations:** Certain transformations retain more informative features than others. For instance, in the REDDIT-BINARY (graph classification task), the random subgraph transformation may retain more graph label-related information compared to dropping edges, as the latter tends to lose more global information. This discrepancy in testing performance, where dropping edges outperforms random subgraph extraction, could be due to the preservation of crucial information. However, conclusions may vary across datasets or tasks depending on how information influences final predictions. For CiteSeer (a node classification task), a random subgraph might preserve more local node information, potentially explaining why its testing performance surpasses dropping edges in this specific task.\n\n- **Complexity of transformation:** Certain transformations inherently generate more diverse graphs than others. If the model lacks the expressive capacity to capture such diversity, it may lead to a decline in testing performance.\n\n- **Model Sensitivity:** Certain transformations may be easier for a model to learn due to compatibility with specific model architectures. This extends beyond transformation complexity and emphasizes how different model architectures may prefer learning particular distributions from one of the extrapolated datasets, which can also contribute to the difference in the testing performance.\n\nWe included the above discussion to Appendix F: Open Discussions. We hope this response can answer your question and improve the soundness of our work.\n\n---\n\n# Summary \n\nWe are grateful for your time and insightful suggestions! \n\nWe would like to highlight that our main contribution is framing the graph generalization problem on top of an equivalent mixture, a simple yet novel and tractable \"middle ground\", as well as proposing the training framework which effectively guarantees the generalization. While our method relies on a set of predefined transform functions, we believe they cover a wide range of distribution shifts based on our empirical results. Also, we agree that there could be some scenarios where the transform functions may not cover complex distributions, and we discuss two future directions and include them into our future works. Moreover, while selecting the hyperparameters for the transform functions introduce extra complexity, the issue could be minor in practice and we also conduct more experiments to justify the applicability better. Finally, we address several questions about clarification and presentation, as well as including more future works. \n\nLastly, we prudently ask you to reevaluate our work given the clarification in our responses, which we also updated our paper correspondingly. Overall, we believe our work makes good contributions to the field of graph distribution learning by proposing a novel and effective solution, and we would appreciate your reconsideration on this point. Thank you for your efforts again!\n\n---\n\n### **Reference**\n\n[1] Menon, Aditya Krishna, et al. \"Long-tail learning via logit adjustment.\" International Conference on Learning Representations. 2020.\n\n[2] Cao, Kaidi, et al. \"Learning imbalanced datasets with label-distribution-aware margin loss.\" Advances in neural information processing systems 32 (2019).\n\n[3] DARTS: Differentiable Architecture Search. Hanxiao Liu, Karen Simonyan, Yiming Yang. 2018.\n\n---\n\n**(3) What is the complexity of the transform functions and how does it affect generalization?** \n\nInteresting question! In fact, our implementation and framework could easily avoid selecting hyperparameters on the transform functions. Specifically, we can make multiple transform functions of the same type with different ranges of hyperparameters. Specifically, GraphMETRO allows three edge dropping transform functions, $\\tau_1^{\\alpha_1}, \\tau_2^{\\alpha_2}, \\tau_3^{\\alpha_3}$, where $\\alpha_i$ (i=1, 2, 3) are three different ranges of edge dropping probabilities, e.g., [0.1,0.3], [0.3, 0.6], [0.6,0.9], representing different transform extents. Thus, given an input from the validation dataset, the gating model will highlight the transform function which simultaneously selects the corresponding hyperparameter that matches the distribution of the validation set. Interestingly, this idea is in the same spirit as how DARTS [3]  proposes to perform architecture search by formulating the task in a differentiable manner. \n\nIn our previous experiments, we did try this scheme where we replaced a single edge dropping transform function with the ratio range [0.3, 0.5] to three transform functions as mentioned above. While we didn’t see a significantly different performance in that case, we believe this would be a flexible solution which avoids the need to conduct hyperparameter selection. \n\nWe include the above discussion in our open discussion and future works (Appendix F). We hope this response can alleviate your concern about the applicability of our method. \n\n---\n\n### **Comment 4: Limitations of our work**\n\nGreat point! We think the issue of label distributional shift, while important, is orthogonal and complementary to the focus of our current study. To elaborate, label distributional shifts exert analogous impacts across various modalities, such as graphs or images. Moreover, existing methods [1,2] designed to tackle label distributional shifts can be seamlessly integrated into our proposed framework. Such integration would necessitate minimal adjustments, potentially involving modifications to the loss function or the training pipeline. We added this as a future work in Appendix F.\n\n---\n\n### **Question 1: Statistical significance of the results on Table 1**\n\nThanks for the question! We compute the p-value of our method against the best baselines method as follows:\n\n| | WebKB | Twitch | Twitter | SST2 | \n|:--|:--|:--|:--|:--|\n|p-value|< 0.001| 0.023 | 0.042| 0.081|\n\nGiven the cut-off threshold as 0.05, we believe the performances of GraphMETRO are statistically significant on WekGB, Twitch, and Twitter datasets, while on the SST2, we see relatively weak evidence. We added the p-value results to our revision and hope our response can alleviate your concern on our improvements.\n\n---\n\n###  **Question 2: Can we say we use ERM for the node classification even if nodes and their labels are not i.i.d.?** \n\nIf we understand correctly, the reviewer is asking for clarification on the 2nd term of our objective. Please let us know if otherwise. Here, our thinking is that the cross-entropy loss for node classification already assumes node labels are conditionally independent given the model (the negative log-likelihood is a sum over the labeled nodes in training). Then, we use the same assumption of cross-entropy on Empirical Risk Minimization (ERM). That is, for a given model we must also minimize the error variance across nodes. The task is then to find the model with the best performance and small variance.\n\n---\n\n### **Question 3: Numerical results on Figure 2**\n\nThanks for pointing it out! We included all of the numerical results of Figure 2 in Appendix E, while showing the results on DBLP below.\n\n| |i.i.d. (0)|noisy feature (1)|add edge (2)|drop edge (3)|drop node (4)|random subgraph (5)|\n|:--|:--|:--|:--|:--|:--|:--|\n|ERM | 85.71 | 84.48 | 71.08 | 79.69 | 83.41 | 76.9|\n|ERM-Aug | 85.66 | 85.29 | 74.85 | 82.34 | 84.44 | 72.81|\n|GraphMETRO | 85.92 | 85.78 | 76.61 | 82.95 | 84.98 | 81.32|\n\n| |(4, 5)|(3, 5)|(2, 5)|(1, 5)|(2, 4)|(1, 4)|     (2, 3)|(1, 3)|\n|:--|:--|:--|:--|:--|:--|:--|:--|:--|\n|ERM | 70.4 | 77.63 | 81.99 | 79.69 | 70.55 | 71.52 | 77.73 | 79.59|\n|ERM-Aug | 74.16 | 81.04 | 83.65 | 68.62 | 74.01 | 68.27 | 81.13 | 84.49|\n|GraphMETRO | 76.18 | 81.71 | 84.26 | 80.31 | 75.1 | 71.05 | 81.85 | 87.14|\n\nAcross all of the synthetic environments, GraphMETRO averagely outperforms ERM and ERM-Aug by 3.20% and 2.45%, respectively.\n\n---\n\nWe appreciate your comments! To address your concerns, below we prudently justify the assumption of our method, the predefined transformation functions, as well as their complexity, and clarify our presented results.\n\n---\n\n### **Comment 1-3: Applicability and our assumptions**\n\nThanks for these great comments! Here we provide response in three folds:\n\n**(1) How does GraphMETRO identify all distribution shifts from transform functions if they are treated independently when combined together?** \n\nIf we understand correctly, by “treated independently”, the reviewer is referring to the first term in our objective $\\text{BCE}(\\phi(\\tau^{(k)}(\\mathcal{G})), Y (\\tau^{(k)}))$, where we formulate predicting the distribution shifts types of a jointly transformed graph as a binary multiclass classification problem. We believe the difficulty of this task comes from both the property of transform functions and the expressiveness of the gating model. \n\n- Firstly, some transform functions are **inherently disentangled**, e.g., adding nodes feature noise and random subgraph extraction. In this case, there will be certain distinction between any pair from these three data distributions, i.e., (graphs with node noise, random subgraph graphs, random subgraphs with node noise), which the gating model can easily tell. \n- While some transform functions can be **essentially similar**, e.g., drop path and drop edges, this won’t affect the performance of our method as long as each expert outputs the corresponding invariant representation. \n- Lastly, indeed, there could be more **complex combinations of the transform functions**, which poses challenges to the gating model’s expressiveness in identifying the combinations. However, this challenge may be minor in the practice. Specifically, we observe fairly high accuracy performances of the gating model, which are above 85% and 73% averagely on extrapolated datasets with one transformation and multiple transformations, respectively.\n\nWe added the above discussion to Appendix F to enable a more comprehensive view towards our methodology. We hope this can alleviate your concern on our gating model’s performance in identifying the distribution shift types.\n\n**(2) How does the predefined transform functions cover complex distributions causing the distribution shift?**\n\nThis is also a great question! We believe there are two angles for this question. \n\n**(a) For general domain**: In our experiments, we mainly use the five stochastic transform functions, which are universal graph augmentations as listed in Zhao et al., (2021) [3]. In our code implementation, we have also included additional transform functions as shown in Appendix B. We believe these transform functions, while not exhaustive, still cover a wide range of distribution shifts observing from our experimental results.\n\nNevertheless, we agree that the real graph distribution shifts can go beyond any possible combinations of the predefined transform functions. In that case, the assumption may not hold, meaning that GraphMETRO may not capture and precisely mitigate the unknown distribution shift. This scenario could always possibly exist due to the lack of information about the testing distribution or its domain knowledge. We include it as a limitation in Appendix 5, while we further discuss how we could alleviate the problem with additional information.\n\n**(b) For specific domains where additional knowledge is available**: In fact, knowing the tendency of the distribution shifts, such as increasing malicious users in a trading system, would be very helpful in constructing the transform functions that can cover the target distribution shifts well. We believe that such knowledge can come from two sources: \n- **Domain knowledge**, e.g., on molecular datasets, the transform function could be adding additional carbon structure to a molecule (while preserving its functional groups). Or, in a particular social network, transform functions can be defined from known user behaviors. \n- **Leveraging a few samples from target distribution**. Specifically, with the guide from a few target samples, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space.\n\n---\n\n[1] Sui et al. Unleashing the Power of Graph Data Augmentation on Covariate Distribution Shift. NeurIPS 2023.\n\n[2] Nianzu Yang et al. Learning substructure invariance for out-of-distribution molecular representations, NeurIPS 2022\n\n[3] Yongduo Sui et al. Causal Attention for Interpretable and Generalizable Graph Classification, KDD 2022.\n\n[4] Jiaqi Ma et al. Subgroup Generalization and Fairness of Graph Neural Networks, NeurIPS 2022.\n\n---\n\n### **Comment 2: Our assumption**\n\nThanks for this comment! In Section 3.1, we have discussed the assumptions in detail with specifications. We repeat part of it here for your convenience:\n\n```Assumption 1 essentially states that the distribution shifts (whatever they are) can be decomposed into several mixture components of stochastic graph transformations. For example, on a social network dataset, each mixture component can represent different patterns of user behavior or network dynamics shifts. Specifically, one mixture component might correspond to increased user activity, while another could signify a particular trend of interaction within a certain group of users. Such a mixture pattern is common and well-studied in the real-world network datasets (Newman, 2003; Leskovec et al., 2005; 2007; Peel et al., 2017).```\n\nWe hope the analysis provides detailed illustration to the assumption. We are happy to add more discussion if you think anything  is still unclear!\n\n---\n\n### **Comment 2 & 3: Implementation of our method**\n\n- **Model architectures**: \nPlease see Tables 2 and 3 in Appendix A for detailed model architecture information. Specifically, we use the same encoders and classifiers from GOOD benchmarkfor real-world datasets to ensure fair comparisons. We employ backbones based on the best ERM performance for synthetic datasets. Moreover, GraphMETRO is model-agnostic, which consistently improves performance across varied model architectures.\n\n- **Stochastic transformation**: Please see Appendix B where we include the introduction of the stochastic transform functions. \n\n- **Optimization process**: Please see Table 2 and 3 (Appendix A), where we included hyperparameters on each dataset. In the second paragraph after Eq. (3), we also described our training pipeline: `“...we set apart the other loss terms from backpropagating to it to avoid interference with the training of the gating model… We optimize the objective via stochastic gradient descent”`. \n\n- **For a more intricate optimization strategy:** This is a great catch! In fact, we did try to pretrain the gating model for several epochs as warm up before training the whole model in an end-to-end fashion via the objective Eq. in (3). However, we didn’t notice a statistically significant difference in their performance, which can be due to that expert models take more time to convergence (since they need to align with the base model during the training) compared to the gating model.\n\nDue to the space limitation, we had to include most of the implementation details in the appendix while we added pointers in the main paper. However, we will try to make it more detailed and feel free to let us know if anything is missing! \n\n---\n\n### **Comment 2 (Cont.): Understand the functionality, underlying mechanism, and performance gain**\n\nThanks for the comment. For clarity, we provide the following pointers to the paper:\n\n- In Section 4.3, we provide a study to reveal the underlying **mechanism** of GraphMETRO, i.e., each expert excels in generating invariant representations concerning a stochastic transform function, which provide a solid foundation in generating referential invariant representations w.r.t. the specific transformations and further guarantee the generalization.\n\n- In Appendix C, we study the **impact of the MoE architecture** on model performance, which shows that the model performance may decline if the expressiveness of the expert model decreases. \n\n- In Appendix D, we study the **impact of the stochastic transform function** on model performance, where we also provide a detailed discussion of the modeling mechanism.\n\nWe believe the above studies and discussion in our paper provides an in-depth view, highlighting the roles of our objective, architecture, and stochastic transform function.  Please let us know if any of these perspectives is still unclear, and we can further improve our experimental study.\n\n-----\n\n# Summary\n\nWe hope our answers can address all of the concerns. We are happy to follow up if you have any further questions.\n\nWe also prudently ask you to reevaluate our work. To highlight, our motivation is supported by the common fine-grained graph shift heterogeneity, and the fact that most of the previous works could not model such nuanced distribution shifts in an effective and flexible manner. Moreover, we added more related works and modified our statement to position our work better. In general, we believe GraphMETRO is a more general and flexible solution that can mitigate a wider range of distribution shifts, which is backed by the experimental results. Detailed justification on our assumption and implementation details are also available in our paper. \n\nThus, we believe our work makes important contributions and provides a clear presentation. We are happy to discuss more and revise our paper if any concern remains. Thank you for your efforts and we are looking forward to your reply!!\n\n---\n\nWe appreciate your comments! To address your concerns, below we prudently justify the motivation of our proposed method, clarify our assumptions, and provide details regarding our experiment implementation.\n\n---------------------\n\n### **Comment 1: Motivation of this work**\n\nThank you for this comment! We believe there might be a bit of misunderstanding due to our different definitions of *“graph shift heterogeneity”*. We firstly discuss the related works mentioned and then justify our statement:\n\n**(1) Related works**\n\nPlease see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nMoreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you! \n\nSpecifically, Sui et al. [1] proposed a graph data augmentation strategy that alleviates covariate shift by generating diverse and invariant causal features. However, the trainable augmenter they used may not distill diverse augmentations or construct unseen perturbations. Moreover, Sui et al. [1] test its method only on graph classification tasks, while GraphMETRO can be applied to both node and graph classification tasks. Besides, we have discussed graph augmentation and attention-based methods in our related works, and we added more recent works on graph OOD [2,3,4], and we hope our response clears your concern on the related work discussion.\n\n\n**(2) The definition of graph shift heterogeneity**\n\nIn this work, we refer to **“heterogeneous shifts”** as multiple and different levels of shifts which vary across different instances (nodes or graphs), as illustrated in the example in the abstract. While we agree that the existing invariant learning approches can accommodate multiple distribution shifts, it could be hard for them to tackle nuanced distribution shifts for individual instances (nodes or graphs) since the distribution shifts are inferred from variance across multiple data environments. If GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering **a mixture of transformations as a proxy** for the target distribution shifts rather than invariance to whole-graph environment shifts. This is the type of heterogeneity we are interested in our paper. \n\n**(3) Regarding our original motivation statement**\n\nWhile the statement pointed out by the reviewer serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category. And we have provided detailed discussion about three lines of research in the related work section.\n\nHowever, we agree that we could make this statement border to cover the previous invariant learning methods. To improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to \"**previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments…**”. We also modified our introduction correspondingly, we hope this will better position our work.\n\n**(4) The choice of our MoE design**\n\n- The choice of our approach comes as a consequence of our motivation to model the graph/instance shift heterogeneity. As mentioned, mitigating multiple and nuanced distribution shifts simply goes beyond certain distribution shift types or environment construction as seen in the previous methods. \n- Thus, GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts. This enables the prediction of multiple different distribution shifts and the flexibility to model fine-grained heterogeneity since the mixture can be varied across different instances. We then tackled the proxy to mitigate the target distribution shifts. Intuitively, this solution provides a **“middle ground”** to deem graph generalization as an equivalent mixture, which, we believe, is a more tractable solution.\n\nWe updated our paper to make the above point more clear. We genuinely hope our answer can justify the motivation and solve your concern.\n\n---\n\n### **Question 2: How does the computational cost of GraphMETRO compare to other methods?**\n\nPlease see the last paragraph of Section 3.4, where we analyze the computation complexity of GraphMETRO. We repeat part of it here for your convenience:\n\n```Consider the scenario where we use an individual encoder for each expert. The forward process of $f$ involves $O(K)$ forward times using the weighted sum aggregation (or $O(1)$ if using the maximum selection). Since we extend the dataset to $(K + 1)$ times larger than the original data, the computation complexity is $O(K^2 |D_s|)$, where |Ds| is the size of the source dataset.```\n\nThus, the computation cost is about $K^2$ or $K$ times (if using the maximum selection) than an ERM model, where $K=5$ in our experiments. Compared to DIR, as they extract $B$ spurious subgraphs from each batch to conduct the intervention, their computation cost is $B$ times compared to ERM, where $B$ could be 32. Thus, we believe the computation cost of GraphMETRO is fair for the gains we get, considering $K$ is usually small.\n\n----------------------\n\n### **Question 3: Can GraphMETRO handle multiple different types of distribution shifts that simultaneously exist in data?**\n\nYes! The distribution shift types corresponding to the gating outputs with high scores will be tackled during training. That is, if the gating output highlights multiple mixture components, their corresponding distribution shift types will be handled jointly.\n\n----------------------\n\n### **Question 4: Can GraphMETRO  tackle distribution shifts on molecular graphs?**\n\nThat is a great idea! Yes, GraphMETRO can be applied to molecular datasets if one designs transform functions to cover typical molecular variants. For instance, a transform function may add carbon structures to the molecules. These domain-specific transform functions are outside the scope of our work, however, we believe these would be interesting future work directions!\n\n----------------------\n\n# Summary \n\nWe thank the reviewer for the time and insightful suggestions! We hope our answers can address your concerns well.\n\nWe also prudently ask you to reconsider our work if the concerns are addressed. To highlight, our novelty comes from the formulation of an equivalent mixture for graph OOD and the training framework to effectively realize generalization. We also provide an in-depth analysis on our originality compared to some previous invariant learning methods. While we discussed and compared with previous works, we added more related works and modified our statement to position our work better. Finally, our method achieves great improvements on both node and graph classification tasks, and is a more general solution to mitigate multiple and nuanced distribution shifts. \n\nOverall, we believe our work proposes a new paradigm and novel training framework and makes good contributions in the fields of graph generalization, and we would appreciate your reconsideration on this point. Thank you for your efforts again!\n\n----------------------\n\n**Reference**\n\n\n[1-5] The same as listed by the reviewer\n\n\n[6] Beatrice Bevilacqua, Yangze Zhou, and Bruno Ribeiro. Size-invariant graph representations for graph classification extrapolations. In ICML, 2021.\n\n[7] Davide Buffelli, Pietro Li´o, and Fabio Vandin. Sizeshiftreg: a regularization method for improving size-generalization in graph neural networks. In NeurIPS, 2022.\n\n[8] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In NeurIPS, 2019.\n\n[9] Mucong Ding, Kezhi Kong, Jiuhai Chen, John Kirchenbauer, Micah Goldblum, David Wipf, Furong Huang, and Tom Goldstein. A closer look at distribution shifts and out-of-distribution generalization on graphs. In NeurIPS DistShift, 2021.\n\n---\n\n### **Comment 3 & Question 1: Related works on invariant learning and clarification on our statement.**\n\nThanks for pointing it out! Below we clarify the statement and clear potential misunderstanding:\n\n**(1) How does GraphMETRO compare with invariant learning methods like DIR and EERM?** \n\nPlease see our related work section where we discussed these two papers, i.e., DIR (Wu et al., 2022c) and EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on environmental patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nBesides, we also provide a more in-depth comparison in our point (3) below to highlight our key technical originality.\n\n**(2) Regarding our statement about previous works:** \n\nWhile the statement serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category, and we have provided detailed discussion about three lines of research in the related work section.\n\nTo improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to **\"previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments** (which is highly limited when confronted with nuanced distribution shifts)”. We also modified our introduction correspondingly. Thanks for letting us know our statement could be misinterpreted.\n\n**(3) Why do we say our method could be more broad than the existing invariant learning approaches?** \n\n- Invariant subgraph learning approaches, e.g., [1,2], consider variance of constructed data environments, which are designed very differently compared to our work. \nWhile they can accommodate multiple distribution shifts (as in multiple environments), these focus on patterns within each environment and ignore the variety across instances (e.g., shifts at the resolution of nodes), which may not be well-captured by the environment assignments. \n- GraphMETRO considers that specific parts of the test graph may have different shifts. Particularly, our goal is to make the generalization to unknown testing distribution more adaptive and broad, as opposed to limiting the distribution shifts to being invariant to specific types of subgraphs. \n\nIn other words, if GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering a mixture of transformations as a proxy for the target distribution shifts rather than invariance to environment shifts.\n\n**(4) Key technical originality compared to invariance learning (going more deeply)**\n\nAnother interesting view to see the innovation of GraphMETRO is that it breaks the typical invariant learning formulation, which assumes the data is manipulated by the environment variables (and then can be “decoded” into multiple environments). Instead, GraphMETRO sees the distribution shifts on an instance as a mixture, which is represented by the score vector output by the gating function over the basis of the transform functions. In other words, GraphMETRO can produce infinite environments as the elements in the score vector are continuous. One can see that once we limit the output domain of the gating function into, e.g., binary {0, 1}, GraphMETRO can also produce a limited number of environments (if we categorize the instances based on the score vector), which covers the environment construction in invariant learning. Moreover, as mentioned, we propose the concept of referential invariant representation with a base model $\\xi_0$, which is also different from previous works on invariant learning. We added the above discussion to Appendix F to improve the depth of our analysis.\n\n---\n\nWe appreciate your efforts and insightful comments! To address your concerns, we provide point-to-point responses below.\n\n---------------------\n\n### **Comment 1: Regarding the novelty of GraphMETRO.**\n\nThanks for the comment! We believe our novelty comes from the proposal of an equivalent mixture for graph OOD and the construction of our training framework, as detailed below:\n\n- **An equivalent mixture for graph OOD**: The key challenge we faced to mitigate multiple and nuanced distribution shifts is the intrinsic complexity and heterogeneity of graph distribution shifts, which simply goes beyond certain distribution shift types [6,7,8,9] or environment construction as seen in the previous methods [1,2,3]. GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts, where the mixture can be varied across different instances, and then tackled the proxy to mitigate the target distribution shifts. We believe the high-level idea is succinct, nevertheless, novel, in the sense that it provides a “middle ground” to deem graph generalization as an equivalent mixture that is more tractable.\n\n- **Training framework**: With the guide of our formulation, the training framework is still non-trivial due to two problems, i.e., “how to provide supervision for predicting the mixture” and “how to ensure the experts corresponding to mixture components are compatible when working as a whole”. Specifically, GraphMETRO solves the first problem by conducting graph extrapolation. This is somewhat similar to the spirit of graph pretraining in the sense that we inject heterogeneity to promote the expressiveness of the gating model in recognizing the mixture components. For the second problem, we introduce the concept of Referential Invariant Representation, along with the novel objective in Eq. (3) to enforce the invariance and compatibility. It is worth mentioning that the model performance is much worse than the reported numbers (e.g., 2.7\\% lower on Twitch dataset) without the compatibility constraint, indicating the proposed referential invariance concept is indispensable. \n\nWe added more justification in the introduction (updated in the revision). We genuinely hope our responses can solve your concerns about the novelty of our work.\n\n------------------------------\n\n### **Comment 2: Comparison with recent methods [3,4,5].**\n\nIn compacting the paper to fit in the page limit we mistakenly did not include these relevant references, we apologize. We added discussion in the revised version. Here we summarize these works and point out their key differences with our method: \n\n- In particular, **Yang et al. [3]** explore molecule representation learning in out-of-distribution (OOD) scenarios. They achieve this by directing the molecule encoder to utilize stable and environment-invariant substructures relevant to the labels without the need for environmental labels. \n- Similarly, **Sui et al. [4]** introduces causal attention modules to identify key invariant subgraph features that can be described as causing the graph label. The type of OOD task that Sui et al. [4] considers assumes the graph label is caused by a subgraph, which is quite different from ours. Moreover, both Yang et al. [3] and Sui et al. [4] consider tasks where the graph label is caused by a subgraph. \n- **Ma et al.[5]** is an interesting theoretical work which studies GNN generalization and examines their fairness, showing that the test subgroup's distance from the training set impacts GNN performance. Ma et al.[5], as far as we could assess, does not propose any specific architecture to solve the type of OOD tasks we consider in our work.\n\nOverall, the goal of GraphMETRO is to be invariant to a mixture of selected stochastic transform functions (and the mixture can vary across different instances), which is a more flexible and general solution. \nWe added a discussion of these works to our revision, hopefully providing a more comprehensive comparison and literature overview. We hope our responses can solve your concerns about the related work.\n\n---\n\nWe sincerely appreciate all reviewers' time, efforts, and valuable suggestions in reviewing our paper. We are glad that most of the reviewers reached a positive consensus on our work's motivation and experimental results. Here is a summary of our responses:\n\n- **Clarification**: We clarify our key assumptions (`m8JS, FURX`), novelty (`m8JS, NLg1`), and the experimental settings (`m8JS`).\n\n- **Related work**: : We add more related works on recent graph OOD learning (`NLg1, m8JS`) and graph transfer learning (`FURX`)\n\n- **Limitation**: We add discussion about the limitations of GraphMETRO when confronted with unknown distribution shifts that surpass our key assumptions (`FURX`, `C9mM`).\n\nWe hope our responses can clarify your confusion and alleviate concerns and we updated our paper **(highlighted in green)** corespondingly. We thank all reviewers again, and look forward to your reply!", "author_response": "We appreciate your comments! To address your concerns, below we prudently justify the motivation of our proposed method, clarify our assumptions, and provide details regarding our experiment implementation.\n\n---------------------\n\n### **Comment 1: Motivation of this work**\n\nThank you for this comment! We believe there might be a bit of misunderstanding due to our different definitions of *“graph shift heterogeneity”*. We firstly discuss the related works mentioned and then justify our statement:\n\n**(1) Related works**\n\nPlease see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nMoreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you! \n\nSpecifically, Sui et al. [1] proposed a graph data augmentation strategy that alleviates covariate shift by generating diverse and invariant causal features. However, the trainable augmenter they used may not distill diverse augmentations or construct unseen perturbations. Moreover, Sui et al. [1] test its method only on graph classification tasks, while GraphMETRO can be applied to both node and graph classification tasks. Besides, we have discussed graph augmentation and attention-based methods in our related works, and we added more recent works on graph OOD [2,3,4], and we hope our response clears your concern on the related work discussion.\n\n\n**(2) The definition of graph shift heterogeneity**\n\nIn this work, we refer to **“heterogeneous shifts”** as multiple and different levels of shifts which vary across different instances (nodes or graphs), as illustrated in the example in the abstract. While we agree that the existing invariant learning approches can accommodate multiple distribution shifts, it could be hard for them to tackle nuanced distribution shifts for individual instances (nodes or graphs) since the distribution shifts are inferred from variance across multiple data environments. If GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering **a mixture of transformations as a proxy** for the target distribution shifts rather than invariance to whole-graph environment shifts. This is the type of heterogeneity we are interested in our paper. \n\n**(3) Regarding our original motivation statement**\n\nWhile the statement pointed out by the reviewer serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category. And we have provided detailed discussion about three lines of research in the related work section.\n\nHowever, we agree that we could make this statement border to cover the previous invariant learning methods. To improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to \"**previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments…**”. We also modified our introduction correspondingly, we hope this will better position our work.\n\n**(4) The choice of our MoE design**\n\n- The choice of our approach comes as a consequence of our motivation to model the graph/instance shift heterogeneity. As mentioned, mitigating multiple and nuanced distribution shifts simply goes beyond certain distribution shift types or environment construction as seen in the previous methods. \n- Thus, GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts. This enables the prediction of multiple different distribution shifts and the flexibility to model fine-grained heterogeneity since the mixture can be varied across different instances. We then tackled the proxy to mitigate the target distribution shifts. Intuitively, this solution provides a **“middle ground”** to deem graph generalization as an equivalent mixture, which, we believe, is a more tractable solution.\n\nWe updated our paper to make the above point more clear. We genuinely hope our answer can justify the motivation and solve your concern."}
{"claim": "The claim that graph shift heterogeneity is under-explored is contradicted by a wide spectrum of existing approaches and literature.", "claim_type": "novelty", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ppMpJyIYRZ", "reviewer": "Reviewer_m8JS", "review_text": "Summary: The paper introduces a method to enhance the out-of-distribution performance of graph neural networks (GNN) by learning to understand distribution shifts instead of addressing the assumed ones. To achieve this, the Mixture of Experts architecture is integrated into the GNN, supplemented by an alignment procedure to recognize the shift. Empirical experiments are conducted to validate the theoretical assertion.\n\nOn the whole, I believe the proposed method lacks the necessary motivation and its novelty isn't substantial enough to meet the standard.\n\nStrengths: - The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\n- Real-world datasets back the claims through experiments.\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\n\nWeaknesses: - The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \"previous research has concentrated on addressing specific types of distribution shifts.\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \"environment generators\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\n\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\n\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\n\nQuestions: Please check Weaknesses.", "labeling_timestamp": "2026-01-11T16:32:10.333767", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The authors explicitly disagree with the reviewer’s characterization and defend their original claim: they acknowledge related prior work but argue that existing methods do not address the instance-wise (nuanced) graph shift heterogeneity they target, and they revised/clarified their positioning rather than conceding the reviewer's point.", "evidence": "\"We believe nuanced heterogeneous shifts are common, important, yet being typically ignored in the research domain of distribution shifts, we made these argument clear and also promise to refine.\"", "combined_author_response": "Dear reviewer NLg1,\n\nAs the discussion period is closing soon, we hope to engage with you and check if your concerns are addressed.\n\nWe sincerely appreciate your time and attention!\n\nBest,\n\nAuthors of Paper 1560\n\n---\n\nThanks for reading our response! We would also appreciate your patience for reading the two points below \n\n---\n\n**Regarding point #1:**\n\nYes, we agree that the concept of heterogeneous shifts is relatively new for the current studies on distribution shifts, however, this is not new for studies on network patterns (Newman, 2003; Leskovec et al., 2005; 2007; Peel et al., 2017). In fact, the ignorance of such nuanced heterogeneous shifts in the previous studies instead emphasises our motivation and the potential impact of this work. \n\nFrom a causality perspective, distribution shifts can naturally happen in the instance level when (1) additional causal variables, beyond environmental factors and randomized noise, influence these shifts, or (2) multiple causal variables simultaneously affect the shifts with different strengths. Without modeling these explicitly, the mitigation of distribution shifts can easily fail. \n\nIn terms of our presentation, we have illustrated these cases of interest in abstract and introduction, we made further explanations in Section 3.1, we also present the specific results of distribution types in Figure 3 (b). \n\n\n**Regarding point #2:**\n\nWe think we can all agree that, if the ground truth of the instance shifts is available on the real-world datasets, it would be crystal clear to see where the improvement comes from since we can conduct case study to compare our method and the baselines on instances with nuanced distribution shifts, to see the influence of the modeling these heterogeneous shifts. \n\nHowever, with such ground truth not available, we had to seek other seemingly less intuitive but also in-depth way to illustrate the insights (esp Section 4.3) as mentioned above in our previous response. This is also why we designed the synthetic experiments at the first place. We think we did try hard to explain the underlying rationale with the ground truth being absent.\n\n----\n\n**Refinement**: We can of course add a causal graph in the our assumption section to make the concept more clear. And we can illustrate more if you could let us know the specific obstable to understand our mechanism, which will be extremely helpful.\n\n----\n\n### **Summary/TL;DR**\nWe understand the reviewer's clarification concerns, however, we don't agree that they, based on our justificaton, are the cause of rejection. We believe nuanced heterogeneous shifts are common, important, yet being typically ignored in the research domain of distribution shifts, we made these argument clear and also promise to refine. While explaining the underlying rationale is hindered by the lack of ground truth, we did try hard to dissection it from the gating model, the invariant representations generated, and more ablations in the appendix. We respect the reviewer's current opinion. Still, reconsideration will be greatly appreciated.\n\n---\n\nWe thank the reviewer once again! We also enjoyed the process of making our work more sound from your suggestions. And your approval surely means a lot to us.\n\nWe do apologize for not making our solutions towards the limitations clear enough. We summarize them into a short table, hopefully could alleviate your concerns a bit more.\n\n| Limitation| Potential solutions | Location of discussion/action |\n|:---|:---|:---|\n|**Coverage of the transformations** | 1) Extend the coverage by adding representative transform functions. 2) Or include tranform functions based on domain knowledge or a few samples from target distribution | Appendix F, paragraph #3|\n|**Complexity of the transformations** | Make experts dedicated to different hyperparameters for the same type of transformation | Argument options are available in our codebase. Will make it more detailed in the experimental settings. |\n|**Label distributional shift**|Integrate the objective of the existing methods studying labe distributional shifts into our framework. |Appendix F, paragraph #4 |\n\n----\n\nWe completely agree on the existence of these limitations and we will move some limitations (esp #1) to the main paper in our final version. While they could be important in the practice, they are, in our perspective, fair \"side effects\" considering the benefits (i.e., mitigating multiple and nuanced distribution shifts and better interpretability), and may not be the central part of our novelty and main contribution (i.e., the proposal of an equivalent mixture, the concept of referential invariant representations, as well as the training framework). We are also eager to further improve other aspects.\n\nOnce again thank you so much for your support!!\n\n---\n\nDear Reviewer m8JS,\n\nA gentle nudge that we would like to know if our response adequately addresses your concerns. \n\nYour time and feedback is greatly appreciated!\n\nThank you sincerely,\n\nAuthors of Paper 1560\n\n---\n\nWe deeply appreciate your approval. Your suggestions definitely inspired us a lot and have greatly improved our work.\n\nA few additional notes for the further comments:\n\n- Regarding **point #2**: In node classification tasks, the BCE objective already considers node embeddings other than the graph embedding of the k-hop subgraph. What we described previously refers to the computational graph (i.e., k-hop message passing), which generates node embeddings. We apologize for any confusion.\n\n- Regarding **point #3**: Yes it would be quite interesting to see how graph pretraining methods perform on the current OOD benchmarks. Graph pretraining methods like GCC and GPT-GNN also consider graph extrapolation to some extent, e.g., through subgraph extraction and masked attributes/structures. The key difference between graph pretraining and generalization may lie in their different focuses on **expressiveness** and **invariance**. While these two aspects do not always conflict, ensuring invariance w.r.t. a certain type of extrapolation might affect expressiveness (if the change is relevant to labels), and vice versa. To seek a balance in between, one might need prior knowledge in which types of transformations may or may not be sensitive to the labels (and perhaps build experts with different goals to make ensure invariance or expressivess). We believe there is still a lot to explore in this domain.\n\nOverall, we are grateful for your positive stand on this work. We believe that our [current version](https://openreview.net/pdf?id=QQ5eVDIMu4), incorporating opinions from you and other reviewers, is sound and well-refined. We are also committed to further improving it. We would appreciate your support based on our current version! \n\nThank you!\n\n---\n\nDear Reviewers of Paper 1560,\n\nWe hope this message finds you well.\n\nAs the discussion phase approaches its end, we hope you find our responses useful. We would like to ask if the issues have been addressed.\n\nWe understand that the discussion time is short, and some of you might be enjoying holidays at the moment. We apologize for posting our responses a bit late as we aimed to address your concerns clearly.\n\nWe sincerely appreciate your time and attention!\n\nBest regards,\n\nAuthors of Paper 1560\n\n---\n\n**Comment 4: Related works on graph transfer learning.**\n\nYes! We agree that graph transfer learning is a relevant topic. Thank you for providing the additional related work! We added the discussion on these works in our revision, and we repeat it here for your convenience:\n\n```It is also worth mentioning that graph domain adaptation (Zhang et al., 2019; Wu et al., 2020), different from the problem studied in this work, commonly relies on limited labeled samples from target datasets for improved transferability. For instance, to generate domain adaptive network embedding, DANE (Zhang et al., 2019) uses shared weight graph convolutional networks and adversarial learning regularization, while UDA-GCN (Wu et al., 2020) employs attention mechanisms to merge global and local consistencies.```\n\nMoreover, we believe this is actually relevant to `Comment & Question 1` regarding the generality of the distribution shifts covered by the transform functions. In our future works, we added the following discussion:\n\n```...Leveraging a few samples from target distribution (i.e., domain adaptation). Specifically, we can leverage the samples from the target distribution to inform the selection or construction of transform functions, which can better guarantee the distribution shifts are covered by the transform functions.  For example, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space. We believe this would be an interesting future direction.```\n\n---\n\n# Summary\n\nOnce again thank you very much for helping us position our work and reflect the literature better. We hope all of your concerns are solved and we are happy to engage further if there are any other points we missed!\n\n---\n\n### **Reference**\n\n[1] Unsupervised Domain Adaptive Graph Convolutional Networks. Wu et al. WWW 2020\n\n[2] DANE: Domain adaptive network embedding. Zhang et al. IJCAI 2019.\n\n[3] Tong Zhao, Yozen Liu, Leonardo Neves, Oliver J. Woodford, Meng Jiang, and Neil Shah. Data augmentation for graph neural networks. AAAI, 2021.\n\n---\n\nWe are grateful for your positive feedback and detailed suggestions! We provide responses below to address your remaining concerns. \n\n---\n\n### **Comment & Question 1: Generality of the distribution shifts covered by the transform functions.**\n\nGood question! We provide the response from two angles:\n\n\n**(a) For general domain**: In our experiments, we mainly use the five stochastic transform functions, which are universal graph augmentations as listed in Zhao et al., (2021) [3]. In our code implementation, we have also included additional transform functions as shown in Appendix B. We believe these transform functions, while not exhaustive, still cover a wide range of distribution shifts observing from our experimental results.\n\nNevertheless, we agree that the real graph distribution shifts can go beyond any possible combinations of the predefined transform functions. In that case, the assumption may not hold, meaning that GraphMETRO may not capture and precisely mitigate the unknown distribution shift. This scenario could always possibly exist due to the lack of information about the testing distribution or its domain knowledge. We include it as a limitation in Appendix 5, while we further discuss how we could alleviate the problem with additional information.\n\n**(b) For specific domains where additional knowledge is available**: In fact, knowing the tendency of the distribution shifts, such as increasing malicious users in a trading system, would be very helpful in constructing the transform functions that can cover the target distribution shifts well. We believe that such knowledge can come from two sources: \n- **Domain knowledge**, e.g., on molecular datasets, the transform function could be adding additional carbon structure to a molecule (while preserving its functional groups). Or, in a particular social network, transform functions can be defined from known user behaviors. \n- **Leveraging a few samples from target distribution**. This is in fact in line with the reviewer’s Comment #4 regarding graph transfer learning. Specifically, with the guide from a few target samples, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space.\n\n We hope this response can alleviate your concern about our applicability.\n\n---\n\n### **Comment & Question 2: The mechanism of GaphMETRO in handling heterogeneous shifts within one graph.**\n\nGood question! For simplicity, suppose we have two transform functions, i.e., adding edges and dropping edges. Given a node classification task and the objective in Eq. (3), the model is trained on the extrapolated datasets based on the transform functions. After that, given an unseen graph with imbalanced distribution shifts, the gating model outputs scores to identify nodes likely to experience increasing or decreasing degrees, while others might adhere to the original distribution. Then, for each node, each expert takes its multihop subgraph, and outputs its referential invariant representation w.r.t. the correposing transform function. These expert outputs and gating model results are then aggregated to form the final representation. Assuming accurate predictions by the gating function, nodes in denser/sparser subgraphs are represented by expert models corresponding to adding/dropping edges. As each expert is trained to create invariant representations, the final node representations remain unaffected by their individual distribution shifts.\n\n---\n\n### **Comment 3: How to leverage graph pretraining.**\n\nIf we understand correctly, the reviewer was asking how to leverage a pretrained model to further aid the training of GraphMETRO. Please let us know if otherwise. \n\nThis is in fact an interesting point! We believe a model pretrained on a wide variety of data can be very helpful to initialize the gating model, which is required to output the mixture of a node or graph (after it is finetuned on the extrapolated dataset). By enhancing the gating model's predictive capability regarding mixtures, GraphMETRO's final representation should become more resilient. This becomes particularly advantageous when dealing with graphs not previously encountered in the extrapolated dataset. Moreover, it is possible that the pretrained model will also benefit the expert models, while one minor concern would be that the expert model may tend to be similar instead of dedicating to generating invariant representation w.r.t. their corresponding transform function. We added the above discussion to Appendix F as a future work. Thanks again for this comment!\n\n---\n\n### **Question 4: Why does the test accuracy vary across different transformations?**\n\nGreat question! Here we summarize three possible reasons:\n\n- **Information Preservation in Transformations:** Certain transformations retain more informative features than others. For instance, in the REDDIT-BINARY (graph classification task), the random subgraph transformation may retain more graph label-related information compared to dropping edges, as the latter tends to lose more global information. This discrepancy in testing performance, where dropping edges outperforms random subgraph extraction, could be due to the preservation of crucial information. However, conclusions may vary across datasets or tasks depending on how information influences final predictions. For CiteSeer (a node classification task), a random subgraph might preserve more local node information, potentially explaining why its testing performance surpasses dropping edges in this specific task.\n\n- **Complexity of transformation:** Certain transformations inherently generate more diverse graphs than others. If the model lacks the expressive capacity to capture such diversity, it may lead to a decline in testing performance.\n\n- **Model Sensitivity:** Certain transformations may be easier for a model to learn due to compatibility with specific model architectures. This extends beyond transformation complexity and emphasizes how different model architectures may prefer learning particular distributions from one of the extrapolated datasets, which can also contribute to the difference in the testing performance.\n\nWe included the above discussion to Appendix F: Open Discussions. We hope this response can answer your question and improve the soundness of our work.\n\n---\n\n# Summary \n\nWe are grateful for your time and insightful suggestions! \n\nWe would like to highlight that our main contribution is framing the graph generalization problem on top of an equivalent mixture, a simple yet novel and tractable \"middle ground\", as well as proposing the training framework which effectively guarantees the generalization. While our method relies on a set of predefined transform functions, we believe they cover a wide range of distribution shifts based on our empirical results. Also, we agree that there could be some scenarios where the transform functions may not cover complex distributions, and we discuss two future directions and include them into our future works. Moreover, while selecting the hyperparameters for the transform functions introduce extra complexity, the issue could be minor in practice and we also conduct more experiments to justify the applicability better. Finally, we address several questions about clarification and presentation, as well as including more future works. \n\nLastly, we prudently ask you to reevaluate our work given the clarification in our responses, which we also updated our paper correspondingly. Overall, we believe our work makes good contributions to the field of graph distribution learning by proposing a novel and effective solution, and we would appreciate your reconsideration on this point. Thank you for your efforts again!\n\n---\n\n### **Reference**\n\n[1] Menon, Aditya Krishna, et al. \"Long-tail learning via logit adjustment.\" International Conference on Learning Representations. 2020.\n\n[2] Cao, Kaidi, et al. \"Learning imbalanced datasets with label-distribution-aware margin loss.\" Advances in neural information processing systems 32 (2019).\n\n[3] DARTS: Differentiable Architecture Search. Hanxiao Liu, Karen Simonyan, Yiming Yang. 2018.\n\n---\n\n**(3) What is the complexity of the transform functions and how does it affect generalization?** \n\nInteresting question! In fact, our implementation and framework could easily avoid selecting hyperparameters on the transform functions. Specifically, we can make multiple transform functions of the same type with different ranges of hyperparameters. Specifically, GraphMETRO allows three edge dropping transform functions, $\\tau_1^{\\alpha_1}, \\tau_2^{\\alpha_2}, \\tau_3^{\\alpha_3}$, where $\\alpha_i$ (i=1, 2, 3) are three different ranges of edge dropping probabilities, e.g., [0.1,0.3], [0.3, 0.6], [0.6,0.9], representing different transform extents. Thus, given an input from the validation dataset, the gating model will highlight the transform function which simultaneously selects the corresponding hyperparameter that matches the distribution of the validation set. Interestingly, this idea is in the same spirit as how DARTS [3]  proposes to perform architecture search by formulating the task in a differentiable manner. \n\nIn our previous experiments, we did try this scheme where we replaced a single edge dropping transform function with the ratio range [0.3, 0.5] to three transform functions as mentioned above. While we didn’t see a significantly different performance in that case, we believe this would be a flexible solution which avoids the need to conduct hyperparameter selection. \n\nWe include the above discussion in our open discussion and future works (Appendix F). We hope this response can alleviate your concern about the applicability of our method. \n\n---\n\n### **Comment 4: Limitations of our work**\n\nGreat point! We think the issue of label distributional shift, while important, is orthogonal and complementary to the focus of our current study. To elaborate, label distributional shifts exert analogous impacts across various modalities, such as graphs or images. Moreover, existing methods [1,2] designed to tackle label distributional shifts can be seamlessly integrated into our proposed framework. Such integration would necessitate minimal adjustments, potentially involving modifications to the loss function or the training pipeline. We added this as a future work in Appendix F.\n\n---\n\n### **Question 1: Statistical significance of the results on Table 1**\n\nThanks for the question! We compute the p-value of our method against the best baselines method as follows:\n\n| | WebKB | Twitch | Twitter | SST2 | \n|:--|:--|:--|:--|:--|\n|p-value|< 0.001| 0.023 | 0.042| 0.081|\n\nGiven the cut-off threshold as 0.05, we believe the performances of GraphMETRO are statistically significant on WekGB, Twitch, and Twitter datasets, while on the SST2, we see relatively weak evidence. We added the p-value results to our revision and hope our response can alleviate your concern on our improvements.\n\n---\n\n###  **Question 2: Can we say we use ERM for the node classification even if nodes and their labels are not i.i.d.?** \n\nIf we understand correctly, the reviewer is asking for clarification on the 2nd term of our objective. Please let us know if otherwise. Here, our thinking is that the cross-entropy loss for node classification already assumes node labels are conditionally independent given the model (the negative log-likelihood is a sum over the labeled nodes in training). Then, we use the same assumption of cross-entropy on Empirical Risk Minimization (ERM). That is, for a given model we must also minimize the error variance across nodes. The task is then to find the model with the best performance and small variance.\n\n---\n\n### **Question 3: Numerical results on Figure 2**\n\nThanks for pointing it out! We included all of the numerical results of Figure 2 in Appendix E, while showing the results on DBLP below.\n\n| |i.i.d. (0)|noisy feature (1)|add edge (2)|drop edge (3)|drop node (4)|random subgraph (5)|\n|:--|:--|:--|:--|:--|:--|:--|\n|ERM | 85.71 | 84.48 | 71.08 | 79.69 | 83.41 | 76.9|\n|ERM-Aug | 85.66 | 85.29 | 74.85 | 82.34 | 84.44 | 72.81|\n|GraphMETRO | 85.92 | 85.78 | 76.61 | 82.95 | 84.98 | 81.32|\n\n| |(4, 5)|(3, 5)|(2, 5)|(1, 5)|(2, 4)|(1, 4)|     (2, 3)|(1, 3)|\n|:--|:--|:--|:--|:--|:--|:--|:--|:--|\n|ERM | 70.4 | 77.63 | 81.99 | 79.69 | 70.55 | 71.52 | 77.73 | 79.59|\n|ERM-Aug | 74.16 | 81.04 | 83.65 | 68.62 | 74.01 | 68.27 | 81.13 | 84.49|\n|GraphMETRO | 76.18 | 81.71 | 84.26 | 80.31 | 75.1 | 71.05 | 81.85 | 87.14|\n\nAcross all of the synthetic environments, GraphMETRO averagely outperforms ERM and ERM-Aug by 3.20% and 2.45%, respectively.\n\n---\n\nWe appreciate your comments! To address your concerns, below we prudently justify the assumption of our method, the predefined transformation functions, as well as their complexity, and clarify our presented results.\n\n---\n\n### **Comment 1-3: Applicability and our assumptions**\n\nThanks for these great comments! Here we provide response in three folds:\n\n**(1) How does GraphMETRO identify all distribution shifts from transform functions if they are treated independently when combined together?** \n\nIf we understand correctly, by “treated independently”, the reviewer is referring to the first term in our objective $\\text{BCE}(\\phi(\\tau^{(k)}(\\mathcal{G})), Y (\\tau^{(k)}))$, where we formulate predicting the distribution shifts types of a jointly transformed graph as a binary multiclass classification problem. We believe the difficulty of this task comes from both the property of transform functions and the expressiveness of the gating model. \n\n- Firstly, some transform functions are **inherently disentangled**, e.g., adding nodes feature noise and random subgraph extraction. In this case, there will be certain distinction between any pair from these three data distributions, i.e., (graphs with node noise, random subgraph graphs, random subgraphs with node noise), which the gating model can easily tell. \n- While some transform functions can be **essentially similar**, e.g., drop path and drop edges, this won’t affect the performance of our method as long as each expert outputs the corresponding invariant representation. \n- Lastly, indeed, there could be more **complex combinations of the transform functions**, which poses challenges to the gating model’s expressiveness in identifying the combinations. However, this challenge may be minor in the practice. Specifically, we observe fairly high accuracy performances of the gating model, which are above 85% and 73% averagely on extrapolated datasets with one transformation and multiple transformations, respectively.\n\nWe added the above discussion to Appendix F to enable a more comprehensive view towards our methodology. We hope this can alleviate your concern on our gating model’s performance in identifying the distribution shift types.\n\n**(2) How does the predefined transform functions cover complex distributions causing the distribution shift?**\n\nThis is also a great question! We believe there are two angles for this question. \n\n**(a) For general domain**: In our experiments, we mainly use the five stochastic transform functions, which are universal graph augmentations as listed in Zhao et al., (2021) [3]. In our code implementation, we have also included additional transform functions as shown in Appendix B. We believe these transform functions, while not exhaustive, still cover a wide range of distribution shifts observing from our experimental results.\n\nNevertheless, we agree that the real graph distribution shifts can go beyond any possible combinations of the predefined transform functions. In that case, the assumption may not hold, meaning that GraphMETRO may not capture and precisely mitigate the unknown distribution shift. This scenario could always possibly exist due to the lack of information about the testing distribution or its domain knowledge. We include it as a limitation in Appendix 5, while we further discuss how we could alleviate the problem with additional information.\n\n**(b) For specific domains where additional knowledge is available**: In fact, knowing the tendency of the distribution shifts, such as increasing malicious users in a trading system, would be very helpful in constructing the transform functions that can cover the target distribution shifts well. We believe that such knowledge can come from two sources: \n- **Domain knowledge**, e.g., on molecular datasets, the transform function could be adding additional carbon structure to a molecule (while preserving its functional groups). Or, in a particular social network, transform functions can be defined from known user behaviors. \n- **Leveraging a few samples from target distribution**. Specifically, with the guide from a few target samples, we can select more relevant transform functions by, e.g., measuring the distance of the extrapolated datasets under a certain transform function with the target samples in the embedding space.\n\n---\n\n[1] Sui et al. Unleashing the Power of Graph Data Augmentation on Covariate Distribution Shift. NeurIPS 2023.\n\n[2] Nianzu Yang et al. Learning substructure invariance for out-of-distribution molecular representations, NeurIPS 2022\n\n[3] Yongduo Sui et al. Causal Attention for Interpretable and Generalizable Graph Classification, KDD 2022.\n\n[4] Jiaqi Ma et al. Subgroup Generalization and Fairness of Graph Neural Networks, NeurIPS 2022.\n\n---\n\n### **Comment 2: Our assumption**\n\nThanks for this comment! In Section 3.1, we have discussed the assumptions in detail with specifications. We repeat part of it here for your convenience:\n\n```Assumption 1 essentially states that the distribution shifts (whatever they are) can be decomposed into several mixture components of stochastic graph transformations. For example, on a social network dataset, each mixture component can represent different patterns of user behavior or network dynamics shifts. Specifically, one mixture component might correspond to increased user activity, while another could signify a particular trend of interaction within a certain group of users. Such a mixture pattern is common and well-studied in the real-world network datasets (Newman, 2003; Leskovec et al., 2005; 2007; Peel et al., 2017).```\n\nWe hope the analysis provides detailed illustration to the assumption. We are happy to add more discussion if you think anything  is still unclear!\n\n---\n\n### **Comment 2 & 3: Implementation of our method**\n\n- **Model architectures**: \nPlease see Tables 2 and 3 in Appendix A for detailed model architecture information. Specifically, we use the same encoders and classifiers from GOOD benchmarkfor real-world datasets to ensure fair comparisons. We employ backbones based on the best ERM performance for synthetic datasets. Moreover, GraphMETRO is model-agnostic, which consistently improves performance across varied model architectures.\n\n- **Stochastic transformation**: Please see Appendix B where we include the introduction of the stochastic transform functions. \n\n- **Optimization process**: Please see Table 2 and 3 (Appendix A), where we included hyperparameters on each dataset. In the second paragraph after Eq. (3), we also described our training pipeline: `“...we set apart the other loss terms from backpropagating to it to avoid interference with the training of the gating model… We optimize the objective via stochastic gradient descent”`. \n\n- **For a more intricate optimization strategy:** This is a great catch! In fact, we did try to pretrain the gating model for several epochs as warm up before training the whole model in an end-to-end fashion via the objective Eq. in (3). However, we didn’t notice a statistically significant difference in their performance, which can be due to that expert models take more time to convergence (since they need to align with the base model during the training) compared to the gating model.\n\nDue to the space limitation, we had to include most of the implementation details in the appendix while we added pointers in the main paper. However, we will try to make it more detailed and feel free to let us know if anything is missing! \n\n---\n\n### **Comment 2 (Cont.): Understand the functionality, underlying mechanism, and performance gain**\n\nThanks for the comment. For clarity, we provide the following pointers to the paper:\n\n- In Section 4.3, we provide a study to reveal the underlying **mechanism** of GraphMETRO, i.e., each expert excels in generating invariant representations concerning a stochastic transform function, which provide a solid foundation in generating referential invariant representations w.r.t. the specific transformations and further guarantee the generalization.\n\n- In Appendix C, we study the **impact of the MoE architecture** on model performance, which shows that the model performance may decline if the expressiveness of the expert model decreases. \n\n- In Appendix D, we study the **impact of the stochastic transform function** on model performance, where we also provide a detailed discussion of the modeling mechanism.\n\nWe believe the above studies and discussion in our paper provides an in-depth view, highlighting the roles of our objective, architecture, and stochastic transform function.  Please let us know if any of these perspectives is still unclear, and we can further improve our experimental study.\n\n-----\n\n# Summary\n\nWe hope our answers can address all of the concerns. We are happy to follow up if you have any further questions.\n\nWe also prudently ask you to reevaluate our work. To highlight, our motivation is supported by the common fine-grained graph shift heterogeneity, and the fact that most of the previous works could not model such nuanced distribution shifts in an effective and flexible manner. Moreover, we added more related works and modified our statement to position our work better. In general, we believe GraphMETRO is a more general and flexible solution that can mitigate a wider range of distribution shifts, which is backed by the experimental results. Detailed justification on our assumption and implementation details are also available in our paper. \n\nThus, we believe our work makes important contributions and provides a clear presentation. We are happy to discuss more and revise our paper if any concern remains. Thank you for your efforts and we are looking forward to your reply!!\n\n---\n\nWe appreciate your comments! To address your concerns, below we prudently justify the motivation of our proposed method, clarify our assumptions, and provide details regarding our experiment implementation.\n\n---------------------\n\n### **Comment 1: Motivation of this work**\n\nThank you for this comment! We believe there might be a bit of misunderstanding due to our different definitions of *“graph shift heterogeneity”*. We firstly discuss the related works mentioned and then justify our statement:\n\n**(1) Related works**\n\nPlease see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nMoreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you! \n\nSpecifically, Sui et al. [1] proposed a graph data augmentation strategy that alleviates covariate shift by generating diverse and invariant causal features. However, the trainable augmenter they used may not distill diverse augmentations or construct unseen perturbations. Moreover, Sui et al. [1] test its method only on graph classification tasks, while GraphMETRO can be applied to both node and graph classification tasks. Besides, we have discussed graph augmentation and attention-based methods in our related works, and we added more recent works on graph OOD [2,3,4], and we hope our response clears your concern on the related work discussion.\n\n\n**(2) The definition of graph shift heterogeneity**\n\nIn this work, we refer to **“heterogeneous shifts”** as multiple and different levels of shifts which vary across different instances (nodes or graphs), as illustrated in the example in the abstract. While we agree that the existing invariant learning approches can accommodate multiple distribution shifts, it could be hard for them to tackle nuanced distribution shifts for individual instances (nodes or graphs) since the distribution shifts are inferred from variance across multiple data environments. If GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering **a mixture of transformations as a proxy** for the target distribution shifts rather than invariance to whole-graph environment shifts. This is the type of heterogeneity we are interested in our paper. \n\n**(3) Regarding our original motivation statement**\n\nWhile the statement pointed out by the reviewer serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category. And we have provided detailed discussion about three lines of research in the related work section.\n\nHowever, we agree that we could make this statement border to cover the previous invariant learning methods. To improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to \"**previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments…**”. We also modified our introduction correspondingly, we hope this will better position our work.\n\n**(4) The choice of our MoE design**\n\n- The choice of our approach comes as a consequence of our motivation to model the graph/instance shift heterogeneity. As mentioned, mitigating multiple and nuanced distribution shifts simply goes beyond certain distribution shift types or environment construction as seen in the previous methods. \n- Thus, GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts. This enables the prediction of multiple different distribution shifts and the flexibility to model fine-grained heterogeneity since the mixture can be varied across different instances. We then tackled the proxy to mitigate the target distribution shifts. Intuitively, this solution provides a **“middle ground”** to deem graph generalization as an equivalent mixture, which, we believe, is a more tractable solution.\n\nWe updated our paper to make the above point more clear. We genuinely hope our answer can justify the motivation and solve your concern.\n\n---\n\n### **Question 2: How does the computational cost of GraphMETRO compare to other methods?**\n\nPlease see the last paragraph of Section 3.4, where we analyze the computation complexity of GraphMETRO. We repeat part of it here for your convenience:\n\n```Consider the scenario where we use an individual encoder for each expert. The forward process of $f$ involves $O(K)$ forward times using the weighted sum aggregation (or $O(1)$ if using the maximum selection). Since we extend the dataset to $(K + 1)$ times larger than the original data, the computation complexity is $O(K^2 |D_s|)$, where |Ds| is the size of the source dataset.```\n\nThus, the computation cost is about $K^2$ or $K$ times (if using the maximum selection) than an ERM model, where $K=5$ in our experiments. Compared to DIR, as they extract $B$ spurious subgraphs from each batch to conduct the intervention, their computation cost is $B$ times compared to ERM, where $B$ could be 32. Thus, we believe the computation cost of GraphMETRO is fair for the gains we get, considering $K$ is usually small.\n\n----------------------\n\n### **Question 3: Can GraphMETRO handle multiple different types of distribution shifts that simultaneously exist in data?**\n\nYes! The distribution shift types corresponding to the gating outputs with high scores will be tackled during training. That is, if the gating output highlights multiple mixture components, their corresponding distribution shift types will be handled jointly.\n\n----------------------\n\n### **Question 4: Can GraphMETRO  tackle distribution shifts on molecular graphs?**\n\nThat is a great idea! Yes, GraphMETRO can be applied to molecular datasets if one designs transform functions to cover typical molecular variants. For instance, a transform function may add carbon structures to the molecules. These domain-specific transform functions are outside the scope of our work, however, we believe these would be interesting future work directions!\n\n----------------------\n\n# Summary \n\nWe thank the reviewer for the time and insightful suggestions! We hope our answers can address your concerns well.\n\nWe also prudently ask you to reconsider our work if the concerns are addressed. To highlight, our novelty comes from the formulation of an equivalent mixture for graph OOD and the training framework to effectively realize generalization. We also provide an in-depth analysis on our originality compared to some previous invariant learning methods. While we discussed and compared with previous works, we added more related works and modified our statement to position our work better. Finally, our method achieves great improvements on both node and graph classification tasks, and is a more general solution to mitigate multiple and nuanced distribution shifts. \n\nOverall, we believe our work proposes a new paradigm and novel training framework and makes good contributions in the fields of graph generalization, and we would appreciate your reconsideration on this point. Thank you for your efforts again!\n\n----------------------\n\n**Reference**\n\n\n[1-5] The same as listed by the reviewer\n\n\n[6] Beatrice Bevilacqua, Yangze Zhou, and Bruno Ribeiro. Size-invariant graph representations for graph classification extrapolations. In ICML, 2021.\n\n[7] Davide Buffelli, Pietro Li´o, and Fabio Vandin. Sizeshiftreg: a regularization method for improving size-generalization in graph neural networks. In NeurIPS, 2022.\n\n[8] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In NeurIPS, 2019.\n\n[9] Mucong Ding, Kezhi Kong, Jiuhai Chen, John Kirchenbauer, Micah Goldblum, David Wipf, Furong Huang, and Tom Goldstein. A closer look at distribution shifts and out-of-distribution generalization on graphs. In NeurIPS DistShift, 2021.\n\n---\n\n### **Comment 3 & Question 1: Related works on invariant learning and clarification on our statement.**\n\nThanks for pointing it out! Below we clarify the statement and clear potential misunderstanding:\n\n**(1) How does GraphMETRO compare with invariant learning methods like DIR and EERM?** \n\nPlease see our related work section where we discussed these two papers, i.e., DIR (Wu et al., 2022c) and EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on environmental patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nBesides, we also provide a more in-depth comparison in our point (3) below to highlight our key technical originality.\n\n**(2) Regarding our statement about previous works:** \n\nWhile the statement serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category, and we have provided detailed discussion about three lines of research in the related work section.\n\nTo improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to **\"previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments** (which is highly limited when confronted with nuanced distribution shifts)”. We also modified our introduction correspondingly. Thanks for letting us know our statement could be misinterpreted.\n\n**(3) Why do we say our method could be more broad than the existing invariant learning approaches?** \n\n- Invariant subgraph learning approaches, e.g., [1,2], consider variance of constructed data environments, which are designed very differently compared to our work. \nWhile they can accommodate multiple distribution shifts (as in multiple environments), these focus on patterns within each environment and ignore the variety across instances (e.g., shifts at the resolution of nodes), which may not be well-captured by the environment assignments. \n- GraphMETRO considers that specific parts of the test graph may have different shifts. Particularly, our goal is to make the generalization to unknown testing distribution more adaptive and broad, as opposed to limiting the distribution shifts to being invariant to specific types of subgraphs. \n\nIn other words, if GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering a mixture of transformations as a proxy for the target distribution shifts rather than invariance to environment shifts.\n\n**(4) Key technical originality compared to invariance learning (going more deeply)**\n\nAnother interesting view to see the innovation of GraphMETRO is that it breaks the typical invariant learning formulation, which assumes the data is manipulated by the environment variables (and then can be “decoded” into multiple environments). Instead, GraphMETRO sees the distribution shifts on an instance as a mixture, which is represented by the score vector output by the gating function over the basis of the transform functions. In other words, GraphMETRO can produce infinite environments as the elements in the score vector are continuous. One can see that once we limit the output domain of the gating function into, e.g., binary {0, 1}, GraphMETRO can also produce a limited number of environments (if we categorize the instances based on the score vector), which covers the environment construction in invariant learning. Moreover, as mentioned, we propose the concept of referential invariant representation with a base model $\\xi_0$, which is also different from previous works on invariant learning. We added the above discussion to Appendix F to improve the depth of our analysis.\n\n---\n\nWe appreciate your efforts and insightful comments! To address your concerns, we provide point-to-point responses below.\n\n---------------------\n\n### **Comment 1: Regarding the novelty of GraphMETRO.**\n\nThanks for the comment! We believe our novelty comes from the proposal of an equivalent mixture for graph OOD and the construction of our training framework, as detailed below:\n\n- **An equivalent mixture for graph OOD**: The key challenge we faced to mitigate multiple and nuanced distribution shifts is the intrinsic complexity and heterogeneity of graph distribution shifts, which simply goes beyond certain distribution shift types [6,7,8,9] or environment construction as seen in the previous methods [1,2,3]. GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts, where the mixture can be varied across different instances, and then tackled the proxy to mitigate the target distribution shifts. We believe the high-level idea is succinct, nevertheless, novel, in the sense that it provides a “middle ground” to deem graph generalization as an equivalent mixture that is more tractable.\n\n- **Training framework**: With the guide of our formulation, the training framework is still non-trivial due to two problems, i.e., “how to provide supervision for predicting the mixture” and “how to ensure the experts corresponding to mixture components are compatible when working as a whole”. Specifically, GraphMETRO solves the first problem by conducting graph extrapolation. This is somewhat similar to the spirit of graph pretraining in the sense that we inject heterogeneity to promote the expressiveness of the gating model in recognizing the mixture components. For the second problem, we introduce the concept of Referential Invariant Representation, along with the novel objective in Eq. (3) to enforce the invariance and compatibility. It is worth mentioning that the model performance is much worse than the reported numbers (e.g., 2.7\\% lower on Twitch dataset) without the compatibility constraint, indicating the proposed referential invariance concept is indispensable. \n\nWe added more justification in the introduction (updated in the revision). We genuinely hope our responses can solve your concerns about the novelty of our work.\n\n------------------------------\n\n### **Comment 2: Comparison with recent methods [3,4,5].**\n\nIn compacting the paper to fit in the page limit we mistakenly did not include these relevant references, we apologize. We added discussion in the revised version. Here we summarize these works and point out their key differences with our method: \n\n- In particular, **Yang et al. [3]** explore molecule representation learning in out-of-distribution (OOD) scenarios. They achieve this by directing the molecule encoder to utilize stable and environment-invariant substructures relevant to the labels without the need for environmental labels. \n- Similarly, **Sui et al. [4]** introduces causal attention modules to identify key invariant subgraph features that can be described as causing the graph label. The type of OOD task that Sui et al. [4] considers assumes the graph label is caused by a subgraph, which is quite different from ours. Moreover, both Yang et al. [3] and Sui et al. [4] consider tasks where the graph label is caused by a subgraph. \n- **Ma et al.[5]** is an interesting theoretical work which studies GNN generalization and examines their fairness, showing that the test subgroup's distance from the training set impacts GNN performance. Ma et al.[5], as far as we could assess, does not propose any specific architecture to solve the type of OOD tasks we consider in our work.\n\nOverall, the goal of GraphMETRO is to be invariant to a mixture of selected stochastic transform functions (and the mixture can vary across different instances), which is a more flexible and general solution. \nWe added a discussion of these works to our revision, hopefully providing a more comprehensive comparison and literature overview. We hope our responses can solve your concerns about the related work.\n\n---\n\nWe sincerely appreciate all reviewers' time, efforts, and valuable suggestions in reviewing our paper. We are glad that most of the reviewers reached a positive consensus on our work's motivation and experimental results. Here is a summary of our responses:\n\n- **Clarification**: We clarify our key assumptions (`m8JS, FURX`), novelty (`m8JS, NLg1`), and the experimental settings (`m8JS`).\n\n- **Related work**: : We add more related works on recent graph OOD learning (`NLg1, m8JS`) and graph transfer learning (`FURX`)\n\n- **Limitation**: We add discussion about the limitations of GraphMETRO when confronted with unknown distribution shifts that surpass our key assumptions (`FURX`, `C9mM`).\n\nWe hope our responses can clarify your confusion and alleviate concerns and we updated our paper **(highlighted in green)** corespondingly. We thank all reviewers again, and look forward to your reply!", "author_response": "We appreciate your comments! To address your concerns, below we prudently justify the motivation of our proposed method, clarify our assumptions, and provide details regarding our experiment implementation.\n\n---------------------\n\n### **Comment 1: Motivation of this work**\n\nThank you for this comment! We believe there might be a bit of misunderstanding due to our different definitions of *“graph shift heterogeneity”*. We firstly discuss the related works mentioned and then justify our statement:\n\n**(1) Related works**\n\nPlease see Section 2 where we discussed the paper mentioned by the reviewer, i.e., EERM (Wu et al., 2022a). We repeat part of it here for you convenience:\n\n```The prevailing invariant learning approaches assume that there exist an underlying graph structure (i.e., subgraph) (Wu et al., 2022c; Li et al., 2022b;a) or representation (Arjovsky et al., 2019; Wu et al., 2022a; Chen et al., 2022; Bevilacqua et al., 2021; Zhang et al., 2022) that is invariant to different environments and / or causally related to the label of a given instance. However, these approaches focus on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making their applicability limited.```\n\nMoreover, we apologize for missing the recent interesting work by **Sui et al. [1]** which officially came out two days before the ICLR abstract deadline. We added it to our revision, thank you! \n\nSpecifically, Sui et al. [1] proposed a graph data augmentation strategy that alleviates covariate shift by generating diverse and invariant causal features. However, the trainable augmenter they used may not distill diverse augmentations or construct unseen perturbations. Moreover, Sui et al. [1] test its method only on graph classification tasks, while GraphMETRO can be applied to both node and graph classification tasks. Besides, we have discussed graph augmentation and attention-based methods in our related works, and we added more recent works on graph OOD [2,3,4], and we hope our response clears your concern on the related work discussion.\n\n\n**(2) The definition of graph shift heterogeneity**\n\nIn this work, we refer to **“heterogeneous shifts”** as multiple and different levels of shifts which vary across different instances (nodes or graphs), as illustrated in the example in the abstract. While we agree that the existing invariant learning approches can accommodate multiple distribution shifts, it could be hard for them to tackle nuanced distribution shifts for individual instances (nodes or graphs) since the distribution shifts are inferred from variance across multiple data environments. If GraphMETRO's approach were described via environments, we would have a combinatorial number of such environments in training (the product of all different subsets of nodes and all their possible distinct shifts). Thankfully, GraphMETRO avoids this combinatorial explosion by considering **a mixture of transformations as a proxy** for the target distribution shifts rather than invariance to whole-graph environment shifts. This is the type of heterogeneity we are interested in our paper. \n\n**(3) Regarding our original motivation statement**\n\nWhile the statement pointed out by the reviewer serves as our primary motivation, we would like to note that we did not claim all of the previous works fall into this category. And we have provided detailed discussion about three lines of research in the related work section.\n\nHowever, we agree that we could make this statement border to cover the previous invariant learning methods. To improve the clarity, we change the statement from \"previous works mostly focus on addressing specific types of distribution shifts\" to \"**previous works mostly focus on addressing specific types of distribution shifts or inferring distribution shifts from data environments…**”. We also modified our introduction correspondingly, we hope this will better position our work.\n\n**(4) The choice of our MoE design**\n\n- The choice of our approach comes as a consequence of our motivation to model the graph/instance shift heterogeneity. As mentioned, mitigating multiple and nuanced distribution shifts simply goes beyond certain distribution shift types or environment construction as seen in the previous methods. \n- Thus, GraphMETRO takes a different path, i.e., predicting a mixture of transformations as the proxy of the target distribution shifts. This enables the prediction of multiple different distribution shifts and the flexibility to model fine-grained heterogeneity since the mixture can be varied across different instances. We then tackled the proxy to mitigate the target distribution shifts. Intuitively, this solution provides a **“middle ground”** to deem graph generalization as an equivalent mixture, which, we believe, is a more tractable solution.\n\nWe updated our paper to make the above point more clear. We genuinely hope our answer can justify the motivation and solve your concern."}
{"claim": "The authors primarily apply their physical compatibility optimization to enhance 3D models produced by off-the-shelf methods, instead of reconstructing 3D objects directly from a single input image.", "claim_type": "methodology", "paper_id": "k29Iv0XrBF", "paper_title": "Physically Compatible 3D Object Modeling from a Single Image", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "RasXwPlfVF", "reviewer": "Reviewer_H9KU", "review_text": "Summary: This paper introduces a physical compatibility optimization framework for reconstructed objects from a single image. The approach considers mechanical properties, external forces, and rest-shape geometry, integrating static equilibrium as a hard constraint. This framework improves upon existing methods by ensuring the stability and accuracy of reconstructed objects under external influences. Quantitative and qualitative evaluations show enhancements in physical compatibility.\n\nStrengths: 1. Performance: The proposed method achieves state-of-the-art results. The experiments well validate the effectiveness of the proposed methods.\n\n2. Clarity: The paper is well-written and easy to follow.\n\n3. Technical Novelty: The main contributions of this paper are twofold: 1) They propose a physical compatibility optimization framework for 3D objects and decompose the mechanical properties, external forces, and rest-shape geometry. 2) They optimize the rest-shape geometry using predefined mechanical properties and external forces and ensure the object’s shape aligns with the target image when in static equilibrium.\n\nWeaknesses: In this paper, the authors primarily apply the physical compatibility optimization framework to enhance the physical attributes of 3D models obtained from existing methods rather than reconstructing them from a single image. Therefore, I think the title \"Physically Compatible 3D Object Modeling from a Single Image\" may not be suitable, as the focus lies on enhancing physically compatible modeling of 3D objects derived from off-the-shelf methods.\n\nQuestions: 1. All $\\mathbf{x}\\_{static}$ may be $\\mathbf{X}_{static}$ as the defination in line 89.\n2. In Some cases discussed in the paper, like the flamingo standing on one leg, should it be standable even after optimization? I think it should not be standable under gravity.\n3. Why do you evaluate the different off-the-shelf methods using connected components when this cannot demonstrate the superiority of your proposed method?", "labeling_timestamp": "2026-01-11T16:32:03.565938", "model": "gpt-5-mini", "label": "Supported", "justification": "Authors agree to change the paper title to emphasize '3D Object Modeling,' which aligns with the reviewer's characterization that their work focuses on modeling/enhancing 3D objects rather than single-image reconstruction.", "evidence": "\"Based on your suggestions, we will change the title to ‘Physically Compatible 3D Object Modeling.’\"", "combined_author_response": "Thank you for your feedback. Based on your suggestions, we will change the title to ‘Physically Compatible 3D Object Modeling.’ We are also open to any other title suggestions from the reviewer and will adjust the paper title based on the meta-review if necessary.\n\nPlease feel free to let us know if there's anything else we can provide to encourage you to increase the score.\n\nBest,\n\nAuthors", "author_response": "Thank you for your feedback. Based on your suggestions, we will change the title to ‘Physically Compatible 3D Object Modeling.’ We are also open to any other title suggestions from the reviewer and will adjust the paper title based on the meta-review if necessary.\n\nPlease feel free to let us know if there's anything else we can provide to encourage you to increase the score.\n\nBest,\n\nAuthors"}
{"claim": "The paper title 'Physically Compatible 3D Object Modeling from a Single Image' is misleading because the work focuses on enhancing models from existing methods rather than full single-image reconstruction.", "claim_type": "methodology", "paper_id": "k29Iv0XrBF", "paper_title": "Physically Compatible 3D Object Modeling from a Single Image", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "RasXwPlfVF", "reviewer": "Reviewer_H9KU", "review_text": "Summary: This paper introduces a physical compatibility optimization framework for reconstructed objects from a single image. The approach considers mechanical properties, external forces, and rest-shape geometry, integrating static equilibrium as a hard constraint. This framework improves upon existing methods by ensuring the stability and accuracy of reconstructed objects under external influences. Quantitative and qualitative evaluations show enhancements in physical compatibility.\n\nStrengths: 1. Performance: The proposed method achieves state-of-the-art results. The experiments well validate the effectiveness of the proposed methods.\n\n2. Clarity: The paper is well-written and easy to follow.\n\n3. Technical Novelty: The main contributions of this paper are twofold: 1) They propose a physical compatibility optimization framework for 3D objects and decompose the mechanical properties, external forces, and rest-shape geometry. 2) They optimize the rest-shape geometry using predefined mechanical properties and external forces and ensure the object’s shape aligns with the target image when in static equilibrium.\n\nWeaknesses: In this paper, the authors primarily apply the physical compatibility optimization framework to enhance the physical attributes of 3D models obtained from existing methods rather than reconstructing them from a single image. Therefore, I think the title \"Physically Compatible 3D Object Modeling from a Single Image\" may not be suitable, as the focus lies on enhancing physically compatible modeling of 3D objects derived from off-the-shelf methods.\n\nQuestions: 1. All $\\mathbf{x}\\_{static}$ may be $\\mathbf{X}_{static}$ as the defination in line 89.\n2. In Some cases discussed in the paper, like the flamingo standing on one leg, should it be standable even after optimization? I think it should not be standable under gravity.\n3. Why do you evaluate the different off-the-shelf methods using connected components when this cannot demonstrate the superiority of your proposed method?", "labeling_timestamp": "2026-01-11T16:32:31.107877", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors agree that the title should be changed and commit to updating it, acknowledging the reviewer's concern that the original title was misleading.", "evidence": "Based on your suggestions, we will change the title to ‘Physically Compatible 3D Object Modeling.’", "combined_author_response": "Thank you for your feedback. Based on your suggestions, we will change the title to ‘Physically Compatible 3D Object Modeling.’ We are also open to any other title suggestions from the reviewer and will adjust the paper title based on the meta-review if necessary.\n\nPlease feel free to let us know if there's anything else we can provide to encourage you to increase the score.\n\nBest,\n\nAuthors", "author_response": "Thank you for your feedback. Based on your suggestions, we will change the title to ‘Physically Compatible 3D Object Modeling.’ We are also open to any other title suggestions from the reviewer and will adjust the paper title based on the meta-review if necessary.\n\nPlease feel free to let us know if there's anything else we can provide to encourage you to increase the score.\n\nBest,\n\nAuthors"}
{"claim": "The manuscript uses inconsistent notation: 'x_static' appears where 'X_static' is defined on line 89, indicating a presentation or typographical error.", "claim_type": "presentation", "paper_id": "k29Iv0XrBF", "paper_title": "Physically Compatible 3D Object Modeling from a Single Image", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "RasXwPlfVF", "reviewer": "Reviewer_H9KU", "review_text": "Summary: This paper introduces a physical compatibility optimization framework for reconstructed objects from a single image. The approach considers mechanical properties, external forces, and rest-shape geometry, integrating static equilibrium as a hard constraint. This framework improves upon existing methods by ensuring the stability and accuracy of reconstructed objects under external influences. Quantitative and qualitative evaluations show enhancements in physical compatibility.\n\nStrengths: 1. Performance: The proposed method achieves state-of-the-art results. The experiments well validate the effectiveness of the proposed methods.\n\n2. Clarity: The paper is well-written and easy to follow.\n\n3. Technical Novelty: The main contributions of this paper are twofold: 1) They propose a physical compatibility optimization framework for 3D objects and decompose the mechanical properties, external forces, and rest-shape geometry. 2) They optimize the rest-shape geometry using predefined mechanical properties and external forces and ensure the object’s shape aligns with the target image when in static equilibrium.\n\nWeaknesses: In this paper, the authors primarily apply the physical compatibility optimization framework to enhance the physical attributes of 3D models obtained from existing methods rather than reconstructing them from a single image. Therefore, I think the title \"Physically Compatible 3D Object Modeling from a Single Image\" may not be suitable, as the focus lies on enhancing physically compatible modeling of 3D objects derived from off-the-shelf methods.\n\nQuestions: 1. All $\\mathbf{x}\\_{static}$ may be $\\mathbf{X}_{static}$ as the defination in line 89.\n2. In Some cases discussed in the paper, like the flamingo standing on one leg, should it be standable even after optimization? I think it should not be standable under gravity.\n3. Why do you evaluate the different off-the-shelf methods using connected components when this cannot demonstrate the superiority of your proposed method?", "labeling_timestamp": "2026-01-11T16:32:25.590946", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The authors' response discusses changing the paper title and offers further assistance, but does not acknowledge or address the reviewer’s point about inconsistent notation.", "evidence": "Based on your suggestions, we will change the title to ‘Physically Compatible 3D Object Modeling.’", "combined_author_response": "Thank you for your feedback. Based on your suggestions, we will change the title to ‘Physically Compatible 3D Object Modeling.’ We are also open to any other title suggestions from the reviewer and will adjust the paper title based on the meta-review if necessary.\n\nPlease feel free to let us know if there's anything else we can provide to encourage you to increase the score.\n\nBest,\n\nAuthors", "author_response": "Thank you for your feedback. Based on your suggestions, we will change the title to ‘Physically Compatible 3D Object Modeling.’ We are also open to any other title suggestions from the reviewer and will adjust the paper title based on the meta-review if necessary.\n\nPlease feel free to let us know if there's anything else we can provide to encourage you to increase the score.\n\nBest,\n\nAuthors"}
{"claim": "The method may incorrectly deem inherently unstable poses (for example, a flamingo standing on one leg) as standable under gravity after optimization, which contradicts physical realism.", "claim_type": "methodology", "paper_id": "k29Iv0XrBF", "paper_title": "Physically Compatible 3D Object Modeling from a Single Image", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "RasXwPlfVF", "reviewer": "Reviewer_H9KU", "review_text": "Summary: This paper introduces a physical compatibility optimization framework for reconstructed objects from a single image. The approach considers mechanical properties, external forces, and rest-shape geometry, integrating static equilibrium as a hard constraint. This framework improves upon existing methods by ensuring the stability and accuracy of reconstructed objects under external influences. Quantitative and qualitative evaluations show enhancements in physical compatibility.\n\nStrengths: 1. Performance: The proposed method achieves state-of-the-art results. The experiments well validate the effectiveness of the proposed methods.\n\n2. Clarity: The paper is well-written and easy to follow.\n\n3. Technical Novelty: The main contributions of this paper are twofold: 1) They propose a physical compatibility optimization framework for 3D objects and decompose the mechanical properties, external forces, and rest-shape geometry. 2) They optimize the rest-shape geometry using predefined mechanical properties and external forces and ensure the object’s shape aligns with the target image when in static equilibrium.\n\nWeaknesses: In this paper, the authors primarily apply the physical compatibility optimization framework to enhance the physical attributes of 3D models obtained from existing methods rather than reconstructing them from a single image. Therefore, I think the title \"Physically Compatible 3D Object Modeling from a Single Image\" may not be suitable, as the focus lies on enhancing physically compatible modeling of 3D objects derived from off-the-shelf methods.\n\nQuestions: 1. All $\\mathbf{x}\\_{static}$ may be $\\mathbf{X}_{static}$ as the defination in line 89.\n2. In Some cases discussed in the paper, like the flamingo standing on one leg, should it be standable even after optimization? I think it should not be standable under gravity.\n3. Why do you evaluate the different off-the-shelf methods using connected components when this cannot demonstrate the superiority of your proposed method?", "labeling_timestamp": "2026-01-11T16:32:36.395695", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The authors only discuss changing the paper title and offering to provide additional materials to improve the review score; they do not address the reviewer's concern about incorrectly deeming unstable poses as standable or discuss physical realism or pose stability.", "evidence": "\"Thank you for your feedback. Based on your suggestions, we will change the title to ‘Physically Compatible 3D Object Modeling.’ We are also open to any other title suggestions from the reviewer and will adjust the paper title based on the meta-review if necessary.\"", "combined_author_response": "Thank you for your feedback. Based on your suggestions, we will change the title to ‘Physically Compatible 3D Object Modeling.’ We are also open to any other title suggestions from the reviewer and will adjust the paper title based on the meta-review if necessary.\n\nPlease feel free to let us know if there's anything else we can provide to encourage you to increase the score.\n\nBest,\n\nAuthors", "author_response": "Thank you for your feedback. Based on your suggestions, we will change the title to ‘Physically Compatible 3D Object Modeling.’ We are also open to any other title suggestions from the reviewer and will adjust the paper title based on the meta-review if necessary.\n\nPlease feel free to let us know if there's anything else we can provide to encourage you to increase the score.\n\nBest,\n\nAuthors"}
{"claim": "Evaluating different off-the-shelf methods using connected components is an inappropriate metric and cannot demonstrate the proposed method's superiority in physical compatibility.", "claim_type": "methodology", "paper_id": "k29Iv0XrBF", "paper_title": "Physically Compatible 3D Object Modeling from a Single Image", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "RasXwPlfVF", "reviewer": "Reviewer_H9KU", "review_text": "Summary: This paper introduces a physical compatibility optimization framework for reconstructed objects from a single image. The approach considers mechanical properties, external forces, and rest-shape geometry, integrating static equilibrium as a hard constraint. This framework improves upon existing methods by ensuring the stability and accuracy of reconstructed objects under external influences. Quantitative and qualitative evaluations show enhancements in physical compatibility.\n\nStrengths: 1. Performance: The proposed method achieves state-of-the-art results. The experiments well validate the effectiveness of the proposed methods.\n\n2. Clarity: The paper is well-written and easy to follow.\n\n3. Technical Novelty: The main contributions of this paper are twofold: 1) They propose a physical compatibility optimization framework for 3D objects and decompose the mechanical properties, external forces, and rest-shape geometry. 2) They optimize the rest-shape geometry using predefined mechanical properties and external forces and ensure the object’s shape aligns with the target image when in static equilibrium.\n\nWeaknesses: In this paper, the authors primarily apply the physical compatibility optimization framework to enhance the physical attributes of 3D models obtained from existing methods rather than reconstructing them from a single image. Therefore, I think the title \"Physically Compatible 3D Object Modeling from a Single Image\" may not be suitable, as the focus lies on enhancing physically compatible modeling of 3D objects derived from off-the-shelf methods.\n\nQuestions: 1. All $\\mathbf{x}\\_{static}$ may be $\\mathbf{X}_{static}$ as the defination in line 89.\n2. In Some cases discussed in the paper, like the flamingo standing on one leg, should it be standable even after optimization? I think it should not be standable under gravity.\n3. Why do you evaluate the different off-the-shelf methods using connected components when this cannot demonstrate the superiority of your proposed method?", "labeling_timestamp": "2026-01-11T16:32:24.927453", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The authors' response only discusses changing the paper title and solicits further suggestions; it does not address the reviewer's critique about the evaluation metric or the use of connected components for demonstrating physical compatibility.", "evidence": "\"Based on your suggestions, we will change the title to ‘Physically Compatible 3D Object Modeling.’\"", "combined_author_response": "Thank you for your feedback. Based on your suggestions, we will change the title to ‘Physically Compatible 3D Object Modeling.’ We are also open to any other title suggestions from the reviewer and will adjust the paper title based on the meta-review if necessary.\n\nPlease feel free to let us know if there's anything else we can provide to encourage you to increase the score.\n\nBest,\n\nAuthors", "author_response": "Thank you for your feedback. Based on your suggestions, we will change the title to ‘Physically Compatible 3D Object Modeling.’ We are also open to any other title suggestions from the reviewer and will adjust the paper title based on the meta-review if necessary.\n\nPlease feel free to let us know if there's anything else we can provide to encourage you to increase the score.\n\nBest,\n\nAuthors"}
{"claim": "The paper's claim regarding the knowledge base is insufficiently clarified and lacks explanation of how the knowledge base is constructed or used.", "claim_type": "methodology", "paper_id": "aaYBsuGRne", "paper_title": "Understanding In-context Learning with a Pelican Soup Hypothesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "EIVr3uVfUn", "reviewer": "Reviewer_rNJR", "review_text": "Summary: This paper focuses on understanding the in-context learning ability of large language models. The authors propose the Pelican soup hypothesis. It explains the in-context learning ability originating from learning the knowledge via the next token prediction. To support this hypothesis, the authors build a dataset and demonstrate the linkage between linguistic phenomena and in-context learning.\n\nStrengths: This paper provides substantial numerical results to support the proposed hypothesis. The linguistic phenomena analysis is also interesting to the community. In addition, the built dataset may be of independent interest.\n\nWeaknesses: 1. The claim related to the knowledge base needs more clarification. The experiments in [1] demonstrate that input-output mapping is not very important to the ICL. If the label space is correct, LLMs can even implement efficient ICL given wrong mapping. However, this wrong mapping conflicts with the knowledge base. More discussions are needed here.\n\n2. In Section 5.1, some assumptions are presented, but there is a notable absence of justification for these assumptions within the paper. This absence makes it challenging to ascertain the realism of these assumptions.\n3. I would greatly appreciate further elucidation on the distinction between the hypothesis presented in this paper and that discussed in [2]. Specifically, the variance between the \"atomic elements of NLP tasks\" and \"a set of atom concepts\" requires additional clarification.\n\n4. It is advantageous to include more highly relevant works in the related works. For example, besides HMM, implicit Bayesian inference is modeled for ICL in many different data assumptions [3,4,5]. [6] also studies the optimization side of ICL.\n\n[1] Min S, Lyu X, Holtzman A, et al. Rethinking the role of demonstrations: What makes in-context learning work?[J]. arXiv preprint arXiv:2202.12837, 2022.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Jiang H. A latent space theory for emergent abilities in large language models[J]. arXiv preprint arXiv:2304.09960, 2023.\n\n[4] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[5] Wang X, Zhu W, Wang W Y. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning[J]. arXiv preprint arXiv:2301.11916, 2023.\n\n[6] Dai D, Sun Y, Dong L, et al. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers[J]. arXiv preprint arXiv:2212.10559, 2022.\n\nQuestions: Questions are specified in Weakness part.", "labeling_timestamp": "2026-01-11T16:32:44.858035", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "Authors acknowledge the reviewer’s concern, say they revised the paper to clarify Section 3 and link it to the formalism in Section 5.1, and point to Calcutec as an instantiation; however, they do not in this response give a direct, detailed explanation of how the knowledge base itself is constructed or used and request further guidance, so the claim is only partially addressed.", "evidence": "We have revised the paper to address the main concern that Section 3 is not substantiated... We added more elaboration on the linkage between Section 3 and Section 5.1, stressing that Section 3 is to draw intuitions for the assumptions in Section 5.1 and that Section 5.1 is the formal description for the characteristics described in Section 3. (… ) The construction of Calcutec is more complicated than the toy data used in previous theoretical works such as [3] because Calcutec relies on milder assumptions and is more realistic. (… ) Could you be more specific on what details are missing and how we should improve? We will appreciate your suggestion.", "combined_author_response": "We appreciate all reviewers' insightful comments.\n\nWe are particularly grateful that the reviewers agreed that our paper has the following strengths:\n\n- The linguistic phenomena analysis is interesting to the community (rNJR, MUGa, 5jds).\n- The Calcutec dataset and the digit-addition task may be useful for future research (rNJR, MUGa, 5jds, hbQa,cdtQ).\n- Provides evidence that even smaller LLMs can perform ICL with artificial/task-agnostic label symbols (MUGa, 5jds).\n- A new formalism for NLP classification tasks (5jds, hbQa, cdtQ).\n- Empirical results supporting the Pelican Soup Hypothesis (rNJR, cdtQ) and provide a potential explanation for in-context learning (hbQa).\n\nWe have revised the paper to address the main concern that Section 3 is not substantiated, which we acknowledge is the confusion caused by the inclarity. We added more elaboration on the linkage between Section 3 and Section 5.1, stressing that Section 3 is to draw intuitions for the assumptions in Section 5.1 and that Section 5.1 is the formal description for the characteristics described in Section 3.\n\nWe also address each reviewer’s concerns below respectively. Please let us know if any aspects are still unclear. We are more than happy to provide additional clarification.\n\n---\n\nWe thank you for the positive feedback. With regard to the weakness:\n\n1. While Section 3 is purely conceptual, we provide rigorous characterization in Section 5.1. We consider this as one of our main contributions, an instantiation for the assumptions made in previous theoretic works, such as [1] and [2]. For [1], our framework constituted with the assumptions we made in Section 5.1 satisfies the requirements of Corollary 4.2, therefore we can use their analysis to get an $O(1/T)$ regret bound ($T$ is the number of examples in the demonstration). For [2], please refer to our reply to reviewer rNJR.\n2. Thanks for the suggestion! We can run more experiments. Could you let us know what experiments you think are important are missing?\n3. We admit that we only focus on classification tasks in this work. However, we would like to stress that even for the classification setting, the mechanism of ICL is not yet clear. Future work may also extend our framework to generation tasks.\n\nWith regard to the questions:\n\n1. By preprocessing the training data for LLMs, we think it’s possible. However, training an LLM requires lots of computational resources. We leave it for future work.\n2. The formalism in Section 2 can handle arbitrarily difficult reasoning tasks as long as there is a logic system that can handle it and the induction searching process is computationally feasible, e.g. not NP-hard. We believe that most human-solvable tasks are in this class.\n3. We think this is an important open question. A possible solution would be augmenting the training data, e.g. [3].\n4. Our conjecture would be that for many commonsense rules, LLMs are trained with many instances for each of them. Because the training data may cover a large portion of common surface forms, even though it’s possible that LLMs are doing no more than pattern matching, they are still able to do “reasoning” following the commonsense rules.  \n\n[1] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Zhou, Wangchunshu, Ronan Le Bras, and Yejin Choi. \"Commonsense Knowledge Transfer for Pre-trained Language Models.\" arXiv preprint arXiv:2306.02388 (2023).\n\n---\n\nThank you for your positive review. \n\nWe would like to point out that producing an environment, with which in-context learning still works, is a practice used in most (if not all) theoretical analyses such as [1] and [2] and even some empirical analyses such as [3]. We consider one of our main contributions to be proposing an environment that is closer to the real-world scenario than previous works.\n\nWith regard to the questions you stated:\n\n- The yes/no questions in a Pelican Soup riddle have a similar function as the demonstrations for in-context learning, because based on the answer of those yes/no questions, the participants are able to figure out the latent story and thus are able to answer other following questions. Note that we use Pelican Soup riddles just to motivate the importance of the role of commonsense.\n- Our argument is that solving these two problems (ICL and modeling coreference) requires similar capabilities. This argument is to draw the intuition for the framework/assumptions in Section 5.1. Surely the two problems are not identical, thus we characterize the difference between these two problems with the distribution shifts discussed in Section 4. To show that LMs may be able to generalize under these distribution shifts, we replicate these distribution shifts in our experiments, as discussed in Section 5.3 and results in Section 5.5 are aligned with our hypothesis.\n- We understand ICL as generating verbalizers based on the meaning of the verbalizers, which is recovered according to the preceding context, so we think it is similar to modeling anaphora. Could you please elaborate more on why you think ICL is more like handling catephora?\n\nPlease let us know if you have any other comments. We are happy to discuss more!\n\n[1] Xie, Sang Michael, et al. \"An Explanation of In-context Learning as Implicit Bayesian Inference.\" International Conference on Learning Representations. 2021.\n\n[2] Hahn, Michael, and Navin Goyal. \"A theory of emergent in-context learning as implicit structure induction.\" arXiv preprint arXiv:2303.07971 (2023).\n\n[3] Chan, Stephanie, et al. \"Data distributional properties drive emergent in-context learning in transformers.\" Advances in Neural Information Processing Systems 35 (2022): 18878-18891.\n\n---\n\nWe thank you for your review. We would like to address the weakness below:\n\n1. Please let us know what are the arguments that you think are not substantiated. For the example you provided, we apologize for not including proper citations. We think that LMs are able to acquire some knowledge about commonsense (though imperfectly) has been widely accepted. Many studies can be found in previous works, e.g. [1][2].\n2. Validating theories with a synthetic dataset is a common practice. We admit that our setting cannot explain domain bias. It may require more assumptions to theoretically explain. We would like to mention that, we think it will make the construction of the toy data even more complicated, which based on your 4th point, is not acceptable. \n3. We justify our assumptions made in Section 5.1 in Section 3. As argued in Section 3.1, we posit that training data usually contain text that has some logical structure and/or follows chronicle order, which can contribute to LMs’ reasoning capability. The dataset Calcutec is a formal abstraction of such structure.\n4. Could you be more specific on what details are missing and how we should improve? We will appreciate your suggestion. The construction of Calcutec is more complicated than the toy data used in previous theoretical works such as [3] because Calcutec relies on milder assumptions and is more realistic. \n5. We apologize that we didn’t make the purpose of Section 3 clearer. We treat Section 3 as the justification/intuition for the abstraction/assumptions/framework we made in Section 5.1. We then instantiate the abstraction/framework with Calcutec and show that LMs trained with it can do in-context learning.\n6. We use Pelican Soup riddles only to motivate the intuition that commonsense plays an important role for ICL. We would not say our hypothesis is *based on* this similarity.\n7. Please let us know if you think any experiment is important but missing. Also, could you elaborate more on the possibility that the models solve the task by learning shortcuts (and maybe also your definition of shortcuts)?\n\nOur answer to the question\n\n1. We think mesa-optimization mainly explains why models of Transformer architecture trained with autoregressive loss have the capacity to model data that involves ICL-like phenomena. However, in works such as [4], they only study on some toy data. It is unclear how their toy data is relevant to natural language data. Our work starts from another direction, aiming to understand what characteristics of training data lead to the ICL ability.\n\n\n[1] West, Peter, et al. \"Symbolic knowledge distillation: from general language models to commonsense models.\" arXiv preprint arXiv:2110.07178 (2021).\n\n[2] Li, Xiang Lorraine, et al. \"A systematic investigation of commonsense knowledge in large language models.\" Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.\n\n[3] Xie, Sang Michael, et al. \"An explanation of in-context learning as implicit bayesian inference.\" arXiv preprint arXiv:2111.02080 (2021).\n\n[4] von Oswald, Johannes, et al. \"Uncovering mesa-optimization algorithms in transformers.\" arXiv preprint arXiv:2309.05858 (2023).\n\n---\n\nThank you for your thoughtful comments. We would like to address the weakness below:\n\n1. In Section 2, we propose a formalism for NLP classification tasks while the main purpose of Section 3 is to justify the assumptions made in Section 5. We see the assumptions in Section 5 as the formal statement/abstraction of our theorem in Section 3. We are sorry that we didn’t describe it more clearly. We will improve this part in our later revision. Please let us know if we need to make more clarifications.\n2. As far as we know, all theoretical analyses on ICL depend on some repetitive nature of prompts or training data, e.g. Xie et al and Hahn & Goyal. One difference in our work is that we focus on the repetitive usage and the consistent meaning of pronouns, which is very realistic in real-world data. In Figure 2, we also show that the model can still do ICL when the task description is more complicated than the meaning of the pronouns seen in the training set, i.e., when the task description is composed of 3 atomic concepts (the green lines for “triple” in Figure 2).\n3. We discuss this point below:\n    - The digit addition task: Please refer to Figure 8 for the distribution of the number of steps in the datasets. For the 5-digit addition task, when the dropping rate is 0.2, there are only 265 (0.265%) training samples where all the intermediate steps are dropped. We think it is reasonable to say the testing samples are out-of-domain in this case. (And GPT-2-sized model can achieve ~80% accuracy with only 265 training samples in the testing domain.) Meanwhile, considering that the real-world data also has a small fraction of text that drops all the reasoning steps, we think this setting is not unrealistic.\n    - Calcutec: Please refer to Figure 5. It shows that even when no steps are dropped in the training set, the model still can do ICL, though the performance is not as good.\n\nQuestion:\n\n- The prompt is as the one in Table 1. We only include the reasoning steps of some (3) randomly selected examples and the premise of the testing input in the prompt. (For the example in Table 1, the testing input is `x55 x76 x84 x99`). We unroll until the first verbalizer is generated. \n\nPlease let us know if you have any other questions. We are more than happy to discuss with you.\n\n---\n\nThank you for your comments. \n\n## With Regard to the Weakness\n\n1. We consider one main contribution of our work to be explaining the ICL ability in the *semantically-unrelated label ICL (SUL-ICL)* setting [8], which is a step forward from the theorem by [7]. Indeed, our work can not explain the random label setting in [1]. However, we would like to point out that as far as we know, there has been no theorem able to explain it. We suggest that explaining it requires making more assumptions on the relationship between the input-label mapping and the domain/topic/theme distribution in the training data.\n2. We are sorry for the confusion. The justification mainly follows the argument in Section 3. We will make the connection clearer in our later revision. For now, please allow us to elaborate it below:\n    - Assumption 1: It follows the efforts made by early linguistics on linguistic formalism and cognitive psychology theories such as language of thought.\n    - Assumption 2: This assumption follows the argument in Section 3.1 that general text is usually organized in a similar way as logical induction processes. We assume that “a paragraph is a traversal of the nodes of the proving tree in the topological ordering” because in general text, statements usually follow some causal and/or chronological order, so it can convince readers or be easy for readers to understand.\n    - Assumption 3: In other words, this assumption assumes that every paragraph in an article mentions the topic of the article.\n    - Assumption 4: In other words, this assumption assumes that some pronouns are used in the article and each pronoun in the article is associated with a single entity.\n    - Assumption 5: This assumption follows the argument in Section 2.\n\n    In our opinion, these assumptions are relatively mild compared with previous theoretical works. We are happy to discuss more if you still have some concerns about the realism of these assumptions.\n3. To make our main response short, please allow us to discuss it below.\n4. Thank you for the list of relevant works. We will include some of them in our later revision (as we have some concerns about the technical soundness of some of them). \n\n\n## Atomic elements of NLP tasks in [2]\n\n\nIf we understand [2] correctly, [2] does not specify what characteristics the “atomic skills” of NLP need to have. We think our work could be an instantiation of the “atoms” proposed in [2]. \n\nIt seems that defining a set of “atomic skills” is not trivial and can’t be done arbitrarily. “Atomic skills” must be defined in a way such that the skills that work for the training data can generalize to the downstream task. To understand why it is non-trivial to define a set of atomic skills, we can probably think about a trivial case where we define the set “atomic skill” as the 26 skills to predict the words starting with A-Z. With this setting, the argument in [2] is greatly simplified. It seems that Theorem 14 only tells us that for most of the letters, the model can do the cloze problems in the training set well if the answer starts with that letter. It doesn’t seem to be guaranteed that the model can do the cloze problems in the testing set well.\n\nOn the other hand, defining the atomic skills with our atomic concepts seems to be more generalizable. Still, we posit it is necessary to discuss the discrepancies between the training set and the downstream task. That’s why in this work we explicitly characterize the distribution shifts in Section 4 and demonstrate that LMs are able to generalize under these distribution shifts in Section 5 and 7. Surely our settings do not fully replicate the real-world data distribution, but we think it still complements some aspects of [2]. \n\nPlease let us know if you still have any questions. We are more than happy to discuss further.\n\n\n[7] Xie, Sang Michael, et al. \"An explanation of in-context learning as implicit bayesian inference.\" arXiv preprint arXiv:2111.02080 (2021).\n\n[8] Wei, Jerry, et al. \"Larger language models do in-context learning differently.\" arXiv preprint arXiv:2303.03846 (2023).", "author_response": "Thank you for your comments. \n\n## With Regard to the Weakness\n\n1. We consider one main contribution of our work to be explaining the ICL ability in the *semantically-unrelated label ICL (SUL-ICL)* setting [8], which is a step forward from the theorem by [7]. Indeed, our work can not explain the random label setting in [1]. However, we would like to point out that as far as we know, there has been no theorem able to explain it. We suggest that explaining it requires making more assumptions on the relationship between the input-label mapping and the domain/topic/theme distribution in the training data.\n2. We are sorry for the confusion. The justification mainly follows the argument in Section 3. We will make the connection clearer in our later revision. For now, please allow us to elaborate it below:\n    - Assumption 1: It follows the efforts made by early linguistics on linguistic formalism and cognitive psychology theories such as language of thought.\n    - Assumption 2: This assumption follows the argument in Section 3.1 that general text is usually organized in a similar way as logical induction processes. We assume that “a paragraph is a traversal of the nodes of the proving tree in the topological ordering” because in general text, statements usually follow some causal and/or chronological order, so it can convince readers or be easy for readers to understand.\n    - Assumption 3: In other words, this assumption assumes that every paragraph in an article mentions the topic of the article.\n    - Assumption 4: In other words, this assumption assumes that some pronouns are used in the article and each pronoun in the article is associated with a single entity.\n    - Assumption 5: This assumption follows the argument in Section 2.\n\n    In our opinion, these assumptions are relatively mild compared with previous theoretical works. We are happy to discuss more if you still have some concerns about the realism of these assumptions.\n3. To make our main response short, please allow us to discuss it below.\n4. Thank you for the list of relevant works. We will include some of them in our later revision (as we have some concerns about the technical soundness of some of them). \n\n\n## Atomic elements of NLP tasks in [2]\n\n\nIf we understand [2] correctly, [2] does not specify what characteristics the “atomic skills” of NLP need to have. We think our work could be an instantiation of the “atoms” proposed in [2]. \n\nIt seems that defining a set of “atomic skills” is not trivial and can’t be done arbitrarily. “Atomic skills” must be defined in a way such that the skills that work for the training data can generalize to the downstream task. To understand why it is non-trivial to define a set of atomic skills, we can probably think about a trivial case where we define the set “atomic skill” as the 26 skills to predict the words starting with A-Z. With this setting, the argument in [2] is greatly simplified. It seems that Theorem 14 only tells us that for most of the letters, the model can do the cloze problems in the training set well if the answer starts with that letter. It doesn’t seem to be guaranteed that the model can do the cloze problems in the testing set well.\n\nOn the other hand, defining the atomic skills with our atomic concepts seems to be more generalizable. Still, we posit it is necessary to discuss the discrepancies between the training set and the downstream task. That’s why in this work we explicitly characterize the distribution shifts in Section 4 and demonstrate that LMs are able to generalize under these distribution shifts in Section 5 and 7. Surely our settings do not fully replicate the real-world data distribution, but we think it still complements some aspects of [2]. \n\nPlease let us know if you still have any questions. We are more than happy to discuss further.\n\n\n[7] Xie, Sang Michael, et al. \"An explanation of in-context learning as implicit bayesian inference.\" arXiv preprint arXiv:2111.02080 (2021).\n\n[8] Wei, Jerry, et al. \"Larger language models do in-context learning differently.\" arXiv preprint arXiv:2303.03846 (2023)."}
{"claim": "The paper fails to discuss how conflicting input-output mappings that still allow in-context learning interact with the proposed knowledge-base explanation.", "claim_type": "methodology", "paper_id": "aaYBsuGRne", "paper_title": "Understanding In-context Learning with a Pelican Soup Hypothesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "EIVr3uVfUn", "reviewer": "Reviewer_rNJR", "review_text": "Summary: This paper focuses on understanding the in-context learning ability of large language models. The authors propose the Pelican soup hypothesis. It explains the in-context learning ability originating from learning the knowledge via the next token prediction. To support this hypothesis, the authors build a dataset and demonstrate the linkage between linguistic phenomena and in-context learning.\n\nStrengths: This paper provides substantial numerical results to support the proposed hypothesis. The linguistic phenomena analysis is also interesting to the community. In addition, the built dataset may be of independent interest.\n\nWeaknesses: 1. The claim related to the knowledge base needs more clarification. The experiments in [1] demonstrate that input-output mapping is not very important to the ICL. If the label space is correct, LLMs can even implement efficient ICL given wrong mapping. However, this wrong mapping conflicts with the knowledge base. More discussions are needed here.\n\n2. In Section 5.1, some assumptions are presented, but there is a notable absence of justification for these assumptions within the paper. This absence makes it challenging to ascertain the realism of these assumptions.\n3. I would greatly appreciate further elucidation on the distinction between the hypothesis presented in this paper and that discussed in [2]. Specifically, the variance between the \"atomic elements of NLP tasks\" and \"a set of atom concepts\" requires additional clarification.\n\n4. It is advantageous to include more highly relevant works in the related works. For example, besides HMM, implicit Bayesian inference is modeled for ICL in many different data assumptions [3,4,5]. [6] also studies the optimization side of ICL.\n\n[1] Min S, Lyu X, Holtzman A, et al. Rethinking the role of demonstrations: What makes in-context learning work?[J]. arXiv preprint arXiv:2202.12837, 2022.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Jiang H. A latent space theory for emergent abilities in large language models[J]. arXiv preprint arXiv:2304.09960, 2023.\n\n[4] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[5] Wang X, Zhu W, Wang W Y. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning[J]. arXiv preprint arXiv:2301.11916, 2023.\n\n[6] Dai D, Sun Y, Dong L, et al. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers[J]. arXiv preprint arXiv:2212.10559, 2022.\n\nQuestions: Questions are specified in Weakness part.", "labeling_timestamp": "2026-01-11T16:33:02.599638", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors acknowledge they do not explain settings with arbitrary/random input-label mappings and state that explaining such cases would require additional assumptions relating input-label mapping to training-domain distributions, thus admitting the paper does not fully address the reviewer's point but noting it is outside their current scope.", "evidence": "\"Indeed, our work can not explain the random label setting in [1]. However, we would like to point out that as far as we know, there has been no theorem able to explain it. We suggest that explaining it requires making more assumptions on the relationship between the input-label mapping and the domain/topic/theme distribution in the training data.\"", "combined_author_response": "We appreciate all reviewers' insightful comments.\n\nWe are particularly grateful that the reviewers agreed that our paper has the following strengths:\n\n- The linguistic phenomena analysis is interesting to the community (rNJR, MUGa, 5jds).\n- The Calcutec dataset and the digit-addition task may be useful for future research (rNJR, MUGa, 5jds, hbQa,cdtQ).\n- Provides evidence that even smaller LLMs can perform ICL with artificial/task-agnostic label symbols (MUGa, 5jds).\n- A new formalism for NLP classification tasks (5jds, hbQa, cdtQ).\n- Empirical results supporting the Pelican Soup Hypothesis (rNJR, cdtQ) and provide a potential explanation for in-context learning (hbQa).\n\nWe have revised the paper to address the main concern that Section 3 is not substantiated, which we acknowledge is the confusion caused by the inclarity. We added more elaboration on the linkage between Section 3 and Section 5.1, stressing that Section 3 is to draw intuitions for the assumptions in Section 5.1 and that Section 5.1 is the formal description for the characteristics described in Section 3.\n\nWe also address each reviewer’s concerns below respectively. Please let us know if any aspects are still unclear. We are more than happy to provide additional clarification.\n\n---\n\nWe thank you for the positive feedback. With regard to the weakness:\n\n1. While Section 3 is purely conceptual, we provide rigorous characterization in Section 5.1. We consider this as one of our main contributions, an instantiation for the assumptions made in previous theoretic works, such as [1] and [2]. For [1], our framework constituted with the assumptions we made in Section 5.1 satisfies the requirements of Corollary 4.2, therefore we can use their analysis to get an $O(1/T)$ regret bound ($T$ is the number of examples in the demonstration). For [2], please refer to our reply to reviewer rNJR.\n2. Thanks for the suggestion! We can run more experiments. Could you let us know what experiments you think are important are missing?\n3. We admit that we only focus on classification tasks in this work. However, we would like to stress that even for the classification setting, the mechanism of ICL is not yet clear. Future work may also extend our framework to generation tasks.\n\nWith regard to the questions:\n\n1. By preprocessing the training data for LLMs, we think it’s possible. However, training an LLM requires lots of computational resources. We leave it for future work.\n2. The formalism in Section 2 can handle arbitrarily difficult reasoning tasks as long as there is a logic system that can handle it and the induction searching process is computationally feasible, e.g. not NP-hard. We believe that most human-solvable tasks are in this class.\n3. We think this is an important open question. A possible solution would be augmenting the training data, e.g. [3].\n4. Our conjecture would be that for many commonsense rules, LLMs are trained with many instances for each of them. Because the training data may cover a large portion of common surface forms, even though it’s possible that LLMs are doing no more than pattern matching, they are still able to do “reasoning” following the commonsense rules.  \n\n[1] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Zhou, Wangchunshu, Ronan Le Bras, and Yejin Choi. \"Commonsense Knowledge Transfer for Pre-trained Language Models.\" arXiv preprint arXiv:2306.02388 (2023).\n\n---\n\nThank you for your positive review. \n\nWe would like to point out that producing an environment, with which in-context learning still works, is a practice used in most (if not all) theoretical analyses such as [1] and [2] and even some empirical analyses such as [3]. We consider one of our main contributions to be proposing an environment that is closer to the real-world scenario than previous works.\n\nWith regard to the questions you stated:\n\n- The yes/no questions in a Pelican Soup riddle have a similar function as the demonstrations for in-context learning, because based on the answer of those yes/no questions, the participants are able to figure out the latent story and thus are able to answer other following questions. Note that we use Pelican Soup riddles just to motivate the importance of the role of commonsense.\n- Our argument is that solving these two problems (ICL and modeling coreference) requires similar capabilities. This argument is to draw the intuition for the framework/assumptions in Section 5.1. Surely the two problems are not identical, thus we characterize the difference between these two problems with the distribution shifts discussed in Section 4. To show that LMs may be able to generalize under these distribution shifts, we replicate these distribution shifts in our experiments, as discussed in Section 5.3 and results in Section 5.5 are aligned with our hypothesis.\n- We understand ICL as generating verbalizers based on the meaning of the verbalizers, which is recovered according to the preceding context, so we think it is similar to modeling anaphora. Could you please elaborate more on why you think ICL is more like handling catephora?\n\nPlease let us know if you have any other comments. We are happy to discuss more!\n\n[1] Xie, Sang Michael, et al. \"An Explanation of In-context Learning as Implicit Bayesian Inference.\" International Conference on Learning Representations. 2021.\n\n[2] Hahn, Michael, and Navin Goyal. \"A theory of emergent in-context learning as implicit structure induction.\" arXiv preprint arXiv:2303.07971 (2023).\n\n[3] Chan, Stephanie, et al. \"Data distributional properties drive emergent in-context learning in transformers.\" Advances in Neural Information Processing Systems 35 (2022): 18878-18891.\n\n---\n\nWe thank you for your review. We would like to address the weakness below:\n\n1. Please let us know what are the arguments that you think are not substantiated. For the example you provided, we apologize for not including proper citations. We think that LMs are able to acquire some knowledge about commonsense (though imperfectly) has been widely accepted. Many studies can be found in previous works, e.g. [1][2].\n2. Validating theories with a synthetic dataset is a common practice. We admit that our setting cannot explain domain bias. It may require more assumptions to theoretically explain. We would like to mention that, we think it will make the construction of the toy data even more complicated, which based on your 4th point, is not acceptable. \n3. We justify our assumptions made in Section 5.1 in Section 3. As argued in Section 3.1, we posit that training data usually contain text that has some logical structure and/or follows chronicle order, which can contribute to LMs’ reasoning capability. The dataset Calcutec is a formal abstraction of such structure.\n4. Could you be more specific on what details are missing and how we should improve? We will appreciate your suggestion. The construction of Calcutec is more complicated than the toy data used in previous theoretical works such as [3] because Calcutec relies on milder assumptions and is more realistic. \n5. We apologize that we didn’t make the purpose of Section 3 clearer. We treat Section 3 as the justification/intuition for the abstraction/assumptions/framework we made in Section 5.1. We then instantiate the abstraction/framework with Calcutec and show that LMs trained with it can do in-context learning.\n6. We use Pelican Soup riddles only to motivate the intuition that commonsense plays an important role for ICL. We would not say our hypothesis is *based on* this similarity.\n7. Please let us know if you think any experiment is important but missing. Also, could you elaborate more on the possibility that the models solve the task by learning shortcuts (and maybe also your definition of shortcuts)?\n\nOur answer to the question\n\n1. We think mesa-optimization mainly explains why models of Transformer architecture trained with autoregressive loss have the capacity to model data that involves ICL-like phenomena. However, in works such as [4], they only study on some toy data. It is unclear how their toy data is relevant to natural language data. Our work starts from another direction, aiming to understand what characteristics of training data lead to the ICL ability.\n\n\n[1] West, Peter, et al. \"Symbolic knowledge distillation: from general language models to commonsense models.\" arXiv preprint arXiv:2110.07178 (2021).\n\n[2] Li, Xiang Lorraine, et al. \"A systematic investigation of commonsense knowledge in large language models.\" Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.\n\n[3] Xie, Sang Michael, et al. \"An explanation of in-context learning as implicit bayesian inference.\" arXiv preprint arXiv:2111.02080 (2021).\n\n[4] von Oswald, Johannes, et al. \"Uncovering mesa-optimization algorithms in transformers.\" arXiv preprint arXiv:2309.05858 (2023).\n\n---\n\nThank you for your thoughtful comments. We would like to address the weakness below:\n\n1. In Section 2, we propose a formalism for NLP classification tasks while the main purpose of Section 3 is to justify the assumptions made in Section 5. We see the assumptions in Section 5 as the formal statement/abstraction of our theorem in Section 3. We are sorry that we didn’t describe it more clearly. We will improve this part in our later revision. Please let us know if we need to make more clarifications.\n2. As far as we know, all theoretical analyses on ICL depend on some repetitive nature of prompts or training data, e.g. Xie et al and Hahn & Goyal. One difference in our work is that we focus on the repetitive usage and the consistent meaning of pronouns, which is very realistic in real-world data. In Figure 2, we also show that the model can still do ICL when the task description is more complicated than the meaning of the pronouns seen in the training set, i.e., when the task description is composed of 3 atomic concepts (the green lines for “triple” in Figure 2).\n3. We discuss this point below:\n    - The digit addition task: Please refer to Figure 8 for the distribution of the number of steps in the datasets. For the 5-digit addition task, when the dropping rate is 0.2, there are only 265 (0.265%) training samples where all the intermediate steps are dropped. We think it is reasonable to say the testing samples are out-of-domain in this case. (And GPT-2-sized model can achieve ~80% accuracy with only 265 training samples in the testing domain.) Meanwhile, considering that the real-world data also has a small fraction of text that drops all the reasoning steps, we think this setting is not unrealistic.\n    - Calcutec: Please refer to Figure 5. It shows that even when no steps are dropped in the training set, the model still can do ICL, though the performance is not as good.\n\nQuestion:\n\n- The prompt is as the one in Table 1. We only include the reasoning steps of some (3) randomly selected examples and the premise of the testing input in the prompt. (For the example in Table 1, the testing input is `x55 x76 x84 x99`). We unroll until the first verbalizer is generated. \n\nPlease let us know if you have any other questions. We are more than happy to discuss with you.\n\n---\n\nThank you for your comments. \n\n## With Regard to the Weakness\n\n1. We consider one main contribution of our work to be explaining the ICL ability in the *semantically-unrelated label ICL (SUL-ICL)* setting [8], which is a step forward from the theorem by [7]. Indeed, our work can not explain the random label setting in [1]. However, we would like to point out that as far as we know, there has been no theorem able to explain it. We suggest that explaining it requires making more assumptions on the relationship between the input-label mapping and the domain/topic/theme distribution in the training data.\n2. We are sorry for the confusion. The justification mainly follows the argument in Section 3. We will make the connection clearer in our later revision. For now, please allow us to elaborate it below:\n    - Assumption 1: It follows the efforts made by early linguistics on linguistic formalism and cognitive psychology theories such as language of thought.\n    - Assumption 2: This assumption follows the argument in Section 3.1 that general text is usually organized in a similar way as logical induction processes. We assume that “a paragraph is a traversal of the nodes of the proving tree in the topological ordering” because in general text, statements usually follow some causal and/or chronological order, so it can convince readers or be easy for readers to understand.\n    - Assumption 3: In other words, this assumption assumes that every paragraph in an article mentions the topic of the article.\n    - Assumption 4: In other words, this assumption assumes that some pronouns are used in the article and each pronoun in the article is associated with a single entity.\n    - Assumption 5: This assumption follows the argument in Section 2.\n\n    In our opinion, these assumptions are relatively mild compared with previous theoretical works. We are happy to discuss more if you still have some concerns about the realism of these assumptions.\n3. To make our main response short, please allow us to discuss it below.\n4. Thank you for the list of relevant works. We will include some of them in our later revision (as we have some concerns about the technical soundness of some of them). \n\n\n## Atomic elements of NLP tasks in [2]\n\n\nIf we understand [2] correctly, [2] does not specify what characteristics the “atomic skills” of NLP need to have. We think our work could be an instantiation of the “atoms” proposed in [2]. \n\nIt seems that defining a set of “atomic skills” is not trivial and can’t be done arbitrarily. “Atomic skills” must be defined in a way such that the skills that work for the training data can generalize to the downstream task. To understand why it is non-trivial to define a set of atomic skills, we can probably think about a trivial case where we define the set “atomic skill” as the 26 skills to predict the words starting with A-Z. With this setting, the argument in [2] is greatly simplified. It seems that Theorem 14 only tells us that for most of the letters, the model can do the cloze problems in the training set well if the answer starts with that letter. It doesn’t seem to be guaranteed that the model can do the cloze problems in the testing set well.\n\nOn the other hand, defining the atomic skills with our atomic concepts seems to be more generalizable. Still, we posit it is necessary to discuss the discrepancies between the training set and the downstream task. That’s why in this work we explicitly characterize the distribution shifts in Section 4 and demonstrate that LMs are able to generalize under these distribution shifts in Section 5 and 7. Surely our settings do not fully replicate the real-world data distribution, but we think it still complements some aspects of [2]. \n\nPlease let us know if you still have any questions. We are more than happy to discuss further.\n\n\n[7] Xie, Sang Michael, et al. \"An explanation of in-context learning as implicit bayesian inference.\" arXiv preprint arXiv:2111.02080 (2021).\n\n[8] Wei, Jerry, et al. \"Larger language models do in-context learning differently.\" arXiv preprint arXiv:2303.03846 (2023).", "author_response": "Thank you for your comments. \n\n## With Regard to the Weakness\n\n1. We consider one main contribution of our work to be explaining the ICL ability in the *semantically-unrelated label ICL (SUL-ICL)* setting [8], which is a step forward from the theorem by [7]. Indeed, our work can not explain the random label setting in [1]. However, we would like to point out that as far as we know, there has been no theorem able to explain it. We suggest that explaining it requires making more assumptions on the relationship between the input-label mapping and the domain/topic/theme distribution in the training data.\n2. We are sorry for the confusion. The justification mainly follows the argument in Section 3. We will make the connection clearer in our later revision. For now, please allow us to elaborate it below:\n    - Assumption 1: It follows the efforts made by early linguistics on linguistic formalism and cognitive psychology theories such as language of thought.\n    - Assumption 2: This assumption follows the argument in Section 3.1 that general text is usually organized in a similar way as logical induction processes. We assume that “a paragraph is a traversal of the nodes of the proving tree in the topological ordering” because in general text, statements usually follow some causal and/or chronological order, so it can convince readers or be easy for readers to understand.\n    - Assumption 3: In other words, this assumption assumes that every paragraph in an article mentions the topic of the article.\n    - Assumption 4: In other words, this assumption assumes that some pronouns are used in the article and each pronoun in the article is associated with a single entity.\n    - Assumption 5: This assumption follows the argument in Section 2.\n\n    In our opinion, these assumptions are relatively mild compared with previous theoretical works. We are happy to discuss more if you still have some concerns about the realism of these assumptions.\n3. To make our main response short, please allow us to discuss it below.\n4. Thank you for the list of relevant works. We will include some of them in our later revision (as we have some concerns about the technical soundness of some of them). \n\n\n## Atomic elements of NLP tasks in [2]\n\n\nIf we understand [2] correctly, [2] does not specify what characteristics the “atomic skills” of NLP need to have. We think our work could be an instantiation of the “atoms” proposed in [2]. \n\nIt seems that defining a set of “atomic skills” is not trivial and can’t be done arbitrarily. “Atomic skills” must be defined in a way such that the skills that work for the training data can generalize to the downstream task. To understand why it is non-trivial to define a set of atomic skills, we can probably think about a trivial case where we define the set “atomic skill” as the 26 skills to predict the words starting with A-Z. With this setting, the argument in [2] is greatly simplified. It seems that Theorem 14 only tells us that for most of the letters, the model can do the cloze problems in the training set well if the answer starts with that letter. It doesn’t seem to be guaranteed that the model can do the cloze problems in the testing set well.\n\nOn the other hand, defining the atomic skills with our atomic concepts seems to be more generalizable. Still, we posit it is necessary to discuss the discrepancies between the training set and the downstream task. That’s why in this work we explicitly characterize the distribution shifts in Section 4 and demonstrate that LMs are able to generalize under these distribution shifts in Section 5 and 7. Surely our settings do not fully replicate the real-world data distribution, but we think it still complements some aspects of [2]. \n\nPlease let us know if you still have any questions. We are more than happy to discuss further.\n\n\n[7] Xie, Sang Michael, et al. \"An explanation of in-context learning as implicit bayesian inference.\" arXiv preprint arXiv:2111.02080 (2021).\n\n[8] Wei, Jerry, et al. \"Larger language models do in-context learning differently.\" arXiv preprint arXiv:2303.03846 (2023)."}
{"claim": "The manuscript does not reconcile prior findings that label-space correctness enables ICL even with wrong mappings with its own knowledge-base claims.", "claim_type": "subjective", "paper_id": "aaYBsuGRne", "paper_title": "Understanding In-context Learning with a Pelican Soup Hypothesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "EIVr3uVfUn", "reviewer": "Reviewer_rNJR", "review_text": "Summary: This paper focuses on understanding the in-context learning ability of large language models. The authors propose the Pelican soup hypothesis. It explains the in-context learning ability originating from learning the knowledge via the next token prediction. To support this hypothesis, the authors build a dataset and demonstrate the linkage between linguistic phenomena and in-context learning.\n\nStrengths: This paper provides substantial numerical results to support the proposed hypothesis. The linguistic phenomena analysis is also interesting to the community. In addition, the built dataset may be of independent interest.\n\nWeaknesses: 1. The claim related to the knowledge base needs more clarification. The experiments in [1] demonstrate that input-output mapping is not very important to the ICL. If the label space is correct, LLMs can even implement efficient ICL given wrong mapping. However, this wrong mapping conflicts with the knowledge base. More discussions are needed here.\n\n2. In Section 5.1, some assumptions are presented, but there is a notable absence of justification for these assumptions within the paper. This absence makes it challenging to ascertain the realism of these assumptions.\n3. I would greatly appreciate further elucidation on the distinction between the hypothesis presented in this paper and that discussed in [2]. Specifically, the variance between the \"atomic elements of NLP tasks\" and \"a set of atom concepts\" requires additional clarification.\n\n4. It is advantageous to include more highly relevant works in the related works. For example, besides HMM, implicit Bayesian inference is modeled for ICL in many different data assumptions [3,4,5]. [6] also studies the optimization side of ICL.\n\n[1] Min S, Lyu X, Holtzman A, et al. Rethinking the role of demonstrations: What makes in-context learning work?[J]. arXiv preprint arXiv:2202.12837, 2022.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Jiang H. A latent space theory for emergent abilities in large language models[J]. arXiv preprint arXiv:2304.09960, 2023.\n\n[4] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[5] Wang X, Zhu W, Wang W Y. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning[J]. arXiv preprint arXiv:2301.11916, 2023.\n\n[6] Dai D, Sun Y, Dong L, et al. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers[J]. arXiv preprint arXiv:2212.10559, 2022.\n\nQuestions: Questions are specified in Weakness part.", "labeling_timestamp": "2026-01-11T16:32:52.519460", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors explicitly acknowledge that their work does not explain the random-label setting and thereby do not reconcile those prior findings; they state their contribution is limited to a different setting (SUL-ICL) and note more assumptions would be needed to explain random-label results.", "evidence": "“Indeed, our work can not explain the random label setting in [1]. However, we would like to point out that as far as we know, there has been no theorem able to explain it. We suggest that explaining it requires making more assumptions on the relationship between the input-label mapping and the domain/topic/theme distribution in the training data.”", "combined_author_response": "We appreciate all reviewers' insightful comments.\n\nWe are particularly grateful that the reviewers agreed that our paper has the following strengths:\n\n- The linguistic phenomena analysis is interesting to the community (rNJR, MUGa, 5jds).\n- The Calcutec dataset and the digit-addition task may be useful for future research (rNJR, MUGa, 5jds, hbQa,cdtQ).\n- Provides evidence that even smaller LLMs can perform ICL with artificial/task-agnostic label symbols (MUGa, 5jds).\n- A new formalism for NLP classification tasks (5jds, hbQa, cdtQ).\n- Empirical results supporting the Pelican Soup Hypothesis (rNJR, cdtQ) and provide a potential explanation for in-context learning (hbQa).\n\nWe have revised the paper to address the main concern that Section 3 is not substantiated, which we acknowledge is the confusion caused by the inclarity. We added more elaboration on the linkage between Section 3 and Section 5.1, stressing that Section 3 is to draw intuitions for the assumptions in Section 5.1 and that Section 5.1 is the formal description for the characteristics described in Section 3.\n\nWe also address each reviewer’s concerns below respectively. Please let us know if any aspects are still unclear. We are more than happy to provide additional clarification.\n\n---\n\nWe thank you for the positive feedback. With regard to the weakness:\n\n1. While Section 3 is purely conceptual, we provide rigorous characterization in Section 5.1. We consider this as one of our main contributions, an instantiation for the assumptions made in previous theoretic works, such as [1] and [2]. For [1], our framework constituted with the assumptions we made in Section 5.1 satisfies the requirements of Corollary 4.2, therefore we can use their analysis to get an $O(1/T)$ regret bound ($T$ is the number of examples in the demonstration). For [2], please refer to our reply to reviewer rNJR.\n2. Thanks for the suggestion! We can run more experiments. Could you let us know what experiments you think are important are missing?\n3. We admit that we only focus on classification tasks in this work. However, we would like to stress that even for the classification setting, the mechanism of ICL is not yet clear. Future work may also extend our framework to generation tasks.\n\nWith regard to the questions:\n\n1. By preprocessing the training data for LLMs, we think it’s possible. However, training an LLM requires lots of computational resources. We leave it for future work.\n2. The formalism in Section 2 can handle arbitrarily difficult reasoning tasks as long as there is a logic system that can handle it and the induction searching process is computationally feasible, e.g. not NP-hard. We believe that most human-solvable tasks are in this class.\n3. We think this is an important open question. A possible solution would be augmenting the training data, e.g. [3].\n4. Our conjecture would be that for many commonsense rules, LLMs are trained with many instances for each of them. Because the training data may cover a large portion of common surface forms, even though it’s possible that LLMs are doing no more than pattern matching, they are still able to do “reasoning” following the commonsense rules.  \n\n[1] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Zhou, Wangchunshu, Ronan Le Bras, and Yejin Choi. \"Commonsense Knowledge Transfer for Pre-trained Language Models.\" arXiv preprint arXiv:2306.02388 (2023).\n\n---\n\nThank you for your positive review. \n\nWe would like to point out that producing an environment, with which in-context learning still works, is a practice used in most (if not all) theoretical analyses such as [1] and [2] and even some empirical analyses such as [3]. We consider one of our main contributions to be proposing an environment that is closer to the real-world scenario than previous works.\n\nWith regard to the questions you stated:\n\n- The yes/no questions in a Pelican Soup riddle have a similar function as the demonstrations for in-context learning, because based on the answer of those yes/no questions, the participants are able to figure out the latent story and thus are able to answer other following questions. Note that we use Pelican Soup riddles just to motivate the importance of the role of commonsense.\n- Our argument is that solving these two problems (ICL and modeling coreference) requires similar capabilities. This argument is to draw the intuition for the framework/assumptions in Section 5.1. Surely the two problems are not identical, thus we characterize the difference between these two problems with the distribution shifts discussed in Section 4. To show that LMs may be able to generalize under these distribution shifts, we replicate these distribution shifts in our experiments, as discussed in Section 5.3 and results in Section 5.5 are aligned with our hypothesis.\n- We understand ICL as generating verbalizers based on the meaning of the verbalizers, which is recovered according to the preceding context, so we think it is similar to modeling anaphora. Could you please elaborate more on why you think ICL is more like handling catephora?\n\nPlease let us know if you have any other comments. We are happy to discuss more!\n\n[1] Xie, Sang Michael, et al. \"An Explanation of In-context Learning as Implicit Bayesian Inference.\" International Conference on Learning Representations. 2021.\n\n[2] Hahn, Michael, and Navin Goyal. \"A theory of emergent in-context learning as implicit structure induction.\" arXiv preprint arXiv:2303.07971 (2023).\n\n[3] Chan, Stephanie, et al. \"Data distributional properties drive emergent in-context learning in transformers.\" Advances in Neural Information Processing Systems 35 (2022): 18878-18891.\n\n---\n\nWe thank you for your review. We would like to address the weakness below:\n\n1. Please let us know what are the arguments that you think are not substantiated. For the example you provided, we apologize for not including proper citations. We think that LMs are able to acquire some knowledge about commonsense (though imperfectly) has been widely accepted. Many studies can be found in previous works, e.g. [1][2].\n2. Validating theories with a synthetic dataset is a common practice. We admit that our setting cannot explain domain bias. It may require more assumptions to theoretically explain. We would like to mention that, we think it will make the construction of the toy data even more complicated, which based on your 4th point, is not acceptable. \n3. We justify our assumptions made in Section 5.1 in Section 3. As argued in Section 3.1, we posit that training data usually contain text that has some logical structure and/or follows chronicle order, which can contribute to LMs’ reasoning capability. The dataset Calcutec is a formal abstraction of such structure.\n4. Could you be more specific on what details are missing and how we should improve? We will appreciate your suggestion. The construction of Calcutec is more complicated than the toy data used in previous theoretical works such as [3] because Calcutec relies on milder assumptions and is more realistic. \n5. We apologize that we didn’t make the purpose of Section 3 clearer. We treat Section 3 as the justification/intuition for the abstraction/assumptions/framework we made in Section 5.1. We then instantiate the abstraction/framework with Calcutec and show that LMs trained with it can do in-context learning.\n6. We use Pelican Soup riddles only to motivate the intuition that commonsense plays an important role for ICL. We would not say our hypothesis is *based on* this similarity.\n7. Please let us know if you think any experiment is important but missing. Also, could you elaborate more on the possibility that the models solve the task by learning shortcuts (and maybe also your definition of shortcuts)?\n\nOur answer to the question\n\n1. We think mesa-optimization mainly explains why models of Transformer architecture trained with autoregressive loss have the capacity to model data that involves ICL-like phenomena. However, in works such as [4], they only study on some toy data. It is unclear how their toy data is relevant to natural language data. Our work starts from another direction, aiming to understand what characteristics of training data lead to the ICL ability.\n\n\n[1] West, Peter, et al. \"Symbolic knowledge distillation: from general language models to commonsense models.\" arXiv preprint arXiv:2110.07178 (2021).\n\n[2] Li, Xiang Lorraine, et al. \"A systematic investigation of commonsense knowledge in large language models.\" Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.\n\n[3] Xie, Sang Michael, et al. \"An explanation of in-context learning as implicit bayesian inference.\" arXiv preprint arXiv:2111.02080 (2021).\n\n[4] von Oswald, Johannes, et al. \"Uncovering mesa-optimization algorithms in transformers.\" arXiv preprint arXiv:2309.05858 (2023).\n\n---\n\nThank you for your thoughtful comments. We would like to address the weakness below:\n\n1. In Section 2, we propose a formalism for NLP classification tasks while the main purpose of Section 3 is to justify the assumptions made in Section 5. We see the assumptions in Section 5 as the formal statement/abstraction of our theorem in Section 3. We are sorry that we didn’t describe it more clearly. We will improve this part in our later revision. Please let us know if we need to make more clarifications.\n2. As far as we know, all theoretical analyses on ICL depend on some repetitive nature of prompts or training data, e.g. Xie et al and Hahn & Goyal. One difference in our work is that we focus on the repetitive usage and the consistent meaning of pronouns, which is very realistic in real-world data. In Figure 2, we also show that the model can still do ICL when the task description is more complicated than the meaning of the pronouns seen in the training set, i.e., when the task description is composed of 3 atomic concepts (the green lines for “triple” in Figure 2).\n3. We discuss this point below:\n    - The digit addition task: Please refer to Figure 8 for the distribution of the number of steps in the datasets. For the 5-digit addition task, when the dropping rate is 0.2, there are only 265 (0.265%) training samples where all the intermediate steps are dropped. We think it is reasonable to say the testing samples are out-of-domain in this case. (And GPT-2-sized model can achieve ~80% accuracy with only 265 training samples in the testing domain.) Meanwhile, considering that the real-world data also has a small fraction of text that drops all the reasoning steps, we think this setting is not unrealistic.\n    - Calcutec: Please refer to Figure 5. It shows that even when no steps are dropped in the training set, the model still can do ICL, though the performance is not as good.\n\nQuestion:\n\n- The prompt is as the one in Table 1. We only include the reasoning steps of some (3) randomly selected examples and the premise of the testing input in the prompt. (For the example in Table 1, the testing input is `x55 x76 x84 x99`). We unroll until the first verbalizer is generated. \n\nPlease let us know if you have any other questions. We are more than happy to discuss with you.\n\n---\n\nThank you for your comments. \n\n## With Regard to the Weakness\n\n1. We consider one main contribution of our work to be explaining the ICL ability in the *semantically-unrelated label ICL (SUL-ICL)* setting [8], which is a step forward from the theorem by [7]. Indeed, our work can not explain the random label setting in [1]. However, we would like to point out that as far as we know, there has been no theorem able to explain it. We suggest that explaining it requires making more assumptions on the relationship between the input-label mapping and the domain/topic/theme distribution in the training data.\n2. We are sorry for the confusion. The justification mainly follows the argument in Section 3. We will make the connection clearer in our later revision. For now, please allow us to elaborate it below:\n    - Assumption 1: It follows the efforts made by early linguistics on linguistic formalism and cognitive psychology theories such as language of thought.\n    - Assumption 2: This assumption follows the argument in Section 3.1 that general text is usually organized in a similar way as logical induction processes. We assume that “a paragraph is a traversal of the nodes of the proving tree in the topological ordering” because in general text, statements usually follow some causal and/or chronological order, so it can convince readers or be easy for readers to understand.\n    - Assumption 3: In other words, this assumption assumes that every paragraph in an article mentions the topic of the article.\n    - Assumption 4: In other words, this assumption assumes that some pronouns are used in the article and each pronoun in the article is associated with a single entity.\n    - Assumption 5: This assumption follows the argument in Section 2.\n\n    In our opinion, these assumptions are relatively mild compared with previous theoretical works. We are happy to discuss more if you still have some concerns about the realism of these assumptions.\n3. To make our main response short, please allow us to discuss it below.\n4. Thank you for the list of relevant works. We will include some of them in our later revision (as we have some concerns about the technical soundness of some of them). \n\n\n## Atomic elements of NLP tasks in [2]\n\n\nIf we understand [2] correctly, [2] does not specify what characteristics the “atomic skills” of NLP need to have. We think our work could be an instantiation of the “atoms” proposed in [2]. \n\nIt seems that defining a set of “atomic skills” is not trivial and can’t be done arbitrarily. “Atomic skills” must be defined in a way such that the skills that work for the training data can generalize to the downstream task. To understand why it is non-trivial to define a set of atomic skills, we can probably think about a trivial case where we define the set “atomic skill” as the 26 skills to predict the words starting with A-Z. With this setting, the argument in [2] is greatly simplified. It seems that Theorem 14 only tells us that for most of the letters, the model can do the cloze problems in the training set well if the answer starts with that letter. It doesn’t seem to be guaranteed that the model can do the cloze problems in the testing set well.\n\nOn the other hand, defining the atomic skills with our atomic concepts seems to be more generalizable. Still, we posit it is necessary to discuss the discrepancies between the training set and the downstream task. That’s why in this work we explicitly characterize the distribution shifts in Section 4 and demonstrate that LMs are able to generalize under these distribution shifts in Section 5 and 7. Surely our settings do not fully replicate the real-world data distribution, but we think it still complements some aspects of [2]. \n\nPlease let us know if you still have any questions. We are more than happy to discuss further.\n\n\n[7] Xie, Sang Michael, et al. \"An explanation of in-context learning as implicit bayesian inference.\" arXiv preprint arXiv:2111.02080 (2021).\n\n[8] Wei, Jerry, et al. \"Larger language models do in-context learning differently.\" arXiv preprint arXiv:2303.03846 (2023).", "author_response": "Thank you for your comments. \n\n## With Regard to the Weakness\n\n1. We consider one main contribution of our work to be explaining the ICL ability in the *semantically-unrelated label ICL (SUL-ICL)* setting [8], which is a step forward from the theorem by [7]. Indeed, our work can not explain the random label setting in [1]. However, we would like to point out that as far as we know, there has been no theorem able to explain it. We suggest that explaining it requires making more assumptions on the relationship between the input-label mapping and the domain/topic/theme distribution in the training data.\n2. We are sorry for the confusion. The justification mainly follows the argument in Section 3. We will make the connection clearer in our later revision. For now, please allow us to elaborate it below:\n    - Assumption 1: It follows the efforts made by early linguistics on linguistic formalism and cognitive psychology theories such as language of thought.\n    - Assumption 2: This assumption follows the argument in Section 3.1 that general text is usually organized in a similar way as logical induction processes. We assume that “a paragraph is a traversal of the nodes of the proving tree in the topological ordering” because in general text, statements usually follow some causal and/or chronological order, so it can convince readers or be easy for readers to understand.\n    - Assumption 3: In other words, this assumption assumes that every paragraph in an article mentions the topic of the article.\n    - Assumption 4: In other words, this assumption assumes that some pronouns are used in the article and each pronoun in the article is associated with a single entity.\n    - Assumption 5: This assumption follows the argument in Section 2.\n\n    In our opinion, these assumptions are relatively mild compared with previous theoretical works. We are happy to discuss more if you still have some concerns about the realism of these assumptions.\n3. To make our main response short, please allow us to discuss it below.\n4. Thank you for the list of relevant works. We will include some of them in our later revision (as we have some concerns about the technical soundness of some of them). \n\n\n## Atomic elements of NLP tasks in [2]\n\n\nIf we understand [2] correctly, [2] does not specify what characteristics the “atomic skills” of NLP need to have. We think our work could be an instantiation of the “atoms” proposed in [2]. \n\nIt seems that defining a set of “atomic skills” is not trivial and can’t be done arbitrarily. “Atomic skills” must be defined in a way such that the skills that work for the training data can generalize to the downstream task. To understand why it is non-trivial to define a set of atomic skills, we can probably think about a trivial case where we define the set “atomic skill” as the 26 skills to predict the words starting with A-Z. With this setting, the argument in [2] is greatly simplified. It seems that Theorem 14 only tells us that for most of the letters, the model can do the cloze problems in the training set well if the answer starts with that letter. It doesn’t seem to be guaranteed that the model can do the cloze problems in the testing set well.\n\nOn the other hand, defining the atomic skills with our atomic concepts seems to be more generalizable. Still, we posit it is necessary to discuss the discrepancies between the training set and the downstream task. That’s why in this work we explicitly characterize the distribution shifts in Section 4 and demonstrate that LMs are able to generalize under these distribution shifts in Section 5 and 7. Surely our settings do not fully replicate the real-world data distribution, but we think it still complements some aspects of [2]. \n\nPlease let us know if you still have any questions. We are more than happy to discuss further.\n\n\n[7] Xie, Sang Michael, et al. \"An explanation of in-context learning as implicit bayesian inference.\" arXiv preprint arXiv:2111.02080 (2021).\n\n[8] Wei, Jerry, et al. \"Larger language models do in-context learning differently.\" arXiv preprint arXiv:2303.03846 (2023)."}
{"claim": "Section 5.1 presents key assumptions without providing justification, derivation, or references supporting the realism of those assumptions.", "claim_type": "methodology", "paper_id": "aaYBsuGRne", "paper_title": "Understanding In-context Learning with a Pelican Soup Hypothesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "EIVr3uVfUn", "reviewer": "Reviewer_rNJR", "review_text": "Summary: This paper focuses on understanding the in-context learning ability of large language models. The authors propose the Pelican soup hypothesis. It explains the in-context learning ability originating from learning the knowledge via the next token prediction. To support this hypothesis, the authors build a dataset and demonstrate the linkage between linguistic phenomena and in-context learning.\n\nStrengths: This paper provides substantial numerical results to support the proposed hypothesis. The linguistic phenomena analysis is also interesting to the community. In addition, the built dataset may be of independent interest.\n\nWeaknesses: 1. The claim related to the knowledge base needs more clarification. The experiments in [1] demonstrate that input-output mapping is not very important to the ICL. If the label space is correct, LLMs can even implement efficient ICL given wrong mapping. However, this wrong mapping conflicts with the knowledge base. More discussions are needed here.\n\n2. In Section 5.1, some assumptions are presented, but there is a notable absence of justification for these assumptions within the paper. This absence makes it challenging to ascertain the realism of these assumptions.\n3. I would greatly appreciate further elucidation on the distinction between the hypothesis presented in this paper and that discussed in [2]. Specifically, the variance between the \"atomic elements of NLP tasks\" and \"a set of atom concepts\" requires additional clarification.\n\n4. It is advantageous to include more highly relevant works in the related works. For example, besides HMM, implicit Bayesian inference is modeled for ICL in many different data assumptions [3,4,5]. [6] also studies the optimization side of ICL.\n\n[1] Min S, Lyu X, Holtzman A, et al. Rethinking the role of demonstrations: What makes in-context learning work?[J]. arXiv preprint arXiv:2202.12837, 2022.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Jiang H. A latent space theory for emergent abilities in large language models[J]. arXiv preprint arXiv:2304.09960, 2023.\n\n[4] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[5] Wang X, Zhu W, Wang W Y. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning[J]. arXiv preprint arXiv:2301.11916, 2023.\n\n[6] Dai D, Sun Y, Dong L, et al. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers[J]. arXiv preprint arXiv:2212.10559, 2022.\n\nQuestions: Questions are specified in Weakness part.", "labeling_timestamp": "2026-01-11T16:33:13.946382", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors acknowledge the reviewer’s concern, say they revised the paper to add linkage and elaboration justifying the assumptions in Section 5.1, provide per-assumption explanations and cite prior work as supporting references, and promise further clarification in revision.", "evidence": "We have revised the paper to address the main concern that Section 3 is not substantiated, which we acknowledge is the confusion caused by the inclarity. We added more elaboration on the linkage between Section 3 and Section 5.1, stressing that Section 3 is to draw intuitions for the assumptions in Section 5.1 and that Section 5.1 is the formal description for the characteristics described in Section 3. ; The justification mainly follows the argument in Section 3. We will make the connection clearer in our later revision. For now, please allow us to elaborate it below: - Assumption 1: It follows the efforts made by early linguistics on linguistic formalism and cognitive psychology theories such as language of thought. - Assumption 2: This assumption follows the argument in Section 3.1 that general text is usually organized in a similar way as logical induction processes. ... - Assumption 5: This assumption follows the argument in Section 2.", "combined_author_response": "We appreciate all reviewers' insightful comments.\n\nWe are particularly grateful that the reviewers agreed that our paper has the following strengths:\n\n- The linguistic phenomena analysis is interesting to the community (rNJR, MUGa, 5jds).\n- The Calcutec dataset and the digit-addition task may be useful for future research (rNJR, MUGa, 5jds, hbQa,cdtQ).\n- Provides evidence that even smaller LLMs can perform ICL with artificial/task-agnostic label symbols (MUGa, 5jds).\n- A new formalism for NLP classification tasks (5jds, hbQa, cdtQ).\n- Empirical results supporting the Pelican Soup Hypothesis (rNJR, cdtQ) and provide a potential explanation for in-context learning (hbQa).\n\nWe have revised the paper to address the main concern that Section 3 is not substantiated, which we acknowledge is the confusion caused by the inclarity. We added more elaboration on the linkage between Section 3 and Section 5.1, stressing that Section 3 is to draw intuitions for the assumptions in Section 5.1 and that Section 5.1 is the formal description for the characteristics described in Section 3.\n\nWe also address each reviewer’s concerns below respectively. Please let us know if any aspects are still unclear. We are more than happy to provide additional clarification.\n\n---\n\nWe thank you for the positive feedback. With regard to the weakness:\n\n1. While Section 3 is purely conceptual, we provide rigorous characterization in Section 5.1. We consider this as one of our main contributions, an instantiation for the assumptions made in previous theoretic works, such as [1] and [2]. For [1], our framework constituted with the assumptions we made in Section 5.1 satisfies the requirements of Corollary 4.2, therefore we can use their analysis to get an $O(1/T)$ regret bound ($T$ is the number of examples in the demonstration). For [2], please refer to our reply to reviewer rNJR.\n2. Thanks for the suggestion! We can run more experiments. Could you let us know what experiments you think are important are missing?\n3. We admit that we only focus on classification tasks in this work. However, we would like to stress that even for the classification setting, the mechanism of ICL is not yet clear. Future work may also extend our framework to generation tasks.\n\nWith regard to the questions:\n\n1. By preprocessing the training data for LLMs, we think it’s possible. However, training an LLM requires lots of computational resources. We leave it for future work.\n2. The formalism in Section 2 can handle arbitrarily difficult reasoning tasks as long as there is a logic system that can handle it and the induction searching process is computationally feasible, e.g. not NP-hard. We believe that most human-solvable tasks are in this class.\n3. We think this is an important open question. A possible solution would be augmenting the training data, e.g. [3].\n4. Our conjecture would be that for many commonsense rules, LLMs are trained with many instances for each of them. Because the training data may cover a large portion of common surface forms, even though it’s possible that LLMs are doing no more than pattern matching, they are still able to do “reasoning” following the commonsense rules.  \n\n[1] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Zhou, Wangchunshu, Ronan Le Bras, and Yejin Choi. \"Commonsense Knowledge Transfer for Pre-trained Language Models.\" arXiv preprint arXiv:2306.02388 (2023).\n\n---\n\nThank you for your positive review. \n\nWe would like to point out that producing an environment, with which in-context learning still works, is a practice used in most (if not all) theoretical analyses such as [1] and [2] and even some empirical analyses such as [3]. We consider one of our main contributions to be proposing an environment that is closer to the real-world scenario than previous works.\n\nWith regard to the questions you stated:\n\n- The yes/no questions in a Pelican Soup riddle have a similar function as the demonstrations for in-context learning, because based on the answer of those yes/no questions, the participants are able to figure out the latent story and thus are able to answer other following questions. Note that we use Pelican Soup riddles just to motivate the importance of the role of commonsense.\n- Our argument is that solving these two problems (ICL and modeling coreference) requires similar capabilities. This argument is to draw the intuition for the framework/assumptions in Section 5.1. Surely the two problems are not identical, thus we characterize the difference between these two problems with the distribution shifts discussed in Section 4. To show that LMs may be able to generalize under these distribution shifts, we replicate these distribution shifts in our experiments, as discussed in Section 5.3 and results in Section 5.5 are aligned with our hypothesis.\n- We understand ICL as generating verbalizers based on the meaning of the verbalizers, which is recovered according to the preceding context, so we think it is similar to modeling anaphora. Could you please elaborate more on why you think ICL is more like handling catephora?\n\nPlease let us know if you have any other comments. We are happy to discuss more!\n\n[1] Xie, Sang Michael, et al. \"An Explanation of In-context Learning as Implicit Bayesian Inference.\" International Conference on Learning Representations. 2021.\n\n[2] Hahn, Michael, and Navin Goyal. \"A theory of emergent in-context learning as implicit structure induction.\" arXiv preprint arXiv:2303.07971 (2023).\n\n[3] Chan, Stephanie, et al. \"Data distributional properties drive emergent in-context learning in transformers.\" Advances in Neural Information Processing Systems 35 (2022): 18878-18891.\n\n---\n\nWe thank you for your review. We would like to address the weakness below:\n\n1. Please let us know what are the arguments that you think are not substantiated. For the example you provided, we apologize for not including proper citations. We think that LMs are able to acquire some knowledge about commonsense (though imperfectly) has been widely accepted. Many studies can be found in previous works, e.g. [1][2].\n2. Validating theories with a synthetic dataset is a common practice. We admit that our setting cannot explain domain bias. It may require more assumptions to theoretically explain. We would like to mention that, we think it will make the construction of the toy data even more complicated, which based on your 4th point, is not acceptable. \n3. We justify our assumptions made in Section 5.1 in Section 3. As argued in Section 3.1, we posit that training data usually contain text that has some logical structure and/or follows chronicle order, which can contribute to LMs’ reasoning capability. The dataset Calcutec is a formal abstraction of such structure.\n4. Could you be more specific on what details are missing and how we should improve? We will appreciate your suggestion. The construction of Calcutec is more complicated than the toy data used in previous theoretical works such as [3] because Calcutec relies on milder assumptions and is more realistic. \n5. We apologize that we didn’t make the purpose of Section 3 clearer. We treat Section 3 as the justification/intuition for the abstraction/assumptions/framework we made in Section 5.1. We then instantiate the abstraction/framework with Calcutec and show that LMs trained with it can do in-context learning.\n6. We use Pelican Soup riddles only to motivate the intuition that commonsense plays an important role for ICL. We would not say our hypothesis is *based on* this similarity.\n7. Please let us know if you think any experiment is important but missing. Also, could you elaborate more on the possibility that the models solve the task by learning shortcuts (and maybe also your definition of shortcuts)?\n\nOur answer to the question\n\n1. We think mesa-optimization mainly explains why models of Transformer architecture trained with autoregressive loss have the capacity to model data that involves ICL-like phenomena. However, in works such as [4], they only study on some toy data. It is unclear how their toy data is relevant to natural language data. Our work starts from another direction, aiming to understand what characteristics of training data lead to the ICL ability.\n\n\n[1] West, Peter, et al. \"Symbolic knowledge distillation: from general language models to commonsense models.\" arXiv preprint arXiv:2110.07178 (2021).\n\n[2] Li, Xiang Lorraine, et al. \"A systematic investigation of commonsense knowledge in large language models.\" Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.\n\n[3] Xie, Sang Michael, et al. \"An explanation of in-context learning as implicit bayesian inference.\" arXiv preprint arXiv:2111.02080 (2021).\n\n[4] von Oswald, Johannes, et al. \"Uncovering mesa-optimization algorithms in transformers.\" arXiv preprint arXiv:2309.05858 (2023).\n\n---\n\nThank you for your thoughtful comments. We would like to address the weakness below:\n\n1. In Section 2, we propose a formalism for NLP classification tasks while the main purpose of Section 3 is to justify the assumptions made in Section 5. We see the assumptions in Section 5 as the formal statement/abstraction of our theorem in Section 3. We are sorry that we didn’t describe it more clearly. We will improve this part in our later revision. Please let us know if we need to make more clarifications.\n2. As far as we know, all theoretical analyses on ICL depend on some repetitive nature of prompts or training data, e.g. Xie et al and Hahn & Goyal. One difference in our work is that we focus on the repetitive usage and the consistent meaning of pronouns, which is very realistic in real-world data. In Figure 2, we also show that the model can still do ICL when the task description is more complicated than the meaning of the pronouns seen in the training set, i.e., when the task description is composed of 3 atomic concepts (the green lines for “triple” in Figure 2).\n3. We discuss this point below:\n    - The digit addition task: Please refer to Figure 8 for the distribution of the number of steps in the datasets. For the 5-digit addition task, when the dropping rate is 0.2, there are only 265 (0.265%) training samples where all the intermediate steps are dropped. We think it is reasonable to say the testing samples are out-of-domain in this case. (And GPT-2-sized model can achieve ~80% accuracy with only 265 training samples in the testing domain.) Meanwhile, considering that the real-world data also has a small fraction of text that drops all the reasoning steps, we think this setting is not unrealistic.\n    - Calcutec: Please refer to Figure 5. It shows that even when no steps are dropped in the training set, the model still can do ICL, though the performance is not as good.\n\nQuestion:\n\n- The prompt is as the one in Table 1. We only include the reasoning steps of some (3) randomly selected examples and the premise of the testing input in the prompt. (For the example in Table 1, the testing input is `x55 x76 x84 x99`). We unroll until the first verbalizer is generated. \n\nPlease let us know if you have any other questions. We are more than happy to discuss with you.\n\n---\n\nThank you for your comments. \n\n## With Regard to the Weakness\n\n1. We consider one main contribution of our work to be explaining the ICL ability in the *semantically-unrelated label ICL (SUL-ICL)* setting [8], which is a step forward from the theorem by [7]. Indeed, our work can not explain the random label setting in [1]. However, we would like to point out that as far as we know, there has been no theorem able to explain it. We suggest that explaining it requires making more assumptions on the relationship between the input-label mapping and the domain/topic/theme distribution in the training data.\n2. We are sorry for the confusion. The justification mainly follows the argument in Section 3. We will make the connection clearer in our later revision. For now, please allow us to elaborate it below:\n    - Assumption 1: It follows the efforts made by early linguistics on linguistic formalism and cognitive psychology theories such as language of thought.\n    - Assumption 2: This assumption follows the argument in Section 3.1 that general text is usually organized in a similar way as logical induction processes. We assume that “a paragraph is a traversal of the nodes of the proving tree in the topological ordering” because in general text, statements usually follow some causal and/or chronological order, so it can convince readers or be easy for readers to understand.\n    - Assumption 3: In other words, this assumption assumes that every paragraph in an article mentions the topic of the article.\n    - Assumption 4: In other words, this assumption assumes that some pronouns are used in the article and each pronoun in the article is associated with a single entity.\n    - Assumption 5: This assumption follows the argument in Section 2.\n\n    In our opinion, these assumptions are relatively mild compared with previous theoretical works. We are happy to discuss more if you still have some concerns about the realism of these assumptions.\n3. To make our main response short, please allow us to discuss it below.\n4. Thank you for the list of relevant works. We will include some of them in our later revision (as we have some concerns about the technical soundness of some of them). \n\n\n## Atomic elements of NLP tasks in [2]\n\n\nIf we understand [2] correctly, [2] does not specify what characteristics the “atomic skills” of NLP need to have. We think our work could be an instantiation of the “atoms” proposed in [2]. \n\nIt seems that defining a set of “atomic skills” is not trivial and can’t be done arbitrarily. “Atomic skills” must be defined in a way such that the skills that work for the training data can generalize to the downstream task. To understand why it is non-trivial to define a set of atomic skills, we can probably think about a trivial case where we define the set “atomic skill” as the 26 skills to predict the words starting with A-Z. With this setting, the argument in [2] is greatly simplified. It seems that Theorem 14 only tells us that for most of the letters, the model can do the cloze problems in the training set well if the answer starts with that letter. It doesn’t seem to be guaranteed that the model can do the cloze problems in the testing set well.\n\nOn the other hand, defining the atomic skills with our atomic concepts seems to be more generalizable. Still, we posit it is necessary to discuss the discrepancies between the training set and the downstream task. That’s why in this work we explicitly characterize the distribution shifts in Section 4 and demonstrate that LMs are able to generalize under these distribution shifts in Section 5 and 7. Surely our settings do not fully replicate the real-world data distribution, but we think it still complements some aspects of [2]. \n\nPlease let us know if you still have any questions. We are more than happy to discuss further.\n\n\n[7] Xie, Sang Michael, et al. \"An explanation of in-context learning as implicit bayesian inference.\" arXiv preprint arXiv:2111.02080 (2021).\n\n[8] Wei, Jerry, et al. \"Larger language models do in-context learning differently.\" arXiv preprint arXiv:2303.03846 (2023).", "author_response": "Thank you for your comments. \n\n## With Regard to the Weakness\n\n1. We consider one main contribution of our work to be explaining the ICL ability in the *semantically-unrelated label ICL (SUL-ICL)* setting [8], which is a step forward from the theorem by [7]. Indeed, our work can not explain the random label setting in [1]. However, we would like to point out that as far as we know, there has been no theorem able to explain it. We suggest that explaining it requires making more assumptions on the relationship between the input-label mapping and the domain/topic/theme distribution in the training data.\n2. We are sorry for the confusion. The justification mainly follows the argument in Section 3. We will make the connection clearer in our later revision. For now, please allow us to elaborate it below:\n    - Assumption 1: It follows the efforts made by early linguistics on linguistic formalism and cognitive psychology theories such as language of thought.\n    - Assumption 2: This assumption follows the argument in Section 3.1 that general text is usually organized in a similar way as logical induction processes. We assume that “a paragraph is a traversal of the nodes of the proving tree in the topological ordering” because in general text, statements usually follow some causal and/or chronological order, so it can convince readers or be easy for readers to understand.\n    - Assumption 3: In other words, this assumption assumes that every paragraph in an article mentions the topic of the article.\n    - Assumption 4: In other words, this assumption assumes that some pronouns are used in the article and each pronoun in the article is associated with a single entity.\n    - Assumption 5: This assumption follows the argument in Section 2.\n\n    In our opinion, these assumptions are relatively mild compared with previous theoretical works. We are happy to discuss more if you still have some concerns about the realism of these assumptions.\n3. To make our main response short, please allow us to discuss it below.\n4. Thank you for the list of relevant works. We will include some of them in our later revision (as we have some concerns about the technical soundness of some of them). \n\n\n## Atomic elements of NLP tasks in [2]\n\n\nIf we understand [2] correctly, [2] does not specify what characteristics the “atomic skills” of NLP need to have. We think our work could be an instantiation of the “atoms” proposed in [2]. \n\nIt seems that defining a set of “atomic skills” is not trivial and can’t be done arbitrarily. “Atomic skills” must be defined in a way such that the skills that work for the training data can generalize to the downstream task. To understand why it is non-trivial to define a set of atomic skills, we can probably think about a trivial case where we define the set “atomic skill” as the 26 skills to predict the words starting with A-Z. With this setting, the argument in [2] is greatly simplified. It seems that Theorem 14 only tells us that for most of the letters, the model can do the cloze problems in the training set well if the answer starts with that letter. It doesn’t seem to be guaranteed that the model can do the cloze problems in the testing set well.\n\nOn the other hand, defining the atomic skills with our atomic concepts seems to be more generalizable. Still, we posit it is necessary to discuss the discrepancies between the training set and the downstream task. That’s why in this work we explicitly characterize the distribution shifts in Section 4 and demonstrate that LMs are able to generalize under these distribution shifts in Section 5 and 7. Surely our settings do not fully replicate the real-world data distribution, but we think it still complements some aspects of [2]. \n\nPlease let us know if you still have any questions. We are more than happy to discuss further.\n\n\n[7] Xie, Sang Michael, et al. \"An explanation of in-context learning as implicit bayesian inference.\" arXiv preprint arXiv:2111.02080 (2021).\n\n[8] Wei, Jerry, et al. \"Larger language models do in-context learning differently.\" arXiv preprint arXiv:2303.03846 (2023)."}
{"claim": "The paper lacks empirical analysis or validation demonstrating that the assumptions in Section 5.1 hold in realistic data or model settings.", "claim_type": "experimental", "paper_id": "aaYBsuGRne", "paper_title": "Understanding In-context Learning with a Pelican Soup Hypothesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "EIVr3uVfUn", "reviewer": "Reviewer_rNJR", "review_text": "Summary: This paper focuses on understanding the in-context learning ability of large language models. The authors propose the Pelican soup hypothesis. It explains the in-context learning ability originating from learning the knowledge via the next token prediction. To support this hypothesis, the authors build a dataset and demonstrate the linkage between linguistic phenomena and in-context learning.\n\nStrengths: This paper provides substantial numerical results to support the proposed hypothesis. The linguistic phenomena analysis is also interesting to the community. In addition, the built dataset may be of independent interest.\n\nWeaknesses: 1. The claim related to the knowledge base needs more clarification. The experiments in [1] demonstrate that input-output mapping is not very important to the ICL. If the label space is correct, LLMs can even implement efficient ICL given wrong mapping. However, this wrong mapping conflicts with the knowledge base. More discussions are needed here.\n\n2. In Section 5.1, some assumptions are presented, but there is a notable absence of justification for these assumptions within the paper. This absence makes it challenging to ascertain the realism of these assumptions.\n3. I would greatly appreciate further elucidation on the distinction between the hypothesis presented in this paper and that discussed in [2]. Specifically, the variance between the \"atomic elements of NLP tasks\" and \"a set of atom concepts\" requires additional clarification.\n\n4. It is advantageous to include more highly relevant works in the related works. For example, besides HMM, implicit Bayesian inference is modeled for ICL in many different data assumptions [3,4,5]. [6] also studies the optimization side of ICL.\n\n[1] Min S, Lyu X, Holtzman A, et al. Rethinking the role of demonstrations: What makes in-context learning work?[J]. arXiv preprint arXiv:2202.12837, 2022.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Jiang H. A latent space theory for emergent abilities in large language models[J]. arXiv preprint arXiv:2304.09960, 2023.\n\n[4] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[5] Wang X, Zhu W, Wang W Y. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning[J]. arXiv preprint arXiv:2301.11916, 2023.\n\n[6] Dai D, Sun Y, Dong L, et al. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers[J]. arXiv preprint arXiv:2212.10559, 2022.\n\nQuestions: Questions are specified in Weakness part.", "labeling_timestamp": "2026-01-11T16:33:13.810757", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors report empirical instantiation of their assumptions (via the Calcutec dataset and experiments replicating distribution shifts) and clarify the link between Sections 3 and 5.1, but they also acknowledge limitations (e.g., inability to explain domain bias) and do not fully demonstrate the assumptions hold broadly in realistic data/model settings.", "evidence": "\"We then instantiate the abstraction/framework with Calcutec and show that LMs trained with it can do in-context learning.\"", "combined_author_response": "We appreciate all reviewers' insightful comments.\n\nWe are particularly grateful that the reviewers agreed that our paper has the following strengths:\n\n- The linguistic phenomena analysis is interesting to the community (rNJR, MUGa, 5jds).\n- The Calcutec dataset and the digit-addition task may be useful for future research (rNJR, MUGa, 5jds, hbQa,cdtQ).\n- Provides evidence that even smaller LLMs can perform ICL with artificial/task-agnostic label symbols (MUGa, 5jds).\n- A new formalism for NLP classification tasks (5jds, hbQa, cdtQ).\n- Empirical results supporting the Pelican Soup Hypothesis (rNJR, cdtQ) and provide a potential explanation for in-context learning (hbQa).\n\nWe have revised the paper to address the main concern that Section 3 is not substantiated, which we acknowledge is the confusion caused by the inclarity. We added more elaboration on the linkage between Section 3 and Section 5.1, stressing that Section 3 is to draw intuitions for the assumptions in Section 5.1 and that Section 5.1 is the formal description for the characteristics described in Section 3.\n\nWe also address each reviewer’s concerns below respectively. Please let us know if any aspects are still unclear. We are more than happy to provide additional clarification.\n\n---\n\nWe thank you for the positive feedback. With regard to the weakness:\n\n1. While Section 3 is purely conceptual, we provide rigorous characterization in Section 5.1. We consider this as one of our main contributions, an instantiation for the assumptions made in previous theoretic works, such as [1] and [2]. For [1], our framework constituted with the assumptions we made in Section 5.1 satisfies the requirements of Corollary 4.2, therefore we can use their analysis to get an $O(1/T)$ regret bound ($T$ is the number of examples in the demonstration). For [2], please refer to our reply to reviewer rNJR.\n2. Thanks for the suggestion! We can run more experiments. Could you let us know what experiments you think are important are missing?\n3. We admit that we only focus on classification tasks in this work. However, we would like to stress that even for the classification setting, the mechanism of ICL is not yet clear. Future work may also extend our framework to generation tasks.\n\nWith regard to the questions:\n\n1. By preprocessing the training data for LLMs, we think it’s possible. However, training an LLM requires lots of computational resources. We leave it for future work.\n2. The formalism in Section 2 can handle arbitrarily difficult reasoning tasks as long as there is a logic system that can handle it and the induction searching process is computationally feasible, e.g. not NP-hard. We believe that most human-solvable tasks are in this class.\n3. We think this is an important open question. A possible solution would be augmenting the training data, e.g. [3].\n4. Our conjecture would be that for many commonsense rules, LLMs are trained with many instances for each of them. Because the training data may cover a large portion of common surface forms, even though it’s possible that LLMs are doing no more than pattern matching, they are still able to do “reasoning” following the commonsense rules.  \n\n[1] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Zhou, Wangchunshu, Ronan Le Bras, and Yejin Choi. \"Commonsense Knowledge Transfer for Pre-trained Language Models.\" arXiv preprint arXiv:2306.02388 (2023).\n\n---\n\nThank you for your positive review. \n\nWe would like to point out that producing an environment, with which in-context learning still works, is a practice used in most (if not all) theoretical analyses such as [1] and [2] and even some empirical analyses such as [3]. We consider one of our main contributions to be proposing an environment that is closer to the real-world scenario than previous works.\n\nWith regard to the questions you stated:\n\n- The yes/no questions in a Pelican Soup riddle have a similar function as the demonstrations for in-context learning, because based on the answer of those yes/no questions, the participants are able to figure out the latent story and thus are able to answer other following questions. Note that we use Pelican Soup riddles just to motivate the importance of the role of commonsense.\n- Our argument is that solving these two problems (ICL and modeling coreference) requires similar capabilities. This argument is to draw the intuition for the framework/assumptions in Section 5.1. Surely the two problems are not identical, thus we characterize the difference between these two problems with the distribution shifts discussed in Section 4. To show that LMs may be able to generalize under these distribution shifts, we replicate these distribution shifts in our experiments, as discussed in Section 5.3 and results in Section 5.5 are aligned with our hypothesis.\n- We understand ICL as generating verbalizers based on the meaning of the verbalizers, which is recovered according to the preceding context, so we think it is similar to modeling anaphora. Could you please elaborate more on why you think ICL is more like handling catephora?\n\nPlease let us know if you have any other comments. We are happy to discuss more!\n\n[1] Xie, Sang Michael, et al. \"An Explanation of In-context Learning as Implicit Bayesian Inference.\" International Conference on Learning Representations. 2021.\n\n[2] Hahn, Michael, and Navin Goyal. \"A theory of emergent in-context learning as implicit structure induction.\" arXiv preprint arXiv:2303.07971 (2023).\n\n[3] Chan, Stephanie, et al. \"Data distributional properties drive emergent in-context learning in transformers.\" Advances in Neural Information Processing Systems 35 (2022): 18878-18891.\n\n---\n\nWe thank you for your review. We would like to address the weakness below:\n\n1. Please let us know what are the arguments that you think are not substantiated. For the example you provided, we apologize for not including proper citations. We think that LMs are able to acquire some knowledge about commonsense (though imperfectly) has been widely accepted. Many studies can be found in previous works, e.g. [1][2].\n2. Validating theories with a synthetic dataset is a common practice. We admit that our setting cannot explain domain bias. It may require more assumptions to theoretically explain. We would like to mention that, we think it will make the construction of the toy data even more complicated, which based on your 4th point, is not acceptable. \n3. We justify our assumptions made in Section 5.1 in Section 3. As argued in Section 3.1, we posit that training data usually contain text that has some logical structure and/or follows chronicle order, which can contribute to LMs’ reasoning capability. The dataset Calcutec is a formal abstraction of such structure.\n4. Could you be more specific on what details are missing and how we should improve? We will appreciate your suggestion. The construction of Calcutec is more complicated than the toy data used in previous theoretical works such as [3] because Calcutec relies on milder assumptions and is more realistic. \n5. We apologize that we didn’t make the purpose of Section 3 clearer. We treat Section 3 as the justification/intuition for the abstraction/assumptions/framework we made in Section 5.1. We then instantiate the abstraction/framework with Calcutec and show that LMs trained with it can do in-context learning.\n6. We use Pelican Soup riddles only to motivate the intuition that commonsense plays an important role for ICL. We would not say our hypothesis is *based on* this similarity.\n7. Please let us know if you think any experiment is important but missing. Also, could you elaborate more on the possibility that the models solve the task by learning shortcuts (and maybe also your definition of shortcuts)?\n\nOur answer to the question\n\n1. We think mesa-optimization mainly explains why models of Transformer architecture trained with autoregressive loss have the capacity to model data that involves ICL-like phenomena. However, in works such as [4], they only study on some toy data. It is unclear how their toy data is relevant to natural language data. Our work starts from another direction, aiming to understand what characteristics of training data lead to the ICL ability.\n\n\n[1] West, Peter, et al. \"Symbolic knowledge distillation: from general language models to commonsense models.\" arXiv preprint arXiv:2110.07178 (2021).\n\n[2] Li, Xiang Lorraine, et al. \"A systematic investigation of commonsense knowledge in large language models.\" Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.\n\n[3] Xie, Sang Michael, et al. \"An explanation of in-context learning as implicit bayesian inference.\" arXiv preprint arXiv:2111.02080 (2021).\n\n[4] von Oswald, Johannes, et al. \"Uncovering mesa-optimization algorithms in transformers.\" arXiv preprint arXiv:2309.05858 (2023).\n\n---\n\nThank you for your thoughtful comments. We would like to address the weakness below:\n\n1. In Section 2, we propose a formalism for NLP classification tasks while the main purpose of Section 3 is to justify the assumptions made in Section 5. We see the assumptions in Section 5 as the formal statement/abstraction of our theorem in Section 3. We are sorry that we didn’t describe it more clearly. We will improve this part in our later revision. Please let us know if we need to make more clarifications.\n2. As far as we know, all theoretical analyses on ICL depend on some repetitive nature of prompts or training data, e.g. Xie et al and Hahn & Goyal. One difference in our work is that we focus on the repetitive usage and the consistent meaning of pronouns, which is very realistic in real-world data. In Figure 2, we also show that the model can still do ICL when the task description is more complicated than the meaning of the pronouns seen in the training set, i.e., when the task description is composed of 3 atomic concepts (the green lines for “triple” in Figure 2).\n3. We discuss this point below:\n    - The digit addition task: Please refer to Figure 8 for the distribution of the number of steps in the datasets. For the 5-digit addition task, when the dropping rate is 0.2, there are only 265 (0.265%) training samples where all the intermediate steps are dropped. We think it is reasonable to say the testing samples are out-of-domain in this case. (And GPT-2-sized model can achieve ~80% accuracy with only 265 training samples in the testing domain.) Meanwhile, considering that the real-world data also has a small fraction of text that drops all the reasoning steps, we think this setting is not unrealistic.\n    - Calcutec: Please refer to Figure 5. It shows that even when no steps are dropped in the training set, the model still can do ICL, though the performance is not as good.\n\nQuestion:\n\n- The prompt is as the one in Table 1. We only include the reasoning steps of some (3) randomly selected examples and the premise of the testing input in the prompt. (For the example in Table 1, the testing input is `x55 x76 x84 x99`). We unroll until the first verbalizer is generated. \n\nPlease let us know if you have any other questions. We are more than happy to discuss with you.\n\n---\n\nThank you for your comments. \n\n## With Regard to the Weakness\n\n1. We consider one main contribution of our work to be explaining the ICL ability in the *semantically-unrelated label ICL (SUL-ICL)* setting [8], which is a step forward from the theorem by [7]. Indeed, our work can not explain the random label setting in [1]. However, we would like to point out that as far as we know, there has been no theorem able to explain it. We suggest that explaining it requires making more assumptions on the relationship between the input-label mapping and the domain/topic/theme distribution in the training data.\n2. We are sorry for the confusion. The justification mainly follows the argument in Section 3. We will make the connection clearer in our later revision. For now, please allow us to elaborate it below:\n    - Assumption 1: It follows the efforts made by early linguistics on linguistic formalism and cognitive psychology theories such as language of thought.\n    - Assumption 2: This assumption follows the argument in Section 3.1 that general text is usually organized in a similar way as logical induction processes. We assume that “a paragraph is a traversal of the nodes of the proving tree in the topological ordering” because in general text, statements usually follow some causal and/or chronological order, so it can convince readers or be easy for readers to understand.\n    - Assumption 3: In other words, this assumption assumes that every paragraph in an article mentions the topic of the article.\n    - Assumption 4: In other words, this assumption assumes that some pronouns are used in the article and each pronoun in the article is associated with a single entity.\n    - Assumption 5: This assumption follows the argument in Section 2.\n\n    In our opinion, these assumptions are relatively mild compared with previous theoretical works. We are happy to discuss more if you still have some concerns about the realism of these assumptions.\n3. To make our main response short, please allow us to discuss it below.\n4. Thank you for the list of relevant works. We will include some of them in our later revision (as we have some concerns about the technical soundness of some of them). \n\n\n## Atomic elements of NLP tasks in [2]\n\n\nIf we understand [2] correctly, [2] does not specify what characteristics the “atomic skills” of NLP need to have. We think our work could be an instantiation of the “atoms” proposed in [2]. \n\nIt seems that defining a set of “atomic skills” is not trivial and can’t be done arbitrarily. “Atomic skills” must be defined in a way such that the skills that work for the training data can generalize to the downstream task. To understand why it is non-trivial to define a set of atomic skills, we can probably think about a trivial case where we define the set “atomic skill” as the 26 skills to predict the words starting with A-Z. With this setting, the argument in [2] is greatly simplified. It seems that Theorem 14 only tells us that for most of the letters, the model can do the cloze problems in the training set well if the answer starts with that letter. It doesn’t seem to be guaranteed that the model can do the cloze problems in the testing set well.\n\nOn the other hand, defining the atomic skills with our atomic concepts seems to be more generalizable. Still, we posit it is necessary to discuss the discrepancies between the training set and the downstream task. That’s why in this work we explicitly characterize the distribution shifts in Section 4 and demonstrate that LMs are able to generalize under these distribution shifts in Section 5 and 7. Surely our settings do not fully replicate the real-world data distribution, but we think it still complements some aspects of [2]. \n\nPlease let us know if you still have any questions. We are more than happy to discuss further.\n\n\n[7] Xie, Sang Michael, et al. \"An explanation of in-context learning as implicit bayesian inference.\" arXiv preprint arXiv:2111.02080 (2021).\n\n[8] Wei, Jerry, et al. \"Larger language models do in-context learning differently.\" arXiv preprint arXiv:2303.03846 (2023).", "author_response": "Thank you for your comments. \n\n## With Regard to the Weakness\n\n1. We consider one main contribution of our work to be explaining the ICL ability in the *semantically-unrelated label ICL (SUL-ICL)* setting [8], which is a step forward from the theorem by [7]. Indeed, our work can not explain the random label setting in [1]. However, we would like to point out that as far as we know, there has been no theorem able to explain it. We suggest that explaining it requires making more assumptions on the relationship between the input-label mapping and the domain/topic/theme distribution in the training data.\n2. We are sorry for the confusion. The justification mainly follows the argument in Section 3. We will make the connection clearer in our later revision. For now, please allow us to elaborate it below:\n    - Assumption 1: It follows the efforts made by early linguistics on linguistic formalism and cognitive psychology theories such as language of thought.\n    - Assumption 2: This assumption follows the argument in Section 3.1 that general text is usually organized in a similar way as logical induction processes. We assume that “a paragraph is a traversal of the nodes of the proving tree in the topological ordering” because in general text, statements usually follow some causal and/or chronological order, so it can convince readers or be easy for readers to understand.\n    - Assumption 3: In other words, this assumption assumes that every paragraph in an article mentions the topic of the article.\n    - Assumption 4: In other words, this assumption assumes that some pronouns are used in the article and each pronoun in the article is associated with a single entity.\n    - Assumption 5: This assumption follows the argument in Section 2.\n\n    In our opinion, these assumptions are relatively mild compared with previous theoretical works. We are happy to discuss more if you still have some concerns about the realism of these assumptions.\n3. To make our main response short, please allow us to discuss it below.\n4. Thank you for the list of relevant works. We will include some of them in our later revision (as we have some concerns about the technical soundness of some of them). \n\n\n## Atomic elements of NLP tasks in [2]\n\n\nIf we understand [2] correctly, [2] does not specify what characteristics the “atomic skills” of NLP need to have. We think our work could be an instantiation of the “atoms” proposed in [2]. \n\nIt seems that defining a set of “atomic skills” is not trivial and can’t be done arbitrarily. “Atomic skills” must be defined in a way such that the skills that work for the training data can generalize to the downstream task. To understand why it is non-trivial to define a set of atomic skills, we can probably think about a trivial case where we define the set “atomic skill” as the 26 skills to predict the words starting with A-Z. With this setting, the argument in [2] is greatly simplified. It seems that Theorem 14 only tells us that for most of the letters, the model can do the cloze problems in the training set well if the answer starts with that letter. It doesn’t seem to be guaranteed that the model can do the cloze problems in the testing set well.\n\nOn the other hand, defining the atomic skills with our atomic concepts seems to be more generalizable. Still, we posit it is necessary to discuss the discrepancies between the training set and the downstream task. That’s why in this work we explicitly characterize the distribution shifts in Section 4 and demonstrate that LMs are able to generalize under these distribution shifts in Section 5 and 7. Surely our settings do not fully replicate the real-world data distribution, but we think it still complements some aspects of [2]. \n\nPlease let us know if you still have any questions. We are more than happy to discuss further.\n\n\n[7] Xie, Sang Michael, et al. \"An explanation of in-context learning as implicit bayesian inference.\" arXiv preprint arXiv:2111.02080 (2021).\n\n[8] Wei, Jerry, et al. \"Larger language models do in-context learning differently.\" arXiv preprint arXiv:2303.03846 (2023)."}
{"claim": "The distinction between the paper's Pelican soup hypothesis and the hypothesis in Sanjeev & Anirudh (2023) is not clearly explained.", "claim_type": "presentation", "paper_id": "aaYBsuGRne", "paper_title": "Understanding In-context Learning with a Pelican Soup Hypothesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "EIVr3uVfUn", "reviewer": "Reviewer_rNJR", "review_text": "Summary: This paper focuses on understanding the in-context learning ability of large language models. The authors propose the Pelican soup hypothesis. It explains the in-context learning ability originating from learning the knowledge via the next token prediction. To support this hypothesis, the authors build a dataset and demonstrate the linkage between linguistic phenomena and in-context learning.\n\nStrengths: This paper provides substantial numerical results to support the proposed hypothesis. The linguistic phenomena analysis is also interesting to the community. In addition, the built dataset may be of independent interest.\n\nWeaknesses: 1. The claim related to the knowledge base needs more clarification. The experiments in [1] demonstrate that input-output mapping is not very important to the ICL. If the label space is correct, LLMs can even implement efficient ICL given wrong mapping. However, this wrong mapping conflicts with the knowledge base. More discussions are needed here.\n\n2. In Section 5.1, some assumptions are presented, but there is a notable absence of justification for these assumptions within the paper. This absence makes it challenging to ascertain the realism of these assumptions.\n3. I would greatly appreciate further elucidation on the distinction between the hypothesis presented in this paper and that discussed in [2]. Specifically, the variance between the \"atomic elements of NLP tasks\" and \"a set of atom concepts\" requires additional clarification.\n\n4. It is advantageous to include more highly relevant works in the related works. For example, besides HMM, implicit Bayesian inference is modeled for ICL in many different data assumptions [3,4,5]. [6] also studies the optimization side of ICL.\n\n[1] Min S, Lyu X, Holtzman A, et al. Rethinking the role of demonstrations: What makes in-context learning work?[J]. arXiv preprint arXiv:2202.12837, 2022.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Jiang H. A latent space theory for emergent abilities in large language models[J]. arXiv preprint arXiv:2304.09960, 2023.\n\n[4] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[5] Wang X, Zhu W, Wang W Y. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning[J]. arXiv preprint arXiv:2301.11916, 2023.\n\n[6] Dai D, Sun Y, Dong L, et al. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers[J]. arXiv preprint arXiv:2212.10559, 2022.\n\nQuestions: Questions are specified in Weakness part.", "labeling_timestamp": "2026-01-11T16:33:13.207698", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors acknowledge and discuss the relationship to Sanjeev & Anirudh (2023), claiming their work can instantiate or complement that theory and explaining differences (e.g., explicit characterization of distribution shifts). However, they do not provide a direct, detailed contrast specifically between the Pelican Soup hypothesis and Sanjeev & Anirudh’s hypothesis, so the reviewer’s request for a clear distinction is only partially addressed.", "evidence": "“We consider this as one of our main contributions, an instantiation for the assumptions made in previous theoretic works, such as [1] and [2].” ; “We think our work could be an instantiation of the ‘atoms’ proposed in [2].” ; “We use Pelican Soup riddles only to motivate the intuition that commonsense plays an important role for ICL. We would not say our hypothesis is *based on* this similarity.”", "combined_author_response": "We appreciate all reviewers' insightful comments.\n\nWe are particularly grateful that the reviewers agreed that our paper has the following strengths:\n\n- The linguistic phenomena analysis is interesting to the community (rNJR, MUGa, 5jds).\n- The Calcutec dataset and the digit-addition task may be useful for future research (rNJR, MUGa, 5jds, hbQa,cdtQ).\n- Provides evidence that even smaller LLMs can perform ICL with artificial/task-agnostic label symbols (MUGa, 5jds).\n- A new formalism for NLP classification tasks (5jds, hbQa, cdtQ).\n- Empirical results supporting the Pelican Soup Hypothesis (rNJR, cdtQ) and provide a potential explanation for in-context learning (hbQa).\n\nWe have revised the paper to address the main concern that Section 3 is not substantiated, which we acknowledge is the confusion caused by the inclarity. We added more elaboration on the linkage between Section 3 and Section 5.1, stressing that Section 3 is to draw intuitions for the assumptions in Section 5.1 and that Section 5.1 is the formal description for the characteristics described in Section 3.\n\nWe also address each reviewer’s concerns below respectively. Please let us know if any aspects are still unclear. We are more than happy to provide additional clarification.\n\n---\n\nWe thank you for the positive feedback. With regard to the weakness:\n\n1. While Section 3 is purely conceptual, we provide rigorous characterization in Section 5.1. We consider this as one of our main contributions, an instantiation for the assumptions made in previous theoretic works, such as [1] and [2]. For [1], our framework constituted with the assumptions we made in Section 5.1 satisfies the requirements of Corollary 4.2, therefore we can use their analysis to get an $O(1/T)$ regret bound ($T$ is the number of examples in the demonstration). For [2], please refer to our reply to reviewer rNJR.\n2. Thanks for the suggestion! We can run more experiments. Could you let us know what experiments you think are important are missing?\n3. We admit that we only focus on classification tasks in this work. However, we would like to stress that even for the classification setting, the mechanism of ICL is not yet clear. Future work may also extend our framework to generation tasks.\n\nWith regard to the questions:\n\n1. By preprocessing the training data for LLMs, we think it’s possible. However, training an LLM requires lots of computational resources. We leave it for future work.\n2. The formalism in Section 2 can handle arbitrarily difficult reasoning tasks as long as there is a logic system that can handle it and the induction searching process is computationally feasible, e.g. not NP-hard. We believe that most human-solvable tasks are in this class.\n3. We think this is an important open question. A possible solution would be augmenting the training data, e.g. [3].\n4. Our conjecture would be that for many commonsense rules, LLMs are trained with many instances for each of them. Because the training data may cover a large portion of common surface forms, even though it’s possible that LLMs are doing no more than pattern matching, they are still able to do “reasoning” following the commonsense rules.  \n\n[1] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Zhou, Wangchunshu, Ronan Le Bras, and Yejin Choi. \"Commonsense Knowledge Transfer for Pre-trained Language Models.\" arXiv preprint arXiv:2306.02388 (2023).\n\n---\n\nThank you for your positive review. \n\nWe would like to point out that producing an environment, with which in-context learning still works, is a practice used in most (if not all) theoretical analyses such as [1] and [2] and even some empirical analyses such as [3]. We consider one of our main contributions to be proposing an environment that is closer to the real-world scenario than previous works.\n\nWith regard to the questions you stated:\n\n- The yes/no questions in a Pelican Soup riddle have a similar function as the demonstrations for in-context learning, because based on the answer of those yes/no questions, the participants are able to figure out the latent story and thus are able to answer other following questions. Note that we use Pelican Soup riddles just to motivate the importance of the role of commonsense.\n- Our argument is that solving these two problems (ICL and modeling coreference) requires similar capabilities. This argument is to draw the intuition for the framework/assumptions in Section 5.1. Surely the two problems are not identical, thus we characterize the difference between these two problems with the distribution shifts discussed in Section 4. To show that LMs may be able to generalize under these distribution shifts, we replicate these distribution shifts in our experiments, as discussed in Section 5.3 and results in Section 5.5 are aligned with our hypothesis.\n- We understand ICL as generating verbalizers based on the meaning of the verbalizers, which is recovered according to the preceding context, so we think it is similar to modeling anaphora. Could you please elaborate more on why you think ICL is more like handling catephora?\n\nPlease let us know if you have any other comments. We are happy to discuss more!\n\n[1] Xie, Sang Michael, et al. \"An Explanation of In-context Learning as Implicit Bayesian Inference.\" International Conference on Learning Representations. 2021.\n\n[2] Hahn, Michael, and Navin Goyal. \"A theory of emergent in-context learning as implicit structure induction.\" arXiv preprint arXiv:2303.07971 (2023).\n\n[3] Chan, Stephanie, et al. \"Data distributional properties drive emergent in-context learning in transformers.\" Advances in Neural Information Processing Systems 35 (2022): 18878-18891.\n\n---\n\nWe thank you for your review. We would like to address the weakness below:\n\n1. Please let us know what are the arguments that you think are not substantiated. For the example you provided, we apologize for not including proper citations. We think that LMs are able to acquire some knowledge about commonsense (though imperfectly) has been widely accepted. Many studies can be found in previous works, e.g. [1][2].\n2. Validating theories with a synthetic dataset is a common practice. We admit that our setting cannot explain domain bias. It may require more assumptions to theoretically explain. We would like to mention that, we think it will make the construction of the toy data even more complicated, which based on your 4th point, is not acceptable. \n3. We justify our assumptions made in Section 5.1 in Section 3. As argued in Section 3.1, we posit that training data usually contain text that has some logical structure and/or follows chronicle order, which can contribute to LMs’ reasoning capability. The dataset Calcutec is a formal abstraction of such structure.\n4. Could you be more specific on what details are missing and how we should improve? We will appreciate your suggestion. The construction of Calcutec is more complicated than the toy data used in previous theoretical works such as [3] because Calcutec relies on milder assumptions and is more realistic. \n5. We apologize that we didn’t make the purpose of Section 3 clearer. We treat Section 3 as the justification/intuition for the abstraction/assumptions/framework we made in Section 5.1. We then instantiate the abstraction/framework with Calcutec and show that LMs trained with it can do in-context learning.\n6. We use Pelican Soup riddles only to motivate the intuition that commonsense plays an important role for ICL. We would not say our hypothesis is *based on* this similarity.\n7. Please let us know if you think any experiment is important but missing. Also, could you elaborate more on the possibility that the models solve the task by learning shortcuts (and maybe also your definition of shortcuts)?\n\nOur answer to the question\n\n1. We think mesa-optimization mainly explains why models of Transformer architecture trained with autoregressive loss have the capacity to model data that involves ICL-like phenomena. However, in works such as [4], they only study on some toy data. It is unclear how their toy data is relevant to natural language data. Our work starts from another direction, aiming to understand what characteristics of training data lead to the ICL ability.\n\n\n[1] West, Peter, et al. \"Symbolic knowledge distillation: from general language models to commonsense models.\" arXiv preprint arXiv:2110.07178 (2021).\n\n[2] Li, Xiang Lorraine, et al. \"A systematic investigation of commonsense knowledge in large language models.\" Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.\n\n[3] Xie, Sang Michael, et al. \"An explanation of in-context learning as implicit bayesian inference.\" arXiv preprint arXiv:2111.02080 (2021).\n\n[4] von Oswald, Johannes, et al. \"Uncovering mesa-optimization algorithms in transformers.\" arXiv preprint arXiv:2309.05858 (2023).\n\n---\n\nThank you for your thoughtful comments. We would like to address the weakness below:\n\n1. In Section 2, we propose a formalism for NLP classification tasks while the main purpose of Section 3 is to justify the assumptions made in Section 5. We see the assumptions in Section 5 as the formal statement/abstraction of our theorem in Section 3. We are sorry that we didn’t describe it more clearly. We will improve this part in our later revision. Please let us know if we need to make more clarifications.\n2. As far as we know, all theoretical analyses on ICL depend on some repetitive nature of prompts or training data, e.g. Xie et al and Hahn & Goyal. One difference in our work is that we focus on the repetitive usage and the consistent meaning of pronouns, which is very realistic in real-world data. In Figure 2, we also show that the model can still do ICL when the task description is more complicated than the meaning of the pronouns seen in the training set, i.e., when the task description is composed of 3 atomic concepts (the green lines for “triple” in Figure 2).\n3. We discuss this point below:\n    - The digit addition task: Please refer to Figure 8 for the distribution of the number of steps in the datasets. For the 5-digit addition task, when the dropping rate is 0.2, there are only 265 (0.265%) training samples where all the intermediate steps are dropped. We think it is reasonable to say the testing samples are out-of-domain in this case. (And GPT-2-sized model can achieve ~80% accuracy with only 265 training samples in the testing domain.) Meanwhile, considering that the real-world data also has a small fraction of text that drops all the reasoning steps, we think this setting is not unrealistic.\n    - Calcutec: Please refer to Figure 5. It shows that even when no steps are dropped in the training set, the model still can do ICL, though the performance is not as good.\n\nQuestion:\n\n- The prompt is as the one in Table 1. We only include the reasoning steps of some (3) randomly selected examples and the premise of the testing input in the prompt. (For the example in Table 1, the testing input is `x55 x76 x84 x99`). We unroll until the first verbalizer is generated. \n\nPlease let us know if you have any other questions. We are more than happy to discuss with you.\n\n---\n\nThank you for your comments. \n\n## With Regard to the Weakness\n\n1. We consider one main contribution of our work to be explaining the ICL ability in the *semantically-unrelated label ICL (SUL-ICL)* setting [8], which is a step forward from the theorem by [7]. Indeed, our work can not explain the random label setting in [1]. However, we would like to point out that as far as we know, there has been no theorem able to explain it. We suggest that explaining it requires making more assumptions on the relationship between the input-label mapping and the domain/topic/theme distribution in the training data.\n2. We are sorry for the confusion. The justification mainly follows the argument in Section 3. We will make the connection clearer in our later revision. For now, please allow us to elaborate it below:\n    - Assumption 1: It follows the efforts made by early linguistics on linguistic formalism and cognitive psychology theories such as language of thought.\n    - Assumption 2: This assumption follows the argument in Section 3.1 that general text is usually organized in a similar way as logical induction processes. We assume that “a paragraph is a traversal of the nodes of the proving tree in the topological ordering” because in general text, statements usually follow some causal and/or chronological order, so it can convince readers or be easy for readers to understand.\n    - Assumption 3: In other words, this assumption assumes that every paragraph in an article mentions the topic of the article.\n    - Assumption 4: In other words, this assumption assumes that some pronouns are used in the article and each pronoun in the article is associated with a single entity.\n    - Assumption 5: This assumption follows the argument in Section 2.\n\n    In our opinion, these assumptions are relatively mild compared with previous theoretical works. We are happy to discuss more if you still have some concerns about the realism of these assumptions.\n3. To make our main response short, please allow us to discuss it below.\n4. Thank you for the list of relevant works. We will include some of them in our later revision (as we have some concerns about the technical soundness of some of them). \n\n\n## Atomic elements of NLP tasks in [2]\n\n\nIf we understand [2] correctly, [2] does not specify what characteristics the “atomic skills” of NLP need to have. We think our work could be an instantiation of the “atoms” proposed in [2]. \n\nIt seems that defining a set of “atomic skills” is not trivial and can’t be done arbitrarily. “Atomic skills” must be defined in a way such that the skills that work for the training data can generalize to the downstream task. To understand why it is non-trivial to define a set of atomic skills, we can probably think about a trivial case where we define the set “atomic skill” as the 26 skills to predict the words starting with A-Z. With this setting, the argument in [2] is greatly simplified. It seems that Theorem 14 only tells us that for most of the letters, the model can do the cloze problems in the training set well if the answer starts with that letter. It doesn’t seem to be guaranteed that the model can do the cloze problems in the testing set well.\n\nOn the other hand, defining the atomic skills with our atomic concepts seems to be more generalizable. Still, we posit it is necessary to discuss the discrepancies between the training set and the downstream task. That’s why in this work we explicitly characterize the distribution shifts in Section 4 and demonstrate that LMs are able to generalize under these distribution shifts in Section 5 and 7. Surely our settings do not fully replicate the real-world data distribution, but we think it still complements some aspects of [2]. \n\nPlease let us know if you still have any questions. We are more than happy to discuss further.\n\n\n[7] Xie, Sang Michael, et al. \"An explanation of in-context learning as implicit bayesian inference.\" arXiv preprint arXiv:2111.02080 (2021).\n\n[8] Wei, Jerry, et al. \"Larger language models do in-context learning differently.\" arXiv preprint arXiv:2303.03846 (2023).", "author_response": "Thank you for your comments. \n\n## With Regard to the Weakness\n\n1. We consider one main contribution of our work to be explaining the ICL ability in the *semantically-unrelated label ICL (SUL-ICL)* setting [8], which is a step forward from the theorem by [7]. Indeed, our work can not explain the random label setting in [1]. However, we would like to point out that as far as we know, there has been no theorem able to explain it. We suggest that explaining it requires making more assumptions on the relationship between the input-label mapping and the domain/topic/theme distribution in the training data.\n2. We are sorry for the confusion. The justification mainly follows the argument in Section 3. We will make the connection clearer in our later revision. For now, please allow us to elaborate it below:\n    - Assumption 1: It follows the efforts made by early linguistics on linguistic formalism and cognitive psychology theories such as language of thought.\n    - Assumption 2: This assumption follows the argument in Section 3.1 that general text is usually organized in a similar way as logical induction processes. We assume that “a paragraph is a traversal of the nodes of the proving tree in the topological ordering” because in general text, statements usually follow some causal and/or chronological order, so it can convince readers or be easy for readers to understand.\n    - Assumption 3: In other words, this assumption assumes that every paragraph in an article mentions the topic of the article.\n    - Assumption 4: In other words, this assumption assumes that some pronouns are used in the article and each pronoun in the article is associated with a single entity.\n    - Assumption 5: This assumption follows the argument in Section 2.\n\n    In our opinion, these assumptions are relatively mild compared with previous theoretical works. We are happy to discuss more if you still have some concerns about the realism of these assumptions.\n3. To make our main response short, please allow us to discuss it below.\n4. Thank you for the list of relevant works. We will include some of them in our later revision (as we have some concerns about the technical soundness of some of them). \n\n\n## Atomic elements of NLP tasks in [2]\n\n\nIf we understand [2] correctly, [2] does not specify what characteristics the “atomic skills” of NLP need to have. We think our work could be an instantiation of the “atoms” proposed in [2]. \n\nIt seems that defining a set of “atomic skills” is not trivial and can’t be done arbitrarily. “Atomic skills” must be defined in a way such that the skills that work for the training data can generalize to the downstream task. To understand why it is non-trivial to define a set of atomic skills, we can probably think about a trivial case where we define the set “atomic skill” as the 26 skills to predict the words starting with A-Z. With this setting, the argument in [2] is greatly simplified. It seems that Theorem 14 only tells us that for most of the letters, the model can do the cloze problems in the training set well if the answer starts with that letter. It doesn’t seem to be guaranteed that the model can do the cloze problems in the testing set well.\n\nOn the other hand, defining the atomic skills with our atomic concepts seems to be more generalizable. Still, we posit it is necessary to discuss the discrepancies between the training set and the downstream task. That’s why in this work we explicitly characterize the distribution shifts in Section 4 and demonstrate that LMs are able to generalize under these distribution shifts in Section 5 and 7. Surely our settings do not fully replicate the real-world data distribution, but we think it still complements some aspects of [2]. \n\nPlease let us know if you still have any questions. We are more than happy to discuss further.\n\n\n[7] Xie, Sang Michael, et al. \"An explanation of in-context learning as implicit bayesian inference.\" arXiv preprint arXiv:2111.02080 (2021).\n\n[8] Wei, Jerry, et al. \"Larger language models do in-context learning differently.\" arXiv preprint arXiv:2303.03846 (2023)."}
{"claim": "The difference between 'atomic elements of NLP tasks' and 'a set of atom concepts' is not adequately defined or exemplified in the manuscript.", "claim_type": "presentation", "paper_id": "aaYBsuGRne", "paper_title": "Understanding In-context Learning with a Pelican Soup Hypothesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "EIVr3uVfUn", "reviewer": "Reviewer_rNJR", "review_text": "Summary: This paper focuses on understanding the in-context learning ability of large language models. The authors propose the Pelican soup hypothesis. It explains the in-context learning ability originating from learning the knowledge via the next token prediction. To support this hypothesis, the authors build a dataset and demonstrate the linkage between linguistic phenomena and in-context learning.\n\nStrengths: This paper provides substantial numerical results to support the proposed hypothesis. The linguistic phenomena analysis is also interesting to the community. In addition, the built dataset may be of independent interest.\n\nWeaknesses: 1. The claim related to the knowledge base needs more clarification. The experiments in [1] demonstrate that input-output mapping is not very important to the ICL. If the label space is correct, LLMs can even implement efficient ICL given wrong mapping. However, this wrong mapping conflicts with the knowledge base. More discussions are needed here.\n\n2. In Section 5.1, some assumptions are presented, but there is a notable absence of justification for these assumptions within the paper. This absence makes it challenging to ascertain the realism of these assumptions.\n3. I would greatly appreciate further elucidation on the distinction between the hypothesis presented in this paper and that discussed in [2]. Specifically, the variance between the \"atomic elements of NLP tasks\" and \"a set of atom concepts\" requires additional clarification.\n\n4. It is advantageous to include more highly relevant works in the related works. For example, besides HMM, implicit Bayesian inference is modeled for ICL in many different data assumptions [3,4,5]. [6] also studies the optimization side of ICL.\n\n[1] Min S, Lyu X, Holtzman A, et al. Rethinking the role of demonstrations: What makes in-context learning work?[J]. arXiv preprint arXiv:2202.12837, 2022.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Jiang H. A latent space theory for emergent abilities in large language models[J]. arXiv preprint arXiv:2304.09960, 2023.\n\n[4] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[5] Wang X, Zhu W, Wang W Y. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning[J]. arXiv preprint arXiv:2301.11916, 2023.\n\n[6] Dai D, Sun Y, Dong L, et al. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers[J]. arXiv preprint arXiv:2212.10559, 2022.\n\nQuestions: Questions are specified in Weakness part.", "labeling_timestamp": "2026-01-11T16:33:19.766708", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors directly discuss the distinction and argue that their 'atomic concepts' can instantiate the 'atomic skills' of prior work, acknowledging the non-triviality of defining atoms and providing reasoning why their formulation is more generalizable. However, they do not supply a concrete, formal definition or worked examples in this response, so the reviewer's concern is only partially resolved.", "evidence": "\"If we understand [2] correctly, [2] does not specify what characteristics the \\\"atomic skills\\\" of NLP need to have. We think our work could be an instantiation of the \\\"atoms\\\" proposed in [2]. ... On the other hand, defining the atomic skills with our atomic concepts seems to be more generalizable. Still, we posit it is necessary to discuss the discrepancies between the training set and the downstream task. That’s why in this work we explicitly characterize the distribution shifts in Section 4 and demonstrate that LMs are able to generalize under these distribution shifts in Section 5 and 7.\"", "combined_author_response": "We appreciate all reviewers' insightful comments.\n\nWe are particularly grateful that the reviewers agreed that our paper has the following strengths:\n\n- The linguistic phenomena analysis is interesting to the community (rNJR, MUGa, 5jds).\n- The Calcutec dataset and the digit-addition task may be useful for future research (rNJR, MUGa, 5jds, hbQa,cdtQ).\n- Provides evidence that even smaller LLMs can perform ICL with artificial/task-agnostic label symbols (MUGa, 5jds).\n- A new formalism for NLP classification tasks (5jds, hbQa, cdtQ).\n- Empirical results supporting the Pelican Soup Hypothesis (rNJR, cdtQ) and provide a potential explanation for in-context learning (hbQa).\n\nWe have revised the paper to address the main concern that Section 3 is not substantiated, which we acknowledge is the confusion caused by the inclarity. We added more elaboration on the linkage between Section 3 and Section 5.1, stressing that Section 3 is to draw intuitions for the assumptions in Section 5.1 and that Section 5.1 is the formal description for the characteristics described in Section 3.\n\nWe also address each reviewer’s concerns below respectively. Please let us know if any aspects are still unclear. We are more than happy to provide additional clarification.\n\n---\n\nWe thank you for the positive feedback. With regard to the weakness:\n\n1. While Section 3 is purely conceptual, we provide rigorous characterization in Section 5.1. We consider this as one of our main contributions, an instantiation for the assumptions made in previous theoretic works, such as [1] and [2]. For [1], our framework constituted with the assumptions we made in Section 5.1 satisfies the requirements of Corollary 4.2, therefore we can use their analysis to get an $O(1/T)$ regret bound ($T$ is the number of examples in the demonstration). For [2], please refer to our reply to reviewer rNJR.\n2. Thanks for the suggestion! We can run more experiments. Could you let us know what experiments you think are important are missing?\n3. We admit that we only focus on classification tasks in this work. However, we would like to stress that even for the classification setting, the mechanism of ICL is not yet clear. Future work may also extend our framework to generation tasks.\n\nWith regard to the questions:\n\n1. By preprocessing the training data for LLMs, we think it’s possible. However, training an LLM requires lots of computational resources. We leave it for future work.\n2. The formalism in Section 2 can handle arbitrarily difficult reasoning tasks as long as there is a logic system that can handle it and the induction searching process is computationally feasible, e.g. not NP-hard. We believe that most human-solvable tasks are in this class.\n3. We think this is an important open question. A possible solution would be augmenting the training data, e.g. [3].\n4. Our conjecture would be that for many commonsense rules, LLMs are trained with many instances for each of them. Because the training data may cover a large portion of common surface forms, even though it’s possible that LLMs are doing no more than pattern matching, they are still able to do “reasoning” following the commonsense rules.  \n\n[1] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Zhou, Wangchunshu, Ronan Le Bras, and Yejin Choi. \"Commonsense Knowledge Transfer for Pre-trained Language Models.\" arXiv preprint arXiv:2306.02388 (2023).\n\n---\n\nThank you for your positive review. \n\nWe would like to point out that producing an environment, with which in-context learning still works, is a practice used in most (if not all) theoretical analyses such as [1] and [2] and even some empirical analyses such as [3]. We consider one of our main contributions to be proposing an environment that is closer to the real-world scenario than previous works.\n\nWith regard to the questions you stated:\n\n- The yes/no questions in a Pelican Soup riddle have a similar function as the demonstrations for in-context learning, because based on the answer of those yes/no questions, the participants are able to figure out the latent story and thus are able to answer other following questions. Note that we use Pelican Soup riddles just to motivate the importance of the role of commonsense.\n- Our argument is that solving these two problems (ICL and modeling coreference) requires similar capabilities. This argument is to draw the intuition for the framework/assumptions in Section 5.1. Surely the two problems are not identical, thus we characterize the difference between these two problems with the distribution shifts discussed in Section 4. To show that LMs may be able to generalize under these distribution shifts, we replicate these distribution shifts in our experiments, as discussed in Section 5.3 and results in Section 5.5 are aligned with our hypothesis.\n- We understand ICL as generating verbalizers based on the meaning of the verbalizers, which is recovered according to the preceding context, so we think it is similar to modeling anaphora. Could you please elaborate more on why you think ICL is more like handling catephora?\n\nPlease let us know if you have any other comments. We are happy to discuss more!\n\n[1] Xie, Sang Michael, et al. \"An Explanation of In-context Learning as Implicit Bayesian Inference.\" International Conference on Learning Representations. 2021.\n\n[2] Hahn, Michael, and Navin Goyal. \"A theory of emergent in-context learning as implicit structure induction.\" arXiv preprint arXiv:2303.07971 (2023).\n\n[3] Chan, Stephanie, et al. \"Data distributional properties drive emergent in-context learning in transformers.\" Advances in Neural Information Processing Systems 35 (2022): 18878-18891.\n\n---\n\nWe thank you for your review. We would like to address the weakness below:\n\n1. Please let us know what are the arguments that you think are not substantiated. For the example you provided, we apologize for not including proper citations. We think that LMs are able to acquire some knowledge about commonsense (though imperfectly) has been widely accepted. Many studies can be found in previous works, e.g. [1][2].\n2. Validating theories with a synthetic dataset is a common practice. We admit that our setting cannot explain domain bias. It may require more assumptions to theoretically explain. We would like to mention that, we think it will make the construction of the toy data even more complicated, which based on your 4th point, is not acceptable. \n3. We justify our assumptions made in Section 5.1 in Section 3. As argued in Section 3.1, we posit that training data usually contain text that has some logical structure and/or follows chronicle order, which can contribute to LMs’ reasoning capability. The dataset Calcutec is a formal abstraction of such structure.\n4. Could you be more specific on what details are missing and how we should improve? We will appreciate your suggestion. The construction of Calcutec is more complicated than the toy data used in previous theoretical works such as [3] because Calcutec relies on milder assumptions and is more realistic. \n5. We apologize that we didn’t make the purpose of Section 3 clearer. We treat Section 3 as the justification/intuition for the abstraction/assumptions/framework we made in Section 5.1. We then instantiate the abstraction/framework with Calcutec and show that LMs trained with it can do in-context learning.\n6. We use Pelican Soup riddles only to motivate the intuition that commonsense plays an important role for ICL. We would not say our hypothesis is *based on* this similarity.\n7. Please let us know if you think any experiment is important but missing. Also, could you elaborate more on the possibility that the models solve the task by learning shortcuts (and maybe also your definition of shortcuts)?\n\nOur answer to the question\n\n1. We think mesa-optimization mainly explains why models of Transformer architecture trained with autoregressive loss have the capacity to model data that involves ICL-like phenomena. However, in works such as [4], they only study on some toy data. It is unclear how their toy data is relevant to natural language data. Our work starts from another direction, aiming to understand what characteristics of training data lead to the ICL ability.\n\n\n[1] West, Peter, et al. \"Symbolic knowledge distillation: from general language models to commonsense models.\" arXiv preprint arXiv:2110.07178 (2021).\n\n[2] Li, Xiang Lorraine, et al. \"A systematic investigation of commonsense knowledge in large language models.\" Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.\n\n[3] Xie, Sang Michael, et al. \"An explanation of in-context learning as implicit bayesian inference.\" arXiv preprint arXiv:2111.02080 (2021).\n\n[4] von Oswald, Johannes, et al. \"Uncovering mesa-optimization algorithms in transformers.\" arXiv preprint arXiv:2309.05858 (2023).\n\n---\n\nThank you for your thoughtful comments. We would like to address the weakness below:\n\n1. In Section 2, we propose a formalism for NLP classification tasks while the main purpose of Section 3 is to justify the assumptions made in Section 5. We see the assumptions in Section 5 as the formal statement/abstraction of our theorem in Section 3. We are sorry that we didn’t describe it more clearly. We will improve this part in our later revision. Please let us know if we need to make more clarifications.\n2. As far as we know, all theoretical analyses on ICL depend on some repetitive nature of prompts or training data, e.g. Xie et al and Hahn & Goyal. One difference in our work is that we focus on the repetitive usage and the consistent meaning of pronouns, which is very realistic in real-world data. In Figure 2, we also show that the model can still do ICL when the task description is more complicated than the meaning of the pronouns seen in the training set, i.e., when the task description is composed of 3 atomic concepts (the green lines for “triple” in Figure 2).\n3. We discuss this point below:\n    - The digit addition task: Please refer to Figure 8 for the distribution of the number of steps in the datasets. For the 5-digit addition task, when the dropping rate is 0.2, there are only 265 (0.265%) training samples where all the intermediate steps are dropped. We think it is reasonable to say the testing samples are out-of-domain in this case. (And GPT-2-sized model can achieve ~80% accuracy with only 265 training samples in the testing domain.) Meanwhile, considering that the real-world data also has a small fraction of text that drops all the reasoning steps, we think this setting is not unrealistic.\n    - Calcutec: Please refer to Figure 5. It shows that even when no steps are dropped in the training set, the model still can do ICL, though the performance is not as good.\n\nQuestion:\n\n- The prompt is as the one in Table 1. We only include the reasoning steps of some (3) randomly selected examples and the premise of the testing input in the prompt. (For the example in Table 1, the testing input is `x55 x76 x84 x99`). We unroll until the first verbalizer is generated. \n\nPlease let us know if you have any other questions. We are more than happy to discuss with you.\n\n---\n\nThank you for your comments. \n\n## With Regard to the Weakness\n\n1. We consider one main contribution of our work to be explaining the ICL ability in the *semantically-unrelated label ICL (SUL-ICL)* setting [8], which is a step forward from the theorem by [7]. Indeed, our work can not explain the random label setting in [1]. However, we would like to point out that as far as we know, there has been no theorem able to explain it. We suggest that explaining it requires making more assumptions on the relationship between the input-label mapping and the domain/topic/theme distribution in the training data.\n2. We are sorry for the confusion. The justification mainly follows the argument in Section 3. We will make the connection clearer in our later revision. For now, please allow us to elaborate it below:\n    - Assumption 1: It follows the efforts made by early linguistics on linguistic formalism and cognitive psychology theories such as language of thought.\n    - Assumption 2: This assumption follows the argument in Section 3.1 that general text is usually organized in a similar way as logical induction processes. We assume that “a paragraph is a traversal of the nodes of the proving tree in the topological ordering” because in general text, statements usually follow some causal and/or chronological order, so it can convince readers or be easy for readers to understand.\n    - Assumption 3: In other words, this assumption assumes that every paragraph in an article mentions the topic of the article.\n    - Assumption 4: In other words, this assumption assumes that some pronouns are used in the article and each pronoun in the article is associated with a single entity.\n    - Assumption 5: This assumption follows the argument in Section 2.\n\n    In our opinion, these assumptions are relatively mild compared with previous theoretical works. We are happy to discuss more if you still have some concerns about the realism of these assumptions.\n3. To make our main response short, please allow us to discuss it below.\n4. Thank you for the list of relevant works. We will include some of them in our later revision (as we have some concerns about the technical soundness of some of them). \n\n\n## Atomic elements of NLP tasks in [2]\n\n\nIf we understand [2] correctly, [2] does not specify what characteristics the “atomic skills” of NLP need to have. We think our work could be an instantiation of the “atoms” proposed in [2]. \n\nIt seems that defining a set of “atomic skills” is not trivial and can’t be done arbitrarily. “Atomic skills” must be defined in a way such that the skills that work for the training data can generalize to the downstream task. To understand why it is non-trivial to define a set of atomic skills, we can probably think about a trivial case where we define the set “atomic skill” as the 26 skills to predict the words starting with A-Z. With this setting, the argument in [2] is greatly simplified. It seems that Theorem 14 only tells us that for most of the letters, the model can do the cloze problems in the training set well if the answer starts with that letter. It doesn’t seem to be guaranteed that the model can do the cloze problems in the testing set well.\n\nOn the other hand, defining the atomic skills with our atomic concepts seems to be more generalizable. Still, we posit it is necessary to discuss the discrepancies between the training set and the downstream task. That’s why in this work we explicitly characterize the distribution shifts in Section 4 and demonstrate that LMs are able to generalize under these distribution shifts in Section 5 and 7. Surely our settings do not fully replicate the real-world data distribution, but we think it still complements some aspects of [2]. \n\nPlease let us know if you still have any questions. We are more than happy to discuss further.\n\n\n[7] Xie, Sang Michael, et al. \"An explanation of in-context learning as implicit bayesian inference.\" arXiv preprint arXiv:2111.02080 (2021).\n\n[8] Wei, Jerry, et al. \"Larger language models do in-context learning differently.\" arXiv preprint arXiv:2303.03846 (2023).", "author_response": "Thank you for your comments. \n\n## With Regard to the Weakness\n\n1. We consider one main contribution of our work to be explaining the ICL ability in the *semantically-unrelated label ICL (SUL-ICL)* setting [8], which is a step forward from the theorem by [7]. Indeed, our work can not explain the random label setting in [1]. However, we would like to point out that as far as we know, there has been no theorem able to explain it. We suggest that explaining it requires making more assumptions on the relationship between the input-label mapping and the domain/topic/theme distribution in the training data.\n2. We are sorry for the confusion. The justification mainly follows the argument in Section 3. We will make the connection clearer in our later revision. For now, please allow us to elaborate it below:\n    - Assumption 1: It follows the efforts made by early linguistics on linguistic formalism and cognitive psychology theories such as language of thought.\n    - Assumption 2: This assumption follows the argument in Section 3.1 that general text is usually organized in a similar way as logical induction processes. We assume that “a paragraph is a traversal of the nodes of the proving tree in the topological ordering” because in general text, statements usually follow some causal and/or chronological order, so it can convince readers or be easy for readers to understand.\n    - Assumption 3: In other words, this assumption assumes that every paragraph in an article mentions the topic of the article.\n    - Assumption 4: In other words, this assumption assumes that some pronouns are used in the article and each pronoun in the article is associated with a single entity.\n    - Assumption 5: This assumption follows the argument in Section 2.\n\n    In our opinion, these assumptions are relatively mild compared with previous theoretical works. We are happy to discuss more if you still have some concerns about the realism of these assumptions.\n3. To make our main response short, please allow us to discuss it below.\n4. Thank you for the list of relevant works. We will include some of them in our later revision (as we have some concerns about the technical soundness of some of them). \n\n\n## Atomic elements of NLP tasks in [2]\n\n\nIf we understand [2] correctly, [2] does not specify what characteristics the “atomic skills” of NLP need to have. We think our work could be an instantiation of the “atoms” proposed in [2]. \n\nIt seems that defining a set of “atomic skills” is not trivial and can’t be done arbitrarily. “Atomic skills” must be defined in a way such that the skills that work for the training data can generalize to the downstream task. To understand why it is non-trivial to define a set of atomic skills, we can probably think about a trivial case where we define the set “atomic skill” as the 26 skills to predict the words starting with A-Z. With this setting, the argument in [2] is greatly simplified. It seems that Theorem 14 only tells us that for most of the letters, the model can do the cloze problems in the training set well if the answer starts with that letter. It doesn’t seem to be guaranteed that the model can do the cloze problems in the testing set well.\n\nOn the other hand, defining the atomic skills with our atomic concepts seems to be more generalizable. Still, we posit it is necessary to discuss the discrepancies between the training set and the downstream task. That’s why in this work we explicitly characterize the distribution shifts in Section 4 and demonstrate that LMs are able to generalize under these distribution shifts in Section 5 and 7. Surely our settings do not fully replicate the real-world data distribution, but we think it still complements some aspects of [2]. \n\nPlease let us know if you still have any questions. We are more than happy to discuss further.\n\n\n[7] Xie, Sang Michael, et al. \"An explanation of in-context learning as implicit bayesian inference.\" arXiv preprint arXiv:2111.02080 (2021).\n\n[8] Wei, Jerry, et al. \"Larger language models do in-context learning differently.\" arXiv preprint arXiv:2303.03846 (2023)."}
{"claim": "The related-work section omits several relevant models of implicit Bayesian inference for ICL, including Jiang 2023 and Zhang et al. 2023.", "claim_type": "presentation", "paper_id": "aaYBsuGRne", "paper_title": "Understanding In-context Learning with a Pelican Soup Hypothesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "EIVr3uVfUn", "reviewer": "Reviewer_rNJR", "review_text": "Summary: This paper focuses on understanding the in-context learning ability of large language models. The authors propose the Pelican soup hypothesis. It explains the in-context learning ability originating from learning the knowledge via the next token prediction. To support this hypothesis, the authors build a dataset and demonstrate the linkage between linguistic phenomena and in-context learning.\n\nStrengths: This paper provides substantial numerical results to support the proposed hypothesis. The linguistic phenomena analysis is also interesting to the community. In addition, the built dataset may be of independent interest.\n\nWeaknesses: 1. The claim related to the knowledge base needs more clarification. The experiments in [1] demonstrate that input-output mapping is not very important to the ICL. If the label space is correct, LLMs can even implement efficient ICL given wrong mapping. However, this wrong mapping conflicts with the knowledge base. More discussions are needed here.\n\n2. In Section 5.1, some assumptions are presented, but there is a notable absence of justification for these assumptions within the paper. This absence makes it challenging to ascertain the realism of these assumptions.\n3. I would greatly appreciate further elucidation on the distinction between the hypothesis presented in this paper and that discussed in [2]. Specifically, the variance between the \"atomic elements of NLP tasks\" and \"a set of atom concepts\" requires additional clarification.\n\n4. It is advantageous to include more highly relevant works in the related works. For example, besides HMM, implicit Bayesian inference is modeled for ICL in many different data assumptions [3,4,5]. [6] also studies the optimization side of ICL.\n\n[1] Min S, Lyu X, Holtzman A, et al. Rethinking the role of demonstrations: What makes in-context learning work?[J]. arXiv preprint arXiv:2202.12837, 2022.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Jiang H. A latent space theory for emergent abilities in large language models[J]. arXiv preprint arXiv:2304.09960, 2023.\n\n[4] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[5] Wang X, Zhu W, Wang W Y. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning[J]. arXiv preprint arXiv:2301.11916, 2023.\n\n[6] Dai D, Sun Y, Dong L, et al. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers[J]. arXiv preprint arXiv:2212.10559, 2022.\n\nQuestions: Questions are specified in Weakness part.", "labeling_timestamp": "2026-01-11T16:33:39.057531", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The authors explicitly cite and discuss Zhang et al. (2023) and relate their framework to it, and they state they will include additional relevant works in a revision. However, they do not explicitly mention Jiang 2023 by name nor explicitly confirm inclusion of that specific work, so the reviewer's claim is only partially addressed.", "evidence": "For [1], our framework constituted with the assumptions we made in Section 5.1 satisfies the requirements of Corollary 4.2, therefore we can use their analysis to get an $O(1/T)$ regret bound ($T$ is the number of examples in the demonstration).\n\n[1] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\nThank you for the list of relevant works. We will include some of them in our later revision (as we have some concerns about the technical soundness of some of them).", "combined_author_response": "We appreciate all reviewers' insightful comments.\n\nWe are particularly grateful that the reviewers agreed that our paper has the following strengths:\n\n- The linguistic phenomena analysis is interesting to the community (rNJR, MUGa, 5jds).\n- The Calcutec dataset and the digit-addition task may be useful for future research (rNJR, MUGa, 5jds, hbQa,cdtQ).\n- Provides evidence that even smaller LLMs can perform ICL with artificial/task-agnostic label symbols (MUGa, 5jds).\n- A new formalism for NLP classification tasks (5jds, hbQa, cdtQ).\n- Empirical results supporting the Pelican Soup Hypothesis (rNJR, cdtQ) and provide a potential explanation for in-context learning (hbQa).\n\nWe have revised the paper to address the main concern that Section 3 is not substantiated, which we acknowledge is the confusion caused by the inclarity. We added more elaboration on the linkage between Section 3 and Section 5.1, stressing that Section 3 is to draw intuitions for the assumptions in Section 5.1 and that Section 5.1 is the formal description for the characteristics described in Section 3.\n\nWe also address each reviewer’s concerns below respectively. Please let us know if any aspects are still unclear. We are more than happy to provide additional clarification.\n\n---\n\nWe thank you for the positive feedback. With regard to the weakness:\n\n1. While Section 3 is purely conceptual, we provide rigorous characterization in Section 5.1. We consider this as one of our main contributions, an instantiation for the assumptions made in previous theoretic works, such as [1] and [2]. For [1], our framework constituted with the assumptions we made in Section 5.1 satisfies the requirements of Corollary 4.2, therefore we can use their analysis to get an $O(1/T)$ regret bound ($T$ is the number of examples in the demonstration). For [2], please refer to our reply to reviewer rNJR.\n2. Thanks for the suggestion! We can run more experiments. Could you let us know what experiments you think are important are missing?\n3. We admit that we only focus on classification tasks in this work. However, we would like to stress that even for the classification setting, the mechanism of ICL is not yet clear. Future work may also extend our framework to generation tasks.\n\nWith regard to the questions:\n\n1. By preprocessing the training data for LLMs, we think it’s possible. However, training an LLM requires lots of computational resources. We leave it for future work.\n2. The formalism in Section 2 can handle arbitrarily difficult reasoning tasks as long as there is a logic system that can handle it and the induction searching process is computationally feasible, e.g. not NP-hard. We believe that most human-solvable tasks are in this class.\n3. We think this is an important open question. A possible solution would be augmenting the training data, e.g. [3].\n4. Our conjecture would be that for many commonsense rules, LLMs are trained with many instances for each of them. Because the training data may cover a large portion of common surface forms, even though it’s possible that LLMs are doing no more than pattern matching, they are still able to do “reasoning” following the commonsense rules.  \n\n[1] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Zhou, Wangchunshu, Ronan Le Bras, and Yejin Choi. \"Commonsense Knowledge Transfer for Pre-trained Language Models.\" arXiv preprint arXiv:2306.02388 (2023).\n\n---\n\nThank you for your positive review. \n\nWe would like to point out that producing an environment, with which in-context learning still works, is a practice used in most (if not all) theoretical analyses such as [1] and [2] and even some empirical analyses such as [3]. We consider one of our main contributions to be proposing an environment that is closer to the real-world scenario than previous works.\n\nWith regard to the questions you stated:\n\n- The yes/no questions in a Pelican Soup riddle have a similar function as the demonstrations for in-context learning, because based on the answer of those yes/no questions, the participants are able to figure out the latent story and thus are able to answer other following questions. Note that we use Pelican Soup riddles just to motivate the importance of the role of commonsense.\n- Our argument is that solving these two problems (ICL and modeling coreference) requires similar capabilities. This argument is to draw the intuition for the framework/assumptions in Section 5.1. Surely the two problems are not identical, thus we characterize the difference between these two problems with the distribution shifts discussed in Section 4. To show that LMs may be able to generalize under these distribution shifts, we replicate these distribution shifts in our experiments, as discussed in Section 5.3 and results in Section 5.5 are aligned with our hypothesis.\n- We understand ICL as generating verbalizers based on the meaning of the verbalizers, which is recovered according to the preceding context, so we think it is similar to modeling anaphora. Could you please elaborate more on why you think ICL is more like handling catephora?\n\nPlease let us know if you have any other comments. We are happy to discuss more!\n\n[1] Xie, Sang Michael, et al. \"An Explanation of In-context Learning as Implicit Bayesian Inference.\" International Conference on Learning Representations. 2021.\n\n[2] Hahn, Michael, and Navin Goyal. \"A theory of emergent in-context learning as implicit structure induction.\" arXiv preprint arXiv:2303.07971 (2023).\n\n[3] Chan, Stephanie, et al. \"Data distributional properties drive emergent in-context learning in transformers.\" Advances in Neural Information Processing Systems 35 (2022): 18878-18891.\n\n---\n\nWe thank you for your review. We would like to address the weakness below:\n\n1. Please let us know what are the arguments that you think are not substantiated. For the example you provided, we apologize for not including proper citations. We think that LMs are able to acquire some knowledge about commonsense (though imperfectly) has been widely accepted. Many studies can be found in previous works, e.g. [1][2].\n2. Validating theories with a synthetic dataset is a common practice. We admit that our setting cannot explain domain bias. It may require more assumptions to theoretically explain. We would like to mention that, we think it will make the construction of the toy data even more complicated, which based on your 4th point, is not acceptable. \n3. We justify our assumptions made in Section 5.1 in Section 3. As argued in Section 3.1, we posit that training data usually contain text that has some logical structure and/or follows chronicle order, which can contribute to LMs’ reasoning capability. The dataset Calcutec is a formal abstraction of such structure.\n4. Could you be more specific on what details are missing and how we should improve? We will appreciate your suggestion. The construction of Calcutec is more complicated than the toy data used in previous theoretical works such as [3] because Calcutec relies on milder assumptions and is more realistic. \n5. We apologize that we didn’t make the purpose of Section 3 clearer. We treat Section 3 as the justification/intuition for the abstraction/assumptions/framework we made in Section 5.1. We then instantiate the abstraction/framework with Calcutec and show that LMs trained with it can do in-context learning.\n6. We use Pelican Soup riddles only to motivate the intuition that commonsense plays an important role for ICL. We would not say our hypothesis is *based on* this similarity.\n7. Please let us know if you think any experiment is important but missing. Also, could you elaborate more on the possibility that the models solve the task by learning shortcuts (and maybe also your definition of shortcuts)?\n\nOur answer to the question\n\n1. We think mesa-optimization mainly explains why models of Transformer architecture trained with autoregressive loss have the capacity to model data that involves ICL-like phenomena. However, in works such as [4], they only study on some toy data. It is unclear how their toy data is relevant to natural language data. Our work starts from another direction, aiming to understand what characteristics of training data lead to the ICL ability.\n\n\n[1] West, Peter, et al. \"Symbolic knowledge distillation: from general language models to commonsense models.\" arXiv preprint arXiv:2110.07178 (2021).\n\n[2] Li, Xiang Lorraine, et al. \"A systematic investigation of commonsense knowledge in large language models.\" Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.\n\n[3] Xie, Sang Michael, et al. \"An explanation of in-context learning as implicit bayesian inference.\" arXiv preprint arXiv:2111.02080 (2021).\n\n[4] von Oswald, Johannes, et al. \"Uncovering mesa-optimization algorithms in transformers.\" arXiv preprint arXiv:2309.05858 (2023).\n\n---\n\nThank you for your thoughtful comments. We would like to address the weakness below:\n\n1. In Section 2, we propose a formalism for NLP classification tasks while the main purpose of Section 3 is to justify the assumptions made in Section 5. We see the assumptions in Section 5 as the formal statement/abstraction of our theorem in Section 3. We are sorry that we didn’t describe it more clearly. We will improve this part in our later revision. Please let us know if we need to make more clarifications.\n2. As far as we know, all theoretical analyses on ICL depend on some repetitive nature of prompts or training data, e.g. Xie et al and Hahn & Goyal. One difference in our work is that we focus on the repetitive usage and the consistent meaning of pronouns, which is very realistic in real-world data. In Figure 2, we also show that the model can still do ICL when the task description is more complicated than the meaning of the pronouns seen in the training set, i.e., when the task description is composed of 3 atomic concepts (the green lines for “triple” in Figure 2).\n3. We discuss this point below:\n    - The digit addition task: Please refer to Figure 8 for the distribution of the number of steps in the datasets. For the 5-digit addition task, when the dropping rate is 0.2, there are only 265 (0.265%) training samples where all the intermediate steps are dropped. We think it is reasonable to say the testing samples are out-of-domain in this case. (And GPT-2-sized model can achieve ~80% accuracy with only 265 training samples in the testing domain.) Meanwhile, considering that the real-world data also has a small fraction of text that drops all the reasoning steps, we think this setting is not unrealistic.\n    - Calcutec: Please refer to Figure 5. It shows that even when no steps are dropped in the training set, the model still can do ICL, though the performance is not as good.\n\nQuestion:\n\n- The prompt is as the one in Table 1. We only include the reasoning steps of some (3) randomly selected examples and the premise of the testing input in the prompt. (For the example in Table 1, the testing input is `x55 x76 x84 x99`). We unroll until the first verbalizer is generated. \n\nPlease let us know if you have any other questions. We are more than happy to discuss with you.\n\n---\n\nThank you for your comments. \n\n## With Regard to the Weakness\n\n1. We consider one main contribution of our work to be explaining the ICL ability in the *semantically-unrelated label ICL (SUL-ICL)* setting [8], which is a step forward from the theorem by [7]. Indeed, our work can not explain the random label setting in [1]. However, we would like to point out that as far as we know, there has been no theorem able to explain it. We suggest that explaining it requires making more assumptions on the relationship between the input-label mapping and the domain/topic/theme distribution in the training data.\n2. We are sorry for the confusion. The justification mainly follows the argument in Section 3. We will make the connection clearer in our later revision. For now, please allow us to elaborate it below:\n    - Assumption 1: It follows the efforts made by early linguistics on linguistic formalism and cognitive psychology theories such as language of thought.\n    - Assumption 2: This assumption follows the argument in Section 3.1 that general text is usually organized in a similar way as logical induction processes. We assume that “a paragraph is a traversal of the nodes of the proving tree in the topological ordering” because in general text, statements usually follow some causal and/or chronological order, so it can convince readers or be easy for readers to understand.\n    - Assumption 3: In other words, this assumption assumes that every paragraph in an article mentions the topic of the article.\n    - Assumption 4: In other words, this assumption assumes that some pronouns are used in the article and each pronoun in the article is associated with a single entity.\n    - Assumption 5: This assumption follows the argument in Section 2.\n\n    In our opinion, these assumptions are relatively mild compared with previous theoretical works. We are happy to discuss more if you still have some concerns about the realism of these assumptions.\n3. To make our main response short, please allow us to discuss it below.\n4. Thank you for the list of relevant works. We will include some of them in our later revision (as we have some concerns about the technical soundness of some of them). \n\n\n## Atomic elements of NLP tasks in [2]\n\n\nIf we understand [2] correctly, [2] does not specify what characteristics the “atomic skills” of NLP need to have. We think our work could be an instantiation of the “atoms” proposed in [2]. \n\nIt seems that defining a set of “atomic skills” is not trivial and can’t be done arbitrarily. “Atomic skills” must be defined in a way such that the skills that work for the training data can generalize to the downstream task. To understand why it is non-trivial to define a set of atomic skills, we can probably think about a trivial case where we define the set “atomic skill” as the 26 skills to predict the words starting with A-Z. With this setting, the argument in [2] is greatly simplified. It seems that Theorem 14 only tells us that for most of the letters, the model can do the cloze problems in the training set well if the answer starts with that letter. It doesn’t seem to be guaranteed that the model can do the cloze problems in the testing set well.\n\nOn the other hand, defining the atomic skills with our atomic concepts seems to be more generalizable. Still, we posit it is necessary to discuss the discrepancies between the training set and the downstream task. That’s why in this work we explicitly characterize the distribution shifts in Section 4 and demonstrate that LMs are able to generalize under these distribution shifts in Section 5 and 7. Surely our settings do not fully replicate the real-world data distribution, but we think it still complements some aspects of [2]. \n\nPlease let us know if you still have any questions. We are more than happy to discuss further.\n\n\n[7] Xie, Sang Michael, et al. \"An explanation of in-context learning as implicit bayesian inference.\" arXiv preprint arXiv:2111.02080 (2021).\n\n[8] Wei, Jerry, et al. \"Larger language models do in-context learning differently.\" arXiv preprint arXiv:2303.03846 (2023).", "author_response": "Thank you for your comments. \n\n## With Regard to the Weakness\n\n1. We consider one main contribution of our work to be explaining the ICL ability in the *semantically-unrelated label ICL (SUL-ICL)* setting [8], which is a step forward from the theorem by [7]. Indeed, our work can not explain the random label setting in [1]. However, we would like to point out that as far as we know, there has been no theorem able to explain it. We suggest that explaining it requires making more assumptions on the relationship between the input-label mapping and the domain/topic/theme distribution in the training data.\n2. We are sorry for the confusion. The justification mainly follows the argument in Section 3. We will make the connection clearer in our later revision. For now, please allow us to elaborate it below:\n    - Assumption 1: It follows the efforts made by early linguistics on linguistic formalism and cognitive psychology theories such as language of thought.\n    - Assumption 2: This assumption follows the argument in Section 3.1 that general text is usually organized in a similar way as logical induction processes. We assume that “a paragraph is a traversal of the nodes of the proving tree in the topological ordering” because in general text, statements usually follow some causal and/or chronological order, so it can convince readers or be easy for readers to understand.\n    - Assumption 3: In other words, this assumption assumes that every paragraph in an article mentions the topic of the article.\n    - Assumption 4: In other words, this assumption assumes that some pronouns are used in the article and each pronoun in the article is associated with a single entity.\n    - Assumption 5: This assumption follows the argument in Section 2.\n\n    In our opinion, these assumptions are relatively mild compared with previous theoretical works. We are happy to discuss more if you still have some concerns about the realism of these assumptions.\n3. To make our main response short, please allow us to discuss it below.\n4. Thank you for the list of relevant works. We will include some of them in our later revision (as we have some concerns about the technical soundness of some of them). \n\n\n## Atomic elements of NLP tasks in [2]\n\n\nIf we understand [2] correctly, [2] does not specify what characteristics the “atomic skills” of NLP need to have. We think our work could be an instantiation of the “atoms” proposed in [2]. \n\nIt seems that defining a set of “atomic skills” is not trivial and can’t be done arbitrarily. “Atomic skills” must be defined in a way such that the skills that work for the training data can generalize to the downstream task. To understand why it is non-trivial to define a set of atomic skills, we can probably think about a trivial case where we define the set “atomic skill” as the 26 skills to predict the words starting with A-Z. With this setting, the argument in [2] is greatly simplified. It seems that Theorem 14 only tells us that for most of the letters, the model can do the cloze problems in the training set well if the answer starts with that letter. It doesn’t seem to be guaranteed that the model can do the cloze problems in the testing set well.\n\nOn the other hand, defining the atomic skills with our atomic concepts seems to be more generalizable. Still, we posit it is necessary to discuss the discrepancies between the training set and the downstream task. That’s why in this work we explicitly characterize the distribution shifts in Section 4 and demonstrate that LMs are able to generalize under these distribution shifts in Section 5 and 7. Surely our settings do not fully replicate the real-world data distribution, but we think it still complements some aspects of [2]. \n\nPlease let us know if you still have any questions. We are more than happy to discuss further.\n\n\n[7] Xie, Sang Michael, et al. \"An explanation of in-context learning as implicit bayesian inference.\" arXiv preprint arXiv:2111.02080 (2021).\n\n[8] Wei, Jerry, et al. \"Larger language models do in-context learning differently.\" arXiv preprint arXiv:2303.03846 (2023)."}
{"claim": "The paper does not cite or discuss optimization-based explanations for ICL, such as work showing language models perform implicit gradient descent (e.g., Dai et al. 2022).", "claim_type": "subjective", "paper_id": "aaYBsuGRne", "paper_title": "Understanding In-context Learning with a Pelican Soup Hypothesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "EIVr3uVfUn", "reviewer": "Reviewer_rNJR", "review_text": "Summary: This paper focuses on understanding the in-context learning ability of large language models. The authors propose the Pelican soup hypothesis. It explains the in-context learning ability originating from learning the knowledge via the next token prediction. To support this hypothesis, the authors build a dataset and demonstrate the linkage between linguistic phenomena and in-context learning.\n\nStrengths: This paper provides substantial numerical results to support the proposed hypothesis. The linguistic phenomena analysis is also interesting to the community. In addition, the built dataset may be of independent interest.\n\nWeaknesses: 1. The claim related to the knowledge base needs more clarification. The experiments in [1] demonstrate that input-output mapping is not very important to the ICL. If the label space is correct, LLMs can even implement efficient ICL given wrong mapping. However, this wrong mapping conflicts with the knowledge base. More discussions are needed here.\n\n2. In Section 5.1, some assumptions are presented, but there is a notable absence of justification for these assumptions within the paper. This absence makes it challenging to ascertain the realism of these assumptions.\n3. I would greatly appreciate further elucidation on the distinction between the hypothesis presented in this paper and that discussed in [2]. Specifically, the variance between the \"atomic elements of NLP tasks\" and \"a set of atom concepts\" requires additional clarification.\n\n4. It is advantageous to include more highly relevant works in the related works. For example, besides HMM, implicit Bayesian inference is modeled for ICL in many different data assumptions [3,4,5]. [6] also studies the optimization side of ICL.\n\n[1] Min S, Lyu X, Holtzman A, et al. Rethinking the role of demonstrations: What makes in-context learning work?[J]. arXiv preprint arXiv:2202.12837, 2022.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Jiang H. A latent space theory for emergent abilities in large language models[J]. arXiv preprint arXiv:2304.09960, 2023.\n\n[4] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[5] Wang X, Zhu W, Wang W Y. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning[J]. arXiv preprint arXiv:2301.11916, 2023.\n\n[6] Dai D, Sun Y, Dong L, et al. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers[J]. arXiv preprint arXiv:2212.10559, 2022.\n\nQuestions: Questions are specified in Weakness part.", "labeling_timestamp": "2026-01-11T16:33:43.227780", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "Authors acknowledge and discuss optimization/mesa-optimization explanations, apologize for missing citations, and say they will include relevant works in revision, but they do not fully concede the reviewer's point and argue their approach is different (focusing on training-data characteristics) rather than adopting the optimization-based explanations.", "evidence": "\"For the example you provided, we apologize for not including proper citations.\"; \"We think mesa-optimization mainly explains why models of Transformer architecture trained with autoregressive loss have the capacity to model data that involves ICL-like phenomena. However, in works such as [4], they only study on some toy data. It is unclear how their toy data is relevant to natural language data. Our work starts from another direction, aiming to understand what characteristics of training data lead to the ICL ability.\"; \"Thank you for the list of relevant works. We will include some of them in our later revision (as we have some concerns about the technical soundness of some of them).\"", "combined_author_response": "We appreciate all reviewers' insightful comments.\n\nWe are particularly grateful that the reviewers agreed that our paper has the following strengths:\n\n- The linguistic phenomena analysis is interesting to the community (rNJR, MUGa, 5jds).\n- The Calcutec dataset and the digit-addition task may be useful for future research (rNJR, MUGa, 5jds, hbQa,cdtQ).\n- Provides evidence that even smaller LLMs can perform ICL with artificial/task-agnostic label symbols (MUGa, 5jds).\n- A new formalism for NLP classification tasks (5jds, hbQa, cdtQ).\n- Empirical results supporting the Pelican Soup Hypothesis (rNJR, cdtQ) and provide a potential explanation for in-context learning (hbQa).\n\nWe have revised the paper to address the main concern that Section 3 is not substantiated, which we acknowledge is the confusion caused by the inclarity. We added more elaboration on the linkage between Section 3 and Section 5.1, stressing that Section 3 is to draw intuitions for the assumptions in Section 5.1 and that Section 5.1 is the formal description for the characteristics described in Section 3.\n\nWe also address each reviewer’s concerns below respectively. Please let us know if any aspects are still unclear. We are more than happy to provide additional clarification.\n\n---\n\nWe thank you for the positive feedback. With regard to the weakness:\n\n1. While Section 3 is purely conceptual, we provide rigorous characterization in Section 5.1. We consider this as one of our main contributions, an instantiation for the assumptions made in previous theoretic works, such as [1] and [2]. For [1], our framework constituted with the assumptions we made in Section 5.1 satisfies the requirements of Corollary 4.2, therefore we can use their analysis to get an $O(1/T)$ regret bound ($T$ is the number of examples in the demonstration). For [2], please refer to our reply to reviewer rNJR.\n2. Thanks for the suggestion! We can run more experiments. Could you let us know what experiments you think are important are missing?\n3. We admit that we only focus on classification tasks in this work. However, we would like to stress that even for the classification setting, the mechanism of ICL is not yet clear. Future work may also extend our framework to generation tasks.\n\nWith regard to the questions:\n\n1. By preprocessing the training data for LLMs, we think it’s possible. However, training an LLM requires lots of computational resources. We leave it for future work.\n2. The formalism in Section 2 can handle arbitrarily difficult reasoning tasks as long as there is a logic system that can handle it and the induction searching process is computationally feasible, e.g. not NP-hard. We believe that most human-solvable tasks are in this class.\n3. We think this is an important open question. A possible solution would be augmenting the training data, e.g. [3].\n4. Our conjecture would be that for many commonsense rules, LLMs are trained with many instances for each of them. Because the training data may cover a large portion of common surface forms, even though it’s possible that LLMs are doing no more than pattern matching, they are still able to do “reasoning” following the commonsense rules.  \n\n[1] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Zhou, Wangchunshu, Ronan Le Bras, and Yejin Choi. \"Commonsense Knowledge Transfer for Pre-trained Language Models.\" arXiv preprint arXiv:2306.02388 (2023).\n\n---\n\nThank you for your positive review. \n\nWe would like to point out that producing an environment, with which in-context learning still works, is a practice used in most (if not all) theoretical analyses such as [1] and [2] and even some empirical analyses such as [3]. We consider one of our main contributions to be proposing an environment that is closer to the real-world scenario than previous works.\n\nWith regard to the questions you stated:\n\n- The yes/no questions in a Pelican Soup riddle have a similar function as the demonstrations for in-context learning, because based on the answer of those yes/no questions, the participants are able to figure out the latent story and thus are able to answer other following questions. Note that we use Pelican Soup riddles just to motivate the importance of the role of commonsense.\n- Our argument is that solving these two problems (ICL and modeling coreference) requires similar capabilities. This argument is to draw the intuition for the framework/assumptions in Section 5.1. Surely the two problems are not identical, thus we characterize the difference between these two problems with the distribution shifts discussed in Section 4. To show that LMs may be able to generalize under these distribution shifts, we replicate these distribution shifts in our experiments, as discussed in Section 5.3 and results in Section 5.5 are aligned with our hypothesis.\n- We understand ICL as generating verbalizers based on the meaning of the verbalizers, which is recovered according to the preceding context, so we think it is similar to modeling anaphora. Could you please elaborate more on why you think ICL is more like handling catephora?\n\nPlease let us know if you have any other comments. We are happy to discuss more!\n\n[1] Xie, Sang Michael, et al. \"An Explanation of In-context Learning as Implicit Bayesian Inference.\" International Conference on Learning Representations. 2021.\n\n[2] Hahn, Michael, and Navin Goyal. \"A theory of emergent in-context learning as implicit structure induction.\" arXiv preprint arXiv:2303.07971 (2023).\n\n[3] Chan, Stephanie, et al. \"Data distributional properties drive emergent in-context learning in transformers.\" Advances in Neural Information Processing Systems 35 (2022): 18878-18891.\n\n---\n\nWe thank you for your review. We would like to address the weakness below:\n\n1. Please let us know what are the arguments that you think are not substantiated. For the example you provided, we apologize for not including proper citations. We think that LMs are able to acquire some knowledge about commonsense (though imperfectly) has been widely accepted. Many studies can be found in previous works, e.g. [1][2].\n2. Validating theories with a synthetic dataset is a common practice. We admit that our setting cannot explain domain bias. It may require more assumptions to theoretically explain. We would like to mention that, we think it will make the construction of the toy data even more complicated, which based on your 4th point, is not acceptable. \n3. We justify our assumptions made in Section 5.1 in Section 3. As argued in Section 3.1, we posit that training data usually contain text that has some logical structure and/or follows chronicle order, which can contribute to LMs’ reasoning capability. The dataset Calcutec is a formal abstraction of such structure.\n4. Could you be more specific on what details are missing and how we should improve? We will appreciate your suggestion. The construction of Calcutec is more complicated than the toy data used in previous theoretical works such as [3] because Calcutec relies on milder assumptions and is more realistic. \n5. We apologize that we didn’t make the purpose of Section 3 clearer. We treat Section 3 as the justification/intuition for the abstraction/assumptions/framework we made in Section 5.1. We then instantiate the abstraction/framework with Calcutec and show that LMs trained with it can do in-context learning.\n6. We use Pelican Soup riddles only to motivate the intuition that commonsense plays an important role for ICL. We would not say our hypothesis is *based on* this similarity.\n7. Please let us know if you think any experiment is important but missing. Also, could you elaborate more on the possibility that the models solve the task by learning shortcuts (and maybe also your definition of shortcuts)?\n\nOur answer to the question\n\n1. We think mesa-optimization mainly explains why models of Transformer architecture trained with autoregressive loss have the capacity to model data that involves ICL-like phenomena. However, in works such as [4], they only study on some toy data. It is unclear how their toy data is relevant to natural language data. Our work starts from another direction, aiming to understand what characteristics of training data lead to the ICL ability.\n\n\n[1] West, Peter, et al. \"Symbolic knowledge distillation: from general language models to commonsense models.\" arXiv preprint arXiv:2110.07178 (2021).\n\n[2] Li, Xiang Lorraine, et al. \"A systematic investigation of commonsense knowledge in large language models.\" Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.\n\n[3] Xie, Sang Michael, et al. \"An explanation of in-context learning as implicit bayesian inference.\" arXiv preprint arXiv:2111.02080 (2021).\n\n[4] von Oswald, Johannes, et al. \"Uncovering mesa-optimization algorithms in transformers.\" arXiv preprint arXiv:2309.05858 (2023).\n\n---\n\nThank you for your thoughtful comments. We would like to address the weakness below:\n\n1. In Section 2, we propose a formalism for NLP classification tasks while the main purpose of Section 3 is to justify the assumptions made in Section 5. We see the assumptions in Section 5 as the formal statement/abstraction of our theorem in Section 3. We are sorry that we didn’t describe it more clearly. We will improve this part in our later revision. Please let us know if we need to make more clarifications.\n2. As far as we know, all theoretical analyses on ICL depend on some repetitive nature of prompts or training data, e.g. Xie et al and Hahn & Goyal. One difference in our work is that we focus on the repetitive usage and the consistent meaning of pronouns, which is very realistic in real-world data. In Figure 2, we also show that the model can still do ICL when the task description is more complicated than the meaning of the pronouns seen in the training set, i.e., when the task description is composed of 3 atomic concepts (the green lines for “triple” in Figure 2).\n3. We discuss this point below:\n    - The digit addition task: Please refer to Figure 8 for the distribution of the number of steps in the datasets. For the 5-digit addition task, when the dropping rate is 0.2, there are only 265 (0.265%) training samples where all the intermediate steps are dropped. We think it is reasonable to say the testing samples are out-of-domain in this case. (And GPT-2-sized model can achieve ~80% accuracy with only 265 training samples in the testing domain.) Meanwhile, considering that the real-world data also has a small fraction of text that drops all the reasoning steps, we think this setting is not unrealistic.\n    - Calcutec: Please refer to Figure 5. It shows that even when no steps are dropped in the training set, the model still can do ICL, though the performance is not as good.\n\nQuestion:\n\n- The prompt is as the one in Table 1. We only include the reasoning steps of some (3) randomly selected examples and the premise of the testing input in the prompt. (For the example in Table 1, the testing input is `x55 x76 x84 x99`). We unroll until the first verbalizer is generated. \n\nPlease let us know if you have any other questions. We are more than happy to discuss with you.\n\n---\n\nThank you for your comments. \n\n## With Regard to the Weakness\n\n1. We consider one main contribution of our work to be explaining the ICL ability in the *semantically-unrelated label ICL (SUL-ICL)* setting [8], which is a step forward from the theorem by [7]. Indeed, our work can not explain the random label setting in [1]. However, we would like to point out that as far as we know, there has been no theorem able to explain it. We suggest that explaining it requires making more assumptions on the relationship between the input-label mapping and the domain/topic/theme distribution in the training data.\n2. We are sorry for the confusion. The justification mainly follows the argument in Section 3. We will make the connection clearer in our later revision. For now, please allow us to elaborate it below:\n    - Assumption 1: It follows the efforts made by early linguistics on linguistic formalism and cognitive psychology theories such as language of thought.\n    - Assumption 2: This assumption follows the argument in Section 3.1 that general text is usually organized in a similar way as logical induction processes. We assume that “a paragraph is a traversal of the nodes of the proving tree in the topological ordering” because in general text, statements usually follow some causal and/or chronological order, so it can convince readers or be easy for readers to understand.\n    - Assumption 3: In other words, this assumption assumes that every paragraph in an article mentions the topic of the article.\n    - Assumption 4: In other words, this assumption assumes that some pronouns are used in the article and each pronoun in the article is associated with a single entity.\n    - Assumption 5: This assumption follows the argument in Section 2.\n\n    In our opinion, these assumptions are relatively mild compared with previous theoretical works. We are happy to discuss more if you still have some concerns about the realism of these assumptions.\n3. To make our main response short, please allow us to discuss it below.\n4. Thank you for the list of relevant works. We will include some of them in our later revision (as we have some concerns about the technical soundness of some of them). \n\n\n## Atomic elements of NLP tasks in [2]\n\n\nIf we understand [2] correctly, [2] does not specify what characteristics the “atomic skills” of NLP need to have. We think our work could be an instantiation of the “atoms” proposed in [2]. \n\nIt seems that defining a set of “atomic skills” is not trivial and can’t be done arbitrarily. “Atomic skills” must be defined in a way such that the skills that work for the training data can generalize to the downstream task. To understand why it is non-trivial to define a set of atomic skills, we can probably think about a trivial case where we define the set “atomic skill” as the 26 skills to predict the words starting with A-Z. With this setting, the argument in [2] is greatly simplified. It seems that Theorem 14 only tells us that for most of the letters, the model can do the cloze problems in the training set well if the answer starts with that letter. It doesn’t seem to be guaranteed that the model can do the cloze problems in the testing set well.\n\nOn the other hand, defining the atomic skills with our atomic concepts seems to be more generalizable. Still, we posit it is necessary to discuss the discrepancies between the training set and the downstream task. That’s why in this work we explicitly characterize the distribution shifts in Section 4 and demonstrate that LMs are able to generalize under these distribution shifts in Section 5 and 7. Surely our settings do not fully replicate the real-world data distribution, but we think it still complements some aspects of [2]. \n\nPlease let us know if you still have any questions. We are more than happy to discuss further.\n\n\n[7] Xie, Sang Michael, et al. \"An explanation of in-context learning as implicit bayesian inference.\" arXiv preprint arXiv:2111.02080 (2021).\n\n[8] Wei, Jerry, et al. \"Larger language models do in-context learning differently.\" arXiv preprint arXiv:2303.03846 (2023).", "author_response": "Thank you for your comments. \n\n## With Regard to the Weakness\n\n1. We consider one main contribution of our work to be explaining the ICL ability in the *semantically-unrelated label ICL (SUL-ICL)* setting [8], which is a step forward from the theorem by [7]. Indeed, our work can not explain the random label setting in [1]. However, we would like to point out that as far as we know, there has been no theorem able to explain it. We suggest that explaining it requires making more assumptions on the relationship between the input-label mapping and the domain/topic/theme distribution in the training data.\n2. We are sorry for the confusion. The justification mainly follows the argument in Section 3. We will make the connection clearer in our later revision. For now, please allow us to elaborate it below:\n    - Assumption 1: It follows the efforts made by early linguistics on linguistic formalism and cognitive psychology theories such as language of thought.\n    - Assumption 2: This assumption follows the argument in Section 3.1 that general text is usually organized in a similar way as logical induction processes. We assume that “a paragraph is a traversal of the nodes of the proving tree in the topological ordering” because in general text, statements usually follow some causal and/or chronological order, so it can convince readers or be easy for readers to understand.\n    - Assumption 3: In other words, this assumption assumes that every paragraph in an article mentions the topic of the article.\n    - Assumption 4: In other words, this assumption assumes that some pronouns are used in the article and each pronoun in the article is associated with a single entity.\n    - Assumption 5: This assumption follows the argument in Section 2.\n\n    In our opinion, these assumptions are relatively mild compared with previous theoretical works. We are happy to discuss more if you still have some concerns about the realism of these assumptions.\n3. To make our main response short, please allow us to discuss it below.\n4. Thank you for the list of relevant works. We will include some of them in our later revision (as we have some concerns about the technical soundness of some of them). \n\n\n## Atomic elements of NLP tasks in [2]\n\n\nIf we understand [2] correctly, [2] does not specify what characteristics the “atomic skills” of NLP need to have. We think our work could be an instantiation of the “atoms” proposed in [2]. \n\nIt seems that defining a set of “atomic skills” is not trivial and can’t be done arbitrarily. “Atomic skills” must be defined in a way such that the skills that work for the training data can generalize to the downstream task. To understand why it is non-trivial to define a set of atomic skills, we can probably think about a trivial case where we define the set “atomic skill” as the 26 skills to predict the words starting with A-Z. With this setting, the argument in [2] is greatly simplified. It seems that Theorem 14 only tells us that for most of the letters, the model can do the cloze problems in the training set well if the answer starts with that letter. It doesn’t seem to be guaranteed that the model can do the cloze problems in the testing set well.\n\nOn the other hand, defining the atomic skills with our atomic concepts seems to be more generalizable. Still, we posit it is necessary to discuss the discrepancies between the training set and the downstream task. That’s why in this work we explicitly characterize the distribution shifts in Section 4 and demonstrate that LMs are able to generalize under these distribution shifts in Section 5 and 7. Surely our settings do not fully replicate the real-world data distribution, but we think it still complements some aspects of [2]. \n\nPlease let us know if you still have any questions. We are more than happy to discuss further.\n\n\n[7] Xie, Sang Michael, et al. \"An explanation of in-context learning as implicit bayesian inference.\" arXiv preprint arXiv:2111.02080 (2021).\n\n[8] Wei, Jerry, et al. \"Larger language models do in-context learning differently.\" arXiv preprint arXiv:2303.03846 (2023)."}
{"claim": "The proposed approach is not the first to explore extraction of local and global explanations; prior works like GLANCE and Glocalx address similar problems.", "claim_type": "novelty", "paper_id": "UqZecMwLTo", "paper_title": "A Unified Concept-Based System for Local, Global, and Misclassification Explanations", "paper_venue": "iclr2024", "paper_decision": null, "decision": "Unknown", "review_id": "Xnu3bVVbeW", "reviewer": "Reviewer_DJR7", "review_text": "Summary: The authors propose a unified framework for generating both local and global concept based explanations. The proposed framework considers a surrogate model trained with superpixels of an image for generating concepts, which are used then used to extract local and global explanations.\n\nStrengths: - The proposed approach is innovative and can be used to generate and score the relevance of concepts\n- The illustrations of local explanations are well presented, nice framework and illustration figures\n- The paper is well written and easy to follow\n\nWeaknesses: - The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2]. \n- What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n- The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n- I'm unsure of how to interpret global explanations. Can you please provide more discussion for the same? \n- It is a bit unclear how the scores in Table 1 are computed, given the UCBS is fine-tuned on superpixel images; the scores like insertion, deletion, and faithfulness are expected to be high for UCBS, given they are on the manifold, while in case of LIME, SHAPE, GradCAM, and LRP the concepts are off-manifold resulting in lower score. Could you please justify this?\n  \n\n\n[1] Kori, A., Glocker, B. and Toni, F., 2022. GLANCE: Global to Local Architecture-Neutral Concept-based Explanations. _arXiv preprint arXiv:2207.01917_.\n\n[2] Setzu, M., Guidotti, R., Monreale, A., Turini, F., Pedreschi, D. and Giannotti, F., 2021. Glocalx-from local to global explanations of black box ai models. _Artificial Intelligence_, _294_, p.103457.\n\nQuestions: Please refer to weakness section", "labeling_timestamp": "2026-01-11T16:33:37.337004", "model": "gpt-5-mini", "label": "Supported", "justification": "Authors explicitly acknowledge the reviewer’s point, admit the claim was overstated, and state they removed the assertion that UCBS was among the first; they also cite the prior works and explain distinctions.", "evidence": "“The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2].” and “However, in order to avoid potential confusion, we decided to remove this statement.”", "combined_author_response": "We are grateful for your thorough review and insightful suggestions.  Below, we address your concerns mentioned in the Weaknesses.\n\n1. The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2].\n\nThanks for your valuable mention. In our paper, we initially stated that \"UCBS is, to the best of our knowledge, one of the first unsupervised unified concept extraction frameworks.\" This claim is based on three important factors: the unified nature encompassing local, global, and misclassification explanations, the focus on concept-based extraction, and the unsupervised nature in terms of explanation labels. We meant to emphasize that there are few concentrated works in these areas targeting vision-based applications, making UCBS possibly one of the pioneering methods. However, in order to avoid potential confusion, we decided to remove this statement.\n\nMore clarification: Ref [2] has primarily been designed for classification tasks on tabular data, and we believe that its applicability to pixel-based data may be limited due to its rule-based nature. Furthermore, Ref [1] offers global explanations in terms of graphs and local explanations in terms of feature importance scores, not concepts.\n\n\n2. What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n\nSurrogate models are indeed binary classifiers that try to fully comprehend the target of interest.  In order to accomplish this, their initial task is to differentiate the target's objects from the non-target samples. Subsequently, they should be able to discriminate the target's concepts and assign them scores based on their significance. The out-of-distribution samples (the non-target samples), which are known relative to the distribution of the target samples, are crucial for achieving the first objective. These samples, in companion with the target samples and the super-pixelated images, are utilized to teach the surrogate network a better representation of the target’s objects as well as the target’s concepts. In order to further clarify this comment, we revised the 5-th paragraph of the introduction as follows:\nIn an analogous case, we engage surrogate explainer networks and equip them with auxiliary datasets to purposely concentrate on the target of interest, rather than a multi-class classification approach. That is, the surrogate networks try to comprehensively learn one target class against the non-target samples. This allows them to thoroughly learn the target's objects as well as their informative concepts.\n\n3. The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n\nNo, the current version of the method is not able to detect abstract and complex concepts like brightness or daylight, which are challenging concepts necessitating additional pre/post-processing tasks. We mentioned this limitation in section 5 (Discussion, limitations, and future works) as follows: \n\nHowever, it is important to acknowledge that the limitations and drawbacks of superpixels are inherited by the UCBS, which may potentially result in the creation of meaningless concepts. Moreover, the automatic extraction of abstract and complex concepts may be challenging or necessitate additional post-processing. Addressing these challenges presents an intriguing direction for future research, particularly in identifying and surrounding the most influential concepts within bounding boxes, which could help mitigate the issues.\n\nIn addition, in appendix section C.5 (More discussion on limitations: Imperfectly super-pixelated inputs and Dataset biases), we further discussed that this limitation refers to incorporating super-pixelating methods:\n\nMoreover, super-pixelating methods typically offer no solution for tracking entirely or partially abstract concepts such as brightness, daylight, and sun angle in sunrise or sunset images.", "author_response": "We are grateful for your thorough review and insightful suggestions.  Below, we address your concerns mentioned in the Weaknesses.\n\n1. The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2].\n\nThanks for your valuable mention. In our paper, we initially stated that \"UCBS is, to the best of our knowledge, one of the first unsupervised unified concept extraction frameworks.\" This claim is based on three important factors: the unified nature encompassing local, global, and misclassification explanations, the focus on concept-based extraction, and the unsupervised nature in terms of explanation labels. We meant to emphasize that there are few concentrated works in these areas targeting vision-based applications, making UCBS possibly one of the pioneering methods. However, in order to avoid potential confusion, we decided to remove this statement.\n\nMore clarification: Ref [2] has primarily been designed for classification tasks on tabular data, and we believe that its applicability to pixel-based data may be limited due to its rule-based nature. Furthermore, Ref [1] offers global explanations in terms of graphs and local explanations in terms of feature importance scores, not concepts.\n\n\n2. What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n\nSurrogate models are indeed binary classifiers that try to fully comprehend the target of interest.  In order to accomplish this, their initial task is to differentiate the target's objects from the non-target samples. Subsequently, they should be able to discriminate the target's concepts and assign them scores based on their significance. The out-of-distribution samples (the non-target samples), which are known relative to the distribution of the target samples, are crucial for achieving the first objective. These samples, in companion with the target samples and the super-pixelated images, are utilized to teach the surrogate network a better representation of the target’s objects as well as the target’s concepts. In order to further clarify this comment, we revised the 5-th paragraph of the introduction as follows:\nIn an analogous case, we engage surrogate explainer networks and equip them with auxiliary datasets to purposely concentrate on the target of interest, rather than a multi-class classification approach. That is, the surrogate networks try to comprehensively learn one target class against the non-target samples. This allows them to thoroughly learn the target's objects as well as their informative concepts.\n\n3. The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n\nNo, the current version of the method is not able to detect abstract and complex concepts like brightness or daylight, which are challenging concepts necessitating additional pre/post-processing tasks. We mentioned this limitation in section 5 (Discussion, limitations, and future works) as follows: \n\nHowever, it is important to acknowledge that the limitations and drawbacks of superpixels are inherited by the UCBS, which may potentially result in the creation of meaningless concepts. Moreover, the automatic extraction of abstract and complex concepts may be challenging or necessitate additional post-processing. Addressing these challenges presents an intriguing direction for future research, particularly in identifying and surrounding the most influential concepts within bounding boxes, which could help mitigate the issues.\n\nIn addition, in appendix section C.5 (More discussion on limitations: Imperfectly super-pixelated inputs and Dataset biases), we further discussed that this limitation refers to incorporating super-pixelating methods:\n\nMoreover, super-pixelating methods typically offer no solution for tracking entirely or partially abstract concepts such as brightness, daylight, and sun angle in sunrise or sunset images."}
{"claim": "The paper does not cite or compare its method to prior works GLANCE (2022) and Glocalx (2021), which are relevant baselines.", "claim_type": "baseline", "paper_id": "UqZecMwLTo", "paper_title": "A Unified Concept-Based System for Local, Global, and Misclassification Explanations", "paper_venue": "iclr2024", "paper_decision": null, "decision": "Unknown", "review_id": "Xnu3bVVbeW", "reviewer": "Reviewer_DJR7", "review_text": "Summary: The authors propose a unified framework for generating both local and global concept based explanations. The proposed framework considers a surrogate model trained with superpixels of an image for generating concepts, which are used then used to extract local and global explanations.\n\nStrengths: - The proposed approach is innovative and can be used to generate and score the relevance of concepts\n- The illustrations of local explanations are well presented, nice framework and illustration figures\n- The paper is well written and easy to follow\n\nWeaknesses: - The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2]. \n- What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n- The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n- I'm unsure of how to interpret global explanations. Can you please provide more discussion for the same? \n- It is a bit unclear how the scores in Table 1 are computed, given the UCBS is fine-tuned on superpixel images; the scores like insertion, deletion, and faithfulness are expected to be high for UCBS, given they are on the manifold, while in case of LIME, SHAPE, GradCAM, and LRP the concepts are off-manifold resulting in lower score. Could you please justify this?\n  \n\n\n[1] Kori, A., Glocker, B. and Toni, F., 2022. GLANCE: Global to Local Architecture-Neutral Concept-based Explanations. _arXiv preprint arXiv:2207.01917_.\n\n[2] Setzu, M., Guidotti, R., Monreale, A., Turini, F., Pedreschi, D. and Giannotti, F., 2021. Glocalx-from local to global explanations of black box ai models. _Artificial Intelligence_, _294_, p.103457.\n\nQuestions: Please refer to weakness section", "labeling_timestamp": "2026-01-11T16:34:00.667174", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "Authors acknowledge prior works by adding references and removing their original claim of novelty, and they explain differences (tabular vs. pixel data; different explanation types). However, they do not report an empirical comparison to GLANCE and Glocalx, instead arguing limited applicability—so the reviewer's request for citation/comparison is only partially addressed.", "evidence": "\"The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2].\"; \"However, in order to avoid potential confusion, we decided to remove this statement.\"; \"Ref [2] has primarily been designed for classification tasks on tabular data, and we believe that its applicability to pixel-based data may be limited due to its rule-based nature. Furthermore, Ref [1] offers global explanations in terms of graphs and local explanations in terms of feature importance scores, not concepts.\"", "combined_author_response": "We are grateful for your thorough review and insightful suggestions.  Below, we address your concerns mentioned in the Weaknesses.\n\n1. The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2].\n\nThanks for your valuable mention. In our paper, we initially stated that \"UCBS is, to the best of our knowledge, one of the first unsupervised unified concept extraction frameworks.\" This claim is based on three important factors: the unified nature encompassing local, global, and misclassification explanations, the focus on concept-based extraction, and the unsupervised nature in terms of explanation labels. We meant to emphasize that there are few concentrated works in these areas targeting vision-based applications, making UCBS possibly one of the pioneering methods. However, in order to avoid potential confusion, we decided to remove this statement.\n\nMore clarification: Ref [2] has primarily been designed for classification tasks on tabular data, and we believe that its applicability to pixel-based data may be limited due to its rule-based nature. Furthermore, Ref [1] offers global explanations in terms of graphs and local explanations in terms of feature importance scores, not concepts.\n\n\n2. What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n\nSurrogate models are indeed binary classifiers that try to fully comprehend the target of interest.  In order to accomplish this, their initial task is to differentiate the target's objects from the non-target samples. Subsequently, they should be able to discriminate the target's concepts and assign them scores based on their significance. The out-of-distribution samples (the non-target samples), which are known relative to the distribution of the target samples, are crucial for achieving the first objective. These samples, in companion with the target samples and the super-pixelated images, are utilized to teach the surrogate network a better representation of the target’s objects as well as the target’s concepts. In order to further clarify this comment, we revised the 5-th paragraph of the introduction as follows:\nIn an analogous case, we engage surrogate explainer networks and equip them with auxiliary datasets to purposely concentrate on the target of interest, rather than a multi-class classification approach. That is, the surrogate networks try to comprehensively learn one target class against the non-target samples. This allows them to thoroughly learn the target's objects as well as their informative concepts.\n\n3. The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n\nNo, the current version of the method is not able to detect abstract and complex concepts like brightness or daylight, which are challenging concepts necessitating additional pre/post-processing tasks. We mentioned this limitation in section 5 (Discussion, limitations, and future works) as follows: \n\nHowever, it is important to acknowledge that the limitations and drawbacks of superpixels are inherited by the UCBS, which may potentially result in the creation of meaningless concepts. Moreover, the automatic extraction of abstract and complex concepts may be challenging or necessitate additional post-processing. Addressing these challenges presents an intriguing direction for future research, particularly in identifying and surrounding the most influential concepts within bounding boxes, which could help mitigate the issues.\n\nIn addition, in appendix section C.5 (More discussion on limitations: Imperfectly super-pixelated inputs and Dataset biases), we further discussed that this limitation refers to incorporating super-pixelating methods:\n\nMoreover, super-pixelating methods typically offer no solution for tracking entirely or partially abstract concepts such as brightness, daylight, and sun angle in sunrise or sunset images.", "author_response": "We are grateful for your thorough review and insightful suggestions.  Below, we address your concerns mentioned in the Weaknesses.\n\n1. The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2].\n\nThanks for your valuable mention. In our paper, we initially stated that \"UCBS is, to the best of our knowledge, one of the first unsupervised unified concept extraction frameworks.\" This claim is based on three important factors: the unified nature encompassing local, global, and misclassification explanations, the focus on concept-based extraction, and the unsupervised nature in terms of explanation labels. We meant to emphasize that there are few concentrated works in these areas targeting vision-based applications, making UCBS possibly one of the pioneering methods. However, in order to avoid potential confusion, we decided to remove this statement.\n\nMore clarification: Ref [2] has primarily been designed for classification tasks on tabular data, and we believe that its applicability to pixel-based data may be limited due to its rule-based nature. Furthermore, Ref [1] offers global explanations in terms of graphs and local explanations in terms of feature importance scores, not concepts.\n\n\n2. What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n\nSurrogate models are indeed binary classifiers that try to fully comprehend the target of interest.  In order to accomplish this, their initial task is to differentiate the target's objects from the non-target samples. Subsequently, they should be able to discriminate the target's concepts and assign them scores based on their significance. The out-of-distribution samples (the non-target samples), which are known relative to the distribution of the target samples, are crucial for achieving the first objective. These samples, in companion with the target samples and the super-pixelated images, are utilized to teach the surrogate network a better representation of the target’s objects as well as the target’s concepts. In order to further clarify this comment, we revised the 5-th paragraph of the introduction as follows:\nIn an analogous case, we engage surrogate explainer networks and equip them with auxiliary datasets to purposely concentrate on the target of interest, rather than a multi-class classification approach. That is, the surrogate networks try to comprehensively learn one target class against the non-target samples. This allows them to thoroughly learn the target's objects as well as their informative concepts.\n\n3. The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n\nNo, the current version of the method is not able to detect abstract and complex concepts like brightness or daylight, which are challenging concepts necessitating additional pre/post-processing tasks. We mentioned this limitation in section 5 (Discussion, limitations, and future works) as follows: \n\nHowever, it is important to acknowledge that the limitations and drawbacks of superpixels are inherited by the UCBS, which may potentially result in the creation of meaningless concepts. Moreover, the automatic extraction of abstract and complex concepts may be challenging or necessitate additional post-processing. Addressing these challenges presents an intriguing direction for future research, particularly in identifying and surrounding the most influential concepts within bounding boxes, which could help mitigate the issues.\n\nIn addition, in appendix section C.5 (More discussion on limitations: Imperfectly super-pixelated inputs and Dataset biases), we further discussed that this limitation refers to incorporating super-pixelating methods:\n\nMoreover, super-pixelating methods typically offer no solution for tracking entirely or partially abstract concepts such as brightness, daylight, and sun angle in sunrise or sunset images."}
{"claim": "The manuscript does not provide a clear motivation for using an out-of-distribution dataset to train the surrogate model.", "claim_type": "presentation", "paper_id": "UqZecMwLTo", "paper_title": "A Unified Concept-Based System for Local, Global, and Misclassification Explanations", "paper_venue": "iclr2024", "paper_decision": null, "decision": "Unknown", "review_id": "Xnu3bVVbeW", "reviewer": "Reviewer_DJR7", "review_text": "Summary: The authors propose a unified framework for generating both local and global concept based explanations. The proposed framework considers a surrogate model trained with superpixels of an image for generating concepts, which are used then used to extract local and global explanations.\n\nStrengths: - The proposed approach is innovative and can be used to generate and score the relevance of concepts\n- The illustrations of local explanations are well presented, nice framework and illustration figures\n- The paper is well written and easy to follow\n\nWeaknesses: - The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2]. \n- What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n- The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n- I'm unsure of how to interpret global explanations. Can you please provide more discussion for the same? \n- It is a bit unclear how the scores in Table 1 are computed, given the UCBS is fine-tuned on superpixel images; the scores like insertion, deletion, and faithfulness are expected to be high for UCBS, given they are on the manifold, while in case of LIME, SHAPE, GradCAM, and LRP the concepts are off-manifold resulting in lower score. Could you please justify this?\n  \n\n\n[1] Kori, A., Glocker, B. and Toni, F., 2022. GLANCE: Global to Local Architecture-Neutral Concept-based Explanations. _arXiv preprint arXiv:2207.01917_.\n\n[2] Setzu, M., Guidotti, R., Monreale, A., Turini, F., Pedreschi, D. and Giannotti, F., 2021. Glocalx-from local to global explanations of black box ai models. _Artificial Intelligence_, _294_, p.103457.\n\nQuestions: Please refer to weakness section", "labeling_timestamp": "2026-01-11T16:33:48.704338", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors directly address the concern by explaining that out-of-distribution samples serve as non-target examples for surrogate binary classifiers, enabling the model to distinguish target vs non-target and thereby learn better representations and informative concepts; they also state they revised the manuscript to clarify this motivation.", "evidence": "Surrogate models are indeed binary classifiers that try to fully comprehend the target of interest. In order to accomplish this, their initial task is to differentiate the target's objects from the non-target samples. Subsequently, they should be able to discriminate the target's concepts and assign them scores based on their significance. The out-of-distribution samples (the non-target samples), which are known relative to the distribution of the target samples, are crucial for achieving the first objective. ... In an analogous case, we engage surrogate explainer networks and equip them with auxiliary datasets to purposely concentrate on the target of interest, rather than a multi-class classification approach. That is, the surrogate networks try to comprehensively learn one target class against the non-target samples. This allows them to thoroughly learn the target's objects as well as their informative concepts.", "combined_author_response": "We are grateful for your thorough review and insightful suggestions.  Below, we address your concerns mentioned in the Weaknesses.\n\n1. The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2].\n\nThanks for your valuable mention. In our paper, we initially stated that \"UCBS is, to the best of our knowledge, one of the first unsupervised unified concept extraction frameworks.\" This claim is based on three important factors: the unified nature encompassing local, global, and misclassification explanations, the focus on concept-based extraction, and the unsupervised nature in terms of explanation labels. We meant to emphasize that there are few concentrated works in these areas targeting vision-based applications, making UCBS possibly one of the pioneering methods. However, in order to avoid potential confusion, we decided to remove this statement.\n\nMore clarification: Ref [2] has primarily been designed for classification tasks on tabular data, and we believe that its applicability to pixel-based data may be limited due to its rule-based nature. Furthermore, Ref [1] offers global explanations in terms of graphs and local explanations in terms of feature importance scores, not concepts.\n\n\n2. What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n\nSurrogate models are indeed binary classifiers that try to fully comprehend the target of interest.  In order to accomplish this, their initial task is to differentiate the target's objects from the non-target samples. Subsequently, they should be able to discriminate the target's concepts and assign them scores based on their significance. The out-of-distribution samples (the non-target samples), which are known relative to the distribution of the target samples, are crucial for achieving the first objective. These samples, in companion with the target samples and the super-pixelated images, are utilized to teach the surrogate network a better representation of the target’s objects as well as the target’s concepts. In order to further clarify this comment, we revised the 5-th paragraph of the introduction as follows:\nIn an analogous case, we engage surrogate explainer networks and equip them with auxiliary datasets to purposely concentrate on the target of interest, rather than a multi-class classification approach. That is, the surrogate networks try to comprehensively learn one target class against the non-target samples. This allows them to thoroughly learn the target's objects as well as their informative concepts.\n\n3. The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n\nNo, the current version of the method is not able to detect abstract and complex concepts like brightness or daylight, which are challenging concepts necessitating additional pre/post-processing tasks. We mentioned this limitation in section 5 (Discussion, limitations, and future works) as follows: \n\nHowever, it is important to acknowledge that the limitations and drawbacks of superpixels are inherited by the UCBS, which may potentially result in the creation of meaningless concepts. Moreover, the automatic extraction of abstract and complex concepts may be challenging or necessitate additional post-processing. Addressing these challenges presents an intriguing direction for future research, particularly in identifying and surrounding the most influential concepts within bounding boxes, which could help mitigate the issues.\n\nIn addition, in appendix section C.5 (More discussion on limitations: Imperfectly super-pixelated inputs and Dataset biases), we further discussed that this limitation refers to incorporating super-pixelating methods:\n\nMoreover, super-pixelating methods typically offer no solution for tracking entirely or partially abstract concepts such as brightness, daylight, and sun angle in sunrise or sunset images.", "author_response": "We are grateful for your thorough review and insightful suggestions.  Below, we address your concerns mentioned in the Weaknesses.\n\n1. The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2].\n\nThanks for your valuable mention. In our paper, we initially stated that \"UCBS is, to the best of our knowledge, one of the first unsupervised unified concept extraction frameworks.\" This claim is based on three important factors: the unified nature encompassing local, global, and misclassification explanations, the focus on concept-based extraction, and the unsupervised nature in terms of explanation labels. We meant to emphasize that there are few concentrated works in these areas targeting vision-based applications, making UCBS possibly one of the pioneering methods. However, in order to avoid potential confusion, we decided to remove this statement.\n\nMore clarification: Ref [2] has primarily been designed for classification tasks on tabular data, and we believe that its applicability to pixel-based data may be limited due to its rule-based nature. Furthermore, Ref [1] offers global explanations in terms of graphs and local explanations in terms of feature importance scores, not concepts.\n\n\n2. What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n\nSurrogate models are indeed binary classifiers that try to fully comprehend the target of interest.  In order to accomplish this, their initial task is to differentiate the target's objects from the non-target samples. Subsequently, they should be able to discriminate the target's concepts and assign them scores based on their significance. The out-of-distribution samples (the non-target samples), which are known relative to the distribution of the target samples, are crucial for achieving the first objective. These samples, in companion with the target samples and the super-pixelated images, are utilized to teach the surrogate network a better representation of the target’s objects as well as the target’s concepts. In order to further clarify this comment, we revised the 5-th paragraph of the introduction as follows:\nIn an analogous case, we engage surrogate explainer networks and equip them with auxiliary datasets to purposely concentrate on the target of interest, rather than a multi-class classification approach. That is, the surrogate networks try to comprehensively learn one target class against the non-target samples. This allows them to thoroughly learn the target's objects as well as their informative concepts.\n\n3. The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n\nNo, the current version of the method is not able to detect abstract and complex concepts like brightness or daylight, which are challenging concepts necessitating additional pre/post-processing tasks. We mentioned this limitation in section 5 (Discussion, limitations, and future works) as follows: \n\nHowever, it is important to acknowledge that the limitations and drawbacks of superpixels are inherited by the UCBS, which may potentially result in the creation of meaningless concepts. Moreover, the automatic extraction of abstract and complex concepts may be challenging or necessitate additional post-processing. Addressing these challenges presents an intriguing direction for future research, particularly in identifying and surrounding the most influential concepts within bounding boxes, which could help mitigate the issues.\n\nIn addition, in appendix section C.5 (More discussion on limitations: Imperfectly super-pixelated inputs and Dataset biases), we further discussed that this limitation refers to incorporating super-pixelating methods:\n\nMoreover, super-pixelating methods typically offer no solution for tracking entirely or partially abstract concepts such as brightness, daylight, and sun angle in sunrise or sunset images."}
{"claim": "The proposed approach only captures concepts describable as image parts and may fail to capture whole-image concepts such as lighting or global illumination.", "claim_type": "methodology", "paper_id": "UqZecMwLTo", "paper_title": "A Unified Concept-Based System for Local, Global, and Misclassification Explanations", "paper_venue": "iclr2024", "paper_decision": null, "decision": "Unknown", "review_id": "Xnu3bVVbeW", "reviewer": "Reviewer_DJR7", "review_text": "Summary: The authors propose a unified framework for generating both local and global concept based explanations. The proposed framework considers a surrogate model trained with superpixels of an image for generating concepts, which are used then used to extract local and global explanations.\n\nStrengths: - The proposed approach is innovative and can be used to generate and score the relevance of concepts\n- The illustrations of local explanations are well presented, nice framework and illustration figures\n- The paper is well written and easy to follow\n\nWeaknesses: - The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2]. \n- What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n- The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n- I'm unsure of how to interpret global explanations. Can you please provide more discussion for the same? \n- It is a bit unclear how the scores in Table 1 are computed, given the UCBS is fine-tuned on superpixel images; the scores like insertion, deletion, and faithfulness are expected to be high for UCBS, given they are on the manifold, while in case of LIME, SHAPE, GradCAM, and LRP the concepts are off-manifold resulting in lower score. Could you please justify this?\n  \n\n\n[1] Kori, A., Glocker, B. and Toni, F., 2022. GLANCE: Global to Local Architecture-Neutral Concept-based Explanations. _arXiv preprint arXiv:2207.01917_.\n\n[2] Setzu, M., Guidotti, R., Monreale, A., Turini, F., Pedreschi, D. and Giannotti, F., 2021. Glocalx-from local to global explanations of black box ai models. _Artificial Intelligence_, _294_, p.103457.\n\nQuestions: Please refer to weakness section", "labeling_timestamp": "2026-01-11T16:34:03.169651", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors explicitly acknowledge the limitation: the current method cannot detect abstract or global image concepts like brightness or daylight and they report this as a limitation in the paper and appendix.", "evidence": "No, the current version of the method is not able to detect abstract and complex concepts like brightness or daylight, which are challenging concepts necessitating additional pre/post-processing tasks.", "combined_author_response": "We are grateful for your thorough review and insightful suggestions.  Below, we address your concerns mentioned in the Weaknesses.\n\n1. The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2].\n\nThanks for your valuable mention. In our paper, we initially stated that \"UCBS is, to the best of our knowledge, one of the first unsupervised unified concept extraction frameworks.\" This claim is based on three important factors: the unified nature encompassing local, global, and misclassification explanations, the focus on concept-based extraction, and the unsupervised nature in terms of explanation labels. We meant to emphasize that there are few concentrated works in these areas targeting vision-based applications, making UCBS possibly one of the pioneering methods. However, in order to avoid potential confusion, we decided to remove this statement.\n\nMore clarification: Ref [2] has primarily been designed for classification tasks on tabular data, and we believe that its applicability to pixel-based data may be limited due to its rule-based nature. Furthermore, Ref [1] offers global explanations in terms of graphs and local explanations in terms of feature importance scores, not concepts.\n\n\n2. What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n\nSurrogate models are indeed binary classifiers that try to fully comprehend the target of interest.  In order to accomplish this, their initial task is to differentiate the target's objects from the non-target samples. Subsequently, they should be able to discriminate the target's concepts and assign them scores based on their significance. The out-of-distribution samples (the non-target samples), which are known relative to the distribution of the target samples, are crucial for achieving the first objective. These samples, in companion with the target samples and the super-pixelated images, are utilized to teach the surrogate network a better representation of the target’s objects as well as the target’s concepts. In order to further clarify this comment, we revised the 5-th paragraph of the introduction as follows:\nIn an analogous case, we engage surrogate explainer networks and equip them with auxiliary datasets to purposely concentrate on the target of interest, rather than a multi-class classification approach. That is, the surrogate networks try to comprehensively learn one target class against the non-target samples. This allows them to thoroughly learn the target's objects as well as their informative concepts.\n\n3. The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n\nNo, the current version of the method is not able to detect abstract and complex concepts like brightness or daylight, which are challenging concepts necessitating additional pre/post-processing tasks. We mentioned this limitation in section 5 (Discussion, limitations, and future works) as follows: \n\nHowever, it is important to acknowledge that the limitations and drawbacks of superpixels are inherited by the UCBS, which may potentially result in the creation of meaningless concepts. Moreover, the automatic extraction of abstract and complex concepts may be challenging or necessitate additional post-processing. Addressing these challenges presents an intriguing direction for future research, particularly in identifying and surrounding the most influential concepts within bounding boxes, which could help mitigate the issues.\n\nIn addition, in appendix section C.5 (More discussion on limitations: Imperfectly super-pixelated inputs and Dataset biases), we further discussed that this limitation refers to incorporating super-pixelating methods:\n\nMoreover, super-pixelating methods typically offer no solution for tracking entirely or partially abstract concepts such as brightness, daylight, and sun angle in sunrise or sunset images.", "author_response": "We are grateful for your thorough review and insightful suggestions.  Below, we address your concerns mentioned in the Weaknesses.\n\n1. The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2].\n\nThanks for your valuable mention. In our paper, we initially stated that \"UCBS is, to the best of our knowledge, one of the first unsupervised unified concept extraction frameworks.\" This claim is based on three important factors: the unified nature encompassing local, global, and misclassification explanations, the focus on concept-based extraction, and the unsupervised nature in terms of explanation labels. We meant to emphasize that there are few concentrated works in these areas targeting vision-based applications, making UCBS possibly one of the pioneering methods. However, in order to avoid potential confusion, we decided to remove this statement.\n\nMore clarification: Ref [2] has primarily been designed for classification tasks on tabular data, and we believe that its applicability to pixel-based data may be limited due to its rule-based nature. Furthermore, Ref [1] offers global explanations in terms of graphs and local explanations in terms of feature importance scores, not concepts.\n\n\n2. What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n\nSurrogate models are indeed binary classifiers that try to fully comprehend the target of interest.  In order to accomplish this, their initial task is to differentiate the target's objects from the non-target samples. Subsequently, they should be able to discriminate the target's concepts and assign them scores based on their significance. The out-of-distribution samples (the non-target samples), which are known relative to the distribution of the target samples, are crucial for achieving the first objective. These samples, in companion with the target samples and the super-pixelated images, are utilized to teach the surrogate network a better representation of the target’s objects as well as the target’s concepts. In order to further clarify this comment, we revised the 5-th paragraph of the introduction as follows:\nIn an analogous case, we engage surrogate explainer networks and equip them with auxiliary datasets to purposely concentrate on the target of interest, rather than a multi-class classification approach. That is, the surrogate networks try to comprehensively learn one target class against the non-target samples. This allows them to thoroughly learn the target's objects as well as their informative concepts.\n\n3. The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n\nNo, the current version of the method is not able to detect abstract and complex concepts like brightness or daylight, which are challenging concepts necessitating additional pre/post-processing tasks. We mentioned this limitation in section 5 (Discussion, limitations, and future works) as follows: \n\nHowever, it is important to acknowledge that the limitations and drawbacks of superpixels are inherited by the UCBS, which may potentially result in the creation of meaningless concepts. Moreover, the automatic extraction of abstract and complex concepts may be challenging or necessitate additional post-processing. Addressing these challenges presents an intriguing direction for future research, particularly in identifying and surrounding the most influential concepts within bounding boxes, which could help mitigate the issues.\n\nIn addition, in appendix section C.5 (More discussion on limitations: Imperfectly super-pixelated inputs and Dataset biases), we further discussed that this limitation refers to incorporating super-pixelating methods:\n\nMoreover, super-pixelating methods typically offer no solution for tracking entirely or partially abstract concepts such as brightness, daylight, and sun angle in sunrise or sunset images."}
{"claim": "The paper lacks sufficient explanation and guidance on how to interpret the reported global explanations.", "claim_type": "presentation", "paper_id": "UqZecMwLTo", "paper_title": "A Unified Concept-Based System for Local, Global, and Misclassification Explanations", "paper_venue": "iclr2024", "paper_decision": null, "decision": "Unknown", "review_id": "Xnu3bVVbeW", "reviewer": "Reviewer_DJR7", "review_text": "Summary: The authors propose a unified framework for generating both local and global concept based explanations. The proposed framework considers a surrogate model trained with superpixels of an image for generating concepts, which are used then used to extract local and global explanations.\n\nStrengths: - The proposed approach is innovative and can be used to generate and score the relevance of concepts\n- The illustrations of local explanations are well presented, nice framework and illustration figures\n- The paper is well written and easy to follow\n\nWeaknesses: - The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2]. \n- What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n- The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n- I'm unsure of how to interpret global explanations. Can you please provide more discussion for the same? \n- It is a bit unclear how the scores in Table 1 are computed, given the UCBS is fine-tuned on superpixel images; the scores like insertion, deletion, and faithfulness are expected to be high for UCBS, given they are on the manifold, while in case of LIME, SHAPE, GradCAM, and LRP the concepts are off-manifold resulting in lower score. Could you please justify this?\n  \n\n\n[1] Kori, A., Glocker, B. and Toni, F., 2022. GLANCE: Global to Local Architecture-Neutral Concept-based Explanations. _arXiv preprint arXiv:2207.01917_.\n\n[2] Setzu, M., Guidotti, R., Monreale, A., Turini, F., Pedreschi, D. and Giannotti, F., 2021. Glocalx-from local to global explanations of black box ai models. _Artificial Intelligence_, _294_, p.103457.\n\nQuestions: Please refer to weakness section", "labeling_timestamp": "2026-01-11T16:34:06.859541", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The authors' response addresses other reviewer points (prior work, surrogate training data, and limitations regarding abstract concepts) but does not discuss or provide additional explanation/guidance on how to interpret the reported global explanations.", "evidence": "\"Below, we address your concerns mentioned in the Weaknesses.\" \"1. The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2].\" \"2. What is the motivation for having the out-of-distribution dataset to train the surrogate model?\" \"3. The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\"", "combined_author_response": "We are grateful for your thorough review and insightful suggestions.  Below, we address your concerns mentioned in the Weaknesses.\n\n1. The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2].\n\nThanks for your valuable mention. In our paper, we initially stated that \"UCBS is, to the best of our knowledge, one of the first unsupervised unified concept extraction frameworks.\" This claim is based on three important factors: the unified nature encompassing local, global, and misclassification explanations, the focus on concept-based extraction, and the unsupervised nature in terms of explanation labels. We meant to emphasize that there are few concentrated works in these areas targeting vision-based applications, making UCBS possibly one of the pioneering methods. However, in order to avoid potential confusion, we decided to remove this statement.\n\nMore clarification: Ref [2] has primarily been designed for classification tasks on tabular data, and we believe that its applicability to pixel-based data may be limited due to its rule-based nature. Furthermore, Ref [1] offers global explanations in terms of graphs and local explanations in terms of feature importance scores, not concepts.\n\n\n2. What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n\nSurrogate models are indeed binary classifiers that try to fully comprehend the target of interest.  In order to accomplish this, their initial task is to differentiate the target's objects from the non-target samples. Subsequently, they should be able to discriminate the target's concepts and assign them scores based on their significance. The out-of-distribution samples (the non-target samples), which are known relative to the distribution of the target samples, are crucial for achieving the first objective. These samples, in companion with the target samples and the super-pixelated images, are utilized to teach the surrogate network a better representation of the target’s objects as well as the target’s concepts. In order to further clarify this comment, we revised the 5-th paragraph of the introduction as follows:\nIn an analogous case, we engage surrogate explainer networks and equip them with auxiliary datasets to purposely concentrate on the target of interest, rather than a multi-class classification approach. That is, the surrogate networks try to comprehensively learn one target class against the non-target samples. This allows them to thoroughly learn the target's objects as well as their informative concepts.\n\n3. The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n\nNo, the current version of the method is not able to detect abstract and complex concepts like brightness or daylight, which are challenging concepts necessitating additional pre/post-processing tasks. We mentioned this limitation in section 5 (Discussion, limitations, and future works) as follows: \n\nHowever, it is important to acknowledge that the limitations and drawbacks of superpixels are inherited by the UCBS, which may potentially result in the creation of meaningless concepts. Moreover, the automatic extraction of abstract and complex concepts may be challenging or necessitate additional post-processing. Addressing these challenges presents an intriguing direction for future research, particularly in identifying and surrounding the most influential concepts within bounding boxes, which could help mitigate the issues.\n\nIn addition, in appendix section C.5 (More discussion on limitations: Imperfectly super-pixelated inputs and Dataset biases), we further discussed that this limitation refers to incorporating super-pixelating methods:\n\nMoreover, super-pixelating methods typically offer no solution for tracking entirely or partially abstract concepts such as brightness, daylight, and sun angle in sunrise or sunset images.", "author_response": "We are grateful for your thorough review and insightful suggestions.  Below, we address your concerns mentioned in the Weaknesses.\n\n1. The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2].\n\nThanks for your valuable mention. In our paper, we initially stated that \"UCBS is, to the best of our knowledge, one of the first unsupervised unified concept extraction frameworks.\" This claim is based on three important factors: the unified nature encompassing local, global, and misclassification explanations, the focus on concept-based extraction, and the unsupervised nature in terms of explanation labels. We meant to emphasize that there are few concentrated works in these areas targeting vision-based applications, making UCBS possibly one of the pioneering methods. However, in order to avoid potential confusion, we decided to remove this statement.\n\nMore clarification: Ref [2] has primarily been designed for classification tasks on tabular data, and we believe that its applicability to pixel-based data may be limited due to its rule-based nature. Furthermore, Ref [1] offers global explanations in terms of graphs and local explanations in terms of feature importance scores, not concepts.\n\n\n2. What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n\nSurrogate models are indeed binary classifiers that try to fully comprehend the target of interest.  In order to accomplish this, their initial task is to differentiate the target's objects from the non-target samples. Subsequently, they should be able to discriminate the target's concepts and assign them scores based on their significance. The out-of-distribution samples (the non-target samples), which are known relative to the distribution of the target samples, are crucial for achieving the first objective. These samples, in companion with the target samples and the super-pixelated images, are utilized to teach the surrogate network a better representation of the target’s objects as well as the target’s concepts. In order to further clarify this comment, we revised the 5-th paragraph of the introduction as follows:\nIn an analogous case, we engage surrogate explainer networks and equip them with auxiliary datasets to purposely concentrate on the target of interest, rather than a multi-class classification approach. That is, the surrogate networks try to comprehensively learn one target class against the non-target samples. This allows them to thoroughly learn the target's objects as well as their informative concepts.\n\n3. The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n\nNo, the current version of the method is not able to detect abstract and complex concepts like brightness or daylight, which are challenging concepts necessitating additional pre/post-processing tasks. We mentioned this limitation in section 5 (Discussion, limitations, and future works) as follows: \n\nHowever, it is important to acknowledge that the limitations and drawbacks of superpixels are inherited by the UCBS, which may potentially result in the creation of meaningless concepts. Moreover, the automatic extraction of abstract and complex concepts may be challenging or necessitate additional post-processing. Addressing these challenges presents an intriguing direction for future research, particularly in identifying and surrounding the most influential concepts within bounding boxes, which could help mitigate the issues.\n\nIn addition, in appendix section C.5 (More discussion on limitations: Imperfectly super-pixelated inputs and Dataset biases), we further discussed that this limitation refers to incorporating super-pixelating methods:\n\nMoreover, super-pixelating methods typically offer no solution for tracking entirely or partially abstract concepts such as brightness, daylight, and sun angle in sunrise or sunset images."}
{"claim": "It is unclear how the scores reported in Table 1 are computed, given UCBS is fine-tuned on superpixel images.", "claim_type": "experimental", "paper_id": "UqZecMwLTo", "paper_title": "A Unified Concept-Based System for Local, Global, and Misclassification Explanations", "paper_venue": "iclr2024", "paper_decision": null, "decision": "Unknown", "review_id": "Xnu3bVVbeW", "reviewer": "Reviewer_DJR7", "review_text": "Summary: The authors propose a unified framework for generating both local and global concept based explanations. The proposed framework considers a surrogate model trained with superpixels of an image for generating concepts, which are used then used to extract local and global explanations.\n\nStrengths: - The proposed approach is innovative and can be used to generate and score the relevance of concepts\n- The illustrations of local explanations are well presented, nice framework and illustration figures\n- The paper is well written and easy to follow\n\nWeaknesses: - The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2]. \n- What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n- The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n- I'm unsure of how to interpret global explanations. Can you please provide more discussion for the same? \n- It is a bit unclear how the scores in Table 1 are computed, given the UCBS is fine-tuned on superpixel images; the scores like insertion, deletion, and faithfulness are expected to be high for UCBS, given they are on the manifold, while in case of LIME, SHAPE, GradCAM, and LRP the concepts are off-manifold resulting in lower score. Could you please justify this?\n  \n\n\n[1] Kori, A., Glocker, B. and Toni, F., 2022. GLANCE: Global to Local Architecture-Neutral Concept-based Explanations. _arXiv preprint arXiv:2207.01917_.\n\n[2] Setzu, M., Guidotti, R., Monreale, A., Turini, F., Pedreschi, D. and Giannotti, F., 2021. Glocalx-from local to global explanations of black box ai models. _Artificial Intelligence_, _294_, p.103457.\n\nQuestions: Please refer to weakness section", "labeling_timestamp": "2026-01-11T16:34:06.373012", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The authors' response discusses related work, the surrogate model's out-of-distribution data motivation, and limitations regarding abstract concepts, but does not explain how the Table 1 scores were computed or how fine-tuning on superpixel images affects those scores.", "evidence": "\"Below, we address your concerns mentioned in the Weaknesses. 1. The proposed approach is certainly not the first to explore the extraction of local and global explanations... 2. What is the motivation for having the out-of-distribution dataset to train the surrogate model? ... 3. The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\"", "combined_author_response": "We are grateful for your thorough review and insightful suggestions.  Below, we address your concerns mentioned in the Weaknesses.\n\n1. The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2].\n\nThanks for your valuable mention. In our paper, we initially stated that \"UCBS is, to the best of our knowledge, one of the first unsupervised unified concept extraction frameworks.\" This claim is based on three important factors: the unified nature encompassing local, global, and misclassification explanations, the focus on concept-based extraction, and the unsupervised nature in terms of explanation labels. We meant to emphasize that there are few concentrated works in these areas targeting vision-based applications, making UCBS possibly one of the pioneering methods. However, in order to avoid potential confusion, we decided to remove this statement.\n\nMore clarification: Ref [2] has primarily been designed for classification tasks on tabular data, and we believe that its applicability to pixel-based data may be limited due to its rule-based nature. Furthermore, Ref [1] offers global explanations in terms of graphs and local explanations in terms of feature importance scores, not concepts.\n\n\n2. What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n\nSurrogate models are indeed binary classifiers that try to fully comprehend the target of interest.  In order to accomplish this, their initial task is to differentiate the target's objects from the non-target samples. Subsequently, they should be able to discriminate the target's concepts and assign them scores based on their significance. The out-of-distribution samples (the non-target samples), which are known relative to the distribution of the target samples, are crucial for achieving the first objective. These samples, in companion with the target samples and the super-pixelated images, are utilized to teach the surrogate network a better representation of the target’s objects as well as the target’s concepts. In order to further clarify this comment, we revised the 5-th paragraph of the introduction as follows:\nIn an analogous case, we engage surrogate explainer networks and equip them with auxiliary datasets to purposely concentrate on the target of interest, rather than a multi-class classification approach. That is, the surrogate networks try to comprehensively learn one target class against the non-target samples. This allows them to thoroughly learn the target's objects as well as their informative concepts.\n\n3. The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n\nNo, the current version of the method is not able to detect abstract and complex concepts like brightness or daylight, which are challenging concepts necessitating additional pre/post-processing tasks. We mentioned this limitation in section 5 (Discussion, limitations, and future works) as follows: \n\nHowever, it is important to acknowledge that the limitations and drawbacks of superpixels are inherited by the UCBS, which may potentially result in the creation of meaningless concepts. Moreover, the automatic extraction of abstract and complex concepts may be challenging or necessitate additional post-processing. Addressing these challenges presents an intriguing direction for future research, particularly in identifying and surrounding the most influential concepts within bounding boxes, which could help mitigate the issues.\n\nIn addition, in appendix section C.5 (More discussion on limitations: Imperfectly super-pixelated inputs and Dataset biases), we further discussed that this limitation refers to incorporating super-pixelating methods:\n\nMoreover, super-pixelating methods typically offer no solution for tracking entirely or partially abstract concepts such as brightness, daylight, and sun angle in sunrise or sunset images.", "author_response": "We are grateful for your thorough review and insightful suggestions.  Below, we address your concerns mentioned in the Weaknesses.\n\n1. The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2].\n\nThanks for your valuable mention. In our paper, we initially stated that \"UCBS is, to the best of our knowledge, one of the first unsupervised unified concept extraction frameworks.\" This claim is based on three important factors: the unified nature encompassing local, global, and misclassification explanations, the focus on concept-based extraction, and the unsupervised nature in terms of explanation labels. We meant to emphasize that there are few concentrated works in these areas targeting vision-based applications, making UCBS possibly one of the pioneering methods. However, in order to avoid potential confusion, we decided to remove this statement.\n\nMore clarification: Ref [2] has primarily been designed for classification tasks on tabular data, and we believe that its applicability to pixel-based data may be limited due to its rule-based nature. Furthermore, Ref [1] offers global explanations in terms of graphs and local explanations in terms of feature importance scores, not concepts.\n\n\n2. What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n\nSurrogate models are indeed binary classifiers that try to fully comprehend the target of interest.  In order to accomplish this, their initial task is to differentiate the target's objects from the non-target samples. Subsequently, they should be able to discriminate the target's concepts and assign them scores based on their significance. The out-of-distribution samples (the non-target samples), which are known relative to the distribution of the target samples, are crucial for achieving the first objective. These samples, in companion with the target samples and the super-pixelated images, are utilized to teach the surrogate network a better representation of the target’s objects as well as the target’s concepts. In order to further clarify this comment, we revised the 5-th paragraph of the introduction as follows:\nIn an analogous case, we engage surrogate explainer networks and equip them with auxiliary datasets to purposely concentrate on the target of interest, rather than a multi-class classification approach. That is, the surrogate networks try to comprehensively learn one target class against the non-target samples. This allows them to thoroughly learn the target's objects as well as their informative concepts.\n\n3. The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n\nNo, the current version of the method is not able to detect abstract and complex concepts like brightness or daylight, which are challenging concepts necessitating additional pre/post-processing tasks. We mentioned this limitation in section 5 (Discussion, limitations, and future works) as follows: \n\nHowever, it is important to acknowledge that the limitations and drawbacks of superpixels are inherited by the UCBS, which may potentially result in the creation of meaningless concepts. Moreover, the automatic extraction of abstract and complex concepts may be challenging or necessitate additional post-processing. Addressing these challenges presents an intriguing direction for future research, particularly in identifying and surrounding the most influential concepts within bounding boxes, which could help mitigate the issues.\n\nIn addition, in appendix section C.5 (More discussion on limitations: Imperfectly super-pixelated inputs and Dataset biases), we further discussed that this limitation refers to incorporating super-pixelating methods:\n\nMoreover, super-pixelating methods typically offer no solution for tracking entirely or partially abstract concepts such as brightness, daylight, and sun angle in sunrise or sunset images."}
{"claim": "Because UCBS is fine-tuned on superpixel images, insertion, deletion, and faithfulness scores may be artificially inflated for UCBS compared to other methods.", "claim_type": "methodology", "paper_id": "UqZecMwLTo", "paper_title": "A Unified Concept-Based System for Local, Global, and Misclassification Explanations", "paper_venue": "iclr2024", "paper_decision": null, "decision": "Unknown", "review_id": "Xnu3bVVbeW", "reviewer": "Reviewer_DJR7", "review_text": "Summary: The authors propose a unified framework for generating both local and global concept based explanations. The proposed framework considers a surrogate model trained with superpixels of an image for generating concepts, which are used then used to extract local and global explanations.\n\nStrengths: - The proposed approach is innovative and can be used to generate and score the relevance of concepts\n- The illustrations of local explanations are well presented, nice framework and illustration figures\n- The paper is well written and easy to follow\n\nWeaknesses: - The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2]. \n- What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n- The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n- I'm unsure of how to interpret global explanations. Can you please provide more discussion for the same? \n- It is a bit unclear how the scores in Table 1 are computed, given the UCBS is fine-tuned on superpixel images; the scores like insertion, deletion, and faithfulness are expected to be high for UCBS, given they are on the manifold, while in case of LIME, SHAPE, GradCAM, and LRP the concepts are off-manifold resulting in lower score. Could you please justify this?\n  \n\n\n[1] Kori, A., Glocker, B. and Toni, F., 2022. GLANCE: Global to Local Architecture-Neutral Concept-based Explanations. _arXiv preprint arXiv:2207.01917_.\n\n[2] Setzu, M., Guidotti, R., Monreale, A., Turini, F., Pedreschi, D. and Giannotti, F., 2021. Glocalx-from local to global explanations of black box ai models. _Artificial Intelligence_, _294_, p.103457.\n\nQuestions: Please refer to weakness section", "labeling_timestamp": "2026-01-11T16:34:19.300685", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The authors do not address the reviewer's specific concern about insertion/deletion/faithfulness scores being artificially inflated by fine-tuning on superpixel images. Their response discusses prior work, the role of out-of-distribution samples, and general limitations of superpixels (e.g., inability to capture abstract concepts), but never discusses evaluation metrics or potential inflation of insertion/deletion/faithfulness scores.", "evidence": "\"However, it is important to acknowledge that the limitations and drawbacks of superpixels are inherited by the UCBS, which may potentially result in the creation of meaningless concepts.\"", "combined_author_response": "We are grateful for your thorough review and insightful suggestions.  Below, we address your concerns mentioned in the Weaknesses.\n\n1. The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2].\n\nThanks for your valuable mention. In our paper, we initially stated that \"UCBS is, to the best of our knowledge, one of the first unsupervised unified concept extraction frameworks.\" This claim is based on three important factors: the unified nature encompassing local, global, and misclassification explanations, the focus on concept-based extraction, and the unsupervised nature in terms of explanation labels. We meant to emphasize that there are few concentrated works in these areas targeting vision-based applications, making UCBS possibly one of the pioneering methods. However, in order to avoid potential confusion, we decided to remove this statement.\n\nMore clarification: Ref [2] has primarily been designed for classification tasks on tabular data, and we believe that its applicability to pixel-based data may be limited due to its rule-based nature. Furthermore, Ref [1] offers global explanations in terms of graphs and local explanations in terms of feature importance scores, not concepts.\n\n\n2. What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n\nSurrogate models are indeed binary classifiers that try to fully comprehend the target of interest.  In order to accomplish this, their initial task is to differentiate the target's objects from the non-target samples. Subsequently, they should be able to discriminate the target's concepts and assign them scores based on their significance. The out-of-distribution samples (the non-target samples), which are known relative to the distribution of the target samples, are crucial for achieving the first objective. These samples, in companion with the target samples and the super-pixelated images, are utilized to teach the surrogate network a better representation of the target’s objects as well as the target’s concepts. In order to further clarify this comment, we revised the 5-th paragraph of the introduction as follows:\nIn an analogous case, we engage surrogate explainer networks and equip them with auxiliary datasets to purposely concentrate on the target of interest, rather than a multi-class classification approach. That is, the surrogate networks try to comprehensively learn one target class against the non-target samples. This allows them to thoroughly learn the target's objects as well as their informative concepts.\n\n3. The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n\nNo, the current version of the method is not able to detect abstract and complex concepts like brightness or daylight, which are challenging concepts necessitating additional pre/post-processing tasks. We mentioned this limitation in section 5 (Discussion, limitations, and future works) as follows: \n\nHowever, it is important to acknowledge that the limitations and drawbacks of superpixels are inherited by the UCBS, which may potentially result in the creation of meaningless concepts. Moreover, the automatic extraction of abstract and complex concepts may be challenging or necessitate additional post-processing. Addressing these challenges presents an intriguing direction for future research, particularly in identifying and surrounding the most influential concepts within bounding boxes, which could help mitigate the issues.\n\nIn addition, in appendix section C.5 (More discussion on limitations: Imperfectly super-pixelated inputs and Dataset biases), we further discussed that this limitation refers to incorporating super-pixelating methods:\n\nMoreover, super-pixelating methods typically offer no solution for tracking entirely or partially abstract concepts such as brightness, daylight, and sun angle in sunrise or sunset images.", "author_response": "We are grateful for your thorough review and insightful suggestions.  Below, we address your concerns mentioned in the Weaknesses.\n\n1. The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2].\n\nThanks for your valuable mention. In our paper, we initially stated that \"UCBS is, to the best of our knowledge, one of the first unsupervised unified concept extraction frameworks.\" This claim is based on three important factors: the unified nature encompassing local, global, and misclassification explanations, the focus on concept-based extraction, and the unsupervised nature in terms of explanation labels. We meant to emphasize that there are few concentrated works in these areas targeting vision-based applications, making UCBS possibly one of the pioneering methods. However, in order to avoid potential confusion, we decided to remove this statement.\n\nMore clarification: Ref [2] has primarily been designed for classification tasks on tabular data, and we believe that its applicability to pixel-based data may be limited due to its rule-based nature. Furthermore, Ref [1] offers global explanations in terms of graphs and local explanations in terms of feature importance scores, not concepts.\n\n\n2. What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n\nSurrogate models are indeed binary classifiers that try to fully comprehend the target of interest.  In order to accomplish this, their initial task is to differentiate the target's objects from the non-target samples. Subsequently, they should be able to discriminate the target's concepts and assign them scores based on their significance. The out-of-distribution samples (the non-target samples), which are known relative to the distribution of the target samples, are crucial for achieving the first objective. These samples, in companion with the target samples and the super-pixelated images, are utilized to teach the surrogate network a better representation of the target’s objects as well as the target’s concepts. In order to further clarify this comment, we revised the 5-th paragraph of the introduction as follows:\nIn an analogous case, we engage surrogate explainer networks and equip them with auxiliary datasets to purposely concentrate on the target of interest, rather than a multi-class classification approach. That is, the surrogate networks try to comprehensively learn one target class against the non-target samples. This allows them to thoroughly learn the target's objects as well as their informative concepts.\n\n3. The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n\nNo, the current version of the method is not able to detect abstract and complex concepts like brightness or daylight, which are challenging concepts necessitating additional pre/post-processing tasks. We mentioned this limitation in section 5 (Discussion, limitations, and future works) as follows: \n\nHowever, it is important to acknowledge that the limitations and drawbacks of superpixels are inherited by the UCBS, which may potentially result in the creation of meaningless concepts. Moreover, the automatic extraction of abstract and complex concepts may be challenging or necessitate additional post-processing. Addressing these challenges presents an intriguing direction for future research, particularly in identifying and surrounding the most influential concepts within bounding boxes, which could help mitigate the issues.\n\nIn addition, in appendix section C.5 (More discussion on limitations: Imperfectly super-pixelated inputs and Dataset biases), we further discussed that this limitation refers to incorporating super-pixelating methods:\n\nMoreover, super-pixelating methods typically offer no solution for tracking entirely or partially abstract concepts such as brightness, daylight, and sun angle in sunrise or sunset images."}
{"claim": "Comparative methods like LIME, SHAP, GradCAM, and LRP produce off-manifold concepts, which likely leads to lower evaluation scores than on-manifold UCBS.", "claim_type": "baseline", "paper_id": "UqZecMwLTo", "paper_title": "A Unified Concept-Based System for Local, Global, and Misclassification Explanations", "paper_venue": "iclr2024", "paper_decision": null, "decision": "Unknown", "review_id": "Xnu3bVVbeW", "reviewer": "Reviewer_DJR7", "review_text": "Summary: The authors propose a unified framework for generating both local and global concept based explanations. The proposed framework considers a surrogate model trained with superpixels of an image for generating concepts, which are used then used to extract local and global explanations.\n\nStrengths: - The proposed approach is innovative and can be used to generate and score the relevance of concepts\n- The illustrations of local explanations are well presented, nice framework and illustration figures\n- The paper is well written and easy to follow\n\nWeaknesses: - The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2]. \n- What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n- The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n- I'm unsure of how to interpret global explanations. Can you please provide more discussion for the same? \n- It is a bit unclear how the scores in Table 1 are computed, given the UCBS is fine-tuned on superpixel images; the scores like insertion, deletion, and faithfulness are expected to be high for UCBS, given they are on the manifold, while in case of LIME, SHAPE, GradCAM, and LRP the concepts are off-manifold resulting in lower score. Could you please justify this?\n  \n\n\n[1] Kori, A., Glocker, B. and Toni, F., 2022. GLANCE: Global to Local Architecture-Neutral Concept-based Explanations. _arXiv preprint arXiv:2207.01917_.\n\n[2] Setzu, M., Guidotti, R., Monreale, A., Turini, F., Pedreschi, D. and Giannotti, F., 2021. Glocalx-from local to global explanations of black box ai models. _Artificial Intelligence_, _294_, p.103457.\n\nQuestions: Please refer to weakness section", "labeling_timestamp": "2026-01-11T16:34:31.511434", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The authors' response does not address the reviewer's specific claim about comparative methods (LIME, SHAP, GradCAM, LRP) producing off-manifold concepts or how that causes lower evaluation scores compared to UCBS. Their replies cover other points (related work, surrogate training data, and limitations regarding abstract concepts) but make no mention of those comparative methods or the off-manifold issue.", "evidence": "Examples of the response content without addressing the claim: \"The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2].\"; \"Surrogate models are indeed binary classifiers that try to fully comprehend the target of interest.\"; \"No, the current version of the method is not able to detect abstract and complex concepts like brightness or daylight, which are challenging concepts necessitating additional pre/post-processing tasks.\"", "combined_author_response": "We are grateful for your thorough review and insightful suggestions.  Below, we address your concerns mentioned in the Weaknesses.\n\n1. The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2].\n\nThanks for your valuable mention. In our paper, we initially stated that \"UCBS is, to the best of our knowledge, one of the first unsupervised unified concept extraction frameworks.\" This claim is based on three important factors: the unified nature encompassing local, global, and misclassification explanations, the focus on concept-based extraction, and the unsupervised nature in terms of explanation labels. We meant to emphasize that there are few concentrated works in these areas targeting vision-based applications, making UCBS possibly one of the pioneering methods. However, in order to avoid potential confusion, we decided to remove this statement.\n\nMore clarification: Ref [2] has primarily been designed for classification tasks on tabular data, and we believe that its applicability to pixel-based data may be limited due to its rule-based nature. Furthermore, Ref [1] offers global explanations in terms of graphs and local explanations in terms of feature importance scores, not concepts.\n\n\n2. What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n\nSurrogate models are indeed binary classifiers that try to fully comprehend the target of interest.  In order to accomplish this, their initial task is to differentiate the target's objects from the non-target samples. Subsequently, they should be able to discriminate the target's concepts and assign them scores based on their significance. The out-of-distribution samples (the non-target samples), which are known relative to the distribution of the target samples, are crucial for achieving the first objective. These samples, in companion with the target samples and the super-pixelated images, are utilized to teach the surrogate network a better representation of the target’s objects as well as the target’s concepts. In order to further clarify this comment, we revised the 5-th paragraph of the introduction as follows:\nIn an analogous case, we engage surrogate explainer networks and equip them with auxiliary datasets to purposely concentrate on the target of interest, rather than a multi-class classification approach. That is, the surrogate networks try to comprehensively learn one target class against the non-target samples. This allows them to thoroughly learn the target's objects as well as their informative concepts.\n\n3. The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n\nNo, the current version of the method is not able to detect abstract and complex concepts like brightness or daylight, which are challenging concepts necessitating additional pre/post-processing tasks. We mentioned this limitation in section 5 (Discussion, limitations, and future works) as follows: \n\nHowever, it is important to acknowledge that the limitations and drawbacks of superpixels are inherited by the UCBS, which may potentially result in the creation of meaningless concepts. Moreover, the automatic extraction of abstract and complex concepts may be challenging or necessitate additional post-processing. Addressing these challenges presents an intriguing direction for future research, particularly in identifying and surrounding the most influential concepts within bounding boxes, which could help mitigate the issues.\n\nIn addition, in appendix section C.5 (More discussion on limitations: Imperfectly super-pixelated inputs and Dataset biases), we further discussed that this limitation refers to incorporating super-pixelating methods:\n\nMoreover, super-pixelating methods typically offer no solution for tracking entirely or partially abstract concepts such as brightness, daylight, and sun angle in sunrise or sunset images.", "author_response": "We are grateful for your thorough review and insightful suggestions.  Below, we address your concerns mentioned in the Weaknesses.\n\n1. The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2].\n\nThanks for your valuable mention. In our paper, we initially stated that \"UCBS is, to the best of our knowledge, one of the first unsupervised unified concept extraction frameworks.\" This claim is based on three important factors: the unified nature encompassing local, global, and misclassification explanations, the focus on concept-based extraction, and the unsupervised nature in terms of explanation labels. We meant to emphasize that there are few concentrated works in these areas targeting vision-based applications, making UCBS possibly one of the pioneering methods. However, in order to avoid potential confusion, we decided to remove this statement.\n\nMore clarification: Ref [2] has primarily been designed for classification tasks on tabular data, and we believe that its applicability to pixel-based data may be limited due to its rule-based nature. Furthermore, Ref [1] offers global explanations in terms of graphs and local explanations in terms of feature importance scores, not concepts.\n\n\n2. What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n\nSurrogate models are indeed binary classifiers that try to fully comprehend the target of interest.  In order to accomplish this, their initial task is to differentiate the target's objects from the non-target samples. Subsequently, they should be able to discriminate the target's concepts and assign them scores based on their significance. The out-of-distribution samples (the non-target samples), which are known relative to the distribution of the target samples, are crucial for achieving the first objective. These samples, in companion with the target samples and the super-pixelated images, are utilized to teach the surrogate network a better representation of the target’s objects as well as the target’s concepts. In order to further clarify this comment, we revised the 5-th paragraph of the introduction as follows:\nIn an analogous case, we engage surrogate explainer networks and equip them with auxiliary datasets to purposely concentrate on the target of interest, rather than a multi-class classification approach. That is, the surrogate networks try to comprehensively learn one target class against the non-target samples. This allows them to thoroughly learn the target's objects as well as their informative concepts.\n\n3. The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n\nNo, the current version of the method is not able to detect abstract and complex concepts like brightness or daylight, which are challenging concepts necessitating additional pre/post-processing tasks. We mentioned this limitation in section 5 (Discussion, limitations, and future works) as follows: \n\nHowever, it is important to acknowledge that the limitations and drawbacks of superpixels are inherited by the UCBS, which may potentially result in the creation of meaningless concepts. Moreover, the automatic extraction of abstract and complex concepts may be challenging or necessitate additional post-processing. Addressing these challenges presents an intriguing direction for future research, particularly in identifying and surrounding the most influential concepts within bounding boxes, which could help mitigate the issues.\n\nIn addition, in appendix section C.5 (More discussion on limitations: Imperfectly super-pixelated inputs and Dataset biases), we further discussed that this limitation refers to incorporating super-pixelating methods:\n\nMoreover, super-pixelating methods typically offer no solution for tracking entirely or partially abstract concepts such as brightness, daylight, and sun angle in sunrise or sunset images."}
{"claim": "The explanation of the VQ-KD tokenizer in the paper remains insufficient and unclear.", "claim_type": "presentation", "paper_id": "RMmgu49lwn", "paper_title": "Image Understanding Makes for A Good Tokenizer for Image Generation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Q5Or46ndhS", "reviewer": "Reviewer_aFBf", "review_text": "Comment: I really appreciate the author's additional experiments on additional datasets, which better prove the advantages of the method. The author addressed most of my concerns, but I still have some concerns about the explanation of VQ-KD tokenizer.\nI think it is better to keep the original rating.", "labeling_timestamp": "2026-01-11T16:34:19.750861", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "Authors acknowledge the concern and request clarification about what explanation is needed, but they do not provide the missing explanation or additional details in the response.", "evidence": "\"Could you kindly specify which kind of explanation would be most helpful in better supporting the effectiveness of the VQ-KD tokenizer?\"", "combined_author_response": "Dear Reviewer,\n\nThank you for the suggestions that help us improve the paper. As the deadline for discussion is approaching, please let us know if you have any additional questions. We genuinely hope you can consider raising the score if we have satisfactorily addressed your concerns.\n\nThanks again,\n\nThe Authors\n\n---\n\nDear Reviewer,\n\nThank you for the suggestions that help us improve the paper. As the deadline for discussion is approaching, please let us know if you have any additional questions. We genuinely hope you can consider raising the score if we have satisfactorily addressed your concerns.\n\nThanks again,\n\nThe Authors\n\n---\n\nThank you so much for your detailed response and raising your score.\n\n---\n\nDear Reviewer,\n\nThank you for your feedback on our rebuttal. We appreciate your thorough review and would like to ensure we address any remaining concerns you may have. Could you kindly specify which kind of explanation would be most helpful in better supporting the effectiveness of the VQ-KD tokenizer?\n\nThank you once again for your valuable insights.\n\nBest regards,\nThe Authors", "author_response": "Dear Reviewer,\n\nThank you for your feedback on our rebuttal. We appreciate your thorough review and would like to ensure we address any remaining concerns you may have. Could you kindly specify which kind of explanation would be most helpful in better supporting the effectiveness of the VQ-KD tokenizer?\n\nThank you once again for your valuable insights.\n\nBest regards,\nThe Authors"}
{"claim": "The sentence in line 50 ('That is, the prediction is a single (potentially high-dimensional) point (or maybe a small number of such points).') is hard to read.", "claim_type": "presentation", "paper_id": "JEKXTLjEIq", "paper_title": "Binary Search with Distributional Predictions", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BELGkUQqxK", "reviewer": "Reviewer_kf39", "review_text": "Summary: This paper proposes a learning-augmented algorithm for searching in a sorted array. Different from all previous learning-augmented algorithms, it takes in distributional predictions. The main result is an algorithm with query complexity $O(H(p) + \\log \\eta)$, where $H(p)$ is the entropy of the true distribution and $\\eta$ is the Earth Mover's distance between the true and predicted distributions. The paper also includes proofs to show the theoretical optimality and experiments to validate the practical usefulness.\n\nStrengths: - The paper follows the recent line of work on \"learning-augmented algorithm\", or \"algorithm with predictions\". This is a promising new direction that tries to combine the theoretical soundness of classic algorithms with the learning ability of machine learning algorithms.\n- The section on theoretical analysis (though the ideas are simple) is effective.\n- The paper includes experimental results to back up the theory. The experimental settings are diverse. The performance of the proposed algorithm is strong compared to all the baselines.\n\nWeaknesses: - I think the presentation of the paper can be greatly improved. To list a few points:\n    - in line 50, the sentence \"That is, the prediction is a single (potentially high-dimensional) point (or maybe a small number of such points).\" is hard to read for me.\n    - in line 56, the sentence \"Or, can we in fact do better by taking full advantage of the entire predicted distribution.\" should end with a question mark.\n    - the abusive use of the word \"essentially\" greatly weakens the soundness of the paper (For example, in lines 66, 70, 78 87, 88). The expression \"essentially optimal\" should be clarified with collaboration on complexity and constants.\n- I think several key works in the field of learning-augmented algorithms are missing, which makes it hard to position this paper in the correct context. For example, I think the algorithms proposed in \"Learning-Augmented Binary Search Trees\" by Lin et al should be at least discussed and even compared against (now, this paper is only mentioned as a very general reference for learning-augmented data structures).\n- It is not straightforward to me why the techniques used in the proposed algorithm are novel and not hard to come up with. I encourage the author to make a clearer point on \"which components of the proposed algorithm are novel and different from existing techniques\".\n\nQuestions: - The algorithm in [Mitzenmacher and Vassilvitskii, 2021] that searches in a sorted list with predictions receive separate predictions for each query. Is this also the case for the setting discussed in this paper? \n    - If so, as I understand, the proposed algorithm needs to rebuild the binary search tree every time it receives a new query along with its distributional predictions. Then, this would lower bound the time complexity to answer each query with $O(n)$. Is that correct?\n- I am confused by the reference to \"binary search tree\" in the paper (even in the title). Does the proposed algorithm actually require building a binary search tree in its specification and implementation? Why does section 3 not contain any explanation related to the binary search tree?", "labeling_timestamp": "2026-01-11T16:34:25.017667", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The authors discuss the technical content around single-point vs. distributional predictions, but they do not acknowledge or promise to rephrase or clarify the specific sentence in line 50 that the reviewer called \"hard to read.\" There is no explicit response addressing sentence readability or a commit to rewrite that line.", "evidence": "\"We agree that the proof of our specific claim, that one cannot black-box reduce to a single point prediction, does not directly imply that one cannot reduce to *collections* of point predictions.\"", "combined_author_response": "Thank you again very much for the feedback!  We will certainly include the discussions of model 1 and 2, and more comparison to Lin et al., in the paper as you suggest.  \n\n> “The discussion of basic baselines seems insufficient”\n\nWe agree that the proof of our specific claim, that one cannot black-box reduce to a single point prediction, does not directly imply that one cannot reduce to *collections* of point predictions.  However, it is relatively straightforward to generalize our proof to this stronger claim – instead of a 2 point distribution, consider a multi-modal distribution with $d = \\log^2 n$ points. The method suggested by the reviewer, of doing a black-box reduction to MV for each of a collection of point predictions (by running an instance of MV in parallel for each of them), fails for this example.  Either there is no prediction for one of the $d$ points, in which case every instantiation of MV will do many queries, or we do have a prediction for each of the points, and thus, when we run them in parallel we would probe all $d$ of them, again resulting in suboptimal query complexity. Developing an algorithm that can handle such multi-modal distributions (and generalizations) in an optimal manner is a significant part of our contribution.    \n\n> “I still find the use of \"binary search tree\" somewhat confusing.”\n\nWe completely agree that it would be best to change the title of the paper as you suggest. \n\n> “There is a lack of justification for the novelty of \"algorithms with distributional prediction.\"\n\nWe will include a more detailed discussion of Diakonikolas et al. We would like to highlight that our use of distributions is different. Specifically, the work of Diakonikolas et al. does not consider what happens when the distribution is erroneous (a major theme of our work), rather their focus is on minimizing the number of samples from the true distribution that they need. Since predictions are often erroneous, there is a dire need to make sure our usage of predictions is robust. This is what we focus on in this work, and believe would be of interest to the general community.  Of course, the empirical distribution of samples can be viewed as an erroneous prediction of the true distribution, but our setting is far more general, and allows for general distributional predictions with general errors (measured by EMD).  So we do believe that our setting is novel and will lead to significant follow-up work.  \n\n\n> “A new question (minor): In line 303, why set the exponential coefficient of $d$ to 8? If this is purely a result of hyperparameter tuning, is there any justification for why this would be preferable in all cases (rather than overfitting to the experiments)?”\n\nSetting the exponential coefficient of d to a small constant is preferable for improved empirical results, and does not change the asymptotic complexity. To see why, recall that the algorithm explores segments of length $2^{2^i}$ in the $i$-th iteration. When i is very small, these segments are very small, making the iterations overly fast and unlikely to succeed. We found that setting the length to $2^{8 \\cdot 2^i}$ allowed us to balance this trade-off better. The exact setting of the exponential coefficient is not important (4 and 16 worked almost equally well). Also, see the further experimental results in our response to Reviewer EXym which take place on bimodal instances.\n\n> “Results on time complexity.”\n\n We note that in model 2, the time complexity is basically equivalent to the query complexity.  In this model, there is a single distribution over queries, so the time to build our BST is just preprocessing – we expect to answer far more queries than the time it takes to build the BST (as long as the preprocessing time is at least somewhat reasonable, as ours is).  And then once the BST is built, the time it takes to search in it is essentially equal (up to constants involving following pointers) to the query complexity that we analyze.  So in this setting, the query complexity *is* the time complexity!\n\n---\n\nThanks for the clarification.  We agree – on one hand this is similar to how most algorithms with predictions literature uses predictions. That is, the algorithm changes its control flow deterministically based on some information extracted from the prediction. On the other hand, note that the distribution that the algorithm considers is updated from phase to phase, since we need to look at the median of the distribution conditioned on the endpoints of the current search space. So in some ways this is like the online setting mentioned by the reviewer, in that we use a “new” prediction at each step.  But this is not an online problem, and our “new” prediction is just a computational transformation of the old prediction.  So we completely agree that there are many similarities!  But there are also important differences that only arise when considering distributional predictions.\n\nWe also agree that trying to formally understand what makes a distributional prediction better is an interesting open question. For some intuition in the binary search problem, consider the setting where the truth is multimodal (e.g. the distribution has mass of 1/(k+1) at points 0, n/k, 2n/k, …, n}, for intermediate values of k). Here a point prediction will necessarily drop a lot of information, whereas a distribution prediction will preserve the richness of the input space.  This is the essence of our lower bound from Section 2.1, and we believe that this points towards what makes distributional predictions more powerful.  \n\nOverall, we don’t believe that distributional predictions are a silver bullet and should be used in all situations – we (again) completely agree that with the reviewer that “non-distributional learning-augmented algorithms may still hold value”, and, in fact, there are likely many settings where the non-distributional view is at least as useful as the distributional.  But the simple binary search example seems to imply that in the right context, distributional predictions bring a lot of additional power.  Understanding exactly which problems and contexts are a good fit for distributional predictions is a fascinating set of open questions.", "author_response": "Thank you again very much for the feedback!  We will certainly include the discussions of model 1 and 2, and more comparison to Lin et al., in the paper as you suggest.  \n\n> “The discussion of basic baselines seems insufficient”\n\nWe agree that the proof of our specific claim, that one cannot black-box reduce to a single point prediction, does not directly imply that one cannot reduce to *collections* of point predictions.  However, it is relatively straightforward to generalize our proof to this stronger claim – instead of a 2 point distribution, consider a multi-modal distribution with $d = \\log^2 n$ points. The method suggested by the reviewer, of doing a black-box reduction to MV for each of a collection of point predictions (by running an instance of MV in parallel for each of them), fails for this example.  Either there is no prediction for one of the $d$ points, in which case every instantiation of MV will do many queries, or we do have a prediction for each of the points, and thus, when we run them in parallel we would probe all $d$ of them, again resulting in suboptimal query complexity. Developing an algorithm that can handle such multi-modal distributions (and generalizations) in an optimal manner is a significant part of our contribution.    \n\n> “I still find the use of \"binary search tree\" somewhat confusing.”\n\nWe completely agree that it would be best to change the title of the paper as you suggest. \n\n> “There is a lack of justification for the novelty of \"algorithms with distributional prediction.\"\n\nWe will include a more detailed discussion of Diakonikolas et al. We would like to highlight that our use of distributions is different. Specifically, the work of Diakonikolas et al. does not consider what happens when the distribution is erroneous (a major theme of our work), rather their focus is on minimizing the number of samples from the true distribution that they need. Since predictions are often erroneous, there is a dire need to make sure our usage of predictions is robust. This is what we focus on in this work, and believe would be of interest to the general community.  Of course, the empirical distribution of samples can be viewed as an erroneous prediction of the true distribution, but our setting is far more general, and allows for general distributional predictions with general errors (measured by EMD).  So we do believe that our setting is novel and will lead to significant follow-up work.  \n\n\n> “A new question (minor): In line 303, why set the exponential coefficient of $d$ to 8? If this is purely a result of hyperparameter tuning, is there any justification for why this would be preferable in all cases (rather than overfitting to the experiments)?”\n\nSetting the exponential coefficient of d to a small constant is preferable for improved empirical results, and does not change the asymptotic complexity. To see why, recall that the algorithm explores segments of length $2^{2^i}$ in the $i$-th iteration. When i is very small, these segments are very small, making the iterations overly fast and unlikely to succeed. We found that setting the length to $2^{8 \\cdot 2^i}$ allowed us to balance this trade-off better. The exact setting of the exponential coefficient is not important (4 and 16 worked almost equally well). Also, see the further experimental results in our response to Reviewer EXym which take place on bimodal instances.\n\n> “Results on time complexity.”\n\n We note that in model 2, the time complexity is basically equivalent to the query complexity.  In this model, there is a single distribution over queries, so the time to build our BST is just preprocessing – we expect to answer far more queries than the time it takes to build the BST (as long as the preprocessing time is at least somewhat reasonable, as ours is).  And then once the BST is built, the time it takes to search in it is essentially equal (up to constants involving following pointers) to the query complexity that we analyze.  So in this setting, the query complexity *is* the time complexity!"}
{"claim": "The sentence in line 56 ('Or, can we in fact do better by taking full advantage of the entire predicted distribution.') should end with a question mark.", "claim_type": "presentation", "paper_id": "JEKXTLjEIq", "paper_title": "Binary Search with Distributional Predictions", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BELGkUQqxK", "reviewer": "Reviewer_kf39", "review_text": "Summary: This paper proposes a learning-augmented algorithm for searching in a sorted array. Different from all previous learning-augmented algorithms, it takes in distributional predictions. The main result is an algorithm with query complexity $O(H(p) + \\log \\eta)$, where $H(p)$ is the entropy of the true distribution and $\\eta$ is the Earth Mover's distance between the true and predicted distributions. The paper also includes proofs to show the theoretical optimality and experiments to validate the practical usefulness.\n\nStrengths: - The paper follows the recent line of work on \"learning-augmented algorithm\", or \"algorithm with predictions\". This is a promising new direction that tries to combine the theoretical soundness of classic algorithms with the learning ability of machine learning algorithms.\n- The section on theoretical analysis (though the ideas are simple) is effective.\n- The paper includes experimental results to back up the theory. The experimental settings are diverse. The performance of the proposed algorithm is strong compared to all the baselines.\n\nWeaknesses: - I think the presentation of the paper can be greatly improved. To list a few points:\n    - in line 50, the sentence \"That is, the prediction is a single (potentially high-dimensional) point (or maybe a small number of such points).\" is hard to read for me.\n    - in line 56, the sentence \"Or, can we in fact do better by taking full advantage of the entire predicted distribution.\" should end with a question mark.\n    - the abusive use of the word \"essentially\" greatly weakens the soundness of the paper (For example, in lines 66, 70, 78 87, 88). The expression \"essentially optimal\" should be clarified with collaboration on complexity and constants.\n- I think several key works in the field of learning-augmented algorithms are missing, which makes it hard to position this paper in the correct context. For example, I think the algorithms proposed in \"Learning-Augmented Binary Search Trees\" by Lin et al should be at least discussed and even compared against (now, this paper is only mentioned as a very general reference for learning-augmented data structures).\n- It is not straightforward to me why the techniques used in the proposed algorithm are novel and not hard to come up with. I encourage the author to make a clearer point on \"which components of the proposed algorithm are novel and different from existing techniques\".\n\nQuestions: - The algorithm in [Mitzenmacher and Vassilvitskii, 2021] that searches in a sorted list with predictions receive separate predictions for each query. Is this also the case for the setting discussed in this paper? \n    - If so, as I understand, the proposed algorithm needs to rebuild the binary search tree every time it receives a new query along with its distributional predictions. Then, this would lower bound the time complexity to answer each query with $O(n)$. Is that correct?\n- I am confused by the reference to \"binary search tree\" in the paper (even in the title). Does the proposed algorithm actually require building a binary search tree in its specification and implementation? Why does section 3 not contain any explanation related to the binary search tree?", "labeling_timestamp": "2026-01-11T16:34:21.781004", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The authors do not address the specific grammatical/punctuation suggestion about adding a question mark to line 56 anywhere in their response.", "evidence": "\"Thank you again very much for the feedback!  We will certainly include the discussions of model 1 and 2, and more comparison to Lin et al., in the paper as you suggest.\"", "combined_author_response": "Thank you again very much for the feedback!  We will certainly include the discussions of model 1 and 2, and more comparison to Lin et al., in the paper as you suggest.  \n\n> “The discussion of basic baselines seems insufficient”\n\nWe agree that the proof of our specific claim, that one cannot black-box reduce to a single point prediction, does not directly imply that one cannot reduce to *collections* of point predictions.  However, it is relatively straightforward to generalize our proof to this stronger claim – instead of a 2 point distribution, consider a multi-modal distribution with $d = \\log^2 n$ points. The method suggested by the reviewer, of doing a black-box reduction to MV for each of a collection of point predictions (by running an instance of MV in parallel for each of them), fails for this example.  Either there is no prediction for one of the $d$ points, in which case every instantiation of MV will do many queries, or we do have a prediction for each of the points, and thus, when we run them in parallel we would probe all $d$ of them, again resulting in suboptimal query complexity. Developing an algorithm that can handle such multi-modal distributions (and generalizations) in an optimal manner is a significant part of our contribution.    \n\n> “I still find the use of \"binary search tree\" somewhat confusing.”\n\nWe completely agree that it would be best to change the title of the paper as you suggest. \n\n> “There is a lack of justification for the novelty of \"algorithms with distributional prediction.\"\n\nWe will include a more detailed discussion of Diakonikolas et al. We would like to highlight that our use of distributions is different. Specifically, the work of Diakonikolas et al. does not consider what happens when the distribution is erroneous (a major theme of our work), rather their focus is on minimizing the number of samples from the true distribution that they need. Since predictions are often erroneous, there is a dire need to make sure our usage of predictions is robust. This is what we focus on in this work, and believe would be of interest to the general community.  Of course, the empirical distribution of samples can be viewed as an erroneous prediction of the true distribution, but our setting is far more general, and allows for general distributional predictions with general errors (measured by EMD).  So we do believe that our setting is novel and will lead to significant follow-up work.  \n\n\n> “A new question (minor): In line 303, why set the exponential coefficient of $d$ to 8? If this is purely a result of hyperparameter tuning, is there any justification for why this would be preferable in all cases (rather than overfitting to the experiments)?”\n\nSetting the exponential coefficient of d to a small constant is preferable for improved empirical results, and does not change the asymptotic complexity. To see why, recall that the algorithm explores segments of length $2^{2^i}$ in the $i$-th iteration. When i is very small, these segments are very small, making the iterations overly fast and unlikely to succeed. We found that setting the length to $2^{8 \\cdot 2^i}$ allowed us to balance this trade-off better. The exact setting of the exponential coefficient is not important (4 and 16 worked almost equally well). Also, see the further experimental results in our response to Reviewer EXym which take place on bimodal instances.\n\n> “Results on time complexity.”\n\n We note that in model 2, the time complexity is basically equivalent to the query complexity.  In this model, there is a single distribution over queries, so the time to build our BST is just preprocessing – we expect to answer far more queries than the time it takes to build the BST (as long as the preprocessing time is at least somewhat reasonable, as ours is).  And then once the BST is built, the time it takes to search in it is essentially equal (up to constants involving following pointers) to the query complexity that we analyze.  So in this setting, the query complexity *is* the time complexity!\n\n---\n\nThanks for the clarification.  We agree – on one hand this is similar to how most algorithms with predictions literature uses predictions. That is, the algorithm changes its control flow deterministically based on some information extracted from the prediction. On the other hand, note that the distribution that the algorithm considers is updated from phase to phase, since we need to look at the median of the distribution conditioned on the endpoints of the current search space. So in some ways this is like the online setting mentioned by the reviewer, in that we use a “new” prediction at each step.  But this is not an online problem, and our “new” prediction is just a computational transformation of the old prediction.  So we completely agree that there are many similarities!  But there are also important differences that only arise when considering distributional predictions.\n\nWe also agree that trying to formally understand what makes a distributional prediction better is an interesting open question. For some intuition in the binary search problem, consider the setting where the truth is multimodal (e.g. the distribution has mass of 1/(k+1) at points 0, n/k, 2n/k, …, n}, for intermediate values of k). Here a point prediction will necessarily drop a lot of information, whereas a distribution prediction will preserve the richness of the input space.  This is the essence of our lower bound from Section 2.1, and we believe that this points towards what makes distributional predictions more powerful.  \n\nOverall, we don’t believe that distributional predictions are a silver bullet and should be used in all situations – we (again) completely agree that with the reviewer that “non-distributional learning-augmented algorithms may still hold value”, and, in fact, there are likely many settings where the non-distributional view is at least as useful as the distributional.  But the simple binary search example seems to imply that in the right context, distributional predictions bring a lot of additional power.  Understanding exactly which problems and contexts are a good fit for distributional predictions is a fascinating set of open questions.", "author_response": "Thank you again very much for the feedback!  We will certainly include the discussions of model 1 and 2, and more comparison to Lin et al., in the paper as you suggest.  \n\n> “The discussion of basic baselines seems insufficient”\n\nWe agree that the proof of our specific claim, that one cannot black-box reduce to a single point prediction, does not directly imply that one cannot reduce to *collections* of point predictions.  However, it is relatively straightforward to generalize our proof to this stronger claim – instead of a 2 point distribution, consider a multi-modal distribution with $d = \\log^2 n$ points. The method suggested by the reviewer, of doing a black-box reduction to MV for each of a collection of point predictions (by running an instance of MV in parallel for each of them), fails for this example.  Either there is no prediction for one of the $d$ points, in which case every instantiation of MV will do many queries, or we do have a prediction for each of the points, and thus, when we run them in parallel we would probe all $d$ of them, again resulting in suboptimal query complexity. Developing an algorithm that can handle such multi-modal distributions (and generalizations) in an optimal manner is a significant part of our contribution.    \n\n> “I still find the use of \"binary search tree\" somewhat confusing.”\n\nWe completely agree that it would be best to change the title of the paper as you suggest. \n\n> “There is a lack of justification for the novelty of \"algorithms with distributional prediction.\"\n\nWe will include a more detailed discussion of Diakonikolas et al. We would like to highlight that our use of distributions is different. Specifically, the work of Diakonikolas et al. does not consider what happens when the distribution is erroneous (a major theme of our work), rather their focus is on minimizing the number of samples from the true distribution that they need. Since predictions are often erroneous, there is a dire need to make sure our usage of predictions is robust. This is what we focus on in this work, and believe would be of interest to the general community.  Of course, the empirical distribution of samples can be viewed as an erroneous prediction of the true distribution, but our setting is far more general, and allows for general distributional predictions with general errors (measured by EMD).  So we do believe that our setting is novel and will lead to significant follow-up work.  \n\n\n> “A new question (minor): In line 303, why set the exponential coefficient of $d$ to 8? If this is purely a result of hyperparameter tuning, is there any justification for why this would be preferable in all cases (rather than overfitting to the experiments)?”\n\nSetting the exponential coefficient of d to a small constant is preferable for improved empirical results, and does not change the asymptotic complexity. To see why, recall that the algorithm explores segments of length $2^{2^i}$ in the $i$-th iteration. When i is very small, these segments are very small, making the iterations overly fast and unlikely to succeed. We found that setting the length to $2^{8 \\cdot 2^i}$ allowed us to balance this trade-off better. The exact setting of the exponential coefficient is not important (4 and 16 worked almost equally well). Also, see the further experimental results in our response to Reviewer EXym which take place on bimodal instances.\n\n> “Results on time complexity.”\n\n We note that in model 2, the time complexity is basically equivalent to the query complexity.  In this model, there is a single distribution over queries, so the time to build our BST is just preprocessing – we expect to answer far more queries than the time it takes to build the BST (as long as the preprocessing time is at least somewhat reasonable, as ours is).  And then once the BST is built, the time it takes to search in it is essentially equal (up to constants involving following pointers) to the query complexity that we analyze.  So in this setting, the query complexity *is* the time complexity!"}
{"claim": "The paper repeatedly uses the word 'essentially' (lines 66, 70, 78, 87, 88), which weakens the clarity and soundness of its claims.", "claim_type": "presentation", "paper_id": "JEKXTLjEIq", "paper_title": "Binary Search with Distributional Predictions", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BELGkUQqxK", "reviewer": "Reviewer_kf39", "review_text": "Summary: This paper proposes a learning-augmented algorithm for searching in a sorted array. Different from all previous learning-augmented algorithms, it takes in distributional predictions. The main result is an algorithm with query complexity $O(H(p) + \\log \\eta)$, where $H(p)$ is the entropy of the true distribution and $\\eta$ is the Earth Mover's distance between the true and predicted distributions. The paper also includes proofs to show the theoretical optimality and experiments to validate the practical usefulness.\n\nStrengths: - The paper follows the recent line of work on \"learning-augmented algorithm\", or \"algorithm with predictions\". This is a promising new direction that tries to combine the theoretical soundness of classic algorithms with the learning ability of machine learning algorithms.\n- The section on theoretical analysis (though the ideas are simple) is effective.\n- The paper includes experimental results to back up the theory. The experimental settings are diverse. The performance of the proposed algorithm is strong compared to all the baselines.\n\nWeaknesses: - I think the presentation of the paper can be greatly improved. To list a few points:\n    - in line 50, the sentence \"That is, the prediction is a single (potentially high-dimensional) point (or maybe a small number of such points).\" is hard to read for me.\n    - in line 56, the sentence \"Or, can we in fact do better by taking full advantage of the entire predicted distribution.\" should end with a question mark.\n    - the abusive use of the word \"essentially\" greatly weakens the soundness of the paper (For example, in lines 66, 70, 78 87, 88). The expression \"essentially optimal\" should be clarified with collaboration on complexity and constants.\n- I think several key works in the field of learning-augmented algorithms are missing, which makes it hard to position this paper in the correct context. For example, I think the algorithms proposed in \"Learning-Augmented Binary Search Trees\" by Lin et al should be at least discussed and even compared against (now, this paper is only mentioned as a very general reference for learning-augmented data structures).\n- It is not straightforward to me why the techniques used in the proposed algorithm are novel and not hard to come up with. I encourage the author to make a clearer point on \"which components of the proposed algorithm are novel and different from existing techniques\".\n\nQuestions: - The algorithm in [Mitzenmacher and Vassilvitskii, 2021] that searches in a sorted list with predictions receive separate predictions for each query. Is this also the case for the setting discussed in this paper? \n    - If so, as I understand, the proposed algorithm needs to rebuild the binary search tree every time it receives a new query along with its distributional predictions. Then, this would lower bound the time complexity to answer each query with $O(n)$. Is that correct?\n- I am confused by the reference to \"binary search tree\" in the paper (even in the title). Does the proposed algorithm actually require building a binary search tree in its specification and implementation? Why does section 3 not contain any explanation related to the binary search tree?", "labeling_timestamp": "2026-01-11T16:35:00.856222", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The authors' response addresses several substantive reviewer points (baselines, title, novelty, hyperparameters, time complexity) but does not mention or respond to the reviewer’s concern about repeated use of the word 'essentially' or any wording/clarity edits, so it is unclear that this specific stylistic issue will be addressed.", "evidence": "\"Thank you again very much for the feedback!  We will certainly include the discussions of model 1 and 2, and more comparison to Lin et al., in the paper as you suggest.\"", "combined_author_response": "Thank you again very much for the feedback!  We will certainly include the discussions of model 1 and 2, and more comparison to Lin et al., in the paper as you suggest.  \n\n> “The discussion of basic baselines seems insufficient”\n\nWe agree that the proof of our specific claim, that one cannot black-box reduce to a single point prediction, does not directly imply that one cannot reduce to *collections* of point predictions.  However, it is relatively straightforward to generalize our proof to this stronger claim – instead of a 2 point distribution, consider a multi-modal distribution with $d = \\log^2 n$ points. The method suggested by the reviewer, of doing a black-box reduction to MV for each of a collection of point predictions (by running an instance of MV in parallel for each of them), fails for this example.  Either there is no prediction for one of the $d$ points, in which case every instantiation of MV will do many queries, or we do have a prediction for each of the points, and thus, when we run them in parallel we would probe all $d$ of them, again resulting in suboptimal query complexity. Developing an algorithm that can handle such multi-modal distributions (and generalizations) in an optimal manner is a significant part of our contribution.    \n\n> “I still find the use of \"binary search tree\" somewhat confusing.”\n\nWe completely agree that it would be best to change the title of the paper as you suggest. \n\n> “There is a lack of justification for the novelty of \"algorithms with distributional prediction.\"\n\nWe will include a more detailed discussion of Diakonikolas et al. We would like to highlight that our use of distributions is different. Specifically, the work of Diakonikolas et al. does not consider what happens when the distribution is erroneous (a major theme of our work), rather their focus is on minimizing the number of samples from the true distribution that they need. Since predictions are often erroneous, there is a dire need to make sure our usage of predictions is robust. This is what we focus on in this work, and believe would be of interest to the general community.  Of course, the empirical distribution of samples can be viewed as an erroneous prediction of the true distribution, but our setting is far more general, and allows for general distributional predictions with general errors (measured by EMD).  So we do believe that our setting is novel and will lead to significant follow-up work.  \n\n\n> “A new question (minor): In line 303, why set the exponential coefficient of $d$ to 8? If this is purely a result of hyperparameter tuning, is there any justification for why this would be preferable in all cases (rather than overfitting to the experiments)?”\n\nSetting the exponential coefficient of d to a small constant is preferable for improved empirical results, and does not change the asymptotic complexity. To see why, recall that the algorithm explores segments of length $2^{2^i}$ in the $i$-th iteration. When i is very small, these segments are very small, making the iterations overly fast and unlikely to succeed. We found that setting the length to $2^{8 \\cdot 2^i}$ allowed us to balance this trade-off better. The exact setting of the exponential coefficient is not important (4 and 16 worked almost equally well). Also, see the further experimental results in our response to Reviewer EXym which take place on bimodal instances.\n\n> “Results on time complexity.”\n\n We note that in model 2, the time complexity is basically equivalent to the query complexity.  In this model, there is a single distribution over queries, so the time to build our BST is just preprocessing – we expect to answer far more queries than the time it takes to build the BST (as long as the preprocessing time is at least somewhat reasonable, as ours is).  And then once the BST is built, the time it takes to search in it is essentially equal (up to constants involving following pointers) to the query complexity that we analyze.  So in this setting, the query complexity *is* the time complexity!\n\n---\n\nThanks for the clarification.  We agree – on one hand this is similar to how most algorithms with predictions literature uses predictions. That is, the algorithm changes its control flow deterministically based on some information extracted from the prediction. On the other hand, note that the distribution that the algorithm considers is updated from phase to phase, since we need to look at the median of the distribution conditioned on the endpoints of the current search space. So in some ways this is like the online setting mentioned by the reviewer, in that we use a “new” prediction at each step.  But this is not an online problem, and our “new” prediction is just a computational transformation of the old prediction.  So we completely agree that there are many similarities!  But there are also important differences that only arise when considering distributional predictions.\n\nWe also agree that trying to formally understand what makes a distributional prediction better is an interesting open question. For some intuition in the binary search problem, consider the setting where the truth is multimodal (e.g. the distribution has mass of 1/(k+1) at points 0, n/k, 2n/k, …, n}, for intermediate values of k). Here a point prediction will necessarily drop a lot of information, whereas a distribution prediction will preserve the richness of the input space.  This is the essence of our lower bound from Section 2.1, and we believe that this points towards what makes distributional predictions more powerful.  \n\nOverall, we don’t believe that distributional predictions are a silver bullet and should be used in all situations – we (again) completely agree that with the reviewer that “non-distributional learning-augmented algorithms may still hold value”, and, in fact, there are likely many settings where the non-distributional view is at least as useful as the distributional.  But the simple binary search example seems to imply that in the right context, distributional predictions bring a lot of additional power.  Understanding exactly which problems and contexts are a good fit for distributional predictions is a fascinating set of open questions.", "author_response": "Thank you again very much for the feedback!  We will certainly include the discussions of model 1 and 2, and more comparison to Lin et al., in the paper as you suggest.  \n\n> “The discussion of basic baselines seems insufficient”\n\nWe agree that the proof of our specific claim, that one cannot black-box reduce to a single point prediction, does not directly imply that one cannot reduce to *collections* of point predictions.  However, it is relatively straightforward to generalize our proof to this stronger claim – instead of a 2 point distribution, consider a multi-modal distribution with $d = \\log^2 n$ points. The method suggested by the reviewer, of doing a black-box reduction to MV for each of a collection of point predictions (by running an instance of MV in parallel for each of them), fails for this example.  Either there is no prediction for one of the $d$ points, in which case every instantiation of MV will do many queries, or we do have a prediction for each of the points, and thus, when we run them in parallel we would probe all $d$ of them, again resulting in suboptimal query complexity. Developing an algorithm that can handle such multi-modal distributions (and generalizations) in an optimal manner is a significant part of our contribution.    \n\n> “I still find the use of \"binary search tree\" somewhat confusing.”\n\nWe completely agree that it would be best to change the title of the paper as you suggest. \n\n> “There is a lack of justification for the novelty of \"algorithms with distributional prediction.\"\n\nWe will include a more detailed discussion of Diakonikolas et al. We would like to highlight that our use of distributions is different. Specifically, the work of Diakonikolas et al. does not consider what happens when the distribution is erroneous (a major theme of our work), rather their focus is on minimizing the number of samples from the true distribution that they need. Since predictions are often erroneous, there is a dire need to make sure our usage of predictions is robust. This is what we focus on in this work, and believe would be of interest to the general community.  Of course, the empirical distribution of samples can be viewed as an erroneous prediction of the true distribution, but our setting is far more general, and allows for general distributional predictions with general errors (measured by EMD).  So we do believe that our setting is novel and will lead to significant follow-up work.  \n\n\n> “A new question (minor): In line 303, why set the exponential coefficient of $d$ to 8? If this is purely a result of hyperparameter tuning, is there any justification for why this would be preferable in all cases (rather than overfitting to the experiments)?”\n\nSetting the exponential coefficient of d to a small constant is preferable for improved empirical results, and does not change the asymptotic complexity. To see why, recall that the algorithm explores segments of length $2^{2^i}$ in the $i$-th iteration. When i is very small, these segments are very small, making the iterations overly fast and unlikely to succeed. We found that setting the length to $2^{8 \\cdot 2^i}$ allowed us to balance this trade-off better. The exact setting of the exponential coefficient is not important (4 and 16 worked almost equally well). Also, see the further experimental results in our response to Reviewer EXym which take place on bimodal instances.\n\n> “Results on time complexity.”\n\n We note that in model 2, the time complexity is basically equivalent to the query complexity.  In this model, there is a single distribution over queries, so the time to build our BST is just preprocessing – we expect to answer far more queries than the time it takes to build the BST (as long as the preprocessing time is at least somewhat reasonable, as ours is).  And then once the BST is built, the time it takes to search in it is essentially equal (up to constants involving following pointers) to the query complexity that we analyze.  So in this setting, the query complexity *is* the time complexity!"}
{"claim": "The phrase 'essentially optimal' is used without clarifying algorithmic complexity, hidden constants, or precise approximation guarantees.", "claim_type": "methodology", "paper_id": "JEKXTLjEIq", "paper_title": "Binary Search with Distributional Predictions", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BELGkUQqxK", "reviewer": "Reviewer_kf39", "review_text": "Summary: This paper proposes a learning-augmented algorithm for searching in a sorted array. Different from all previous learning-augmented algorithms, it takes in distributional predictions. The main result is an algorithm with query complexity $O(H(p) + \\log \\eta)$, where $H(p)$ is the entropy of the true distribution and $\\eta$ is the Earth Mover's distance between the true and predicted distributions. The paper also includes proofs to show the theoretical optimality and experiments to validate the practical usefulness.\n\nStrengths: - The paper follows the recent line of work on \"learning-augmented algorithm\", or \"algorithm with predictions\". This is a promising new direction that tries to combine the theoretical soundness of classic algorithms with the learning ability of machine learning algorithms.\n- The section on theoretical analysis (though the ideas are simple) is effective.\n- The paper includes experimental results to back up the theory. The experimental settings are diverse. The performance of the proposed algorithm is strong compared to all the baselines.\n\nWeaknesses: - I think the presentation of the paper can be greatly improved. To list a few points:\n    - in line 50, the sentence \"That is, the prediction is a single (potentially high-dimensional) point (or maybe a small number of such points).\" is hard to read for me.\n    - in line 56, the sentence \"Or, can we in fact do better by taking full advantage of the entire predicted distribution.\" should end with a question mark.\n    - the abusive use of the word \"essentially\" greatly weakens the soundness of the paper (For example, in lines 66, 70, 78 87, 88). The expression \"essentially optimal\" should be clarified with collaboration on complexity and constants.\n- I think several key works in the field of learning-augmented algorithms are missing, which makes it hard to position this paper in the correct context. For example, I think the algorithms proposed in \"Learning-Augmented Binary Search Trees\" by Lin et al should be at least discussed and even compared against (now, this paper is only mentioned as a very general reference for learning-augmented data structures).\n- It is not straightforward to me why the techniques used in the proposed algorithm are novel and not hard to come up with. I encourage the author to make a clearer point on \"which components of the proposed algorithm are novel and different from existing techniques\".\n\nQuestions: - The algorithm in [Mitzenmacher and Vassilvitskii, 2021] that searches in a sorted list with predictions receive separate predictions for each query. Is this also the case for the setting discussed in this paper? \n    - If so, as I understand, the proposed algorithm needs to rebuild the binary search tree every time it receives a new query along with its distributional predictions. Then, this would lower bound the time complexity to answer each query with $O(n)$. Is that correct?\n- I am confused by the reference to \"binary search tree\" in the paper (even in the title). Does the proposed algorithm actually require building a binary search tree in its specification and implementation? Why does section 3 not contain any explanation related to the binary search tree?", "labeling_timestamp": "2026-01-11T16:34:52.046130", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "Authors acknowledge time vs. query complexity and promise to add discussions, and they note asymptotic invariance of a tuning constant; however they do not provide the missing hidden constants or precise approximation guarantees the reviewer requested.", "evidence": "\"We will certainly include the discussions of model 1 and 2, and more comparison to Lin et al., in the paper as you suggest.\" \"Setting the exponential coefficient of d to a small constant is preferable for improved empirical results, and does not change the asymptotic complexity.\" \"We note that in model 2, the time complexity is basically equivalent to the query complexity. ... And then once the BST is built, the time it takes to search in it is essentially equal (up to constants involving following pointers) to the query complexity that we analyze. So in this setting, the query complexity *is* the time complexity!\"", "combined_author_response": "Thank you again very much for the feedback!  We will certainly include the discussions of model 1 and 2, and more comparison to Lin et al., in the paper as you suggest.  \n\n> “The discussion of basic baselines seems insufficient”\n\nWe agree that the proof of our specific claim, that one cannot black-box reduce to a single point prediction, does not directly imply that one cannot reduce to *collections* of point predictions.  However, it is relatively straightforward to generalize our proof to this stronger claim – instead of a 2 point distribution, consider a multi-modal distribution with $d = \\log^2 n$ points. The method suggested by the reviewer, of doing a black-box reduction to MV for each of a collection of point predictions (by running an instance of MV in parallel for each of them), fails for this example.  Either there is no prediction for one of the $d$ points, in which case every instantiation of MV will do many queries, or we do have a prediction for each of the points, and thus, when we run them in parallel we would probe all $d$ of them, again resulting in suboptimal query complexity. Developing an algorithm that can handle such multi-modal distributions (and generalizations) in an optimal manner is a significant part of our contribution.    \n\n> “I still find the use of \"binary search tree\" somewhat confusing.”\n\nWe completely agree that it would be best to change the title of the paper as you suggest. \n\n> “There is a lack of justification for the novelty of \"algorithms with distributional prediction.\"\n\nWe will include a more detailed discussion of Diakonikolas et al. We would like to highlight that our use of distributions is different. Specifically, the work of Diakonikolas et al. does not consider what happens when the distribution is erroneous (a major theme of our work), rather their focus is on minimizing the number of samples from the true distribution that they need. Since predictions are often erroneous, there is a dire need to make sure our usage of predictions is robust. This is what we focus on in this work, and believe would be of interest to the general community.  Of course, the empirical distribution of samples can be viewed as an erroneous prediction of the true distribution, but our setting is far more general, and allows for general distributional predictions with general errors (measured by EMD).  So we do believe that our setting is novel and will lead to significant follow-up work.  \n\n\n> “A new question (minor): In line 303, why set the exponential coefficient of $d$ to 8? If this is purely a result of hyperparameter tuning, is there any justification for why this would be preferable in all cases (rather than overfitting to the experiments)?”\n\nSetting the exponential coefficient of d to a small constant is preferable for improved empirical results, and does not change the asymptotic complexity. To see why, recall that the algorithm explores segments of length $2^{2^i}$ in the $i$-th iteration. When i is very small, these segments are very small, making the iterations overly fast and unlikely to succeed. We found that setting the length to $2^{8 \\cdot 2^i}$ allowed us to balance this trade-off better. The exact setting of the exponential coefficient is not important (4 and 16 worked almost equally well). Also, see the further experimental results in our response to Reviewer EXym which take place on bimodal instances.\n\n> “Results on time complexity.”\n\n We note that in model 2, the time complexity is basically equivalent to the query complexity.  In this model, there is a single distribution over queries, so the time to build our BST is just preprocessing – we expect to answer far more queries than the time it takes to build the BST (as long as the preprocessing time is at least somewhat reasonable, as ours is).  And then once the BST is built, the time it takes to search in it is essentially equal (up to constants involving following pointers) to the query complexity that we analyze.  So in this setting, the query complexity *is* the time complexity!\n\n---\n\nThanks for the clarification.  We agree – on one hand this is similar to how most algorithms with predictions literature uses predictions. That is, the algorithm changes its control flow deterministically based on some information extracted from the prediction. On the other hand, note that the distribution that the algorithm considers is updated from phase to phase, since we need to look at the median of the distribution conditioned on the endpoints of the current search space. So in some ways this is like the online setting mentioned by the reviewer, in that we use a “new” prediction at each step.  But this is not an online problem, and our “new” prediction is just a computational transformation of the old prediction.  So we completely agree that there are many similarities!  But there are also important differences that only arise when considering distributional predictions.\n\nWe also agree that trying to formally understand what makes a distributional prediction better is an interesting open question. For some intuition in the binary search problem, consider the setting where the truth is multimodal (e.g. the distribution has mass of 1/(k+1) at points 0, n/k, 2n/k, …, n}, for intermediate values of k). Here a point prediction will necessarily drop a lot of information, whereas a distribution prediction will preserve the richness of the input space.  This is the essence of our lower bound from Section 2.1, and we believe that this points towards what makes distributional predictions more powerful.  \n\nOverall, we don’t believe that distributional predictions are a silver bullet and should be used in all situations – we (again) completely agree that with the reviewer that “non-distributional learning-augmented algorithms may still hold value”, and, in fact, there are likely many settings where the non-distributional view is at least as useful as the distributional.  But the simple binary search example seems to imply that in the right context, distributional predictions bring a lot of additional power.  Understanding exactly which problems and contexts are a good fit for distributional predictions is a fascinating set of open questions.", "author_response": "Thank you again very much for the feedback!  We will certainly include the discussions of model 1 and 2, and more comparison to Lin et al., in the paper as you suggest.  \n\n> “The discussion of basic baselines seems insufficient”\n\nWe agree that the proof of our specific claim, that one cannot black-box reduce to a single point prediction, does not directly imply that one cannot reduce to *collections* of point predictions.  However, it is relatively straightforward to generalize our proof to this stronger claim – instead of a 2 point distribution, consider a multi-modal distribution with $d = \\log^2 n$ points. The method suggested by the reviewer, of doing a black-box reduction to MV for each of a collection of point predictions (by running an instance of MV in parallel for each of them), fails for this example.  Either there is no prediction for one of the $d$ points, in which case every instantiation of MV will do many queries, or we do have a prediction for each of the points, and thus, when we run them in parallel we would probe all $d$ of them, again resulting in suboptimal query complexity. Developing an algorithm that can handle such multi-modal distributions (and generalizations) in an optimal manner is a significant part of our contribution.    \n\n> “I still find the use of \"binary search tree\" somewhat confusing.”\n\nWe completely agree that it would be best to change the title of the paper as you suggest. \n\n> “There is a lack of justification for the novelty of \"algorithms with distributional prediction.\"\n\nWe will include a more detailed discussion of Diakonikolas et al. We would like to highlight that our use of distributions is different. Specifically, the work of Diakonikolas et al. does not consider what happens when the distribution is erroneous (a major theme of our work), rather their focus is on minimizing the number of samples from the true distribution that they need. Since predictions are often erroneous, there is a dire need to make sure our usage of predictions is robust. This is what we focus on in this work, and believe would be of interest to the general community.  Of course, the empirical distribution of samples can be viewed as an erroneous prediction of the true distribution, but our setting is far more general, and allows for general distributional predictions with general errors (measured by EMD).  So we do believe that our setting is novel and will lead to significant follow-up work.  \n\n\n> “A new question (minor): In line 303, why set the exponential coefficient of $d$ to 8? If this is purely a result of hyperparameter tuning, is there any justification for why this would be preferable in all cases (rather than overfitting to the experiments)?”\n\nSetting the exponential coefficient of d to a small constant is preferable for improved empirical results, and does not change the asymptotic complexity. To see why, recall that the algorithm explores segments of length $2^{2^i}$ in the $i$-th iteration. When i is very small, these segments are very small, making the iterations overly fast and unlikely to succeed. We found that setting the length to $2^{8 \\cdot 2^i}$ allowed us to balance this trade-off better. The exact setting of the exponential coefficient is not important (4 and 16 worked almost equally well). Also, see the further experimental results in our response to Reviewer EXym which take place on bimodal instances.\n\n> “Results on time complexity.”\n\n We note that in model 2, the time complexity is basically equivalent to the query complexity.  In this model, there is a single distribution over queries, so the time to build our BST is just preprocessing – we expect to answer far more queries than the time it takes to build the BST (as long as the preprocessing time is at least somewhat reasonable, as ours is).  And then once the BST is built, the time it takes to search in it is essentially equal (up to constants involving following pointers) to the query complexity that we analyze.  So in this setting, the query complexity *is* the time complexity!"}
{"claim": "The paper omits discussion and empirical or theoretical comparison with 'Learning-Augmented Binary Search Trees' by Lin et al., a directly relevant recent work.", "claim_type": "subjective", "paper_id": "JEKXTLjEIq", "paper_title": "Binary Search with Distributional Predictions", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BELGkUQqxK", "reviewer": "Reviewer_kf39", "review_text": "Summary: This paper proposes a learning-augmented algorithm for searching in a sorted array. Different from all previous learning-augmented algorithms, it takes in distributional predictions. The main result is an algorithm with query complexity $O(H(p) + \\log \\eta)$, where $H(p)$ is the entropy of the true distribution and $\\eta$ is the Earth Mover's distance between the true and predicted distributions. The paper also includes proofs to show the theoretical optimality and experiments to validate the practical usefulness.\n\nStrengths: - The paper follows the recent line of work on \"learning-augmented algorithm\", or \"algorithm with predictions\". This is a promising new direction that tries to combine the theoretical soundness of classic algorithms with the learning ability of machine learning algorithms.\n- The section on theoretical analysis (though the ideas are simple) is effective.\n- The paper includes experimental results to back up the theory. The experimental settings are diverse. The performance of the proposed algorithm is strong compared to all the baselines.\n\nWeaknesses: - I think the presentation of the paper can be greatly improved. To list a few points:\n    - in line 50, the sentence \"That is, the prediction is a single (potentially high-dimensional) point (or maybe a small number of such points).\" is hard to read for me.\n    - in line 56, the sentence \"Or, can we in fact do better by taking full advantage of the entire predicted distribution.\" should end with a question mark.\n    - the abusive use of the word \"essentially\" greatly weakens the soundness of the paper (For example, in lines 66, 70, 78 87, 88). The expression \"essentially optimal\" should be clarified with collaboration on complexity and constants.\n- I think several key works in the field of learning-augmented algorithms are missing, which makes it hard to position this paper in the correct context. For example, I think the algorithms proposed in \"Learning-Augmented Binary Search Trees\" by Lin et al should be at least discussed and even compared against (now, this paper is only mentioned as a very general reference for learning-augmented data structures).\n- It is not straightforward to me why the techniques used in the proposed algorithm are novel and not hard to come up with. I encourage the author to make a clearer point on \"which components of the proposed algorithm are novel and different from existing techniques\".\n\nQuestions: - The algorithm in [Mitzenmacher and Vassilvitskii, 2021] that searches in a sorted list with predictions receive separate predictions for each query. Is this also the case for the setting discussed in this paper? \n    - If so, as I understand, the proposed algorithm needs to rebuild the binary search tree every time it receives a new query along with its distributional predictions. Then, this would lower bound the time complexity to answer each query with $O(n)$. Is that correct?\n- I am confused by the reference to \"binary search tree\" in the paper (even in the title). Does the proposed algorithm actually require building a binary search tree in its specification and implementation? Why does section 3 not contain any explanation related to the binary search tree?", "labeling_timestamp": "2026-01-11T16:34:37.182238", "model": "gpt-5-mini", "label": "Supported", "justification": "Authors explicitly acknowledge the omission and commit to adding discussion and more comparison to Lin et al.; they agree to revise the paper accordingly.", "evidence": "We will certainly include the discussions of model 1 and 2, and more comparison to Lin et al., in the paper as you suggest.", "combined_author_response": "Thank you again very much for the feedback!  We will certainly include the discussions of model 1 and 2, and more comparison to Lin et al., in the paper as you suggest.  \n\n> “The discussion of basic baselines seems insufficient”\n\nWe agree that the proof of our specific claim, that one cannot black-box reduce to a single point prediction, does not directly imply that one cannot reduce to *collections* of point predictions.  However, it is relatively straightforward to generalize our proof to this stronger claim – instead of a 2 point distribution, consider a multi-modal distribution with $d = \\log^2 n$ points. The method suggested by the reviewer, of doing a black-box reduction to MV for each of a collection of point predictions (by running an instance of MV in parallel for each of them), fails for this example.  Either there is no prediction for one of the $d$ points, in which case every instantiation of MV will do many queries, or we do have a prediction for each of the points, and thus, when we run them in parallel we would probe all $d$ of them, again resulting in suboptimal query complexity. Developing an algorithm that can handle such multi-modal distributions (and generalizations) in an optimal manner is a significant part of our contribution.    \n\n> “I still find the use of \"binary search tree\" somewhat confusing.”\n\nWe completely agree that it would be best to change the title of the paper as you suggest. \n\n> “There is a lack of justification for the novelty of \"algorithms with distributional prediction.\"\n\nWe will include a more detailed discussion of Diakonikolas et al. We would like to highlight that our use of distributions is different. Specifically, the work of Diakonikolas et al. does not consider what happens when the distribution is erroneous (a major theme of our work), rather their focus is on minimizing the number of samples from the true distribution that they need. Since predictions are often erroneous, there is a dire need to make sure our usage of predictions is robust. This is what we focus on in this work, and believe would be of interest to the general community.  Of course, the empirical distribution of samples can be viewed as an erroneous prediction of the true distribution, but our setting is far more general, and allows for general distributional predictions with general errors (measured by EMD).  So we do believe that our setting is novel and will lead to significant follow-up work.  \n\n\n> “A new question (minor): In line 303, why set the exponential coefficient of $d$ to 8? If this is purely a result of hyperparameter tuning, is there any justification for why this would be preferable in all cases (rather than overfitting to the experiments)?”\n\nSetting the exponential coefficient of d to a small constant is preferable for improved empirical results, and does not change the asymptotic complexity. To see why, recall that the algorithm explores segments of length $2^{2^i}$ in the $i$-th iteration. When i is very small, these segments are very small, making the iterations overly fast and unlikely to succeed. We found that setting the length to $2^{8 \\cdot 2^i}$ allowed us to balance this trade-off better. The exact setting of the exponential coefficient is not important (4 and 16 worked almost equally well). Also, see the further experimental results in our response to Reviewer EXym which take place on bimodal instances.\n\n> “Results on time complexity.”\n\n We note that in model 2, the time complexity is basically equivalent to the query complexity.  In this model, there is a single distribution over queries, so the time to build our BST is just preprocessing – we expect to answer far more queries than the time it takes to build the BST (as long as the preprocessing time is at least somewhat reasonable, as ours is).  And then once the BST is built, the time it takes to search in it is essentially equal (up to constants involving following pointers) to the query complexity that we analyze.  So in this setting, the query complexity *is* the time complexity!\n\n---\n\nThanks for the clarification.  We agree – on one hand this is similar to how most algorithms with predictions literature uses predictions. That is, the algorithm changes its control flow deterministically based on some information extracted from the prediction. On the other hand, note that the distribution that the algorithm considers is updated from phase to phase, since we need to look at the median of the distribution conditioned on the endpoints of the current search space. So in some ways this is like the online setting mentioned by the reviewer, in that we use a “new” prediction at each step.  But this is not an online problem, and our “new” prediction is just a computational transformation of the old prediction.  So we completely agree that there are many similarities!  But there are also important differences that only arise when considering distributional predictions.\n\nWe also agree that trying to formally understand what makes a distributional prediction better is an interesting open question. For some intuition in the binary search problem, consider the setting where the truth is multimodal (e.g. the distribution has mass of 1/(k+1) at points 0, n/k, 2n/k, …, n}, for intermediate values of k). Here a point prediction will necessarily drop a lot of information, whereas a distribution prediction will preserve the richness of the input space.  This is the essence of our lower bound from Section 2.1, and we believe that this points towards what makes distributional predictions more powerful.  \n\nOverall, we don’t believe that distributional predictions are a silver bullet and should be used in all situations – we (again) completely agree that with the reviewer that “non-distributional learning-augmented algorithms may still hold value”, and, in fact, there are likely many settings where the non-distributional view is at least as useful as the distributional.  But the simple binary search example seems to imply that in the right context, distributional predictions bring a lot of additional power.  Understanding exactly which problems and contexts are a good fit for distributional predictions is a fascinating set of open questions.", "author_response": "Thank you again very much for the feedback!  We will certainly include the discussions of model 1 and 2, and more comparison to Lin et al., in the paper as you suggest.  \n\n> “The discussion of basic baselines seems insufficient”\n\nWe agree that the proof of our specific claim, that one cannot black-box reduce to a single point prediction, does not directly imply that one cannot reduce to *collections* of point predictions.  However, it is relatively straightforward to generalize our proof to this stronger claim – instead of a 2 point distribution, consider a multi-modal distribution with $d = \\log^2 n$ points. The method suggested by the reviewer, of doing a black-box reduction to MV for each of a collection of point predictions (by running an instance of MV in parallel for each of them), fails for this example.  Either there is no prediction for one of the $d$ points, in which case every instantiation of MV will do many queries, or we do have a prediction for each of the points, and thus, when we run them in parallel we would probe all $d$ of them, again resulting in suboptimal query complexity. Developing an algorithm that can handle such multi-modal distributions (and generalizations) in an optimal manner is a significant part of our contribution.    \n\n> “I still find the use of \"binary search tree\" somewhat confusing.”\n\nWe completely agree that it would be best to change the title of the paper as you suggest. \n\n> “There is a lack of justification for the novelty of \"algorithms with distributional prediction.\"\n\nWe will include a more detailed discussion of Diakonikolas et al. We would like to highlight that our use of distributions is different. Specifically, the work of Diakonikolas et al. does not consider what happens when the distribution is erroneous (a major theme of our work), rather their focus is on minimizing the number of samples from the true distribution that they need. Since predictions are often erroneous, there is a dire need to make sure our usage of predictions is robust. This is what we focus on in this work, and believe would be of interest to the general community.  Of course, the empirical distribution of samples can be viewed as an erroneous prediction of the true distribution, but our setting is far more general, and allows for general distributional predictions with general errors (measured by EMD).  So we do believe that our setting is novel and will lead to significant follow-up work.  \n\n\n> “A new question (minor): In line 303, why set the exponential coefficient of $d$ to 8? If this is purely a result of hyperparameter tuning, is there any justification for why this would be preferable in all cases (rather than overfitting to the experiments)?”\n\nSetting the exponential coefficient of d to a small constant is preferable for improved empirical results, and does not change the asymptotic complexity. To see why, recall that the algorithm explores segments of length $2^{2^i}$ in the $i$-th iteration. When i is very small, these segments are very small, making the iterations overly fast and unlikely to succeed. We found that setting the length to $2^{8 \\cdot 2^i}$ allowed us to balance this trade-off better. The exact setting of the exponential coefficient is not important (4 and 16 worked almost equally well). Also, see the further experimental results in our response to Reviewer EXym which take place on bimodal instances.\n\n> “Results on time complexity.”\n\n We note that in model 2, the time complexity is basically equivalent to the query complexity.  In this model, there is a single distribution over queries, so the time to build our BST is just preprocessing – we expect to answer far more queries than the time it takes to build the BST (as long as the preprocessing time is at least somewhat reasonable, as ours is).  And then once the BST is built, the time it takes to search in it is essentially equal (up to constants involving following pointers) to the query complexity that we analyze.  So in this setting, the query complexity *is* the time complexity!"}
{"claim": "Several other key works in learning-augmented algorithms are missing, making it difficult to position this paper within the literature.", "claim_type": "subjective", "paper_id": "JEKXTLjEIq", "paper_title": "Binary Search with Distributional Predictions", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BELGkUQqxK", "reviewer": "Reviewer_kf39", "review_text": "Summary: This paper proposes a learning-augmented algorithm for searching in a sorted array. Different from all previous learning-augmented algorithms, it takes in distributional predictions. The main result is an algorithm with query complexity $O(H(p) + \\log \\eta)$, where $H(p)$ is the entropy of the true distribution and $\\eta$ is the Earth Mover's distance between the true and predicted distributions. The paper also includes proofs to show the theoretical optimality and experiments to validate the practical usefulness.\n\nStrengths: - The paper follows the recent line of work on \"learning-augmented algorithm\", or \"algorithm with predictions\". This is a promising new direction that tries to combine the theoretical soundness of classic algorithms with the learning ability of machine learning algorithms.\n- The section on theoretical analysis (though the ideas are simple) is effective.\n- The paper includes experimental results to back up the theory. The experimental settings are diverse. The performance of the proposed algorithm is strong compared to all the baselines.\n\nWeaknesses: - I think the presentation of the paper can be greatly improved. To list a few points:\n    - in line 50, the sentence \"That is, the prediction is a single (potentially high-dimensional) point (or maybe a small number of such points).\" is hard to read for me.\n    - in line 56, the sentence \"Or, can we in fact do better by taking full advantage of the entire predicted distribution.\" should end with a question mark.\n    - the abusive use of the word \"essentially\" greatly weakens the soundness of the paper (For example, in lines 66, 70, 78 87, 88). The expression \"essentially optimal\" should be clarified with collaboration on complexity and constants.\n- I think several key works in the field of learning-augmented algorithms are missing, which makes it hard to position this paper in the correct context. For example, I think the algorithms proposed in \"Learning-Augmented Binary Search Trees\" by Lin et al should be at least discussed and even compared against (now, this paper is only mentioned as a very general reference for learning-augmented data structures).\n- It is not straightforward to me why the techniques used in the proposed algorithm are novel and not hard to come up with. I encourage the author to make a clearer point on \"which components of the proposed algorithm are novel and different from existing techniques\".\n\nQuestions: - The algorithm in [Mitzenmacher and Vassilvitskii, 2021] that searches in a sorted list with predictions receive separate predictions for each query. Is this also the case for the setting discussed in this paper? \n    - If so, as I understand, the proposed algorithm needs to rebuild the binary search tree every time it receives a new query along with its distributional predictions. Then, this would lower bound the time complexity to answer each query with $O(n)$. Is that correct?\n- I am confused by the reference to \"binary search tree\" in the paper (even in the title). Does the proposed algorithm actually require building a binary search tree in its specification and implementation? Why does section 3 not contain any explanation related to the binary search tree?", "labeling_timestamp": "2026-01-11T16:35:04.162140", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors acknowledge the omission and commit to adding the missing discussions and comparisons (Lin et al., Diakonikolas et al., models 1 and 2), while also explaining how their work differs to justify novelty.", "evidence": "\"We will certainly include the discussions of model 1 and 2, and more comparison to Lin et al., in the paper as you suggest.\" \"We will include a more detailed discussion of Diakonikolas et al. We would like to highlight that our use of distributions is different.\"", "combined_author_response": "Thank you again very much for the feedback!  We will certainly include the discussions of model 1 and 2, and more comparison to Lin et al., in the paper as you suggest.  \n\n> “The discussion of basic baselines seems insufficient”\n\nWe agree that the proof of our specific claim, that one cannot black-box reduce to a single point prediction, does not directly imply that one cannot reduce to *collections* of point predictions.  However, it is relatively straightforward to generalize our proof to this stronger claim – instead of a 2 point distribution, consider a multi-modal distribution with $d = \\log^2 n$ points. The method suggested by the reviewer, of doing a black-box reduction to MV for each of a collection of point predictions (by running an instance of MV in parallel for each of them), fails for this example.  Either there is no prediction for one of the $d$ points, in which case every instantiation of MV will do many queries, or we do have a prediction for each of the points, and thus, when we run them in parallel we would probe all $d$ of them, again resulting in suboptimal query complexity. Developing an algorithm that can handle such multi-modal distributions (and generalizations) in an optimal manner is a significant part of our contribution.    \n\n> “I still find the use of \"binary search tree\" somewhat confusing.”\n\nWe completely agree that it would be best to change the title of the paper as you suggest. \n\n> “There is a lack of justification for the novelty of \"algorithms with distributional prediction.\"\n\nWe will include a more detailed discussion of Diakonikolas et al. We would like to highlight that our use of distributions is different. Specifically, the work of Diakonikolas et al. does not consider what happens when the distribution is erroneous (a major theme of our work), rather their focus is on minimizing the number of samples from the true distribution that they need. Since predictions are often erroneous, there is a dire need to make sure our usage of predictions is robust. This is what we focus on in this work, and believe would be of interest to the general community.  Of course, the empirical distribution of samples can be viewed as an erroneous prediction of the true distribution, but our setting is far more general, and allows for general distributional predictions with general errors (measured by EMD).  So we do believe that our setting is novel and will lead to significant follow-up work.  \n\n\n> “A new question (minor): In line 303, why set the exponential coefficient of $d$ to 8? If this is purely a result of hyperparameter tuning, is there any justification for why this would be preferable in all cases (rather than overfitting to the experiments)?”\n\nSetting the exponential coefficient of d to a small constant is preferable for improved empirical results, and does not change the asymptotic complexity. To see why, recall that the algorithm explores segments of length $2^{2^i}$ in the $i$-th iteration. When i is very small, these segments are very small, making the iterations overly fast and unlikely to succeed. We found that setting the length to $2^{8 \\cdot 2^i}$ allowed us to balance this trade-off better. The exact setting of the exponential coefficient is not important (4 and 16 worked almost equally well). Also, see the further experimental results in our response to Reviewer EXym which take place on bimodal instances.\n\n> “Results on time complexity.”\n\n We note that in model 2, the time complexity is basically equivalent to the query complexity.  In this model, there is a single distribution over queries, so the time to build our BST is just preprocessing – we expect to answer far more queries than the time it takes to build the BST (as long as the preprocessing time is at least somewhat reasonable, as ours is).  And then once the BST is built, the time it takes to search in it is essentially equal (up to constants involving following pointers) to the query complexity that we analyze.  So in this setting, the query complexity *is* the time complexity!\n\n---\n\nThanks for the clarification.  We agree – on one hand this is similar to how most algorithms with predictions literature uses predictions. That is, the algorithm changes its control flow deterministically based on some information extracted from the prediction. On the other hand, note that the distribution that the algorithm considers is updated from phase to phase, since we need to look at the median of the distribution conditioned on the endpoints of the current search space. So in some ways this is like the online setting mentioned by the reviewer, in that we use a “new” prediction at each step.  But this is not an online problem, and our “new” prediction is just a computational transformation of the old prediction.  So we completely agree that there are many similarities!  But there are also important differences that only arise when considering distributional predictions.\n\nWe also agree that trying to formally understand what makes a distributional prediction better is an interesting open question. For some intuition in the binary search problem, consider the setting where the truth is multimodal (e.g. the distribution has mass of 1/(k+1) at points 0, n/k, 2n/k, …, n}, for intermediate values of k). Here a point prediction will necessarily drop a lot of information, whereas a distribution prediction will preserve the richness of the input space.  This is the essence of our lower bound from Section 2.1, and we believe that this points towards what makes distributional predictions more powerful.  \n\nOverall, we don’t believe that distributional predictions are a silver bullet and should be used in all situations – we (again) completely agree that with the reviewer that “non-distributional learning-augmented algorithms may still hold value”, and, in fact, there are likely many settings where the non-distributional view is at least as useful as the distributional.  But the simple binary search example seems to imply that in the right context, distributional predictions bring a lot of additional power.  Understanding exactly which problems and contexts are a good fit for distributional predictions is a fascinating set of open questions.", "author_response": "Thank you again very much for the feedback!  We will certainly include the discussions of model 1 and 2, and more comparison to Lin et al., in the paper as you suggest.  \n\n> “The discussion of basic baselines seems insufficient”\n\nWe agree that the proof of our specific claim, that one cannot black-box reduce to a single point prediction, does not directly imply that one cannot reduce to *collections* of point predictions.  However, it is relatively straightforward to generalize our proof to this stronger claim – instead of a 2 point distribution, consider a multi-modal distribution with $d = \\log^2 n$ points. The method suggested by the reviewer, of doing a black-box reduction to MV for each of a collection of point predictions (by running an instance of MV in parallel for each of them), fails for this example.  Either there is no prediction for one of the $d$ points, in which case every instantiation of MV will do many queries, or we do have a prediction for each of the points, and thus, when we run them in parallel we would probe all $d$ of them, again resulting in suboptimal query complexity. Developing an algorithm that can handle such multi-modal distributions (and generalizations) in an optimal manner is a significant part of our contribution.    \n\n> “I still find the use of \"binary search tree\" somewhat confusing.”\n\nWe completely agree that it would be best to change the title of the paper as you suggest. \n\n> “There is a lack of justification for the novelty of \"algorithms with distributional prediction.\"\n\nWe will include a more detailed discussion of Diakonikolas et al. We would like to highlight that our use of distributions is different. Specifically, the work of Diakonikolas et al. does not consider what happens when the distribution is erroneous (a major theme of our work), rather their focus is on minimizing the number of samples from the true distribution that they need. Since predictions are often erroneous, there is a dire need to make sure our usage of predictions is robust. This is what we focus on in this work, and believe would be of interest to the general community.  Of course, the empirical distribution of samples can be viewed as an erroneous prediction of the true distribution, but our setting is far more general, and allows for general distributional predictions with general errors (measured by EMD).  So we do believe that our setting is novel and will lead to significant follow-up work.  \n\n\n> “A new question (minor): In line 303, why set the exponential coefficient of $d$ to 8? If this is purely a result of hyperparameter tuning, is there any justification for why this would be preferable in all cases (rather than overfitting to the experiments)?”\n\nSetting the exponential coefficient of d to a small constant is preferable for improved empirical results, and does not change the asymptotic complexity. To see why, recall that the algorithm explores segments of length $2^{2^i}$ in the $i$-th iteration. When i is very small, these segments are very small, making the iterations overly fast and unlikely to succeed. We found that setting the length to $2^{8 \\cdot 2^i}$ allowed us to balance this trade-off better. The exact setting of the exponential coefficient is not important (4 and 16 worked almost equally well). Also, see the further experimental results in our response to Reviewer EXym which take place on bimodal instances.\n\n> “Results on time complexity.”\n\n We note that in model 2, the time complexity is basically equivalent to the query complexity.  In this model, there is a single distribution over queries, so the time to build our BST is just preprocessing – we expect to answer far more queries than the time it takes to build the BST (as long as the preprocessing time is at least somewhat reasonable, as ours is).  And then once the BST is built, the time it takes to search in it is essentially equal (up to constants involving following pointers) to the query complexity that we analyze.  So in this setting, the query complexity *is* the time complexity!"}
{"claim": "The paper does not clearly state which components of the proposed algorithm are novel versus which follow existing techniques.", "claim_type": "novelty", "paper_id": "JEKXTLjEIq", "paper_title": "Binary Search with Distributional Predictions", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BELGkUQqxK", "reviewer": "Reviewer_kf39", "review_text": "Summary: This paper proposes a learning-augmented algorithm for searching in a sorted array. Different from all previous learning-augmented algorithms, it takes in distributional predictions. The main result is an algorithm with query complexity $O(H(p) + \\log \\eta)$, where $H(p)$ is the entropy of the true distribution and $\\eta$ is the Earth Mover's distance between the true and predicted distributions. The paper also includes proofs to show the theoretical optimality and experiments to validate the practical usefulness.\n\nStrengths: - The paper follows the recent line of work on \"learning-augmented algorithm\", or \"algorithm with predictions\". This is a promising new direction that tries to combine the theoretical soundness of classic algorithms with the learning ability of machine learning algorithms.\n- The section on theoretical analysis (though the ideas are simple) is effective.\n- The paper includes experimental results to back up the theory. The experimental settings are diverse. The performance of the proposed algorithm is strong compared to all the baselines.\n\nWeaknesses: - I think the presentation of the paper can be greatly improved. To list a few points:\n    - in line 50, the sentence \"That is, the prediction is a single (potentially high-dimensional) point (or maybe a small number of such points).\" is hard to read for me.\n    - in line 56, the sentence \"Or, can we in fact do better by taking full advantage of the entire predicted distribution.\" should end with a question mark.\n    - the abusive use of the word \"essentially\" greatly weakens the soundness of the paper (For example, in lines 66, 70, 78 87, 88). The expression \"essentially optimal\" should be clarified with collaboration on complexity and constants.\n- I think several key works in the field of learning-augmented algorithms are missing, which makes it hard to position this paper in the correct context. For example, I think the algorithms proposed in \"Learning-Augmented Binary Search Trees\" by Lin et al should be at least discussed and even compared against (now, this paper is only mentioned as a very general reference for learning-augmented data structures).\n- It is not straightforward to me why the techniques used in the proposed algorithm are novel and not hard to come up with. I encourage the author to make a clearer point on \"which components of the proposed algorithm are novel and different from existing techniques\".\n\nQuestions: - The algorithm in [Mitzenmacher and Vassilvitskii, 2021] that searches in a sorted list with predictions receive separate predictions for each query. Is this also the case for the setting discussed in this paper? \n    - If so, as I understand, the proposed algorithm needs to rebuild the binary search tree every time it receives a new query along with its distributional predictions. Then, this would lower bound the time complexity to answer each query with $O(n)$. Is that correct?\n- I am confused by the reference to \"binary search tree\" in the paper (even in the title). Does the proposed algorithm actually require building a binary search tree in its specification and implementation? Why does section 3 not contain any explanation related to the binary search tree?", "labeling_timestamp": "2026-01-11T16:34:58.965771", "model": "gpt-5-mini", "label": "Supported", "justification": "The authors explicitly agree to add discussions and comparisons to clarify novelty, and they provide explanations distinguishing their contributions (e.g., robustness to erroneous distributions) while committing to expand the paper to make these points clearer.", "evidence": "We will certainly include the discussions of model 1 and 2, and more comparison to Lin et al., in the paper as you suggest. | We will include a more detailed discussion of Diakonikolas et al. ... So we do believe that our setting is novel and will lead to significant follow-up work.", "combined_author_response": "Thank you again very much for the feedback!  We will certainly include the discussions of model 1 and 2, and more comparison to Lin et al., in the paper as you suggest.  \n\n> “The discussion of basic baselines seems insufficient”\n\nWe agree that the proof of our specific claim, that one cannot black-box reduce to a single point prediction, does not directly imply that one cannot reduce to *collections* of point predictions.  However, it is relatively straightforward to generalize our proof to this stronger claim – instead of a 2 point distribution, consider a multi-modal distribution with $d = \\log^2 n$ points. The method suggested by the reviewer, of doing a black-box reduction to MV for each of a collection of point predictions (by running an instance of MV in parallel for each of them), fails for this example.  Either there is no prediction for one of the $d$ points, in which case every instantiation of MV will do many queries, or we do have a prediction for each of the points, and thus, when we run them in parallel we would probe all $d$ of them, again resulting in suboptimal query complexity. Developing an algorithm that can handle such multi-modal distributions (and generalizations) in an optimal manner is a significant part of our contribution.    \n\n> “I still find the use of \"binary search tree\" somewhat confusing.”\n\nWe completely agree that it would be best to change the title of the paper as you suggest. \n\n> “There is a lack of justification for the novelty of \"algorithms with distributional prediction.\"\n\nWe will include a more detailed discussion of Diakonikolas et al. We would like to highlight that our use of distributions is different. Specifically, the work of Diakonikolas et al. does not consider what happens when the distribution is erroneous (a major theme of our work), rather their focus is on minimizing the number of samples from the true distribution that they need. Since predictions are often erroneous, there is a dire need to make sure our usage of predictions is robust. This is what we focus on in this work, and believe would be of interest to the general community.  Of course, the empirical distribution of samples can be viewed as an erroneous prediction of the true distribution, but our setting is far more general, and allows for general distributional predictions with general errors (measured by EMD).  So we do believe that our setting is novel and will lead to significant follow-up work.  \n\n\n> “A new question (minor): In line 303, why set the exponential coefficient of $d$ to 8? If this is purely a result of hyperparameter tuning, is there any justification for why this would be preferable in all cases (rather than overfitting to the experiments)?”\n\nSetting the exponential coefficient of d to a small constant is preferable for improved empirical results, and does not change the asymptotic complexity. To see why, recall that the algorithm explores segments of length $2^{2^i}$ in the $i$-th iteration. When i is very small, these segments are very small, making the iterations overly fast and unlikely to succeed. We found that setting the length to $2^{8 \\cdot 2^i}$ allowed us to balance this trade-off better. The exact setting of the exponential coefficient is not important (4 and 16 worked almost equally well). Also, see the further experimental results in our response to Reviewer EXym which take place on bimodal instances.\n\n> “Results on time complexity.”\n\n We note that in model 2, the time complexity is basically equivalent to the query complexity.  In this model, there is a single distribution over queries, so the time to build our BST is just preprocessing – we expect to answer far more queries than the time it takes to build the BST (as long as the preprocessing time is at least somewhat reasonable, as ours is).  And then once the BST is built, the time it takes to search in it is essentially equal (up to constants involving following pointers) to the query complexity that we analyze.  So in this setting, the query complexity *is* the time complexity!\n\n---\n\nThanks for the clarification.  We agree – on one hand this is similar to how most algorithms with predictions literature uses predictions. That is, the algorithm changes its control flow deterministically based on some information extracted from the prediction. On the other hand, note that the distribution that the algorithm considers is updated from phase to phase, since we need to look at the median of the distribution conditioned on the endpoints of the current search space. So in some ways this is like the online setting mentioned by the reviewer, in that we use a “new” prediction at each step.  But this is not an online problem, and our “new” prediction is just a computational transformation of the old prediction.  So we completely agree that there are many similarities!  But there are also important differences that only arise when considering distributional predictions.\n\nWe also agree that trying to formally understand what makes a distributional prediction better is an interesting open question. For some intuition in the binary search problem, consider the setting where the truth is multimodal (e.g. the distribution has mass of 1/(k+1) at points 0, n/k, 2n/k, …, n}, for intermediate values of k). Here a point prediction will necessarily drop a lot of information, whereas a distribution prediction will preserve the richness of the input space.  This is the essence of our lower bound from Section 2.1, and we believe that this points towards what makes distributional predictions more powerful.  \n\nOverall, we don’t believe that distributional predictions are a silver bullet and should be used in all situations – we (again) completely agree that with the reviewer that “non-distributional learning-augmented algorithms may still hold value”, and, in fact, there are likely many settings where the non-distributional view is at least as useful as the distributional.  But the simple binary search example seems to imply that in the right context, distributional predictions bring a lot of additional power.  Understanding exactly which problems and contexts are a good fit for distributional predictions is a fascinating set of open questions.", "author_response": "Thank you again very much for the feedback!  We will certainly include the discussions of model 1 and 2, and more comparison to Lin et al., in the paper as you suggest.  \n\n> “The discussion of basic baselines seems insufficient”\n\nWe agree that the proof of our specific claim, that one cannot black-box reduce to a single point prediction, does not directly imply that one cannot reduce to *collections* of point predictions.  However, it is relatively straightforward to generalize our proof to this stronger claim – instead of a 2 point distribution, consider a multi-modal distribution with $d = \\log^2 n$ points. The method suggested by the reviewer, of doing a black-box reduction to MV for each of a collection of point predictions (by running an instance of MV in parallel for each of them), fails for this example.  Either there is no prediction for one of the $d$ points, in which case every instantiation of MV will do many queries, or we do have a prediction for each of the points, and thus, when we run them in parallel we would probe all $d$ of them, again resulting in suboptimal query complexity. Developing an algorithm that can handle such multi-modal distributions (and generalizations) in an optimal manner is a significant part of our contribution.    \n\n> “I still find the use of \"binary search tree\" somewhat confusing.”\n\nWe completely agree that it would be best to change the title of the paper as you suggest. \n\n> “There is a lack of justification for the novelty of \"algorithms with distributional prediction.\"\n\nWe will include a more detailed discussion of Diakonikolas et al. We would like to highlight that our use of distributions is different. Specifically, the work of Diakonikolas et al. does not consider what happens when the distribution is erroneous (a major theme of our work), rather their focus is on minimizing the number of samples from the true distribution that they need. Since predictions are often erroneous, there is a dire need to make sure our usage of predictions is robust. This is what we focus on in this work, and believe would be of interest to the general community.  Of course, the empirical distribution of samples can be viewed as an erroneous prediction of the true distribution, but our setting is far more general, and allows for general distributional predictions with general errors (measured by EMD).  So we do believe that our setting is novel and will lead to significant follow-up work.  \n\n\n> “A new question (minor): In line 303, why set the exponential coefficient of $d$ to 8? If this is purely a result of hyperparameter tuning, is there any justification for why this would be preferable in all cases (rather than overfitting to the experiments)?”\n\nSetting the exponential coefficient of d to a small constant is preferable for improved empirical results, and does not change the asymptotic complexity. To see why, recall that the algorithm explores segments of length $2^{2^i}$ in the $i$-th iteration. When i is very small, these segments are very small, making the iterations overly fast and unlikely to succeed. We found that setting the length to $2^{8 \\cdot 2^i}$ allowed us to balance this trade-off better. The exact setting of the exponential coefficient is not important (4 and 16 worked almost equally well). Also, see the further experimental results in our response to Reviewer EXym which take place on bimodal instances.\n\n> “Results on time complexity.”\n\n We note that in model 2, the time complexity is basically equivalent to the query complexity.  In this model, there is a single distribution over queries, so the time to build our BST is just preprocessing – we expect to answer far more queries than the time it takes to build the BST (as long as the preprocessing time is at least somewhat reasonable, as ours is).  And then once the BST is built, the time it takes to search in it is essentially equal (up to constants involving following pointers) to the query complexity that we analyze.  So in this setting, the query complexity *is* the time complexity!"}

{"claim": "The paper lacks ablative experiments analyzing the impact of removing the low-pass filter on low-frequency versus high-frequency information and on accuracy and generalization.", "claim_type": "experimental", "paper_id": "Cf4FJGmHRQ", "paper_title": "PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "1Zfm29TuCw", "reviewer": "Reviewer_nwGj", "review_text": "Summary: This work has developed a neural network architecture for image recognition that is designed to address the influence of complex degradation factors. It aims to capture both low-frequency and high-frequency components to balance accuracy and generalization. The authors first propose to discard the low-pass filters in the existing FNO structure to retain all frequency components. Subsequently, a parallel structure is introduced to further enhance the utilization of frequency domain information. Finally, the authors design a two-stage training strategy to ensure performance stability.\n\nStrengths: 1. The overall paper has a clear logical structure, and the explanation of the methodology and the presentation of the constructed mechanisms are intuitive and easy to understand.\n2. The author provides a sufficiently detailed explanation for the motivation behind each component in PAC-FNO.\n3. The problem that this work aims to address holds a certain degree of practical application value.\n\nWeaknesses: 1. The abandonment of the low-pass filter is one of the main innovations in this work. Although the author provides an explanation for the motivation behind this operation, it is still recommended that the author conduct ablative experiments to analyze the impact of low-frequency/high-frequency information on accuracy/generalization.\n2. As for parallel architecture, the relevant experimental results have indeed proven its effectiveness. However, the explanation of parallel architecture in the method section appears somewhat lacking. It is hoped that the author can provide further analysis of the mechanism that enables it to be effective.\n3. In terms of comparative experiments, the methods used by the author for comparison appear to be lacking in both quantity and novelty. The comprehensiveness of the complex scenarios considered by the author is commendable, but it is hoped that the author can still increase the comparison results with more advanced works to more effectively validate the superiority of the proposed method.\n4. The author mentions the advantages of this work in terms of efficiency, but it seems that no experimental analysis related to efficiency has been provided (such as FLOPs and runtime on data at different resolutions).\n\nQuestions: Please refer to the Weaknesses.", "labeling_timestamp": "2026-01-11T16:21:29.981047", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper proposes removing the low-pass filter (AC-FNO) and argues about its effects (Section 3), but the provided content includes no ablation experiments that analyze the specific impact of removing the low-pass filter on low- vs high-frequency information or on accuracy vs generalization. The methods and claims are described, and broad evaluations are mentioned, but no targeted ablative study is presented in the shown sections.", "evidence": "“We argue that the low pass filter removes useful information for generalization of the model and harms the image classification accuracy. Thus, we propose a FNO block without the low pass filter, and call it 'all component' FNO (AC-FNO).”\n\n“ We perform a comprehensive evaluation of PAC-FNO with seven image recognition benchmarks under various input quality degradation. In the evaluation, PAC-FNO demonstrates superior performance over other baselines…”", "section": "Section 3 (PAC-FNO / AC-FNO Block) and Introduction (evaluation statement)"}
{"claim": "The method section does not provide a sufficient mechanistic explanation for why the proposed parallel architecture is effective.", "claim_type": "subjective", "paper_id": "Cf4FJGmHRQ", "paper_title": "PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "1Zfm29TuCw", "reviewer": "Reviewer_nwGj", "review_text": "Summary: This work has developed a neural network architecture for image recognition that is designed to address the influence of complex degradation factors. It aims to capture both low-frequency and high-frequency components to balance accuracy and generalization. The authors first propose to discard the low-pass filters in the existing FNO structure to retain all frequency components. Subsequently, a parallel structure is introduced to further enhance the utilization of frequency domain information. Finally, the authors design a two-stage training strategy to ensure performance stability.\n\nStrengths: 1. The overall paper has a clear logical structure, and the explanation of the methodology and the presentation of the constructed mechanisms are intuitive and easy to understand.\n2. The author provides a sufficiently detailed explanation for the motivation behind each component in PAC-FNO.\n3. The problem that this work aims to address holds a certain degree of practical application value.\n\nWeaknesses: 1. The abandonment of the low-pass filter is one of the main innovations in this work. Although the author provides an explanation for the motivation behind this operation, it is still recommended that the author conduct ablative experiments to analyze the impact of low-frequency/high-frequency information on accuracy/generalization.\n2. As for parallel architecture, the relevant experimental results have indeed proven its effectiveness. However, the explanation of parallel architecture in the method section appears somewhat lacking. It is hoped that the author can provide further analysis of the mechanism that enables it to be effective.\n3. In terms of comparative experiments, the methods used by the author for comparison appear to be lacking in both quantity and novelty. The comprehensiveness of the complex scenarios considered by the author is commendable, but it is hoped that the author can still increase the comparison results with more advanced works to more effectively validate the superiority of the proposed method.\n4. The author mentions the advantages of this work in terms of efficiency, but it seems that no experimental analysis related to efficiency has been provided (such as FLOPs and runtime on data at different resolutions).\n\nQuestions: Please refer to the Weaknesses.", "labeling_timestamp": "2026-01-11T16:21:28.357046", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper gives only high-level, qualitative motivation for the parallel design (e.g., 'increase the capacity' and retain all frequencies) but does not provide a detailed mechanistic explanation or analysis for why the parallel AC-FNO configuration leads to improved effectiveness over serial FNOs. The relevant content in Section 3 and 3.2 is descriptive rather than mechanistic.", "evidence": "1) \"Thus, we propose a FNO block without the low pass filter, and call it 'all component' FNO (AC-FNO). We then propose to stack the AC-FNO blocks in a parallel structure to increase the capacity of encoding spatial information of images, call the whole structure as 'parallel' AC-FNO or PAC-FNO.\" (Section 3)\n\n2) \"We now propose to configure AC-FNO blocks in a parallel structure to increase the capacity to learn various types of input variations. Previous FNO models have a serial structure, and the first layer of the model, which is directly related to data, consists of only one block. It has too small a capacity to consider all components of the data. Therefore, we increase the capacity of the first layer with the parallel configuration of AC-FNO blocks, allowing it to utilize all frequencies of the image, including high-frequency and low-frequency components.\" (Section 3.2)", "section": "Section 3 (AC-FNO block) and Section 3.2 (Parallel configuration of AC-FNO blocks)"}
{"claim": "The comparative experiments use an insufficient number of baseline methods and the chosen comparisons lack novelty.", "claim_type": "novelty", "paper_id": "Cf4FJGmHRQ", "paper_title": "PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "1Zfm29TuCw", "reviewer": "Reviewer_nwGj", "review_text": "Summary: This work has developed a neural network architecture for image recognition that is designed to address the influence of complex degradation factors. It aims to capture both low-frequency and high-frequency components to balance accuracy and generalization. The authors first propose to discard the low-pass filters in the existing FNO structure to retain all frequency components. Subsequently, a parallel structure is introduced to further enhance the utilization of frequency domain information. Finally, the authors design a two-stage training strategy to ensure performance stability.\n\nStrengths: 1. The overall paper has a clear logical structure, and the explanation of the methodology and the presentation of the constructed mechanisms are intuitive and easy to understand.\n2. The author provides a sufficiently detailed explanation for the motivation behind each component in PAC-FNO.\n3. The problem that this work aims to address holds a certain degree of practical application value.\n\nWeaknesses: 1. The abandonment of the low-pass filter is one of the main innovations in this work. Although the author provides an explanation for the motivation behind this operation, it is still recommended that the author conduct ablative experiments to analyze the impact of low-frequency/high-frequency information on accuracy/generalization.\n2. As for parallel architecture, the relevant experimental results have indeed proven its effectiveness. However, the explanation of parallel architecture in the method section appears somewhat lacking. It is hoped that the author can provide further analysis of the mechanism that enables it to be effective.\n3. In terms of comparative experiments, the methods used by the author for comparison appear to be lacking in both quantity and novelty. The comprehensiveness of the complex scenarios considered by the author is commendable, but it is hoped that the author can still increase the comparison results with more advanced works to more effectively validate the superiority of the proposed method.\n4. The author mentions the advantages of this work in terms of efficiency, but it seems that no experimental analysis related to efficiency has been provided (such as FLOPs and runtime on data at different resolutions).\n\nQuestions: Please refer to the Weaknesses.", "labeling_timestamp": "2026-01-11T16:21:25.172686", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpts state that the method was evaluated on seven benchmarks and compares to 'existing baseline models,' but they do not list which baseline methods were used or details of the comparative experiments. Therefore it is not possible from the supplied content to determine whether the number of baselines was insufficient or whether the chosen comparisons lack novelty.", "evidence": "Abstract: \"Extensively evaluating methods with seven image recognition benchmarks, we show that the proposed PAC-FNO improves the performance of existing baseline models on images with various resolutions by up to 77.1% and various types of natural variations in the images at inference.\" \n\nIntroduction/Contributions: \"We perform a comprehensive evaluation of PAC-FNO with seven benchmarking tasks. We show that existing image recognition models, fine-tuned with PAC-FNO, can handle any input resolution degradation and are resilient to input variations that occur in real-world deployment settings.\"", "section": "ABSTRACT; 1 INTRODUCTION / Contributions"}
{"claim": "The paper does not compare the proposed method with more recent or stronger state-of-the-art methods to convincingly validate its superiority.", "claim_type": "baseline", "paper_id": "Cf4FJGmHRQ", "paper_title": "PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "1Zfm29TuCw", "reviewer": "Reviewer_nwGj", "review_text": "Summary: This work has developed a neural network architecture for image recognition that is designed to address the influence of complex degradation factors. It aims to capture both low-frequency and high-frequency components to balance accuracy and generalization. The authors first propose to discard the low-pass filters in the existing FNO structure to retain all frequency components. Subsequently, a parallel structure is introduced to further enhance the utilization of frequency domain information. Finally, the authors design a two-stage training strategy to ensure performance stability.\n\nStrengths: 1. The overall paper has a clear logical structure, and the explanation of the methodology and the presentation of the constructed mechanisms are intuitive and easy to understand.\n2. The author provides a sufficiently detailed explanation for the motivation behind each component in PAC-FNO.\n3. The problem that this work aims to address holds a certain degree of practical application value.\n\nWeaknesses: 1. The abandonment of the low-pass filter is one of the main innovations in this work. Although the author provides an explanation for the motivation behind this operation, it is still recommended that the author conduct ablative experiments to analyze the impact of low-frequency/high-frequency information on accuracy/generalization.\n2. As for parallel architecture, the relevant experimental results have indeed proven its effectiveness. However, the explanation of parallel architecture in the method section appears somewhat lacking. It is hoped that the author can provide further analysis of the mechanism that enables it to be effective.\n3. In terms of comparative experiments, the methods used by the author for comparison appear to be lacking in both quantity and novelty. The comprehensiveness of the complex scenarios considered by the author is commendable, but it is hoped that the author can still increase the comparison results with more advanced works to more effectively validate the superiority of the proposed method.\n4. The author mentions the advantages of this work in terms of efficiency, but it seems that no experimental analysis related to efficiency has been provided (such as FLOPs and runtime on data at different resolutions).\n\nQuestions: Please refer to the Weaknesses.", "labeling_timestamp": "2026-01-11T16:21:29.124234", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt claims extensive evaluation on seven benchmarks and superior performance over baselines, but the excerpt does not include the experimental details, tables, or a list of compared methods. Therefore there is insufficient information in the given text to verify whether PAC-FNO was (or was not) compared against more recent or stronger state-of-the-art methods.", "evidence": "Abstract: \"Extensively evaluating methods with seven image recognition benchmarks, we show that the proposed PAC-FNO improves the performance of existing baseline models on images with various resolutions by up to 77.1% and various types of natural variations in the images at inference.\" Introduction / Contributions: \"We perform an extensive evaluation of PAC-FNO with seven benchmarking tasks. We show that existing image recognition models, fine-tuned with PAC-FNO, can handle any input resolution degradation and are resilient to input variations that occur in real-world deployment settings.\"", "section": "Abstract; Introduction (Contributions)"}
{"claim": "The authors claim efficiency advantages but provide no experimental efficiency analysis such as FLOPs or runtime measurements across different input resolutions.", "claim_type": "experimental", "paper_id": "Cf4FJGmHRQ", "paper_title": "PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "1Zfm29TuCw", "reviewer": "Reviewer_nwGj", "review_text": "Summary: This work has developed a neural network architecture for image recognition that is designed to address the influence of complex degradation factors. It aims to capture both low-frequency and high-frequency components to balance accuracy and generalization. The authors first propose to discard the low-pass filters in the existing FNO structure to retain all frequency components. Subsequently, a parallel structure is introduced to further enhance the utilization of frequency domain information. Finally, the authors design a two-stage training strategy to ensure performance stability.\n\nStrengths: 1. The overall paper has a clear logical structure, and the explanation of the methodology and the presentation of the constructed mechanisms are intuitive and easy to understand.\n2. The author provides a sufficiently detailed explanation for the motivation behind each component in PAC-FNO.\n3. The problem that this work aims to address holds a certain degree of practical application value.\n\nWeaknesses: 1. The abandonment of the low-pass filter is one of the main innovations in this work. Although the author provides an explanation for the motivation behind this operation, it is still recommended that the author conduct ablative experiments to analyze the impact of low-frequency/high-frequency information on accuracy/generalization.\n2. As for parallel architecture, the relevant experimental results have indeed proven its effectiveness. However, the explanation of parallel architecture in the method section appears somewhat lacking. It is hoped that the author can provide further analysis of the mechanism that enables it to be effective.\n3. In terms of comparative experiments, the methods used by the author for comparison appear to be lacking in both quantity and novelty. The comprehensiveness of the complex scenarios considered by the author is commendable, but it is hoped that the author can still increase the comparison results with more advanced works to more effectively validate the superiority of the proposed method.\n4. The author mentions the advantages of this work in terms of efficiency, but it seems that no experimental analysis related to efficiency has been provided (such as FLOPs and runtime on data at different resolutions).\n\nQuestions: Please refer to the Weaknesses.", "labeling_timestamp": "2026-01-11T16:21:35.700853", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper does make efficiency-related claims (e.g., a single model handling multiple resolutions and 'minimal modification' to downstream models, and prior FNO work is said to have 'small computational costs'), but in the provided content there are no experimental efficiency analyses (no FLOPs, latency, or runtime measurements across input resolutions). Thus the reviewer is correct that no such experimental efficiency metrics are presented, but the paper's claims are limited/implicit rather than explicit broad efficiency claims.", "evidence": "Abstract: \"PAC-FNO operates in the frequency domain, allowing it to handle images of varying resolutions within a single model. We also propose a twostage algorithm for training PAC-FNO with a minimal modification to the original, downstream model.\" Contributions: \"Our design choices of PAC-FNO can offer such resilience with a single model, able to attach to a downstream visual recognition model. Not only does this handle multiple input variations at once, but the approach also minimizes the changes in the downstream model during fine-tuning.\" Section 2: \"The FNOs achieve remarkable performance in solving PDEs with small computational costs...\"", "section": "Abstract; Contributions; 2 PRELIMINARIES"}
{"claim": "The weakly-supervised contrastive distillation approach is not novel, having been previously used in prior works such as R1 and R2.", "claim_type": "novelty", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "P8fCEoWdti", "reviewer": "Reviewer_svRz", "review_text": "Summary: This work aims to tackle the image-to-LiDAR contrastive learning problem for LiDAR-based point cloud segmentation. Previous approaches designed the cross-modal contrastive learning objective for model pretraining, using superpixels and superpoints as guidance.\n\nIn this work, the authors observe that the superpixel-driven contrastive loss tends to involve ‘’self-conflict’’ issues during representation learning. A weakly-supervised contrastive distillation method is proposed, which generates semantic superpixels/superpoints using the Segment Anything Model (SAM). Additionally, to balance the imbalanced class distributions of LiDAR scene categories during representation, a density and category-aware sampling strategy is proposed to adjust the sampling probabilities of different anchor points using the weak semantic labels.\n\nThe overall framework is named OLIVINE, which adopts three optimization objectives:\n- Weakly-supervised contrastive distillation using coarse semantic labels to identify positive pairs by category.\n- Self-supervised contrastive distillation applied to randomly sampled point-pixel pairs.\n- A regularization framework based on the von Mises-Fisher (vMF) distribution to ensure semantic consistency.\n\nThe proposed OLIVINE method is evaluated on the nuScenes, SemanticKITTI, and KITTI object detection datasets. The results exhibit a consistent improvement of the proposed method compared to existing approaches.\n\nStrengths: (+) This work aims to improve the image-to-LiDAR self-supervised representation learning problem on LiDAR-based point cloud datasets, which is one of the current research hotspots, especially for applications related to autonomous driving and robotics.\n\n(+) The proposed method has exhibited promising performance on mainstream benchmarks, including nuScenes linear probing, nuScenes fine-tuning, SemanticKITTI fine-tuning, and KITTI object detection.\n\nWeaknesses: (-) The weakly-supervised contrastive distillation method has been used in previous literature, such as [R1] and [R2]. Adding semantic categories seems not to cause a major improvement over class-agnostic masks, as the Segment Anything Model is able to segment rather complete and semantically consistent objects and backgrounds. Additionally, using weak labels (which might be erroneous) could introduce additional errors during pretraining.\n\n(-) The motivation for using the von Mises-Fisher (vMF) distribution to enforce consistency regularization for image-to-LiDAR representation learning is not clear enough to demonstrate its superiority. A more detailed explanation and theoretical justification would strengthen this aspect of the work.\n\n(-) Compared to some of the most related works, for example, [R1] and [R3], the scale and depth regarding the experiments (for example, downstream fine-tuning on other datasets than SemanticKITTI) could be further enhanced.\n\n---\n\n### References:\n- [R1] Youquan Liu, et al. “Segment Any Point Cloud Sequences by Distilling Vision Foundation Models,” NeurIPS, 2023.\n- [R2] Ayça Takmaz, et al. “OpenMask3D: Open-Vocabulary 3D Instance Segmentation,” NeurIPS, 2023.\n- [R3] Gilles Puy, et al. “Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving,” arXiv, 2023.\n\nQuestions: - **Q1:** As mentioned in Weakness 1, the semantic masks generated by the Segment Anything Model could inevitably involve errors (e.g., wrong segmentation results). How do the authors handle the propagated errors during image-to-LiDAR representation learning?\n\n- **Q2:** As mentioned in Weakness 2, could the authors provide more details on the hyperparameter settings for the vMF distribution and the reasoning behind their chosen values? Adding a more detailed explanation and theoretical justification would be even better.\n\n- **Q3:** As mentioned in Weakness 3, having more thorough experimental analyses on other LiDAR-based point cloud datasets, such as SemanticPOSS, Waymo, SynLiDAR, etc., could further consolidate the findings and conclusions drawn in the manuscript.\n\n- **Q4:** As most 2D and 3D representation learning approaches (MoCo, SimCLR, Seal, etc.) do, having empirical analyses of models under out-of-distribution datasets is recommended. \n\n- **[Minor]:** The computational cost of the proposed multi-modal contrastive distillation approach is not thoroughly analyzed, which is crucial for real-time applications in autonomous driving.\n\n- **[Minor]:** The generalizability of OLIVINE to other types of sensors (for example, hybrid-solid LiDARs) or environments (for example, off-board environments) beyond the evaluated datasets is not discussed.\n\n- **[Minor]:** “NuScenes” should be revised to “nuScenes”.", "labeling_timestamp": "2026-01-11T16:21:53.222144", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper itself cites prior works that use semantic superpixels or semantic-aware contrastive distillation (e.g., Seal [38], Sautier et al. [50], ST-SLidR [41]), indicating that using semantics or VFMs for pixel-to-point contrastive learning is not entirely novel. However, the paper introduces additional components (vMF-based semantic consistency regularization and a density/category-aware sampling strategy) that are presented as new contributions, so the reviewer's claim that the entire approach is not novel is only partially correct.", "evidence": "\"Seal [38] utilizes semantically rich superpixels generated by visual foundation models and introduces temporal consistency regularization across point segments at different times.\"; \"Sautier et al.[50] pioneered a superpixel-to-superpoint contrastive loss for self-supervised 2D-to-3D representation distillation.\"; \"1) To tackle the challenge of 'self-conflict', we utilize off-the-shelf VFMs to generate semantic labels for weakly-supervised pixel-to-point contrastive distillation.\"", "section": "Introduction; Related Work; Contributions"}
{"claim": "Adding semantic categories to SAM-generated masks does not appear to provide significant improvement over class-agnostic masks.", "claim_type": "experimental", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "P8fCEoWdti", "reviewer": "Reviewer_svRz", "review_text": "Summary: This work aims to tackle the image-to-LiDAR contrastive learning problem for LiDAR-based point cloud segmentation. Previous approaches designed the cross-modal contrastive learning objective for model pretraining, using superpixels and superpoints as guidance.\n\nIn this work, the authors observe that the superpixel-driven contrastive loss tends to involve ‘’self-conflict’’ issues during representation learning. A weakly-supervised contrastive distillation method is proposed, which generates semantic superpixels/superpoints using the Segment Anything Model (SAM). Additionally, to balance the imbalanced class distributions of LiDAR scene categories during representation, a density and category-aware sampling strategy is proposed to adjust the sampling probabilities of different anchor points using the weak semantic labels.\n\nThe overall framework is named OLIVINE, which adopts three optimization objectives:\n- Weakly-supervised contrastive distillation using coarse semantic labels to identify positive pairs by category.\n- Self-supervised contrastive distillation applied to randomly sampled point-pixel pairs.\n- A regularization framework based on the von Mises-Fisher (vMF) distribution to ensure semantic consistency.\n\nThe proposed OLIVINE method is evaluated on the nuScenes, SemanticKITTI, and KITTI object detection datasets. The results exhibit a consistent improvement of the proposed method compared to existing approaches.\n\nStrengths: (+) This work aims to improve the image-to-LiDAR self-supervised representation learning problem on LiDAR-based point cloud datasets, which is one of the current research hotspots, especially for applications related to autonomous driving and robotics.\n\n(+) The proposed method has exhibited promising performance on mainstream benchmarks, including nuScenes linear probing, nuScenes fine-tuning, SemanticKITTI fine-tuning, and KITTI object detection.\n\nWeaknesses: (-) The weakly-supervised contrastive distillation method has been used in previous literature, such as [R1] and [R2]. Adding semantic categories seems not to cause a major improvement over class-agnostic masks, as the Segment Anything Model is able to segment rather complete and semantically consistent objects and backgrounds. Additionally, using weak labels (which might be erroneous) could introduce additional errors during pretraining.\n\n(-) The motivation for using the von Mises-Fisher (vMF) distribution to enforce consistency regularization for image-to-LiDAR representation learning is not clear enough to demonstrate its superiority. A more detailed explanation and theoretical justification would strengthen this aspect of the work.\n\n(-) Compared to some of the most related works, for example, [R1] and [R3], the scale and depth regarding the experiments (for example, downstream fine-tuning on other datasets than SemanticKITTI) could be further enhanced.\n\n---\n\n### References:\n- [R1] Youquan Liu, et al. “Segment Any Point Cloud Sequences by Distilling Vision Foundation Models,” NeurIPS, 2023.\n- [R2] Ayça Takmaz, et al. “OpenMask3D: Open-Vocabulary 3D Instance Segmentation,” NeurIPS, 2023.\n- [R3] Gilles Puy, et al. “Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving,” arXiv, 2023.\n\nQuestions: - **Q1:** As mentioned in Weakness 1, the semantic masks generated by the Segment Anything Model could inevitably involve errors (e.g., wrong segmentation results). How do the authors handle the propagated errors during image-to-LiDAR representation learning?\n\n- **Q2:** As mentioned in Weakness 2, could the authors provide more details on the hyperparameter settings for the vMF distribution and the reasoning behind their chosen values? Adding a more detailed explanation and theoretical justification would be even better.\n\n- **Q3:** As mentioned in Weakness 3, having more thorough experimental analyses on other LiDAR-based point cloud datasets, such as SemanticPOSS, Waymo, SynLiDAR, etc., could further consolidate the findings and conclusions drawn in the manuscript.\n\n- **Q4:** As most 2D and 3D representation learning approaches (MoCo, SimCLR, Seal, etc.) do, having empirical analyses of models under out-of-distribution datasets is recommended. \n\n- **[Minor]:** The computational cost of the proposed multi-modal contrastive distillation approach is not thoroughly analyzed, which is crucial for real-time applications in autonomous driving.\n\n- **[Minor]:** The generalizability of OLIVINE to other types of sensors (for example, hybrid-solid LiDARs) or environments (for example, off-board environments) beyond the evaluated datasets is not discussed.\n\n- **[Minor]:** “NuScenes” should be revised to “nuScenes”.", "labeling_timestamp": "2026-01-11T16:21:47.482750", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper text describes using SAM / VFMs to generate semantic labels and claims improvements from weakly-supervised semantic contrastive distillation, but the excerpt contains no experimental results or ablation comparing SAM-generated semantic (class-aware) masks versus class-agnostic masks. Therefore the claim about lack of significant improvement cannot be confirmed or contradicted from the available content.", "evidence": "Abstract: \"we utilize offthe-shelf VFMs to generate semantic labels for weakly-supervised pixel-to-point contrastive distillation.\"; Introduction: \"To tackle the 'self-conflict', we utilize off-the-shelf VFMs to generate semantic labels for weakly-supervised pixel-to-point contrastive distillation.\"; Introduction (results claim): \"Extensive experiments demonstrate that our approach mitigates the challenges posed by traditional methods and consistently surpasses existing image-to-LiDAR contrastive distillation methods in downstream tasks.\"", "section": "Abstract; 1 Introduction"}
{"claim": "Using weak semantic labels from SAM can introduce erroneous labels that propagate errors during pretraining.", "claim_type": "experimental", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "P8fCEoWdti", "reviewer": "Reviewer_svRz", "review_text": "Summary: This work aims to tackle the image-to-LiDAR contrastive learning problem for LiDAR-based point cloud segmentation. Previous approaches designed the cross-modal contrastive learning objective for model pretraining, using superpixels and superpoints as guidance.\n\nIn this work, the authors observe that the superpixel-driven contrastive loss tends to involve ‘’self-conflict’’ issues during representation learning. A weakly-supervised contrastive distillation method is proposed, which generates semantic superpixels/superpoints using the Segment Anything Model (SAM). Additionally, to balance the imbalanced class distributions of LiDAR scene categories during representation, a density and category-aware sampling strategy is proposed to adjust the sampling probabilities of different anchor points using the weak semantic labels.\n\nThe overall framework is named OLIVINE, which adopts three optimization objectives:\n- Weakly-supervised contrastive distillation using coarse semantic labels to identify positive pairs by category.\n- Self-supervised contrastive distillation applied to randomly sampled point-pixel pairs.\n- A regularization framework based on the von Mises-Fisher (vMF) distribution to ensure semantic consistency.\n\nThe proposed OLIVINE method is evaluated on the nuScenes, SemanticKITTI, and KITTI object detection datasets. The results exhibit a consistent improvement of the proposed method compared to existing approaches.\n\nStrengths: (+) This work aims to improve the image-to-LiDAR self-supervised representation learning problem on LiDAR-based point cloud datasets, which is one of the current research hotspots, especially for applications related to autonomous driving and robotics.\n\n(+) The proposed method has exhibited promising performance on mainstream benchmarks, including nuScenes linear probing, nuScenes fine-tuning, SemanticKITTI fine-tuning, and KITTI object detection.\n\nWeaknesses: (-) The weakly-supervised contrastive distillation method has been used in previous literature, such as [R1] and [R2]. Adding semantic categories seems not to cause a major improvement over class-agnostic masks, as the Segment Anything Model is able to segment rather complete and semantically consistent objects and backgrounds. Additionally, using weak labels (which might be erroneous) could introduce additional errors during pretraining.\n\n(-) The motivation for using the von Mises-Fisher (vMF) distribution to enforce consistency regularization for image-to-LiDAR representation learning is not clear enough to demonstrate its superiority. A more detailed explanation and theoretical justification would strengthen this aspect of the work.\n\n(-) Compared to some of the most related works, for example, [R1] and [R3], the scale and depth regarding the experiments (for example, downstream fine-tuning on other datasets than SemanticKITTI) could be further enhanced.\n\n---\n\n### References:\n- [R1] Youquan Liu, et al. “Segment Any Point Cloud Sequences by Distilling Vision Foundation Models,” NeurIPS, 2023.\n- [R2] Ayça Takmaz, et al. “OpenMask3D: Open-Vocabulary 3D Instance Segmentation,” NeurIPS, 2023.\n- [R3] Gilles Puy, et al. “Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving,” arXiv, 2023.\n\nQuestions: - **Q1:** As mentioned in Weakness 1, the semantic masks generated by the Segment Anything Model could inevitably involve errors (e.g., wrong segmentation results). How do the authors handle the propagated errors during image-to-LiDAR representation learning?\n\n- **Q2:** As mentioned in Weakness 2, could the authors provide more details on the hyperparameter settings for the vMF distribution and the reasoning behind their chosen values? Adding a more detailed explanation and theoretical justification would be even better.\n\n- **Q3:** As mentioned in Weakness 3, having more thorough experimental analyses on other LiDAR-based point cloud datasets, such as SemanticPOSS, Waymo, SynLiDAR, etc., could further consolidate the findings and conclusions drawn in the manuscript.\n\n- **Q4:** As most 2D and 3D representation learning approaches (MoCo, SimCLR, Seal, etc.) do, having empirical analyses of models under out-of-distribution datasets is recommended. \n\n- **[Minor]:** The computational cost of the proposed multi-modal contrastive distillation approach is not thoroughly analyzed, which is crucial for real-time applications in autonomous driving.\n\n- **[Minor]:** The generalizability of OLIVINE to other types of sensors (for example, hybrid-solid LiDARs) or environments (for example, off-board environments) beyond the evaluated datasets is not discussed.\n\n- **[Minor]:** “NuScenes” should be revised to “nuScenes”.", "labeling_timestamp": "2026-01-11T16:21:53.610761", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper presents SAM-generated semantic labels as high-quality and beneficial for mitigating the 'self-conflict' in contrastive distillation, and does not claim that these weak labels introduce erroneous labels that propagate errors during pretraining. Thus the reviewer's claim is contrary to the paper's statements.", "evidence": "1) \"SAM (Segment Anything Model), trained on expansive datasets, has revolutionized computer vision by simplifying the acquisition of pixel-level semantics. These models are exceptionally adaptable, enabling the direct derivation of semantic labels through specified prompts without the need for retraining.\"  2) \"This application of SAM allows us to generate high-quality semantic labels without repetitive training, enhancing the learning process.\"", "section": "Introduction; 3.2 Weakly-supervised Contrastive Distillation"}
{"claim": "The manuscript does not clearly justify the motivation or theoretical basis for using the von Mises-Fisher distribution.", "claim_type": "presentation", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "P8fCEoWdti", "reviewer": "Reviewer_svRz", "review_text": "Summary: This work aims to tackle the image-to-LiDAR contrastive learning problem for LiDAR-based point cloud segmentation. Previous approaches designed the cross-modal contrastive learning objective for model pretraining, using superpixels and superpoints as guidance.\n\nIn this work, the authors observe that the superpixel-driven contrastive loss tends to involve ‘’self-conflict’’ issues during representation learning. A weakly-supervised contrastive distillation method is proposed, which generates semantic superpixels/superpoints using the Segment Anything Model (SAM). Additionally, to balance the imbalanced class distributions of LiDAR scene categories during representation, a density and category-aware sampling strategy is proposed to adjust the sampling probabilities of different anchor points using the weak semantic labels.\n\nThe overall framework is named OLIVINE, which adopts three optimization objectives:\n- Weakly-supervised contrastive distillation using coarse semantic labels to identify positive pairs by category.\n- Self-supervised contrastive distillation applied to randomly sampled point-pixel pairs.\n- A regularization framework based on the von Mises-Fisher (vMF) distribution to ensure semantic consistency.\n\nThe proposed OLIVINE method is evaluated on the nuScenes, SemanticKITTI, and KITTI object detection datasets. The results exhibit a consistent improvement of the proposed method compared to existing approaches.\n\nStrengths: (+) This work aims to improve the image-to-LiDAR self-supervised representation learning problem on LiDAR-based point cloud datasets, which is one of the current research hotspots, especially for applications related to autonomous driving and robotics.\n\n(+) The proposed method has exhibited promising performance on mainstream benchmarks, including nuScenes linear probing, nuScenes fine-tuning, SemanticKITTI fine-tuning, and KITTI object detection.\n\nWeaknesses: (-) The weakly-supervised contrastive distillation method has been used in previous literature, such as [R1] and [R2]. Adding semantic categories seems not to cause a major improvement over class-agnostic masks, as the Segment Anything Model is able to segment rather complete and semantically consistent objects and backgrounds. Additionally, using weak labels (which might be erroneous) could introduce additional errors during pretraining.\n\n(-) The motivation for using the von Mises-Fisher (vMF) distribution to enforce consistency regularization for image-to-LiDAR representation learning is not clear enough to demonstrate its superiority. A more detailed explanation and theoretical justification would strengthen this aspect of the work.\n\n(-) Compared to some of the most related works, for example, [R1] and [R3], the scale and depth regarding the experiments (for example, downstream fine-tuning on other datasets than SemanticKITTI) could be further enhanced.\n\n---\n\n### References:\n- [R1] Youquan Liu, et al. “Segment Any Point Cloud Sequences by Distilling Vision Foundation Models,” NeurIPS, 2023.\n- [R2] Ayça Takmaz, et al. “OpenMask3D: Open-Vocabulary 3D Instance Segmentation,” NeurIPS, 2023.\n- [R3] Gilles Puy, et al. “Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving,” arXiv, 2023.\n\nQuestions: - **Q1:** As mentioned in Weakness 1, the semantic masks generated by the Segment Anything Model could inevitably involve errors (e.g., wrong segmentation results). How do the authors handle the propagated errors during image-to-LiDAR representation learning?\n\n- **Q2:** As mentioned in Weakness 2, could the authors provide more details on the hyperparameter settings for the vMF distribution and the reasoning behind their chosen values? Adding a more detailed explanation and theoretical justification would be even better.\n\n- **Q3:** As mentioned in Weakness 3, having more thorough experimental analyses on other LiDAR-based point cloud datasets, such as SemanticPOSS, Waymo, SynLiDAR, etc., could further consolidate the findings and conclusions drawn in the manuscript.\n\n- **Q4:** As most 2D and 3D representation learning approaches (MoCo, SimCLR, Seal, etc.) do, having empirical analyses of models under out-of-distribution datasets is recommended. \n\n- **[Minor]:** The computational cost of the proposed multi-modal contrastive distillation approach is not thoroughly analyzed, which is crucial for real-time applications in autonomous driving.\n\n- **[Minor]:** The generalizability of OLIVINE to other types of sensors (for example, hybrid-solid LiDARs) or environments (for example, off-board environments) beyond the evaluated datasets is not discussed.\n\n- **[Minor]:** “NuScenes” should be revised to “nuScenes”.", "labeling_timestamp": "2026-01-11T16:21:51.641687", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper states the purpose of using von Mises–Fisher (vMF) distributions — to structure the feature space and enforce semantic consistency (Abstract, Introduction, Sec.3) — but provides only an intuitive motivation without a detailed theoretical justification or derivation of why vMF is specifically appropriate or superior.", "evidence": "Abstract: \"we employ von Mises-Fisher distributions to structure the feature space, ensuring semantic embeddings within the same class remain consistent across varying inputs.\"; Introduction: \"we introduce semantic-guided consistency regularization to enhance 3D representation learning. This approach structures the feature space by modeling each class with a von Mises-Fisher distribution and making point features adhere closely to their respective distributions.\"; Sec.3 Overview: \"a regularization framework based on the von Mises-Fisher distribution to ensure semantic consistency.\"", "section": "Abstract; Introduction; Section 3 (Proposed Method)"}
{"claim": "The paper lacks detailed explanations of hyperparameter settings and reasoning for chosen von Mises-Fisher distribution values.", "claim_type": "methodology", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "P8fCEoWdti", "reviewer": "Reviewer_svRz", "review_text": "Summary: This work aims to tackle the image-to-LiDAR contrastive learning problem for LiDAR-based point cloud segmentation. Previous approaches designed the cross-modal contrastive learning objective for model pretraining, using superpixels and superpoints as guidance.\n\nIn this work, the authors observe that the superpixel-driven contrastive loss tends to involve ‘’self-conflict’’ issues during representation learning. A weakly-supervised contrastive distillation method is proposed, which generates semantic superpixels/superpoints using the Segment Anything Model (SAM). Additionally, to balance the imbalanced class distributions of LiDAR scene categories during representation, a density and category-aware sampling strategy is proposed to adjust the sampling probabilities of different anchor points using the weak semantic labels.\n\nThe overall framework is named OLIVINE, which adopts three optimization objectives:\n- Weakly-supervised contrastive distillation using coarse semantic labels to identify positive pairs by category.\n- Self-supervised contrastive distillation applied to randomly sampled point-pixel pairs.\n- A regularization framework based on the von Mises-Fisher (vMF) distribution to ensure semantic consistency.\n\nThe proposed OLIVINE method is evaluated on the nuScenes, SemanticKITTI, and KITTI object detection datasets. The results exhibit a consistent improvement of the proposed method compared to existing approaches.\n\nStrengths: (+) This work aims to improve the image-to-LiDAR self-supervised representation learning problem on LiDAR-based point cloud datasets, which is one of the current research hotspots, especially for applications related to autonomous driving and robotics.\n\n(+) The proposed method has exhibited promising performance on mainstream benchmarks, including nuScenes linear probing, nuScenes fine-tuning, SemanticKITTI fine-tuning, and KITTI object detection.\n\nWeaknesses: (-) The weakly-supervised contrastive distillation method has been used in previous literature, such as [R1] and [R2]. Adding semantic categories seems not to cause a major improvement over class-agnostic masks, as the Segment Anything Model is able to segment rather complete and semantically consistent objects and backgrounds. Additionally, using weak labels (which might be erroneous) could introduce additional errors during pretraining.\n\n(-) The motivation for using the von Mises-Fisher (vMF) distribution to enforce consistency regularization for image-to-LiDAR representation learning is not clear enough to demonstrate its superiority. A more detailed explanation and theoretical justification would strengthen this aspect of the work.\n\n(-) Compared to some of the most related works, for example, [R1] and [R3], the scale and depth regarding the experiments (for example, downstream fine-tuning on other datasets than SemanticKITTI) could be further enhanced.\n\n---\n\n### References:\n- [R1] Youquan Liu, et al. “Segment Any Point Cloud Sequences by Distilling Vision Foundation Models,” NeurIPS, 2023.\n- [R2] Ayça Takmaz, et al. “OpenMask3D: Open-Vocabulary 3D Instance Segmentation,” NeurIPS, 2023.\n- [R3] Gilles Puy, et al. “Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving,” arXiv, 2023.\n\nQuestions: - **Q1:** As mentioned in Weakness 1, the semantic masks generated by the Segment Anything Model could inevitably involve errors (e.g., wrong segmentation results). How do the authors handle the propagated errors during image-to-LiDAR representation learning?\n\n- **Q2:** As mentioned in Weakness 2, could the authors provide more details on the hyperparameter settings for the vMF distribution and the reasoning behind their chosen values? Adding a more detailed explanation and theoretical justification would be even better.\n\n- **Q3:** As mentioned in Weakness 3, having more thorough experimental analyses on other LiDAR-based point cloud datasets, such as SemanticPOSS, Waymo, SynLiDAR, etc., could further consolidate the findings and conclusions drawn in the manuscript.\n\n- **Q4:** As most 2D and 3D representation learning approaches (MoCo, SimCLR, Seal, etc.) do, having empirical analyses of models under out-of-distribution datasets is recommended. \n\n- **[Minor]:** The computational cost of the proposed multi-modal contrastive distillation approach is not thoroughly analyzed, which is crucial for real-time applications in autonomous driving.\n\n- **[Minor]:** The generalizability of OLIVINE to other types of sensors (for example, hybrid-solid LiDARs) or environments (for example, off-board environments) beyond the evaluated datasets is not discussed.\n\n- **[Minor]:** “NuScenes” should be revised to “nuScenes”.", "labeling_timestamp": "2026-01-11T16:21:58.114136", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper refers to using von Mises–Fisher distributions and mentions contrastive hyperparameters (e.g., τ) but does not provide numerical hyperparameter settings or any rationale for chosen vMF parameters (e.g., concentration κ or how class distributions are parametrized). The provided sections describe the method conceptually but omit detailed hyperparameter choices or justification.", "evidence": "Abstract: \"...we employ von Mises-Fisher distributions to structure the feature space...\"; Introduction: \"...we introduce semantic-guided consistency regularization to enhance 3D representation learning. This approach structures the feature space by modeling each class with a von Mises-Fisher distribution and making point features adhere closely to their respective distributions.\"; Section 3.1: \"where τ is the temperature factor...\"", "section": "Abstract; 1 Introduction; 3.1 Baseline Architecture"}
{"claim": "The experiments lack scale and depth compared to related works and omit downstream fine-tuning on datasets beyond SemanticKITTI.", "claim_type": "experimental", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "P8fCEoWdti", "reviewer": "Reviewer_svRz", "review_text": "Summary: This work aims to tackle the image-to-LiDAR contrastive learning problem for LiDAR-based point cloud segmentation. Previous approaches designed the cross-modal contrastive learning objective for model pretraining, using superpixels and superpoints as guidance.\n\nIn this work, the authors observe that the superpixel-driven contrastive loss tends to involve ‘’self-conflict’’ issues during representation learning. A weakly-supervised contrastive distillation method is proposed, which generates semantic superpixels/superpoints using the Segment Anything Model (SAM). Additionally, to balance the imbalanced class distributions of LiDAR scene categories during representation, a density and category-aware sampling strategy is proposed to adjust the sampling probabilities of different anchor points using the weak semantic labels.\n\nThe overall framework is named OLIVINE, which adopts three optimization objectives:\n- Weakly-supervised contrastive distillation using coarse semantic labels to identify positive pairs by category.\n- Self-supervised contrastive distillation applied to randomly sampled point-pixel pairs.\n- A regularization framework based on the von Mises-Fisher (vMF) distribution to ensure semantic consistency.\n\nThe proposed OLIVINE method is evaluated on the nuScenes, SemanticKITTI, and KITTI object detection datasets. The results exhibit a consistent improvement of the proposed method compared to existing approaches.\n\nStrengths: (+) This work aims to improve the image-to-LiDAR self-supervised representation learning problem on LiDAR-based point cloud datasets, which is one of the current research hotspots, especially for applications related to autonomous driving and robotics.\n\n(+) The proposed method has exhibited promising performance on mainstream benchmarks, including nuScenes linear probing, nuScenes fine-tuning, SemanticKITTI fine-tuning, and KITTI object detection.\n\nWeaknesses: (-) The weakly-supervised contrastive distillation method has been used in previous literature, such as [R1] and [R2]. Adding semantic categories seems not to cause a major improvement over class-agnostic masks, as the Segment Anything Model is able to segment rather complete and semantically consistent objects and backgrounds. Additionally, using weak labels (which might be erroneous) could introduce additional errors during pretraining.\n\n(-) The motivation for using the von Mises-Fisher (vMF) distribution to enforce consistency regularization for image-to-LiDAR representation learning is not clear enough to demonstrate its superiority. A more detailed explanation and theoretical justification would strengthen this aspect of the work.\n\n(-) Compared to some of the most related works, for example, [R1] and [R3], the scale and depth regarding the experiments (for example, downstream fine-tuning on other datasets than SemanticKITTI) could be further enhanced.\n\n---\n\n### References:\n- [R1] Youquan Liu, et al. “Segment Any Point Cloud Sequences by Distilling Vision Foundation Models,” NeurIPS, 2023.\n- [R2] Ayça Takmaz, et al. “OpenMask3D: Open-Vocabulary 3D Instance Segmentation,” NeurIPS, 2023.\n- [R3] Gilles Puy, et al. “Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving,” arXiv, 2023.\n\nQuestions: - **Q1:** As mentioned in Weakness 1, the semantic masks generated by the Segment Anything Model could inevitably involve errors (e.g., wrong segmentation results). How do the authors handle the propagated errors during image-to-LiDAR representation learning?\n\n- **Q2:** As mentioned in Weakness 2, could the authors provide more details on the hyperparameter settings for the vMF distribution and the reasoning behind their chosen values? Adding a more detailed explanation and theoretical justification would be even better.\n\n- **Q3:** As mentioned in Weakness 3, having more thorough experimental analyses on other LiDAR-based point cloud datasets, such as SemanticPOSS, Waymo, SynLiDAR, etc., could further consolidate the findings and conclusions drawn in the manuscript.\n\n- **Q4:** As most 2D and 3D representation learning approaches (MoCo, SimCLR, Seal, etc.) do, having empirical analyses of models under out-of-distribution datasets is recommended. \n\n- **[Minor]:** The computational cost of the proposed multi-modal contrastive distillation approach is not thoroughly analyzed, which is crucial for real-time applications in autonomous driving.\n\n- **[Minor]:** The generalizability of OLIVINE to other types of sensors (for example, hybrid-solid LiDARs) or environments (for example, off-board environments) beyond the evaluated datasets is not discussed.\n\n- **[Minor]:** “NuScenes” should be revised to “nuScenes”.", "labeling_timestamp": "2026-01-11T16:22:04.366165", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper states it runs extensive experiments and evaluates on both SemanticKITTI and nuScenes, contradicting the reviewer's claim that it omits downstream fine-tuning beyond SemanticKITTI and lacks scale/depth.", "evidence": "\"Extensive experiments demonstrate that our approach mitigates the challenges posed by traditional methods and consistently surpasses existing image-to-LiDAR contrastive distillation methods in downstream tasks.\" \"Extensive experiments reveal that our pre-training approach outperforms state-of-the-art 3D selfsupervised methods on both the nuScenes and SemanticKITTI datasets.\"", "section": "Abstract; 1 Introduction"}
{"claim": "The paper does not explain how propagated errors from SAM-generated semantic masks are mitigated during image-to-LiDAR representation learning.", "claim_type": "methodology", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "P8fCEoWdti", "reviewer": "Reviewer_svRz", "review_text": "Summary: This work aims to tackle the image-to-LiDAR contrastive learning problem for LiDAR-based point cloud segmentation. Previous approaches designed the cross-modal contrastive learning objective for model pretraining, using superpixels and superpoints as guidance.\n\nIn this work, the authors observe that the superpixel-driven contrastive loss tends to involve ‘’self-conflict’’ issues during representation learning. A weakly-supervised contrastive distillation method is proposed, which generates semantic superpixels/superpoints using the Segment Anything Model (SAM). Additionally, to balance the imbalanced class distributions of LiDAR scene categories during representation, a density and category-aware sampling strategy is proposed to adjust the sampling probabilities of different anchor points using the weak semantic labels.\n\nThe overall framework is named OLIVINE, which adopts three optimization objectives:\n- Weakly-supervised contrastive distillation using coarse semantic labels to identify positive pairs by category.\n- Self-supervised contrastive distillation applied to randomly sampled point-pixel pairs.\n- A regularization framework based on the von Mises-Fisher (vMF) distribution to ensure semantic consistency.\n\nThe proposed OLIVINE method is evaluated on the nuScenes, SemanticKITTI, and KITTI object detection datasets. The results exhibit a consistent improvement of the proposed method compared to existing approaches.\n\nStrengths: (+) This work aims to improve the image-to-LiDAR self-supervised representation learning problem on LiDAR-based point cloud datasets, which is one of the current research hotspots, especially for applications related to autonomous driving and robotics.\n\n(+) The proposed method has exhibited promising performance on mainstream benchmarks, including nuScenes linear probing, nuScenes fine-tuning, SemanticKITTI fine-tuning, and KITTI object detection.\n\nWeaknesses: (-) The weakly-supervised contrastive distillation method has been used in previous literature, such as [R1] and [R2]. Adding semantic categories seems not to cause a major improvement over class-agnostic masks, as the Segment Anything Model is able to segment rather complete and semantically consistent objects and backgrounds. Additionally, using weak labels (which might be erroneous) could introduce additional errors during pretraining.\n\n(-) The motivation for using the von Mises-Fisher (vMF) distribution to enforce consistency regularization for image-to-LiDAR representation learning is not clear enough to demonstrate its superiority. A more detailed explanation and theoretical justification would strengthen this aspect of the work.\n\n(-) Compared to some of the most related works, for example, [R1] and [R3], the scale and depth regarding the experiments (for example, downstream fine-tuning on other datasets than SemanticKITTI) could be further enhanced.\n\n---\n\n### References:\n- [R1] Youquan Liu, et al. “Segment Any Point Cloud Sequences by Distilling Vision Foundation Models,” NeurIPS, 2023.\n- [R2] Ayça Takmaz, et al. “OpenMask3D: Open-Vocabulary 3D Instance Segmentation,” NeurIPS, 2023.\n- [R3] Gilles Puy, et al. “Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving,” arXiv, 2023.\n\nQuestions: - **Q1:** As mentioned in Weakness 1, the semantic masks generated by the Segment Anything Model could inevitably involve errors (e.g., wrong segmentation results). How do the authors handle the propagated errors during image-to-LiDAR representation learning?\n\n- **Q2:** As mentioned in Weakness 2, could the authors provide more details on the hyperparameter settings for the vMF distribution and the reasoning behind their chosen values? Adding a more detailed explanation and theoretical justification would be even better.\n\n- **Q3:** As mentioned in Weakness 3, having more thorough experimental analyses on other LiDAR-based point cloud datasets, such as SemanticPOSS, Waymo, SynLiDAR, etc., could further consolidate the findings and conclusions drawn in the manuscript.\n\n- **Q4:** As most 2D and 3D representation learning approaches (MoCo, SimCLR, Seal, etc.) do, having empirical analyses of models under out-of-distribution datasets is recommended. \n\n- **[Minor]:** The computational cost of the proposed multi-modal contrastive distillation approach is not thoroughly analyzed, which is crucial for real-time applications in autonomous driving.\n\n- **[Minor]:** The generalizability of OLIVINE to other types of sensors (for example, hybrid-solid LiDARs) or environments (for example, off-board environments) beyond the evaluated datasets is not discussed.\n\n- **[Minor]:** “NuScenes” should be revised to “nuScenes”.", "labeling_timestamp": "2026-01-11T16:22:18.722351", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes using SAM / VFMs to produce weak semantic labels (Introduction and Sec.3.2) but does not describe any mechanism to detect, filter, or otherwise mitigate errors propagated from those SAM-generated masks into the image-to-LiDAR distillation. No robustness, confidence-thresholding, noise-aware loss, or label-cleaning procedure is presented in the cited sections.", "evidence": "\"we utilize off-the-shelf VFMs to generate semantic labels for weakly-supervised pixel-to-point contrastive distillation.\" (Introduction)\n\n\"we utilize the Segment Anything Model, which adeptly interprets and translates semantic cues from text prompts into precise semantic segmentation of images. This application of SAM allows us to generate high-quality semantic labels without repetitive training, enhancing the learning process.\" (Sec. 3.2)", "section": "Introduction; 3.2 Weakly-supervised Contrastive Distillation"}
{"claim": "The manuscript does not provide evaluations on additional LiDAR datasets such as SemanticPOSS, Waymo, or SynLiDAR.", "claim_type": "experimental", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "P8fCEoWdti", "reviewer": "Reviewer_svRz", "review_text": "Summary: This work aims to tackle the image-to-LiDAR contrastive learning problem for LiDAR-based point cloud segmentation. Previous approaches designed the cross-modal contrastive learning objective for model pretraining, using superpixels and superpoints as guidance.\n\nIn this work, the authors observe that the superpixel-driven contrastive loss tends to involve ‘’self-conflict’’ issues during representation learning. A weakly-supervised contrastive distillation method is proposed, which generates semantic superpixels/superpoints using the Segment Anything Model (SAM). Additionally, to balance the imbalanced class distributions of LiDAR scene categories during representation, a density and category-aware sampling strategy is proposed to adjust the sampling probabilities of different anchor points using the weak semantic labels.\n\nThe overall framework is named OLIVINE, which adopts three optimization objectives:\n- Weakly-supervised contrastive distillation using coarse semantic labels to identify positive pairs by category.\n- Self-supervised contrastive distillation applied to randomly sampled point-pixel pairs.\n- A regularization framework based on the von Mises-Fisher (vMF) distribution to ensure semantic consistency.\n\nThe proposed OLIVINE method is evaluated on the nuScenes, SemanticKITTI, and KITTI object detection datasets. The results exhibit a consistent improvement of the proposed method compared to existing approaches.\n\nStrengths: (+) This work aims to improve the image-to-LiDAR self-supervised representation learning problem on LiDAR-based point cloud datasets, which is one of the current research hotspots, especially for applications related to autonomous driving and robotics.\n\n(+) The proposed method has exhibited promising performance on mainstream benchmarks, including nuScenes linear probing, nuScenes fine-tuning, SemanticKITTI fine-tuning, and KITTI object detection.\n\nWeaknesses: (-) The weakly-supervised contrastive distillation method has been used in previous literature, such as [R1] and [R2]. Adding semantic categories seems not to cause a major improvement over class-agnostic masks, as the Segment Anything Model is able to segment rather complete and semantically consistent objects and backgrounds. Additionally, using weak labels (which might be erroneous) could introduce additional errors during pretraining.\n\n(-) The motivation for using the von Mises-Fisher (vMF) distribution to enforce consistency regularization for image-to-LiDAR representation learning is not clear enough to demonstrate its superiority. A more detailed explanation and theoretical justification would strengthen this aspect of the work.\n\n(-) Compared to some of the most related works, for example, [R1] and [R3], the scale and depth regarding the experiments (for example, downstream fine-tuning on other datasets than SemanticKITTI) could be further enhanced.\n\n---\n\n### References:\n- [R1] Youquan Liu, et al. “Segment Any Point Cloud Sequences by Distilling Vision Foundation Models,” NeurIPS, 2023.\n- [R2] Ayça Takmaz, et al. “OpenMask3D: Open-Vocabulary 3D Instance Segmentation,” NeurIPS, 2023.\n- [R3] Gilles Puy, et al. “Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving,” arXiv, 2023.\n\nQuestions: - **Q1:** As mentioned in Weakness 1, the semantic masks generated by the Segment Anything Model could inevitably involve errors (e.g., wrong segmentation results). How do the authors handle the propagated errors during image-to-LiDAR representation learning?\n\n- **Q2:** As mentioned in Weakness 2, could the authors provide more details on the hyperparameter settings for the vMF distribution and the reasoning behind their chosen values? Adding a more detailed explanation and theoretical justification would be even better.\n\n- **Q3:** As mentioned in Weakness 3, having more thorough experimental analyses on other LiDAR-based point cloud datasets, such as SemanticPOSS, Waymo, SynLiDAR, etc., could further consolidate the findings and conclusions drawn in the manuscript.\n\n- **Q4:** As most 2D and 3D representation learning approaches (MoCo, SimCLR, Seal, etc.) do, having empirical analyses of models under out-of-distribution datasets is recommended. \n\n- **[Minor]:** The computational cost of the proposed multi-modal contrastive distillation approach is not thoroughly analyzed, which is crucial for real-time applications in autonomous driving.\n\n- **[Minor]:** The generalizability of OLIVINE to other types of sensors (for example, hybrid-solid LiDARs) or environments (for example, off-board environments) beyond the evaluated datasets is not discussed.\n\n- **[Minor]:** “NuScenes” should be revised to “nuScenes”.", "labeling_timestamp": "2026-01-11T16:22:12.063175", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper reports experiments only on nuScenes and SemanticKITTI and does not mention evaluations on SemanticPOSS, Waymo, or SynLiDAR anywhere in the provided content.", "evidence": "\"Extensive experiments reveal that our pre-training approach outperforms state-of-the-art 3D selfsupervised methods on both the nuScenes and SemanticKITTI datasets.\"", "section": "Introduction"}
{"claim": "The paper lacks empirical analyses of model performance on out-of-distribution datasets to assess robustness.", "claim_type": "experimental", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "P8fCEoWdti", "reviewer": "Reviewer_svRz", "review_text": "Summary: This work aims to tackle the image-to-LiDAR contrastive learning problem for LiDAR-based point cloud segmentation. Previous approaches designed the cross-modal contrastive learning objective for model pretraining, using superpixels and superpoints as guidance.\n\nIn this work, the authors observe that the superpixel-driven contrastive loss tends to involve ‘’self-conflict’’ issues during representation learning. A weakly-supervised contrastive distillation method is proposed, which generates semantic superpixels/superpoints using the Segment Anything Model (SAM). Additionally, to balance the imbalanced class distributions of LiDAR scene categories during representation, a density and category-aware sampling strategy is proposed to adjust the sampling probabilities of different anchor points using the weak semantic labels.\n\nThe overall framework is named OLIVINE, which adopts three optimization objectives:\n- Weakly-supervised contrastive distillation using coarse semantic labels to identify positive pairs by category.\n- Self-supervised contrastive distillation applied to randomly sampled point-pixel pairs.\n- A regularization framework based on the von Mises-Fisher (vMF) distribution to ensure semantic consistency.\n\nThe proposed OLIVINE method is evaluated on the nuScenes, SemanticKITTI, and KITTI object detection datasets. The results exhibit a consistent improvement of the proposed method compared to existing approaches.\n\nStrengths: (+) This work aims to improve the image-to-LiDAR self-supervised representation learning problem on LiDAR-based point cloud datasets, which is one of the current research hotspots, especially for applications related to autonomous driving and robotics.\n\n(+) The proposed method has exhibited promising performance on mainstream benchmarks, including nuScenes linear probing, nuScenes fine-tuning, SemanticKITTI fine-tuning, and KITTI object detection.\n\nWeaknesses: (-) The weakly-supervised contrastive distillation method has been used in previous literature, such as [R1] and [R2]. Adding semantic categories seems not to cause a major improvement over class-agnostic masks, as the Segment Anything Model is able to segment rather complete and semantically consistent objects and backgrounds. Additionally, using weak labels (which might be erroneous) could introduce additional errors during pretraining.\n\n(-) The motivation for using the von Mises-Fisher (vMF) distribution to enforce consistency regularization for image-to-LiDAR representation learning is not clear enough to demonstrate its superiority. A more detailed explanation and theoretical justification would strengthen this aspect of the work.\n\n(-) Compared to some of the most related works, for example, [R1] and [R3], the scale and depth regarding the experiments (for example, downstream fine-tuning on other datasets than SemanticKITTI) could be further enhanced.\n\n---\n\n### References:\n- [R1] Youquan Liu, et al. “Segment Any Point Cloud Sequences by Distilling Vision Foundation Models,” NeurIPS, 2023.\n- [R2] Ayça Takmaz, et al. “OpenMask3D: Open-Vocabulary 3D Instance Segmentation,” NeurIPS, 2023.\n- [R3] Gilles Puy, et al. “Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving,” arXiv, 2023.\n\nQuestions: - **Q1:** As mentioned in Weakness 1, the semantic masks generated by the Segment Anything Model could inevitably involve errors (e.g., wrong segmentation results). How do the authors handle the propagated errors during image-to-LiDAR representation learning?\n\n- **Q2:** As mentioned in Weakness 2, could the authors provide more details on the hyperparameter settings for the vMF distribution and the reasoning behind their chosen values? Adding a more detailed explanation and theoretical justification would be even better.\n\n- **Q3:** As mentioned in Weakness 3, having more thorough experimental analyses on other LiDAR-based point cloud datasets, such as SemanticPOSS, Waymo, SynLiDAR, etc., could further consolidate the findings and conclusions drawn in the manuscript.\n\n- **Q4:** As most 2D and 3D representation learning approaches (MoCo, SimCLR, Seal, etc.) do, having empirical analyses of models under out-of-distribution datasets is recommended. \n\n- **[Minor]:** The computational cost of the proposed multi-modal contrastive distillation approach is not thoroughly analyzed, which is crucial for real-time applications in autonomous driving.\n\n- **[Minor]:** The generalizability of OLIVINE to other types of sensors (for example, hybrid-solid LiDARs) or environments (for example, off-board environments) beyond the evaluated datasets is not discussed.\n\n- **[Minor]:** “NuScenes” should be revised to “nuScenes”.", "labeling_timestamp": "2026-01-11T16:22:12.193077", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper only reports experiments on standard benchmarks (nuScenes and SemanticKITTI) and does not present evaluations on out-of-distribution datasets or explicit robustness analyses in the provided content.", "evidence": "“Extensive experiments demonstrate that our approach mitigates the challenges posed by traditional methods and consistently surpasses existing image-to-LiDAR contrastive distillation methods in downstream tasks.”\n\n“Extensive experiments reveal that our pre-training approach outperforms state-of-the-art 3D selfsupervised methods on both the nuScenes and SemanticKITTI datasets.”", "section": "Abstract; 1 Introduction"}
{"claim": "The computational cost and runtime implications of the proposed multi-modal contrastive distillation are not thoroughly analyzed.", "claim_type": "methodology", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "P8fCEoWdti", "reviewer": "Reviewer_svRz", "review_text": "Summary: This work aims to tackle the image-to-LiDAR contrastive learning problem for LiDAR-based point cloud segmentation. Previous approaches designed the cross-modal contrastive learning objective for model pretraining, using superpixels and superpoints as guidance.\n\nIn this work, the authors observe that the superpixel-driven contrastive loss tends to involve ‘’self-conflict’’ issues during representation learning. A weakly-supervised contrastive distillation method is proposed, which generates semantic superpixels/superpoints using the Segment Anything Model (SAM). Additionally, to balance the imbalanced class distributions of LiDAR scene categories during representation, a density and category-aware sampling strategy is proposed to adjust the sampling probabilities of different anchor points using the weak semantic labels.\n\nThe overall framework is named OLIVINE, which adopts three optimization objectives:\n- Weakly-supervised contrastive distillation using coarse semantic labels to identify positive pairs by category.\n- Self-supervised contrastive distillation applied to randomly sampled point-pixel pairs.\n- A regularization framework based on the von Mises-Fisher (vMF) distribution to ensure semantic consistency.\n\nThe proposed OLIVINE method is evaluated on the nuScenes, SemanticKITTI, and KITTI object detection datasets. The results exhibit a consistent improvement of the proposed method compared to existing approaches.\n\nStrengths: (+) This work aims to improve the image-to-LiDAR self-supervised representation learning problem on LiDAR-based point cloud datasets, which is one of the current research hotspots, especially for applications related to autonomous driving and robotics.\n\n(+) The proposed method has exhibited promising performance on mainstream benchmarks, including nuScenes linear probing, nuScenes fine-tuning, SemanticKITTI fine-tuning, and KITTI object detection.\n\nWeaknesses: (-) The weakly-supervised contrastive distillation method has been used in previous literature, such as [R1] and [R2]. Adding semantic categories seems not to cause a major improvement over class-agnostic masks, as the Segment Anything Model is able to segment rather complete and semantically consistent objects and backgrounds. Additionally, using weak labels (which might be erroneous) could introduce additional errors during pretraining.\n\n(-) The motivation for using the von Mises-Fisher (vMF) distribution to enforce consistency regularization for image-to-LiDAR representation learning is not clear enough to demonstrate its superiority. A more detailed explanation and theoretical justification would strengthen this aspect of the work.\n\n(-) Compared to some of the most related works, for example, [R1] and [R3], the scale and depth regarding the experiments (for example, downstream fine-tuning on other datasets than SemanticKITTI) could be further enhanced.\n\n---\n\n### References:\n- [R1] Youquan Liu, et al. “Segment Any Point Cloud Sequences by Distilling Vision Foundation Models,” NeurIPS, 2023.\n- [R2] Ayça Takmaz, et al. “OpenMask3D: Open-Vocabulary 3D Instance Segmentation,” NeurIPS, 2023.\n- [R3] Gilles Puy, et al. “Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving,” arXiv, 2023.\n\nQuestions: - **Q1:** As mentioned in Weakness 1, the semantic masks generated by the Segment Anything Model could inevitably involve errors (e.g., wrong segmentation results). How do the authors handle the propagated errors during image-to-LiDAR representation learning?\n\n- **Q2:** As mentioned in Weakness 2, could the authors provide more details on the hyperparameter settings for the vMF distribution and the reasoning behind their chosen values? Adding a more detailed explanation and theoretical justification would be even better.\n\n- **Q3:** As mentioned in Weakness 3, having more thorough experimental analyses on other LiDAR-based point cloud datasets, such as SemanticPOSS, Waymo, SynLiDAR, etc., could further consolidate the findings and conclusions drawn in the manuscript.\n\n- **Q4:** As most 2D and 3D representation learning approaches (MoCo, SimCLR, Seal, etc.) do, having empirical analyses of models under out-of-distribution datasets is recommended. \n\n- **[Minor]:** The computational cost of the proposed multi-modal contrastive distillation approach is not thoroughly analyzed, which is crucial for real-time applications in autonomous driving.\n\n- **[Minor]:** The generalizability of OLIVINE to other types of sensors (for example, hybrid-solid LiDARs) or environments (for example, off-board environments) beyond the evaluated datasets is not discussed.\n\n- **[Minor]:** “NuScenes” should be revised to “nuScenes”.", "labeling_timestamp": "2026-01-11T16:22:22.281282", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper presents methodological details and experimental claims (e.g., outperforming prior methods) but contains no analysis, measurements, or discussion of computational cost, runtime, FLOPs, memory, or training/inference speed in the provided sections (Abstract, Introduction, Related Work, and Proposed Method).", "evidence": "Abstract: \"Extensive experiments demonstrate that our approach mitigates the challenges posed by traditional methods and consistently surpasses existing image-to-LiDAR contrastive distillation methods in downstream tasks.\"", "section": "Abstract / Entire provided content (no section presents computational/runtime analysis)"}
{"claim": "The generalizability of OLIVINE to different sensor types or environmental settings is not discussed.", "claim_type": "methodology", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "P8fCEoWdti", "reviewer": "Reviewer_svRz", "review_text": "Summary: This work aims to tackle the image-to-LiDAR contrastive learning problem for LiDAR-based point cloud segmentation. Previous approaches designed the cross-modal contrastive learning objective for model pretraining, using superpixels and superpoints as guidance.\n\nIn this work, the authors observe that the superpixel-driven contrastive loss tends to involve ‘’self-conflict’’ issues during representation learning. A weakly-supervised contrastive distillation method is proposed, which generates semantic superpixels/superpoints using the Segment Anything Model (SAM). Additionally, to balance the imbalanced class distributions of LiDAR scene categories during representation, a density and category-aware sampling strategy is proposed to adjust the sampling probabilities of different anchor points using the weak semantic labels.\n\nThe overall framework is named OLIVINE, which adopts three optimization objectives:\n- Weakly-supervised contrastive distillation using coarse semantic labels to identify positive pairs by category.\n- Self-supervised contrastive distillation applied to randomly sampled point-pixel pairs.\n- A regularization framework based on the von Mises-Fisher (vMF) distribution to ensure semantic consistency.\n\nThe proposed OLIVINE method is evaluated on the nuScenes, SemanticKITTI, and KITTI object detection datasets. The results exhibit a consistent improvement of the proposed method compared to existing approaches.\n\nStrengths: (+) This work aims to improve the image-to-LiDAR self-supervised representation learning problem on LiDAR-based point cloud datasets, which is one of the current research hotspots, especially for applications related to autonomous driving and robotics.\n\n(+) The proposed method has exhibited promising performance on mainstream benchmarks, including nuScenes linear probing, nuScenes fine-tuning, SemanticKITTI fine-tuning, and KITTI object detection.\n\nWeaknesses: (-) The weakly-supervised contrastive distillation method has been used in previous literature, such as [R1] and [R2]. Adding semantic categories seems not to cause a major improvement over class-agnostic masks, as the Segment Anything Model is able to segment rather complete and semantically consistent objects and backgrounds. Additionally, using weak labels (which might be erroneous) could introduce additional errors during pretraining.\n\n(-) The motivation for using the von Mises-Fisher (vMF) distribution to enforce consistency regularization for image-to-LiDAR representation learning is not clear enough to demonstrate its superiority. A more detailed explanation and theoretical justification would strengthen this aspect of the work.\n\n(-) Compared to some of the most related works, for example, [R1] and [R3], the scale and depth regarding the experiments (for example, downstream fine-tuning on other datasets than SemanticKITTI) could be further enhanced.\n\n---\n\n### References:\n- [R1] Youquan Liu, et al. “Segment Any Point Cloud Sequences by Distilling Vision Foundation Models,” NeurIPS, 2023.\n- [R2] Ayça Takmaz, et al. “OpenMask3D: Open-Vocabulary 3D Instance Segmentation,” NeurIPS, 2023.\n- [R3] Gilles Puy, et al. “Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving,” arXiv, 2023.\n\nQuestions: - **Q1:** As mentioned in Weakness 1, the semantic masks generated by the Segment Anything Model could inevitably involve errors (e.g., wrong segmentation results). How do the authors handle the propagated errors during image-to-LiDAR representation learning?\n\n- **Q2:** As mentioned in Weakness 2, could the authors provide more details on the hyperparameter settings for the vMF distribution and the reasoning behind their chosen values? Adding a more detailed explanation and theoretical justification would be even better.\n\n- **Q3:** As mentioned in Weakness 3, having more thorough experimental analyses on other LiDAR-based point cloud datasets, such as SemanticPOSS, Waymo, SynLiDAR, etc., could further consolidate the findings and conclusions drawn in the manuscript.\n\n- **Q4:** As most 2D and 3D representation learning approaches (MoCo, SimCLR, Seal, etc.) do, having empirical analyses of models under out-of-distribution datasets is recommended. \n\n- **[Minor]:** The computational cost of the proposed multi-modal contrastive distillation approach is not thoroughly analyzed, which is crucial for real-time applications in autonomous driving.\n\n- **[Minor]:** The generalizability of OLIVINE to other types of sensors (for example, hybrid-solid LiDARs) or environments (for example, off-board environments) beyond the evaluated datasets is not discussed.\n\n- **[Minor]:** “NuScenes” should be revised to “nuScenes”.", "labeling_timestamp": "2026-01-11T16:22:24.624655", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper frames the problem around LiDAR + image pairs and reports experiments only on nuScenes and SemanticKITTI, but does not discuss applying OLIVINE to other sensor types or different environmental settings (e.g., different LiDAR hardware, indoor scenes, adverse weather).", "evidence": "1) \"Extensive experiments reveal that our pre-training approach outperforms state-of-the-art 3D selfsupervised methods on both the nuScenes and SemanticKITTI datasets.\" 2) \"Let P = { p1, p2, ..., pN | pi ∈ R3 } be a point cloud consisting of N points collected by a LiDAR sensor, and I = {Ic | c = 1, ..., Ncam} multi-view images captured by Ncam synchronized cameras, where Ic ∈ RH×W×3 is a single image...\"", "section": "Introduction; 3 Proposed Method - Notation"}
{"claim": "The manuscript uses the capitalized dataset name 'NuScenes' instead of the correct 'nuScenes' formatting.", "claim_type": "other", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "P8fCEoWdti", "reviewer": "Reviewer_svRz", "review_text": "Summary: This work aims to tackle the image-to-LiDAR contrastive learning problem for LiDAR-based point cloud segmentation. Previous approaches designed the cross-modal contrastive learning objective for model pretraining, using superpixels and superpoints as guidance.\n\nIn this work, the authors observe that the superpixel-driven contrastive loss tends to involve ‘’self-conflict’’ issues during representation learning. A weakly-supervised contrastive distillation method is proposed, which generates semantic superpixels/superpoints using the Segment Anything Model (SAM). Additionally, to balance the imbalanced class distributions of LiDAR scene categories during representation, a density and category-aware sampling strategy is proposed to adjust the sampling probabilities of different anchor points using the weak semantic labels.\n\nThe overall framework is named OLIVINE, which adopts three optimization objectives:\n- Weakly-supervised contrastive distillation using coarse semantic labels to identify positive pairs by category.\n- Self-supervised contrastive distillation applied to randomly sampled point-pixel pairs.\n- A regularization framework based on the von Mises-Fisher (vMF) distribution to ensure semantic consistency.\n\nThe proposed OLIVINE method is evaluated on the nuScenes, SemanticKITTI, and KITTI object detection datasets. The results exhibit a consistent improvement of the proposed method compared to existing approaches.\n\nStrengths: (+) This work aims to improve the image-to-LiDAR self-supervised representation learning problem on LiDAR-based point cloud datasets, which is one of the current research hotspots, especially for applications related to autonomous driving and robotics.\n\n(+) The proposed method has exhibited promising performance on mainstream benchmarks, including nuScenes linear probing, nuScenes fine-tuning, SemanticKITTI fine-tuning, and KITTI object detection.\n\nWeaknesses: (-) The weakly-supervised contrastive distillation method has been used in previous literature, such as [R1] and [R2]. Adding semantic categories seems not to cause a major improvement over class-agnostic masks, as the Segment Anything Model is able to segment rather complete and semantically consistent objects and backgrounds. Additionally, using weak labels (which might be erroneous) could introduce additional errors during pretraining.\n\n(-) The motivation for using the von Mises-Fisher (vMF) distribution to enforce consistency regularization for image-to-LiDAR representation learning is not clear enough to demonstrate its superiority. A more detailed explanation and theoretical justification would strengthen this aspect of the work.\n\n(-) Compared to some of the most related works, for example, [R1] and [R3], the scale and depth regarding the experiments (for example, downstream fine-tuning on other datasets than SemanticKITTI) could be further enhanced.\n\n---\n\n### References:\n- [R1] Youquan Liu, et al. “Segment Any Point Cloud Sequences by Distilling Vision Foundation Models,” NeurIPS, 2023.\n- [R2] Ayça Takmaz, et al. “OpenMask3D: Open-Vocabulary 3D Instance Segmentation,” NeurIPS, 2023.\n- [R3] Gilles Puy, et al. “Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving,” arXiv, 2023.\n\nQuestions: - **Q1:** As mentioned in Weakness 1, the semantic masks generated by the Segment Anything Model could inevitably involve errors (e.g., wrong segmentation results). How do the authors handle the propagated errors during image-to-LiDAR representation learning?\n\n- **Q2:** As mentioned in Weakness 2, could the authors provide more details on the hyperparameter settings for the vMF distribution and the reasoning behind their chosen values? Adding a more detailed explanation and theoretical justification would be even better.\n\n- **Q3:** As mentioned in Weakness 3, having more thorough experimental analyses on other LiDAR-based point cloud datasets, such as SemanticPOSS, Waymo, SynLiDAR, etc., could further consolidate the findings and conclusions drawn in the manuscript.\n\n- **Q4:** As most 2D and 3D representation learning approaches (MoCo, SimCLR, Seal, etc.) do, having empirical analyses of models under out-of-distribution datasets is recommended. \n\n- **[Minor]:** The computational cost of the proposed multi-modal contrastive distillation approach is not thoroughly analyzed, which is crucial for real-time applications in autonomous driving.\n\n- **[Minor]:** The generalizability of OLIVINE to other types of sensors (for example, hybrid-solid LiDARs) or environments (for example, off-board environments) beyond the evaluated datasets is not discussed.\n\n- **[Minor]:** “NuScenes” should be revised to “nuScenes”.", "labeling_timestamp": "2026-01-11T16:22:36.208390", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper consistently uses the 'nuScenes' formatting (lowercase 'nu' with capital 'S'), not 'NuScenes'. Multiple instances in the Introduction use 'nuScenes' and 'nuSceneslidarseg', contradicting the reviewer's claim.", "evidence": "1) \"For example, bicycles comprise only 1.47% of annotations in the nuSceneslidarseg dataset, whereas drivable surfaces make up 37.66%.\" 2) \"Extensive experiments reveal that our pre-training approach outperforms state-of-the-art 3D selfsupervised methods on both the nuScenes and SemanticKITTI datasets.\"", "section": "1 Introduction"}
{"claim": "The paper does not demonstrate that vMF-based regularization yields superior performance compared to alternative regularizers.", "claim_type": "baseline", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "P8fCEoWdti", "reviewer": "Reviewer_svRz", "review_text": "Summary: This work aims to tackle the image-to-LiDAR contrastive learning problem for LiDAR-based point cloud segmentation. Previous approaches designed the cross-modal contrastive learning objective for model pretraining, using superpixels and superpoints as guidance.\n\nIn this work, the authors observe that the superpixel-driven contrastive loss tends to involve ‘’self-conflict’’ issues during representation learning. A weakly-supervised contrastive distillation method is proposed, which generates semantic superpixels/superpoints using the Segment Anything Model (SAM). Additionally, to balance the imbalanced class distributions of LiDAR scene categories during representation, a density and category-aware sampling strategy is proposed to adjust the sampling probabilities of different anchor points using the weak semantic labels.\n\nThe overall framework is named OLIVINE, which adopts three optimization objectives:\n- Weakly-supervised contrastive distillation using coarse semantic labels to identify positive pairs by category.\n- Self-supervised contrastive distillation applied to randomly sampled point-pixel pairs.\n- A regularization framework based on the von Mises-Fisher (vMF) distribution to ensure semantic consistency.\n\nThe proposed OLIVINE method is evaluated on the nuScenes, SemanticKITTI, and KITTI object detection datasets. The results exhibit a consistent improvement of the proposed method compared to existing approaches.\n\nStrengths: (+) This work aims to improve the image-to-LiDAR self-supervised representation learning problem on LiDAR-based point cloud datasets, which is one of the current research hotspots, especially for applications related to autonomous driving and robotics.\n\n(+) The proposed method has exhibited promising performance on mainstream benchmarks, including nuScenes linear probing, nuScenes fine-tuning, SemanticKITTI fine-tuning, and KITTI object detection.\n\nWeaknesses: (-) The weakly-supervised contrastive distillation method has been used in previous literature, such as [R1] and [R2]. Adding semantic categories seems not to cause a major improvement over class-agnostic masks, as the Segment Anything Model is able to segment rather complete and semantically consistent objects and backgrounds. Additionally, using weak labels (which might be erroneous) could introduce additional errors during pretraining.\n\n(-) The motivation for using the von Mises-Fisher (vMF) distribution to enforce consistency regularization for image-to-LiDAR representation learning is not clear enough to demonstrate its superiority. A more detailed explanation and theoretical justification would strengthen this aspect of the work.\n\n(-) Compared to some of the most related works, for example, [R1] and [R3], the scale and depth regarding the experiments (for example, downstream fine-tuning on other datasets than SemanticKITTI) could be further enhanced.\n\n---\n\n### References:\n- [R1] Youquan Liu, et al. “Segment Any Point Cloud Sequences by Distilling Vision Foundation Models,” NeurIPS, 2023.\n- [R2] Ayça Takmaz, et al. “OpenMask3D: Open-Vocabulary 3D Instance Segmentation,” NeurIPS, 2023.\n- [R3] Gilles Puy, et al. “Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving,” arXiv, 2023.\n\nQuestions: - **Q1:** As mentioned in Weakness 1, the semantic masks generated by the Segment Anything Model could inevitably involve errors (e.g., wrong segmentation results). How do the authors handle the propagated errors during image-to-LiDAR representation learning?\n\n- **Q2:** As mentioned in Weakness 2, could the authors provide more details on the hyperparameter settings for the vMF distribution and the reasoning behind their chosen values? Adding a more detailed explanation and theoretical justification would be even better.\n\n- **Q3:** As mentioned in Weakness 3, having more thorough experimental analyses on other LiDAR-based point cloud datasets, such as SemanticPOSS, Waymo, SynLiDAR, etc., could further consolidate the findings and conclusions drawn in the manuscript.\n\n- **Q4:** As most 2D and 3D representation learning approaches (MoCo, SimCLR, Seal, etc.) do, having empirical analyses of models under out-of-distribution datasets is recommended. \n\n- **[Minor]:** The computational cost of the proposed multi-modal contrastive distillation approach is not thoroughly analyzed, which is crucial for real-time applications in autonomous driving.\n\n- **[Minor]:** The generalizability of OLIVINE to other types of sensors (for example, hybrid-solid LiDARs) or environments (for example, off-board environments) beyond the evaluated datasets is not discussed.\n\n- **[Minor]:** “NuScenes” should be revised to “nuScenes”.", "labeling_timestamp": "2026-01-11T16:22:36.767558", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper clearly introduces a von Mises–Fisher (vMF) based semantic-guided consistency regularization (multiple statements in Abstract, Introduction, and Section 3), but the provided content contains no experimental comparison or ablation that demonstrates vMF regularization is superior to alternative regularizers. The experiments described claim overall superiority over existing image-to-LiDAR distillation methods, but do not report direct comparisons between vMF and other regularization schemes.", "evidence": "Abstract: \"Additionally, we employ von Mises-Fisher distributions to structure the feature space, ensuring semantic embeddings within the same class remain consistent across varying inputs.\"; Introduction: \"...we introduce semantic-guided consistency regularization to enhance 3D representation learning. This approach structures the feature space by modeling each class with a von Mises-Fisher distribution and making point features adhere closely to their respective distributions.\"; Overview (Sec.3): \"The representation learning in OLIVINE is driven by three objectives: ... and a regularization framework based on the von Mises-Fisher distribution to ensure semantic consistency.\"; Experiments summary: \"Extensive experiments reveal that our pre-training approach outperforms state-of-the-art 3D selfsupervised methods on both the nuScenes and SemanticKITTI datasets.\"", "section": "Abstract; Introduction; Section 3 (Proposed Method)"}
{"claim": "The paper does not sufficiently compare OLIVINE against closely related methods like R1, R2, and R3.", "claim_type": "baseline", "paper_id": "63xeWav1lU", "paper_title": "Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "P8fCEoWdti", "reviewer": "Reviewer_svRz", "review_text": "Summary: This work aims to tackle the image-to-LiDAR contrastive learning problem for LiDAR-based point cloud segmentation. Previous approaches designed the cross-modal contrastive learning objective for model pretraining, using superpixels and superpoints as guidance.\n\nIn this work, the authors observe that the superpixel-driven contrastive loss tends to involve ‘’self-conflict’’ issues during representation learning. A weakly-supervised contrastive distillation method is proposed, which generates semantic superpixels/superpoints using the Segment Anything Model (SAM). Additionally, to balance the imbalanced class distributions of LiDAR scene categories during representation, a density and category-aware sampling strategy is proposed to adjust the sampling probabilities of different anchor points using the weak semantic labels.\n\nThe overall framework is named OLIVINE, which adopts three optimization objectives:\n- Weakly-supervised contrastive distillation using coarse semantic labels to identify positive pairs by category.\n- Self-supervised contrastive distillation applied to randomly sampled point-pixel pairs.\n- A regularization framework based on the von Mises-Fisher (vMF) distribution to ensure semantic consistency.\n\nThe proposed OLIVINE method is evaluated on the nuScenes, SemanticKITTI, and KITTI object detection datasets. The results exhibit a consistent improvement of the proposed method compared to existing approaches.\n\nStrengths: (+) This work aims to improve the image-to-LiDAR self-supervised representation learning problem on LiDAR-based point cloud datasets, which is one of the current research hotspots, especially for applications related to autonomous driving and robotics.\n\n(+) The proposed method has exhibited promising performance on mainstream benchmarks, including nuScenes linear probing, nuScenes fine-tuning, SemanticKITTI fine-tuning, and KITTI object detection.\n\nWeaknesses: (-) The weakly-supervised contrastive distillation method has been used in previous literature, such as [R1] and [R2]. Adding semantic categories seems not to cause a major improvement over class-agnostic masks, as the Segment Anything Model is able to segment rather complete and semantically consistent objects and backgrounds. Additionally, using weak labels (which might be erroneous) could introduce additional errors during pretraining.\n\n(-) The motivation for using the von Mises-Fisher (vMF) distribution to enforce consistency regularization for image-to-LiDAR representation learning is not clear enough to demonstrate its superiority. A more detailed explanation and theoretical justification would strengthen this aspect of the work.\n\n(-) Compared to some of the most related works, for example, [R1] and [R3], the scale and depth regarding the experiments (for example, downstream fine-tuning on other datasets than SemanticKITTI) could be further enhanced.\n\n---\n\n### References:\n- [R1] Youquan Liu, et al. “Segment Any Point Cloud Sequences by Distilling Vision Foundation Models,” NeurIPS, 2023.\n- [R2] Ayça Takmaz, et al. “OpenMask3D: Open-Vocabulary 3D Instance Segmentation,” NeurIPS, 2023.\n- [R3] Gilles Puy, et al. “Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving,” arXiv, 2023.\n\nQuestions: - **Q1:** As mentioned in Weakness 1, the semantic masks generated by the Segment Anything Model could inevitably involve errors (e.g., wrong segmentation results). How do the authors handle the propagated errors during image-to-LiDAR representation learning?\n\n- **Q2:** As mentioned in Weakness 2, could the authors provide more details on the hyperparameter settings for the vMF distribution and the reasoning behind their chosen values? Adding a more detailed explanation and theoretical justification would be even better.\n\n- **Q3:** As mentioned in Weakness 3, having more thorough experimental analyses on other LiDAR-based point cloud datasets, such as SemanticPOSS, Waymo, SynLiDAR, etc., could further consolidate the findings and conclusions drawn in the manuscript.\n\n- **Q4:** As most 2D and 3D representation learning approaches (MoCo, SimCLR, Seal, etc.) do, having empirical analyses of models under out-of-distribution datasets is recommended. \n\n- **[Minor]:** The computational cost of the proposed multi-modal contrastive distillation approach is not thoroughly analyzed, which is crucial for real-time applications in autonomous driving.\n\n- **[Minor]:** The generalizability of OLIVINE to other types of sensors (for example, hybrid-solid LiDARs) or environments (for example, off-board environments) beyond the evaluated datasets is not discussed.\n\n- **[Minor]:** “NuScenes” should be revised to “nuScenes”.", "labeling_timestamp": "2026-01-11T16:22:43.397932", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper claims to outperform existing image-to-LiDAR contrastive distillation methods (Abstract, Introduction) and lists related prior works (Related Work), but the provided excerpt does not include the experimental results, tables, or explicit comparisons vs. the specific methods R1, R2, and R3. Therefore it is not possible from the given content to confirm whether comparisons against those particular methods are present or sufficient.", "evidence": "Abstract: \"Extensive experiments demonstrate that our approach mitigates the challenges posed by traditional methods and consistently surpasses existing image-to-LiDAR contrastive distillation methods in downstream tasks.\" \nIntroduction: \"Extensive experiments reveal that our pre-training approach outperforms state-of-the-art 3D selfsupervised methods on both the nuScenes and SemanticKITTI datasets.\" \nRelated Work: \"PPKT [39] ... SLidR [50] ... Seal [38] ... HVDistill [73] ...\"", "section": "Abstract; 1 Introduction; 2 Related Work"}
{"claim": "The paper does not provide strong arguments that hybrid domain adaptation is a meaningful, distinct research task compared to existing domain adaptation and generalization fields.", "claim_type": "novelty", "paper_id": "FE2e8664Sl", "paper_title": "Few-shot Hybrid Domain Adaptation of Image Generator", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "u2SOmGevjE", "reviewer": "Reviewer_fvXy", "review_text": "Summary: The paper introduces the few-shot generative Hybrid Domain Adaptation (HDA) task for image-based data. It is assumed that an image generator is accessed from a source domain and then from several target domains. Adaptation is performed on all target domains. To perform HDA, the paper presents a discriminator-free approach with a style GAN pre-trained on a source domain. It is adapted to individual target domains using a few shots from each domain and generates an image style showing composite attributes of each target domain separately.  The proposed approach relies on features extracted from standard transformer-based architectures. In addition, distance and direction losses are used to guide the generator to produce images with integrated features of the target domains while preserving the features of the source domain. Experiments are conducted using the FFHQ dataset and three evaluation metrics are used to measure performance and compare with different baselines.\n\nStrengths: - The paper is well written and easy to understand. The method is well described and the experiments are clearly presented. \n\n- The discriminator-free approach presented is interesting as it reduces the computational cost of fitting compared to existing approaches. \n\n- The approach shows promising results for all metrics compared to the baselines on the FFHQ dataset.\n\nWeaknesses: - (Major) The paper does not provide strong arguments as to why hybrid domain adaptation is a meaningful task. It is not clear why the current approaches and benchmarks, e.g. domain adaptation and domain generalisation, are not sufficient to address the problem presented. These points need further elaboration to motivate the paper. \n\n- (Major) results are reported using 10 shots, while it would be interesting to see results using different numbers of shots, e.g. 1-shot and 5-shot.\n\n- It is unclear whether the methodology is applicable to the generation of conditioned image styles. For example, generating an image that is conditioned on one or more styles of the target domain, rather than an image that spans all attributes of the target domains. It could discussed how the proposed methodology generalises to conditional generation. \n\n- A study of the effect of different pre-trained image encoders would add value to the paper.\n\nQuestions: - How would the proposed generator compare with well-known generator models such as DALLE, Imagen and etc.?", "labeling_timestamp": "2026-01-11T16:23:10.604904", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly introduces HDA as a new task and provides several arguments why it is distinct from standard DA and from inference-time interpolation approaches, citing concrete challenges (lack of hybrid-domain images, discriminator overfitting) and practical advantages (flexibility, single adapted model vs multiple per-domain models). These points are presented in the Abstract, Introduction, and Related Works.", "evidence": "Abstract: \"Can a pre-trained generator be adapted to the hybrid of multiple target domains and generate images with integrated attributes of them? In this work, we introduce a new task - Few-shot Hybrid Domain Adaptation (HDA). ... Compared with Domain Adaptation (DA), HDA offers greater flexibility and versatility to adapt generators to more composite and expansive domains. Simultaneously, HDA also presents more challenges than DA as we have access only to images from individual target domains and lack authentic images from the hybrid domain.\" Introduction: \"Compared with conventional DA, HDA is more challenging in two aspects: (1) Existing DA approaches typically employ the discriminator ... we only have images sampled from individual domains and lack real images of the hybrid domain, which presents challenges for designing the discriminator-based adaptation framework. (2) Since there are extremely few reference images, the discriminator could easily overfit ... leading to missing the characteristics from other target domains in the generator.\" Related Works: \"However, the approach of inference time interpolation needs to train models on multiple individual domains, which necessitates multiple times the model size and training time. In contrast, our method can acquire one model for the hybrid domain in just few minutes.\"", "section": "Abstract; 1 INTRODUCTION; 2 RELATED WORKS"}
{"claim": "The paper does not justify why existing domain adaptation and domain generalization benchmarks are insufficient to address the proposed hybrid domain adaptation problem.", "claim_type": "subjective", "paper_id": "FE2e8664Sl", "paper_title": "Few-shot Hybrid Domain Adaptation of Image Generator", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "u2SOmGevjE", "reviewer": "Reviewer_fvXy", "review_text": "Summary: The paper introduces the few-shot generative Hybrid Domain Adaptation (HDA) task for image-based data. It is assumed that an image generator is accessed from a source domain and then from several target domains. Adaptation is performed on all target domains. To perform HDA, the paper presents a discriminator-free approach with a style GAN pre-trained on a source domain. It is adapted to individual target domains using a few shots from each domain and generates an image style showing composite attributes of each target domain separately.  The proposed approach relies on features extracted from standard transformer-based architectures. In addition, distance and direction losses are used to guide the generator to produce images with integrated features of the target domains while preserving the features of the source domain. Experiments are conducted using the FFHQ dataset and three evaluation metrics are used to measure performance and compare with different baselines.\n\nStrengths: - The paper is well written and easy to understand. The method is well described and the experiments are clearly presented. \n\n- The discriminator-free approach presented is interesting as it reduces the computational cost of fitting compared to existing approaches. \n\n- The approach shows promising results for all metrics compared to the baselines on the FFHQ dataset.\n\nWeaknesses: - (Major) The paper does not provide strong arguments as to why hybrid domain adaptation is a meaningful task. It is not clear why the current approaches and benchmarks, e.g. domain adaptation and domain generalisation, are not sufficient to address the problem presented. These points need further elaboration to motivate the paper. \n\n- (Major) results are reported using 10 shots, while it would be interesting to see results using different numbers of shots, e.g. 1-shot and 5-shot.\n\n- It is unclear whether the methodology is applicable to the generation of conditioned image styles. For example, generating an image that is conditioned on one or more styles of the target domain, rather than an image that spans all attributes of the target domains. It could discussed how the proposed methodology generalises to conditional generation. \n\n- A study of the effect of different pre-trained image encoders would add value to the paper.\n\nQuestions: - How would the proposed generator compare with well-known generator models such as DALLE, Imagen and etc.?", "labeling_timestamp": "2026-01-11T16:23:03.087386", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper does provide explicit justifications for why existing domain adaptation (DA) methods are insufficient for the hybrid domain adaptation (HDA) setting (e.g., lack of real hybrid-domain images, discriminator overfitting, and inference-time interpolation requiring multiple per-domain models). However, the paper does not discuss or justify why existing domain generalization benchmarks (or benchmarks in general) are insufficient for HDA; there is no explicit treatment of 'benchmarks' or 'domain generalization' in the text.", "evidence": [{"quote": "Compared with Domain Adaptation (DA), HDA offers greater flexibility and versatility to adapt generators to more composite and expansive domains. Simultaneously, HDA also presents more challenges than DA as we have access only to images from individual target domains and lack authentic images from the hybrid domain.", "section": "Abstract"}, {"quote": "Under such circumstances, conventional DA becomes less feasible.", "section": "1 INTRODUCTION"}, {"quote": "Compared with conventional DA, HDA is more challenging in two aspects: (1) Existing DA approaches typically employ the discriminator to discern whether generated images belong to the target domain. However, we only have images sampled from individual domains and lack real images of the hybrid domain, which presents challenges for designing the discriminator-based adaptation framework. (2) Since there are extremely few reference images, the discriminator could easily overfit the easy-to-learn characteristics of certain target domains (Ojha et al., 2021), leading to missing the characteristics from other target domains in the generator.", "section": "1 INTRODUCTION"}, {"quote": "While previous works achieve promising progress toward generative domain adaptation, they rely fundamentally on the discriminator which makes it difficult to handle hybrid domain adaptation.", "section": "2 RELATED WORKS"}, {"quote": "However, the approach of inference time interpolation needs to train models on multiple individual domains, which necessitates multiple times the model size and training time. In contrast, our method can acquire one model for the hybrid domain in just few minutes.", "section": "2 RELATED WORKS"}], "section": ""}
{"claim": "Results are reported only for 10-shot experiments; the paper lacks results for other shot counts such as 1-shot and 5-shot to assess performance sensitivity.", "claim_type": "experimental", "paper_id": "FE2e8664Sl", "paper_title": "Few-shot Hybrid Domain Adaptation of Image Generator", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "u2SOmGevjE", "reviewer": "Reviewer_fvXy", "review_text": "Summary: The paper introduces the few-shot generative Hybrid Domain Adaptation (HDA) task for image-based data. It is assumed that an image generator is accessed from a source domain and then from several target domains. Adaptation is performed on all target domains. To perform HDA, the paper presents a discriminator-free approach with a style GAN pre-trained on a source domain. It is adapted to individual target domains using a few shots from each domain and generates an image style showing composite attributes of each target domain separately.  The proposed approach relies on features extracted from standard transformer-based architectures. In addition, distance and direction losses are used to guide the generator to produce images with integrated features of the target domains while preserving the features of the source domain. Experiments are conducted using the FFHQ dataset and three evaluation metrics are used to measure performance and compare with different baselines.\n\nStrengths: - The paper is well written and easy to understand. The method is well described and the experiments are clearly presented. \n\n- The discriminator-free approach presented is interesting as it reduces the computational cost of fitting compared to existing approaches. \n\n- The approach shows promising results for all metrics compared to the baselines on the FFHQ dataset.\n\nWeaknesses: - (Major) The paper does not provide strong arguments as to why hybrid domain adaptation is a meaningful task. It is not clear why the current approaches and benchmarks, e.g. domain adaptation and domain generalisation, are not sufficient to address the problem presented. These points need further elaboration to motivate the paper. \n\n- (Major) results are reported using 10 shots, while it would be interesting to see results using different numbers of shots, e.g. 1-shot and 5-shot.\n\n- It is unclear whether the methodology is applicable to the generation of conditioned image styles. For example, generating an image that is conditioned on one or more styles of the target domain, rather than an image that spans all attributes of the target domains. It could discussed how the proposed methodology generalises to conditional generation. \n\n- A study of the effect of different pre-trained image encoders would add value to the paper.\n\nQuestions: - How would the proposed generator compare with well-known generator models such as DALLE, Imagen and etc.?", "labeling_timestamp": "2026-01-11T16:23:02.237512", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt (Abstract, Introduction, and Method) repeatedly refers to 'few-shot' references but does not report experimental results or specify which shot counts (e.g., 1-shot, 5-shot, 10-shot) were used. Therefore, from the supplied content we cannot determine whether the paper reports results only for 10-shot experiments.", "evidence": "'Few-shot Hybrid Domain Adaptation (HDA).' 'Given a source generator and few-shot images of several target domains, HDA aims to acquire an adapted image generator...' 'few-shot references from the individual sketch, baby and smile domains' — no mention of specific shot counts such as 1-shot, 5-shot, or 10-shot in the provided text.", "section": "Abstract / 1 INTRODUCTION"}
{"claim": "The paper does not include experiments or analysis showing applicability of the methodology to conditional image generation conditioned on one or more target styles.", "claim_type": "experimental", "paper_id": "FE2e8664Sl", "paper_title": "Few-shot Hybrid Domain Adaptation of Image Generator", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "u2SOmGevjE", "reviewer": "Reviewer_fvXy", "review_text": "Summary: The paper introduces the few-shot generative Hybrid Domain Adaptation (HDA) task for image-based data. It is assumed that an image generator is accessed from a source domain and then from several target domains. Adaptation is performed on all target domains. To perform HDA, the paper presents a discriminator-free approach with a style GAN pre-trained on a source domain. It is adapted to individual target domains using a few shots from each domain and generates an image style showing composite attributes of each target domain separately.  The proposed approach relies on features extracted from standard transformer-based architectures. In addition, distance and direction losses are used to guide the generator to produce images with integrated features of the target domains while preserving the features of the source domain. Experiments are conducted using the FFHQ dataset and three evaluation metrics are used to measure performance and compare with different baselines.\n\nStrengths: - The paper is well written and easy to understand. The method is well described and the experiments are clearly presented. \n\n- The discriminator-free approach presented is interesting as it reduces the computational cost of fitting compared to existing approaches. \n\n- The approach shows promising results for all metrics compared to the baselines on the FFHQ dataset.\n\nWeaknesses: - (Major) The paper does not provide strong arguments as to why hybrid domain adaptation is a meaningful task. It is not clear why the current approaches and benchmarks, e.g. domain adaptation and domain generalisation, are not sufficient to address the problem presented. These points need further elaboration to motivate the paper. \n\n- (Major) results are reported using 10 shots, while it would be interesting to see results using different numbers of shots, e.g. 1-shot and 5-shot.\n\n- It is unclear whether the methodology is applicable to the generation of conditioned image styles. For example, generating an image that is conditioned on one or more styles of the target domain, rather than an image that spans all attributes of the target domains. It could discussed how the proposed methodology generalises to conditional generation. \n\n- A study of the effect of different pre-trained image encoders would add value to the paper.\n\nQuestions: - How would the proposed generator compare with well-known generator models such as DALLE, Imagen and etc.?", "labeling_timestamp": "2026-01-11T16:23:15.271901", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper consistently describes HDA as producing a single adapted generator that integrates attributes from multiple target domains and compares against inference-time interpolation methods, but it does not present experiments or analysis for conditional (per-style or per-target) image generation at inference time.", "evidence": "HDA aims to acquire an adapted image generator that preserves the integrated attributes of all target domains, without overriding the original characteristics.", "section": "Abstract"}
{"claim": "The paper fails to discuss how the proposed methodology generalizes to conditional generation scenarios rather than producing images spanning all target-domain attributes.", "claim_type": "methodology", "paper_id": "FE2e8664Sl", "paper_title": "Few-shot Hybrid Domain Adaptation of Image Generator", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "u2SOmGevjE", "reviewer": "Reviewer_fvXy", "review_text": "Summary: The paper introduces the few-shot generative Hybrid Domain Adaptation (HDA) task for image-based data. It is assumed that an image generator is accessed from a source domain and then from several target domains. Adaptation is performed on all target domains. To perform HDA, the paper presents a discriminator-free approach with a style GAN pre-trained on a source domain. It is adapted to individual target domains using a few shots from each domain and generates an image style showing composite attributes of each target domain separately.  The proposed approach relies on features extracted from standard transformer-based architectures. In addition, distance and direction losses are used to guide the generator to produce images with integrated features of the target domains while preserving the features of the source domain. Experiments are conducted using the FFHQ dataset and three evaluation metrics are used to measure performance and compare with different baselines.\n\nStrengths: - The paper is well written and easy to understand. The method is well described and the experiments are clearly presented. \n\n- The discriminator-free approach presented is interesting as it reduces the computational cost of fitting compared to existing approaches. \n\n- The approach shows promising results for all metrics compared to the baselines on the FFHQ dataset.\n\nWeaknesses: - (Major) The paper does not provide strong arguments as to why hybrid domain adaptation is a meaningful task. It is not clear why the current approaches and benchmarks, e.g. domain adaptation and domain generalisation, are not sufficient to address the problem presented. These points need further elaboration to motivate the paper. \n\n- (Major) results are reported using 10 shots, while it would be interesting to see results using different numbers of shots, e.g. 1-shot and 5-shot.\n\n- It is unclear whether the methodology is applicable to the generation of conditioned image styles. For example, generating an image that is conditioned on one or more styles of the target domain, rather than an image that spans all attributes of the target domains. It could discussed how the proposed methodology generalises to conditional generation. \n\n- A study of the effect of different pre-trained image encoders would add value to the paper.\n\nQuestions: - How would the proposed generator compare with well-known generator models such as DALLE, Imagen and etc.?", "labeling_timestamp": "2026-01-11T16:23:02.796531", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper consistently frames HDA as learning a single adapted generator that produces images with integrated attributes from all target domains and does not discuss or analyze conditional/controllable generation (i.e., generating different attribute combinations on demand). Thus it fails to address generalization to conditional generation scenarios.", "evidence": "\"HDA aims to acquire an adapted generator that preserves the integrated attributes of all target domains, without overriding the source domain's characteristics.\" (Abstract)\n\"HDA aims to acquire an adapted image generator that preserves the integrated attributes of all target domains... For example, given the few-shot references from the individual sketch, baby and smile domains as shown in Fig. 1 (Right), HDA aims to adapt the generator to sketch-smile-baby domain.\" (Introduction)\n\"Hybrid domain adaptation (HDA) aims to acquire a generator G_T that models a hybrid domain T = ∪_{i=1}^N T_i and generate images ˜Y with integrated attributes.\" (Section 3.1 Problem Formulation)", "section": "Abstract; Introduction; Section 3.1 (Problem Formulation)"}
{"claim": "The paper lacks a study of the effect of using different pre-trained image encoders on the method's performance and robustness.", "claim_type": "experimental", "paper_id": "FE2e8664Sl", "paper_title": "Few-shot Hybrid Domain Adaptation of Image Generator", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "u2SOmGevjE", "reviewer": "Reviewer_fvXy", "review_text": "Summary: The paper introduces the few-shot generative Hybrid Domain Adaptation (HDA) task for image-based data. It is assumed that an image generator is accessed from a source domain and then from several target domains. Adaptation is performed on all target domains. To perform HDA, the paper presents a discriminator-free approach with a style GAN pre-trained on a source domain. It is adapted to individual target domains using a few shots from each domain and generates an image style showing composite attributes of each target domain separately.  The proposed approach relies on features extracted from standard transformer-based architectures. In addition, distance and direction losses are used to guide the generator to produce images with integrated features of the target domains while preserving the features of the source domain. Experiments are conducted using the FFHQ dataset and three evaluation metrics are used to measure performance and compare with different baselines.\n\nStrengths: - The paper is well written and easy to understand. The method is well described and the experiments are clearly presented. \n\n- The discriminator-free approach presented is interesting as it reduces the computational cost of fitting compared to existing approaches. \n\n- The approach shows promising results for all metrics compared to the baselines on the FFHQ dataset.\n\nWeaknesses: - (Major) The paper does not provide strong arguments as to why hybrid domain adaptation is a meaningful task. It is not clear why the current approaches and benchmarks, e.g. domain adaptation and domain generalisation, are not sufficient to address the problem presented. These points need further elaboration to motivate the paper. \n\n- (Major) results are reported using 10 shots, while it would be interesting to see results using different numbers of shots, e.g. 1-shot and 5-shot.\n\n- It is unclear whether the methodology is applicable to the generation of conditioned image styles. For example, generating an image that is conditioned on one or more styles of the target domain, rather than an image that spans all attributes of the target domains. It could discussed how the proposed methodology generalises to conditional generation. \n\n- A study of the effect of different pre-trained image encoders would add value to the paper.\n\nQuestions: - How would the proposed generator compare with well-known generator models such as DALLE, Imagen and etc.?", "labeling_timestamp": "2026-01-11T16:23:25.038965", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper states it uses pre-trained image encoders (e.g., Swin and DINOv2) as the embedding backbone but presents no experimental comparison or ablation studying the effect of different pre-trained encoders on performance or robustness in the provided content (no such study or results are described in the main text).", "evidence": "To address the issues, we propose a discriminator-free framework for HDA. Specifically, we utilize the pre-trained image encoder E , such as Swin (Liu et al., 2021) and Dinov2 (Oquab et al., 2023) in image classification task, to encode the few-shot reference images Y i from T i into features f i :", "section": "3.2 DISCRIMINATOR-FREE DOMAIN ADAPTATION"}
{"claim": "The paper does not compare the proposed generator with widely known generative models such as DALL·E and Imagen to assess relative quality and capabilities.", "claim_type": "baseline", "paper_id": "FE2e8664Sl", "paper_title": "Few-shot Hybrid Domain Adaptation of Image Generator", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "u2SOmGevjE", "reviewer": "Reviewer_fvXy", "review_text": "Summary: The paper introduces the few-shot generative Hybrid Domain Adaptation (HDA) task for image-based data. It is assumed that an image generator is accessed from a source domain and then from several target domains. Adaptation is performed on all target domains. To perform HDA, the paper presents a discriminator-free approach with a style GAN pre-trained on a source domain. It is adapted to individual target domains using a few shots from each domain and generates an image style showing composite attributes of each target domain separately.  The proposed approach relies on features extracted from standard transformer-based architectures. In addition, distance and direction losses are used to guide the generator to produce images with integrated features of the target domains while preserving the features of the source domain. Experiments are conducted using the FFHQ dataset and three evaluation metrics are used to measure performance and compare with different baselines.\n\nStrengths: - The paper is well written and easy to understand. The method is well described and the experiments are clearly presented. \n\n- The discriminator-free approach presented is interesting as it reduces the computational cost of fitting compared to existing approaches. \n\n- The approach shows promising results for all metrics compared to the baselines on the FFHQ dataset.\n\nWeaknesses: - (Major) The paper does not provide strong arguments as to why hybrid domain adaptation is a meaningful task. It is not clear why the current approaches and benchmarks, e.g. domain adaptation and domain generalisation, are not sufficient to address the problem presented. These points need further elaboration to motivate the paper. \n\n- (Major) results are reported using 10 shots, while it would be interesting to see results using different numbers of shots, e.g. 1-shot and 5-shot.\n\n- It is unclear whether the methodology is applicable to the generation of conditioned image styles. For example, generating an image that is conditioned on one or more styles of the target domain, rather than an image that spans all attributes of the target domains. It could discussed how the proposed methodology generalises to conditional generation. \n\n- A study of the effect of different pre-trained image encoders would add value to the paper.\n\nQuestions: - How would the proposed generator compare with well-known generator models such as DALLE, Imagen and etc.?", "labeling_timestamp": "2026-01-11T16:23:26.619265", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper's comparisons and related-work discussion reference few-shot DA methods, inference-time interpolation works (e.g., Wu et al., Gal et al.), and discriminator-free/text-driven DA (Style-NADA), but it does not mention or compare against large text-to-image generative models such as DALL·E or Imagen.", "evidence": "1) \"Compared with other potential approaches for achieving hybrid domain (Wu et al., 2023; Gal et al., 2021), our method accomplishes the fastest adaptation without training models on multiple individual domains.\" 2) \"Style-NADA (Gal et al., 2021) proposes text-driven DA and presents a discriminator-free method which utilizes CLIP (Radford et al., 2021) model to produce the CLIP-space direction.\" 3) \"we start with a pre-trained StyleGAN2 (Karras et al., 2019) generator G_S...\"", "section": "Related Works; Abstract/Introduction"}
{"claim": "The authors did not release the code for their method, preventing reproducibility and independent verification of KV-cache experiments.", "claim_type": "methodology", "paper_id": "pNnvzQsS4P", "paper_title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vadn2W9weu", "reviewer": "Reviewer_KwWk", "review_text": "Summary: The authors have addressed the KV-cache compression problem by providing a finer quantization level. The KV-cache can pose a significant barrier to the inference of most autoregressive language models, a challenge that has been well studied in recent publications at ICML and NeurIPS. This paper introduces a novel approach by coupling multiple key/value channels together for quantization, exploiting their interdependence to encode the activations in a more information-efficient manner.\n\nStrengths: - The method is novel and demonstrates comparable accuracy to KVQuant, one of the pivotal approaches in this field.\n- It includes a substantial number of experiments to validate accuracy.\n- The quantization method introduced here is novel compared to other approaches. Additionally, the implementation in PyTorch represents a significant contribution.\n\nWeaknesses: - The code is not available. For research on KV cache, it is important to have the code available.\n\n- The section describing the random variables and entropy, specifically line 120, does not explicitly describe the random variables in mathematical notation. This should be revised for clarity. I would like to see this section more polished.\n\n- I believe the LLAMA3 model was available before the NeurIPS submission. Since that time, the authors may have extended their findings to these models. I would like to see the performance of your model in that setting.\n\n- I want to see how the runtime of your method compares to other methods. Recent works, like **QJL**, include good plots for token-wise generation time or end-to-end timing. Since you compare with KVQuant, it is also good to compare with **KIVI**, as it is one of the best methods. I recommend comparing with QJL and **KIVI**, and plotting the runtime alongside these methods.\n\n- I highly recommend the authors run their code on longer context datasets. LongBench could be a great example to evaluate its performance compared to other methods. I would suggest that perplexity is not the best metric for comparison.\n\nRelevant papers: [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache], [QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead]\n\n**If you address the concerns regarding the experiments and provide a broader comparison to the other methods, I would increase my score.**\n\nQuestions: - There is additional overhead regarding storing the centroid for each coupled key/value pair, making it difficult to track. I would like you to mention this overhead and explain how you set those values.", "labeling_timestamp": "2026-01-11T16:23:33.760239", "model": "gpt-5-mini", "label": "Supported", "justification": "The provided paper content (Abstract, Introduction, and Methodology sections) contains no statement or link indicating that the authors released their code. Therefore the reviewer's claim that code was not released (hindering reproducibility of KV-cache experiments) is supported by the paper text supplied.", "evidence": "Abstract: \"Extensive experiments reveal that CQ compares favorably with existing baselines in preserving model quality, and improves inference throughput by 1.4-3.5 × relative to the uncompressed baseline. Furthermore, we demonstrate that CQ can preserve model quality reasonably with KV cache quantized down to 1 bit.\"; Contributions: \"1. We observe the phenomenon ... 2. We propose Coupled Quantization (CQ) ... 3. Through extensive experiments, we demonstrate the effectiveness of CQ ...\"", "section": "Abstract / Introduction (Contributions)"}
{"claim": "The section on random variables and entropy (line 120) does not explicitly define the random variables in mathematical notation, reducing clarity and rigor.", "claim_type": "methodology", "paper_id": "pNnvzQsS4P", "paper_title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vadn2W9weu", "reviewer": "Reviewer_KwWk", "review_text": "Summary: The authors have addressed the KV-cache compression problem by providing a finer quantization level. The KV-cache can pose a significant barrier to the inference of most autoregressive language models, a challenge that has been well studied in recent publications at ICML and NeurIPS. This paper introduces a novel approach by coupling multiple key/value channels together for quantization, exploiting their interdependence to encode the activations in a more information-efficient manner.\n\nStrengths: - The method is novel and demonstrates comparable accuracy to KVQuant, one of the pivotal approaches in this field.\n- It includes a substantial number of experiments to validate accuracy.\n- The quantization method introduced here is novel compared to other approaches. Additionally, the implementation in PyTorch represents a significant contribution.\n\nWeaknesses: - The code is not available. For research on KV cache, it is important to have the code available.\n\n- The section describing the random variables and entropy, specifically line 120, does not explicitly describe the random variables in mathematical notation. This should be revised for clarity. I would like to see this section more polished.\n\n- I believe the LLAMA3 model was available before the NeurIPS submission. Since that time, the authors may have extended their findings to these models. I would like to see the performance of your model in that setting.\n\n- I want to see how the runtime of your method compares to other methods. Recent works, like **QJL**, include good plots for token-wise generation time or end-to-end timing. Since you compare with KVQuant, it is also good to compare with **KIVI**, as it is one of the best methods. I recommend comparing with QJL and **KIVI**, and plotting the runtime alongside these methods.\n\n- I highly recommend the authors run their code on longer context datasets. LongBench could be a great example to evaluate its performance compared to other methods. I would suggest that perplexity is not the best metric for comparison.\n\nRelevant papers: [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache], [QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead]\n\n**If you address the concerns regarding the experiments and provide a broader comparison to the other methods, I would increase my score.**\n\nQuestions: - There is additional overhead regarding storing the centroid for each coupled key/value pair, making it difficult to track. I would like you to mention this overhead and explain how you set those values.", "labeling_timestamp": "2026-01-11T16:23:29.035219", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper does introduce channels as random variables using mathematical notation and defines entropy and joint entropy formulas in Section 3.1, so the reviewer's claim that the random variables are not explicitly defined is false.", "evidence": "We consider channels in a key/value activation embedding as random variables X 1 , X 2 , . . . . The amount of information (or uncertainty) in channel X can be measured by entropy , defined as H ( X ) = -∫ X p ( x ) log 2 p ( x ) dx , ... The total amount of information (or uncertainty) in two channels X 1 , X 2 is measured by joint entropy , defined as H ( X 1 , X 2 ) = -∫ X 1 ∫ X 2 p ( x 1 , x 2 ) log 2 p ( x 1 , x 2 ) dx 2 dx 1 , where p ( · , · ) is the joint probability density function.", "section": "3.1 Motivations"}
{"claim": "The paper does not report experiments evaluating the method on LLAMA3 models, despite LLAMA3 being available before submission.", "claim_type": "experimental", "paper_id": "pNnvzQsS4P", "paper_title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vadn2W9weu", "reviewer": "Reviewer_KwWk", "review_text": "Summary: The authors have addressed the KV-cache compression problem by providing a finer quantization level. The KV-cache can pose a significant barrier to the inference of most autoregressive language models, a challenge that has been well studied in recent publications at ICML and NeurIPS. This paper introduces a novel approach by coupling multiple key/value channels together for quantization, exploiting their interdependence to encode the activations in a more information-efficient manner.\n\nStrengths: - The method is novel and demonstrates comparable accuracy to KVQuant, one of the pivotal approaches in this field.\n- It includes a substantial number of experiments to validate accuracy.\n- The quantization method introduced here is novel compared to other approaches. Additionally, the implementation in PyTorch represents a significant contribution.\n\nWeaknesses: - The code is not available. For research on KV cache, it is important to have the code available.\n\n- The section describing the random variables and entropy, specifically line 120, does not explicitly describe the random variables in mathematical notation. This should be revised for clarity. I would like to see this section more polished.\n\n- I believe the LLAMA3 model was available before the NeurIPS submission. Since that time, the authors may have extended their findings to these models. I would like to see the performance of your model in that setting.\n\n- I want to see how the runtime of your method compares to other methods. Recent works, like **QJL**, include good plots for token-wise generation time or end-to-end timing. Since you compare with KVQuant, it is also good to compare with **KIVI**, as it is one of the best methods. I recommend comparing with QJL and **KIVI**, and plotting the runtime alongside these methods.\n\n- I highly recommend the authors run their code on longer context datasets. LongBench could be a great example to evaluate its performance compared to other methods. I would suggest that perplexity is not the best metric for comparison.\n\nRelevant papers: [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache], [QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead]\n\n**If you address the concerns regarding the experiments and provide a broader comparison to the other methods, I would increase my score.**\n\nQuestions: - There is additional overhead regarding storing the centroid for each coupled key/value pair, making it difficult to track. I would like you to mention this overhead and explain how you set those values.", "labeling_timestamp": "2026-01-11T16:23:37.866022", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper's experiments and analyses explicitly reference LLaMA models (e.g., LLaMA-7b and 'LLaMA family') but contain no mention of LLAMA3 or experiments on LLAMA3 models. Thus the claim that the paper does not report experiments on LLAMA3 is accurate according to the paper.", "evidence": "\"In Figure 1, we show the perplexity of two models from the LLaMA family [36, 37]...\"; \"Figure 2: (a) Growth rate of joint entropy versus sum of marginal entropies of the key/value activation embeddings of LLaMA-7b on 262k tokens of WikiText-2.\"; \"Specifically, we divide the channels of key and value embeddings of LLaMA-7b [36] into non-overlapping groups...\"", "section": "1 Introduction; 3.1 Motivations"}
{"claim": "The paper does not provide runtime comparisons against the recent QJL method, such as token-wise generation time or end-to-end timing plots.", "claim_type": "baseline", "paper_id": "pNnvzQsS4P", "paper_title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vadn2W9weu", "reviewer": "Reviewer_KwWk", "review_text": "Summary: The authors have addressed the KV-cache compression problem by providing a finer quantization level. The KV-cache can pose a significant barrier to the inference of most autoregressive language models, a challenge that has been well studied in recent publications at ICML and NeurIPS. This paper introduces a novel approach by coupling multiple key/value channels together for quantization, exploiting their interdependence to encode the activations in a more information-efficient manner.\n\nStrengths: - The method is novel and demonstrates comparable accuracy to KVQuant, one of the pivotal approaches in this field.\n- It includes a substantial number of experiments to validate accuracy.\n- The quantization method introduced here is novel compared to other approaches. Additionally, the implementation in PyTorch represents a significant contribution.\n\nWeaknesses: - The code is not available. For research on KV cache, it is important to have the code available.\n\n- The section describing the random variables and entropy, specifically line 120, does not explicitly describe the random variables in mathematical notation. This should be revised for clarity. I would like to see this section more polished.\n\n- I believe the LLAMA3 model was available before the NeurIPS submission. Since that time, the authors may have extended their findings to these models. I would like to see the performance of your model in that setting.\n\n- I want to see how the runtime of your method compares to other methods. Recent works, like **QJL**, include good plots for token-wise generation time or end-to-end timing. Since you compare with KVQuant, it is also good to compare with **KIVI**, as it is one of the best methods. I recommend comparing with QJL and **KIVI**, and plotting the runtime alongside these methods.\n\n- I highly recommend the authors run their code on longer context datasets. LongBench could be a great example to evaluate its performance compared to other methods. I would suggest that perplexity is not the best metric for comparison.\n\nRelevant papers: [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache], [QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead]\n\n**If you address the concerns regarding the experiments and provide a broader comparison to the other methods, I would increase my score.**\n\nQuestions: - There is additional overhead regarding storing the centroid for each coupled key/value pair, making it difficult to track. I would like you to mention this overhead and explain how you set those values.", "labeling_timestamp": "2026-01-11T16:23:50.630905", "model": "gpt-5-mini", "label": "Supported", "justification": "In the provided paper content the authors report throughput improvements versus the uncompressed baseline and state they compare to existing baselines, but there is no mention of the recent QJL method nor any token-wise generation time or end-to-end timing plots. Thus the claim that the paper does not provide runtime comparisons against QJL (including token-wise or end-to-end timing plots) is accurate according to the given content.", "evidence": "1) \"Extensive experiments reveal that CQ compares favorably with existing baselines in preserving model quality, and improves inference throughput by 1.4-3.5 × relative to the uncompressed baseline.\" (Abstract)\n\n2) \"Through extensive experiments, we demonstrate the effectiveness of CQ at preserving model quality and speeding up LLM inference against competitive baselines.\" (Contributions / end of Introduction)\n\nNo mention of the QJL method or any token-wise generation time / end-to-end timing plots appears in the provided sections.", "section": "Abstract / Introduction"}
{"claim": "The paper omits runtime and performance comparisons to KIVI, a leading KV-cache quantization method, despite comparing only with KVQuant.", "claim_type": "baseline", "paper_id": "pNnvzQsS4P", "paper_title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vadn2W9weu", "reviewer": "Reviewer_KwWk", "review_text": "Summary: The authors have addressed the KV-cache compression problem by providing a finer quantization level. The KV-cache can pose a significant barrier to the inference of most autoregressive language models, a challenge that has been well studied in recent publications at ICML and NeurIPS. This paper introduces a novel approach by coupling multiple key/value channels together for quantization, exploiting their interdependence to encode the activations in a more information-efficient manner.\n\nStrengths: - The method is novel and demonstrates comparable accuracy to KVQuant, one of the pivotal approaches in this field.\n- It includes a substantial number of experiments to validate accuracy.\n- The quantization method introduced here is novel compared to other approaches. Additionally, the implementation in PyTorch represents a significant contribution.\n\nWeaknesses: - The code is not available. For research on KV cache, it is important to have the code available.\n\n- The section describing the random variables and entropy, specifically line 120, does not explicitly describe the random variables in mathematical notation. This should be revised for clarity. I would like to see this section more polished.\n\n- I believe the LLAMA3 model was available before the NeurIPS submission. Since that time, the authors may have extended their findings to these models. I would like to see the performance of your model in that setting.\n\n- I want to see how the runtime of your method compares to other methods. Recent works, like **QJL**, include good plots for token-wise generation time or end-to-end timing. Since you compare with KVQuant, it is also good to compare with **KIVI**, as it is one of the best methods. I recommend comparing with QJL and **KIVI**, and plotting the runtime alongside these methods.\n\n- I highly recommend the authors run their code on longer context datasets. LongBench could be a great example to evaluate its performance compared to other methods. I would suggest that perplexity is not the best metric for comparison.\n\nRelevant papers: [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache], [QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead]\n\n**If you address the concerns regarding the experiments and provide a broader comparison to the other methods, I would increase my score.**\n\nQuestions: - There is additional overhead regarding storing the centroid for each coupled key/value pair, making it difficult to track. I would like you to mention this overhead and explain how you set those values.", "labeling_timestamp": "2026-01-11T16:24:20.549808", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not include the experimental results or the full related-work/experiments sections where explicit comparisons to specific baselines (such as KIVI) would appear. While the abstract and introduction state that CQ \"compares favorably with existing baselines,\" KIVI is not mentioned in the supplied text, so there is insufficient information to verify the reviewer's claim that runtime and performance comparisons to KIVI were omitted.", "evidence": "1) \"Extensive experiments reveal that CQ compares favorably with existing baselines in preserving model quality, and improves inference throughput by 1.4-3.5 × relative to the uncompressed baseline.\" (Abstract)\n2) \"Existing approaches typically achieve compression of KV cache through token eviction [47, 24] or activation quantization [25, 15].\" (Introduction)", "section": "Abstract; 1 Introduction"}
{"claim": "The authors do not include token-wise generation time or end-to-end timing plots to characterize inference latency across methods.", "claim_type": "experimental", "paper_id": "pNnvzQsS4P", "paper_title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vadn2W9weu", "reviewer": "Reviewer_KwWk", "review_text": "Summary: The authors have addressed the KV-cache compression problem by providing a finer quantization level. The KV-cache can pose a significant barrier to the inference of most autoregressive language models, a challenge that has been well studied in recent publications at ICML and NeurIPS. This paper introduces a novel approach by coupling multiple key/value channels together for quantization, exploiting their interdependence to encode the activations in a more information-efficient manner.\n\nStrengths: - The method is novel and demonstrates comparable accuracy to KVQuant, one of the pivotal approaches in this field.\n- It includes a substantial number of experiments to validate accuracy.\n- The quantization method introduced here is novel compared to other approaches. Additionally, the implementation in PyTorch represents a significant contribution.\n\nWeaknesses: - The code is not available. For research on KV cache, it is important to have the code available.\n\n- The section describing the random variables and entropy, specifically line 120, does not explicitly describe the random variables in mathematical notation. This should be revised for clarity. I would like to see this section more polished.\n\n- I believe the LLAMA3 model was available before the NeurIPS submission. Since that time, the authors may have extended their findings to these models. I would like to see the performance of your model in that setting.\n\n- I want to see how the runtime of your method compares to other methods. Recent works, like **QJL**, include good plots for token-wise generation time or end-to-end timing. Since you compare with KVQuant, it is also good to compare with **KIVI**, as it is one of the best methods. I recommend comparing with QJL and **KIVI**, and plotting the runtime alongside these methods.\n\n- I highly recommend the authors run their code on longer context datasets. LongBench could be a great example to evaluate its performance compared to other methods. I would suggest that perplexity is not the best metric for comparison.\n\nRelevant papers: [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache], [QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead]\n\n**If you address the concerns regarding the experiments and provide a broader comparison to the other methods, I would increase my score.**\n\nQuestions: - There is additional overhead regarding storing the centroid for each coupled key/value pair, making it difficult to track. I would like you to mention this overhead and explain how you set those values.", "labeling_timestamp": "2026-01-11T16:23:47.977793", "model": "gpt-5-mini", "label": "Supported", "justification": "The provided paper content reports numeric throughput improvements but contains no token-wise generation time or end-to-end timing plots or figures characterizing inference latency across methods in the Abstract, Introduction, or Method sections shown.", "evidence": "\"Extensive experiments reveal that CQ compares favorably with existing baselines in preserving model quality, and improves inference throughput by 1.4-3.5 × relative to the uncompressed baseline.\"; \"KV cache bottlenecks inference throughput since it limits the maximum batch size, and it is a major contributor to latency due to the low compute-to-memory ratio [15].\"", "section": "Abstract; 1 Introduction"}
{"claim": "The evaluation lacks long-context benchmarks such as LongBench to assess performance on extended context lengths.", "claim_type": "experimental", "paper_id": "pNnvzQsS4P", "paper_title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vadn2W9weu", "reviewer": "Reviewer_KwWk", "review_text": "Summary: The authors have addressed the KV-cache compression problem by providing a finer quantization level. The KV-cache can pose a significant barrier to the inference of most autoregressive language models, a challenge that has been well studied in recent publications at ICML and NeurIPS. This paper introduces a novel approach by coupling multiple key/value channels together for quantization, exploiting their interdependence to encode the activations in a more information-efficient manner.\n\nStrengths: - The method is novel and demonstrates comparable accuracy to KVQuant, one of the pivotal approaches in this field.\n- It includes a substantial number of experiments to validate accuracy.\n- The quantization method introduced here is novel compared to other approaches. Additionally, the implementation in PyTorch represents a significant contribution.\n\nWeaknesses: - The code is not available. For research on KV cache, it is important to have the code available.\n\n- The section describing the random variables and entropy, specifically line 120, does not explicitly describe the random variables in mathematical notation. This should be revised for clarity. I would like to see this section more polished.\n\n- I believe the LLAMA3 model was available before the NeurIPS submission. Since that time, the authors may have extended their findings to these models. I would like to see the performance of your model in that setting.\n\n- I want to see how the runtime of your method compares to other methods. Recent works, like **QJL**, include good plots for token-wise generation time or end-to-end timing. Since you compare with KVQuant, it is also good to compare with **KIVI**, as it is one of the best methods. I recommend comparing with QJL and **KIVI**, and plotting the runtime alongside these methods.\n\n- I highly recommend the authors run their code on longer context datasets. LongBench could be a great example to evaluate its performance compared to other methods. I would suggest that perplexity is not the best metric for comparison.\n\nRelevant papers: [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache], [QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead]\n\n**If you address the concerns regarding the experiments and provide a broader comparison to the other methods, I would increase my score.**\n\nQuestions: - There is additional overhead regarding storing the centroid for each coupled key/value pair, making it difficult to track. I would like you to mention this overhead and explain how you set those values.", "labeling_timestamp": "2026-01-11T16:24:42.749072", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper content reports evaluations on WikiText-2 (and mentions experimental details in Section 4) but does not include the full Experiments section. There is no explicit mention of LongBench or other long-context benchmarks in the excerpt, yet the complete evaluation (Section 4) is not present here, so it is impossible to determine from the given content whether LongBench was used.", "evidence": ["\"We show the perplexity of two models from the LLaMA family [36, 37] on WikiText-2 [27] under 1-bit quantization... The full experimental setup is presented in Section 4.\"", "\"Figure 2: (a) Growth rate of joint entropy versus sum of marginal entropies of the key/value activation embeddings of LLaMA-7b on 262k tokens of WikiText-2.\"", "\"By further combining KV cache quantization with a sliding window of 128 recent tokens cached in full precision, we achieve a negligible 0.3-0.33 increase in perplexity with 1-bit KV cache.\""], "section": "Introduction; 3.1 Motivations"}
{"claim": "The paper relies primarily on perplexity as the evaluation metric, which the reviewer argues is not the most appropriate metric for KV-cache comparisons.", "claim_type": "experimental", "paper_id": "pNnvzQsS4P", "paper_title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vadn2W9weu", "reviewer": "Reviewer_KwWk", "review_text": "Summary: The authors have addressed the KV-cache compression problem by providing a finer quantization level. The KV-cache can pose a significant barrier to the inference of most autoregressive language models, a challenge that has been well studied in recent publications at ICML and NeurIPS. This paper introduces a novel approach by coupling multiple key/value channels together for quantization, exploiting their interdependence to encode the activations in a more information-efficient manner.\n\nStrengths: - The method is novel and demonstrates comparable accuracy to KVQuant, one of the pivotal approaches in this field.\n- It includes a substantial number of experiments to validate accuracy.\n- The quantization method introduced here is novel compared to other approaches. Additionally, the implementation in PyTorch represents a significant contribution.\n\nWeaknesses: - The code is not available. For research on KV cache, it is important to have the code available.\n\n- The section describing the random variables and entropy, specifically line 120, does not explicitly describe the random variables in mathematical notation. This should be revised for clarity. I would like to see this section more polished.\n\n- I believe the LLAMA3 model was available before the NeurIPS submission. Since that time, the authors may have extended their findings to these models. I would like to see the performance of your model in that setting.\n\n- I want to see how the runtime of your method compares to other methods. Recent works, like **QJL**, include good plots for token-wise generation time or end-to-end timing. Since you compare with KVQuant, it is also good to compare with **KIVI**, as it is one of the best methods. I recommend comparing with QJL and **KIVI**, and plotting the runtime alongside these methods.\n\n- I highly recommend the authors run their code on longer context datasets. LongBench could be a great example to evaluate its performance compared to other methods. I would suggest that perplexity is not the best metric for comparison.\n\nRelevant papers: [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache], [QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead]\n\n**If you address the concerns regarding the experiments and provide a broader comparison to the other methods, I would increase my score.**\n\nQuestions: - There is additional overhead regarding storing the centroid for each coupled key/value pair, making it difficult to track. I would like you to mention this overhead and explain how you set those values.", "labeling_timestamp": "2026-01-11T16:24:06.179376", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper uses perplexity repeatedly as the primary metric for measuring model quality under KV-cache quantization (e.g., figures and reported perplexity deltas). While it also reports throughput improvements, the main evaluation of quantization impact on model quality is presented via perplexity on WikiText-2.", "evidence": "“In Figure 1, we show the perplexity of two models from the LLaMA family [36, 37] on WikiText-2 [27] under 1-bit quantization with varying numbers of” AND “Figure 1: Perplexity of LLMs with 1-bit quantized KV cache approaches the uncompressed FP16 performance as the number of coupled K/V channels increases.” AND “By further combining KV cache quantization with a sliding window of 128 recent tokens cached in full precision, we achieve a negligible 0.3-0.33 increase in perplexity with 1-bit KV cache.”", "section": "1 Introduction (and Abstract reference to evaluation results)"}
{"claim": "The authors fail to quantify or discuss the additional storage overhead incurred by storing a centroid for each coupled key/value pair.", "claim_type": "quantitative", "paper_id": "pNnvzQsS4P", "paper_title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vadn2W9weu", "reviewer": "Reviewer_KwWk", "review_text": "Summary: The authors have addressed the KV-cache compression problem by providing a finer quantization level. The KV-cache can pose a significant barrier to the inference of most autoregressive language models, a challenge that has been well studied in recent publications at ICML and NeurIPS. This paper introduces a novel approach by coupling multiple key/value channels together for quantization, exploiting their interdependence to encode the activations in a more information-efficient manner.\n\nStrengths: - The method is novel and demonstrates comparable accuracy to KVQuant, one of the pivotal approaches in this field.\n- It includes a substantial number of experiments to validate accuracy.\n- The quantization method introduced here is novel compared to other approaches. Additionally, the implementation in PyTorch represents a significant contribution.\n\nWeaknesses: - The code is not available. For research on KV cache, it is important to have the code available.\n\n- The section describing the random variables and entropy, specifically line 120, does not explicitly describe the random variables in mathematical notation. This should be revised for clarity. I would like to see this section more polished.\n\n- I believe the LLAMA3 model was available before the NeurIPS submission. Since that time, the authors may have extended their findings to these models. I would like to see the performance of your model in that setting.\n\n- I want to see how the runtime of your method compares to other methods. Recent works, like **QJL**, include good plots for token-wise generation time or end-to-end timing. Since you compare with KVQuant, it is also good to compare with **KIVI**, as it is one of the best methods. I recommend comparing with QJL and **KIVI**, and plotting the runtime alongside these methods.\n\n- I highly recommend the authors run their code on longer context datasets. LongBench could be a great example to evaluate its performance compared to other methods. I would suggest that perplexity is not the best metric for comparison.\n\nRelevant papers: [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache], [QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead]\n\n**If you address the concerns regarding the experiments and provide a broader comparison to the other methods, I would increase my score.**\n\nQuestions: - There is additional overhead regarding storing the centroid for each coupled key/value pair, making it difficult to track. I would like you to mention this overhead and explain how you set those values.", "labeling_timestamp": "2026-01-11T16:24:16.486606", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes learning centroids for quantization (Section 2.2) and proposes coupling channels (Introduction, Section 3), but nowhere in the provided text do the authors quantify or discuss the additional storage overhead of storing centroid/codebook entries for coupled key/value groups.", "evidence": "1) \"In non-uniform per-channel quantization, a set of centroids is learned for each channel.\" 2) \"We propose Coupled Quantization (CQ), a novel KV cache quantization method that jointly encodes multiple key/value channels to exploit the dependency across channels.\"", "section": "1 Introduction; 2.2 Per-Channel Quantization"}
{"claim": "The paper does not explain how centroid values for each coupled key/value pair are chosen, configured, or fitted.", "claim_type": "methodology", "paper_id": "pNnvzQsS4P", "paper_title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vadn2W9weu", "reviewer": "Reviewer_KwWk", "review_text": "Summary: The authors have addressed the KV-cache compression problem by providing a finer quantization level. The KV-cache can pose a significant barrier to the inference of most autoregressive language models, a challenge that has been well studied in recent publications at ICML and NeurIPS. This paper introduces a novel approach by coupling multiple key/value channels together for quantization, exploiting their interdependence to encode the activations in a more information-efficient manner.\n\nStrengths: - The method is novel and demonstrates comparable accuracy to KVQuant, one of the pivotal approaches in this field.\n- It includes a substantial number of experiments to validate accuracy.\n- The quantization method introduced here is novel compared to other approaches. Additionally, the implementation in PyTorch represents a significant contribution.\n\nWeaknesses: - The code is not available. For research on KV cache, it is important to have the code available.\n\n- The section describing the random variables and entropy, specifically line 120, does not explicitly describe the random variables in mathematical notation. This should be revised for clarity. I would like to see this section more polished.\n\n- I believe the LLAMA3 model was available before the NeurIPS submission. Since that time, the authors may have extended their findings to these models. I would like to see the performance of your model in that setting.\n\n- I want to see how the runtime of your method compares to other methods. Recent works, like **QJL**, include good plots for token-wise generation time or end-to-end timing. Since you compare with KVQuant, it is also good to compare with **KIVI**, as it is one of the best methods. I recommend comparing with QJL and **KIVI**, and plotting the runtime alongside these methods.\n\n- I highly recommend the authors run their code on longer context datasets. LongBench could be a great example to evaluate its performance compared to other methods. I would suggest that perplexity is not the best metric for comparison.\n\nRelevant papers: [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache], [QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead]\n\n**If you address the concerns regarding the experiments and provide a broader comparison to the other methods, I would increase my score.**\n\nQuestions: - There is additional overhead regarding storing the centroid for each coupled key/value pair, making it difficult to track. I would like you to mention this overhead and explain how you set those values.", "labeling_timestamp": "2026-01-11T16:24:17.708067", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes how centroids are learned for non-uniform per-channel quantization (Section 2.2) but, in the provided content, it introduces Coupled Quantization (CQ) without specifying how centroid values for coupled channel groups are chosen, configured, or fitted. There is discussion of motivation and empirical entropy estimates (Section 3.1) and figures illustrating CQ, but no procedural detail or objective for learning centroids for coupled key/value pairs is given in the included text.", "evidence": "1) \"In non-uniform per-channel quantization, a set of centroids is learned for each channel. Suppose A is a key or value activation matrix ... non-uniform b-bit per-channel quantization aims to learn a set of centroids C⋆_i ⊂ R for each channel i of A through the objective ... where q quantizes each value in A_{i,:} to the nearest centroid in C.\" 2) \"We propose Coupled Quantization (CQ), a novel KV cache quantization method that jointly encodes multiple key/value channels to exploit the dependency across channels.\"", "section": "Sections 2.2 (Per-Channel Quantization) and 3 (Methodology / 3.1 Motivations)"}
{"claim": "The paper's pocket encoder architecture is directly borrowed from the Uni-Mol model, reducing the paper's claimed methodological originality.", "claim_type": "methodology", "paper_id": "uMAujpVi9m", "paper_title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "9ap4s1SZlZ", "reviewer": "Reviewer_yfM3", "review_text": "Summary: This paper primarily aims to enhance the pocket pretraining method, as existing approaches only consider pockets during pretraining. There are two main contributions in this paper: (1) The authors introduce a novel method, ProFSA, for pocket pretraining, which extracts additional information from corresponding ligands. However, the number of pocket-ligand complex structures is quite limited in existing datasets. (2) To address this issue, the authors generate over 5 million complexes by segmenting fragments and their corresponding pockets in protein structures. By aligning features of fragments and pockets, the pocket encoder learns the interaction between fragments and pockets. The authors design downstream tasks such as pocket druggability prediction, pocket matching, and ligand binding affinity prediction to demonstrate the effectiveness of ProFSA.\n\nStrengths: The authors propose a new perspective of pretraining pockets and construct a large-scale dataset, which data distribution is also considered, to make the efficient pre-training possible.\n\nThe results are competitive, especially for zero-shot settings.\n\nAbundant experiments and ablation study support the argument and result of the authors.\n\nWeaknesses: 1. The technical novelty is limited.\n  - The pocket encoder is borrowed from Uni-Mol.\n  - The contrastive loss is the vanilla form of classical contrastive learning.\n\n2. The bound of Theorem 3.1 is trivial. The authors claim that the bound naturally exists for these representations extracted by pretrained molecule models. However, it's a bit counterintuitive, because many models not pretrained on molecule datasets also fulfill this prior. So, can these models be used for this task? **I strongly suggest removing this part from the paper**.\n\n3. Some issues about dataset creation:\n - 3.1. The authors consider the distribution of ligand size and pocket size when designing the dataset. However, molecules possess more properties that can also lead to imbalance. It would be better to, at least, add some discussion about this issue.\n - 3.2. In the second stage of the data construction process, the approach to defining pockets needs further explanation or an ablation study.\n\n4. Experiments: It would be better to add some biological justification or visualization of the results.\n\nFor this paper, one fact is that the technical novelty is below the bar of ICLR. However, I admire the simple but effective model for the right question. It's a struggle for me to make a decision. I will maintain a neutral attitude and make my final decision after the discussion.\n\nQuestions: See weakness.", "labeling_timestamp": "2026-01-11T16:24:29.667836", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper references Uni-Mol and follows some of its setup (e.g., cutoff choice) but does not state that the pocket encoder architecture is directly borrowed from Uni-Mol or provide architecture details to confirm this. Therefore there is insufficient evidence in the provided text to conclude the encoder was directly copied, so the claim cannot be determined from the paper.", "evidence": "1) \"One such method is Uni-Mol (Zhou et al., 2023), which offers a universal 3D molecular pretraining framework and involves a pocket model that has been pretrained on 3 million pocket data.\" 2) \"...designate the surrounding residues containing at least one heavy atom within a 6 ˚ A proximity (following the setup of Uni-Mol, other cutoff values are presented in Table 10)...\"", "section": "2.2 POCKET PRETRAINING METHODS; 3.1 CONSTRUCTING PSEUDO-LIGAND-POCKET COMPLEXES FROM PROTEIN DATA"}
{"claim": "The contrastive loss used by the authors is the vanilla form of classical contrastive learning without apparent innovation.", "claim_type": "methodology", "paper_id": "uMAujpVi9m", "paper_title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "9ap4s1SZlZ", "reviewer": "Reviewer_yfM3", "review_text": "Summary: This paper primarily aims to enhance the pocket pretraining method, as existing approaches only consider pockets during pretraining. There are two main contributions in this paper: (1) The authors introduce a novel method, ProFSA, for pocket pretraining, which extracts additional information from corresponding ligands. However, the number of pocket-ligand complex structures is quite limited in existing datasets. (2) To address this issue, the authors generate over 5 million complexes by segmenting fragments and their corresponding pockets in protein structures. By aligning features of fragments and pockets, the pocket encoder learns the interaction between fragments and pockets. The authors design downstream tasks such as pocket druggability prediction, pocket matching, and ligand binding affinity prediction to demonstrate the effectiveness of ProFSA.\n\nStrengths: The authors propose a new perspective of pretraining pockets and construct a large-scale dataset, which data distribution is also considered, to make the efficient pre-training possible.\n\nThe results are competitive, especially for zero-shot settings.\n\nAbundant experiments and ablation study support the argument and result of the authors.\n\nWeaknesses: 1. The technical novelty is limited.\n  - The pocket encoder is borrowed from Uni-Mol.\n  - The contrastive loss is the vanilla form of classical contrastive learning.\n\n2. The bound of Theorem 3.1 is trivial. The authors claim that the bound naturally exists for these representations extracted by pretrained molecule models. However, it's a bit counterintuitive, because many models not pretrained on molecule datasets also fulfill this prior. So, can these models be used for this task? **I strongly suggest removing this part from the paper**.\n\n3. Some issues about dataset creation:\n - 3.1. The authors consider the distribution of ligand size and pocket size when designing the dataset. However, molecules possess more properties that can also lead to imbalance. It would be better to, at least, add some discussion about this issue.\n - 3.2. In the second stage of the data construction process, the approach to defining pockets needs further explanation or an ablation study.\n\n4. Experiments: It would be better to add some biological justification or visualization of the results.\n\nFor this paper, one fact is that the technical novelty is below the bar of ICLR. However, I admire the simple but effective model for the right question. It's a struggle for me to make a decision. I will maintain a neutral attitude and make my final decision after the discussion.\n\nQuestions: See weakness.", "labeling_timestamp": "2026-01-11T16:24:46.863949", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper does not use a plain vanilla contrastive loss; it proposes a ‘‘molecular-guided’’ contrastive scheme that (1) aligns the pocket encoder to representations from a fixed, pretrained small-molecule encoder and (2) uses two distinct contrastive losses (pocket→ligand and ligand→pocket). These design choices are described explicitly in the paper, so the reviewer claim that the loss is the vanilla form without apparent innovation is contradicted.", "evidence": "\"Subsequently, the pocket encoder is trained in a contrastive manner to align with the representation of pseudo-ligand furnished by some pretrained small molecule encoders.\" (Abstract)\n\n\"To counteract biases from peptide fragments used as ligands, we align the pocket encoder with a pretrained small molecule encoder, whose weights are fixed. This pretrained encoder serves as a guide, transferring binding-specific and biologically relevant information from extensive molecular datasets to our pocket encoder.\" (Introduction)\n\n\"Specifically, for a protein pocket p and its corresponding pseudo-ligand l , we denote their normalized encoding vectors as s and t . Let g_T and g_S denote the ligand and protein prediction networks, respectively. Then two distinct contrastive losses are used to facilitate the training process: ... the primary purpose of the first loss is to identify the true protein pocket from a batch of samples when given a pseudo-ligand. Similarly, the second loss seeks to identify the corresponding ligand fragment for a given pocket.\" (Section 3.2)", "section": "Abstract; 1 Introduction; 3.2 Contrastive learning in pocket-fragment space"}
{"claim": "The paper's technical novelty is judged by the reviewer to be below the acceptance standard expected at ICLR.", "claim_type": "novelty", "paper_id": "uMAujpVi9m", "paper_title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "9ap4s1SZlZ", "reviewer": "Reviewer_yfM3", "review_text": "Summary: This paper primarily aims to enhance the pocket pretraining method, as existing approaches only consider pockets during pretraining. There are two main contributions in this paper: (1) The authors introduce a novel method, ProFSA, for pocket pretraining, which extracts additional information from corresponding ligands. However, the number of pocket-ligand complex structures is quite limited in existing datasets. (2) To address this issue, the authors generate over 5 million complexes by segmenting fragments and their corresponding pockets in protein structures. By aligning features of fragments and pockets, the pocket encoder learns the interaction between fragments and pockets. The authors design downstream tasks such as pocket druggability prediction, pocket matching, and ligand binding affinity prediction to demonstrate the effectiveness of ProFSA.\n\nStrengths: The authors propose a new perspective of pretraining pockets and construct a large-scale dataset, which data distribution is also considered, to make the efficient pre-training possible.\n\nThe results are competitive, especially for zero-shot settings.\n\nAbundant experiments and ablation study support the argument and result of the authors.\n\nWeaknesses: 1. The technical novelty is limited.\n  - The pocket encoder is borrowed from Uni-Mol.\n  - The contrastive loss is the vanilla form of classical contrastive learning.\n\n2. The bound of Theorem 3.1 is trivial. The authors claim that the bound naturally exists for these representations extracted by pretrained molecule models. However, it's a bit counterintuitive, because many models not pretrained on molecule datasets also fulfill this prior. So, can these models be used for this task? **I strongly suggest removing this part from the paper**.\n\n3. Some issues about dataset creation:\n - 3.1. The authors consider the distribution of ligand size and pocket size when designing the dataset. However, molecules possess more properties that can also lead to imbalance. It would be better to, at least, add some discussion about this issue.\n - 3.2. In the second stage of the data construction process, the approach to defining pockets needs further explanation or an ablation study.\n\n4. Experiments: It would be better to add some biological justification or visualization of the results.\n\nFor this paper, one fact is that the technical novelty is below the bar of ICLR. However, I admire the simple but effective model for the right question. It's a struggle for me to make a decision. I will maintain a neutral attitude and make my final decision after the discussion.\n\nQuestions: See weakness.", "labeling_timestamp": "2026-01-11T16:24:43.005263", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper describes its method, claims novelty, and lists contributions, but contains no information about reviewer judgments or whether its technical novelty meets ICLR acceptance standards. The reviewer’s claim is an external opinion not addressed in the paper.", "evidence": "Our contributions are articulated as follows:\n\n1. The proposal of a novel scalable pairwise data synthesis pipeline by extracting pseudo-ligand-pocket pairs from protein-only data, which has the potential to be applied to much larger predicted structure data generated by AlphaFold (Jumper et al., 2021) or ESMFold (Lin et al., 2023).\n\n2. The introduction of a new molecular guided fragment-surroundings contrastive learning method for pocket representations, which naturally distillates comprehensive structural and chemical knowledge from pretrained small molecule encoders.\n\n3. The achievement of significant performance boosts in various downstream applications, including tasks that solely require pocket data, as well as those involving pocket-ligand pair data, underscoring the potential of ProFSA as a powerful tool in the drug discovery field.", "section": "1 INTRODUCTION"}
{"claim": "The theoretical bound presented in Theorem 3.1 is trivial and does not provide meaningful theoretical insight for this work.", "claim_type": "novelty", "paper_id": "uMAujpVi9m", "paper_title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "9ap4s1SZlZ", "reviewer": "Reviewer_yfM3", "review_text": "Summary: This paper primarily aims to enhance the pocket pretraining method, as existing approaches only consider pockets during pretraining. There are two main contributions in this paper: (1) The authors introduce a novel method, ProFSA, for pocket pretraining, which extracts additional information from corresponding ligands. However, the number of pocket-ligand complex structures is quite limited in existing datasets. (2) To address this issue, the authors generate over 5 million complexes by segmenting fragments and their corresponding pockets in protein structures. By aligning features of fragments and pockets, the pocket encoder learns the interaction between fragments and pockets. The authors design downstream tasks such as pocket druggability prediction, pocket matching, and ligand binding affinity prediction to demonstrate the effectiveness of ProFSA.\n\nStrengths: The authors propose a new perspective of pretraining pockets and construct a large-scale dataset, which data distribution is also considered, to make the efficient pre-training possible.\n\nThe results are competitive, especially for zero-shot settings.\n\nAbundant experiments and ablation study support the argument and result of the authors.\n\nWeaknesses: 1. The technical novelty is limited.\n  - The pocket encoder is borrowed from Uni-Mol.\n  - The contrastive loss is the vanilla form of classical contrastive learning.\n\n2. The bound of Theorem 3.1 is trivial. The authors claim that the bound naturally exists for these representations extracted by pretrained molecule models. However, it's a bit counterintuitive, because many models not pretrained on molecule datasets also fulfill this prior. So, can these models be used for this task? **I strongly suggest removing this part from the paper**.\n\n3. Some issues about dataset creation:\n - 3.1. The authors consider the distribution of ligand size and pocket size when designing the dataset. However, molecules possess more properties that can also lead to imbalance. It would be better to, at least, add some discussion about this issue.\n - 3.2. In the second stage of the data construction process, the approach to defining pockets needs further explanation or an ablation study.\n\n4. Experiments: It would be better to add some biological justification or visualization of the results.\n\nFor this paper, one fact is that the technical novelty is below the bar of ICLR. However, I admire the simple but effective model for the right question. It's a struggle for me to make a decision. I will maintain a neutral attitude and make my final decision after the discussion.\n\nQuestions: See weakness.", "labeling_timestamp": "2026-01-11T16:24:49.727202", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt (Sections 1–3.2) does not contain any Theorem 3.1 or a stated theoretical bound, so there is insufficient information in the given content to judge whether such a theorem is trivial or meaningful.", "evidence": "\"Specifically, for a protein pocket p and its corresponding pseudo-ligand l , we denote their normalized encoding vectors as s and t . Let g T and g S denote the ligand and protein prediction networks, respectively. Then two distinct contrastive losses are used to facilitate the training process: ... Hence, the ultimate loss utilized in our training process is:\"", "section": "3.2 CONTRASTIVE LEARNING IN POCKET-FRAGMENT SPACE"}
{"claim": "The authors claim the bound naturally exists for representations from pretrained molecule models, yet many non-molecule-pretrained models also satisfy this prior.", "claim_type": "experimental", "paper_id": "uMAujpVi9m", "paper_title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "9ap4s1SZlZ", "reviewer": "Reviewer_yfM3", "review_text": "Summary: This paper primarily aims to enhance the pocket pretraining method, as existing approaches only consider pockets during pretraining. There are two main contributions in this paper: (1) The authors introduce a novel method, ProFSA, for pocket pretraining, which extracts additional information from corresponding ligands. However, the number of pocket-ligand complex structures is quite limited in existing datasets. (2) To address this issue, the authors generate over 5 million complexes by segmenting fragments and their corresponding pockets in protein structures. By aligning features of fragments and pockets, the pocket encoder learns the interaction between fragments and pockets. The authors design downstream tasks such as pocket druggability prediction, pocket matching, and ligand binding affinity prediction to demonstrate the effectiveness of ProFSA.\n\nStrengths: The authors propose a new perspective of pretraining pockets and construct a large-scale dataset, which data distribution is also considered, to make the efficient pre-training possible.\n\nThe results are competitive, especially for zero-shot settings.\n\nAbundant experiments and ablation study support the argument and result of the authors.\n\nWeaknesses: 1. The technical novelty is limited.\n  - The pocket encoder is borrowed from Uni-Mol.\n  - The contrastive loss is the vanilla form of classical contrastive learning.\n\n2. The bound of Theorem 3.1 is trivial. The authors claim that the bound naturally exists for these representations extracted by pretrained molecule models. However, it's a bit counterintuitive, because many models not pretrained on molecule datasets also fulfill this prior. So, can these models be used for this task? **I strongly suggest removing this part from the paper**.\n\n3. Some issues about dataset creation:\n - 3.1. The authors consider the distribution of ligand size and pocket size when designing the dataset. However, molecules possess more properties that can also lead to imbalance. It would be better to, at least, add some discussion about this issue.\n - 3.2. In the second stage of the data construction process, the approach to defining pockets needs further explanation or an ablation study.\n\n4. Experiments: It would be better to add some biological justification or visualization of the results.\n\nFor this paper, one fact is that the technical novelty is below the bar of ICLR. However, I admire the simple but effective model for the right question. It's a struggle for me to make a decision. I will maintain a neutral attitude and make my final decision after the discussion.\n\nQuestions: See weakness.", "labeling_timestamp": "2026-01-11T16:25:05.690940", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper clearly states that pretrained small-molecule encoders provide useful binding-related priors and that the pocket encoder is aligned to such pretrained molecular representations (supporting the first part). However, the paper does not claim — nor provide evidence — that many non-molecule-pretrained models also satisfy this prior, so the second part of the reviewer's statement is not supported by the paper.", "evidence": "“we align the pocket encoder with a pretrained small molecule encoder, whose weights are fixed. This pretrained encoder serves as a guide, transferring binding-specific and biologically relevant information from extensive molecular datasets to our pocket encoder.”; “The introduction of a new molecular guided fragment-surroundings contrastive learning method for pocket representations, which naturally distillates comprehensive structural and chemical knowledge from pretrained small molecule encoders.”", "section": "Abstract; Introduction; 3.2 (Contrastive Learning in Pocket-Fragment Space)"}
{"claim": "The reviewer strongly suggests removing Theorem 3.1 because its bound is trivial and not informative for the paper's contributions.", "claim_type": "novelty", "paper_id": "uMAujpVi9m", "paper_title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "9ap4s1SZlZ", "reviewer": "Reviewer_yfM3", "review_text": "Summary: This paper primarily aims to enhance the pocket pretraining method, as existing approaches only consider pockets during pretraining. There are two main contributions in this paper: (1) The authors introduce a novel method, ProFSA, for pocket pretraining, which extracts additional information from corresponding ligands. However, the number of pocket-ligand complex structures is quite limited in existing datasets. (2) To address this issue, the authors generate over 5 million complexes by segmenting fragments and their corresponding pockets in protein structures. By aligning features of fragments and pockets, the pocket encoder learns the interaction between fragments and pockets. The authors design downstream tasks such as pocket druggability prediction, pocket matching, and ligand binding affinity prediction to demonstrate the effectiveness of ProFSA.\n\nStrengths: The authors propose a new perspective of pretraining pockets and construct a large-scale dataset, which data distribution is also considered, to make the efficient pre-training possible.\n\nThe results are competitive, especially for zero-shot settings.\n\nAbundant experiments and ablation study support the argument and result of the authors.\n\nWeaknesses: 1. The technical novelty is limited.\n  - The pocket encoder is borrowed from Uni-Mol.\n  - The contrastive loss is the vanilla form of classical contrastive learning.\n\n2. The bound of Theorem 3.1 is trivial. The authors claim that the bound naturally exists for these representations extracted by pretrained molecule models. However, it's a bit counterintuitive, because many models not pretrained on molecule datasets also fulfill this prior. So, can these models be used for this task? **I strongly suggest removing this part from the paper**.\n\n3. Some issues about dataset creation:\n - 3.1. The authors consider the distribution of ligand size and pocket size when designing the dataset. However, molecules possess more properties that can also lead to imbalance. It would be better to, at least, add some discussion about this issue.\n - 3.2. In the second stage of the data construction process, the approach to defining pockets needs further explanation or an ablation study.\n\n4. Experiments: It would be better to add some biological justification or visualization of the results.\n\nFor this paper, one fact is that the technical novelty is below the bar of ICLR. However, I admire the simple but effective model for the right question. It's a struggle for me to make a decision. I will maintain a neutral attitude and make my final decision after the discussion.\n\nQuestions: See weakness.", "labeling_timestamp": "2026-01-11T16:25:05.711841", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper content (Sections 1–3.2) contains methodology, data construction, and contrastive learning details but does not include any theorem labeled 'Theorem 3.1' or a formal bound. Therefore there is insufficient evidence in the given text to judge the reviewer's claim about Theorem 3.1 being trivial or uninformative.", "evidence": "Specifically, for a protein pocket p and its corresponding pseudo-ligand l , we denote their normalized encoding vectors as s and t . Let g^T and g^S denote the ligand and protein prediction networks, respectively. Then two distinct contrastive losses are used to facilitate the training process:", "section": "3.2 CONTRASTIVE LEARNING IN POCKET-FRAGMENT SPACE"}
{"claim": "Existing public datasets contain a limited number of pocket-ligand complex structures, limiting direct pocket-ligand pretraining opportunities.", "claim_type": "quantitative", "paper_id": "uMAujpVi9m", "paper_title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "9ap4s1SZlZ", "reviewer": "Reviewer_yfM3", "review_text": "Summary: This paper primarily aims to enhance the pocket pretraining method, as existing approaches only consider pockets during pretraining. There are two main contributions in this paper: (1) The authors introduce a novel method, ProFSA, for pocket pretraining, which extracts additional information from corresponding ligands. However, the number of pocket-ligand complex structures is quite limited in existing datasets. (2) To address this issue, the authors generate over 5 million complexes by segmenting fragments and their corresponding pockets in protein structures. By aligning features of fragments and pockets, the pocket encoder learns the interaction between fragments and pockets. The authors design downstream tasks such as pocket druggability prediction, pocket matching, and ligand binding affinity prediction to demonstrate the effectiveness of ProFSA.\n\nStrengths: The authors propose a new perspective of pretraining pockets and construct a large-scale dataset, which data distribution is also considered, to make the efficient pre-training possible.\n\nThe results are competitive, especially for zero-shot settings.\n\nAbundant experiments and ablation study support the argument and result of the authors.\n\nWeaknesses: 1. The technical novelty is limited.\n  - The pocket encoder is borrowed from Uni-Mol.\n  - The contrastive loss is the vanilla form of classical contrastive learning.\n\n2. The bound of Theorem 3.1 is trivial. The authors claim that the bound naturally exists for these representations extracted by pretrained molecule models. However, it's a bit counterintuitive, because many models not pretrained on molecule datasets also fulfill this prior. So, can these models be used for this task? **I strongly suggest removing this part from the paper**.\n\n3. Some issues about dataset creation:\n - 3.1. The authors consider the distribution of ligand size and pocket size when designing the dataset. However, molecules possess more properties that can also lead to imbalance. It would be better to, at least, add some discussion about this issue.\n - 3.2. In the second stage of the data construction process, the approach to defining pockets needs further explanation or an ablation study.\n\n4. Experiments: It would be better to add some biological justification or visualization of the results.\n\nFor this paper, one fact is that the technical novelty is below the bar of ICLR. However, I admire the simple but effective model for the right question. It's a struggle for me to make a decision. I will maintain a neutral attitude and make my final decision after the discussion.\n\nQuestions: See weakness.", "labeling_timestamp": "2026-01-11T16:25:01.173041", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly states that publicly available pocket-ligand complex datasets are small (under 100k non-redundant pairs) and gives specific counts (e.g., PDBBind ~19k, BioLip2 467,808 total but only ~71k non-redundant), supporting the reviewer's claim about limited data restricting pretraining.", "evidence": "Abstract: \"However, the limited pocket-ligand complex structures available in the PDB database (less than 100 thousand nonredundant pairs) hampers large-scale pretraining endeavors for interaction modeling.\"; Section 2.1 POCKET PRETRAINING DATA: \"The most famous database is the PDBBind ..., which consists of 19,443 protein-ligand pairs in the latest version (v2020).\" and \"Biolip2 ... includes 467,808 pocket-ligand pairs, but only 71,178 pairs are non-redundant (the weekly version of 2023-09-21).\"", "section": "Abstract; 2.1 POCKET PRETRAINING DATA"}
{"claim": "The authors considered only ligand size and pocket size distributions when designing the dataset, omitting other potentially relevant molecular properties.", "claim_type": "methodology", "paper_id": "uMAujpVi9m", "paper_title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "9ap4s1SZlZ", "reviewer": "Reviewer_yfM3", "review_text": "Summary: This paper primarily aims to enhance the pocket pretraining method, as existing approaches only consider pockets during pretraining. There are two main contributions in this paper: (1) The authors introduce a novel method, ProFSA, for pocket pretraining, which extracts additional information from corresponding ligands. However, the number of pocket-ligand complex structures is quite limited in existing datasets. (2) To address this issue, the authors generate over 5 million complexes by segmenting fragments and their corresponding pockets in protein structures. By aligning features of fragments and pockets, the pocket encoder learns the interaction between fragments and pockets. The authors design downstream tasks such as pocket druggability prediction, pocket matching, and ligand binding affinity prediction to demonstrate the effectiveness of ProFSA.\n\nStrengths: The authors propose a new perspective of pretraining pockets and construct a large-scale dataset, which data distribution is also considered, to make the efficient pre-training possible.\n\nThe results are competitive, especially for zero-shot settings.\n\nAbundant experiments and ablation study support the argument and result of the authors.\n\nWeaknesses: 1. The technical novelty is limited.\n  - The pocket encoder is borrowed from Uni-Mol.\n  - The contrastive loss is the vanilla form of classical contrastive learning.\n\n2. The bound of Theorem 3.1 is trivial. The authors claim that the bound naturally exists for these representations extracted by pretrained molecule models. However, it's a bit counterintuitive, because many models not pretrained on molecule datasets also fulfill this prior. So, can these models be used for this task? **I strongly suggest removing this part from the paper**.\n\n3. Some issues about dataset creation:\n - 3.1. The authors consider the distribution of ligand size and pocket size when designing the dataset. However, molecules possess more properties that can also lead to imbalance. It would be better to, at least, add some discussion about this issue.\n - 3.2. In the second stage of the data construction process, the approach to defining pockets needs further explanation or an ablation study.\n\n4. Experiments: It would be better to add some biological justification or visualization of the results.\n\nFor this paper, one fact is that the technical novelty is below the bar of ICLR. However, I admire the simple but effective model for the right question. It's a struggle for me to make a decision. I will maintain a neutral attitude and make my final decision after the discussion.\n\nQuestions: See weakness.", "labeling_timestamp": "2026-01-11T16:25:37.618977", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states they used multiple criteria when constructing the dataset beyond pocket and ligand size distributions, including sampling by relative buried surface area (rBSA), excluding residues with peptide-bond constraints to focus on long-range interactions, and applying terminal corrections to fragments.", "evidence": "“To bridge the gap between fragment-pocket pairs and ligand-pocket pairs, we employ three strategies. Firstly, we focus on residues with long-range interactions, excluding those constrained with peptide bonds absent in real ligand-protein complexes (Wang et al., 2022a). Secondly, we sample fragment-pocket pairs based on their relative buried surface area (rBSA) and the joint distribution of pocket and ligand sizes to mimic real ligands. Thirdly, we address property mismatches caused by the segmentation process by applying terminal corrections (Marino et al., 2015; Arbour et al., 2020) to fragments.”", "section": "Introduction (bridging strategies) and 3.1 Constructing pseudo-ligand-pocket complexes"}
{"claim": "The constructed dataset neglects other molecular properties that can cause imbalance and the paper lacks discussion about these potential imbalances.", "claim_type": "experimental", "paper_id": "uMAujpVi9m", "paper_title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "9ap4s1SZlZ", "reviewer": "Reviewer_yfM3", "review_text": "Summary: This paper primarily aims to enhance the pocket pretraining method, as existing approaches only consider pockets during pretraining. There are two main contributions in this paper: (1) The authors introduce a novel method, ProFSA, for pocket pretraining, which extracts additional information from corresponding ligands. However, the number of pocket-ligand complex structures is quite limited in existing datasets. (2) To address this issue, the authors generate over 5 million complexes by segmenting fragments and their corresponding pockets in protein structures. By aligning features of fragments and pockets, the pocket encoder learns the interaction between fragments and pockets. The authors design downstream tasks such as pocket druggability prediction, pocket matching, and ligand binding affinity prediction to demonstrate the effectiveness of ProFSA.\n\nStrengths: The authors propose a new perspective of pretraining pockets and construct a large-scale dataset, which data distribution is also considered, to make the efficient pre-training possible.\n\nThe results are competitive, especially for zero-shot settings.\n\nAbundant experiments and ablation study support the argument and result of the authors.\n\nWeaknesses: 1. The technical novelty is limited.\n  - The pocket encoder is borrowed from Uni-Mol.\n  - The contrastive loss is the vanilla form of classical contrastive learning.\n\n2. The bound of Theorem 3.1 is trivial. The authors claim that the bound naturally exists for these representations extracted by pretrained molecule models. However, it's a bit counterintuitive, because many models not pretrained on molecule datasets also fulfill this prior. So, can these models be used for this task? **I strongly suggest removing this part from the paper**.\n\n3. Some issues about dataset creation:\n - 3.1. The authors consider the distribution of ligand size and pocket size when designing the dataset. However, molecules possess more properties that can also lead to imbalance. It would be better to, at least, add some discussion about this issue.\n - 3.2. In the second stage of the data construction process, the approach to defining pockets needs further explanation or an ablation study.\n\n4. Experiments: It would be better to add some biological justification or visualization of the results.\n\nFor this paper, one fact is that the technical novelty is below the bar of ICLR. However, I admire the simple but effective model for the right question. It's a struggle for me to make a decision. I will maintain a neutral attitude and make my final decision after the discussion.\n\nQuestions: See weakness.", "labeling_timestamp": "2026-01-11T16:25:23.687250", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper does acknowledge and mitigate some dataset mismatches (e.g., sampling to match PDBBind rBSA and size distributions, applying terminal corrections, and counteracting peptide-fragment bias by aligning to a pretrained small-molecule encoder). However, it does not provide a broad discussion of other molecular properties that might cause dataset imbalance beyond those specific measures, so the reviewer's claim is only partially accurate.", "evidence": "1) \"Secondly, we sample fragment-pocket pairs based on their relative buried surface area (rBSA) and the joint distribution of pocket and ligand sizes to mimic real ligands.\" 2) \"Thirdly, we address property mismatches caused by the segmentation process by applying terminal corrections (Marino et al., 2015; Arbour et al., 2020) to fragments.\" 3) \"To counteract biases from peptide fragments used as ligands, we align the pocket encoder with a pretrained small molecule encoder, whose weights are fixed.\"", "section": "Introduction; 3.1 Constructing Pseudo-Ligand-Pocket Complexes"}
{"claim": "In the second stage of data construction, the approach to defining pockets lacks sufficient explanation and requires an ablation study or clearer justification.", "claim_type": "methodology", "paper_id": "uMAujpVi9m", "paper_title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "9ap4s1SZlZ", "reviewer": "Reviewer_yfM3", "review_text": "Summary: This paper primarily aims to enhance the pocket pretraining method, as existing approaches only consider pockets during pretraining. There are two main contributions in this paper: (1) The authors introduce a novel method, ProFSA, for pocket pretraining, which extracts additional information from corresponding ligands. However, the number of pocket-ligand complex structures is quite limited in existing datasets. (2) To address this issue, the authors generate over 5 million complexes by segmenting fragments and their corresponding pockets in protein structures. By aligning features of fragments and pockets, the pocket encoder learns the interaction between fragments and pockets. The authors design downstream tasks such as pocket druggability prediction, pocket matching, and ligand binding affinity prediction to demonstrate the effectiveness of ProFSA.\n\nStrengths: The authors propose a new perspective of pretraining pockets and construct a large-scale dataset, which data distribution is also considered, to make the efficient pre-training possible.\n\nThe results are competitive, especially for zero-shot settings.\n\nAbundant experiments and ablation study support the argument and result of the authors.\n\nWeaknesses: 1. The technical novelty is limited.\n  - The pocket encoder is borrowed from Uni-Mol.\n  - The contrastive loss is the vanilla form of classical contrastive learning.\n\n2. The bound of Theorem 3.1 is trivial. The authors claim that the bound naturally exists for these representations extracted by pretrained molecule models. However, it's a bit counterintuitive, because many models not pretrained on molecule datasets also fulfill this prior. So, can these models be used for this task? **I strongly suggest removing this part from the paper**.\n\n3. Some issues about dataset creation:\n - 3.1. The authors consider the distribution of ligand size and pocket size when designing the dataset. However, molecules possess more properties that can also lead to imbalance. It would be better to, at least, add some discussion about this issue.\n - 3.2. In the second stage of the data construction process, the approach to defining pockets needs further explanation or an ablation study.\n\n4. Experiments: It would be better to add some biological justification or visualization of the results.\n\nFor this paper, one fact is that the technical novelty is below the bar of ICLR. However, I admire the simple but effective model for the right question. It's a struggle for me to make a decision. I will maintain a neutral attitude and make my final decision after the discussion.\n\nQuestions: See weakness.", "labeling_timestamp": "2026-01-11T16:25:27.908704", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly describes the pocket-definition procedure (exclude five nearest residues around the fragment; define pocket residues as those with a heavy atom within 6 Å) and provides some justification (focus on long-range interactions; following Uni-Mol) and points to further details in Appendix A and alternative cutoffs in Table 10. However, the provided excerpt contains no ablation study testing these design choices, so the reviewer's call for an ablation or clearer experimental justification is partially accurate.", "evidence": "In the second phase, we exclude the five nearest residues on each side of the acquired fragment, focusing on long-range interactions. We then designate the surrounding residues containing at least one heavy atom within a 6 ˚ A proximity (following the setup of Uni-Mol, other cutoff values are presented in Table 10) to the fragment as the pocket. ... More details are provided in Appendix A.", "section": "3.1 CONSTRUCTING PSEUDO-LIGAND-POCKET COMPLEXES FROM PROTEIN DATA"}
{"claim": "The experimental results lack biological justification and do not include visualizations to support the claimed structural or biological relevance.", "claim_type": "experimental", "paper_id": "uMAujpVi9m", "paper_title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "9ap4s1SZlZ", "reviewer": "Reviewer_yfM3", "review_text": "Summary: This paper primarily aims to enhance the pocket pretraining method, as existing approaches only consider pockets during pretraining. There are two main contributions in this paper: (1) The authors introduce a novel method, ProFSA, for pocket pretraining, which extracts additional information from corresponding ligands. However, the number of pocket-ligand complex structures is quite limited in existing datasets. (2) To address this issue, the authors generate over 5 million complexes by segmenting fragments and their corresponding pockets in protein structures. By aligning features of fragments and pockets, the pocket encoder learns the interaction between fragments and pockets. The authors design downstream tasks such as pocket druggability prediction, pocket matching, and ligand binding affinity prediction to demonstrate the effectiveness of ProFSA.\n\nStrengths: The authors propose a new perspective of pretraining pockets and construct a large-scale dataset, which data distribution is also considered, to make the efficient pre-training possible.\n\nThe results are competitive, especially for zero-shot settings.\n\nAbundant experiments and ablation study support the argument and result of the authors.\n\nWeaknesses: 1. The technical novelty is limited.\n  - The pocket encoder is borrowed from Uni-Mol.\n  - The contrastive loss is the vanilla form of classical contrastive learning.\n\n2. The bound of Theorem 3.1 is trivial. The authors claim that the bound naturally exists for these representations extracted by pretrained molecule models. However, it's a bit counterintuitive, because many models not pretrained on molecule datasets also fulfill this prior. So, can these models be used for this task? **I strongly suggest removing this part from the paper**.\n\n3. Some issues about dataset creation:\n - 3.1. The authors consider the distribution of ligand size and pocket size when designing the dataset. However, molecules possess more properties that can also lead to imbalance. It would be better to, at least, add some discussion about this issue.\n - 3.2. In the second stage of the data construction process, the approach to defining pockets needs further explanation or an ablation study.\n\n4. Experiments: It would be better to add some biological justification or visualization of the results.\n\nFor this paper, one fact is that the technical novelty is below the bar of ICLR. However, I admire the simple but effective model for the right question. It's a struggle for me to make a decision. I will maintain a neutral attitude and make my final decision after the discussion.\n\nQuestions: See weakness.", "labeling_timestamp": "2026-01-11T16:25:27.445440", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper provides biological justification for the pseudo-ligand/pocket approach (discussing specific interaction types and rationale) and includes visualizations and distribution comparisons (Figures 1 and 2) to support structural/biological relevance.", "evidence": ["As we visualize in Figure 2, hydrogen bonds are formed within protein backbones and polar amino acids like serine or histidine; π -π stackings are common between aromatic amino acids including tyrosine, tryptophan, and phenylalanine; also salt bridges can be expected between oppositely charged amino acids like arginine and glutamate.", "Figure 2: Visualization of common interaction types presented in both pseudo pairs and real pairs.", "The resulting dataset has a similar distribution as the PDBBind, in terms of rBSA and the joint distribution of pocket-ligand size, as shown in Figure 1b and c.", "This pretrained encoder serves as a guide, transferring binding-specific and biologically relevant information from extensive molecular datasets to our pocket encoder."], "section": "3.1 CONSTRUCTING PSEUDO-LIGAND-POCKET COMPLEXES FROM PROTEIN DATA"}
{"claim": "It is unclear whether models not pretrained on molecule datasets can be effectively used for the proposed pocket pretraining task.", "claim_type": "experimental", "paper_id": "uMAujpVi9m", "paper_title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "9ap4s1SZlZ", "reviewer": "Reviewer_yfM3", "review_text": "Summary: This paper primarily aims to enhance the pocket pretraining method, as existing approaches only consider pockets during pretraining. There are two main contributions in this paper: (1) The authors introduce a novel method, ProFSA, for pocket pretraining, which extracts additional information from corresponding ligands. However, the number of pocket-ligand complex structures is quite limited in existing datasets. (2) To address this issue, the authors generate over 5 million complexes by segmenting fragments and their corresponding pockets in protein structures. By aligning features of fragments and pockets, the pocket encoder learns the interaction between fragments and pockets. The authors design downstream tasks such as pocket druggability prediction, pocket matching, and ligand binding affinity prediction to demonstrate the effectiveness of ProFSA.\n\nStrengths: The authors propose a new perspective of pretraining pockets and construct a large-scale dataset, which data distribution is also considered, to make the efficient pre-training possible.\n\nThe results are competitive, especially for zero-shot settings.\n\nAbundant experiments and ablation study support the argument and result of the authors.\n\nWeaknesses: 1. The technical novelty is limited.\n  - The pocket encoder is borrowed from Uni-Mol.\n  - The contrastive loss is the vanilla form of classical contrastive learning.\n\n2. The bound of Theorem 3.1 is trivial. The authors claim that the bound naturally exists for these representations extracted by pretrained molecule models. However, it's a bit counterintuitive, because many models not pretrained on molecule datasets also fulfill this prior. So, can these models be used for this task? **I strongly suggest removing this part from the paper**.\n\n3. Some issues about dataset creation:\n - 3.1. The authors consider the distribution of ligand size and pocket size when designing the dataset. However, molecules possess more properties that can also lead to imbalance. It would be better to, at least, add some discussion about this issue.\n - 3.2. In the second stage of the data construction process, the approach to defining pockets needs further explanation or an ablation study.\n\n4. Experiments: It would be better to add some biological justification or visualization of the results.\n\nFor this paper, one fact is that the technical novelty is below the bar of ICLR. However, I admire the simple but effective model for the right question. It's a struggle for me to make a decision. I will maintain a neutral attitude and make my final decision after the discussion.\n\nQuestions: See weakness.", "labeling_timestamp": "2026-01-11T16:25:32.227371", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper consistently describes using a pretrained small-molecule encoder (with fixed weights) to guide pocket pretraining and does not present experiments or discussion about using non-pretrained molecular models, so it remains unclear whether non-pretrained models would be effective.", "evidence": "\"To counteract biases from peptide fragments used as ligands, we align the pocket encoder with a pretrained small molecule encoder, whose weights are fixed. This pretrained encoder serves as a guide, transferring binding-specific and biologically relevant information from extensive molecular datasets to our pocket encoder.\"", "section": "3 OUR APPROACH (Section 3.2 / paragraph describing molecular-guided contrastive learning)"}
{"claim": "The paper's results are about computability rather than algorithmics or neural processing because it does not establish an upper bound on time to generate the next token.", "claim_type": "methodology", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:25:52.858900", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper states an existence result—'There is an algorithm …' that generates in the limit for any countable collection—without giving any time/complexity bounds or discussing runtime to produce the next token. The formal model is framed in terms of computability (existence of an algorithm and oracle membership queries) rather than algorithmic time complexity or neural processing.", "evidence": "There is an algorithm with the property that for any countable collection of languages C = { L1, L2, L3, ... }, and any enumeration of one of these languages K, the algorithm generates from K in the limit. All we assume about the list of languages is that it is specified through a black box that can answer questions of the form 'Is w ∈ L_i ?' for any string w ∈ U and language L_i ∈ C.", "section": "Section 2 Formal Model and Results"}
{"claim": "The contribution is weak from an algorithmic point of view.", "claim_type": "novelty", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:25:43.813359", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper presents a concrete algorithmic result: a constructive algorithm that generates from any countable collection of languages in the limit, and it frames this as the main theorem. Thus the claim that the contribution is weak algorithmically is contradicted by the paper's stated algorithmic contribution and formal proofs.", "evidence": "Our main result is that there is an agent that is able to generate in the limit for every countable list of candidate languages.\n\n(2.1) There is an algorithm with the property that for any countable collection of languages C = {L1, L2, L3, ...}, and any enumeration of one of these languages K, the algorithm generates from K in the limit.\n\nDespite the generality of the model, producing the generation algorithm that proves our main theorem makes use of subtle structural properties of the given list of candidate languages.", "section": "Abstract; 2 Formal Model and Results; 1 Introduction"}
{"claim": "The work is more appropriate for a computational learning theory venue (for example COLT) than for NeurIPS.", "claim_type": "subjective", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:25:47.724151", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper presents a theoretical result and frames it in relation to both classical learning-theory results (Gold, Angluin) and practical language-modeling (LLMs). It does not state or argue that it is more appropriate for a computational learning theory venue (e.g., COLT) than for NeurIPS, so the claim about venue appropriateness cannot be confirmed from the paper's text.", "evidence": "Abstract: \"Our main result is that there is an agent that is able to generate in the limit for every countable list of candidate languages.\" || Introduction (General Connections to Language Modeling): \"Our approach emphasizing theoretical properties of language generation and worst-case guarantees, in the style of the Gold-Angluin model, is a source of limitations but also a source of generality; it is therefore important to discuss how we draw stylized insights about language modeling from our theoretical formalism.\"", "section": "Abstract; Introduction (General Connections to Language Modeling)"}
{"claim": "The paper's results only hold under the assumption that all languages in the list are infinite.", "claim_type": "methodology", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:25:56.586195", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly assumes all candidate languages L_i are infinite when formalizing the model, and the main theorem (2.1) is stated within that formal model. Thus the result is presented under the assumption that every language in the list is infinite.", "evidence": "We will assume that all the languages L_i are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm would eventually run out of new strings to generate. (2.1) There is an algorithm with the property that for any countable collection of languages C = {L_1, L_2, L_3, ...}, and any enumeration of one of these languages K, the algorithm generates from K in the limit.", "section": "2 Formal Model and Results"}
{"claim": "Assuming all languages are infinite is an unnatural and unjustified restriction seemingly introduced to make the proof work.", "claim_type": "subjective", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:25:59.679914", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states and justifies the assumption that all candidate languages are infinite: it argues this is necessary for the generation specification because if a language were finite, an algorithm could not be required to output unseen strings forever. Thus the assumption is justified in the paper rather than an ad-hoc restriction to make the proof work.", "evidence": "We will assume that all the languages L_i are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm would eventually run out of new strings to generate.", "section": "2 Formal Model and Results"}
{"claim": "The proof of the main theorem breaks if the infinite-language assumption is removed.", "claim_type": "methodology", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:26:00.615676", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly assumes all candidate languages are infinite and explains this assumption is needed because an algorithm cannot output unseen strings forever if a language is finite; removing the infinite-language assumption would therefore invalidate the problem specification and the proof.", "evidence": "\"We will assume that all the languages L_i are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm would eventually run out of new strings to generate.\"", "section": "2 Formal Model and Results"}
{"claim": "The authors' explanation in line 126 for requiring all languages to be infinite is unconvincing.", "claim_type": "subjective", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:26:06.539396", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper gives an explicit, direct reason for assuming all languages are infinite: otherwise an algorithm required to output unseen strings forever would eventually run out of new strings. This directly addresses the point made at line 126 and provides a straightforward justification rather than leaving it unconvincing.", "evidence": "We will assume that all the languages L_i are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm would eventually run out of new strings to generate.", "section": "2 Formal Model and Results"}
{"claim": "Specifying that an algorithm must output unseen strings forever is impossible for finite languages, making that requirement problematic.", "claim_type": "methodology", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:26:13.776772", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly states that it assumes all candidate languages are infinite because requiring an algorithm to output unseen strings forever is impossible for a finite language; the authors give this as the reason for the assumption.", "evidence": "We will assume that all the languages L_i are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm would eventually run out of new strings to generate.", "section": "2 Formal Model and Results"}
{"claim": "The infinite-language assumption prevents instantiating the result with any computational model where language finiteness is undecidable.", "claim_type": "methodology", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:26:19.932715", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper explicitly assumes all candidate languages are infinite (to allow endless unseen outputs) but does not discuss decidability of language finiteness or whether that assumption prevents instantiation with computational models where finiteness is undecidable. Thus the paper does not address the reviewer’s specific claim about undecidability.", "evidence": "“We will assume that all the languages L_i are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm would eventually run out of new strings to generate.”", "section": "2 Formal Model and Results"}
{"claim": "Only very restricted classes of languages have decidable finiteness, so the finiteness assumption severely limits applicability.", "claim_type": "other", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:26:19.921342", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper assumes all candidate languages L_i are infinite (so finite languages are excluded) but does not discuss decidability of finiteness or argue that this assumption 'severely limits applicability.' Therefore the reviewer's claim about decidable finiteness and its impact is not addressed in the paper.", "evidence": "We will assume that all the languages L_i are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm would eventually run out of new strings to generate.", "section": "2 Formal Model and Results"}
{"claim": "Going beyond context-free languages typically renders finiteness or emptiness decision problems undecidable.", "claim_type": "other", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:26:20.815749", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does not make claims about decidability of finiteness or emptiness when moving beyond context-free languages. It formulates a model for generation/identification from a (countable) list of candidate languages and assumes access to a black box 'Is w ∈ L_i ?', but it does not discuss decision-problem (un)decidability results for finiteness or emptiness of language classes.", "evidence": "“If the reader finds it helpful for concreteness, they can consider the results that follow in the context of a specific list of languages C, such as the set of all context-free languages or the set of all regular languages; but everything we say applies to general collections of languages.”\n\n“All we assume about the list of languages is that it is specified through a black box that can answer questions of the form 'Is w ∈ L_i ?' for any string w ∈ U and language L_i ∈ C.”\n\n“We will assume that all the languages L_i are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem...”", "section": "2 Formal Model and Results (and Introduction)"}
{"claim": "Undecidability of finiteness rules out enumerating languages by enumerating the machines that represent those languages.", "claim_type": "other", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:26:33.209506", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does not discuss undecidability of finiteness or the feasibility of enumerating languages via enumerating their machine representations. Instead it assumes a countable list of languages presented via a black box and explicitly says it can 'dispense with explicit representations,' so the reviewer's claim is not addressed in the text.", "evidence": "In fact, the formal result is much more general than this, in that it is sufficient to suppose that the unknown language K simply comes from a countable list of candidate languages L1, L2, L3, ..., and we can dispense with explicit representations altogether.", "section": "Introduction / 2 Formal Model and Results"}
{"claim": "The paper does not explicitly acknowledge that assuming all languages are infinite is a drawback.", "claim_type": "methodology", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:26:38.061081", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly assumes all languages are infinite and justifies this as necessary for requiring the algorithm to output unseen strings forever, but it does not characterize this assumption as a drawback. It states the assumption and its necessity without calling it a limitation.", "evidence": "\"We will assume that all the languages L_i are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm would eventually run out of new strings to generate.\"", "section": "2 Formal Model and Results"}
{"claim": "The presentation and clarity of the paper can be significantly improved.", "claim_type": "presentation", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:26:46.708337", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper presents formal definitions, results, and proofs (e.g., a formal model and main theorem) but does not discuss or self-assess its presentation or clarity; the claim is subjective and cannot be verified from the text.", "evidence": "We now provide a formal description of the model and the statement of our results.", "section": "2 Formal Model and Results"}
{"claim": "The reviewer asks why the authors did not explicitly state in the paper that the finiteness assumption is a drawback.", "claim_type": "presentation", "paper_id": "FGTDe6EA0B", "paper_title": "Language Generation in the Limit", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "2eG34J1hN3", "reviewer": "Reviewer_v4Eq", "review_text": "Comment: -----------------------------------\n\nREASON 3) As it is currently stated, the result is a pure theoretical result in the realm of computational learning theory given that in general it is not possible to establish an upper bound on the time necessary to generate the next token. Therefore, the result is a result about \"computability\" not a result about \"algorithmics\" and much less neural processing. This is what I mean when I write: \"Therefore, from an algorithmic point of view, the contribution of the paper is weak.\" and \"It may be the case that the paper is more adequate to a conference specialized in computational learning theory, such as COLT.\". In my opinion, the contributions of the paper do not have much to do with topics covered within NeurIPS.\n\n-----------------------------------\n\nREASON 4) The results of the paper only seem to hold under the assumption that all languages in the list are infinite. This seems to be an unnatural assumption. From what I understand, the proof breaks if this assumption is removed, and therefore, this leads me to conclude that the assumption is made for the sake of making the proof carry over. Please note that the authors write in line 126:\n\n\"We  will assume that all the languages Li are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm  would eventually run out of new strings to generate.\"\n\nI do not agree with this explanation. There is no apparent justification for requiring that all languages in the countable list are infinite, other than to make the proof of the main theorem work. This seems to be a very restrictive assumption, because it rules out the possibility of instantiating the result in a concrete way with any model of computation where language finitetess is undecidable. Please note that only very restricted classes of languages are known to have decidable finiteness. Going a bit beyond context-freeness already renders the finiteness test or even (emptiness) undecidable. So, this rules out the possibility of enumerating over these languages by enumerating the \"machines\" representing the languages. \n\nI believe that in your reply you agree that assuming finiteness is a drawback. Why not make this explicitly in the paper? \n\n-----------------------------------\n\nFor the reasons mentioned above, I will keep my score \"weak reject\" mostly because I believe that presentation of the paper can be significantly improved towards clarity, and also because the results in the paper are much more in the realm of computability theory than in the realm of neural networks.", "labeling_timestamp": "2026-01-11T16:26:43.046686", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly discusses and justifies the assumption that all languages are infinite, noting that finite languages make the generation task impossible because an algorithm cannot output unseen strings forever.", "evidence": "\"We will assume that all the languages L_i are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm would eventually run out of new strings to generate.\"", "section": "2 Formal Model and Results"}
{"claim": "The paper does not report the computational latency or runtime cost of the iterative initialization algorithm used for decomposing pretrained matrices.", "claim_type": "experimental", "paper_id": "xw29VvOMmU", "paper_title": "LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Bm88GSraZd", "reviewer": "Reviewer_CSBT", "review_text": "Summary: This paper proposes LQ-LoRA, a memory-efficient LLM adaptation method that decomposes each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. The algorithm is adapted from QLoRA and applied modification to solve the problem that zero initialization of the low-rank matrix may not be optimal when the fixed matrix is quantized. The method decomposes the matrix by an iterative algorithm and updates only the low-rank matrix weights during fine-tuning. Results showed that the proposed method outperforms QLoRA and LREC with similar bit compression rates.\n\nStrengths: -\tThe proposed method decomposes the pretrained matrix into a quantizable fixed matrix and low-rank matrix that is already optimized before fine-tuning starts, which contributes to improved accuracy.\n-\tThe paper shows that LQ-LoRA can be used as a mixed quantization strategy, and also proposes a data-aware version of the algorithm, which enables users to flexibly set a target memory budget.\n-\tResults show that the proposed method can be generalized to different model families by showing outperforming results with RoBERTa and LLaMA.\n\nWeaknesses: - The authors have introduced a method that employs an iterative algorithm for initialization. Can they provide insights regarding the computational latency associated with their approach?\n\n- The authors assert the efficiency of LQ-LoRA based on empirical evidence, yet lack theoretical backing. To strengthen the credibility of the algorithm, a comparison might be beneficial, especially with methods that initialize the Q(W) + L1L2 matrix in a manner that closely mirrors the original pretrained matrix W. Consider, for instance, the use of GPTQ as a compensatory mechanism.\n\n- It appears that this paper serves as an expanded or refined rendition of the Q-LoRA paper. As such, it seemingly inherits the same limitation, notably the inference overhead, given that this approach must fail to integrate the LoRA layer into an existing linear layer. \n\n- Similarly, I would like to raise a query about the paper's novelty. While this method undeniably enhances the current approach (Q-LoRA), from a PEFT perspective, there could be superior methods, particularly concerning inference challenges. On the topic of novelty, I await the insights of fellow reviewers.\n\nQuestions: Included in the weakness.", "labeling_timestamp": "2026-01-11T16:26:34.513690", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly reports runtime information for the iterative decomposition: it states the per-step time on a modern GPU for a 4096×4096 matrix, so the claim that it does not report computational latency or runtime cost is false.", "evidence": "\"Each step of the algorithm (i.e., randomized SVD followed by quantization) takes a few seconds on a modern GPU for a 4096 × 4096 matrix.\"", "section": "3.1 LOW-RANK PLUS QUANTIZED MATRIX DECOMPOSITION"}
{"claim": "The authors assert empirical efficiency of LQ-LoRA but provide no theoretical analysis or proofs to justify those efficiency claims.", "claim_type": "methodology", "paper_id": "xw29VvOMmU", "paper_title": "LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Bm88GSraZd", "reviewer": "Reviewer_CSBT", "review_text": "Summary: This paper proposes LQ-LoRA, a memory-efficient LLM adaptation method that decomposes each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. The algorithm is adapted from QLoRA and applied modification to solve the problem that zero initialization of the low-rank matrix may not be optimal when the fixed matrix is quantized. The method decomposes the matrix by an iterative algorithm and updates only the low-rank matrix weights during fine-tuning. Results showed that the proposed method outperforms QLoRA and LREC with similar bit compression rates.\n\nStrengths: -\tThe proposed method decomposes the pretrained matrix into a quantizable fixed matrix and low-rank matrix that is already optimized before fine-tuning starts, which contributes to improved accuracy.\n-\tThe paper shows that LQ-LoRA can be used as a mixed quantization strategy, and also proposes a data-aware version of the algorithm, which enables users to flexibly set a target memory budget.\n-\tResults show that the proposed method can be generalized to different model families by showing outperforming results with RoBERTa and LLaMA.\n\nWeaknesses: - The authors have introduced a method that employs an iterative algorithm for initialization. Can they provide insights regarding the computational latency associated with their approach?\n\n- The authors assert the efficiency of LQ-LoRA based on empirical evidence, yet lack theoretical backing. To strengthen the credibility of the algorithm, a comparison might be beneficial, especially with methods that initialize the Q(W) + L1L2 matrix in a manner that closely mirrors the original pretrained matrix W. Consider, for instance, the use of GPTQ as a compensatory mechanism.\n\n- It appears that this paper serves as an expanded or refined rendition of the Q-LoRA paper. As such, it seemingly inherits the same limitation, notably the inference overhead, given that this approach must fail to integrate the LoRA layer into an existing linear layer. \n\n- Similarly, I would like to raise a query about the paper's novelty. While this method undeniably enhances the current approach (Q-LoRA), from a PEFT perspective, there could be superior methods, particularly concerning inference challenges. On the topic of novelty, I await the insights of fellow reviewers.\n\nQuestions: Included in the weakness.", "labeling_timestamp": "2026-01-11T16:26:57.675876", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper presents empirical results claiming LQ-LoRA's effectiveness (Abstract, experiments) but explicitly describes the decomposition algorithm as heuristic and notes that theoretical convergence guarantees are not provided.", "evidence": ["\"Experiments on finetuning RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables aggressive quantization to sub-3 bits with only minor performance degradations.\" (Abstract)", "\"Unlike (some) RPCA algorithms for which theoretical convergence guarantees can be obtained (Ma & Aybat, 2018), the above algorithm is heuristic.\" (Section 3.1)"], "section": "Abstract; Section 3.1 (Low-Rank Plus Quantized Matrix Decomposition)"}
{"claim": "The paper lacks comparisons to alternative initialization methods that set Q(W)+L1L2 to closely approximate the original pretrained matrix W.", "claim_type": "baseline", "paper_id": "xw29VvOMmU", "paper_title": "LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Bm88GSraZd", "reviewer": "Reviewer_CSBT", "review_text": "Summary: This paper proposes LQ-LoRA, a memory-efficient LLM adaptation method that decomposes each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. The algorithm is adapted from QLoRA and applied modification to solve the problem that zero initialization of the low-rank matrix may not be optimal when the fixed matrix is quantized. The method decomposes the matrix by an iterative algorithm and updates only the low-rank matrix weights during fine-tuning. Results showed that the proposed method outperforms QLoRA and LREC with similar bit compression rates.\n\nStrengths: -\tThe proposed method decomposes the pretrained matrix into a quantizable fixed matrix and low-rank matrix that is already optimized before fine-tuning starts, which contributes to improved accuracy.\n-\tThe paper shows that LQ-LoRA can be used as a mixed quantization strategy, and also proposes a data-aware version of the algorithm, which enables users to flexibly set a target memory budget.\n-\tResults show that the proposed method can be generalized to different model families by showing outperforming results with RoBERTa and LLaMA.\n\nWeaknesses: - The authors have introduced a method that employs an iterative algorithm for initialization. Can they provide insights regarding the computational latency associated with their approach?\n\n- The authors assert the efficiency of LQ-LoRA based on empirical evidence, yet lack theoretical backing. To strengthen the credibility of the algorithm, a comparison might be beneficial, especially with methods that initialize the Q(W) + L1L2 matrix in a manner that closely mirrors the original pretrained matrix W. Consider, for instance, the use of GPTQ as a compensatory mechanism.\n\n- It appears that this paper serves as an expanded or refined rendition of the Q-LoRA paper. As such, it seemingly inherits the same limitation, notably the inference overhead, given that this approach must fail to integrate the LoRA layer into an existing linear layer. \n\n- Similarly, I would like to raise a query about the paper's novelty. While this method undeniably enhances the current approach (Q-LoRA), from a PEFT perspective, there could be superior methods, particularly concerning inference challenges. On the topic of novelty, I await the insights of fellow reviewers.\n\nQuestions: Included in the weakness.", "labeling_timestamp": "2026-01-11T16:27:11.191190", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly considers and compares an alternative initialization (Q(0) = Quantize(W)) and reports that it did not significantly change performance; it also compares LQ-LoRA to quantized-LoRA baselines (QLoRA, GPTQ-LoRA). Thus it does not lack comparisons to alternative initializations of the form Q(W)+L1L2 ≈ W.", "evidence": "\"we are not aware of prior work on low-rank plus quantized matrix decomposition, except for a recent preprint which proposes to perform SVD on the residuals E = W -Quantize( W ) to correct for errors after quantization (Yao et al., 2023). This approach can be seen as performing a single step of the iterative algorithm with the initialization Q (0) = Quantize( W ) . In our experiments we did not observe significant differences in performance when we initialized Q (0) to Quantize( W ).\"", "section": "3.1 LOW-RANK PLUS QUANTIZED MATRIX DECOMPOSITION"}
{"claim": "The paper does not compare LQ-LoRA against GPTQ or similar compensatory quantization methods as potential baselines.", "claim_type": "baseline", "paper_id": "xw29VvOMmU", "paper_title": "LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Bm88GSraZd", "reviewer": "Reviewer_CSBT", "review_text": "Summary: This paper proposes LQ-LoRA, a memory-efficient LLM adaptation method that decomposes each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. The algorithm is adapted from QLoRA and applied modification to solve the problem that zero initialization of the low-rank matrix may not be optimal when the fixed matrix is quantized. The method decomposes the matrix by an iterative algorithm and updates only the low-rank matrix weights during fine-tuning. Results showed that the proposed method outperforms QLoRA and LREC with similar bit compression rates.\n\nStrengths: -\tThe proposed method decomposes the pretrained matrix into a quantizable fixed matrix and low-rank matrix that is already optimized before fine-tuning starts, which contributes to improved accuracy.\n-\tThe paper shows that LQ-LoRA can be used as a mixed quantization strategy, and also proposes a data-aware version of the algorithm, which enables users to flexibly set a target memory budget.\n-\tResults show that the proposed method can be generalized to different model families by showing outperforming results with RoBERTa and LLaMA.\n\nWeaknesses: - The authors have introduced a method that employs an iterative algorithm for initialization. Can they provide insights regarding the computational latency associated with their approach?\n\n- The authors assert the efficiency of LQ-LoRA based on empirical evidence, yet lack theoretical backing. To strengthen the credibility of the algorithm, a comparison might be beneficial, especially with methods that initialize the Q(W) + L1L2 matrix in a manner that closely mirrors the original pretrained matrix W. Consider, for instance, the use of GPTQ as a compensatory mechanism.\n\n- It appears that this paper serves as an expanded or refined rendition of the Q-LoRA paper. As such, it seemingly inherits the same limitation, notably the inference overhead, given that this approach must fail to integrate the LoRA layer into an existing linear layer. \n\n- Similarly, I would like to raise a query about the paper's novelty. While this method undeniably enhances the current approach (Q-LoRA), from a PEFT perspective, there could be superior methods, particularly concerning inference challenges. On the topic of novelty, I await the insights of fellow reviewers.\n\nQuestions: Included in the weakness.", "labeling_timestamp": "2026-01-11T16:27:01.621118", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states that it compares against GPTQ-LoRA and reports outperforming it; both the Abstract and Introduction mention GPTQ-LoRA as a baseline.", "evidence": "Abstract: \"Experiments on finetuning RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables aggressive quantization to sub-3 bits with only minor performance degradations.\"", "section": "Abstract; Introduction"}
{"claim": "LQ-LoRA appears to inherit QLoRA's inference overhead limitation because it cannot integrate the LoRA layer directly into existing linear layers.", "claim_type": "methodology", "paper_id": "xw29VvOMmU", "paper_title": "LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Bm88GSraZd", "reviewer": "Reviewer_CSBT", "review_text": "Summary: This paper proposes LQ-LoRA, a memory-efficient LLM adaptation method that decomposes each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. The algorithm is adapted from QLoRA and applied modification to solve the problem that zero initialization of the low-rank matrix may not be optimal when the fixed matrix is quantized. The method decomposes the matrix by an iterative algorithm and updates only the low-rank matrix weights during fine-tuning. Results showed that the proposed method outperforms QLoRA and LREC with similar bit compression rates.\n\nStrengths: -\tThe proposed method decomposes the pretrained matrix into a quantizable fixed matrix and low-rank matrix that is already optimized before fine-tuning starts, which contributes to improved accuracy.\n-\tThe paper shows that LQ-LoRA can be used as a mixed quantization strategy, and also proposes a data-aware version of the algorithm, which enables users to flexibly set a target memory budget.\n-\tResults show that the proposed method can be generalized to different model families by showing outperforming results with RoBERTa and LLaMA.\n\nWeaknesses: - The authors have introduced a method that employs an iterative algorithm for initialization. Can they provide insights regarding the computational latency associated with their approach?\n\n- The authors assert the efficiency of LQ-LoRA based on empirical evidence, yet lack theoretical backing. To strengthen the credibility of the algorithm, a comparison might be beneficial, especially with methods that initialize the Q(W) + L1L2 matrix in a manner that closely mirrors the original pretrained matrix W. Consider, for instance, the use of GPTQ as a compensatory mechanism.\n\n- It appears that this paper serves as an expanded or refined rendition of the Q-LoRA paper. As such, it seemingly inherits the same limitation, notably the inference overhead, given that this approach must fail to integrate the LoRA layer into an existing linear layer. \n\n- Similarly, I would like to raise a query about the paper's novelty. While this method undeniably enhances the current approach (Q-LoRA), from a PEFT perspective, there could be superior methods, particularly concerning inference challenges. On the topic of novelty, I await the insights of fellow reviewers.\n\nQuestions: Included in the weakness.", "labeling_timestamp": "2026-01-11T16:27:16.921174", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper describes that the pretrained weights are decomposed into a fixed quantized component Q and a separate low-rank component L1L2 that is updated, and it explicitly states that dequantization is required \"on the fly\" for finetuning and inference. This implies extra runtime work similar to QLoRA. However, the paper does not explicitly state that the LoRA/low-rank component cannot be merged into existing linear layers for inference or directly claim it inherits QLoRA's exact inference-overhead limitation, so the reviewer claim is not fully confirmed by the text.", "evidence": ["Here Q is the quantized component which remains fixed and L1 L2 is the low-rank component. During adaptation only L1 and L2 (which captures the high-variance subspaces of W ) are finetuned.", "Dequantization, which is needed on the fly for finetuning and inference, simply reverses this process."], "section": ["3.1 LOW-RANK PLUS QUANTIZED MATRIX DECOMPOSITION", "3.2 MIXED-CONFIGURATION QUANTIZATION VIA AN INTEGER LINEAR PROGRAM"]}
{"claim": "The paper's novelty is unclear, appearing as an incremental refinement of Q-LoRA rather than a distinct, original PEFT contribution.", "claim_type": "novelty", "paper_id": "xw29VvOMmU", "paper_title": "LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Bm88GSraZd", "reviewer": "Reviewer_CSBT", "review_text": "Summary: This paper proposes LQ-LoRA, a memory-efficient LLM adaptation method that decomposes each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. The algorithm is adapted from QLoRA and applied modification to solve the problem that zero initialization of the low-rank matrix may not be optimal when the fixed matrix is quantized. The method decomposes the matrix by an iterative algorithm and updates only the low-rank matrix weights during fine-tuning. Results showed that the proposed method outperforms QLoRA and LREC with similar bit compression rates.\n\nStrengths: -\tThe proposed method decomposes the pretrained matrix into a quantizable fixed matrix and low-rank matrix that is already optimized before fine-tuning starts, which contributes to improved accuracy.\n-\tThe paper shows that LQ-LoRA can be used as a mixed quantization strategy, and also proposes a data-aware version of the algorithm, which enables users to flexibly set a target memory budget.\n-\tResults show that the proposed method can be generalized to different model families by showing outperforming results with RoBERTa and LLaMA.\n\nWeaknesses: - The authors have introduced a method that employs an iterative algorithm for initialization. Can they provide insights regarding the computational latency associated with their approach?\n\n- The authors assert the efficiency of LQ-LoRA based on empirical evidence, yet lack theoretical backing. To strengthen the credibility of the algorithm, a comparison might be beneficial, especially with methods that initialize the Q(W) + L1L2 matrix in a manner that closely mirrors the original pretrained matrix W. Consider, for instance, the use of GPTQ as a compensatory mechanism.\n\n- It appears that this paper serves as an expanded or refined rendition of the Q-LoRA paper. As such, it seemingly inherits the same limitation, notably the inference overhead, given that this approach must fail to integrate the LoRA layer into an existing linear layer. \n\n- Similarly, I would like to raise a query about the paper's novelty. While this method undeniably enhances the current approach (Q-LoRA), from a PEFT perspective, there could be superior methods, particularly concerning inference challenges. On the topic of novelty, I await the insights of fellow reviewers.\n\nQuestions: Included in the weakness.", "labeling_timestamp": "2026-01-11T16:27:24.566777", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly presents novel elements beyond QLoRA: an iterative low-rank plus quantized matrix decomposition, an integer linear program for mixed-layer quantization configuration, and a data-aware Fisher-weighted decomposition. The authors also state they are not aware of prior work on this decomposition (except a recent single-step preprint), and they position LQ-LoRA as improving upon QLoRA and GPTQ-LoRA. Thus the claim that the novelty is unclear or that the work is merely an incremental refinement of QLoRA is contradicted by the paper's stated contributions and methods.", "evidence": "Abstract: \"We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. ... We present an integer linear programming formulation of the quantization component which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget. We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition.\"; Introduction/§3.1: \"Despite the simplity of our approach, we are not aware of prior work on low-rank plus quantized matrix decomposition, except for a recent preprint which proposes to perform SVD on the residuals E = W -Quantize(W) to correct for errors after quantization (Yao et al., 2023).\"; Introduction: \"We apply LQ-LoRA to adapt RoBERTa ... and find that it can meaningfully improve upon strong QLoRA (Dettmers et al., 2023a) and GPTQ-LoRA ...\"", "section": "Abstract; Introduction; Section 3.1"}
{"claim": "The paper does not compare with other PEFT methods that might better address inference challenges, leaving potentially superior alternatives unexamined.", "claim_type": "baseline", "paper_id": "xw29VvOMmU", "paper_title": "LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Bm88GSraZd", "reviewer": "Reviewer_CSBT", "review_text": "Summary: This paper proposes LQ-LoRA, a memory-efficient LLM adaptation method that decomposes each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. The algorithm is adapted from QLoRA and applied modification to solve the problem that zero initialization of the low-rank matrix may not be optimal when the fixed matrix is quantized. The method decomposes the matrix by an iterative algorithm and updates only the low-rank matrix weights during fine-tuning. Results showed that the proposed method outperforms QLoRA and LREC with similar bit compression rates.\n\nStrengths: -\tThe proposed method decomposes the pretrained matrix into a quantizable fixed matrix and low-rank matrix that is already optimized before fine-tuning starts, which contributes to improved accuracy.\n-\tThe paper shows that LQ-LoRA can be used as a mixed quantization strategy, and also proposes a data-aware version of the algorithm, which enables users to flexibly set a target memory budget.\n-\tResults show that the proposed method can be generalized to different model families by showing outperforming results with RoBERTa and LLaMA.\n\nWeaknesses: - The authors have introduced a method that employs an iterative algorithm for initialization. Can they provide insights regarding the computational latency associated with their approach?\n\n- The authors assert the efficiency of LQ-LoRA based on empirical evidence, yet lack theoretical backing. To strengthen the credibility of the algorithm, a comparison might be beneficial, especially with methods that initialize the Q(W) + L1L2 matrix in a manner that closely mirrors the original pretrained matrix W. Consider, for instance, the use of GPTQ as a compensatory mechanism.\n\n- It appears that this paper serves as an expanded or refined rendition of the Q-LoRA paper. As such, it seemingly inherits the same limitation, notably the inference overhead, given that this approach must fail to integrate the LoRA layer into an existing linear layer. \n\n- Similarly, I would like to raise a query about the paper's novelty. While this method undeniably enhances the current approach (Q-LoRA), from a PEFT perspective, there could be superior methods, particularly concerning inference challenges. On the topic of novelty, I await the insights of fellow reviewers.\n\nQuestions: Included in the weakness.", "labeling_timestamp": "2026-01-11T16:27:25.478189", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper evaluates LQ-LoRA primarily against QLoRA and GPTQ-LoRA baselines and explicitly cites QLoRA as its main baseline; it does not report comparisons to other PEFT methods (e.g., adapters, prefix tuning, prompt tuning, BitFit) that could address inference concerns.", "evidence": "1) \"Experiments on finetuning RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines...\" 2) \"QLoRA ... serves as the main baseline of the present work.\"", "section": "Abstract; 2.2 WEIGHT QUANTIZATION OF LARGE LANGUAGE MODELS"}
{"claim": "The reviewer is unclear about the biased approximation mentioned in Equation (7) despite the paper's formulation beginning from Equation (4).", "claim_type": "presentation", "paper_id": "c7m1HahBNf", "paper_title": "Exploring Structured Semantic Priors Underlying Diffusion Score for Test-time Adaptation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "U1a50zbUPt", "reviewer": "Reviewer_V2Wk", "review_text": "Comment: **Regarding all questions except Question 5**\n\nThank you for addressing my concerns related to the experiments. Your comprehensive work, especially given the short timeframe, has completely resolved my questions.\n\n**Regarding Question 5**\n\nI am still unclear on a particular point. While I understand that Diffusion-TTA relies on a biased approximation due to the optimization of the ELBO, I’m confused about the biased approximation mentioned in Eq (7), even though the current formulation begins from Eq (4). It seems to me that this could be considered a form of circular reasoning since the denoising estimator in Eq (7) is trained using the ELBO. Is my interpretation correct?", "labeling_timestamp": "2026-01-11T16:27:34.908835", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper does indeed build a formulation starting from Eq. (4) and subsequently refers to Eq. (7) as a derived step (so the reviewer’s note that the paper begins from Eq. (4) is correct). However, the paper does not explicitly describe Eq. (7) as a 'biased approximation' nor provide an explicit clarification of such a biased approximation in the quoted passages; the term 'biased approximation' appears earlier when discussing prior work (Diffusion-TTA) but not as an explanation of Eq. (7) itself. Thus the reviewer’s uncertainty about a biased approximation in Eq. (7) is understandable and only partially addressed by the paper.", "evidence": "Applying Eq. (6) to Eq. (4) at timestep t, we get the following: ... For diffusion models, the conditional score function ∇_{x_t} log p(x_t | y) can be estimated as follows: ... and therefore we can combine Eq. (7) and Eq. (8) to give a structured estimation in the latent space: ...", "section": "3.2 Unlocking the Discriminative Power of Conditional Diffusion Models (and Introduction reference to Diffusion-TTA)"}
{"claim": "Equation (7) appears to introduce a biased denoising estimator without clear justification relative to the derivation that begins at Equation (4).", "claim_type": "methodology", "paper_id": "c7m1HahBNf", "paper_title": "Exploring Structured Semantic Priors Underlying Diffusion Score for Test-time Adaptation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "U1a50zbUPt", "reviewer": "Reviewer_V2Wk", "review_text": "Comment: **Regarding all questions except Question 5**\n\nThank you for addressing my concerns related to the experiments. Your comprehensive work, especially given the short timeframe, has completely resolved my questions.\n\n**Regarding Question 5**\n\nI am still unclear on a particular point. While I understand that Diffusion-TTA relies on a biased approximation due to the optimization of the ELBO, I’m confused about the biased approximation mentioned in Eq (7), even though the current formulation begins from Eq (4). It seems to me that this could be considered a form of circular reasoning since the denoising estimator in Eq (7) is trained using the ELBO. Is my interpretation correct?", "labeling_timestamp": "2026-01-11T16:27:40.269045", "model": "gpt-5-mini", "label": "Supported", "justification": "In Section 3.2 the paper combines intermediate equations (applying Eq. (6) to Eq. (4) and then combining Eq. (7) and Eq. (8)) and then replaces the true posterior p(y | x_t) with the task model prediction p_θ(y | x_0) without deriving or justifying that this substitution yields an unbiased denoising estimator. The text presents the substitution as a practical choice but offers no analysis of bias or formal justification relative to the earlier derivation.", "evidence": "“we can combine Eq. (7) and Eq. (8) to give a structured estimation in the latent space: ... Note that p ( y | x t ) is the implicit prior of a conditional diffusion model at a single timestep t , and thus can be learned by directly plugging the task model prediction on x 0 , dubbed as p_θ ( y | x 0 ) , into Eq. (9).”", "section": "3.2 Unlocking the Discriminative Power of Conditional Diffusion Models"}
{"claim": "Training the denoising estimator in Equation (7) using the ELBO may create circular reasoning, which the manuscript does not explicitly address.", "claim_type": "methodology", "paper_id": "c7m1HahBNf", "paper_title": "Exploring Structured Semantic Priors Underlying Diffusion Score for Test-time Adaptation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "U1a50zbUPt", "reviewer": "Reviewer_V2Wk", "review_text": "Comment: **Regarding all questions except Question 5**\n\nThank you for addressing my concerns related to the experiments. Your comprehensive work, especially given the short timeframe, has completely resolved my questions.\n\n**Regarding Question 5**\n\nI am still unclear on a particular point. While I understand that Diffusion-TTA relies on a biased approximation due to the optimization of the ELBO, I’m confused about the biased approximation mentioned in Eq (7), even though the current formulation begins from Eq (4). It seems to me that this could be considered a form of circular reasoning since the denoising estimator in Eq (7) is trained using the ELBO. Is my interpretation correct?", "labeling_timestamp": "2026-01-11T16:27:44.234062", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The reviewer claim mixes two points. The paper does not discuss or rebut a potential circularity when plugging task-model predictions into the diffusion-based score objective (supporting the reviewer’s concern that this is not explicitly addressed). However, the paper does not state that the denoising estimator in Eq. (7) is trained using the ELBO; it instead refers to the simplified denoising objective / score-matching formulation, so the ELBO premise is not supported by the manuscript.", "evidence": ["Preliminaries (Sec. 2): \"a conditional diffusion model ... is trained to predict the noise added to x_t with condition c, by minimizing the simplified denoising objective:\"", "Sec. 3.2: \"Note that the posteriors { p(y | x) : y ∈ Y } are not directly modeled, and thus can be seen as the implicit priors hidden in the construction of (conditional) score functions.\"", "Sec. 3.2: \"Note that p(y | x_t) is the implicit prior of a conditional diffusion model at a single timestep t, and thus can be learned by directly plugging the task model prediction on x_0, dubbed as p_θ(y | x_0), into Eq. (9).\""], "section": "Preliminaries (Sec. 2); Sec. 3.2 (\"Unlocking the Discriminative Power of Conditional Diffusion Models\")"}
{"claim": "The paper states Diffusion-TTA relies on a biased approximation due to ELBO optimization, raising concerns about estimator consistency and validity that are not resolved.", "claim_type": "methodology", "paper_id": "c7m1HahBNf", "paper_title": "Exploring Structured Semantic Priors Underlying Diffusion Score for Test-time Adaptation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "U1a50zbUPt", "reviewer": "Reviewer_V2Wk", "review_text": "Comment: **Regarding all questions except Question 5**\n\nThank you for addressing my concerns related to the experiments. Your comprehensive work, especially given the short timeframe, has completely resolved my questions.\n\n**Regarding Question 5**\n\nI am still unclear on a particular point. While I understand that Diffusion-TTA relies on a biased approximation due to the optimization of the ELBO, I’m confused about the biased approximation mentioned in Eq (7), even though the current formulation begins from Eq (4). It seems to me that this could be considered a form of circular reasoning since the denoising estimator in Eq (7) is trained using the ELBO. Is my interpretation correct?", "labeling_timestamp": "2026-01-11T16:27:42.934154", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper criticizes Diffusion-TTA for relying on Monte Carlo sampling over many timesteps to estimate a biased approximation of the likelihood and for being computationally inefficient, but it does not state that this bias arises from ELBO optimization nor does it raise unresolved concerns about estimator consistency or validity. The paper's critique is about Monte Carlo timestep-based likelihood estimation and efficiency, not ELBO-related estimator consistency.", "evidence": "From Introduction: \"Diffusion-TTA is heavily reliant on the Monte Carlo method over as many as 180 timesteps to estimate a biased approximation of likelihood [32, 38], resulting in high computational complexity proportional to sampled timesteps.\"; From Sec. 3.1: \"The loss is averaged over hundreds of (t, ε) pairs to remedy the performance degradation caused by incorrect class prediction using a single timestep [32], at the cost of reduced efficiency.\"", "section": "Introduction; 3.1 A Brief Review of Diffusion-TTA"}
{"claim": "The reviewer asks whether interpreting Equation (7) as circular reasoning, because its denoising estimator is trained with the ELBO, is correct.", "claim_type": "methodology", "paper_id": "c7m1HahBNf", "paper_title": "Exploring Structured Semantic Priors Underlying Diffusion Score for Test-time Adaptation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "U1a50zbUPt", "reviewer": "Reviewer_V2Wk", "review_text": "Comment: **Regarding all questions except Question 5**\n\nThank you for addressing my concerns related to the experiments. Your comprehensive work, especially given the short timeframe, has completely resolved my questions.\n\n**Regarding Question 5**\n\nI am still unclear on a particular point. While I understand that Diffusion-TTA relies on a biased approximation due to the optimization of the ELBO, I’m confused about the biased approximation mentioned in Eq (7), even though the current formulation begins from Eq (4). It seems to me that this could be considered a form of circular reasoning since the denoising estimator in Eq (7) is trained using the ELBO. Is my interpretation correct?", "labeling_timestamp": "2026-01-11T16:27:47.340939", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does not discuss Equation (7) as circular reasoning nor state that the denoising estimator was trained with the ELBO. It describes the diffusion model being trained with a simplified denoising objective (MSE) and then uses the trained noise predictor to relate conditional score functions to task-model predictions, but it does not address or rule on a potential circularity stemming from ELBO-based training.", "evidence": "In the reverse process, a conditional diffusion model ε_φ(x_t, t, c) is trained to predict the noise added to x_t with condition c, by minimizing the simplified denoising objective: ... For diffusion models, the conditional score function ∇_{x_t} log p(x_t | y) can be estimated as follows: ... and therefore we can combine Eq. (7) and Eq. (8) to give a structured estimation in the latent space: ... Note that p(y | x_t) is the implicit prior of a conditional diffusion model at a single timestep t, and thus can be learned by directly plugging the task model prediction on x_0, dubbed as p_θ(y | x_0), into Eq. (9).", "section": "Sections 2 (Preliminaries) and 3.2 (Unlocking the Discriminative Power of Conditional Diffusion Models)"}
{"claim": "The paper does not report results from gpt-3.5-turbo-16k-0613 on the example without prompt engineering, few-shot learning, or chain-of-thought.", "claim_type": "experimental", "paper_id": "UkxJd64mki", "paper_title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "fGEvjzIyly", "reviewer": "Reviewer_D2vF", "review_text": "Comment: Thanks again for your thorough explanations. Can authors let me know what they expect to get from \"gpt-3.5-turbo-16k-0613\" without using any prompt engineering, few shot learning, and CoT if they pose the following question directly to the prompt?\n\"Suppose that I have 6 different books, 2 of which are math books. In how many ways can I stack my 6 books on a shelf if I want a math book on both ends of the stack?\" My observation is that for this particular example, even a simple prompt without using more advanced approaches such as CoT or StrategyML should be able to solve the problem. Therefore, I suspect something in the CoT implementation is not working as intended and attributing a bug/issue in CoT implementation to its inferior performance compared with StrategyML is not advisable.", "labeling_timestamp": "2026-01-11T16:28:04.915266", "model": "gpt-5-mini", "label": "Supported", "justification": "The provided paper text mentions using 'the latest version of GPT-3.5-Turbo' for prompt generation and presents experimental evaluations, but it does not mention or report results from the specific model 'gpt-3.5-turbo-16k-0613' nor any explicit baseline evaluation of that model in a zero-shot / no-prompt-engineering / no-CoT setting in the given sections.", "evidence": "1) \"StrategyLLM expends less than $0.24 to develop a strategy-based prompt for a variety of tasks using the latest version of GPT-3.5-Turbo.\" 2) \"We conduct comprehensive evaluations of StrategyLLM on 13 datasets across 4 challenging tasks...\"", "section": "Introduction; 4.1 Experimental Setup"}
{"claim": "A simple direct prompt without advanced techniques like CoT or StrategyML should be able to solve the given combinatorics example.", "claim_type": "methodology", "paper_id": "UkxJd64mki", "paper_title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "fGEvjzIyly", "reviewer": "Reviewer_D2vF", "review_text": "Comment: Thanks again for your thorough explanations. Can authors let me know what they expect to get from \"gpt-3.5-turbo-16k-0613\" without using any prompt engineering, few shot learning, and CoT if they pose the following question directly to the prompt?\n\"Suppose that I have 6 different books, 2 of which are math books. In how many ways can I stack my 6 books on a shelf if I want a math book on both ends of the stack?\" My observation is that for this particular example, even a simple prompt without using more advanced approaches such as CoT or StrategyML should be able to solve the problem. Therefore, I suspect something in the CoT implementation is not working as intended and attributing a bug/issue in CoT implementation to its inferior performance compared with StrategyML is not advisable.", "labeling_timestamp": "2026-01-11T16:28:05.392868", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper argues that instance-specific or simple direct few-shot prompts lack generalizability and consistency for problem solving and presents StrategyLLM as an approach that outperforms such baselines (including on math reasoning). Thus the claim that a simple direct prompt without CoT/StrategyML should solve the combinatorics example contradicts the paper's stated findings.", "evidence": "Most existing prompting methods suffer from the issues of generalizability and consistency, as they often rely on instance-specific solutions that may not be applicable to other instances and lack task-level consistency across the selected few-shot examples.", "section": "Abstract"}
{"claim": "Something in the chain-of-thought (CoT) implementation appears to be malfunctioning or not working as intended for the provided example.", "claim_type": "methodology", "paper_id": "UkxJd64mki", "paper_title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "fGEvjzIyly", "reviewer": "Reviewer_D2vF", "review_text": "Comment: Thanks again for your thorough explanations. Can authors let me know what they expect to get from \"gpt-3.5-turbo-16k-0613\" without using any prompt engineering, few shot learning, and CoT if they pose the following question directly to the prompt?\n\"Suppose that I have 6 different books, 2 of which are math books. In how many ways can I stack my 6 books on a shelf if I want a math book on both ends of the stack?\" My observation is that for this particular example, even a simple prompt without using more advanced approaches such as CoT or StrategyML should be able to solve the problem. Therefore, I suspect something in the CoT implementation is not working as intended and attributing a bug/issue in CoT implementation to its inferior performance compared with StrategyML is not advisable.", "labeling_timestamp": "2026-01-11T16:27:56.428258", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper discusses chain-of-thought (CoT) prompting and its limitations (generalizability and consistency) but does not mention any malfunctioning CoT implementation or errors in a provided example; there is insufficient information in the paper to confirm the reviewer's claim.", "evidence": "\"chain-of-thought (CoT) prompting methods [43, 6, 12, 41], which condition LLMs on a few task examples with step-by-step solutions, guide LLMs to break down complex reasoning processes into simpler steps.\" \"Despite their potential, current CoT approaches employing few-shot prompts with instance-specific solutions may face challenges in terms of generalizability and consistency .\"", "section": "1 Introduction"}
{"claim": "Attributing CoT's inferior performance relative to StrategyML to a bug in the CoT implementation is not justified without further investigation.", "claim_type": "subjective", "paper_id": "UkxJd64mki", "paper_title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "fGEvjzIyly", "reviewer": "Reviewer_D2vF", "review_text": "Comment: Thanks again for your thorough explanations. Can authors let me know what they expect to get from \"gpt-3.5-turbo-16k-0613\" without using any prompt engineering, few shot learning, and CoT if they pose the following question directly to the prompt?\n\"Suppose that I have 6 different books, 2 of which are math books. In how many ways can I stack my 6 books on a shelf if I want a math book on both ends of the stack?\" My observation is that for this particular example, even a simple prompt without using more advanced approaches such as CoT or StrategyML should be able to solve the problem. Therefore, I suspect something in the CoT implementation is not working as intended and attributing a bug/issue in CoT implementation to its inferior performance compared with StrategyML is not advisable.", "labeling_timestamp": "2026-01-11T16:28:07.501443", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper attributes CoT's weaker performance to issues of generalizability and consistency and presents StrategyLLM as an alternative; it does not claim or investigate that a software/implementation bug causes CoT's inferior performance, so blaming a bug is not justified by the paper.", "evidence": "Abstract: \"StrategyLLM outperforms the competitive baseline CoT-SC that requires human-annotated solutions on 13 datasets...\"; Introduction: \"Despite their potential, current CoT approaches employing few-shot prompts with instance-specific solutions may face challenges in terms of generalizability and consistency.\"", "section": "Abstract, Introduction"}
{"claim": "The comparison between CoT and StrategyML may be invalid because potential CoT implementation issues could explain observed performance differences.", "claim_type": "baseline", "paper_id": "UkxJd64mki", "paper_title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "fGEvjzIyly", "reviewer": "Reviewer_D2vF", "review_text": "Comment: Thanks again for your thorough explanations. Can authors let me know what they expect to get from \"gpt-3.5-turbo-16k-0613\" without using any prompt engineering, few shot learning, and CoT if they pose the following question directly to the prompt?\n\"Suppose that I have 6 different books, 2 of which are math books. In how many ways can I stack my 6 books on a shelf if I want a math book on both ends of the stack?\" My observation is that for this particular example, even a simple prompt without using more advanced approaches such as CoT or StrategyML should be able to solve the problem. Therefore, I suspect something in the CoT implementation is not working as intended and attributing a bug/issue in CoT implementation to its inferior performance compared with StrategyML is not advisable.", "labeling_timestamp": "2026-01-11T16:28:03.524920", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper reports that StrategyLLM outperforms a CoT-SC baseline and gives numeric results, but it does not discuss CoT implementation details or potential implementation issues that could confound the comparison. Therefore the paper neither confirms nor refutes the reviewer's concern.", "evidence": "Experimental results demonstrate that StrategyLLM outperforms the competitive baseline CoT-SC that requires human-annotated solutions on 13 datasets across 4 challenging tasks without human involvement, including math reasoning (34.2% → 38.8%), commonsense reasoning (70.3% → 72.5%), algorithmic reasoning (73.7% → 85.0%), and symbolic reasoning (30.0% → 79.2%).", "section": "Abstract"}
{"claim": "The paper lacks clarity regarding the expected outputs from the baseline gpt-3.5-turbo-16k-0613 model when given the example without any prompt engineering.", "claim_type": "baseline", "paper_id": "UkxJd64mki", "paper_title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "fGEvjzIyly", "reviewer": "Reviewer_D2vF", "review_text": "Comment: Thanks again for your thorough explanations. Can authors let me know what they expect to get from \"gpt-3.5-turbo-16k-0613\" without using any prompt engineering, few shot learning, and CoT if they pose the following question directly to the prompt?\n\"Suppose that I have 6 different books, 2 of which are math books. In how many ways can I stack my 6 books on a shelf if I want a math book on both ends of the stack?\" My observation is that for this particular example, even a simple prompt without using more advanced approaches such as CoT or StrategyML should be able to solve the problem. Therefore, I suspect something in the CoT implementation is not working as intended and attributing a bug/issue in CoT implementation to its inferior performance compared with StrategyML is not advisable.", "labeling_timestamp": "2026-01-11T16:28:24.716398", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper references using GPT-3.5-Turbo for cost estimates and defines a generic model output format, but it does not specify the expected outputs from a baseline gpt-3.5-turbo-16k-0613 when given the example without any prompt engineering. There is no section describing baseline behavior or expected raw outputs for that model/version.", "evidence": "“StrategyLLM expends less than $0.24 to develop a strategy-based prompt for a variety of tasks using the latest version of GPT-3.5-Turbo.”\n\n“During inference, given a question q, the language model M : ( p, q ) → ( so, a ) generates a solution so and an answer a for it conditioned on the prompt p.”", "section": "Introduction; Notations (Section 2)"}
{"claim": "The paper reports results only at 20K training steps and lacks experiments exploring stability or performance when training increases to 60K or more.", "claim_type": "experimental", "paper_id": "tQukGCDaNT", "paper_title": "Improved Distribution Matching Distillation for Fast Image Synthesis", "paper_venue": "neurips2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "K5GmPvDd81", "reviewer": "Reviewer_aF5v", "review_text": "Comment: Happy to raise my score.\n\nOne thing i want to understand from authors, is how stable is this formulation when number of steps go from 20K to say 60K or more? Do you see performance peak around 20K or further training improves quality? unlike original DMD as authors report only 20K was curious if there were any interesting empirical findings which would provide further insights to community.\n\nThis goes back to fake score function alignment, does it help to reinitialize fake score function with teacher model again as student converges well to teacher? As it seems a bit unclear on properties/fit of fake score function and how it effects overall training stability.\n\nLooking forward for more results in later version of paper.", "labeling_timestamp": "2026-01-11T16:28:20.130119", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper content reports final generation results (FID scores) and discusses training methodology and compute, but does not mention training-step counts (e.g., 20K) nor any experiments increasing training to 60K or more. Therefore, based on the supplied text, we cannot confirm the reviewer's claim.", "evidence": "Abstract: \"Taken together, our improvements set new benchmarks in onestep image generation, with FID scores of 1.28 on ImageNet-64×64 and 8.35 on zero-shot COCO 2014...\"", "section": "Abstract"}
{"claim": "It is unclear whether model performance peaks around 20K training steps or continues to improve with further training beyond that point.", "claim_type": "experimental", "paper_id": "tQukGCDaNT", "paper_title": "Improved Distribution Matching Distillation for Fast Image Synthesis", "paper_venue": "neurips2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "K5GmPvDd81", "reviewer": "Reviewer_aF5v", "review_text": "Comment: Happy to raise my score.\n\nOne thing i want to understand from authors, is how stable is this formulation when number of steps go from 20K to say 60K or more? Do you see performance peak around 20K or further training improves quality? unlike original DMD as authors report only 20K was curious if there were any interesting empirical findings which would provide further insights to community.\n\nThis goes back to fake score function alignment, does it help to reinitialize fake score function with teacher model again as student converges well to teacher? As it seems a bit unclear on properties/fit of fake score function and how it effects overall training stability.\n\nLooking forward for more results in later version of paper.", "labeling_timestamp": "2026-01-11T16:28:22.016288", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not report training-step counts, learning curves, or any statement about performance peaking at ~20K steps or continuing to improve afterward. The supplied sections describe methodology, contributions, and dataset construction costs but do not address whether performance saturates at a specific number of training steps.", "evidence": "1) \"We propose a new distribution matching distillation technique that does not require a regression loss for stable training, thereby eliminating the need for costly data collection, and allowing for more flexible and scalable training.\" 2) \"generating one noise-image pair for SDXL [58] takes around 5 seconds, amounting to about 700 A100 days to cover the 12 million prompts in the LAION 6.0 dataset [59], as utilized by Yin et al. [22].\"", "section": "1 Introduction; 3 Background: Diffusion and Distribution Matching Distillation"}
{"claim": "Unlike the original DMD work, the authors present only 20K-step results, preventing meaningful comparison across longer training schedules.", "claim_type": "experimental", "paper_id": "tQukGCDaNT", "paper_title": "Improved Distribution Matching Distillation for Fast Image Synthesis", "paper_venue": "neurips2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "K5GmPvDd81", "reviewer": "Reviewer_aF5v", "review_text": "Comment: Happy to raise my score.\n\nOne thing i want to understand from authors, is how stable is this formulation when number of steps go from 20K to say 60K or more? Do you see performance peak around 20K or further training improves quality? unlike original DMD as authors report only 20K was curious if there were any interesting empirical findings which would provide further insights to community.\n\nThis goes back to fake score function alignment, does it help to reinitialize fake score function with teacher model again as student converges well to teacher? As it seems a bit unclear on properties/fit of fake score function and how it effects overall training stability.\n\nLooking forward for more results in later version of paper.", "labeling_timestamp": "2026-01-11T16:28:29.931911", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt reports final metrics and claims state-of-the-art results but does not specify training schedule lengths or that they only present 20K-step results. Thus there is insufficient information to verify the reviewer’s assertion about '20K-step results.'", "evidence": "\"Taken together, our improvements set new benchmarks in onestep image generation, with FID scores of 1.28 on ImageNet-64×64 and 8.35 on zero-shot COCO 2014, surpassing the original teacher despite a 500 × reduction in inference cost.\"", "section": "Abstract"}
{"claim": "The paper does not evaluate whether reinitializing the fake score function from the teacher model helps training as the student converges.", "claim_type": "experimental", "paper_id": "tQukGCDaNT", "paper_title": "Improved Distribution Matching Distillation for Fast Image Synthesis", "paper_venue": "neurips2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "K5GmPvDd81", "reviewer": "Reviewer_aF5v", "review_text": "Comment: Happy to raise my score.\n\nOne thing i want to understand from authors, is how stable is this formulation when number of steps go from 20K to say 60K or more? Do you see performance peak around 20K or further training improves quality? unlike original DMD as authors report only 20K was curious if there were any interesting empirical findings which would provide further insights to community.\n\nThis goes back to fake score function alignment, does it help to reinitialize fake score function with teacher model again as student converges well to teacher? As it seems a bit unclear on properties/fit of fake score function and how it effects overall training stability.\n\nLooking forward for more results in later version of paper.", "labeling_timestamp": "2026-01-11T16:28:26.977370", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes addressing instability by dynamically training a 'fake' score (µ_fake) and using a two time-scale update rule, but contains no evaluation or mention of reinitializing the fake score from the teacher as the student converges.", "evidence": "“We show that the resulting instability is due to the 'fake' critic not estimating the distribution of generated samples with sufficient accuracy and propose a two time-scale update rule as a remedy.”\n\n“DMD uses a frozen pre-trained diffusion model as µ_real (the teacher), and dynamically updates µ_fake while training G, using a denoising score-matching loss on samples from the one-step generator, i.e., fake data [22, 47].”", "section": "Abstract; Section 3 (Background: Diffusion and Distribution Matching Distillation)"}
{"claim": "The properties, fit, and behavior of the fake score function are not characterized, and its effect on overall training stability is unclear.", "claim_type": "methodology", "paper_id": "tQukGCDaNT", "paper_title": "Improved Distribution Matching Distillation for Fast Image Synthesis", "paper_venue": "neurips2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "K5GmPvDd81", "reviewer": "Reviewer_aF5v", "review_text": "Comment: Happy to raise my score.\n\nOne thing i want to understand from authors, is how stable is this formulation when number of steps go from 20K to say 60K or more? Do you see performance peak around 20K or further training improves quality? unlike original DMD as authors report only 20K was curious if there were any interesting empirical findings which would provide further insights to community.\n\nThis goes back to fake score function alignment, does it help to reinitialize fake score function with teacher model again as student converges well to teacher? As it seems a bit unclear on properties/fit of fake score function and how it effects overall training stability.\n\nLooking forward for more results in later version of paper.", "labeling_timestamp": "2026-01-11T16:28:37.141136", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly identifies that instability without the regression loss is caused by the insufficiently trained 'fake' score/critic and proposes a concrete remedy (a two time-scale update rule), i.e., it characterizes the fake score's role in training stability.", "evidence": "\"We show that the resulting instability is due to the 'fake' critic not estimating the distribution of generated samples with sufficient accuracy and propose a two time-scale update rule as a remedy.\"; \"We show that training instability in DMD [22] without regression loss stems from an insufficiently trained fake diffusion critic , and implement a two time-scale update rule to address this issue.\"", "section": "Abstract; 1 Introduction (Contributions)"}
{"claim": "The combination of EGNN and ESM2 limits the novelty of the method.", "claim_type": "novelty", "paper_id": "Dr4qD9bzZd", "paper_title": "Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "a9hHYfEm0G", "reviewer": "Reviewer_Wq6M", "review_text": "Summary: This paper proposes NAEPro, a model to jointly design Protein sequence and structure. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from the nearest amino acids in three-dimensional (3D) space. The global attention sub-layer parameters are initialized with ESM-2. The author combines ESM2 and EGNN for co-modeling protein sequence and structure.\n\nStrengths: 1. The reported performance is good.\n2. The method is simple.\n\nWeaknesses: 1. Novelty: Both EGNN and ESM2 are existing models, and there are many existing works on antibody structure and sequence co-design. The combination may limit the novelty of this method.  \n2. Significance: It is likely that previous works can be readily applied to the motif-conditioned setting, and the authors can easily adapt their method to antibody design tasks. It would be beneficial if the authors further elaborate on the significance of their work\n3. Code: The authors do not provide code for checking the soundness of the methods.\n4. Experiment setting: The authors do not provide results on standard benchmarks, such as the CATH dataset, for fair comparison on both sequence and structure design.\n\nQuestions: 1. Could you provide experimental results from the CATH dataset to compare with the original SMCDiff and FrameDiff results?\n2. Similarly, could you provide head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design? Please follow the same setting.\n3. Could you provide the code for checking the results?", "labeling_timestamp": "2026-01-11T16:28:50.390394", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper describes a novel Neighborhood Attentive Equivariant Layer (NAEL) architecture combining a Transformer-based global attention sub-layer and a neighborhood equivariant sub-layer designed by the authors. It does not state that the method is merely a combination of EGNN and ESM2, nor does it reference using ESM2; thus the reviewer's claim is contradicted by the paper.", "evidence": "“NAEPro is an interleaving network consisting of stacked neighborhood attentive equivariant layers (NAELs). Each NAEL is composed of two integral components: a global attention sub-layer and a neighborhood equivariant sub-layer.”\n\n“We adopt the Transformer layer (Vaswani et al., 2017) to compute global contextual embeddings.”", "section": "3 PROPOSED METHOD: NAEPRO / 3.1 OVERALL MODEL ARCHITECTURE"}
{"claim": "Many existing works on antibody structure and sequence co-design reduce the novelty of this approach.", "claim_type": "novelty", "paper_id": "Dr4qD9bzZd", "paper_title": "Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "a9hHYfEm0G", "reviewer": "Reviewer_Wq6M", "review_text": "Summary: This paper proposes NAEPro, a model to jointly design Protein sequence and structure. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from the nearest amino acids in three-dimensional (3D) space. The global attention sub-layer parameters are initialized with ESM-2. The author combines ESM2 and EGNN for co-modeling protein sequence and structure.\n\nStrengths: 1. The reported performance is good.\n2. The method is simple.\n\nWeaknesses: 1. Novelty: Both EGNN and ESM2 are existing models, and there are many existing works on antibody structure and sequence co-design. The combination may limit the novelty of this method.  \n2. Significance: It is likely that previous works can be readily applied to the motif-conditioned setting, and the authors can easily adapt their method to antibody design tasks. It would be beneficial if the authors further elaborate on the significance of their work\n3. Code: The authors do not provide code for checking the soundness of the methods.\n4. Experiment setting: The authors do not provide results on standard benchmarks, such as the CATH dataset, for fair comparison on both sequence and structure design.\n\nQuestions: 1. Could you provide experimental results from the CATH dataset to compare with the original SMCDiff and FrameDiff results?\n2. Similarly, could you provide head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design? Please follow the same setting.\n3. Could you provide the code for checking the results?", "labeling_timestamp": "2026-01-11T16:28:45.232754", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper discusses prior sequence-and-structure co-design methods (e.g., Anand & Achim 2022; Shi et al. 2022) in Related Work but does not mention antibody-specific sequence/structure co-design literature. Therefore the paper does not provide information to confirm the reviewer's claim about 'many existing works on antibody structure and sequence co-design' reducing novelty.", "evidence": "Anand & Achim (2022) first propose to co-design protein sequence and structure conditioning on given secondary structures (SS). Following their work, Shi et al. (2022) propose to realize general protein design conditioning on SS and binary contact map. However, designing protein relying on its topology cannot guarantee the designed proteins have the desired functions.", "section": "2 RELATED WORK"}
{"claim": "Previous works can be readily applied to the motif-conditioned setting, which calls into question this paper's significance.", "claim_type": "novelty", "paper_id": "Dr4qD9bzZd", "paper_title": "Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "a9hHYfEm0G", "reviewer": "Reviewer_Wq6M", "review_text": "Summary: This paper proposes NAEPro, a model to jointly design Protein sequence and structure. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from the nearest amino acids in three-dimensional (3D) space. The global attention sub-layer parameters are initialized with ESM-2. The author combines ESM2 and EGNN for co-modeling protein sequence and structure.\n\nStrengths: 1. The reported performance is good.\n2. The method is simple.\n\nWeaknesses: 1. Novelty: Both EGNN and ESM2 are existing models, and there are many existing works on antibody structure and sequence co-design. The combination may limit the novelty of this method.  \n2. Significance: It is likely that previous works can be readily applied to the motif-conditioned setting, and the authors can easily adapt their method to antibody design tasks. It would be beneficial if the authors further elaborate on the significance of their work\n3. Code: The authors do not provide code for checking the soundness of the methods.\n4. Experiment setting: The authors do not provide results on standard benchmarks, such as the CATH dataset, for fair comparison on both sequence and structure design.\n\nQuestions: 1. Could you provide experimental results from the CATH dataset to compare with the original SMCDiff and FrameDiff results?\n2. Similarly, could you provide head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design? Please follow the same setting.\n3. Could you provide the code for checking the results?", "labeling_timestamp": "2026-01-11T16:28:52.030986", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly argues prior methods are not directly applicable to motif-conditioned design—citing that some require pre-specified motifs or prespecified secondary structures, and thus motivate automatic motif-guided co-design. This contradicts the reviewer's claim that previous works can be readily applied to the motif-conditioned setting.", "evidence": "Intro: \"Wang et al. (2022) provide a solution to fill in additional sequence and structure given functional motifs. However, this approach assumes that biologists already possess knowledge of the motifs associated with the target proteins, thereby potentially restricting the model's applicability.\" Related Work: \"Wang et al. (2022) propose Inpainting to reconstruct both missing protein sequences and structures using provided motifs. However, it's essential to note that this approach necessitates the prior specification of motifs for the target proteins, which requires domain-specific expertise.\"", "section": "Introduction; Related Work"}
{"claim": "The authors can easily adapt their method to antibody design tasks, reducing the work's perceived significance.", "claim_type": "novelty", "paper_id": "Dr4qD9bzZd", "paper_title": "Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "a9hHYfEm0G", "reviewer": "Reviewer_Wq6M", "review_text": "Summary: This paper proposes NAEPro, a model to jointly design Protein sequence and structure. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from the nearest amino acids in three-dimensional (3D) space. The global attention sub-layer parameters are initialized with ESM-2. The author combines ESM2 and EGNN for co-modeling protein sequence and structure.\n\nStrengths: 1. The reported performance is good.\n2. The method is simple.\n\nWeaknesses: 1. Novelty: Both EGNN and ESM2 are existing models, and there are many existing works on antibody structure and sequence co-design. The combination may limit the novelty of this method.  \n2. Significance: It is likely that previous works can be readily applied to the motif-conditioned setting, and the authors can easily adapt their method to antibody design tasks. It would be beneficial if the authors further elaborate on the significance of their work\n3. Code: The authors do not provide code for checking the soundness of the methods.\n4. Experiment setting: The authors do not provide results on standard benchmarks, such as the CATH dataset, for fair comparison on both sequence and structure design.\n\nQuestions: 1. Could you provide experimental results from the CATH dataset to compare with the original SMCDiff and FrameDiff results?\n2. Similarly, could you provide head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design? Please follow the same setting.\n3. Could you provide the code for checking the results?", "labeling_timestamp": "2026-01-11T16:29:24.266208", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does not discuss antibody design or claim that NAEPro can be easily adapted to antibody-design tasks. It evaluates on metalloproteins (β-lactamase and myoglobin) and describes designing around generic functional fragments, but provides no evidence about applicability or ease of adaptation to antibodies.", "evidence": "Abstract: \"We evaluate our model and several strong baselines on two protein datasets, β -lactamase and myoglobin.\"; 3 PROPOSED METHOD: \"Meaningful fragments here represent protein functional and conserved sites, in which functional sites vary from setting to setting. For example, they are active sites for de novo enzyme design (Richter et al., 2011), ... while they are binding sites for de novo binder design (Gainza et al., 2023).\"", "section": "Abstract; 3 PROPOSED METHOD"}
{"claim": "The authors do not provide code for checking the soundness or reproducing the methods.", "claim_type": "methodology", "paper_id": "Dr4qD9bzZd", "paper_title": "Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "a9hHYfEm0G", "reviewer": "Reviewer_Wq6M", "review_text": "Summary: This paper proposes NAEPro, a model to jointly design Protein sequence and structure. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from the nearest amino acids in three-dimensional (3D) space. The global attention sub-layer parameters are initialized with ESM-2. The author combines ESM2 and EGNN for co-modeling protein sequence and structure.\n\nStrengths: 1. The reported performance is good.\n2. The method is simple.\n\nWeaknesses: 1. Novelty: Both EGNN and ESM2 are existing models, and there are many existing works on antibody structure and sequence co-design. The combination may limit the novelty of this method.  \n2. Significance: It is likely that previous works can be readily applied to the motif-conditioned setting, and the authors can easily adapt their method to antibody design tasks. It would be beneficial if the authors further elaborate on the significance of their work\n3. Code: The authors do not provide code for checking the soundness of the methods.\n4. Experiment setting: The authors do not provide results on standard benchmarks, such as the CATH dataset, for fair comparison on both sequence and structure design.\n\nQuestions: 1. Could you provide experimental results from the CATH dataset to compare with the original SMCDiff and FrameDiff results?\n2. Similarly, could you provide head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design? Please follow the same setting.\n3. Could you provide the code for checking the results?", "labeling_timestamp": "2026-01-11T16:28:59.626068", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states that the authors will release their code, datasets, and models, which contradicts the claim that they do not provide code for reproducing the methods.", "evidence": "We will release our datasets, code and models.", "section": "Abstract / Contributions"}
{"claim": "The paper does not report results on standard benchmarks such as the CATH dataset for fair comparison on sequence and structure design.", "claim_type": "experimental", "paper_id": "Dr4qD9bzZd", "paper_title": "Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "a9hHYfEm0G", "reviewer": "Reviewer_Wq6M", "review_text": "Summary: This paper proposes NAEPro, a model to jointly design Protein sequence and structure. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from the nearest amino acids in three-dimensional (3D) space. The global attention sub-layer parameters are initialized with ESM-2. The author combines ESM2 and EGNN for co-modeling protein sequence and structure.\n\nStrengths: 1. The reported performance is good.\n2. The method is simple.\n\nWeaknesses: 1. Novelty: Both EGNN and ESM2 are existing models, and there are many existing works on antibody structure and sequence co-design. The combination may limit the novelty of this method.  \n2. Significance: It is likely that previous works can be readily applied to the motif-conditioned setting, and the authors can easily adapt their method to antibody design tasks. It would be beneficial if the authors further elaborate on the significance of their work\n3. Code: The authors do not provide code for checking the soundness of the methods.\n4. Experiment setting: The authors do not provide results on standard benchmarks, such as the CATH dataset, for fair comparison on both sequence and structure design.\n\nQuestions: 1. Could you provide experimental results from the CATH dataset to compare with the original SMCDiff and FrameDiff results?\n2. Similarly, could you provide head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design? Please follow the same setting.\n3. Could you provide the code for checking the results?", "labeling_timestamp": "2026-01-11T16:29:05.017289", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper's experiments are reported only on two specific datasets (β-lactamase and myoglobin); it does not mention evaluation on standard benchmarks such as the CATH dataset.", "evidence": "Abstract: \"We evaluate our model and several strong baselines on two protein datasets, β -lactamase and myoglobin.\" Introduction / Contributions: \"We carry out experiments on two metalloproteins, including β -lactamase and myoglobin.\"", "section": "Abstract; Introduction / Contributions"}
{"claim": "No experimental results are provided on the CATH dataset to compare with SMCDiff and FrameDiff.", "claim_type": "experimental", "paper_id": "Dr4qD9bzZd", "paper_title": "Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "a9hHYfEm0G", "reviewer": "Reviewer_Wq6M", "review_text": "Summary: This paper proposes NAEPro, a model to jointly design Protein sequence and structure. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from the nearest amino acids in three-dimensional (3D) space. The global attention sub-layer parameters are initialized with ESM-2. The author combines ESM2 and EGNN for co-modeling protein sequence and structure.\n\nStrengths: 1. The reported performance is good.\n2. The method is simple.\n\nWeaknesses: 1. Novelty: Both EGNN and ESM2 are existing models, and there are many existing works on antibody structure and sequence co-design. The combination may limit the novelty of this method.  \n2. Significance: It is likely that previous works can be readily applied to the motif-conditioned setting, and the authors can easily adapt their method to antibody design tasks. It would be beneficial if the authors further elaborate on the significance of their work\n3. Code: The authors do not provide code for checking the soundness of the methods.\n4. Experiment setting: The authors do not provide results on standard benchmarks, such as the CATH dataset, for fair comparison on both sequence and structure design.\n\nQuestions: 1. Could you provide experimental results from the CATH dataset to compare with the original SMCDiff and FrameDiff results?\n2. Similarly, could you provide head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design? Please follow the same setting.\n3. Could you provide the code for checking the results?", "labeling_timestamp": "2026-01-11T16:29:15.301521", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper states experiments were run on only two datasets (β-lactamase and myoglobin) and does not report results on the CATH dataset or comparisons to SMCDiff or FrameDiff.", "evidence": "“We evaluate our model and several strong baselines on two protein datasets, β -lactamase and myoglobin.”", "section": "ABSTRACT"}
{"claim": "The paper lacks head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design under the same evaluation setting.", "claim_type": "baseline", "paper_id": "Dr4qD9bzZd", "paper_title": "Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "a9hHYfEm0G", "reviewer": "Reviewer_Wq6M", "review_text": "Summary: This paper proposes NAEPro, a model to jointly design Protein sequence and structure. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from the nearest amino acids in three-dimensional (3D) space. The global attention sub-layer parameters are initialized with ESM-2. The author combines ESM2 and EGNN for co-modeling protein sequence and structure.\n\nStrengths: 1. The reported performance is good.\n2. The method is simple.\n\nWeaknesses: 1. Novelty: Both EGNN and ESM2 are existing models, and there are many existing works on antibody structure and sequence co-design. The combination may limit the novelty of this method.  \n2. Significance: It is likely that previous works can be readily applied to the motif-conditioned setting, and the authors can easily adapt their method to antibody design tasks. It would be beneficial if the authors further elaborate on the significance of their work\n3. Code: The authors do not provide code for checking the soundness of the methods.\n4. Experiment setting: The authors do not provide results on standard benchmarks, such as the CATH dataset, for fair comparison on both sequence and structure design.\n\nQuestions: 1. Could you provide experimental results from the CATH dataset to compare with the original SMCDiff and FrameDiff results?\n2. Similarly, could you provide head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design? Please follow the same setting.\n3. Could you provide the code for checking the results?", "labeling_timestamp": "2026-01-11T16:29:47.864649", "model": "gpt-5-mini", "label": "Supported", "justification": "The provided paper text states it evaluates against “several strong baselines” but does not list or present any head-to-head experimental comparisons with ProteinMPNN, ESMIF, or PiFold. ProteinMPNN is only mentioned in related work/intro, not as an evaluated baseline in the shown experimental description or results excerpts.", "evidence": "\"We evaluate our model and several strong baselines on two protein datasets, β -lactamase and myoglobin.\" (Abstract)\n\n\"followed by the utilization of existing inverse folding models (Ingraham et al., 2019; Jing et al., 2020) like ProteinMPNN (Dauparas et al., 2022) to determine sequences that can fold into the specified structure.\" (1 INTRODUCTION)\n\n\"Our model is at least 17 x faster than all the representative baselines. We will release our datasets, code and models.\" (1 INTRODUCTION, contributions)", "section": "Abstract; 1 INTRODUCTION"}
{"claim": "The paper does not sufficiently elaborate on the significance of the work.", "claim_type": "subjective", "paper_id": "Dr4qD9bzZd", "paper_title": "Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "a9hHYfEm0G", "reviewer": "Reviewer_Wq6M", "review_text": "Summary: This paper proposes NAEPro, a model to jointly design Protein sequence and structure. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from the nearest amino acids in three-dimensional (3D) space. The global attention sub-layer parameters are initialized with ESM-2. The author combines ESM2 and EGNN for co-modeling protein sequence and structure.\n\nStrengths: 1. The reported performance is good.\n2. The method is simple.\n\nWeaknesses: 1. Novelty: Both EGNN and ESM2 are existing models, and there are many existing works on antibody structure and sequence co-design. The combination may limit the novelty of this method.  \n2. Significance: It is likely that previous works can be readily applied to the motif-conditioned setting, and the authors can easily adapt their method to antibody design tasks. It would be beneficial if the authors further elaborate on the significance of their work\n3. Code: The authors do not provide code for checking the soundness of the methods.\n4. Experiment setting: The authors do not provide results on standard benchmarks, such as the CATH dataset, for fair comparison on both sequence and structure design.\n\nQuestions: 1. Could you provide experimental results from the CATH dataset to compare with the original SMCDiff and FrameDiff results?\n2. Similarly, could you provide head-to-head comparisons to ProteinMPNN, ESMIF, and PiFold on protein sequence design? Please follow the same setting.\n3. Could you provide the code for checking the results?", "labeling_timestamp": "2026-01-11T16:29:20.454705", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly motivates the problem, describes the proposed approach, and lists concrete contributions and empirical findings that argue for the work's significance (see Abstract and Introduction). Thus the claim that the paper does not sufficiently elaborate significance is contradicted by the text.", "evidence": "\"The contribution of this paper are listed as follows:\n\n- We propose NAEPro to jointly design protein sequences and backbone structures. This model is powered by the innovative neighborhood attentive equivariant layers (NAELs).\n- Experiments show that NAEPro achieves highest binding affinity in all cases. Additionally, among the randomly selected 20 cases from top-100 candidates, 100% and 90% of them are highly potential to bind the corresponding metallocofactos, respectively for myoglobin and β -lactamase. NAEPro can even generate myoglobin sequences with amino acid identity rate as low as 66 . 0% compared to the closest matches in Uniprot, yet they bear a remarkable resemblance to natural protein structures, with a RMSD of 0 . 458 Å.\n- Our model is at least 17 x faster than all the representative baselines. We will release our datasets, code and models.\"", "section": "1 INTRODUCTION"}
{"claim": "The proposed combinatorial toy model is too simplistic and fails to capture implicit biases of deep learning that drive observed merging phenomena.", "claim_type": "subjective", "paper_id": "ze7DOLi394", "paper_title": "On the Joint Interaction of Models, Data, and Features", "paper_venue": "iclr2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "RUsLYJEhvF", "reviewer": "Reviewer_MLur", "review_text": "Summary: This paper introduces the interaction tensor, for empirically analyzing the interaction between data and model through features. Based on some observations using this tensor, they propose a very simple toy model (a combinatorial model) for feature learning. They show that this model also exhibits Generalization Disagreement Equality (GDE). Finally, the authors use their model to provide data distributions that break GDE in real world experiments.\n\nStrengths: - The problem of feature learning is quite important and there has been a lot of attempts for gaining a better theoretical understanding of it in recent years.\n\n- The authors are able to come up with a toy model that shows GDE which is a very important phenomenon (and it does need the explicit assumption of calibration).\n\n- The model that the authors propose is simple and it can be analyzed fully.\n\n- The paper is very well-written.\n\nWeaknesses: - I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of\nabstraction\". However, I think the model proposed in this paper is too simplistic. The implicit biases of deep learning are core to some of the merging phenomenon that we see these days and the models that the authors propose fails to capture that. I also think that a good toy model should leave the door open to generalizations and getting closer to real world practice (for example, for the random features model of deep learning, there is a very obvious way to move towards making it more realistic). But the models that the authors propose is too abstract and it is not clear what simplifications are made to the real problem to arrive at the proposed model.\n\n- Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\n- It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\n- The notations are a bit confusing (i, j, k, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations (p_d, p_r, etc.). It will also help explain the method.\n\n- The authors \"prove\" GDE in their model without the explicit assumption of calibration. But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\n- The theoretical understanding of feature learning is not as rudimentary as the authors claim. For example,\n\n[1] Alex Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent, 2022.\n\n[2] Zhichao Wang, Andrew Engel, Anand Sarwate, Ioana Dumitriu, and Tony Chiang. Spectral evolution and invariance in linear-width neural networks, 2022.\n\n[3] Eshaan Nichani, Alex Damian, and Jason D Lee. Provable guarantees for nonlinear feature learning in three-layer neural networks, 2023.\n\n\n[4] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. Learning two-layer neural networks, one (giant) step at a time, 2023.\n\n[5] Behrad Moniri, Donghwan Lee, Hamed Hassani, and Edgar Dobriban, A theory of non-linear feature learning with one gradient step in two-layer neural networks, 2023.\n\nand many more.\n\nQuestions: - Looking at figure 3 (a) and 3 (c), it seems that the observations made from them are not that significant. Am I missing something? How does the choice of the thresholds affect your observations?", "labeling_timestamp": "2026-01-11T16:29:41.086240", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly describes the proposed model as simplified and acknowledges it does not model mechanistic/optimization biases (supporting the reviewer’s claim of simplicity and lack of implicit-bias modeling). However, the paper also argues that despite this simplification the framework nevertheless captures and explains merging-related phenomena (e.g., GDE and effects of merging classes), contradicting the reviewer’s assertion that it 'fails to capture' those phenomena.", "evidence": "1) \"we propose an alternative (still simplified) model of feature learning...\"  \n2) \"Note that we do not attempt to derive how our model can arise mechanistically via optimization, but we believe that this can be considered a strength of our approach.\"  \n3) \"Despite the simplification, we show that our framework naturally captures the phenomenon of GDE... Finally, we demonstrate that the framework can make accurate predictions about the effects of merging classes and changing data distribution on GDE and calibration, leading to the construction of natural data distributions that break GDE.\"", "section": "Introduction (Section 1)"}
{"claim": "The proposed model does not leave an obvious path for generalizations toward more realistic models or practical deep learning setups.", "claim_type": "novelty", "paper_id": "ze7DOLi394", "paper_title": "On the Joint Interaction of Models, Data, and Features", "paper_venue": "iclr2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "RUsLYJEhvF", "reviewer": "Reviewer_MLur", "review_text": "Summary: This paper introduces the interaction tensor, for empirically analyzing the interaction between data and model through features. Based on some observations using this tensor, they propose a very simple toy model (a combinatorial model) for feature learning. They show that this model also exhibits Generalization Disagreement Equality (GDE). Finally, the authors use their model to provide data distributions that break GDE in real world experiments.\n\nStrengths: - The problem of feature learning is quite important and there has been a lot of attempts for gaining a better theoretical understanding of it in recent years.\n\n- The authors are able to come up with a toy model that shows GDE which is a very important phenomenon (and it does need the explicit assumption of calibration).\n\n- The model that the authors propose is simple and it can be analyzed fully.\n\n- The paper is very well-written.\n\nWeaknesses: - I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of\nabstraction\". However, I think the model proposed in this paper is too simplistic. The implicit biases of deep learning are core to some of the merging phenomenon that we see these days and the models that the authors propose fails to capture that. I also think that a good toy model should leave the door open to generalizations and getting closer to real world practice (for example, for the random features model of deep learning, there is a very obvious way to move towards making it more realistic). But the models that the authors propose is too abstract and it is not clear what simplifications are made to the real problem to arrive at the proposed model.\n\n- Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\n- It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\n- The notations are a bit confusing (i, j, k, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations (p_d, p_r, etc.). It will also help explain the method.\n\n- The authors \"prove\" GDE in their model without the explicit assumption of calibration. But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\n- The theoretical understanding of feature learning is not as rudimentary as the authors claim. For example,\n\n[1] Alex Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent, 2022.\n\n[2] Zhichao Wang, Andrew Engel, Anand Sarwate, Ioana Dumitriu, and Tony Chiang. Spectral evolution and invariance in linear-width neural networks, 2022.\n\n[3] Eshaan Nichani, Alex Damian, and Jason D Lee. Provable guarantees for nonlinear feature learning in three-layer neural networks, 2023.\n\n\n[4] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. Learning two-layer neural networks, one (giant) step at a time, 2023.\n\n[5] Behrad Moniri, Donghwan Lee, Hamed Hassani, and Edgar Dobriban, A theory of non-linear feature learning with one gradient step in two-layer neural networks, 2023.\n\nand many more.\n\nQuestions: - Looking at figure 3 (a) and 3 (c), it seems that the observations made from them are not that significant. Am I missing something? How does the choice of the thresholds affect your observations?", "labeling_timestamp": "2026-01-11T16:29:51.303931", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly acknowledges that the proposed framework is a simplified, phenomenological model and states they do not attempt to derive mechanistic or optimization-based extensions, which supports the reviewer's point that the work does not provide an explicit path to more realistic/mechanistic generalizations. However, the paper also introduces practical analysis tools (interaction tensor, PCA-based feature identification, clustering) and demonstrates empirical and theoretical implications (e.g., explaining GDE and constructing distributions that break it), which could serve as starting points for future generalizations even though the authors do not make those extensions themselves.", "evidence": "\"Based on these observations, we propose an alternative (still simplified) model of feature learning... Note that we do not attempt to derive how our model can arise mechanistically via optimization, but we believe that this can be considered a strength of our approach.\"", "section": "1 INTRODUCTION"}
{"claim": "The paper does not clearly state which simplifications connect the real problem to the proposed abstract model.", "claim_type": "methodology", "paper_id": "ze7DOLi394", "paper_title": "On the Joint Interaction of Models, Data, and Features", "paper_venue": "iclr2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "RUsLYJEhvF", "reviewer": "Reviewer_MLur", "review_text": "Summary: This paper introduces the interaction tensor, for empirically analyzing the interaction between data and model through features. Based on some observations using this tensor, they propose a very simple toy model (a combinatorial model) for feature learning. They show that this model also exhibits Generalization Disagreement Equality (GDE). Finally, the authors use their model to provide data distributions that break GDE in real world experiments.\n\nStrengths: - The problem of feature learning is quite important and there has been a lot of attempts for gaining a better theoretical understanding of it in recent years.\n\n- The authors are able to come up with a toy model that shows GDE which is a very important phenomenon (and it does need the explicit assumption of calibration).\n\n- The model that the authors propose is simple and it can be analyzed fully.\n\n- The paper is very well-written.\n\nWeaknesses: - I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of\nabstraction\". However, I think the model proposed in this paper is too simplistic. The implicit biases of deep learning are core to some of the merging phenomenon that we see these days and the models that the authors propose fails to capture that. I also think that a good toy model should leave the door open to generalizations and getting closer to real world practice (for example, for the random features model of deep learning, there is a very obvious way to move towards making it more realistic). But the models that the authors propose is too abstract and it is not clear what simplifications are made to the real problem to arrive at the proposed model.\n\n- Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\n- It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\n- The notations are a bit confusing (i, j, k, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations (p_d, p_r, etc.). It will also help explain the method.\n\n- The authors \"prove\" GDE in their model without the explicit assumption of calibration. But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\n- The theoretical understanding of feature learning is not as rudimentary as the authors claim. For example,\n\n[1] Alex Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent, 2022.\n\n[2] Zhichao Wang, Andrew Engel, Anand Sarwate, Ioana Dumitriu, and Tony Chiang. Spectral evolution and invariance in linear-width neural networks, 2022.\n\n[3] Eshaan Nichani, Alex Damian, and Jason D Lee. Provable guarantees for nonlinear feature learning in three-layer neural networks, 2023.\n\n\n[4] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. Learning two-layer neural networks, one (giant) step at a time, 2023.\n\n[5] Behrad Moniri, Donghwan Lee, Hamed Hassani, and Edgar Dobriban, A theory of non-linear feature learning with one gradient step in two-layer neural networks, 2023.\n\nand many more.\n\nQuestions: - Looking at figure 3 (a) and 3 (c), it seems that the observations made from them are not that significant. Am I missing something? How does the choice of the thresholds affect your observations?", "labeling_timestamp": "2026-01-11T16:29:42.794438", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly states several simplifying assumptions of the proposed model (e.g., feature types, how data points contain features, and that models learn features by frequency) but does not fully derive or concretely map how these simplifications connect to the real learning dynamics or provide a mechanistic derivation. Thus the claim that the paper does not clearly state which simplifications connect the real problem to the abstract model is partially true: simplifications are listed, but their precise connection to the real problem is not fully articulated or derived.", "evidence": "\"Based on these observations, we propose an alternative (still simplified) model of feature learning, which better captures the above phenomenon. Specifically, we posit a framework where features come in two types: dominant (more frequent) and rare (less frequent)... We also assume that data points either contain a small number of dominant features or a large number of rare features and that models learn features according to their frequency in the data set; this captures the observed phenomenon where data points with fewer features receive higher-confidence predictions.\" \"Note that we do not attempt to derive how our model can arise mechanistically via optimization, but we believe that this can be considered a strength of our approach.\"", "section": "Introduction"}
{"claim": "The interaction-tensor-based model is not well motivated, and the paper does not explain how it relates to actual deep network training.", "claim_type": "subjective", "paper_id": "ze7DOLi394", "paper_title": "On the Joint Interaction of Models, Data, and Features", "paper_venue": "iclr2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "RUsLYJEhvF", "reviewer": "Reviewer_MLur", "review_text": "Summary: This paper introduces the interaction tensor, for empirically analyzing the interaction between data and model through features. Based on some observations using this tensor, they propose a very simple toy model (a combinatorial model) for feature learning. They show that this model also exhibits Generalization Disagreement Equality (GDE). Finally, the authors use their model to provide data distributions that break GDE in real world experiments.\n\nStrengths: - The problem of feature learning is quite important and there has been a lot of attempts for gaining a better theoretical understanding of it in recent years.\n\n- The authors are able to come up with a toy model that shows GDE which is a very important phenomenon (and it does need the explicit assumption of calibration).\n\n- The model that the authors propose is simple and it can be analyzed fully.\n\n- The paper is very well-written.\n\nWeaknesses: - I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of\nabstraction\". However, I think the model proposed in this paper is too simplistic. The implicit biases of deep learning are core to some of the merging phenomenon that we see these days and the models that the authors propose fails to capture that. I also think that a good toy model should leave the door open to generalizations and getting closer to real world practice (for example, for the random features model of deep learning, there is a very obvious way to move towards making it more realistic). But the models that the authors propose is too abstract and it is not clear what simplifications are made to the real problem to arrive at the proposed model.\n\n- Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\n- It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\n- The notations are a bit confusing (i, j, k, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations (p_d, p_r, etc.). It will also help explain the method.\n\n- The authors \"prove\" GDE in their model without the explicit assumption of calibration. But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\n- The theoretical understanding of feature learning is not as rudimentary as the authors claim. For example,\n\n[1] Alex Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent, 2022.\n\n[2] Zhichao Wang, Andrew Engel, Anand Sarwate, Ioana Dumitriu, and Tony Chiang. Spectral evolution and invariance in linear-width neural networks, 2022.\n\n[3] Eshaan Nichani, Alex Damian, and Jason D Lee. Provable guarantees for nonlinear feature learning in three-layer neural networks, 2023.\n\n\n[4] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. Learning two-layer neural networks, one (giant) step at a time, 2023.\n\n[5] Behrad Moniri, Donghwan Lee, Hamed Hassani, and Edgar Dobriban, A theory of non-linear feature learning with one gradient step in two-layer neural networks, 2023.\n\nand many more.\n\nQuestions: - Looking at figure 3 (a) and 3 (c), it seems that the observations made from them are not that significant. Am I missing something? How does the choice of the thresholds affect your observations?", "labeling_timestamp": "2026-01-11T16:29:51.866758", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper clearly motivates and defines the interaction tensor and describes how it is constructed (so it is motivated and connected to empirical analysis of models and data), but it explicitly states that it does not attempt to derive how the proposed model arises mechanistically from optimization or training, i.e., it does not explain the relation to actual deep network training.", "evidence": "“we introduce a new tool, the interaction tensor , for empirically analyzing the interaction between data and model through features.” || “Note that we do not attempt to derive how our model can arise mechanistically via optimization, but we believe that this can be considered a strength of our approach.”", "section": "Abstract; Introduction (end of Section 1)"}
{"claim": "The authors do not clearly specify how hyperparameters, such as thresholds, were set in their model or experiments.", "claim_type": "methodology", "paper_id": "ze7DOLi394", "paper_title": "On the Joint Interaction of Models, Data, and Features", "paper_venue": "iclr2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "RUsLYJEhvF", "reviewer": "Reviewer_MLur", "review_text": "Summary: This paper introduces the interaction tensor, for empirically analyzing the interaction between data and model through features. Based on some observations using this tensor, they propose a very simple toy model (a combinatorial model) for feature learning. They show that this model also exhibits Generalization Disagreement Equality (GDE). Finally, the authors use their model to provide data distributions that break GDE in real world experiments.\n\nStrengths: - The problem of feature learning is quite important and there has been a lot of attempts for gaining a better theoretical understanding of it in recent years.\n\n- The authors are able to come up with a toy model that shows GDE which is a very important phenomenon (and it does need the explicit assumption of calibration).\n\n- The model that the authors propose is simple and it can be analyzed fully.\n\n- The paper is very well-written.\n\nWeaknesses: - I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of\nabstraction\". However, I think the model proposed in this paper is too simplistic. The implicit biases of deep learning are core to some of the merging phenomenon that we see these days and the models that the authors propose fails to capture that. I also think that a good toy model should leave the door open to generalizations and getting closer to real world practice (for example, for the random features model of deep learning, there is a very obvious way to move towards making it more realistic). But the models that the authors propose is too abstract and it is not clear what simplifications are made to the real problem to arrive at the proposed model.\n\n- Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\n- It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\n- The notations are a bit confusing (i, j, k, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations (p_d, p_r, etc.). It will also help explain the method.\n\n- The authors \"prove\" GDE in their model without the explicit assumption of calibration. But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\n- The theoretical understanding of feature learning is not as rudimentary as the authors claim. For example,\n\n[1] Alex Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent, 2022.\n\n[2] Zhichao Wang, Andrew Engel, Anand Sarwate, Ioana Dumitriu, and Tony Chiang. Spectral evolution and invariance in linear-width neural networks, 2022.\n\n[3] Eshaan Nichani, Alex Damian, and Jason D Lee. Provable guarantees for nonlinear feature learning in three-layer neural networks, 2023.\n\n\n[4] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. Learning two-layer neural networks, one (giant) step at a time, 2023.\n\n[5] Behrad Moniri, Donghwan Lee, Hamed Hassani, and Edgar Dobriban, A theory of non-linear feature learning with one gradient step in two-layer neural networks, 2023.\n\nand many more.\n\nQuestions: - Looking at figure 3 (a) and 3 (c), it seems that the observations made from them are not that significant. Am I missing something? How does the choice of the thresholds affect your observations?", "labeling_timestamp": "2026-01-11T16:30:13.166280", "model": "gpt-5-mini", "label": "Supported", "justification": "The provided paper text mentions hyperparameter-like choices (e.g., selecting the top K principal components and binarizing an interaction tensor) but does not state how such parameters (K, clustering or binarization/threshold criteria) were chosen or tuned. Sections 3.1 and 3.2 describe the procedures but omit specific settings or threshold selection methodology.", "evidence": "1) \"We use the top K principal components V : K and project the representations to R K , Φ_proj ≜ Φ V : K = [ V ⊤ : K φ ( x (1) ) , V ⊤ : K φ ( x (2) ) , . . . , V ⊤ : K φ ( x ( N ) )] ⊤ . For notation simplicity, we will use υ ( x ) to denote V ⊤ : K φ ( x ) ...\"\n\n2) \"In this section, we describe the procedure for constructing the interaction tensor Ω ∈ { 0 , 1 } M × N × T . ... If the n th data point contains the t th feature and the m th model has learned the T th feature, then Ω mnt would be 1 .\"", "section": "3.1 PRINCIPAL COMPONENTS OF ACTIVATION AS FEATURES; 3 CONNECTING MODELS AND DATA WITH FEATURES / 3.2 CONSTRUCTING THE INTERACTION TENSOR"}
{"claim": "It is unclear how the choice of thresholds affects the reported observations and results.", "claim_type": "methodology", "paper_id": "ze7DOLi394", "paper_title": "On the Joint Interaction of Models, Data, and Features", "paper_venue": "iclr2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "RUsLYJEhvF", "reviewer": "Reviewer_MLur", "review_text": "Summary: This paper introduces the interaction tensor, for empirically analyzing the interaction between data and model through features. Based on some observations using this tensor, they propose a very simple toy model (a combinatorial model) for feature learning. They show that this model also exhibits Generalization Disagreement Equality (GDE). Finally, the authors use their model to provide data distributions that break GDE in real world experiments.\n\nStrengths: - The problem of feature learning is quite important and there has been a lot of attempts for gaining a better theoretical understanding of it in recent years.\n\n- The authors are able to come up with a toy model that shows GDE which is a very important phenomenon (and it does need the explicit assumption of calibration).\n\n- The model that the authors propose is simple and it can be analyzed fully.\n\n- The paper is very well-written.\n\nWeaknesses: - I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of\nabstraction\". However, I think the model proposed in this paper is too simplistic. The implicit biases of deep learning are core to some of the merging phenomenon that we see these days and the models that the authors propose fails to capture that. I also think that a good toy model should leave the door open to generalizations and getting closer to real world practice (for example, for the random features model of deep learning, there is a very obvious way to move towards making it more realistic). But the models that the authors propose is too abstract and it is not clear what simplifications are made to the real problem to arrive at the proposed model.\n\n- Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\n- It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\n- The notations are a bit confusing (i, j, k, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations (p_d, p_r, etc.). It will also help explain the method.\n\n- The authors \"prove\" GDE in their model without the explicit assumption of calibration. But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\n- The theoretical understanding of feature learning is not as rudimentary as the authors claim. For example,\n\n[1] Alex Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent, 2022.\n\n[2] Zhichao Wang, Andrew Engel, Anand Sarwate, Ioana Dumitriu, and Tony Chiang. Spectral evolution and invariance in linear-width neural networks, 2022.\n\n[3] Eshaan Nichani, Alex Damian, and Jason D Lee. Provable guarantees for nonlinear feature learning in three-layer neural networks, 2023.\n\n\n[4] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. Learning two-layer neural networks, one (giant) step at a time, 2023.\n\n[5] Behrad Moniri, Donghwan Lee, Hamed Hassani, and Edgar Dobriban, A theory of non-linear feature learning with one gradient step in two-layer neural networks, 2023.\n\nand many more.\n\nQuestions: - Looking at figure 3 (a) and 3 (c), it seems that the observations made from them are not that significant. Am I missing something? How does the choice of the thresholds affect your observations?", "labeling_timestamp": "2026-01-11T16:30:00.795233", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt defines a binary interaction tensor and describes feature extraction via PCA and clustering (Sections 3, 3.1, 3.2) but does not state how binary membership (Ω_mnt = 1) or clustering thresholds are chosen nor how varying such thresholds would affect observations or results. The excerpt lacks explicit discussion of threshold selection or sensitivity analysis, so there is insufficient information to confirm the reviewer's claim from the given content.", "evidence": "\"If the n th data point contains the t th feature and the m th model has learned the T th feature, then Ω mnt would be 1 .\"", "section": "3 (and subsections 3.1–3.2)"}
{"claim": "The notational choices (using generic letters like i, j, k) are confusing and the paper lacks a summary figure for symbols such as p_d and p_r.", "claim_type": "presentation", "paper_id": "ze7DOLi394", "paper_title": "On the Joint Interaction of Models, Data, and Features", "paper_venue": "iclr2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "RUsLYJEhvF", "reviewer": "Reviewer_MLur", "review_text": "Summary: This paper introduces the interaction tensor, for empirically analyzing the interaction between data and model through features. Based on some observations using this tensor, they propose a very simple toy model (a combinatorial model) for feature learning. They show that this model also exhibits Generalization Disagreement Equality (GDE). Finally, the authors use their model to provide data distributions that break GDE in real world experiments.\n\nStrengths: - The problem of feature learning is quite important and there has been a lot of attempts for gaining a better theoretical understanding of it in recent years.\n\n- The authors are able to come up with a toy model that shows GDE which is a very important phenomenon (and it does need the explicit assumption of calibration).\n\n- The model that the authors propose is simple and it can be analyzed fully.\n\n- The paper is very well-written.\n\nWeaknesses: - I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of\nabstraction\". However, I think the model proposed in this paper is too simplistic. The implicit biases of deep learning are core to some of the merging phenomenon that we see these days and the models that the authors propose fails to capture that. I also think that a good toy model should leave the door open to generalizations and getting closer to real world practice (for example, for the random features model of deep learning, there is a very obvious way to move towards making it more realistic). But the models that the authors propose is too abstract and it is not clear what simplifications are made to the real problem to arrive at the proposed model.\n\n- Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\n- It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\n- The notations are a bit confusing (i, j, k, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations (p_d, p_r, etc.). It will also help explain the method.\n\n- The authors \"prove\" GDE in their model without the explicit assumption of calibration. But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\n- The theoretical understanding of feature learning is not as rudimentary as the authors claim. For example,\n\n[1] Alex Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent, 2022.\n\n[2] Zhichao Wang, Andrew Engel, Anand Sarwate, Ioana Dumitriu, and Tony Chiang. Spectral evolution and invariance in linear-width neural networks, 2022.\n\n[3] Eshaan Nichani, Alex Damian, and Jason D Lee. Provable guarantees for nonlinear feature learning in three-layer neural networks, 2023.\n\n\n[4] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. Learning two-layer neural networks, one (giant) step at a time, 2023.\n\n[5] Behrad Moniri, Donghwan Lee, Hamed Hassani, and Edgar Dobriban, A theory of non-linear feature learning with one gradient step in two-layer neural networks, 2023.\n\nand many more.\n\nQuestions: - Looking at figure 3 (a) and 3 (c), it seems that the observations made from them are not that significant. Am I missing something? How does the choice of the thresholds affect your observations?", "labeling_timestamp": "2026-01-11T16:30:26.004174", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper indeed uses generic index notation (i, j, k, m, n, t) in its technical sections, which supports the reviewer's observation about notational choices. However, the reviewer claim that these choices are 'confusing' is a subjective judgment not stated or evidenced in the paper. Regarding a summary figure for symbols such as p_d and p_r, the provided paper content contains multiple figures (e.g., Figure 1) but does not show or mention symbols named p_d or p_r in the excerpt, so there is insufficient evidence in the provided content to confirm whether such a summary figure is missing from the full paper.", "evidence": ["we will use υ ( x ) to denote V ⊤ : K φ ( x ) and υ m,k ( x ) to denote the k th entry of of the m th model's υ ( x ) .", "For models ( f i , f j ) and their respective a th and b th features, we can define their correlation to be:", "Figure 1: Visualization of images with the least features (left) and the most features (right) for classes of CIFAR 10 under our feature definition (defined in Section 3.1)."], "section": "Sections 3.1 and 3.2; Figure 1 (Introduction)"}
{"claim": "The authors claim to prove GDE without explicitly assuming calibration, but they do not clarify whether their model assumptions are stronger or weaker than calibration.", "claim_type": "methodology", "paper_id": "ze7DOLi394", "paper_title": "On the Joint Interaction of Models, Data, and Features", "paper_venue": "iclr2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "RUsLYJEhvF", "reviewer": "Reviewer_MLur", "review_text": "Summary: This paper introduces the interaction tensor, for empirically analyzing the interaction between data and model through features. Based on some observations using this tensor, they propose a very simple toy model (a combinatorial model) for feature learning. They show that this model also exhibits Generalization Disagreement Equality (GDE). Finally, the authors use their model to provide data distributions that break GDE in real world experiments.\n\nStrengths: - The problem of feature learning is quite important and there has been a lot of attempts for gaining a better theoretical understanding of it in recent years.\n\n- The authors are able to come up with a toy model that shows GDE which is a very important phenomenon (and it does need the explicit assumption of calibration).\n\n- The model that the authors propose is simple and it can be analyzed fully.\n\n- The paper is very well-written.\n\nWeaknesses: - I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of\nabstraction\". However, I think the model proposed in this paper is too simplistic. The implicit biases of deep learning are core to some of the merging phenomenon that we see these days and the models that the authors propose fails to capture that. I also think that a good toy model should leave the door open to generalizations and getting closer to real world practice (for example, for the random features model of deep learning, there is a very obvious way to move towards making it more realistic). But the models that the authors propose is too abstract and it is not clear what simplifications are made to the real problem to arrive at the proposed model.\n\n- Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\n- It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\n- The notations are a bit confusing (i, j, k, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations (p_d, p_r, etc.). It will also help explain the method.\n\n- The authors \"prove\" GDE in their model without the explicit assumption of calibration. But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\n- The theoretical understanding of feature learning is not as rudimentary as the authors claim. For example,\n\n[1] Alex Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent, 2022.\n\n[2] Zhichao Wang, Andrew Engel, Anand Sarwate, Ioana Dumitriu, and Tony Chiang. Spectral evolution and invariance in linear-width neural networks, 2022.\n\n[3] Eshaan Nichani, Alex Damian, and Jason D Lee. Provable guarantees for nonlinear feature learning in three-layer neural networks, 2023.\n\n\n[4] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. Learning two-layer neural networks, one (giant) step at a time, 2023.\n\n[5] Behrad Moniri, Donghwan Lee, Hamed Hassani, and Edgar Dobriban, A theory of non-linear feature learning with one gradient step in two-layer neural networks, 2023.\n\nand many more.\n\nQuestions: - Looking at figure 3 (a) and 3 (c), it seems that the observations made from them are not that significant. Am I missing something? How does the choice of the thresholds affect your observations?", "labeling_timestamp": "2026-01-11T16:30:17.968770", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly states that GDE arises from their feature-learning framework without making explicit calibration assumptions, and it does not provide a comparison stating whether its model assumptions are stronger or weaker than the calibration assumption (no such comparison or claim appears in the cited sections).", "evidence": "\"we show that GDE arises immediately as a consequence of how neural networks learn appropriately defined features. This phenomenon was previously explained by assuming calibration of the underlying ensemble, which is often a strong assumption to make.\"; \"To address this gap, this work shows that the phenomenon of GDE can arise within a feature learning context, without making explicit calibration assumptions.\"; \"Note that we do not attempt to derive how our model can arise mechanistically via optimization, but we believe that this can be considered a strength of our approach.\"", "section": "Introduction / Related Works"}
{"claim": "The paper overlooks several recent theoretical works on provable feature learning, contradicting the claim that the theoretical understanding of feature learning is rudimentary.", "claim_type": "novelty", "paper_id": "ze7DOLi394", "paper_title": "On the Joint Interaction of Models, Data, and Features", "paper_venue": "iclr2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "RUsLYJEhvF", "reviewer": "Reviewer_MLur", "review_text": "Summary: This paper introduces the interaction tensor, for empirically analyzing the interaction between data and model through features. Based on some observations using this tensor, they propose a very simple toy model (a combinatorial model) for feature learning. They show that this model also exhibits Generalization Disagreement Equality (GDE). Finally, the authors use their model to provide data distributions that break GDE in real world experiments.\n\nStrengths: - The problem of feature learning is quite important and there has been a lot of attempts for gaining a better theoretical understanding of it in recent years.\n\n- The authors are able to come up with a toy model that shows GDE which is a very important phenomenon (and it does need the explicit assumption of calibration).\n\n- The model that the authors propose is simple and it can be analyzed fully.\n\n- The paper is very well-written.\n\nWeaknesses: - I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of\nabstraction\". However, I think the model proposed in this paper is too simplistic. The implicit biases of deep learning are core to some of the merging phenomenon that we see these days and the models that the authors propose fails to capture that. I also think that a good toy model should leave the door open to generalizations and getting closer to real world practice (for example, for the random features model of deep learning, there is a very obvious way to move towards making it more realistic). But the models that the authors propose is too abstract and it is not clear what simplifications are made to the real problem to arrive at the proposed model.\n\n- Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\n- It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\n- The notations are a bit confusing (i, j, k, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations (p_d, p_r, etc.). It will also help explain the method.\n\n- The authors \"prove\" GDE in their model without the explicit assumption of calibration. But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\n- The theoretical understanding of feature learning is not as rudimentary as the authors claim. For example,\n\n[1] Alex Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent, 2022.\n\n[2] Zhichao Wang, Andrew Engel, Anand Sarwate, Ioana Dumitriu, and Tony Chiang. Spectral evolution and invariance in linear-width neural networks, 2022.\n\n[3] Eshaan Nichani, Alex Damian, and Jason D Lee. Provable guarantees for nonlinear feature learning in three-layer neural networks, 2023.\n\n\n[4] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. Learning two-layer neural networks, one (giant) step at a time, 2023.\n\n[5] Behrad Moniri, Donghwan Lee, Hamed Hassani, and Edgar Dobriban, A theory of non-linear feature learning with one gradient step in two-layer neural networks, 2023.\n\nand many more.\n\nQuestions: - Looking at figure 3 (a) and 3 (c), it seems that the observations made from them are not that significant. Am I missing something? How does the choice of the thresholds affect your observations?", "labeling_timestamp": "2026-01-11T16:30:21.874625", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly cites multiple recent theoretical works on feature learning and discusses them in Related Works, so the reviewer's claim that the paper overlooks such works is false. The paper acknowledges these works while still arguing the area is under-explored.", "evidence": "“Recent works have started to incorporate feature learning into theoretical analysis (Li et al., 2019; Allen-Zhu and Li, 2020; Yang and Hu, 2021; Karp et al., 2021; Wen and Li, 2021; Allen-Zhu and Li, 2022; Ba et al., 2022).”\n\n“to the best of our knowledge, the theoretical understanding of the role features play in deep learning is still under-explored.”", "section": "2 RELATED WORKS; 1 INTRODUCTION"}
{"claim": "The observations shown in Figures 3(a) and 3(c) appear not statistically significant or convincing based on the plotted results.", "claim_type": "experimental", "paper_id": "ze7DOLi394", "paper_title": "On the Joint Interaction of Models, Data, and Features", "paper_venue": "iclr2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "RUsLYJEhvF", "reviewer": "Reviewer_MLur", "review_text": "Summary: This paper introduces the interaction tensor, for empirically analyzing the interaction between data and model through features. Based on some observations using this tensor, they propose a very simple toy model (a combinatorial model) for feature learning. They show that this model also exhibits Generalization Disagreement Equality (GDE). Finally, the authors use their model to provide data distributions that break GDE in real world experiments.\n\nStrengths: - The problem of feature learning is quite important and there has been a lot of attempts for gaining a better theoretical understanding of it in recent years.\n\n- The authors are able to come up with a toy model that shows GDE which is a very important phenomenon (and it does need the explicit assumption of calibration).\n\n- The model that the authors propose is simple and it can be analyzed fully.\n\n- The paper is very well-written.\n\nWeaknesses: - I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of\nabstraction\". However, I think the model proposed in this paper is too simplistic. The implicit biases of deep learning are core to some of the merging phenomenon that we see these days and the models that the authors propose fails to capture that. I also think that a good toy model should leave the door open to generalizations and getting closer to real world practice (for example, for the random features model of deep learning, there is a very obvious way to move towards making it more realistic). But the models that the authors propose is too abstract and it is not clear what simplifications are made to the real problem to arrive at the proposed model.\n\n- Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\n- It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\n- The notations are a bit confusing (i, j, k, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations (p_d, p_r, etc.). It will also help explain the method.\n\n- The authors \"prove\" GDE in their model without the explicit assumption of calibration. But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\n- The theoretical understanding of feature learning is not as rudimentary as the authors claim. For example,\n\n[1] Alex Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent, 2022.\n\n[2] Zhichao Wang, Andrew Engel, Anand Sarwate, Ioana Dumitriu, and Tony Chiang. Spectral evolution and invariance in linear-width neural networks, 2022.\n\n[3] Eshaan Nichani, Alex Damian, and Jason D Lee. Provable guarantees for nonlinear feature learning in three-layer neural networks, 2023.\n\n\n[4] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. Learning two-layer neural networks, one (giant) step at a time, 2023.\n\n[5] Behrad Moniri, Donghwan Lee, Hamed Hassani, and Edgar Dobriban, A theory of non-linear feature learning with one gradient step in two-layer neural networks, 2023.\n\nand many more.\n\nQuestions: - Looking at figure 3 (a) and 3 (c), it seems that the observations made from them are not that significant. Am I missing something? How does the choice of the thresholds affect your observations?", "labeling_timestamp": "2026-01-11T16:30:28.011924", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper content does not include Figures 3(a) or 3(c) nor any discussion or plotted results for them, so there is insufficient information to judge whether those observations are statistically significant or convincing.", "evidence": "Figure 1: Visualization of images with the least features (left) and the most features (right) for classes of CIFAR 10 under our feature definition (defined in Section 3.1). ... (see Figure 12 of the appendix).\n...\n(In Section 3) In this section, we describe the procedure for constructing the interaction tensor Ω ∈ {0,1}^{M×N×T}. ... (see Figure 6 for an illustration).", "section": "Introduction and Section 3 (provided excerpt)"}
{"claim": "The paper does not explore obvious generalizations (for example from random features models) to make the toy model closer to practical deep learning.", "claim_type": "methodology", "paper_id": "ze7DOLi394", "paper_title": "On the Joint Interaction of Models, Data, and Features", "paper_venue": "iclr2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "RUsLYJEhvF", "reviewer": "Reviewer_MLur", "review_text": "Summary: This paper introduces the interaction tensor, for empirically analyzing the interaction between data and model through features. Based on some observations using this tensor, they propose a very simple toy model (a combinatorial model) for feature learning. They show that this model also exhibits Generalization Disagreement Equality (GDE). Finally, the authors use their model to provide data distributions that break GDE in real world experiments.\n\nStrengths: - The problem of feature learning is quite important and there has been a lot of attempts for gaining a better theoretical understanding of it in recent years.\n\n- The authors are able to come up with a toy model that shows GDE which is a very important phenomenon (and it does need the explicit assumption of calibration).\n\n- The model that the authors propose is simple and it can be analyzed fully.\n\n- The paper is very well-written.\n\nWeaknesses: - I definitely agree with \"the empirical phenomena of deep learning can be understood at many different layers of\nabstraction\". However, I think the model proposed in this paper is too simplistic. The implicit biases of deep learning are core to some of the merging phenomenon that we see these days and the models that the authors propose fails to capture that. I also think that a good toy model should leave the door open to generalizations and getting closer to real world practice (for example, for the random features model of deep learning, there is a very obvious way to move towards making it more realistic). But the models that the authors propose is too abstract and it is not clear what simplifications are made to the real problem to arrive at the proposed model.\n\n- Although the model is based on some observations using the interaction tensor, I still find the model to be not very well motivated. Any insights on how this can relate to the training of deep nets?\n\n- It is not very clear how the authors set the hyperparameters in their model (e.g., the thresholds).\n\n- The notations are a bit confusing (i, j, k, etc.). I suggest authors avoid using these generic letters. Also, the paper will benefit greatly from a figure that summarizes all the notations (p_d, p_r, etc.). It will also help explain the method.\n\n- The authors \"prove\" GDE in their model without the explicit assumption of calibration. But the model is very abstract/high-level and I'm not sure if the assumptions that they make are stronger or weaker than calibration.\n\n- The theoretical understanding of feature learning is not as rudimentary as the authors claim. For example,\n\n[1] Alex Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent, 2022.\n\n[2] Zhichao Wang, Andrew Engel, Anand Sarwate, Ioana Dumitriu, and Tony Chiang. Spectral evolution and invariance in linear-width neural networks, 2022.\n\n[3] Eshaan Nichani, Alex Damian, and Jason D Lee. Provable guarantees for nonlinear feature learning in three-layer neural networks, 2023.\n\n\n[4] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. Learning two-layer neural networks, one (giant) step at a time, 2023.\n\n[5] Behrad Moniri, Donghwan Lee, Hamed Hassani, and Edgar Dobriban, A theory of non-linear feature learning with one gradient step in two-layer neural networks, 2023.\n\nand many more.\n\nQuestions: - Looking at figure 3 (a) and 3 (c), it seems that the observations made from them are not that significant. Am I missing something? How does the choice of the thresholds affect your observations?", "labeling_timestamp": "2026-01-11T16:30:40.445873", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly frames its model as a simplified/toy feature-learning model and states it does not attempt mechanistic derivations or extensions; it discusses related theoretical frameworks (e.g. NTK) but does not explore generalizations such as random-features-style models to make the toy model closer to practical deep learning.", "evidence": "\"we propose an alternative (still simplified) model of feature learning, which better captures the above phenomenon.\" \n\n\"Note that we do not attempt to derive how our model can arise mechanistically via optimization, but we believe that this can be considered a strength of our approach.\"", "section": "1 INTRODUCTION"}
{"claim": "The proposed method lacks necessary motivation and the paper fails to convincingly justify why this approach is needed.", "claim_type": "subjective", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ppMpJyIYRZ", "reviewer": "Reviewer_m8JS", "review_text": "Summary: The paper introduces a method to enhance the out-of-distribution performance of graph neural networks (GNN) by learning to understand distribution shifts instead of addressing the assumed ones. To achieve this, the Mixture of Experts architecture is integrated into the GNN, supplemented by an alignment procedure to recognize the shift. Empirical experiments are conducted to validate the theoretical assertion.\n\nOn the whole, I believe the proposed method lacks the necessary motivation and its novelty isn't substantial enough to meet the standard.\n\nStrengths: - The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\n- Real-world datasets back the claims through experiments.\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\n\nWeaknesses: - The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \"previous research has concentrated on addressing specific types of distribution shifts.\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \"environment generators\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\n\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\n\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\n\nQuestions: Please check Weaknesses.", "labeling_timestamp": "2026-01-11T16:30:43.473166", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly motivates the problem (limitations of prior work on specific shift types and the need to handle nuanced, instance-wise distribution shifts), introduces a principled solution (GraphMETRO) with a clear conceptual framework (gating + experts) and a formal assumption (Assumption 1) that justifies the mixture decomposition approach, and reports empirical gains. Thus the claim that the paper 'lacks necessary motivation and fails to convincingly justify' the approach is contradicted by multiple sections of the paper.", "evidence": "1) \"However, previous works mostly focus on addressing specific types of distribution shifts, e.g., larger graph size or node degree, or inferring distribution shifts from data environments, which is highly limited when confronted with nuanced distribution shifts.\" 2) \"Here we introduce GraphMETRO, a novel framework based on a mixture-of-experts (MoE) architecture, enhancing GNN generalizability for both node- and graph-level tasks. The core concept of GraphMETRO includes the construction of a hierarchical architecture composed of a gating model and multiple expert models that are aligned in a common representation space.\" 3) \"Assumption 1 (An Equivalent Mixture for Distribution Shifts) ... We assume that the resulting shift in Dt can be modeled by the selective application of up to k out of K classes of stochastic transformations to each instance in the source distribution Ds (k < K).\"", "section": "Abstract; 1 INTRODUCTION; 3.1 MIXTURE COMPONENTS"}
{"claim": "The novelty of the proposed method is insufficient and does not meet the standards expected for significant research contributions.", "claim_type": "novelty", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ppMpJyIYRZ", "reviewer": "Reviewer_m8JS", "review_text": "Summary: The paper introduces a method to enhance the out-of-distribution performance of graph neural networks (GNN) by learning to understand distribution shifts instead of addressing the assumed ones. To achieve this, the Mixture of Experts architecture is integrated into the GNN, supplemented by an alignment procedure to recognize the shift. Empirical experiments are conducted to validate the theoretical assertion.\n\nOn the whole, I believe the proposed method lacks the necessary motivation and its novelty isn't substantial enough to meet the standard.\n\nStrengths: - The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\n- Real-world datasets back the claims through experiments.\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\n\nWeaknesses: - The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \"previous research has concentrated on addressing specific types of distribution shifts.\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \"environment generators\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\n\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\n\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\n\nQuestions: Please check Weaknesses.", "labeling_timestamp": "2026-01-11T16:30:43.367085", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper repeatedly claims novelty for the proposed approach (GraphMETRO), describing it as a \"novel framework\" and the \"first\" MoE designed for graph distribution shifts, and highlights novel contributions in multiple sections. Therefore the paper does not support the reviewer's assertion that its novelty is insufficient.", "evidence": "Abstract: \"Here we introduce GraphMETRO, a novel framework based on a mixture-of-experts (MoE) architecture...\"; Introduction (end of first page): \"We present a novel framework, GraphMETRO, to enhance model generalizability...\"; Related Works: \"GraphMETRO is the first to design a mixture-of-expert model specifically tailored to address graph distribution shifts, coupled with a novel objective for producing invariant representations.\"; Key benefits: \"It provides a simple yet novel paradigm, which formulates graph generalization as inferring the equivalent mixture as a proxy...\"", "section": "Abstract; Introduction; Related Works; Key benefits (end of Introduction)"}
{"claim": "The paper's motivation is inadequately substantiated and mischaracterizes prior work on distribution shifts in graph neural networks.", "claim_type": "subjective", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ppMpJyIYRZ", "reviewer": "Reviewer_m8JS", "review_text": "Summary: The paper introduces a method to enhance the out-of-distribution performance of graph neural networks (GNN) by learning to understand distribution shifts instead of addressing the assumed ones. To achieve this, the Mixture of Experts architecture is integrated into the GNN, supplemented by an alignment procedure to recognize the shift. Empirical experiments are conducted to validate the theoretical assertion.\n\nOn the whole, I believe the proposed method lacks the necessary motivation and its novelty isn't substantial enough to meet the standard.\n\nStrengths: - The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\n- Real-world datasets back the claims through experiments.\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\n\nWeaknesses: - The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \"previous research has concentrated on addressing specific types of distribution shifts.\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \"environment generators\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\n\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\n\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\n\nQuestions: Please check Weaknesses.", "labeling_timestamp": "2026-01-11T16:30:56.563516", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states and substantiates its motivation (that real-world graph distribution shifts are multifaceted and instance-specific) and cites multiple prior works while characterizing their limitations. It therefore does not leave the motivation inadequately substantiated, nor does it plainly mischaracterize prior work based on the provided text.", "evidence": "Abstract: \"previous works mostly focus on addressing specific types of distribution shifts, e.g., larger graph size or node degree, or inferring distribution shifts from data environments, which is highly limited when confronted with nuanced distribution shifts.\" Introduction: \"A line of previous research has focused on addressing specific types of distribution shifts. For example, works have looked at distribution shifts related to graph size ..., feature noise ..., and node degree or local structure ..., assuming that the target datasets adhere to the corresponding type of distribution shift. However, these approaches are highly limited as the distribution shift types could be multiple in the real-world datasets and may not be consistent with the presumed distribution shifts.\" Related Works: \"The prevailing invariant learning approaches ... focus on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making its applicability limited.\"", "section": "Abstract; Introduction; Related Works"}
{"claim": "The paper overlooks prior works that learn environment generators for GNNs to detect graph shifts (e.g., arXiv:2202.02466).", "claim_type": "subjective", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ppMpJyIYRZ", "reviewer": "Reviewer_m8JS", "review_text": "Summary: The paper introduces a method to enhance the out-of-distribution performance of graph neural networks (GNN) by learning to understand distribution shifts instead of addressing the assumed ones. To achieve this, the Mixture of Experts architecture is integrated into the GNN, supplemented by an alignment procedure to recognize the shift. Empirical experiments are conducted to validate the theoretical assertion.\n\nOn the whole, I believe the proposed method lacks the necessary motivation and its novelty isn't substantial enough to meet the standard.\n\nStrengths: - The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\n- Real-world datasets back the claims through experiments.\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\n\nWeaknesses: - The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \"previous research has concentrated on addressing specific types of distribution shifts.\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \"environment generators\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\n\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\n\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\n\nQuestions: Please check Weaknesses.", "labeling_timestamp": "2026-01-11T16:31:02.630483", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper does discuss prior invariant-learning methods that infer distribution shifts from data environments (so it does not completely overlook that line of work), but it does not mention or cite specific papers that learn environment generators (e.g., arXiv:2202.02466). Thus the reviewer's claim is partially correct.", "evidence": "1) \"While previous invariant learning methods on graphs (Wu et al., 2022c;a; Sui et al., 2023; Chen et al., 2022) is able to accommodate multiple distribution shifts inferred from data environments, they focus on common patterns within each environment and do not explicitly model the variety across node or graph instances.\"  2) \"The prevailing invariant learning approaches assume that there exist an underlying graph structure ... that is invariant to different environments and / or causally related to the label of a given instance.\"", "section": "Introduction; Related Works"}
{"claim": "Existing works focus on learning shift-specific graph transformations (e.g., arXiv:2211.02843), but the paper fails to acknowledge these approaches.", "claim_type": "subjective", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ppMpJyIYRZ", "reviewer": "Reviewer_m8JS", "review_text": "Summary: The paper introduces a method to enhance the out-of-distribution performance of graph neural networks (GNN) by learning to understand distribution shifts instead of addressing the assumed ones. To achieve this, the Mixture of Experts architecture is integrated into the GNN, supplemented by an alignment procedure to recognize the shift. Empirical experiments are conducted to validate the theoretical assertion.\n\nOn the whole, I believe the proposed method lacks the necessary motivation and its novelty isn't substantial enough to meet the standard.\n\nStrengths: - The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\n- Real-world datasets back the claims through experiments.\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\n\nWeaknesses: - The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \"previous research has concentrated on addressing specific types of distribution shifts.\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \"environment generators\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\n\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\n\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\n\nQuestions: Please check Weaknesses.", "labeling_timestamp": "2026-01-11T16:31:13.121024", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly acknowledges prior work that focuses on specific types of distribution shifts (e.g., graph size, feature noise, node degree/local structure), so the claim that the paper \"fails to acknowledge these approaches\" is incorrect.", "evidence": "\"A line of previous research has focused on addressing specific types of distribution shifts. For example, works have looked at distribution shifts related to graph size (Bevilacqua et al., 2021; Buffelli et al., 2022; Yehudai et al., 2021), feature noise (Knyazev et al., 2019; Ding et al., 2021), and node degree or local structure (Wu et al., 2022b; Gui et al., 2022), assuming that the target datasets adhere to the corresponding type of distribution shift.\"", "section": "1 INTRODUCTION"}
{"claim": "The authors do not provide a detailed or robust justification for choosing Mixture of Experts (MOE) over alternative approaches.", "claim_type": "subjective", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ppMpJyIYRZ", "reviewer": "Reviewer_m8JS", "review_text": "Summary: The paper introduces a method to enhance the out-of-distribution performance of graph neural networks (GNN) by learning to understand distribution shifts instead of addressing the assumed ones. To achieve this, the Mixture of Experts architecture is integrated into the GNN, supplemented by an alignment procedure to recognize the shift. Empirical experiments are conducted to validate the theoretical assertion.\n\nOn the whole, I believe the proposed method lacks the necessary motivation and its novelty isn't substantial enough to meet the standard.\n\nStrengths: - The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\n- Real-world datasets back the claims through experiments.\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\n\nWeaknesses: - The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \"previous research has concentrated on addressing specific types of distribution shifts.\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \"environment generators\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\n\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\n\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\n\nQuestions: Please check Weaknesses.", "labeling_timestamp": "2026-01-11T16:31:25.448669", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper offers a high-level, intuitive motivation for using a Mixture-of-Experts—namely that distribution shifts can be decomposed into instance-wise mixture components and MoE (gating + experts) naturally models that—but it does not provide a detailed or robust justification (e.g., theoretical analysis, ablation comparing MoE to concrete alternative architectures, or empirical justification beyond asserting novelty). The support is limited to an assumption and conceptual description rather than in-depth justification or comparisons.", "evidence": "1) \"We design a hierarchical architecture composed of a gating model and multiple expert models, inspired by the mixture-of-experts (MoE) architecture (Jordan & Jacobs, 1994). Specifically, as shown in Figure 1b, the gating model processes an input graph to pinpoint the critical mixture components that govern the given graph instance. Each expert model excels in generating invariant representations with respect to one kind of mixture component, while all of the experts are aligned in a common representation space to ensure model compatibility.\" 2) \"Assumption 1 (An Equivalent Mixture for Distribution Shifts) ... We assume that the resulting shift in D_t can be modeled by the selective application of up to k out of K classes of stochastic transformations to each instance in the source distribution D_s (k < K).\" 3) \"GraphMETRO is the first to design a mixture-of-expert model specifically tailored to address graph distribution shifts, coupled with a novel objective for producing invariant representations.\"", "section": "Introduction; 3 METHOD (MIXTURE COMPONENTS); 2 RELATED WORKS"}
{"claim": "The theoretical assumption about distribution shifts is presented in overly broad terms and lacks sufficient specificity for validation.", "claim_type": "methodology", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ppMpJyIYRZ", "reviewer": "Reviewer_m8JS", "review_text": "Summary: The paper introduces a method to enhance the out-of-distribution performance of graph neural networks (GNN) by learning to understand distribution shifts instead of addressing the assumed ones. To achieve this, the Mixture of Experts architecture is integrated into the GNN, supplemented by an alignment procedure to recognize the shift. Empirical experiments are conducted to validate the theoretical assertion.\n\nOn the whole, I believe the proposed method lacks the necessary motivation and its novelty isn't substantial enough to meet the standard.\n\nStrengths: - The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\n- Real-world datasets back the claims through experiments.\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\n\nWeaknesses: - The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \"previous research has concentrated on addressing specific types of distribution shifts.\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \"environment generators\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\n\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\n\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\n\nQuestions: Please check Weaknesses.", "labeling_timestamp": "2026-01-11T16:31:21.071501", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper itself frames its core theoretical claim as an \"informal assumption\" and states the shift can be modeled as the selective application of up to k out of K stochastic transformations, but it does not define the transformations, specify K or k, nor provide a procedure to validate the assumption—consistent with the reviewer's statement that the assumption is broad and lacks specificity for validation.", "evidence": "To seek a more tractable solution, we propose the following informal assumption:\n\nAssumption 1 (An Equivalent Mixture for Distribution Shifts) Let the distribution shift between the source D_s and target D_t distributions be the result of an unknown intervention in the graph formation mechanism. We assume that the resulting shift in D_t can be modeled by the selective application of up to k out of K classes of stochastic transformations to each instance in the source distribution D_s (k < K).", "section": "3.1 MIXTURE COMPONENTS"}
{"claim": "Key architectural design choices are presented without rationale, making the model architecture appear arbitrary to readers.", "claim_type": "methodology", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ppMpJyIYRZ", "reviewer": "Reviewer_m8JS", "review_text": "Summary: The paper introduces a method to enhance the out-of-distribution performance of graph neural networks (GNN) by learning to understand distribution shifts instead of addressing the assumed ones. To achieve this, the Mixture of Experts architecture is integrated into the GNN, supplemented by an alignment procedure to recognize the shift. Empirical experiments are conducted to validate the theoretical assertion.\n\nOn the whole, I believe the proposed method lacks the necessary motivation and its novelty isn't substantial enough to meet the standard.\n\nStrengths: - The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\n- Real-world datasets back the claims through experiments.\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\n\nWeaknesses: - The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \"previous research has concentrated on addressing specific types of distribution shifts.\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \"environment generators\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\n\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\n\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\n\nQuestions: Please check Weaknesses.", "labeling_timestamp": "2026-01-11T16:31:30.164919", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper provides explicit rationale for the architecture: it motivates a mixture-of-experts design by modeling distribution shifts as a mixture of stochastic transformations (Assumption 1), and explains the roles of the gating model and aligned experts for identifying mixture components and producing invariant representations (Introduction and Method). Thus the architecture is justified rather than presented arbitrarily.", "evidence": "We design a hierarchical architecture composed of a gating model and multiple expert models, inspired by the mixture-of-experts (MoE) architecture (Jordan & Jacobs, 1994). Specifically, as shown in Figure 1b, the gating model processes an input graph to pinpoint the critical mixture components that govern the given graph instance. Each expert model excels in generating invariant representations with respect to one kind of mixture component, while all of the experts are aligned in a common representation space to ensure model compatibility.", "section": "Introduction / 3 METHOD (Mixture components)"}
{"claim": "It is challenging to discern the model's functionality, underlying mechanism, or how it improves upon existing methods from the paper.", "claim_type": "presentation", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ppMpJyIYRZ", "reviewer": "Reviewer_m8JS", "review_text": "Summary: The paper introduces a method to enhance the out-of-distribution performance of graph neural networks (GNN) by learning to understand distribution shifts instead of addressing the assumed ones. To achieve this, the Mixture of Experts architecture is integrated into the GNN, supplemented by an alignment procedure to recognize the shift. Empirical experiments are conducted to validate the theoretical assertion.\n\nOn the whole, I believe the proposed method lacks the necessary motivation and its novelty isn't substantial enough to meet the standard.\n\nStrengths: - The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\n- Real-world datasets back the claims through experiments.\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\n\nWeaknesses: - The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \"previous research has concentrated on addressing specific types of distribution shifts.\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \"environment generators\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\n\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\n\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\n\nQuestions: Please check Weaknesses.", "labeling_timestamp": "2026-01-11T16:31:34.420819", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper clearly describes the model functionality and underlying mechanism (a hierarchical mixture-of-experts with a gating model, aligned experts, and aggregation) and reports quantitative improvements over baselines and prior methods. These descriptions and performance claims are presented in the Abstract, Introduction, and Method sections.", "evidence": "\"The core concept of GraphMETRO includes the construction of a hierarchical architecture composed of a gating model and multiple expert models that are aligned in a common representation space.\"; \"Specifically, the gating model identifies the significant mixture components that govern the distribution shift on a node or graph instance. Each aligned expert produces representations invariant to a type of mixture component. Finally, GraphMETRO aggregates the representations from multiple experts to produce an invariant representation w.r.t. the complex distribution shift for the prediction task.\"; \"Through the systematic experiments, we validate the effectiveness of GraphMETRO which outperforms Empirical Risk Minimization (ERM) by 4.6% averagely on synthetic distribution shifts and achieves state-of-the-art performances on four real-world datasets from GOOD benchmark, including a 67% and 4.2% relative improvement over the best previous method on WebKB and Twitch datasets.\"", "section": "Abstract; 1 Introduction; 3 METHOD"}
{"claim": "The manuscript provides insufficient implementation details for the specific model architectures used, hindering reproducibility.", "claim_type": "methodology", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ppMpJyIYRZ", "reviewer": "Reviewer_m8JS", "review_text": "Summary: The paper introduces a method to enhance the out-of-distribution performance of graph neural networks (GNN) by learning to understand distribution shifts instead of addressing the assumed ones. To achieve this, the Mixture of Experts architecture is integrated into the GNN, supplemented by an alignment procedure to recognize the shift. Empirical experiments are conducted to validate the theoretical assertion.\n\nOn the whole, I believe the proposed method lacks the necessary motivation and its novelty isn't substantial enough to meet the standard.\n\nStrengths: - The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\n- Real-world datasets back the claims through experiments.\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\n\nWeaknesses: - The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \"previous research has concentrated on addressing specific types of distribution shifts.\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \"environment generators\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\n\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\n\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\n\nQuestions: Please check Weaknesses.", "labeling_timestamp": "2026-01-11T16:31:44.034051", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper only provides high-level descriptions of the model (e.g., a gating model and multiple experts) but does not give concrete implementation details (specific architectures, layer sizes, hyperparameters, training procedures, or experimental setup) in the provided text, which would hinder reproducibility.", "evidence": "We design a hierarchical architecture composed of a gating model and multiple expert models, inspired by the mixture-of-experts (MoE) architecture (Jordan & Jacobs, 1994). Specifically, as shown in Figure 1b, the gating model processes an input graph to pinpoint the critical mixture components that govern the given graph instance. Each expert model excels in generating invariant representations with respect to one kind of mixture component, while all of the experts are aligned in a common representation space to ensure model compatibility.", "section": "1 INTRODUCTION"}
{"claim": "The paper lacks sufficient details on how stochastic transformations are implemented within the proposed method.", "claim_type": "methodology", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ppMpJyIYRZ", "reviewer": "Reviewer_m8JS", "review_text": "Summary: The paper introduces a method to enhance the out-of-distribution performance of graph neural networks (GNN) by learning to understand distribution shifts instead of addressing the assumed ones. To achieve this, the Mixture of Experts architecture is integrated into the GNN, supplemented by an alignment procedure to recognize the shift. Empirical experiments are conducted to validate the theoretical assertion.\n\nOn the whole, I believe the proposed method lacks the necessary motivation and its novelty isn't substantial enough to meet the standard.\n\nStrengths: - The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\n- Real-world datasets back the claims through experiments.\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\n\nWeaknesses: - The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \"previous research has concentrated on addressing specific types of distribution shifts.\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \"environment generators\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\n\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\n\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\n\nQuestions: Please check Weaknesses.", "labeling_timestamp": "2026-01-11T16:32:09.120466", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper states an assumption that distribution shifts are modeled via 'stochastic transformations' (Sec. 3.1) and mentions using a 'graph extrapolation technique' to construct mixture components (Introduction), but it does not provide concrete implementation details or descriptions of the specific stochastic transformations or how they are applied.", "evidence": ["Assumption 1 (An Equivalent Mixture for Distribution Shifts) ... We assume that the resulting shift in D_t can be modeled by the selective application of up to k out of K classes of stochastic transformations to each instance in the source distribution D_s (k < K).", "We leverage a graph extrapolation technique to construct these mixture components."], "section": "Introduction; 3.1 MIXTURE COMPONENTS"}
{"claim": "The paper omits important details about the optimization process used to train the model, such as specialized schedules or objectives.", "claim_type": "methodology", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ppMpJyIYRZ", "reviewer": "Reviewer_m8JS", "review_text": "Summary: The paper introduces a method to enhance the out-of-distribution performance of graph neural networks (GNN) by learning to understand distribution shifts instead of addressing the assumed ones. To achieve this, the Mixture of Experts architecture is integrated into the GNN, supplemented by an alignment procedure to recognize the shift. Empirical experiments are conducted to validate the theoretical assertion.\n\nOn the whole, I believe the proposed method lacks the necessary motivation and its novelty isn't substantial enough to meet the standard.\n\nStrengths: - The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\n- Real-world datasets back the claims through experiments.\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\n\nWeaknesses: - The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \"previous research has concentrated on addressing specific types of distribution shifts.\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \"environment generators\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\n\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\n\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\n\nQuestions: Please check Weaknesses.", "labeling_timestamp": "2026-01-11T16:31:57.537719", "model": "gpt-5-mini", "label": "Supported", "justification": "Across the provided sections (Abstract, Related Works, and Method) the paper mentions a novel training framework and objective but does not describe the optimization process, learning-rate/scheduling details, specialized training objectives, or other training protocol specifics. The Method section only states the ERM baseline and introduces assumptions but does not provide the GraphMETRO training objective or optimization/schedule details.", "evidence": "GraphMETRO is the first to design a mixture-of-expert model specifically tailored to address graph distribution shifts, coupled with a novel objective for producing invariant representations.", "section": "2 RELATED WORKS"}
{"claim": "Despite including shift learning and data augmentation components, the paper appears to rely on standard optimization routines without explaining why.", "claim_type": "methodology", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ppMpJyIYRZ", "reviewer": "Reviewer_m8JS", "review_text": "Summary: The paper introduces a method to enhance the out-of-distribution performance of graph neural networks (GNN) by learning to understand distribution shifts instead of addressing the assumed ones. To achieve this, the Mixture of Experts architecture is integrated into the GNN, supplemented by an alignment procedure to recognize the shift. Empirical experiments are conducted to validate the theoretical assertion.\n\nOn the whole, I believe the proposed method lacks the necessary motivation and its novelty isn't substantial enough to meet the standard.\n\nStrengths: - The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\n- Real-world datasets back the claims through experiments.\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\n\nWeaknesses: - The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \"previous research has concentrated on addressing specific types of distribution shifts.\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \"environment generators\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\n\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\n\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\n\nQuestions: Please check Weaknesses.", "labeling_timestamp": "2026-01-11T16:31:54.896528", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper mentions standard Empirical Risk Minimization (ERM) as the baseline training objective and otherwise describes architectural components (gating, experts, augmentation) and an informal assumption, but does not provide details or rationale for any specialized optimization routine or explain why standard optimization would be sufficient. Thus the reviewer's claim that the paper appears to rely on standard optimization routines without explaining why is supported by the paper text.", "evidence": "The standard approach is Empirical Risk Minimization (ERM) , i.e., where L denotes the loss function and y is the label of the graph G. It minimizes the average loss among all examples in the source domain.", "section": "3 METHOD (Problem formulation)"}
{"claim": "The claim that graph shift heterogeneity is under-explored is contradicted by a wide spectrum of existing approaches and literature.", "claim_type": "novelty", "paper_id": "QQ5eVDIMu4", "paper_title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ppMpJyIYRZ", "reviewer": "Reviewer_m8JS", "review_text": "Summary: The paper introduces a method to enhance the out-of-distribution performance of graph neural networks (GNN) by learning to understand distribution shifts instead of addressing the assumed ones. To achieve this, the Mixture of Experts architecture is integrated into the GNN, supplemented by an alignment procedure to recognize the shift. Empirical experiments are conducted to validate the theoretical assertion.\n\nOn the whole, I believe the proposed method lacks the necessary motivation and its novelty isn't substantial enough to meet the standard.\n\nStrengths: - The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\n- Real-world datasets back the claims through experiments.\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\n\nWeaknesses: - The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \"previous research has concentrated on addressing specific types of distribution shifts.\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \"environment generators\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\n\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\n\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\n\nQuestions: Please check Weaknesses.", "labeling_timestamp": "2026-01-11T16:32:10.333767", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly argues that prior work focuses on specific shift types or group/environment-level patterns and does not model nuanced, instance-wise heterogeneity. It even states GraphMETRO is the first MoE tailored to address graph distribution shifts, so the reviewer's claim that a \"wide spectrum\" of existing approaches already contradicts the paper's claim is not supported by the paper.", "evidence": "Introduction: \"A line of previous research has focused on addressing specific types of distribution shifts. For example, works have looked at distribution shifts related to graph size (...) feature noise (...) and node degree or local structure (...). However, these approaches are highly limited as the distribution shift types could be multiple in the real-world datasets and may not be consistent with the presumed distribution shifts. While previous invariant learning methods on graphs (...) is able to accommodate multiple distribution shifts inferred from data environments, they focus on common patterns within each environment and do not explicitly model the variety across node or graph instances.\" Related Works: \"However, this line of research focuses on group patterns without explicitly considering nuanced (instance-wise) distribution shifts, making its applicability limited.\" Also: \"GraphMETRO is the first to design a mixture-of-expert model specifically tailored to address graph distribution shifts...\"", "section": "1 INTRODUCTION; 2 RELATED WORKS"}
{"claim": "The authors primarily apply their physical compatibility optimization to enhance 3D models produced by off-the-shelf methods, instead of reconstructing 3D objects directly from a single input image.", "claim_type": "methodology", "paper_id": "k29Iv0XrBF", "paper_title": "Physically Compatible 3D Object Modeling from a Single Image", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "RasXwPlfVF", "reviewer": "Reviewer_H9KU", "review_text": "Summary: This paper introduces a physical compatibility optimization framework for reconstructed objects from a single image. The approach considers mechanical properties, external forces, and rest-shape geometry, integrating static equilibrium as a hard constraint. This framework improves upon existing methods by ensuring the stability and accuracy of reconstructed objects under external influences. Quantitative and qualitative evaluations show enhancements in physical compatibility.\n\nStrengths: 1. Performance: The proposed method achieves state-of-the-art results. The experiments well validate the effectiveness of the proposed methods.\n\n2. Clarity: The paper is well-written and easy to follow.\n\n3. Technical Novelty: The main contributions of this paper are twofold: 1) They propose a physical compatibility optimization framework for 3D objects and decompose the mechanical properties, external forces, and rest-shape geometry. 2) They optimize the rest-shape geometry using predefined mechanical properties and external forces and ensure the object’s shape aligns with the target image when in static equilibrium.\n\nWeaknesses: In this paper, the authors primarily apply the physical compatibility optimization framework to enhance the physical attributes of 3D models obtained from existing methods rather than reconstructing them from a single image. Therefore, I think the title \"Physically Compatible 3D Object Modeling from a Single Image\" may not be suitable, as the focus lies on enhancing physically compatible modeling of 3D objects derived from off-the-shelf methods.\n\nQuestions: 1. All $\\mathbf{x}\\_{static}$ may be $\\mathbf{X}_{static}$ as the defination in line 89.\n2. In Some cases discussed in the paper, like the flamingo standing on one leg, should it be standable even after optimization? I think it should not be standable under gravity.\n3. Why do you evaluate the different off-the-shelf methods using connected components when this cannot demonstrate the superiority of your proposed method?", "labeling_timestamp": "2026-01-11T16:32:03.565938", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper states it reconstructs physical 3D objects from a single image (claims direct single-image reconstruction) while also explicitly reporting integration with five existing single-view reconstruction methods (indicating the approach is applied to enhance outputs of off-the-shelf methods). Thus the reviewer's claim is partially correct — the method is applied to enhance models from existing reconstructions, but the paper also frames and implements the approach as reconstructing from a single input image.", "evidence": "1) \"To bridge this gap, we explicitly decompose these attributes for reconstructing a physical object from a single image.\" 2) \"Our framework's versatility is demonstrated by its integration with five distinct single-view reconstruction methods, each employing unique geometry representations.\"", "section": "Introduction / Abstract"}
{"claim": "The paper title 'Physically Compatible 3D Object Modeling from a Single Image' is misleading because the work focuses on enhancing models from existing methods rather than full single-image reconstruction.", "claim_type": "methodology", "paper_id": "k29Iv0XrBF", "paper_title": "Physically Compatible 3D Object Modeling from a Single Image", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "RasXwPlfVF", "reviewer": "Reviewer_H9KU", "review_text": "Summary: This paper introduces a physical compatibility optimization framework for reconstructed objects from a single image. The approach considers mechanical properties, external forces, and rest-shape geometry, integrating static equilibrium as a hard constraint. This framework improves upon existing methods by ensuring the stability and accuracy of reconstructed objects under external influences. Quantitative and qualitative evaluations show enhancements in physical compatibility.\n\nStrengths: 1. Performance: The proposed method achieves state-of-the-art results. The experiments well validate the effectiveness of the proposed methods.\n\n2. Clarity: The paper is well-written and easy to follow.\n\n3. Technical Novelty: The main contributions of this paper are twofold: 1) They propose a physical compatibility optimization framework for 3D objects and decompose the mechanical properties, external forces, and rest-shape geometry. 2) They optimize the rest-shape geometry using predefined mechanical properties and external forces and ensure the object’s shape aligns with the target image when in static equilibrium.\n\nWeaknesses: In this paper, the authors primarily apply the physical compatibility optimization framework to enhance the physical attributes of 3D models obtained from existing methods rather than reconstructing them from a single image. Therefore, I think the title \"Physically Compatible 3D Object Modeling from a Single Image\" may not be suitable, as the focus lies on enhancing physically compatible modeling of 3D objects derived from off-the-shelf methods.\n\nQuestions: 1. All $\\mathbf{x}\\_{static}$ may be $\\mathbf{X}_{static}$ as the defination in line 89.\n2. In Some cases discussed in the paper, like the flamingo standing on one leg, should it be standable even after optimization? I think it should not be standable under gravity.\n3. Why do you evaluate the different off-the-shelf methods using connected components when this cannot demonstrate the superiority of your proposed method?", "labeling_timestamp": "2026-01-11T16:32:31.107877", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes an optimization framework that takes predefined mechanical properties and external forces and optimizes a rest-shape geometry (i.e., refines physical models), and explicitly states integration with five existing single-view reconstruction methods rather than presenting an end-to-end single-image reconstruction pipeline that infers all attributes from scratch (Abstract; Section 3).", "evidence": "1) \"Our framework's versatility is demonstrated by its integration with five distinct single-view reconstruction methods, each employing unique geometry representations.\" 2) \"Our framework treats the rest-shape geometry as the optimization variable, assuming that the mechanical properties and external forces are predefined as inputs. Fig. 2 illustrates the overall pipeline.\"", "section": "Abstract; 3 Approach (Section 3)"}
{"claim": "The manuscript uses inconsistent notation: 'x_static' appears where 'X_static' is defined on line 89, indicating a presentation or typographical error.", "claim_type": "presentation", "paper_id": "k29Iv0XrBF", "paper_title": "Physically Compatible 3D Object Modeling from a Single Image", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "RasXwPlfVF", "reviewer": "Reviewer_H9KU", "review_text": "Summary: This paper introduces a physical compatibility optimization framework for reconstructed objects from a single image. The approach considers mechanical properties, external forces, and rest-shape geometry, integrating static equilibrium as a hard constraint. This framework improves upon existing methods by ensuring the stability and accuracy of reconstructed objects under external influences. Quantitative and qualitative evaluations show enhancements in physical compatibility.\n\nStrengths: 1. Performance: The proposed method achieves state-of-the-art results. The experiments well validate the effectiveness of the proposed methods.\n\n2. Clarity: The paper is well-written and easy to follow.\n\n3. Technical Novelty: The main contributions of this paper are twofold: 1) They propose a physical compatibility optimization framework for 3D objects and decompose the mechanical properties, external forces, and rest-shape geometry. 2) They optimize the rest-shape geometry using predefined mechanical properties and external forces and ensure the object’s shape aligns with the target image when in static equilibrium.\n\nWeaknesses: In this paper, the authors primarily apply the physical compatibility optimization framework to enhance the physical attributes of 3D models obtained from existing methods rather than reconstructing them from a single image. Therefore, I think the title \"Physically Compatible 3D Object Modeling from a Single Image\" may not be suitable, as the focus lies on enhancing physically compatible modeling of 3D objects derived from off-the-shelf methods.\n\nQuestions: 1. All $\\mathbf{x}\\_{static}$ may be $\\mathbf{X}_{static}$ as the defination in line 89.\n2. In Some cases discussed in the paper, like the flamingo standing on one leg, should it be standable even after optimization? I think it should not be standable under gravity.\n3. Why do you evaluate the different off-the-shelf methods using connected components when this cannot demonstrate the superiority of your proposed method?", "labeling_timestamp": "2026-01-11T16:32:25.590946", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper consistently uses lowercase x for the deformed/static vertex positions and uppercase X for the rest-shape vertex positions. It defines M = (x, T), M_rest = (X_rest, T) and M_static = (x_static, T), so there is no evidence that 'x_static' was used where 'X_static' was defined.", "evidence": "\"The object is represented by a volumetric mesh, denoted as M = ( x , T ). Here, x ∈ R^{3N} represents the 3D positions of the vertices... The mesh in its rest-shape geometry ... is represented as M_rest = ( X_rest , T ). The input image depicts the static geometry ... denoted as M_static = ( x_static , T ).\"", "section": "3.1 Formulation of Physical Compatibility"}
{"claim": "The method may incorrectly deem inherently unstable poses (for example, a flamingo standing on one leg) as standable under gravity after optimization, which contradicts physical realism.", "claim_type": "methodology", "paper_id": "k29Iv0XrBF", "paper_title": "Physically Compatible 3D Object Modeling from a Single Image", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "RasXwPlfVF", "reviewer": "Reviewer_H9KU", "review_text": "Summary: This paper introduces a physical compatibility optimization framework for reconstructed objects from a single image. The approach considers mechanical properties, external forces, and rest-shape geometry, integrating static equilibrium as a hard constraint. This framework improves upon existing methods by ensuring the stability and accuracy of reconstructed objects under external influences. Quantitative and qualitative evaluations show enhancements in physical compatibility.\n\nStrengths: 1. Performance: The proposed method achieves state-of-the-art results. The experiments well validate the effectiveness of the proposed methods.\n\n2. Clarity: The paper is well-written and easy to follow.\n\n3. Technical Novelty: The main contributions of this paper are twofold: 1) They propose a physical compatibility optimization framework for 3D objects and decompose the mechanical properties, external forces, and rest-shape geometry. 2) They optimize the rest-shape geometry using predefined mechanical properties and external forces and ensure the object’s shape aligns with the target image when in static equilibrium.\n\nWeaknesses: In this paper, the authors primarily apply the physical compatibility optimization framework to enhance the physical attributes of 3D models obtained from existing methods rather than reconstructing them from a single image. Therefore, I think the title \"Physically Compatible 3D Object Modeling from a Single Image\" may not be suitable, as the focus lies on enhancing physically compatible modeling of 3D objects derived from off-the-shelf methods.\n\nQuestions: 1. All $\\mathbf{x}\\_{static}$ may be $\\mathbf{X}_{static}$ as the defination in line 89.\n2. In Some cases discussed in the paper, like the flamingo standing on one leg, should it be standable even after optimization? I think it should not be standable under gravity.\n3. Why do you evaluate the different off-the-shelf methods using connected components when this cannot demonstrate the superiority of your proposed method?", "labeling_timestamp": "2026-01-11T16:32:36.395695", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper assumes the input depicts an object in static equilibrium and models entities as solid objects, enforcing stability via a center-of-mass constraint. It does not discuss cases where observed poses are passively unstable (e.g., animals balancing actively) or analyze failure modes where the method might incorrectly make such poses appear standable. Therefore the reviewer claim is not directly confirmed nor explicitly contradicted by the paper.", "evidence": "\"Fundamentally, an image is more than a visual representation of an object: It captures a physical snapshot of the object in a state of static equilibrium, under the influence of real-world forces.\"; \"In our approach, we treat the entity depicted in the input image as a solid object.\"; \"In the second scenario, the loss function is L(x_static) = || proj_z(C(x_static)) - ˆC ||, ... Minimization of this function ensures the structural stability of M_static.\"", "section": "Introduction; 3.1 Formulation of Physical Compatibility"}
{"claim": "Evaluating different off-the-shelf methods using connected components is an inappropriate metric and cannot demonstrate the proposed method's superiority in physical compatibility.", "claim_type": "methodology", "paper_id": "k29Iv0XrBF", "paper_title": "Physically Compatible 3D Object Modeling from a Single Image", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "RasXwPlfVF", "reviewer": "Reviewer_H9KU", "review_text": "Summary: This paper introduces a physical compatibility optimization framework for reconstructed objects from a single image. The approach considers mechanical properties, external forces, and rest-shape geometry, integrating static equilibrium as a hard constraint. This framework improves upon existing methods by ensuring the stability and accuracy of reconstructed objects under external influences. Quantitative and qualitative evaluations show enhancements in physical compatibility.\n\nStrengths: 1. Performance: The proposed method achieves state-of-the-art results. The experiments well validate the effectiveness of the proposed methods.\n\n2. Clarity: The paper is well-written and easy to follow.\n\n3. Technical Novelty: The main contributions of this paper are twofold: 1) They propose a physical compatibility optimization framework for 3D objects and decompose the mechanical properties, external forces, and rest-shape geometry. 2) They optimize the rest-shape geometry using predefined mechanical properties and external forces and ensure the object’s shape aligns with the target image when in static equilibrium.\n\nWeaknesses: In this paper, the authors primarily apply the physical compatibility optimization framework to enhance the physical attributes of 3D models obtained from existing methods rather than reconstructing them from a single image. Therefore, I think the title \"Physically Compatible 3D Object Modeling from a Single Image\" may not be suitable, as the focus lies on enhancing physically compatible modeling of 3D objects derived from off-the-shelf methods.\n\nQuestions: 1. All $\\mathbf{x}\\_{static}$ may be $\\mathbf{X}_{static}$ as the defination in line 89.\n2. In Some cases discussed in the paper, like the flamingo standing on one leg, should it be standable even after optimization? I think it should not be standable under gravity.\n3. Why do you evaluate the different off-the-shelf methods using connected components when this cannot demonstrate the superiority of your proposed method?", "labeling_timestamp": "2026-01-11T16:32:24.927453", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper describes five specific evaluation metrics (image loss, stability under gravity, and FEM measures like integrity and structural robustness) and reports integration with five reconstruction methods, but it does not mention using 'connected components' as an evaluation metric nor discuss the appropriateness of such a metric. Therefore the paper does not provide information to confirm or refute the reviewer's claim.", "evidence": "For evaluation, we introduce five metrics designed to comprehensively assess the physical compatibility of the modeled 3D objects under simulation. These metrics include image loss between the rendered image of the modeled physical object and the input image, stability under gravity, as well as measures from finite element analysis, such as integrity and structural robustness.", "section": "Abstract"}
{"claim": "The paper's claim regarding the knowledge base is insufficiently clarified and lacks explanation of how the knowledge base is constructed or used.", "claim_type": "methodology", "paper_id": "aaYBsuGRne", "paper_title": "Understanding In-context Learning with a Pelican Soup Hypothesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "EIVr3uVfUn", "reviewer": "Reviewer_rNJR", "review_text": "Summary: This paper focuses on understanding the in-context learning ability of large language models. The authors propose the Pelican soup hypothesis. It explains the in-context learning ability originating from learning the knowledge via the next token prediction. To support this hypothesis, the authors build a dataset and demonstrate the linkage between linguistic phenomena and in-context learning.\n\nStrengths: This paper provides substantial numerical results to support the proposed hypothesis. The linguistic phenomena analysis is also interesting to the community. In addition, the built dataset may be of independent interest.\n\nWeaknesses: 1. The claim related to the knowledge base needs more clarification. The experiments in [1] demonstrate that input-output mapping is not very important to the ICL. If the label space is correct, LLMs can even implement efficient ICL given wrong mapping. However, this wrong mapping conflicts with the knowledge base. More discussions are needed here.\n\n2. In Section 5.1, some assumptions are presented, but there is a notable absence of justification for these assumptions within the paper. This absence makes it challenging to ascertain the realism of these assumptions.\n3. I would greatly appreciate further elucidation on the distinction between the hypothesis presented in this paper and that discussed in [2]. Specifically, the variance between the \"atomic elements of NLP tasks\" and \"a set of atom concepts\" requires additional clarification.\n\n4. It is advantageous to include more highly relevant works in the related works. For example, besides HMM, implicit Bayesian inference is modeled for ICL in many different data assumptions [3,4,5]. [6] also studies the optimization side of ICL.\n\n[1] Min S, Lyu X, Holtzman A, et al. Rethinking the role of demonstrations: What makes in-context learning work?[J]. arXiv preprint arXiv:2202.12837, 2022.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Jiang H. A latent space theory for emergent abilities in large language models[J]. arXiv preprint arXiv:2304.09960, 2023.\n\n[4] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[5] Wang X, Zhu W, Wang W Y. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning[J]. arXiv preprint arXiv:2301.11916, 2023.\n\n[6] Dai D, Sun Y, Dong L, et al. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers[J]. arXiv preprint arXiv:2212.10559, 2022.\n\nQuestions: Questions are specified in Weakness part.", "labeling_timestamp": "2026-01-11T16:32:44.858035", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper defines a commonsense knowledge base (KB) and states its formal role in the task formalism (Section 2) and hypothesizes that language models can acquire KB by modeling general text (Section 3.1). However, it does not provide concrete details or procedures for how such a KB is constructed, instantiated, or operationalized in experiments.", "evidence": "We posit that a knowledge base is indispensable for solving any NLP task as cognitive psychologists have suggested that semantics of words are based on the interrelation among concepts (Fodor, 1975; 2008; Siskind, 1996; Murphy, 2004). For example, the semantic of the verb 'move' is embodied in its implication on 'position'. A knowledge base that allows a system to utilize the semantics of the word 'move' may contain a rule: if X moved, then the position of X changed. Therefore, we follow the early formulation of AI (McCarthy, 1960; Croft, 1993) to include a 'commonsense' knowledge base KB in our formalism of NLP tasks.\n\nThat is, x belongs to class y_i if and only if based on the commonsense rules in KB and the instruction t, x entails z_i.\n\nSince people write articles based on similar KBs of commonsense, language models may be able to acquire the KB by modeling general text. Additionally, language models may learn to do reasoning with the rules in the KB, because articles generally contain statements that are logical and coherent and proceed like a proof induction process.", "section": "Section 2 and Section 3.1"}
{"claim": "The paper fails to discuss how conflicting input-output mappings that still allow in-context learning interact with the proposed knowledge-base explanation.", "claim_type": "methodology", "paper_id": "aaYBsuGRne", "paper_title": "Understanding In-context Learning with a Pelican Soup Hypothesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "EIVr3uVfUn", "reviewer": "Reviewer_rNJR", "review_text": "Summary: This paper focuses on understanding the in-context learning ability of large language models. The authors propose the Pelican soup hypothesis. It explains the in-context learning ability originating from learning the knowledge via the next token prediction. To support this hypothesis, the authors build a dataset and demonstrate the linkage between linguistic phenomena and in-context learning.\n\nStrengths: This paper provides substantial numerical results to support the proposed hypothesis. The linguistic phenomena analysis is also interesting to the community. In addition, the built dataset may be of independent interest.\n\nWeaknesses: 1. The claim related to the knowledge base needs more clarification. The experiments in [1] demonstrate that input-output mapping is not very important to the ICL. If the label space is correct, LLMs can even implement efficient ICL given wrong mapping. However, this wrong mapping conflicts with the knowledge base. More discussions are needed here.\n\n2. In Section 5.1, some assumptions are presented, but there is a notable absence of justification for these assumptions within the paper. This absence makes it challenging to ascertain the realism of these assumptions.\n3. I would greatly appreciate further elucidation on the distinction between the hypothesis presented in this paper and that discussed in [2]. Specifically, the variance between the \"atomic elements of NLP tasks\" and \"a set of atom concepts\" requires additional clarification.\n\n4. It is advantageous to include more highly relevant works in the related works. For example, besides HMM, implicit Bayesian inference is modeled for ICL in many different data assumptions [3,4,5]. [6] also studies the optimization side of ICL.\n\n[1] Min S, Lyu X, Holtzman A, et al. Rethinking the role of demonstrations: What makes in-context learning work?[J]. arXiv preprint arXiv:2202.12837, 2022.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Jiang H. A latent space theory for emergent abilities in large language models[J]. arXiv preprint arXiv:2304.09960, 2023.\n\n[4] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[5] Wang X, Zhu W, Wang W Y. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning[J]. arXiv preprint arXiv:2301.11916, 2023.\n\n[6] Dai D, Sun Y, Dong L, et al. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers[J]. arXiv preprint arXiv:2212.10559, 2022.\n\nQuestions: Questions are specified in Weakness part.", "labeling_timestamp": "2026-01-11T16:33:02.599638", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper formalizes class assignment as entailment from KB and input (§2) and discusses verbalizer/pronoun mismatches (§3.2, §4) but does not analyze how multiple or conflicting input-output mappings that nevertheless permit in-context learning would be reconciled with the KB-based formalism. There is no section that addresses conflicting mappings or their interaction with the proposed KB explanation.", "evidence": "\"That is, x belongs to class y i if and only if based on the commonsense rules in KB and the instruction t , x entails z i .\"", "section": "§2 A FORMALISM FOR NLP CLASSIFICATION TASKS"}
{"claim": "The manuscript does not reconcile prior findings that label-space correctness enables ICL even with wrong mappings with its own knowledge-base claims.", "claim_type": "subjective", "paper_id": "aaYBsuGRne", "paper_title": "Understanding In-context Learning with a Pelican Soup Hypothesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "EIVr3uVfUn", "reviewer": "Reviewer_rNJR", "review_text": "Summary: This paper focuses on understanding the in-context learning ability of large language models. The authors propose the Pelican soup hypothesis. It explains the in-context learning ability originating from learning the knowledge via the next token prediction. To support this hypothesis, the authors build a dataset and demonstrate the linkage between linguistic phenomena and in-context learning.\n\nStrengths: This paper provides substantial numerical results to support the proposed hypothesis. The linguistic phenomena analysis is also interesting to the community. In addition, the built dataset may be of independent interest.\n\nWeaknesses: 1. The claim related to the knowledge base needs more clarification. The experiments in [1] demonstrate that input-output mapping is not very important to the ICL. If the label space is correct, LLMs can even implement efficient ICL given wrong mapping. However, this wrong mapping conflicts with the knowledge base. More discussions are needed here.\n\n2. In Section 5.1, some assumptions are presented, but there is a notable absence of justification for these assumptions within the paper. This absence makes it challenging to ascertain the realism of these assumptions.\n3. I would greatly appreciate further elucidation on the distinction between the hypothesis presented in this paper and that discussed in [2]. Specifically, the variance between the \"atomic elements of NLP tasks\" and \"a set of atom concepts\" requires additional clarification.\n\n4. It is advantageous to include more highly relevant works in the related works. For example, besides HMM, implicit Bayesian inference is modeled for ICL in many different data assumptions [3,4,5]. [6] also studies the optimization side of ICL.\n\n[1] Min S, Lyu X, Holtzman A, et al. Rethinking the role of demonstrations: What makes in-context learning work?[J]. arXiv preprint arXiv:2202.12837, 2022.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Jiang H. A latent space theory for emergent abilities in large language models[J]. arXiv preprint arXiv:2304.09960, 2023.\n\n[4] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[5] Wang X, Zhu W, Wang W Y. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning[J]. arXiv preprint arXiv:2301.11916, 2023.\n\n[6] Dai D, Sun Y, Dong L, et al. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers[J]. arXiv preprint arXiv:2212.10559, 2022.\n\nQuestions: Questions are specified in Weakness part.", "labeling_timestamp": "2026-01-11T16:32:52.519460", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly discusses prior findings about arbitrary/irrelevant verbalizers and offers an explanation reconciling them with its KB-based account (see §3.2), and it further discusses verbalizer mismatch as a distribution shift in §4. Thus it does attempt to reconcile those prior findings with its knowledge-base claims.", "evidence": "\"This may explain why large language models are able to perform in-context learning with labels associated to irrelevant verbalizers (Wei et al., 2023) and also provide an alternative explanation for the linkage between in-context learning and emergent abilities found by Lu et al. (2023).\" \n\n\"Verbalizer Mismatch. In § 3.2, we argue that the pronouns can be the main source where language models learn the in-context learning ability. However, people do not use pronouns as verbalizers in general (Brown et al., 2020; Min et al., 2022a;b). Those commonly used verbalizers do not appear in the training data as frequently as pronouns either.\"", "section": "Section 3.2 and Section 4"}
{"claim": "Section 5.1 presents key assumptions without providing justification, derivation, or references supporting the realism of those assumptions.", "claim_type": "methodology", "paper_id": "aaYBsuGRne", "paper_title": "Understanding In-context Learning with a Pelican Soup Hypothesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "EIVr3uVfUn", "reviewer": "Reviewer_rNJR", "review_text": "Summary: This paper focuses on understanding the in-context learning ability of large language models. The authors propose the Pelican soup hypothesis. It explains the in-context learning ability originating from learning the knowledge via the next token prediction. To support this hypothesis, the authors build a dataset and demonstrate the linkage between linguistic phenomena and in-context learning.\n\nStrengths: This paper provides substantial numerical results to support the proposed hypothesis. The linguistic phenomena analysis is also interesting to the community. In addition, the built dataset may be of independent interest.\n\nWeaknesses: 1. The claim related to the knowledge base needs more clarification. The experiments in [1] demonstrate that input-output mapping is not very important to the ICL. If the label space is correct, LLMs can even implement efficient ICL given wrong mapping. However, this wrong mapping conflicts with the knowledge base. More discussions are needed here.\n\n2. In Section 5.1, some assumptions are presented, but there is a notable absence of justification for these assumptions within the paper. This absence makes it challenging to ascertain the realism of these assumptions.\n3. I would greatly appreciate further elucidation on the distinction between the hypothesis presented in this paper and that discussed in [2]. Specifically, the variance between the \"atomic elements of NLP tasks\" and \"a set of atom concepts\" requires additional clarification.\n\n4. It is advantageous to include more highly relevant works in the related works. For example, besides HMM, implicit Bayesian inference is modeled for ICL in many different data assumptions [3,4,5]. [6] also studies the optimization side of ICL.\n\n[1] Min S, Lyu X, Holtzman A, et al. Rethinking the role of demonstrations: What makes in-context learning work?[J]. arXiv preprint arXiv:2202.12837, 2022.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Jiang H. A latent space theory for emergent abilities in large language models[J]. arXiv preprint arXiv:2304.09960, 2023.\n\n[4] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[5] Wang X, Zhu W, Wang W Y. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning[J]. arXiv preprint arXiv:2301.11916, 2023.\n\n[6] Dai D, Sun Y, Dong L, et al. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers[J]. arXiv preprint arXiv:2212.10559, 2022.\n\nQuestions: Questions are specified in Weakness part.", "labeling_timestamp": "2026-01-11T16:33:13.946382", "model": "gpt-5-mini", "label": "Contradicted", "justification": "Section 5.1 explicitly states Assumption 1 and cites supporting work (e.g. Tarski 1956 for mapping sentences to logic, and cognitive-psychology references Fodor 1975/2008 and Piantadosi 2021), so the section does provide references and brief justification linking the assumptions to prior theory; therefore the reviewer claim that it provides no justification/references is contradicted.", "evidence": "Assumption 1 (Setup) . A sentence in natural languages can be mapped to a logic formula in a logic model (Tarski, 1956), e.g. propositional logic. The logic model has a set of variables Σ that serves as the set of atom concepts (as aligned with cognitive psychology theories, e.g. Fodor (1975; 2008); Piantadosi (2021)). Additionally, there is KB , which is a set of formulas in the logic model. Datasets in this language are generated based on the rules define", "section": "5.1 ASSUMPTIONS"}
{"claim": "The paper lacks empirical analysis or validation demonstrating that the assumptions in Section 5.1 hold in realistic data or model settings.", "claim_type": "experimental", "paper_id": "aaYBsuGRne", "paper_title": "Understanding In-context Learning with a Pelican Soup Hypothesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "EIVr3uVfUn", "reviewer": "Reviewer_rNJR", "review_text": "Summary: This paper focuses on understanding the in-context learning ability of large language models. The authors propose the Pelican soup hypothesis. It explains the in-context learning ability originating from learning the knowledge via the next token prediction. To support this hypothesis, the authors build a dataset and demonstrate the linkage between linguistic phenomena and in-context learning.\n\nStrengths: This paper provides substantial numerical results to support the proposed hypothesis. The linguistic phenomena analysis is also interesting to the community. In addition, the built dataset may be of independent interest.\n\nWeaknesses: 1. The claim related to the knowledge base needs more clarification. The experiments in [1] demonstrate that input-output mapping is not very important to the ICL. If the label space is correct, LLMs can even implement efficient ICL given wrong mapping. However, this wrong mapping conflicts with the knowledge base. More discussions are needed here.\n\n2. In Section 5.1, some assumptions are presented, but there is a notable absence of justification for these assumptions within the paper. This absence makes it challenging to ascertain the realism of these assumptions.\n3. I would greatly appreciate further elucidation on the distinction between the hypothesis presented in this paper and that discussed in [2]. Specifically, the variance between the \"atomic elements of NLP tasks\" and \"a set of atom concepts\" requires additional clarification.\n\n4. It is advantageous to include more highly relevant works in the related works. For example, besides HMM, implicit Bayesian inference is modeled for ICL in many different data assumptions [3,4,5]. [6] also studies the optimization side of ICL.\n\n[1] Min S, Lyu X, Holtzman A, et al. Rethinking the role of demonstrations: What makes in-context learning work?[J]. arXiv preprint arXiv:2202.12837, 2022.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Jiang H. A latent space theory for emergent abilities in large language models[J]. arXiv preprint arXiv:2304.09960, 2023.\n\n[4] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[5] Wang X, Zhu W, Wang W Y. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning[J]. arXiv preprint arXiv:2301.11916, 2023.\n\n[6] Dai D, Sun Y, Dong L, et al. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers[J]. arXiv preprint arXiv:2212.10559, 2022.\n\nQuestions: Questions are specified in Weakness part.", "labeling_timestamp": "2026-01-11T16:33:13.810757", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper does include some empirical validation (synthetic Calcutec experiments in §5 and a GPT-2 experiment on natural language in §6, plus digit-addition experiments in §7) that touch on assumptions related to coreference and absence-of-reasoning shifts. However, it does not provide a comprehensive empirical analysis showing that all of the concrete assumptions listed in §5.1 hold across realistic data or model settings, so the reviewer's claim is only partially true.", "evidence": "\"First, we synthesize a dataset called Calcutec that replicates the linguistic phenomena and show that language models trained with this dataset acquire in-context learning ability and benefit from chain-of-thought.\"; \"Second, our experiment of GPT-2 on some natural language tasks shows the linkage between one of the linguistic phenomena and in-context learning.\"; \"Finally, in §7, we study the distribution shift caused by the absence of reasoning steps with a digit addition task. We find that larger models are better at gaining an intuition that allows them to jump to a conclusion without reasoning steps.\"", "section": "Abstract; §5; §6; §7"}
{"claim": "The distinction between the paper's Pelican soup hypothesis and the hypothesis in Sanjeev & Anirudh (2023) is not clearly explained.", "claim_type": "presentation", "paper_id": "aaYBsuGRne", "paper_title": "Understanding In-context Learning with a Pelican Soup Hypothesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "EIVr3uVfUn", "reviewer": "Reviewer_rNJR", "review_text": "Summary: This paper focuses on understanding the in-context learning ability of large language models. The authors propose the Pelican soup hypothesis. It explains the in-context learning ability originating from learning the knowledge via the next token prediction. To support this hypothesis, the authors build a dataset and demonstrate the linkage between linguistic phenomena and in-context learning.\n\nStrengths: This paper provides substantial numerical results to support the proposed hypothesis. The linguistic phenomena analysis is also interesting to the community. In addition, the built dataset may be of independent interest.\n\nWeaknesses: 1. The claim related to the knowledge base needs more clarification. The experiments in [1] demonstrate that input-output mapping is not very important to the ICL. If the label space is correct, LLMs can even implement efficient ICL given wrong mapping. However, this wrong mapping conflicts with the knowledge base. More discussions are needed here.\n\n2. In Section 5.1, some assumptions are presented, but there is a notable absence of justification for these assumptions within the paper. This absence makes it challenging to ascertain the realism of these assumptions.\n3. I would greatly appreciate further elucidation on the distinction between the hypothesis presented in this paper and that discussed in [2]. Specifically, the variance between the \"atomic elements of NLP tasks\" and \"a set of atom concepts\" requires additional clarification.\n\n4. It is advantageous to include more highly relevant works in the related works. For example, besides HMM, implicit Bayesian inference is modeled for ICL in many different data assumptions [3,4,5]. [6] also studies the optimization side of ICL.\n\n[1] Min S, Lyu X, Holtzman A, et al. Rethinking the role of demonstrations: What makes in-context learning work?[J]. arXiv preprint arXiv:2202.12837, 2022.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Jiang H. A latent space theory for emergent abilities in large language models[J]. arXiv preprint arXiv:2304.09960, 2023.\n\n[4] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[5] Wang X, Zhu W, Wang W Y. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning[J]. arXiv preprint arXiv:2301.11916, 2023.\n\n[6] Dai D, Sun Y, Dong L, et al. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers[J]. arXiv preprint arXiv:2212.10559, 2022.\n\nQuestions: Questions are specified in Weakness part.", "labeling_timestamp": "2026-01-11T16:33:13.207698", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper describes and motivates the Pelican Soup Hypothesis (citing Xie et al. (2022) and the Pelican Soup game) but does not cite or discuss Sanjeev & Anirudh (2023). Because the paper does not address that prior work, there is insufficient information to judge whether a clear distinction was (or was not) provided.", "evidence": "\"We propose a hypothesis, the Pelican Soup Hypothesis , which is inspired by Xie et al. (2022) and a lateral thinking puzzle game, Pelican Soup.\"", "section": "1 INTRODUCTION"}
{"claim": "The difference between 'atomic elements of NLP tasks' and 'a set of atom concepts' is not adequately defined or exemplified in the manuscript.", "claim_type": "presentation", "paper_id": "aaYBsuGRne", "paper_title": "Understanding In-context Learning with a Pelican Soup Hypothesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "EIVr3uVfUn", "reviewer": "Reviewer_rNJR", "review_text": "Summary: This paper focuses on understanding the in-context learning ability of large language models. The authors propose the Pelican soup hypothesis. It explains the in-context learning ability originating from learning the knowledge via the next token prediction. To support this hypothesis, the authors build a dataset and demonstrate the linkage between linguistic phenomena and in-context learning.\n\nStrengths: This paper provides substantial numerical results to support the proposed hypothesis. The linguistic phenomena analysis is also interesting to the community. In addition, the built dataset may be of independent interest.\n\nWeaknesses: 1. The claim related to the knowledge base needs more clarification. The experiments in [1] demonstrate that input-output mapping is not very important to the ICL. If the label space is correct, LLMs can even implement efficient ICL given wrong mapping. However, this wrong mapping conflicts with the knowledge base. More discussions are needed here.\n\n2. In Section 5.1, some assumptions are presented, but there is a notable absence of justification for these assumptions within the paper. This absence makes it challenging to ascertain the realism of these assumptions.\n3. I would greatly appreciate further elucidation on the distinction between the hypothesis presented in this paper and that discussed in [2]. Specifically, the variance between the \"atomic elements of NLP tasks\" and \"a set of atom concepts\" requires additional clarification.\n\n4. It is advantageous to include more highly relevant works in the related works. For example, besides HMM, implicit Bayesian inference is modeled for ICL in many different data assumptions [3,4,5]. [6] also studies the optimization side of ICL.\n\n[1] Min S, Lyu X, Holtzman A, et al. Rethinking the role of demonstrations: What makes in-context learning work?[J]. arXiv preprint arXiv:2202.12837, 2022.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Jiang H. A latent space theory for emergent abilities in large language models[J]. arXiv preprint arXiv:2304.09960, 2023.\n\n[4] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[5] Wang X, Zhu W, Wang W Y. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning[J]. arXiv preprint arXiv:2301.11916, 2023.\n\n[6] Dai D, Sun Y, Dong L, et al. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers[J]. arXiv preprint arXiv:2212.10559, 2022.\n\nQuestions: Questions are specified in Weakness part.", "labeling_timestamp": "2026-01-11T16:33:19.766708", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper defines 'atom concepts' (variables Σ in a logic model) but does not introduce or define the term 'atomic elements of NLP tasks' nor explicitly contrast the two concepts. Thus the claimed lack of an adequate definition/exemplification of their difference is accurate.", "evidence": "Assumption 1 (Setup). A sentence in natural languages can be mapped to a logic formula in a logic model (Tarski, 1956), e.g. propositional logic. The logic model has a set of variables Σ that serves as the set of atom concepts ...", "section": "§5.1 Assumptions (and §2 formalism)"}
{"claim": "The related-work section omits several relevant models of implicit Bayesian inference for ICL, including Jiang 2023 and Zhang et al. 2023.", "claim_type": "presentation", "paper_id": "aaYBsuGRne", "paper_title": "Understanding In-context Learning with a Pelican Soup Hypothesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "EIVr3uVfUn", "reviewer": "Reviewer_rNJR", "review_text": "Summary: This paper focuses on understanding the in-context learning ability of large language models. The authors propose the Pelican soup hypothesis. It explains the in-context learning ability originating from learning the knowledge via the next token prediction. To support this hypothesis, the authors build a dataset and demonstrate the linkage between linguistic phenomena and in-context learning.\n\nStrengths: This paper provides substantial numerical results to support the proposed hypothesis. The linguistic phenomena analysis is also interesting to the community. In addition, the built dataset may be of independent interest.\n\nWeaknesses: 1. The claim related to the knowledge base needs more clarification. The experiments in [1] demonstrate that input-output mapping is not very important to the ICL. If the label space is correct, LLMs can even implement efficient ICL given wrong mapping. However, this wrong mapping conflicts with the knowledge base. More discussions are needed here.\n\n2. In Section 5.1, some assumptions are presented, but there is a notable absence of justification for these assumptions within the paper. This absence makes it challenging to ascertain the realism of these assumptions.\n3. I would greatly appreciate further elucidation on the distinction between the hypothesis presented in this paper and that discussed in [2]. Specifically, the variance between the \"atomic elements of NLP tasks\" and \"a set of atom concepts\" requires additional clarification.\n\n4. It is advantageous to include more highly relevant works in the related works. For example, besides HMM, implicit Bayesian inference is modeled for ICL in many different data assumptions [3,4,5]. [6] also studies the optimization side of ICL.\n\n[1] Min S, Lyu X, Holtzman A, et al. Rethinking the role of demonstrations: What makes in-context learning work?[J]. arXiv preprint arXiv:2202.12837, 2022.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Jiang H. A latent space theory for emergent abilities in large language models[J]. arXiv preprint arXiv:2304.09960, 2023.\n\n[4] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[5] Wang X, Zhu W, Wang W Y. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning[J]. arXiv preprint arXiv:2301.11916, 2023.\n\n[6] Dai D, Sun Y, Dong L, et al. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers[J]. arXiv preprint arXiv:2212.10559, 2022.\n\nQuestions: Questions are specified in Weakness part.", "labeling_timestamp": "2026-01-11T16:33:39.057531", "model": "gpt-5-mini", "label": "Supported", "justification": "The provided paper content and cited prior work (e.g., in the Introduction) do not mention Jiang 2023 or Zhang et al. 2023, and no related-work section including those works appears in the supplied text—so the claim that the related-work omits those models is supported by the paper content.", "evidence": "Following previous studies on language characteristics that lead to the in-context learning ability (Xie et al., 2022; Hahn &Goyal, 2023; Chan et al., 2022), we propose a Pelican Soup Hypothesis , aiming to provide a new perspective to understand the above phenomena.", "section": "1 INTRODUCTION"}
{"claim": "The paper does not cite or discuss optimization-based explanations for ICL, such as work showing language models perform implicit gradient descent (e.g., Dai et al. 2022).", "claim_type": "subjective", "paper_id": "aaYBsuGRne", "paper_title": "Understanding In-context Learning with a Pelican Soup Hypothesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "EIVr3uVfUn", "reviewer": "Reviewer_rNJR", "review_text": "Summary: This paper focuses on understanding the in-context learning ability of large language models. The authors propose the Pelican soup hypothesis. It explains the in-context learning ability originating from learning the knowledge via the next token prediction. To support this hypothesis, the authors build a dataset and demonstrate the linkage between linguistic phenomena and in-context learning.\n\nStrengths: This paper provides substantial numerical results to support the proposed hypothesis. The linguistic phenomena analysis is also interesting to the community. In addition, the built dataset may be of independent interest.\n\nWeaknesses: 1. The claim related to the knowledge base needs more clarification. The experiments in [1] demonstrate that input-output mapping is not very important to the ICL. If the label space is correct, LLMs can even implement efficient ICL given wrong mapping. However, this wrong mapping conflicts with the knowledge base. More discussions are needed here.\n\n2. In Section 5.1, some assumptions are presented, but there is a notable absence of justification for these assumptions within the paper. This absence makes it challenging to ascertain the realism of these assumptions.\n3. I would greatly appreciate further elucidation on the distinction between the hypothesis presented in this paper and that discussed in [2]. Specifically, the variance between the \"atomic elements of NLP tasks\" and \"a set of atom concepts\" requires additional clarification.\n\n4. It is advantageous to include more highly relevant works in the related works. For example, besides HMM, implicit Bayesian inference is modeled for ICL in many different data assumptions [3,4,5]. [6] also studies the optimization side of ICL.\n\n[1] Min S, Lyu X, Holtzman A, et al. Rethinking the role of demonstrations: What makes in-context learning work?[J]. arXiv preprint arXiv:2202.12837, 2022.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Jiang H. A latent space theory for emergent abilities in large language models[J]. arXiv preprint arXiv:2304.09960, 2023.\n\n[4] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[5] Wang X, Zhu W, Wang W Y. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning[J]. arXiv preprint arXiv:2301.11916, 2023.\n\n[6] Dai D, Sun Y, Dong L, et al. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers[J]. arXiv preprint arXiv:2212.10559, 2022.\n\nQuestions: Questions are specified in Weakness part.", "labeling_timestamp": "2026-01-11T16:33:43.227780", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper frames in-context learning as arising from linguistic phenomena and generalization under distribution shifts (the 'Pelican Soup Hypothesis') and cites related linguistic/phenomenon-based works, but the provided content contains no citation or discussion of optimization-based explanations (e.g., implicit gradient descent or Dai et al. 2022).", "evidence": "\"Following previous studies on language characteristics that lead to the in-context learning ability (Xie et al., 2022; Hahn &Goyal, 2023; Chan et al., 2022), we propose a Pelican Soup Hypothesis , aiming to provide a new perspective to understand the above phenomena.\" \n\n\"Therefore, we can view in-context learning as the language modeling task under distribution shifts, such as the absence of reasoning steps (§ 4).\"", "section": "Introduction and §4"}
{"claim": "The proposed approach is not the first to explore extraction of local and global explanations; prior works like GLANCE and Glocalx address similar problems.", "claim_type": "novelty", "paper_id": "UqZecMwLTo", "paper_title": "A Unified Concept-Based System for Local, Global, and Misclassification Explanations", "paper_venue": "iclr2024", "paper_decision": null, "decision": "Unknown", "review_id": "Xnu3bVVbeW", "reviewer": "Reviewer_DJR7", "review_text": "Summary: The authors propose a unified framework for generating both local and global concept based explanations. The proposed framework considers a surrogate model trained with superpixels of an image for generating concepts, which are used then used to extract local and global explanations.\n\nStrengths: - The proposed approach is innovative and can be used to generate and score the relevance of concepts\n- The illustrations of local explanations are well presented, nice framework and illustration figures\n- The paper is well written and easy to follow\n\nWeaknesses: - The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2]. \n- What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n- The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n- I'm unsure of how to interpret global explanations. Can you please provide more discussion for the same? \n- It is a bit unclear how the scores in Table 1 are computed, given the UCBS is fine-tuned on superpixel images; the scores like insertion, deletion, and faithfulness are expected to be high for UCBS, given they are on the manifold, while in case of LIME, SHAPE, GradCAM, and LRP the concepts are off-manifold resulting in lower score. Could you please justify this?\n  \n\n\n[1] Kori, A., Glocker, B. and Toni, F., 2022. GLANCE: Global to Local Architecture-Neutral Concept-based Explanations. _arXiv preprint arXiv:2207.01917_.\n\n[2] Setzu, M., Guidotti, R., Monreale, A., Turini, F., Pedreschi, D. and Giannotti, F., 2021. Glocalx-from local to global explanations of black box ai models. _Artificial Intelligence_, _294_, p.103457.\n\nQuestions: Please refer to weakness section", "labeling_timestamp": "2026-01-11T16:33:37.337004", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly claims UCBS is among the first unsupervised unified concept extraction frameworks and reports a scarcity of unified methods for both local and global explanations (it does not acknowledge GLANCE or Glocalx in the provided text). Thus the reviewer's claim that the approach is not first and that prior works like GLANCE/Glocalx address the same problem contradicts the paper's stated claim of novelty.", "evidence": "\"we propose a Unified Concept-Based System (UCBS) applicable to image data, which is to the best of our knowledge one of the first unsupervised unified concept extraction frameworks.\"", "section": "1 INTRODUCTION"}
{"claim": "The paper does not cite or compare its method to prior works GLANCE (2022) and Glocalx (2021), which are relevant baselines.", "claim_type": "baseline", "paper_id": "UqZecMwLTo", "paper_title": "A Unified Concept-Based System for Local, Global, and Misclassification Explanations", "paper_venue": "iclr2024", "paper_decision": null, "decision": "Unknown", "review_id": "Xnu3bVVbeW", "reviewer": "Reviewer_DJR7", "review_text": "Summary: The authors propose a unified framework for generating both local and global concept based explanations. The proposed framework considers a surrogate model trained with superpixels of an image for generating concepts, which are used then used to extract local and global explanations.\n\nStrengths: - The proposed approach is innovative and can be used to generate and score the relevance of concepts\n- The illustrations of local explanations are well presented, nice framework and illustration figures\n- The paper is well written and easy to follow\n\nWeaknesses: - The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2]. \n- What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n- The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n- I'm unsure of how to interpret global explanations. Can you please provide more discussion for the same? \n- It is a bit unclear how the scores in Table 1 are computed, given the UCBS is fine-tuned on superpixel images; the scores like insertion, deletion, and faithfulness are expected to be high for UCBS, given they are on the manifold, while in case of LIME, SHAPE, GradCAM, and LRP the concepts are off-manifold resulting in lower score. Could you please justify this?\n  \n\n\n[1] Kori, A., Glocker, B. and Toni, F., 2022. GLANCE: Global to Local Architecture-Neutral Concept-based Explanations. _arXiv preprint arXiv:2207.01917_.\n\n[2] Setzu, M., Guidotti, R., Monreale, A., Turini, F., Pedreschi, D. and Giannotti, F., 2021. Glocalx-from local to global explanations of black box ai models. _Artificial Intelligence_, _294_, p.103457.\n\nQuestions: Please refer to weakness section", "labeling_timestamp": "2026-01-11T16:34:00.667174", "model": "gpt-5-mini", "label": "Supported", "justification": "In the provided paper content neither GLANCE (2022) nor Glocalx (2021) are cited, and no experimental comparisons against these methods are described. The introduction lists related concept-based works (e.g., Achtibat et al., Mincu et al.) and refers to Appendix A for more related works, but the main text and methods/experiments contain no mention or comparison to GLANCE or Glocalx.", "evidence": "These challenges have led to a surge of interest in concept-based explanation methods Achtibat et al. (2022); Mincu et al. (2021), which aim to identify the significance of a group of pixels that form a cohesive and cognitively understandable concept.", "section": "1 INTRODUCTION"}
{"claim": "The manuscript does not provide a clear motivation for using an out-of-distribution dataset to train the surrogate model.", "claim_type": "presentation", "paper_id": "UqZecMwLTo", "paper_title": "A Unified Concept-Based System for Local, Global, and Misclassification Explanations", "paper_venue": "iclr2024", "paper_decision": null, "decision": "Unknown", "review_id": "Xnu3bVVbeW", "reviewer": "Reviewer_DJR7", "review_text": "Summary: The authors propose a unified framework for generating both local and global concept based explanations. The proposed framework considers a surrogate model trained with superpixels of an image for generating concepts, which are used then used to extract local and global explanations.\n\nStrengths: - The proposed approach is innovative and can be used to generate and score the relevance of concepts\n- The illustrations of local explanations are well presented, nice framework and illustration figures\n- The paper is well written and easy to follow\n\nWeaknesses: - The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2]. \n- What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n- The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n- I'm unsure of how to interpret global explanations. Can you please provide more discussion for the same? \n- It is a bit unclear how the scores in Table 1 are computed, given the UCBS is fine-tuned on superpixel images; the scores like insertion, deletion, and faithfulness are expected to be high for UCBS, given they are on the manifold, while in case of LIME, SHAPE, GradCAM, and LRP the concepts are off-manifold resulting in lower score. Could you please justify this?\n  \n\n\n[1] Kori, A., Glocker, B. and Toni, F., 2022. GLANCE: Global to Local Architecture-Neutral Concept-based Explanations. _arXiv preprint arXiv:2207.01917_.\n\n[2] Setzu, M., Guidotti, R., Monreale, A., Turini, F., Pedreschi, D. and Giannotti, F., 2021. Glocalx-from local to global explanations of black box ai models. _Artificial Intelligence_, _294_, p.103457.\n\nQuestions: Please refer to weakness section", "labeling_timestamp": "2026-01-11T16:33:48.704338", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly motivates using out-of-distribution (auxiliary) data: it cites Outlier Exposure as inspiration, argues that auxiliary/OOD samples help the surrogate learn better representations of the target and its concepts (by training a binary target-vs-others surrogate), and describes inclusion of OOD samples in the auxiliary dataset and training objective (Introduction and Section 3).", "evidence": "\"These challenges prompted us to explore ways to leverage the knowledge of trained networks to eliminate the need for predefined concept examples and external scoring modules. We drew inspiration from the Outlier Exposure technique in deep anomaly detection, which uses an auxiliary dataset to teach the network better representations of anomaly cases Mirzaei et al. (2022); Hendrycks et al. (2019). Indeed, we engage surrogate explainer networks and teach them the concepts' representations of the given target against some out-of-distribution samples from unrelated classes, rather than a multi-class classification approach. This allows the networks to thoroughly learn the target's objects as well as their informative concepts.\"; \"surrogate networks are fine-tuned as binary classifiers (target vs. others) using auxiliary datasets including a set of super-pixelated images of the target class, the original target images, and a set of out-of-distribution samples;\"", "section": "INTRODUCTION; 3 PROPOSED METHOD"}
{"claim": "The proposed approach only captures concepts describable as image parts and may fail to capture whole-image concepts such as lighting or global illumination.", "claim_type": "methodology", "paper_id": "UqZecMwLTo", "paper_title": "A Unified Concept-Based System for Local, Global, and Misclassification Explanations", "paper_venue": "iclr2024", "paper_decision": null, "decision": "Unknown", "review_id": "Xnu3bVVbeW", "reviewer": "Reviewer_DJR7", "review_text": "Summary: The authors propose a unified framework for generating both local and global concept based explanations. The proposed framework considers a surrogate model trained with superpixels of an image for generating concepts, which are used then used to extract local and global explanations.\n\nStrengths: - The proposed approach is innovative and can be used to generate and score the relevance of concepts\n- The illustrations of local explanations are well presented, nice framework and illustration figures\n- The paper is well written and easy to follow\n\nWeaknesses: - The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2]. \n- What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n- The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n- I'm unsure of how to interpret global explanations. Can you please provide more discussion for the same? \n- It is a bit unclear how the scores in Table 1 are computed, given the UCBS is fine-tuned on superpixel images; the scores like insertion, deletion, and faithfulness are expected to be high for UCBS, given they are on the manifold, while in case of LIME, SHAPE, GradCAM, and LRP the concepts are off-manifold resulting in lower score. Could you please justify this?\n  \n\n\n[1] Kori, A., Glocker, B. and Toni, F., 2022. GLANCE: Global to Local Architecture-Neutral Concept-based Explanations. _arXiv preprint arXiv:2207.01917_.\n\n[2] Setzu, M., Guidotti, R., Monreale, A., Turini, F., Pedreschi, D. and Giannotti, F., 2021. Glocalx-from local to global explanations of black box ai models. _Artificial Intelligence_, _294_, p.103457.\n\nQuestions: Please refer to weakness section", "labeling_timestamp": "2026-01-11T16:34:03.169651", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper defines concepts as superpixels/segments and builds both local and global concepts from those segments (local concepts are selected from segmented parts; global concepts are formed by clustering embeddings of local segments). There is no discussion of representing or scoring whole-image, non-local concepts such as lighting or global illumination, so the claim that the approach captures concepts describable as image parts and may miss whole-image concepts is supported.", "evidence": "Section 2: \"Each of these images can be segmented into k segments/superpixels, i.e., for sample x_c_i, a set of segments as S^c_{x_i} = { s^c_{i,1}, s^c_{i,2}, ..., s^c_{i,k} } is obtained. Each segment is padded with zero value up to the original input size and labeled with the corresponding target class.\" | Section 3: \"This method utilizes surrogate models and auxiliary datasets to identify the most influential part(s) of the images and is developed using a set of super-pixelated images of the given target class\" | Section 3 (Local concepts): \"To extract the local concepts of an unseen image x_i with respect to the class c, the image is first segmented and all its initial concepts are collected in S_{x_i}... the p highest-scoring ones are selected as the local concepts of sample x_i.\" | Section 3 (Concept extraction / global): \"the embeddings of a set of candidate local concepts, associated with a set of unseen images, are clustered and the g top groups, indicated with their best samples, are selected as the global concepts.\"", "section": "Section 3 PROPOSED METHOD (also Section 2 NOTATION)"}
{"claim": "The paper lacks sufficient explanation and guidance on how to interpret the reported global explanations.", "claim_type": "presentation", "paper_id": "UqZecMwLTo", "paper_title": "A Unified Concept-Based System for Local, Global, and Misclassification Explanations", "paper_venue": "iclr2024", "paper_decision": null, "decision": "Unknown", "review_id": "Xnu3bVVbeW", "reviewer": "Reviewer_DJR7", "review_text": "Summary: The authors propose a unified framework for generating both local and global concept based explanations. The proposed framework considers a surrogate model trained with superpixels of an image for generating concepts, which are used then used to extract local and global explanations.\n\nStrengths: - The proposed approach is innovative and can be used to generate and score the relevance of concepts\n- The illustrations of local explanations are well presented, nice framework and illustration figures\n- The paper is well written and easy to follow\n\nWeaknesses: - The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2]. \n- What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n- The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n- I'm unsure of how to interpret global explanations. Can you please provide more discussion for the same? \n- It is a bit unclear how the scores in Table 1 are computed, given the UCBS is fine-tuned on superpixel images; the scores like insertion, deletion, and faithfulness are expected to be high for UCBS, given they are on the manifold, while in case of LIME, SHAPE, GradCAM, and LRP the concepts are off-manifold resulting in lower score. Could you please justify this?\n  \n\n\n[1] Kori, A., Glocker, B. and Toni, F., 2022. GLANCE: Global to Local Architecture-Neutral Concept-based Explanations. _arXiv preprint arXiv:2207.01917_.\n\n[2] Setzu, M., Guidotti, R., Monreale, A., Turini, F., Pedreschi, D. and Giannotti, F., 2021. Glocalx-from local to global explanations of black box ai models. _Artificial Intelligence_, _294_, p.103457.\n\nQuestions: Please refer to weakness section", "labeling_timestamp": "2026-01-11T16:34:06.859541", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper describes how global concepts are formed (clustering embeddings of candidate local concepts and selecting top groups) and states that these concept sets can serve as explanations, but it provides limited guidance on how to interpret the reported global explanations beyond selection/formation (no detailed interpretive procedure or usage guidelines in the provided sections).", "evidence": "\"To identify the concepts, the p top concepts of each image are selected as the local ones, and the embeddings of a set of candidate local concepts, associated with a set of unseen images, are clustered and the g top groups, indicated with their best samples, are selected as the global concepts.\" \"These concept sets can serve as explanations for the network's performance.\"", "section": "3 PROPOSED METHOD (Figure 1 caption and Concept extraction paragraph)"}
{"claim": "It is unclear how the scores reported in Table 1 are computed, given UCBS is fine-tuned on superpixel images.", "claim_type": "experimental", "paper_id": "UqZecMwLTo", "paper_title": "A Unified Concept-Based System for Local, Global, and Misclassification Explanations", "paper_venue": "iclr2024", "paper_decision": null, "decision": "Unknown", "review_id": "Xnu3bVVbeW", "reviewer": "Reviewer_DJR7", "review_text": "Summary: The authors propose a unified framework for generating both local and global concept based explanations. The proposed framework considers a surrogate model trained with superpixels of an image for generating concepts, which are used then used to extract local and global explanations.\n\nStrengths: - The proposed approach is innovative and can be used to generate and score the relevance of concepts\n- The illustrations of local explanations are well presented, nice framework and illustration figures\n- The paper is well written and easy to follow\n\nWeaknesses: - The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2]. \n- What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n- The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n- I'm unsure of how to interpret global explanations. Can you please provide more discussion for the same? \n- It is a bit unclear how the scores in Table 1 are computed, given the UCBS is fine-tuned on superpixel images; the scores like insertion, deletion, and faithfulness are expected to be high for UCBS, given they are on the manifold, while in case of LIME, SHAPE, GradCAM, and LRP the concepts are off-manifold resulting in lower score. Could you please justify this?\n  \n\n\n[1] Kori, A., Glocker, B. and Toni, F., 2022. GLANCE: Global to Local Architecture-Neutral Concept-based Explanations. _arXiv preprint arXiv:2207.01917_.\n\n[2] Setzu, M., Guidotti, R., Monreale, A., Turini, F., Pedreschi, D. and Giannotti, F., 2021. Glocalx-from local to global explanations of black box ai models. _Artificial Intelligence_, _294_, p.103457.\n\nQuestions: Please refer to weakness section", "labeling_timestamp": "2026-01-11T16:34:06.373012", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly defines how concept scores are computed: segments (superpixels) are passed through the fine-tuned surrogate network NetS and the importance score is taken from the network's pre-softmax output for the target class (raw output values preserved). This directly describes the scoring procedure, so the claim that it is unclear is contradicted.", "evidence": "\"Concept scoring Initial concepts must be prioritized based on their contributions to the proper predictions. To this end, UCBS utilizes the knowledge of the adapted surrogate models. Considering the aforementioned framework, the importance score of segment s_i,j ∈ S_x_i, with respect to the target class c, is defined using NetS as follows: ... in which f′,c_θ returns the pre-softmax output associated with the given target class. The raw output values were considered to preserve the original magnitude of the scores.\"", "section": "3 PROPOSED METHOD — Concept scoring"}
{"claim": "Because UCBS is fine-tuned on superpixel images, insertion, deletion, and faithfulness scores may be artificially inflated for UCBS compared to other methods.", "claim_type": "methodology", "paper_id": "UqZecMwLTo", "paper_title": "A Unified Concept-Based System for Local, Global, and Misclassification Explanations", "paper_venue": "iclr2024", "paper_decision": null, "decision": "Unknown", "review_id": "Xnu3bVVbeW", "reviewer": "Reviewer_DJR7", "review_text": "Summary: The authors propose a unified framework for generating both local and global concept based explanations. The proposed framework considers a surrogate model trained with superpixels of an image for generating concepts, which are used then used to extract local and global explanations.\n\nStrengths: - The proposed approach is innovative and can be used to generate and score the relevance of concepts\n- The illustrations of local explanations are well presented, nice framework and illustration figures\n- The paper is well written and easy to follow\n\nWeaknesses: - The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2]. \n- What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n- The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n- I'm unsure of how to interpret global explanations. Can you please provide more discussion for the same? \n- It is a bit unclear how the scores in Table 1 are computed, given the UCBS is fine-tuned on superpixel images; the scores like insertion, deletion, and faithfulness are expected to be high for UCBS, given they are on the manifold, while in case of LIME, SHAPE, GradCAM, and LRP the concepts are off-manifold resulting in lower score. Could you please justify this?\n  \n\n\n[1] Kori, A., Glocker, B. and Toni, F., 2022. GLANCE: Global to Local Architecture-Neutral Concept-based Explanations. _arXiv preprint arXiv:2207.01917_.\n\n[2] Setzu, M., Guidotti, R., Monreale, A., Turini, F., Pedreschi, D. and Giannotti, F., 2021. Glocalx-from local to global explanations of black box ai models. _Artificial Intelligence_, _294_, p.103457.\n\nQuestions: Please refer to weakness section", "labeling_timestamp": "2026-01-11T16:34:19.300685", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper clearly states UCBS is fine-tuned using super-pixelated images (supporting the first part of the claim). However, the paper does not state or provide evidence that insertion, deletion, or faithfulness scores are artificially inflated by this procedure, nor does it discuss such an evaluation bias (so the consequential claim is not supported by the paper).", "evidence": "“This method utilizes surrogate models and auxiliary datasets to identify the most influential part(s) of the images and is developed using a set of super-pixelated images of the given target class, feeding the DNNs' training/fine-tuning process.”\n\n“For the zero-pixel areas of the super-pixelated images, the output of the convolution operation as well as the max or the average pooling, is zero. This prevents these regions' weights from being updated during the back-propagation operation (under some activation functions like relu or sigmoid) ... having only the weights associated with each segment updated assists in better learning of those regions.”", "section": "3 PROPOSED METHOD (Concept learning)"}
{"claim": "Comparative methods like LIME, SHAP, GradCAM, and LRP produce off-manifold concepts, which likely leads to lower evaluation scores than on-manifold UCBS.", "claim_type": "baseline", "paper_id": "UqZecMwLTo", "paper_title": "A Unified Concept-Based System for Local, Global, and Misclassification Explanations", "paper_venue": "iclr2024", "paper_decision": null, "decision": "Unknown", "review_id": "Xnu3bVVbeW", "reviewer": "Reviewer_DJR7", "review_text": "Summary: The authors propose a unified framework for generating both local and global concept based explanations. The proposed framework considers a surrogate model trained with superpixels of an image for generating concepts, which are used then used to extract local and global explanations.\n\nStrengths: - The proposed approach is innovative and can be used to generate and score the relevance of concepts\n- The illustrations of local explanations are well presented, nice framework and illustration figures\n- The paper is well written and easy to follow\n\nWeaknesses: - The proposed approach is certainly not the first to explore the extraction of local and global explanations, please refer to [1, 2]. \n- What is the motivation for having the out-of-distribution dataset to train the surrogate model?\n- The proposed approach only captures the concepts that can be described as a part of an image. Is it possible to capture concepts like lighting that are expressed in an entire image?\n- I'm unsure of how to interpret global explanations. Can you please provide more discussion for the same? \n- It is a bit unclear how the scores in Table 1 are computed, given the UCBS is fine-tuned on superpixel images; the scores like insertion, deletion, and faithfulness are expected to be high for UCBS, given they are on the manifold, while in case of LIME, SHAPE, GradCAM, and LRP the concepts are off-manifold resulting in lower score. Could you please justify this?\n  \n\n\n[1] Kori, A., Glocker, B. and Toni, F., 2022. GLANCE: Global to Local Architecture-Neutral Concept-based Explanations. _arXiv preprint arXiv:2207.01917_.\n\n[2] Setzu, M., Guidotti, R., Monreale, A., Turini, F., Pedreschi, D. and Giannotti, F., 2021. Glocalx-from local to global explanations of black box ai models. _Artificial Intelligence_, _294_, p.103457.\n\nQuestions: Please refer to weakness section", "labeling_timestamp": "2026-01-11T16:34:31.511434", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper criticizes external scoring tools as vulnerable to perturbed inputs and potentially unfaithful (Introduction), and it proposes UCBS as an alternative that uses surrogate networks and auxiliary data to learn concepts. However, it does not explicitly state that LIME, SHAP, GradCAM, or LRP produce \"off-manifold concepts,\" nor does it directly claim that such off-manifold concepts cause those methods to obtain lower evaluation scores than UCBS.", "evidence": "\"a recent study has shown that external scoring tools may be vulnerable methods against the perturbed input samples Uesato et al. (2018); Brown & Kvinge (2021). Additionally, they may mistakenly overestimate the concepts' importance, which makes it potentially unfaithful and sensitive to irrelevant concepts Schrouff et al. (2021).\"", "section": "1 INTRODUCTION"}
{"claim": "The explanation of the VQ-KD tokenizer in the paper remains insufficient and unclear.", "claim_type": "presentation", "paper_id": "RMmgu49lwn", "paper_title": "Image Understanding Makes for A Good Tokenizer for Image Generation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Q5Or46ndhS", "reviewer": "Reviewer_aFBf", "review_text": "Comment: I really appreciate the author's additional experiments on additional datasets, which better prove the advantages of the method. The author addressed most of my concerns, but I still have some concerns about the explanation of VQ-KD tokenizer.\nI think it is better to keep the original rating.", "labeling_timestamp": "2026-01-11T16:34:19.750861", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper provides a dedicated explanation of VQ-KD in Section 3.2 (Image Tokenizers), describing its objective (feature reconstruction / knowledge distillation from a pretrained teacher), the reconstruction loss (based on cosine similarity), and the feature adapter implementation, and it also references Figure 3c and prior mentions in the abstract/introduction. Thus the claim that the explanation 'remains insufficient and unclear' is not supported by the paper text provided.", "evidence": "1) \"This approach distills knowledge from pretrained IU encoders to tokenizers.\" (Abstract/Introduction). 2) \"Unlike VQGAN and FSQ, which are designed for IG, VQ-KD was originally presented in BEiT v2 to provide supervision for IU models. As shown in Fig. 3c, VQ-KD is trained to reconstruct the feature map x_T encoded by a pretrained teacher T'. Formally, the reconstruction loss is defined as: [formula]. where cos(·,·) represents cosine similarity and σ is a feature adapter. σ is implemented as a decoder, which maps C(z) to the same feature space as x_T.\" (Section 3.2 Image Tokenizers).", "section": "3.2 Image Tokenizers (also Abstract/Introduction)"}
{"claim": "The sentence in line 50 ('That is, the prediction is a single (potentially high-dimensional) point (or maybe a small number of such points).') is hard to read.", "claim_type": "presentation", "paper_id": "JEKXTLjEIq", "paper_title": "Binary Search with Distributional Predictions", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BELGkUQqxK", "reviewer": "Reviewer_kf39", "review_text": "Summary: This paper proposes a learning-augmented algorithm for searching in a sorted array. Different from all previous learning-augmented algorithms, it takes in distributional predictions. The main result is an algorithm with query complexity $O(H(p) + \\log \\eta)$, where $H(p)$ is the entropy of the true distribution and $\\eta$ is the Earth Mover's distance between the true and predicted distributions. The paper also includes proofs to show the theoretical optimality and experiments to validate the practical usefulness.\n\nStrengths: - The paper follows the recent line of work on \"learning-augmented algorithm\", or \"algorithm with predictions\". This is a promising new direction that tries to combine the theoretical soundness of classic algorithms with the learning ability of machine learning algorithms.\n- The section on theoretical analysis (though the ideas are simple) is effective.\n- The paper includes experimental results to back up the theory. The experimental settings are diverse. The performance of the proposed algorithm is strong compared to all the baselines.\n\nWeaknesses: - I think the presentation of the paper can be greatly improved. To list a few points:\n    - in line 50, the sentence \"That is, the prediction is a single (potentially high-dimensional) point (or maybe a small number of such points).\" is hard to read for me.\n    - in line 56, the sentence \"Or, can we in fact do better by taking full advantage of the entire predicted distribution.\" should end with a question mark.\n    - the abusive use of the word \"essentially\" greatly weakens the soundness of the paper (For example, in lines 66, 70, 78 87, 88). The expression \"essentially optimal\" should be clarified with collaboration on complexity and constants.\n- I think several key works in the field of learning-augmented algorithms are missing, which makes it hard to position this paper in the correct context. For example, I think the algorithms proposed in \"Learning-Augmented Binary Search Trees\" by Lin et al should be at least discussed and even compared against (now, this paper is only mentioned as a very general reference for learning-augmented data structures).\n- It is not straightforward to me why the techniques used in the proposed algorithm are novel and not hard to come up with. I encourage the author to make a clearer point on \"which components of the proposed algorithm are novel and different from existing techniques\".\n\nQuestions: - The algorithm in [Mitzenmacher and Vassilvitskii, 2021] that searches in a sorted list with predictions receive separate predictions for each query. Is this also the case for the setting discussed in this paper? \n    - If so, as I understand, the proposed algorithm needs to rebuild the binary search tree every time it receives a new query along with its distributional predictions. Then, this would lower bound the time complexity to answer each query with $O(n)$. Is that correct?\n- I am confused by the reference to \"binary search tree\" in the paper (even in the title). Does the proposed algorithm actually require building a binary search tree in its specification and implementation? Why does section 3 not contain any explanation related to the binary search tree?", "labeling_timestamp": "2026-01-11T16:34:25.017667", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper contains the quoted sentence, but whether it is \"hard to read\" is a subjective assessment not addressed in the paper itself. The document does not comment on the readability of its own sentences, so this claim cannot be verified from the paper content.", "evidence": "That is, the prediction is a single (potentially high-dimensional) point (or maybe a small number of such points).", "section": "1 Introduction"}
{"claim": "The sentence in line 56 ('Or, can we in fact do better by taking full advantage of the entire predicted distribution.') should end with a question mark.", "claim_type": "presentation", "paper_id": "JEKXTLjEIq", "paper_title": "Binary Search with Distributional Predictions", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BELGkUQqxK", "reviewer": "Reviewer_kf39", "review_text": "Summary: This paper proposes a learning-augmented algorithm for searching in a sorted array. Different from all previous learning-augmented algorithms, it takes in distributional predictions. The main result is an algorithm with query complexity $O(H(p) + \\log \\eta)$, where $H(p)$ is the entropy of the true distribution and $\\eta$ is the Earth Mover's distance between the true and predicted distributions. The paper also includes proofs to show the theoretical optimality and experiments to validate the practical usefulness.\n\nStrengths: - The paper follows the recent line of work on \"learning-augmented algorithm\", or \"algorithm with predictions\". This is a promising new direction that tries to combine the theoretical soundness of classic algorithms with the learning ability of machine learning algorithms.\n- The section on theoretical analysis (though the ideas are simple) is effective.\n- The paper includes experimental results to back up the theory. The experimental settings are diverse. The performance of the proposed algorithm is strong compared to all the baselines.\n\nWeaknesses: - I think the presentation of the paper can be greatly improved. To list a few points:\n    - in line 50, the sentence \"That is, the prediction is a single (potentially high-dimensional) point (or maybe a small number of such points).\" is hard to read for me.\n    - in line 56, the sentence \"Or, can we in fact do better by taking full advantage of the entire predicted distribution.\" should end with a question mark.\n    - the abusive use of the word \"essentially\" greatly weakens the soundness of the paper (For example, in lines 66, 70, 78 87, 88). The expression \"essentially optimal\" should be clarified with collaboration on complexity and constants.\n- I think several key works in the field of learning-augmented algorithms are missing, which makes it hard to position this paper in the correct context. For example, I think the algorithms proposed in \"Learning-Augmented Binary Search Trees\" by Lin et al should be at least discussed and even compared against (now, this paper is only mentioned as a very general reference for learning-augmented data structures).\n- It is not straightforward to me why the techniques used in the proposed algorithm are novel and not hard to come up with. I encourage the author to make a clearer point on \"which components of the proposed algorithm are novel and different from existing techniques\".\n\nQuestions: - The algorithm in [Mitzenmacher and Vassilvitskii, 2021] that searches in a sorted list with predictions receive separate predictions for each query. Is this also the case for the setting discussed in this paper? \n    - If so, as I understand, the proposed algorithm needs to rebuild the binary search tree every time it receives a new query along with its distributional predictions. Then, this would lower bound the time complexity to answer each query with $O(n)$. Is that correct?\n- I am confused by the reference to \"binary search tree\" in the paper (even in the title). Does the proposed algorithm actually require building a binary search tree in its specification and implementation? Why does section 3 not contain any explanation related to the binary search tree?", "labeling_timestamp": "2026-01-11T16:34:21.781004", "model": "gpt-5-mini", "label": "Supported", "justification": "In the paper's Introduction the sentence in question ends with a period rather than a question mark, so the reviewer's suggestion is correct.", "evidence": "But is that always the right thing to do? Or, can we in fact do better by taking full advantage of the entire predicted distribution.", "section": "1 Introduction"}
{"claim": "The paper repeatedly uses the word 'essentially' (lines 66, 70, 78, 87, 88), which weakens the clarity and soundness of its claims.", "claim_type": "presentation", "paper_id": "JEKXTLjEIq", "paper_title": "Binary Search with Distributional Predictions", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BELGkUQqxK", "reviewer": "Reviewer_kf39", "review_text": "Summary: This paper proposes a learning-augmented algorithm for searching in a sorted array. Different from all previous learning-augmented algorithms, it takes in distributional predictions. The main result is an algorithm with query complexity $O(H(p) + \\log \\eta)$, where $H(p)$ is the entropy of the true distribution and $\\eta$ is the Earth Mover's distance between the true and predicted distributions. The paper also includes proofs to show the theoretical optimality and experiments to validate the practical usefulness.\n\nStrengths: - The paper follows the recent line of work on \"learning-augmented algorithm\", or \"algorithm with predictions\". This is a promising new direction that tries to combine the theoretical soundness of classic algorithms with the learning ability of machine learning algorithms.\n- The section on theoretical analysis (though the ideas are simple) is effective.\n- The paper includes experimental results to back up the theory. The experimental settings are diverse. The performance of the proposed algorithm is strong compared to all the baselines.\n\nWeaknesses: - I think the presentation of the paper can be greatly improved. To list a few points:\n    - in line 50, the sentence \"That is, the prediction is a single (potentially high-dimensional) point (or maybe a small number of such points).\" is hard to read for me.\n    - in line 56, the sentence \"Or, can we in fact do better by taking full advantage of the entire predicted distribution.\" should end with a question mark.\n    - the abusive use of the word \"essentially\" greatly weakens the soundness of the paper (For example, in lines 66, 70, 78 87, 88). The expression \"essentially optimal\" should be clarified with collaboration on complexity and constants.\n- I think several key works in the field of learning-augmented algorithms are missing, which makes it hard to position this paper in the correct context. For example, I think the algorithms proposed in \"Learning-Augmented Binary Search Trees\" by Lin et al should be at least discussed and even compared against (now, this paper is only mentioned as a very general reference for learning-augmented data structures).\n- It is not straightforward to me why the techniques used in the proposed algorithm are novel and not hard to come up with. I encourage the author to make a clearer point on \"which components of the proposed algorithm are novel and different from existing techniques\".\n\nQuestions: - The algorithm in [Mitzenmacher and Vassilvitskii, 2021] that searches in a sorted list with predictions receive separate predictions for each query. Is this also the case for the setting discussed in this paper? \n    - If so, as I understand, the proposed algorithm needs to rebuild the binary search tree every time it receives a new query along with its distributional predictions. Then, this would lower bound the time complexity to answer each query with $O(n)$. Is that correct?\n- I am confused by the reference to \"binary search tree\" in the paper (even in the title). Does the proposed algorithm actually require building a binary search tree in its specification and implementation? Why does section 3 not contain any explanation related to the binary search tree?", "labeling_timestamp": "2026-01-11T16:35:00.856222", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper does repeatedly use the word \"essentially\" in multiple places (Abstract, Introduction, and Section 1.1), so that part of the reviewer claim is accurate. However, the paper does not itself assert that these uses weaken clarity or soundness; that is a subjective judgment by the reviewer and is not supported or demonstrated within the paper text.", "evidence": ["we complement this with a lower bound showing that this query complexity is essentially optimal (up to constants)", "This is in fact essentially optimal: it is known that every algorithm requires at least H ( p ) / 3 queries in expectation when target keys are drawn from p [Mehlhorn, 1975].", "if our prediction ˆ p is close to p , then our algorithm has performance essentially equal to the best possible bound H ( p ) .", "essentially all previous work studies the case in which the prediction is a single point"], "section": "Abstract; Introduction; Section 1.1"}
{"claim": "The phrase 'essentially optimal' is used without clarifying algorithmic complexity, hidden constants, or precise approximation guarantees.", "claim_type": "methodology", "paper_id": "JEKXTLjEIq", "paper_title": "Binary Search with Distributional Predictions", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BELGkUQqxK", "reviewer": "Reviewer_kf39", "review_text": "Summary: This paper proposes a learning-augmented algorithm for searching in a sorted array. Different from all previous learning-augmented algorithms, it takes in distributional predictions. The main result is an algorithm with query complexity $O(H(p) + \\log \\eta)$, where $H(p)$ is the entropy of the true distribution and $\\eta$ is the Earth Mover's distance between the true and predicted distributions. The paper also includes proofs to show the theoretical optimality and experiments to validate the practical usefulness.\n\nStrengths: - The paper follows the recent line of work on \"learning-augmented algorithm\", or \"algorithm with predictions\". This is a promising new direction that tries to combine the theoretical soundness of classic algorithms with the learning ability of machine learning algorithms.\n- The section on theoretical analysis (though the ideas are simple) is effective.\n- The paper includes experimental results to back up the theory. The experimental settings are diverse. The performance of the proposed algorithm is strong compared to all the baselines.\n\nWeaknesses: - I think the presentation of the paper can be greatly improved. To list a few points:\n    - in line 50, the sentence \"That is, the prediction is a single (potentially high-dimensional) point (or maybe a small number of such points).\" is hard to read for me.\n    - in line 56, the sentence \"Or, can we in fact do better by taking full advantage of the entire predicted distribution.\" should end with a question mark.\n    - the abusive use of the word \"essentially\" greatly weakens the soundness of the paper (For example, in lines 66, 70, 78 87, 88). The expression \"essentially optimal\" should be clarified with collaboration on complexity and constants.\n- I think several key works in the field of learning-augmented algorithms are missing, which makes it hard to position this paper in the correct context. For example, I think the algorithms proposed in \"Learning-Augmented Binary Search Trees\" by Lin et al should be at least discussed and even compared against (now, this paper is only mentioned as a very general reference for learning-augmented data structures).\n- It is not straightforward to me why the techniques used in the proposed algorithm are novel and not hard to come up with. I encourage the author to make a clearer point on \"which components of the proposed algorithm are novel and different from existing techniques\".\n\nQuestions: - The algorithm in [Mitzenmacher and Vassilvitskii, 2021] that searches in a sorted list with predictions receive separate predictions for each query. Is this also the case for the setting discussed in this paper? \n    - If so, as I understand, the proposed algorithm needs to rebuild the binary search tree every time it receives a new query along with its distributional predictions. Then, this would lower bound the time complexity to answer each query with $O(n)$. Is that correct?\n- I am confused by the reference to \"binary search tree\" in the paper (even in the title). Does the proposed algorithm actually require building a binary search tree in its specification and implementation? Why does section 3 not contain any explanation related to the binary search tree?", "labeling_timestamp": "2026-01-11T16:34:52.046130", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper does claim the algorithm is \"essentially optimal (up to constants)\" and gives asymptotic bounds O(H(p) + log η) and matching lower bounds (Ω(log η) and known Ω(H(p))). However, it does not state the explicit hidden constants or a precise approximation factor for the combined bound—only asymptotic/\"up to constants\" phrasing—so the reviewer's criticism is partially correct.", "evidence": "Abstract: \"We complement this with a lower bound showing that this query complexity is essentially optimal (up to constants)\"; Section 1.1: \"we give an algorithm with query complexity O ( H ( p ) + log η )\"; Section 1.1 (Worst case lower bound): \"no algorithm can use fewer than Ω(log η) queries in the worst case. Since Ω( H ( p )) is a known lower bound as well ... this implies that our algorithm is asymptotically tight.\"", "section": "Abstract; Section 1.1 (Our Results and Contributions)"}
{"claim": "The paper omits discussion and empirical or theoretical comparison with 'Learning-Augmented Binary Search Trees' by Lin et al., a directly relevant recent work.", "claim_type": "subjective", "paper_id": "JEKXTLjEIq", "paper_title": "Binary Search with Distributional Predictions", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BELGkUQqxK", "reviewer": "Reviewer_kf39", "review_text": "Summary: This paper proposes a learning-augmented algorithm for searching in a sorted array. Different from all previous learning-augmented algorithms, it takes in distributional predictions. The main result is an algorithm with query complexity $O(H(p) + \\log \\eta)$, where $H(p)$ is the entropy of the true distribution and $\\eta$ is the Earth Mover's distance between the true and predicted distributions. The paper also includes proofs to show the theoretical optimality and experiments to validate the practical usefulness.\n\nStrengths: - The paper follows the recent line of work on \"learning-augmented algorithm\", or \"algorithm with predictions\". This is a promising new direction that tries to combine the theoretical soundness of classic algorithms with the learning ability of machine learning algorithms.\n- The section on theoretical analysis (though the ideas are simple) is effective.\n- The paper includes experimental results to back up the theory. The experimental settings are diverse. The performance of the proposed algorithm is strong compared to all the baselines.\n\nWeaknesses: - I think the presentation of the paper can be greatly improved. To list a few points:\n    - in line 50, the sentence \"That is, the prediction is a single (potentially high-dimensional) point (or maybe a small number of such points).\" is hard to read for me.\n    - in line 56, the sentence \"Or, can we in fact do better by taking full advantage of the entire predicted distribution.\" should end with a question mark.\n    - the abusive use of the word \"essentially\" greatly weakens the soundness of the paper (For example, in lines 66, 70, 78 87, 88). The expression \"essentially optimal\" should be clarified with collaboration on complexity and constants.\n- I think several key works in the field of learning-augmented algorithms are missing, which makes it hard to position this paper in the correct context. For example, I think the algorithms proposed in \"Learning-Augmented Binary Search Trees\" by Lin et al should be at least discussed and even compared against (now, this paper is only mentioned as a very general reference for learning-augmented data structures).\n- It is not straightforward to me why the techniques used in the proposed algorithm are novel and not hard to come up with. I encourage the author to make a clearer point on \"which components of the proposed algorithm are novel and different from existing techniques\".\n\nQuestions: - The algorithm in [Mitzenmacher and Vassilvitskii, 2021] that searches in a sorted list with predictions receive separate predictions for each query. Is this also the case for the setting discussed in this paper? \n    - If so, as I understand, the proposed algorithm needs to rebuild the binary search tree every time it receives a new query along with its distributional predictions. Then, this would lower bound the time complexity to answer each query with $O(n)$. Is that correct?\n- I am confused by the reference to \"binary search tree\" in the paper (even in the title). Does the proposed algorithm actually require building a binary search tree in its specification and implementation? Why does section 3 not contain any explanation related to the binary search tree?", "labeling_timestamp": "2026-01-11T16:34:37.182238", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper does mention Lin et al. [2022] in the related work (so discussion is not entirely omitted) but provides only a brief one-sentence remark and no detailed empirical or theoretical comparison is presented in the provided content.", "evidence": "“Particularly relevant to our setting is the work of Lin et al. [2022], which initiated the study of predictions for binary search trees. This work investigates how to improve a treap's guarantees when item frequencies follow distributions such as the Zipfian distribution.”", "section": "1.2 Other Related Work"}
{"claim": "Several other key works in learning-augmented algorithms are missing, making it difficult to position this paper within the literature.", "claim_type": "subjective", "paper_id": "JEKXTLjEIq", "paper_title": "Binary Search with Distributional Predictions", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BELGkUQqxK", "reviewer": "Reviewer_kf39", "review_text": "Summary: This paper proposes a learning-augmented algorithm for searching in a sorted array. Different from all previous learning-augmented algorithms, it takes in distributional predictions. The main result is an algorithm with query complexity $O(H(p) + \\log \\eta)$, where $H(p)$ is the entropy of the true distribution and $\\eta$ is the Earth Mover's distance between the true and predicted distributions. The paper also includes proofs to show the theoretical optimality and experiments to validate the practical usefulness.\n\nStrengths: - The paper follows the recent line of work on \"learning-augmented algorithm\", or \"algorithm with predictions\". This is a promising new direction that tries to combine the theoretical soundness of classic algorithms with the learning ability of machine learning algorithms.\n- The section on theoretical analysis (though the ideas are simple) is effective.\n- The paper includes experimental results to back up the theory. The experimental settings are diverse. The performance of the proposed algorithm is strong compared to all the baselines.\n\nWeaknesses: - I think the presentation of the paper can be greatly improved. To list a few points:\n    - in line 50, the sentence \"That is, the prediction is a single (potentially high-dimensional) point (or maybe a small number of such points).\" is hard to read for me.\n    - in line 56, the sentence \"Or, can we in fact do better by taking full advantage of the entire predicted distribution.\" should end with a question mark.\n    - the abusive use of the word \"essentially\" greatly weakens the soundness of the paper (For example, in lines 66, 70, 78 87, 88). The expression \"essentially optimal\" should be clarified with collaboration on complexity and constants.\n- I think several key works in the field of learning-augmented algorithms are missing, which makes it hard to position this paper in the correct context. For example, I think the algorithms proposed in \"Learning-Augmented Binary Search Trees\" by Lin et al should be at least discussed and even compared against (now, this paper is only mentioned as a very general reference for learning-augmented data structures).\n- It is not straightforward to me why the techniques used in the proposed algorithm are novel and not hard to come up with. I encourage the author to make a clearer point on \"which components of the proposed algorithm are novel and different from existing techniques\".\n\nQuestions: - The algorithm in [Mitzenmacher and Vassilvitskii, 2021] that searches in a sorted list with predictions receive separate predictions for each query. Is this also the case for the setting discussed in this paper? \n    - If so, as I understand, the proposed algorithm needs to rebuild the binary search tree every time it receives a new query along with its distributional predictions. Then, this would lower bound the time complexity to answer each query with $O(n)$. Is that correct?\n- I am confused by the reference to \"binary search tree\" in the paper (even in the title). Does the proposed algorithm actually require building a binary search tree in its specification and implementation? Why does section 3 not contain any explanation related to the binary search tree?", "labeling_timestamp": "2026-01-11T16:35:04.162140", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper includes an explicit related work section (Section 1.2) that cites many foundational and recent works in learning-augmented algorithms and explicitly positions the current work relative to them (e.g., Lykouris & Vassilvitskii, Mitzenmacher & Vassilvitskii, Lin et al.). Thus the claim that several key works are missing and the paper is difficult to position is contradicted by the paper's own related-work discussion.", "evidence": "“Machine learning augmented algorithms have found applications in various areas ... Due to the vast literature, we only provide a few samples.”\n\n“Particularly relevant to our setting is the work of Lin et al. [2022], which initiated the study of predictions for binary search trees.”\n\n“... the formal study of this setting was pioneered by Lykouris and Vassilvitskii [2021], and was particularly motivated by the practical success of learned index structures [Kraska et al., 2018].”", "section": "1.2 Other Related Work (and Introduction)"}
{"claim": "The paper does not clearly state which components of the proposed algorithm are novel versus which follow existing techniques.", "claim_type": "novelty", "paper_id": "JEKXTLjEIq", "paper_title": "Binary Search with Distributional Predictions", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BELGkUQqxK", "reviewer": "Reviewer_kf39", "review_text": "Summary: This paper proposes a learning-augmented algorithm for searching in a sorted array. Different from all previous learning-augmented algorithms, it takes in distributional predictions. The main result is an algorithm with query complexity $O(H(p) + \\log \\eta)$, where $H(p)$ is the entropy of the true distribution and $\\eta$ is the Earth Mover's distance between the true and predicted distributions. The paper also includes proofs to show the theoretical optimality and experiments to validate the practical usefulness.\n\nStrengths: - The paper follows the recent line of work on \"learning-augmented algorithm\", or \"algorithm with predictions\". This is a promising new direction that tries to combine the theoretical soundness of classic algorithms with the learning ability of machine learning algorithms.\n- The section on theoretical analysis (though the ideas are simple) is effective.\n- The paper includes experimental results to back up the theory. The experimental settings are diverse. The performance of the proposed algorithm is strong compared to all the baselines.\n\nWeaknesses: - I think the presentation of the paper can be greatly improved. To list a few points:\n    - in line 50, the sentence \"That is, the prediction is a single (potentially high-dimensional) point (or maybe a small number of such points).\" is hard to read for me.\n    - in line 56, the sentence \"Or, can we in fact do better by taking full advantage of the entire predicted distribution.\" should end with a question mark.\n    - the abusive use of the word \"essentially\" greatly weakens the soundness of the paper (For example, in lines 66, 70, 78 87, 88). The expression \"essentially optimal\" should be clarified with collaboration on complexity and constants.\n- I think several key works in the field of learning-augmented algorithms are missing, which makes it hard to position this paper in the correct context. For example, I think the algorithms proposed in \"Learning-Augmented Binary Search Trees\" by Lin et al should be at least discussed and even compared against (now, this paper is only mentioned as a very general reference for learning-augmented data structures).\n- It is not straightforward to me why the techniques used in the proposed algorithm are novel and not hard to come up with. I encourage the author to make a clearer point on \"which components of the proposed algorithm are novel and different from existing techniques\".\n\nQuestions: - The algorithm in [Mitzenmacher and Vassilvitskii, 2021] that searches in a sorted list with predictions receive separate predictions for each query. Is this also the case for the setting discussed in this paper? \n    - If so, as I understand, the proposed algorithm needs to rebuild the binary search tree every time it receives a new query along with its distributional predictions. Then, this would lower bound the time complexity to answer each query with $O(n)$. Is that correct?\n- I am confused by the reference to \"binary search tree\" in the paper (even in the title). Does the proposed algorithm actually require building a binary search tree in its specification and implementation? Why does section 3 not contain any explanation related to the binary search tree?", "labeling_timestamp": "2026-01-11T16:34:58.965771", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly cites existing components (the known 'median'/'bisection' algorithm and classical binary search) and states the new contribution as an algorithm that interleaves these components, as well as other novel results (lower bound, reduction analysis, distributionally-robust BST). Thus it does indicate which parts are prior work and which are the authors' contributions.", "evidence": "\"we recall the 'median' or 'bisection' algorithm (first described by Knuth [1971] and analyzed by Mehlhorn [1975])... the expected query complexity ... is bounded by H(p)+1\"; \"we give our main result in Section 3: an algorithm which interleaves phases of the 'median' algorithm and classical binary search to obtain a query complexity of O(H(p) + log η)...\"; \"This also yields the first distributionally-robust algorithm for the classical problem of computing an optimal binary search tree given a distribution over target keys.\"", "section": "1.1 Our Results and Contributions"}
{"claim": "It is unclear whether the paper's setting provides separate predictions per query, as in Mitzenmacher and Vassilvitskii (2021).", "claim_type": "methodology", "paper_id": "JEKXTLjEIq", "paper_title": "Binary Search with Distributional Predictions", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BELGkUQqxK", "reviewer": "Reviewer_kf39", "review_text": "Summary: This paper proposes a learning-augmented algorithm for searching in a sorted array. Different from all previous learning-augmented algorithms, it takes in distributional predictions. The main result is an algorithm with query complexity $O(H(p) + \\log \\eta)$, where $H(p)$ is the entropy of the true distribution and $\\eta$ is the Earth Mover's distance between the true and predicted distributions. The paper also includes proofs to show the theoretical optimality and experiments to validate the practical usefulness.\n\nStrengths: - The paper follows the recent line of work on \"learning-augmented algorithm\", or \"algorithm with predictions\". This is a promising new direction that tries to combine the theoretical soundness of classic algorithms with the learning ability of machine learning algorithms.\n- The section on theoretical analysis (though the ideas are simple) is effective.\n- The paper includes experimental results to back up the theory. The experimental settings are diverse. The performance of the proposed algorithm is strong compared to all the baselines.\n\nWeaknesses: - I think the presentation of the paper can be greatly improved. To list a few points:\n    - in line 50, the sentence \"That is, the prediction is a single (potentially high-dimensional) point (or maybe a small number of such points).\" is hard to read for me.\n    - in line 56, the sentence \"Or, can we in fact do better by taking full advantage of the entire predicted distribution.\" should end with a question mark.\n    - the abusive use of the word \"essentially\" greatly weakens the soundness of the paper (For example, in lines 66, 70, 78 87, 88). The expression \"essentially optimal\" should be clarified with collaboration on complexity and constants.\n- I think several key works in the field of learning-augmented algorithms are missing, which makes it hard to position this paper in the correct context. For example, I think the algorithms proposed in \"Learning-Augmented Binary Search Trees\" by Lin et al should be at least discussed and even compared against (now, this paper is only mentioned as a very general reference for learning-augmented data structures).\n- It is not straightforward to me why the techniques used in the proposed algorithm are novel and not hard to come up with. I encourage the author to make a clearer point on \"which components of the proposed algorithm are novel and different from existing techniques\".\n\nQuestions: - The algorithm in [Mitzenmacher and Vassilvitskii, 2021] that searches in a sorted list with predictions receive separate predictions for each query. Is this also the case for the setting discussed in this paper? \n    - If so, as I understand, the proposed algorithm needs to rebuild the binary search tree every time it receives a new query along with its distributional predictions. Then, this would lower bound the time complexity to answer each query with $O(n)$. Is that correct?\n- I am confused by the reference to \"binary search tree\" in the paper (even in the title). Does the proposed algorithm actually require building a binary search tree in its specification and implementation? Why does section 3 not contain any explanation related to the binary search tree?", "labeling_timestamp": "2026-01-11T16:35:07.080249", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states a setting where each target key arrives with its own predicted distribution (a per-query prediction) and also discusses the alternate setting of a single predicted distribution for designing a BST, so the issue is clarified rather than unclear.", "evidence": "“We have so far discussed a distributional prediction setting where a target key arrives with a predicted distribution ˆp of its location. This is a strict generalization of Mitzenmacher and Vassilvitskii [2021], where the prediction is a single location in the array.”", "section": "1.1 Our Results and Contributions"}
{"claim": "If separate predictions are provided per query, the proposed algorithm appears to require rebuilding a binary search tree for each query, implying O(n) time per query.", "claim_type": "methodology", "paper_id": "JEKXTLjEIq", "paper_title": "Binary Search with Distributional Predictions", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BELGkUQqxK", "reviewer": "Reviewer_kf39", "review_text": "Summary: This paper proposes a learning-augmented algorithm for searching in a sorted array. Different from all previous learning-augmented algorithms, it takes in distributional predictions. The main result is an algorithm with query complexity $O(H(p) + \\log \\eta)$, where $H(p)$ is the entropy of the true distribution and $\\eta$ is the Earth Mover's distance between the true and predicted distributions. The paper also includes proofs to show the theoretical optimality and experiments to validate the practical usefulness.\n\nStrengths: - The paper follows the recent line of work on \"learning-augmented algorithm\", or \"algorithm with predictions\". This is a promising new direction that tries to combine the theoretical soundness of classic algorithms with the learning ability of machine learning algorithms.\n- The section on theoretical analysis (though the ideas are simple) is effective.\n- The paper includes experimental results to back up the theory. The experimental settings are diverse. The performance of the proposed algorithm is strong compared to all the baselines.\n\nWeaknesses: - I think the presentation of the paper can be greatly improved. To list a few points:\n    - in line 50, the sentence \"That is, the prediction is a single (potentially high-dimensional) point (or maybe a small number of such points).\" is hard to read for me.\n    - in line 56, the sentence \"Or, can we in fact do better by taking full advantage of the entire predicted distribution.\" should end with a question mark.\n    - the abusive use of the word \"essentially\" greatly weakens the soundness of the paper (For example, in lines 66, 70, 78 87, 88). The expression \"essentially optimal\" should be clarified with collaboration on complexity and constants.\n- I think several key works in the field of learning-augmented algorithms are missing, which makes it hard to position this paper in the correct context. For example, I think the algorithms proposed in \"Learning-Augmented Binary Search Trees\" by Lin et al should be at least discussed and even compared against (now, this paper is only mentioned as a very general reference for learning-augmented data structures).\n- It is not straightforward to me why the techniques used in the proposed algorithm are novel and not hard to come up with. I encourage the author to make a clearer point on \"which components of the proposed algorithm are novel and different from existing techniques\".\n\nQuestions: - The algorithm in [Mitzenmacher and Vassilvitskii, 2021] that searches in a sorted list with predictions receive separate predictions for each query. Is this also the case for the setting discussed in this paper? \n    - If so, as I understand, the proposed algorithm needs to rebuild the binary search tree every time it receives a new query along with its distributional predictions. Then, this would lower bound the time complexity to answer each query with $O(n)$. Is that correct?\n- I am confused by the reference to \"binary search tree\" in the paper (even in the title). Does the proposed algorithm actually require building a binary search tree in its specification and implementation? Why does section 3 not contain any explanation related to the binary search tree?", "labeling_timestamp": "2026-01-11T16:35:39.634774", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper describes an online algorithm for the per-query prediction setting that interleaves median-based and classical binary search and gives a per-query comparison bound of O(H(p) + log η) (Section 1.1). It does not state that a binary search tree must be rebuilt per query; moreover, for the single-distribution setting the paper explicitly says a BST can be computed efficiently once given ˆp. Thus the claim that the proposed algorithm requires rebuilding a BST for each query (implying O(n) time per query) is not supported and is contradicted by the paper.", "evidence": "Main algorithm. We then give our main result in Section 3: an algorithm which interleaves phases of the 'median' algorithm and classical binary search to obtain a query complexity of O ( H ( p ) + log η ) , where η is the earth mover's distance (EMD, also known as the Wasserstein W 1 metric) between p and ˆ p ...\n\nDistributional Robustness of Optimal Binary Search Trees. ... So we can interpret our results as providing distributionally-robust optimal BSTs: given ˆ p , we can efficiently compute a BST where the expected lookup time under the true (but unknown) query distribution p is at most O ( H ( p ) + log η ) .", "section": "1.1 Our Results and Contributions / Distributional Robustness of Optimal Binary Search Trees"}
{"claim": "The paper references a 'binary search tree' in the title and text, but Section 3 contains no explanation of binary search trees or their role in the algorithm.", "claim_type": "methodology", "paper_id": "JEKXTLjEIq", "paper_title": "Binary Search with Distributional Predictions", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BELGkUQqxK", "reviewer": "Reviewer_kf39", "review_text": "Summary: This paper proposes a learning-augmented algorithm for searching in a sorted array. Different from all previous learning-augmented algorithms, it takes in distributional predictions. The main result is an algorithm with query complexity $O(H(p) + \\log \\eta)$, where $H(p)$ is the entropy of the true distribution and $\\eta$ is the Earth Mover's distance between the true and predicted distributions. The paper also includes proofs to show the theoretical optimality and experiments to validate the practical usefulness.\n\nStrengths: - The paper follows the recent line of work on \"learning-augmented algorithm\", or \"algorithm with predictions\". This is a promising new direction that tries to combine the theoretical soundness of classic algorithms with the learning ability of machine learning algorithms.\n- The section on theoretical analysis (though the ideas are simple) is effective.\n- The paper includes experimental results to back up the theory. The experimental settings are diverse. The performance of the proposed algorithm is strong compared to all the baselines.\n\nWeaknesses: - I think the presentation of the paper can be greatly improved. To list a few points:\n    - in line 50, the sentence \"That is, the prediction is a single (potentially high-dimensional) point (or maybe a small number of such points).\" is hard to read for me.\n    - in line 56, the sentence \"Or, can we in fact do better by taking full advantage of the entire predicted distribution.\" should end with a question mark.\n    - the abusive use of the word \"essentially\" greatly weakens the soundness of the paper (For example, in lines 66, 70, 78 87, 88). The expression \"essentially optimal\" should be clarified with collaboration on complexity and constants.\n- I think several key works in the field of learning-augmented algorithms are missing, which makes it hard to position this paper in the correct context. For example, I think the algorithms proposed in \"Learning-Augmented Binary Search Trees\" by Lin et al should be at least discussed and even compared against (now, this paper is only mentioned as a very general reference for learning-augmented data structures).\n- It is not straightforward to me why the techniques used in the proposed algorithm are novel and not hard to come up with. I encourage the author to make a clearer point on \"which components of the proposed algorithm are novel and different from existing techniques\".\n\nQuestions: - The algorithm in [Mitzenmacher and Vassilvitskii, 2021] that searches in a sorted list with predictions receive separate predictions for each query. Is this also the case for the setting discussed in this paper? \n    - If so, as I understand, the proposed algorithm needs to rebuild the binary search tree every time it receives a new query along with its distributional predictions. Then, this would lower bound the time complexity to answer each query with $O(n)$. Is that correct?\n- I am confused by the reference to \"binary search tree\" in the paper (even in the title). Does the proposed algorithm actually require building a binary search tree in its specification and implementation? Why does section 3 not contain any explanation related to the binary search tree?", "labeling_timestamp": "2026-01-11T16:35:20.121793", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt references Section 3 as containing the main algorithm and also discusses binary search trees in Section 1.1, but the actual content of Section 3 is not included here. Therefore we cannot verify whether Section 3 contains an explanation of binary search trees or their role in the algorithm.", "evidence": "1) \"Main algorithm. We then give our main result in Section 3: an algorithm which interleaves phases of the 'median' algorithm and classical binary search to obtain a query complexity of O ( H ( p ) + log η ) , where η is the earth mover's distance...\"  \n2) \"Distributional Robustness of Optimal Binary Search Trees. ... Since any comparison-based search algorithm is equivalent to a particular binary search tree, if ˆ p = p then this is precisely the classical problem of computing an optimal binary search tree [Mehlhorn, 1975].\"", "section": "1.1 Our Results and Contributions"}
{"claim": "The paper does not clarify whether the proposed algorithm's specification and implementation actually require constructing a binary search tree.", "claim_type": "methodology", "paper_id": "JEKXTLjEIq", "paper_title": "Binary Search with Distributional Predictions", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BELGkUQqxK", "reviewer": "Reviewer_kf39", "review_text": "Summary: This paper proposes a learning-augmented algorithm for searching in a sorted array. Different from all previous learning-augmented algorithms, it takes in distributional predictions. The main result is an algorithm with query complexity $O(H(p) + \\log \\eta)$, where $H(p)$ is the entropy of the true distribution and $\\eta$ is the Earth Mover's distance between the true and predicted distributions. The paper also includes proofs to show the theoretical optimality and experiments to validate the practical usefulness.\n\nStrengths: - The paper follows the recent line of work on \"learning-augmented algorithm\", or \"algorithm with predictions\". This is a promising new direction that tries to combine the theoretical soundness of classic algorithms with the learning ability of machine learning algorithms.\n- The section on theoretical analysis (though the ideas are simple) is effective.\n- The paper includes experimental results to back up the theory. The experimental settings are diverse. The performance of the proposed algorithm is strong compared to all the baselines.\n\nWeaknesses: - I think the presentation of the paper can be greatly improved. To list a few points:\n    - in line 50, the sentence \"That is, the prediction is a single (potentially high-dimensional) point (or maybe a small number of such points).\" is hard to read for me.\n    - in line 56, the sentence \"Or, can we in fact do better by taking full advantage of the entire predicted distribution.\" should end with a question mark.\n    - the abusive use of the word \"essentially\" greatly weakens the soundness of the paper (For example, in lines 66, 70, 78 87, 88). The expression \"essentially optimal\" should be clarified with collaboration on complexity and constants.\n- I think several key works in the field of learning-augmented algorithms are missing, which makes it hard to position this paper in the correct context. For example, I think the algorithms proposed in \"Learning-Augmented Binary Search Trees\" by Lin et al should be at least discussed and even compared against (now, this paper is only mentioned as a very general reference for learning-augmented data structures).\n- It is not straightforward to me why the techniques used in the proposed algorithm are novel and not hard to come up with. I encourage the author to make a clearer point on \"which components of the proposed algorithm are novel and different from existing techniques\".\n\nQuestions: - The algorithm in [Mitzenmacher and Vassilvitskii, 2021] that searches in a sorted list with predictions receive separate predictions for each query. Is this also the case for the setting discussed in this paper? \n    - If so, as I understand, the proposed algorithm needs to rebuild the binary search tree every time it receives a new query along with its distributional predictions. Then, this would lower bound the time complexity to answer each query with $O(n)$. Is that correct?\n- I am confused by the reference to \"binary search tree\" in the paper (even in the title). Does the proposed algorithm actually require building a binary search tree in its specification and implementation? Why does section 3 not contain any explanation related to the binary search tree?", "labeling_timestamp": "2026-01-11T16:35:29.269495", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly connects the proposed search algorithm to binary search trees and states it can efficiently compute a BST with the stated guarantees. Thus it does clarify that a BST can be constructed and that the results yield distributionally-robust optimal BSTs (Section 1.1).", "evidence": "\"This also yields the first distributionally-robust algorithm for the classical problem of computing an optimal binary search tree given a distribution over target keys.\" \"Since any comparison-based search algorithm is equivalent to a particular binary search tree, ... So we can interpret our results as providing distributionally-robust optimal BSTs: given ˆ p, we can efficiently compute a BST where the expected lookup time under the true (but unknown) query distribution p is at most O(H(p) + log η).\"", "section": "1.1 Our Results and Contributions (Distributional Robustness of Optimal Binary Search Trees)"}
{"claim": "The paper does not evaluate whether the proposed uncertainty measure improves LeRF or OpenScene performance.", "claim_type": "experimental", "paper_id": "SgjAojPKb3", "paper_title": "OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "ua8o7e9n4j", "reviewer": "Reviewer_Nxv7", "review_text": "Summary: This work explores a similar paradigm to LeRF (Language-embedded radiance fields). Rather than distilling CLIP features to NeRF, rather it uses a different backbone i.e. OpenSeg to distill OpenSeg features to the 3D using NeRFs. The idea of distilling similar open-set features to 3D has been shown by OpenScene which required 3D meshes as input 3D representation, however, the paper studies it in the setting of Neural Radiance Fields which requires posed 2D images as input. Qualitative open vocabulary segmentation comparisons are shown on the Replica dataset coupled with quantitative comparisons with baselines like LeRF and OpenScene\n\nStrengths: The approach discussed very relevant problems i.e. distilling open-set 2D features to the 3D domain and moving away from traditional pipelines that work with a known number of categories i.e. approaches like Semantic-NeRF and Panoptic NeRF etc. The main benefits/strengths of the approach are as follows:\n\n1. Qualitative comparison with LeRF and Open-Scene clearly shows better segmentation results despite the fact that LeRF was not designed for segmentation (which should be clearly pointed out)\n\n2. Clearly better quantitative results on all, head, common, and tail sets\n\n3. A good measure of uncertainty to improve the open-set feature field distillation into 3D.\n\nWeaknesses: 1.  Though the uncertainty measure is sound, I wonder if this improves LeRF and OpenScene's performance as well. A similar measure was introduced in Semantic-NeRF and the authors didn't highlight the difference between their formulation and Semantic NeRF's formulation. \n\n2. This looks like an incremental work that extends LeRF by using a different encoder backbone which is very straightforward to implement. Can the authors justify it with really good segmentation performance on long tail in-the-wild queries etc? I didn't see that comparison\n\n3. Not many in-the-wild examples could be seen in the paper. Replica dataset is easier since we have perfect viewpoint annotations. I wonder if the performance stays steady or breaks for more in the wild examples where there are imperfect viewpoint annotations. \n\n4. Does uncertainty measure really help? What if the initial NeRFs are not that good? Features tend to break for those viewpoints/areas. Do the authors have improved results in those cases?\n\nQuestions: Please see all the questions in the weakness section. Overall I would have liked to see a thorough comparison with LeRF and what technical improvement the work bring (in addition to changing the pre-trained backbone), a comparison with Semantic-NeRF's uncertainty formulation, more in the wild examples and examples where the quality of NeRF is not that good and do the semantic features break or does uncertainty help in those areas", "labeling_timestamp": "2026-01-11T16:35:39.716521", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper proposes a probabilistic novel-view generation based on multi-view disagreement (an uncertainty-like measure) and evaluates its effect within OpenNeRF, but the experiments only compare final OpenNeRF results to LERF and OpenScene. The paper does not report experiments applying the proposed uncertainty/novel-view mechanism to LERF or OpenScene to measure improvement in those methods.", "evidence": "We identify the disagreement from multiple views as a powerful signal and propose a probabilistic approach to generate novel view points.", "section": "Introduction"}
{"claim": "The paper fails to compare or highlight differences between their uncertainty formulation and Semantic-NeRF's uncertainty formulation.", "claim_type": "baseline", "paper_id": "SgjAojPKb3", "paper_title": "OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "ua8o7e9n4j", "reviewer": "Reviewer_Nxv7", "review_text": "Summary: This work explores a similar paradigm to LeRF (Language-embedded radiance fields). Rather than distilling CLIP features to NeRF, rather it uses a different backbone i.e. OpenSeg to distill OpenSeg features to the 3D using NeRFs. The idea of distilling similar open-set features to 3D has been shown by OpenScene which required 3D meshes as input 3D representation, however, the paper studies it in the setting of Neural Radiance Fields which requires posed 2D images as input. Qualitative open vocabulary segmentation comparisons are shown on the Replica dataset coupled with quantitative comparisons with baselines like LeRF and OpenScene\n\nStrengths: The approach discussed very relevant problems i.e. distilling open-set 2D features to the 3D domain and moving away from traditional pipelines that work with a known number of categories i.e. approaches like Semantic-NeRF and Panoptic NeRF etc. The main benefits/strengths of the approach are as follows:\n\n1. Qualitative comparison with LeRF and Open-Scene clearly shows better segmentation results despite the fact that LeRF was not designed for segmentation (which should be clearly pointed out)\n\n2. Clearly better quantitative results on all, head, common, and tail sets\n\n3. A good measure of uncertainty to improve the open-set feature field distillation into 3D.\n\nWeaknesses: 1.  Though the uncertainty measure is sound, I wonder if this improves LeRF and OpenScene's performance as well. A similar measure was introduced in Semantic-NeRF and the authors didn't highlight the difference between their formulation and Semantic NeRF's formulation. \n\n2. This looks like an incremental work that extends LeRF by using a different encoder backbone which is very straightforward to implement. Can the authors justify it with really good segmentation performance on long tail in-the-wild queries etc? I didn't see that comparison\n\n3. Not many in-the-wild examples could be seen in the paper. Replica dataset is easier since we have perfect viewpoint annotations. I wonder if the performance stays steady or breaks for more in the wild examples where there are imperfect viewpoint annotations. \n\n4. Does uncertainty measure really help? What if the initial NeRFs are not that good? Features tend to break for those viewpoints/areas. Do the authors have improved results in those cases?\n\nQuestions: Please see all the questions in the weakness section. Overall I would have liked to see a thorough comparison with LeRF and what technical improvement the work bring (in addition to changing the pre-trained backbone), a comparison with Semantic-NeRF's uncertainty formulation, more in the wild examples and examples where the quality of NeRF is not that good and do the semantic features break or does uncertainty help in those areas", "labeling_timestamp": "2026-01-11T16:35:36.167995", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes its own probabilistic/view-disagreement mechanism for selecting novel views (i.e., an uncertainty-like formulation) and compares to prior works like LERF and OpenScene, but it does not mention or compare against Semantic-NeRF or its uncertainty formulation anywhere in the text.", "evidence": "1) \"We identify the disagreement from multiple views as a powerful signal and propose a probabilistic approach to generate novel view points.\" 2) \"While OpenScene (Peng et al., 2023), OpenMask3D (Takmaz et al., 2023a), and LERF (Kerr et al., 2023) demonstrate impressive capability of segmenting any given concept, they still suffer from several limitations.\"", "section": "Introduction; Related Work"}
{"claim": "The proposed method appears incremental, primarily replacing LeRF's encoder backbone with OpenSeg and lacking clear technical novelty.", "claim_type": "novelty", "paper_id": "SgjAojPKb3", "paper_title": "OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "ua8o7e9n4j", "reviewer": "Reviewer_Nxv7", "review_text": "Summary: This work explores a similar paradigm to LeRF (Language-embedded radiance fields). Rather than distilling CLIP features to NeRF, rather it uses a different backbone i.e. OpenSeg to distill OpenSeg features to the 3D using NeRFs. The idea of distilling similar open-set features to 3D has been shown by OpenScene which required 3D meshes as input 3D representation, however, the paper studies it in the setting of Neural Radiance Fields which requires posed 2D images as input. Qualitative open vocabulary segmentation comparisons are shown on the Replica dataset coupled with quantitative comparisons with baselines like LeRF and OpenScene\n\nStrengths: The approach discussed very relevant problems i.e. distilling open-set 2D features to the 3D domain and moving away from traditional pipelines that work with a known number of categories i.e. approaches like Semantic-NeRF and Panoptic NeRF etc. The main benefits/strengths of the approach are as follows:\n\n1. Qualitative comparison with LeRF and Open-Scene clearly shows better segmentation results despite the fact that LeRF was not designed for segmentation (which should be clearly pointed out)\n\n2. Clearly better quantitative results on all, head, common, and tail sets\n\n3. A good measure of uncertainty to improve the open-set feature field distillation into 3D.\n\nWeaknesses: 1.  Though the uncertainty measure is sound, I wonder if this improves LeRF and OpenScene's performance as well. A similar measure was introduced in Semantic-NeRF and the authors didn't highlight the difference between their formulation and Semantic NeRF's formulation. \n\n2. This looks like an incremental work that extends LeRF by using a different encoder backbone which is very straightforward to implement. Can the authors justify it with really good segmentation performance on long tail in-the-wild queries etc? I didn't see that comparison\n\n3. Not many in-the-wild examples could be seen in the paper. Replica dataset is easier since we have perfect viewpoint annotations. I wonder if the performance stays steady or breaks for more in the wild examples where there are imperfect viewpoint annotations. \n\n4. Does uncertainty measure really help? What if the initial NeRFs are not that good? Features tend to break for those viewpoints/areas. Do the authors have improved results in those cases?\n\nQuestions: Please see all the questions in the weakness section. Overall I would have liked to see a thorough comparison with LeRF and what technical improvement the work bring (in addition to changing the pre-trained backbone), a comparison with Semantic-NeRF's uncertainty formulation, more in the wild examples and examples where the quality of NeRF is not that good and do the semantic features break or does uncertainty help in those areas", "labeling_timestamp": "2026-01-11T16:35:46.594942", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper does replace LERF's global CLIP features with pixel-aligned features from OpenSeg and emphasizes this as a simplifying change (removing multi-resolution patches and DINO regularization). However, it also proposes additional technical contributions—notably a probabilistic novel-viewpoint generation mechanism that leverages NeRF's view synthesis to extract more VLM features—and reports improved performance. Thus the reviewer's statement is only partially accurate.", "evidence": ["our work shows that using pixel-wise VLM features (instead of global CLIP features) results in an overall less complex architecture without the need for additional DINO regularization. Our OpenNeRF further leverages NeRF's ability to render novel views and extract open-set VLM features from areas that are not well observed in the initial posed images.", "Second, we further propose the direct distillation of pixel-aligned CLIP features from OpenSeg (Ghiasi et al., 2022) into our neural radiance field. Compared to LERF, this not only increases the segmentation quality via detailed and pixel-aligned CLIP features, it also significantly simplifies the underlying architecture as it eradicates the need for multi-resolution patches and additional DINO-based regularization terms.", "One key challenge is to determine the relevant parts of the scene requiring further attention. We identify the disagreement from multiple views as a powerful signal and propose a probabilistic approach to generate novel view points."], "section": "Abstract; Introduction"}
{"claim": "The paper lacks evaluation on long-tail in-the-wild queries to justify claimed segmentation improvements.", "claim_type": "experimental", "paper_id": "SgjAojPKb3", "paper_title": "OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "ua8o7e9n4j", "reviewer": "Reviewer_Nxv7", "review_text": "Summary: This work explores a similar paradigm to LeRF (Language-embedded radiance fields). Rather than distilling CLIP features to NeRF, rather it uses a different backbone i.e. OpenSeg to distill OpenSeg features to the 3D using NeRFs. The idea of distilling similar open-set features to 3D has been shown by OpenScene which required 3D meshes as input 3D representation, however, the paper studies it in the setting of Neural Radiance Fields which requires posed 2D images as input. Qualitative open vocabulary segmentation comparisons are shown on the Replica dataset coupled with quantitative comparisons with baselines like LeRF and OpenScene\n\nStrengths: The approach discussed very relevant problems i.e. distilling open-set 2D features to the 3D domain and moving away from traditional pipelines that work with a known number of categories i.e. approaches like Semantic-NeRF and Panoptic NeRF etc. The main benefits/strengths of the approach are as follows:\n\n1. Qualitative comparison with LeRF and Open-Scene clearly shows better segmentation results despite the fact that LeRF was not designed for segmentation (which should be clearly pointed out)\n\n2. Clearly better quantitative results on all, head, common, and tail sets\n\n3. A good measure of uncertainty to improve the open-set feature field distillation into 3D.\n\nWeaknesses: 1.  Though the uncertainty measure is sound, I wonder if this improves LeRF and OpenScene's performance as well. A similar measure was introduced in Semantic-NeRF and the authors didn't highlight the difference between their formulation and Semantic NeRF's formulation. \n\n2. This looks like an incremental work that extends LeRF by using a different encoder backbone which is very straightforward to implement. Can the authors justify it with really good segmentation performance on long tail in-the-wild queries etc? I didn't see that comparison\n\n3. Not many in-the-wild examples could be seen in the paper. Replica dataset is easier since we have perfect viewpoint annotations. I wonder if the performance stays steady or breaks for more in the wild examples where there are imperfect viewpoint annotations. \n\n4. Does uncertainty measure really help? What if the initial NeRFs are not that good? Features tend to break for those viewpoints/areas. Do the authors have improved results in those cases?\n\nQuestions: Please see all the questions in the weakness section. Overall I would have liked to see a thorough comparison with LeRF and what technical improvement the work bring (in addition to changing the pre-trained backbone), a comparison with Semantic-NeRF's uncertainty formulation, more in the wild examples and examples where the quality of NeRF is not that good and do the semantic features break or does uncertainty help in those areas", "labeling_timestamp": "2026-01-11T16:35:51.741908", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper evaluates long-tail classes on the Replica dataset (it explicitly cites Replica's long-tail class distribution and reports mIoU improvements there), but it does not present evaluations of ’in-the-wild’ long-tail queries beyond that dataset in the provided content. Thus the reviewer’s complaint is partially true: there is long-tail evaluation, but not evaluation on in-the-wild queries.", "evidence": "“We identify the Replica dataset as a promising candidate to evaluate open-set 3D semantic segmentation since, unlike ScanNet ... it comes with very accurate mesh reconstruction, per-point semantic labels as well as a long-tail class distribution (see Fig. 4).”\n\n“For 3D point cloud segmentation on the Replica dataset, OpenNeRF outperforms recent open-vocabulary methods such as LERF and OpenScene by at least +4.9 mIoU.”\n\n“NeRF-based representations, such as LERF and ourOpenNeRF, are better suited for detecting small long-tail objects compared to mesh based representations used e.g., in OpenScene.”", "section": "Introduction / Abstract"}
{"claim": "The paper includes few in-the-wild examples and relies primarily on the Replica dataset for evaluation.", "claim_type": "experimental", "paper_id": "SgjAojPKb3", "paper_title": "OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "ua8o7e9n4j", "reviewer": "Reviewer_Nxv7", "review_text": "Summary: This work explores a similar paradigm to LeRF (Language-embedded radiance fields). Rather than distilling CLIP features to NeRF, rather it uses a different backbone i.e. OpenSeg to distill OpenSeg features to the 3D using NeRFs. The idea of distilling similar open-set features to 3D has been shown by OpenScene which required 3D meshes as input 3D representation, however, the paper studies it in the setting of Neural Radiance Fields which requires posed 2D images as input. Qualitative open vocabulary segmentation comparisons are shown on the Replica dataset coupled with quantitative comparisons with baselines like LeRF and OpenScene\n\nStrengths: The approach discussed very relevant problems i.e. distilling open-set 2D features to the 3D domain and moving away from traditional pipelines that work with a known number of categories i.e. approaches like Semantic-NeRF and Panoptic NeRF etc. The main benefits/strengths of the approach are as follows:\n\n1. Qualitative comparison with LeRF and Open-Scene clearly shows better segmentation results despite the fact that LeRF was not designed for segmentation (which should be clearly pointed out)\n\n2. Clearly better quantitative results on all, head, common, and tail sets\n\n3. A good measure of uncertainty to improve the open-set feature field distillation into 3D.\n\nWeaknesses: 1.  Though the uncertainty measure is sound, I wonder if this improves LeRF and OpenScene's performance as well. A similar measure was introduced in Semantic-NeRF and the authors didn't highlight the difference between their formulation and Semantic NeRF's formulation. \n\n2. This looks like an incremental work that extends LeRF by using a different encoder backbone which is very straightforward to implement. Can the authors justify it with really good segmentation performance on long tail in-the-wild queries etc? I didn't see that comparison\n\n3. Not many in-the-wild examples could be seen in the paper. Replica dataset is easier since we have perfect viewpoint annotations. I wonder if the performance stays steady or breaks for more in the wild examples where there are imperfect viewpoint annotations. \n\n4. Does uncertainty measure really help? What if the initial NeRFs are not that good? Features tend to break for those viewpoints/areas. Do the authors have improved results in those cases?\n\nQuestions: Please see all the questions in the weakness section. Overall I would have liked to see a thorough comparison with LeRF and what technical improvement the work bring (in addition to changing the pre-trained backbone), a comparison with Semantic-NeRF's uncertainty formulation, more in the wild examples and examples where the quality of NeRF is not that good and do the semantic features break or does uncertainty help in those areas", "labeling_timestamp": "2026-01-11T16:35:59.181711", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper repeatedly emphasizes evaluation on the Replica dataset (reports quantitative gains on Replica and shows qualitative 'zero-shot results on Replica') and discusses Replica as the chosen benchmark; it does not describe broader in-the-wild evaluations, consistent with the reviewer claim that the work relies primarily on Replica and includes few (if any) in-the-wild examples.", "evidence": "“For 3D point cloud segmentation on the Replica dataset, OpenNeRF outperforms recent open-vocabulary methods such as LERF and OpenScene by at least +4 . 9 mIoU.”\n\n“We identify the Replica dataset as a promising candidate to evaluate open-set 3D semantic segmentation since, unlike ScanNet ... or Matterport ..., it comes with very accurate mesh reconstruction, per-point semantic labels as well as a long-tail class distribution (see Fig. 4).”\n\n“OpenNeRF significantly outperforms the current state-of-the-art for open-vocabulary 3D segmentation with an +4 . 5 mIoU gain on the Replica dataset.”\n\n“Zero-shot results on Replica (Straub et al., 2019).”", "section": "Abstract; Introduction"}
{"claim": "The paper does not evaluate robustness to imperfect camera pose annotations common in in-the-wild datasets.", "claim_type": "experimental", "paper_id": "SgjAojPKb3", "paper_title": "OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "ua8o7e9n4j", "reviewer": "Reviewer_Nxv7", "review_text": "Summary: This work explores a similar paradigm to LeRF (Language-embedded radiance fields). Rather than distilling CLIP features to NeRF, rather it uses a different backbone i.e. OpenSeg to distill OpenSeg features to the 3D using NeRFs. The idea of distilling similar open-set features to 3D has been shown by OpenScene which required 3D meshes as input 3D representation, however, the paper studies it in the setting of Neural Radiance Fields which requires posed 2D images as input. Qualitative open vocabulary segmentation comparisons are shown on the Replica dataset coupled with quantitative comparisons with baselines like LeRF and OpenScene\n\nStrengths: The approach discussed very relevant problems i.e. distilling open-set 2D features to the 3D domain and moving away from traditional pipelines that work with a known number of categories i.e. approaches like Semantic-NeRF and Panoptic NeRF etc. The main benefits/strengths of the approach are as follows:\n\n1. Qualitative comparison with LeRF and Open-Scene clearly shows better segmentation results despite the fact that LeRF was not designed for segmentation (which should be clearly pointed out)\n\n2. Clearly better quantitative results on all, head, common, and tail sets\n\n3. A good measure of uncertainty to improve the open-set feature field distillation into 3D.\n\nWeaknesses: 1.  Though the uncertainty measure is sound, I wonder if this improves LeRF and OpenScene's performance as well. A similar measure was introduced in Semantic-NeRF and the authors didn't highlight the difference between their formulation and Semantic NeRF's formulation. \n\n2. This looks like an incremental work that extends LeRF by using a different encoder backbone which is very straightforward to implement. Can the authors justify it with really good segmentation performance on long tail in-the-wild queries etc? I didn't see that comparison\n\n3. Not many in-the-wild examples could be seen in the paper. Replica dataset is easier since we have perfect viewpoint annotations. I wonder if the performance stays steady or breaks for more in the wild examples where there are imperfect viewpoint annotations. \n\n4. Does uncertainty measure really help? What if the initial NeRFs are not that good? Features tend to break for those viewpoints/areas. Do the authors have improved results in those cases?\n\nQuestions: Please see all the questions in the weakness section. Overall I would have liked to see a thorough comparison with LeRF and what technical improvement the work bring (in addition to changing the pre-trained backbone), a comparison with Semantic-NeRF's uncertainty formulation, more in the wild examples and examples where the quality of NeRF is not that good and do the semantic features break or does uncertainty help in those areas", "labeling_timestamp": "2026-01-11T16:36:16.184562", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper states it operates on posed images and selects the Replica dataset specifically because it has very accurate reconstruction and per-point labels. The paper contains no experiments or analysis on noisy or imperfect camera poses or robustness to pose errors common in in-the-wild datasets.", "evidence": ["we propose OpenNeRF which naturally operates on posed images and directly encodes the VLM features within the NeRF.", "We identify the Replica dataset as a promising candidate to evaluate open-set 3D semantic segmentation since, unlike ScanNet (Dai et al., 2017a) or Matterport (Ramakrishnan et al., 2021), it comes with very accurate mesh reconstruction, per-point semantic labels as well as a long-tail class distribution (see Fig. 4)."], "section": "ABSTRACT; 1 INTRODUCTION"}
{"claim": "The paper does not test whether the uncertainty measure improves results when initial NeRF reconstructions are poor.", "claim_type": "experimental", "paper_id": "SgjAojPKb3", "paper_title": "OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "ua8o7e9n4j", "reviewer": "Reviewer_Nxv7", "review_text": "Summary: This work explores a similar paradigm to LeRF (Language-embedded radiance fields). Rather than distilling CLIP features to NeRF, rather it uses a different backbone i.e. OpenSeg to distill OpenSeg features to the 3D using NeRFs. The idea of distilling similar open-set features to 3D has been shown by OpenScene which required 3D meshes as input 3D representation, however, the paper studies it in the setting of Neural Radiance Fields which requires posed 2D images as input. Qualitative open vocabulary segmentation comparisons are shown on the Replica dataset coupled with quantitative comparisons with baselines like LeRF and OpenScene\n\nStrengths: The approach discussed very relevant problems i.e. distilling open-set 2D features to the 3D domain and moving away from traditional pipelines that work with a known number of categories i.e. approaches like Semantic-NeRF and Panoptic NeRF etc. The main benefits/strengths of the approach are as follows:\n\n1. Qualitative comparison with LeRF and Open-Scene clearly shows better segmentation results despite the fact that LeRF was not designed for segmentation (which should be clearly pointed out)\n\n2. Clearly better quantitative results on all, head, common, and tail sets\n\n3. A good measure of uncertainty to improve the open-set feature field distillation into 3D.\n\nWeaknesses: 1.  Though the uncertainty measure is sound, I wonder if this improves LeRF and OpenScene's performance as well. A similar measure was introduced in Semantic-NeRF and the authors didn't highlight the difference between their formulation and Semantic NeRF's formulation. \n\n2. This looks like an incremental work that extends LeRF by using a different encoder backbone which is very straightforward to implement. Can the authors justify it with really good segmentation performance on long tail in-the-wild queries etc? I didn't see that comparison\n\n3. Not many in-the-wild examples could be seen in the paper. Replica dataset is easier since we have perfect viewpoint annotations. I wonder if the performance stays steady or breaks for more in the wild examples where there are imperfect viewpoint annotations. \n\n4. Does uncertainty measure really help? What if the initial NeRFs are not that good? Features tend to break for those viewpoints/areas. Do the authors have improved results in those cases?\n\nQuestions: Please see all the questions in the weakness section. Overall I would have liked to see a thorough comparison with LeRF and what technical improvement the work bring (in addition to changing the pre-trained backbone), a comparison with Semantic-NeRF's uncertainty formulation, more in the wild examples and examples where the quality of NeRF is not that good and do the semantic features break or does uncertainty help in those areas", "labeling_timestamp": "2026-01-11T16:36:03.823282", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper describes an uncertainty / disagreement-based mechanism to generate novel views (Intro) and claims improved segmentation from rendering novel views, but the provided text contains no experimental details or ablations testing whether the uncertainty measure specifically improves results under poor initial NeRF reconstructions.", "evidence": "We identify the disagreement from multiple views as a powerful signal and propose a probabilistic approach to generate novel view points.", "section": "Introduction"}
{"claim": "Semantic features tend to break in viewpoints or regions where NeRF reconstructions are low quality.", "claim_type": "experimental", "paper_id": "SgjAojPKb3", "paper_title": "OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "ua8o7e9n4j", "reviewer": "Reviewer_Nxv7", "review_text": "Summary: This work explores a similar paradigm to LeRF (Language-embedded radiance fields). Rather than distilling CLIP features to NeRF, rather it uses a different backbone i.e. OpenSeg to distill OpenSeg features to the 3D using NeRFs. The idea of distilling similar open-set features to 3D has been shown by OpenScene which required 3D meshes as input 3D representation, however, the paper studies it in the setting of Neural Radiance Fields which requires posed 2D images as input. Qualitative open vocabulary segmentation comparisons are shown on the Replica dataset coupled with quantitative comparisons with baselines like LeRF and OpenScene\n\nStrengths: The approach discussed very relevant problems i.e. distilling open-set 2D features to the 3D domain and moving away from traditional pipelines that work with a known number of categories i.e. approaches like Semantic-NeRF and Panoptic NeRF etc. The main benefits/strengths of the approach are as follows:\n\n1. Qualitative comparison with LeRF and Open-Scene clearly shows better segmentation results despite the fact that LeRF was not designed for segmentation (which should be clearly pointed out)\n\n2. Clearly better quantitative results on all, head, common, and tail sets\n\n3. A good measure of uncertainty to improve the open-set feature field distillation into 3D.\n\nWeaknesses: 1.  Though the uncertainty measure is sound, I wonder if this improves LeRF and OpenScene's performance as well. A similar measure was introduced in Semantic-NeRF and the authors didn't highlight the difference between their formulation and Semantic NeRF's formulation. \n\n2. This looks like an incremental work that extends LeRF by using a different encoder backbone which is very straightforward to implement. Can the authors justify it with really good segmentation performance on long tail in-the-wild queries etc? I didn't see that comparison\n\n3. Not many in-the-wild examples could be seen in the paper. Replica dataset is easier since we have perfect viewpoint annotations. I wonder if the performance stays steady or breaks for more in the wild examples where there are imperfect viewpoint annotations. \n\n4. Does uncertainty measure really help? What if the initial NeRFs are not that good? Features tend to break for those viewpoints/areas. Do the authors have improved results in those cases?\n\nQuestions: Please see all the questions in the weakness section. Overall I would have liked to see a thorough comparison with LeRF and what technical improvement the work bring (in addition to changing the pre-trained backbone), a comparison with Semantic-NeRF's uncertainty formulation, more in the wild examples and examples where the quality of NeRF is not that good and do the semantic features break or does uncertainty help in those areas", "labeling_timestamp": "2026-01-11T16:36:16.709404", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper states that features extracted from 2D images / reconstructions can be unreliable in poorly observed regions and that they use NeRF rendering of novel views to recover additional VLM features for such areas. This supports the idea that semantic features degrade where observations/reconstructions are poor. However, the paper does not explicitly claim that semantic features \"break\" specifically in viewpoints or regions where NeRF reconstructions themselves are low quality, nor does it directly analyze failure modes of semantic features as a function of NeRF reconstruction quality.", "evidence": "Abstract: \"the reconstructed 3D scene geometry might not project well to the underlying 2D image sequences used to compute pixel-aligned CLIP features.\" Introduction: \"we leverage this ability to extract additional visual-language features from novel views... extract open-set VLM features from areas that are not well observed in the initial posed images.\"", "section": "Abstract, Introduction"}
{"claim": "The authors did not explicitly state that LeRF was not designed for segmentation in their qualitative comparisons.", "claim_type": "baseline", "paper_id": "SgjAojPKb3", "paper_title": "OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "ua8o7e9n4j", "reviewer": "Reviewer_Nxv7", "review_text": "Summary: This work explores a similar paradigm to LeRF (Language-embedded radiance fields). Rather than distilling CLIP features to NeRF, rather it uses a different backbone i.e. OpenSeg to distill OpenSeg features to the 3D using NeRFs. The idea of distilling similar open-set features to 3D has been shown by OpenScene which required 3D meshes as input 3D representation, however, the paper studies it in the setting of Neural Radiance Fields which requires posed 2D images as input. Qualitative open vocabulary segmentation comparisons are shown on the Replica dataset coupled with quantitative comparisons with baselines like LeRF and OpenScene\n\nStrengths: The approach discussed very relevant problems i.e. distilling open-set 2D features to the 3D domain and moving away from traditional pipelines that work with a known number of categories i.e. approaches like Semantic-NeRF and Panoptic NeRF etc. The main benefits/strengths of the approach are as follows:\n\n1. Qualitative comparison with LeRF and Open-Scene clearly shows better segmentation results despite the fact that LeRF was not designed for segmentation (which should be clearly pointed out)\n\n2. Clearly better quantitative results on all, head, common, and tail sets\n\n3. A good measure of uncertainty to improve the open-set feature field distillation into 3D.\n\nWeaknesses: 1.  Though the uncertainty measure is sound, I wonder if this improves LeRF and OpenScene's performance as well. A similar measure was introduced in Semantic-NeRF and the authors didn't highlight the difference between their formulation and Semantic NeRF's formulation. \n\n2. This looks like an incremental work that extends LeRF by using a different encoder backbone which is very straightforward to implement. Can the authors justify it with really good segmentation performance on long tail in-the-wild queries etc? I didn't see that comparison\n\n3. Not many in-the-wild examples could be seen in the paper. Replica dataset is easier since we have perfect viewpoint annotations. I wonder if the performance stays steady or breaks for more in the wild examples where there are imperfect viewpoint annotations. \n\n4. Does uncertainty measure really help? What if the initial NeRFs are not that good? Features tend to break for those viewpoints/areas. Do the authors have improved results in those cases?\n\nQuestions: Please see all the questions in the weakness section. Overall I would have liked to see a thorough comparison with LeRF and what technical improvement the work bring (in addition to changing the pre-trained backbone), a comparison with Semantic-NeRF's uncertainty formulation, more in the wild examples and examples where the quality of NeRF is not that good and do the semantic features break or does uncertainty help in those areas", "labeling_timestamp": "2026-01-11T16:36:36.796522", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper does explicitly state that LERF did not demonstrate results on 3D semantic segmentation and discusses LERF's limitations for segmentation (e.g., use of global CLIP features and need for multi-resolution patches/DINO regularization).", "evidence": "“Our method is also similar to LERF (Kerr et al., 2023) in terms of NeRF representation but does not demonstrate results on 3D semantic segmentation.”", "section": "2 RELATED WORK"}
{"claim": "Using the Replica dataset provides easier evaluation due to perfect viewpoint annotations and may overestimate real-world performance.", "claim_type": "experimental", "paper_id": "SgjAojPKb3", "paper_title": "OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "ua8o7e9n4j", "reviewer": "Reviewer_Nxv7", "review_text": "Summary: This work explores a similar paradigm to LeRF (Language-embedded radiance fields). Rather than distilling CLIP features to NeRF, rather it uses a different backbone i.e. OpenSeg to distill OpenSeg features to the 3D using NeRFs. The idea of distilling similar open-set features to 3D has been shown by OpenScene which required 3D meshes as input 3D representation, however, the paper studies it in the setting of Neural Radiance Fields which requires posed 2D images as input. Qualitative open vocabulary segmentation comparisons are shown on the Replica dataset coupled with quantitative comparisons with baselines like LeRF and OpenScene\n\nStrengths: The approach discussed very relevant problems i.e. distilling open-set 2D features to the 3D domain and moving away from traditional pipelines that work with a known number of categories i.e. approaches like Semantic-NeRF and Panoptic NeRF etc. The main benefits/strengths of the approach are as follows:\n\n1. Qualitative comparison with LeRF and Open-Scene clearly shows better segmentation results despite the fact that LeRF was not designed for segmentation (which should be clearly pointed out)\n\n2. Clearly better quantitative results on all, head, common, and tail sets\n\n3. A good measure of uncertainty to improve the open-set feature field distillation into 3D.\n\nWeaknesses: 1.  Though the uncertainty measure is sound, I wonder if this improves LeRF and OpenScene's performance as well. A similar measure was introduced in Semantic-NeRF and the authors didn't highlight the difference between their formulation and Semantic NeRF's formulation. \n\n2. This looks like an incremental work that extends LeRF by using a different encoder backbone which is very straightforward to implement. Can the authors justify it with really good segmentation performance on long tail in-the-wild queries etc? I didn't see that comparison\n\n3. Not many in-the-wild examples could be seen in the paper. Replica dataset is easier since we have perfect viewpoint annotations. I wonder if the performance stays steady or breaks for more in the wild examples where there are imperfect viewpoint annotations. \n\n4. Does uncertainty measure really help? What if the initial NeRFs are not that good? Features tend to break for those viewpoints/areas. Do the authors have improved results in those cases?\n\nQuestions: Please see all the questions in the weakness section. Overall I would have liked to see a thorough comparison with LeRF and what technical improvement the work bring (in addition to changing the pre-trained backbone), a comparison with Semantic-NeRF's uncertainty formulation, more in the wild examples and examples where the quality of NeRF is not that good and do the semantic features break or does uncertainty help in those areas", "labeling_timestamp": "2026-01-11T16:36:22.626540", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly states Replica is a promising evaluation candidate because it 'comes with very accurate mesh reconstruction, per-point semantic labels as well as a long-tail class distribution,' which supports the reviewer's point that Replica provides easier/more accurate evaluation. However, the paper does not mention 'perfect viewpoint annotations' nor does it claim that using Replica may overestimate real-world performance; those specific assertions are not made in the paper.", "evidence": "We identify the Replica dataset as a promising candidate to evaluate open-set 3D semantic segmentation since, unlike ScanNet (Dai et al., 2017a) or Matterport (Ramakrishnan et al., 2021), it comes with very accurate mesh reconstruction, per-point semantic labels as well as a long-tail class distribution (see Fig. 4).", "section": "1 INTRODUCTION"}
{"claim": "The paper lacks a thorough quantitative comparison showing technical improvements over LeRF beyond changing the pretrained backbone.", "claim_type": "baseline", "paper_id": "SgjAojPKb3", "paper_title": "OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "ua8o7e9n4j", "reviewer": "Reviewer_Nxv7", "review_text": "Summary: This work explores a similar paradigm to LeRF (Language-embedded radiance fields). Rather than distilling CLIP features to NeRF, rather it uses a different backbone i.e. OpenSeg to distill OpenSeg features to the 3D using NeRFs. The idea of distilling similar open-set features to 3D has been shown by OpenScene which required 3D meshes as input 3D representation, however, the paper studies it in the setting of Neural Radiance Fields which requires posed 2D images as input. Qualitative open vocabulary segmentation comparisons are shown on the Replica dataset coupled with quantitative comparisons with baselines like LeRF and OpenScene\n\nStrengths: The approach discussed very relevant problems i.e. distilling open-set 2D features to the 3D domain and moving away from traditional pipelines that work with a known number of categories i.e. approaches like Semantic-NeRF and Panoptic NeRF etc. The main benefits/strengths of the approach are as follows:\n\n1. Qualitative comparison with LeRF and Open-Scene clearly shows better segmentation results despite the fact that LeRF was not designed for segmentation (which should be clearly pointed out)\n\n2. Clearly better quantitative results on all, head, common, and tail sets\n\n3. A good measure of uncertainty to improve the open-set feature field distillation into 3D.\n\nWeaknesses: 1.  Though the uncertainty measure is sound, I wonder if this improves LeRF and OpenScene's performance as well. A similar measure was introduced in Semantic-NeRF and the authors didn't highlight the difference between their formulation and Semantic NeRF's formulation. \n\n2. This looks like an incremental work that extends LeRF by using a different encoder backbone which is very straightforward to implement. Can the authors justify it with really good segmentation performance on long tail in-the-wild queries etc? I didn't see that comparison\n\n3. Not many in-the-wild examples could be seen in the paper. Replica dataset is easier since we have perfect viewpoint annotations. I wonder if the performance stays steady or breaks for more in the wild examples where there are imperfect viewpoint annotations. \n\n4. Does uncertainty measure really help? What if the initial NeRFs are not that good? Features tend to break for those viewpoints/areas. Do the authors have improved results in those cases?\n\nQuestions: Please see all the questions in the weakness section. Overall I would have liked to see a thorough comparison with LeRF and what technical improvement the work bring (in addition to changing the pre-trained backbone), a comparison with Semantic-NeRF's uncertainty formulation, more in the wild examples and examples where the quality of NeRF is not that good and do the semantic features break or does uncertainty help in those areas", "labeling_timestamp": "2026-01-11T16:36:51.129878", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper reports quantitative gains over LERF (e.g., +4.5 / +4.9 mIoU on Replica) and describes architectural/algorithmic differences (pixel-aligned CLIP features, novel-view rendering, no DINO regularization). However, in the provided content there are no detailed controlled quantitative comparisons or ablation studies that isolate which improvements come from changes other than the pretrained backbone (e.g., separating the effect of using pixel-wise OpenSeg features from other design choices). Thus the reviewer's claim that the paper lacks a thorough quantitative comparison beyond backbone changes is correct.", "evidence": "Abstract: \"This is similar in spirit to LERF, however our work shows that using pixel-wise VLM features (instead of global CLIP features) results in an overall less complex architecture without the need for additional DINO regularization.\"; Abstract: \"For 3D point cloud segmentation on the Replica dataset, OpenNeRF outperforms recent open-vocabulary methods such as LERF and OpenScene by at least +4.9 mIoU.\"; Introduction / contributions: \"Compared to LERF, this not only increases the segmentation quality via detailed and pixel-aligned CLIP features, it also significantly simplifies the underlying architecture as it eradicates the need for multi-resolution patches and additional DINO-based regularization terms.\"; Contributions: \"OpenNeRF significantly outperforms the current state-of-the-art for open-vocabulary 3D segmentation with an +4.5 mIoU gain on the Replica dataset.\"", "section": "Abstract; 1 INTRODUCTION; Contributions"}
{"claim": "The proposed notion of post-purification robustness assumes the purified DNN will encounter further fine-tuning, which may not hold in many deployment scenarios.", "claim_type": "methodology", "paper_id": "qZFshkbWDo", "paper_title": "Uncovering, Explaining, and Mitigating the Superficial Safety of Backdoor Defense", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "Ni8p8ZQbkI", "reviewer": "Reviewer_R9Lq", "review_text": "Summary: This paper investigates the effectiveness of current purification-based backdoor defenses and tries to uncover whether purified DNNs are truly free from backdoor vulnerabilities. The authors identify the “post-purification robustness” of DNNs and propose Retuning Attack (RA) and Query-based Reactivation Attack (QRA) respectively to assess the susceptibility of purified DNNs to backdoor reactivation. Additionally, the paper proposes Path-Aware Minimization (PAM) to improve post-purification robustness.\n\nStrengths: The work makes contribution in backdoor robustness evaluation by shifting the focus from merely achieving low ASR to evaluating the post-purification robustness of backdoor defenses. The paper introduces a pipeline of methods: RA, QRA, and PAM, for the assessment of this vulnerability.\n\nWeaknesses: The proposed notion of post-purification robustness lies with the assumption that the purified DNN will encounter further fine-tuning. As such, the entire workflow proposed is not practical in a real-world scenario where the purified model is kept frozen with no further updates.\n\nQuestions: Can the authors further justify the proposed threat model’s real-world implications? How can the proposed workflow be enabled in a real-world attack setting?", "labeling_timestamp": "2026-01-11T16:36:40.801213", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper evaluates \"post-purification robustness\" primarily via a Retuning Attack (RA) that explicitly fine-tunes purified models with a few poisoned samples, so it does analyze the scenario where models undergo further fine-tuning. However, the authors do not strictly assume fine-tuning is the only threat: they explicitly introduce a Query-based Reactivation Attack (QRA) to show reactivation can occur via querying without further fine-tuning. Thus the reviewer claim is partially accurate (RA is a core evaluation) but incomplete because the paper also covers non-fine-tuning (query-only) reactivation.", "evidence": "\"We find that current safety purification methods are vulnerable to the rapid re-learning of backdoor behavior, even when further fine-tuning of purified models is performed using a very small number of poisoned samples.\" (Abstract) \n\n\"we employ the Retuning Attack (RA) where we first retune the purified models using an extremely small number of backdoored samples and tuning epochs.\" (1 Introduction) \n\n\"Since the vulnerability revealed by the Retuning Attack (RA) relies on the use of retuned models, we further propose the more practical Query-based Reactivation Attack (QRA). This attack is capable of generating sample-specific perturbations... simply by querying these purified models.\" (1 Introduction)", "section": "Abstract; 1 Introduction"}
{"claim": "The proposed workflow is impractical in real-world scenarios where purified models are kept frozen and do not receive further updates.", "claim_type": "subjective", "paper_id": "qZFshkbWDo", "paper_title": "Uncovering, Explaining, and Mitigating the Superficial Safety of Backdoor Defense", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "Ni8p8ZQbkI", "reviewer": "Reviewer_R9Lq", "review_text": "Summary: This paper investigates the effectiveness of current purification-based backdoor defenses and tries to uncover whether purified DNNs are truly free from backdoor vulnerabilities. The authors identify the “post-purification robustness” of DNNs and propose Retuning Attack (RA) and Query-based Reactivation Attack (QRA) respectively to assess the susceptibility of purified DNNs to backdoor reactivation. Additionally, the paper proposes Path-Aware Minimization (PAM) to improve post-purification robustness.\n\nStrengths: The work makes contribution in backdoor robustness evaluation by shifting the focus from merely achieving low ASR to evaluating the post-purification robustness of backdoor defenses. The paper introduces a pipeline of methods: RA, QRA, and PAM, for the assessment of this vulnerability.\n\nWeaknesses: The proposed notion of post-purification robustness lies with the assumption that the purified DNN will encounter further fine-tuning. As such, the entire workflow proposed is not practical in a real-world scenario where the purified model is kept frozen with no further updates.\n\nQuestions: Can the authors further justify the proposed threat model’s real-world implications? How can the proposed workflow be enabled in a real-world attack setting?", "labeling_timestamp": "2026-01-11T16:36:43.706022", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper states that the proposed defense (PAM) uses “extra model updates” (so it requires additional tuning) but does not discuss the common real-world operational practice of keeping purified models frozen after deployment or whether PAM must be applied post-deployment. Thus the paper neither affirms nor denies that the workflow is impractical in scenarios where purified models are kept frozen.", "evidence": "1) \"Path-Aware Minimization (PAM), which promotes deviation along backdoor-connected paths with extra model updates.\" 2) \"These methods can be easily incorporated into the existing model deployment pipeline and have demonstrated state-of-the-art effectiveness in reducing the Attack Success Rate (ASR) of backdoored models.\" 3) \"we further propose the more practical Query-based Reactivation Attack, which can reactivate the implanted backdoor of purified models solely through model querying.\"", "section": "Abstract; Introduction; Section 3 (Revealing Superficial Safety)"}
{"claim": "The paper does not sufficiently justify the real-world implications of its threat model, leaving uncertainty about its practical relevance.", "claim_type": "subjective", "paper_id": "qZFshkbWDo", "paper_title": "Uncovering, Explaining, and Mitigating the Superficial Safety of Backdoor Defense", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "Ni8p8ZQbkI", "reviewer": "Reviewer_R9Lq", "review_text": "Summary: This paper investigates the effectiveness of current purification-based backdoor defenses and tries to uncover whether purified DNNs are truly free from backdoor vulnerabilities. The authors identify the “post-purification robustness” of DNNs and propose Retuning Attack (RA) and Query-based Reactivation Attack (QRA) respectively to assess the susceptibility of purified DNNs to backdoor reactivation. Additionally, the paper proposes Path-Aware Minimization (PAM) to improve post-purification robustness.\n\nStrengths: The work makes contribution in backdoor robustness evaluation by shifting the focus from merely achieving low ASR to evaluating the post-purification robustness of backdoor defenses. The paper introduces a pipeline of methods: RA, QRA, and PAM, for the assessment of this vulnerability.\n\nWeaknesses: The proposed notion of post-purification robustness lies with the assumption that the purified DNN will encounter further fine-tuning. As such, the entire workflow proposed is not practical in a real-world scenario where the purified model is kept frozen with no further updates.\n\nQuestions: Can the authors further justify the proposed threat model’s real-world implications? How can the proposed workflow be enabled in a real-world attack setting?", "labeling_timestamp": "2026-01-11T16:36:45.891294", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper claims the threat model (data-poisoning backdoors) and the proposed Query-based Reactivation Attack (QRA) are practical (Intro, Related Work, Abstract) but does not provide detailed real-world case studies, deployment scenarios, or empirical evidence demonstrating feasibility in operational settings. Thus the reviewer’s claim that the paper does not sufficiently justify real-world implications is partially true: the paper asserts practicality but lacks in-depth justification of practical relevance.", "evidence": "1) \"we further propose the practical Query-based Reactivation Attack (QRA) which could effectively reactivate the backdoor by merely querying purified models.\" (Abstract)\n2) \"Note that data-poisoning attacks are more practical in real-world scenarios as they make fewer assumptions about the attacker's capabilities [5, 15, 41].\" (Related Work)\n3) \"This is particularly important because purified models are often used in various downstream scenarios, such as customized fine-tuning [37] for critical tasks [20].\" (Introduction)", "section": "Abstract; Related Work; 1 Introduction"}
{"claim": "The authors do not explain how the proposed workflow could be enabled or executed in a real-world attack setting.", "claim_type": "methodology", "paper_id": "qZFshkbWDo", "paper_title": "Uncovering, Explaining, and Mitigating the Superficial Safety of Backdoor Defense", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "Ni8p8ZQbkI", "reviewer": "Reviewer_R9Lq", "review_text": "Summary: This paper investigates the effectiveness of current purification-based backdoor defenses and tries to uncover whether purified DNNs are truly free from backdoor vulnerabilities. The authors identify the “post-purification robustness” of DNNs and propose Retuning Attack (RA) and Query-based Reactivation Attack (QRA) respectively to assess the susceptibility of purified DNNs to backdoor reactivation. Additionally, the paper proposes Path-Aware Minimization (PAM) to improve post-purification robustness.\n\nStrengths: The work makes contribution in backdoor robustness evaluation by shifting the focus from merely achieving low ASR to evaluating the post-purification robustness of backdoor defenses. The paper introduces a pipeline of methods: RA, QRA, and PAM, for the assessment of this vulnerability.\n\nWeaknesses: The proposed notion of post-purification robustness lies with the assumption that the purified DNN will encounter further fine-tuning. As such, the entire workflow proposed is not practical in a real-world scenario where the purified model is kept frozen with no further updates.\n\nQuestions: Can the authors further justify the proposed threat model’s real-world implications? How can the proposed workflow be enabled in a real-world attack setting?", "labeling_timestamp": "2026-01-11T16:37:04.597607", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper proposes the Retuning Attack (RA) and the Query-based Reactivation Attack (QRA) and states these are practical, but it does not provide operational details about how an attacker would enable or execute these workflows in a real-world setting (e.g., attacker capabilities, how poisoned samples are obtained for RA, query budgets or procedures for QRA). The paper describes the attacks at a high level but lacks concrete real-world execution steps.", "evidence": "Abstract: \"Based on this, we further propose the practical Query-based Reactivation Attack (QRA) which could effectively reactivate the backdoor by merely querying purified models.\"; Section 3 introduction: \"we then present a practical threat known as the Query-based Reactivation Attack (QRA), which exploits the vulnerabilities in postpurification robustness to reactivate the implanted backdoor in purified models, using only model querying.\"; Section 3: \"we employ the Retuning Attack (RA) where we first retune the purified models using an extremely small number of backdoored samples and tuning epochs.\"", "section": "Abstract; Section 3 (Introduction to Section 3 and Section 3.1 Problem Setup)"}
{"claim": "The paper evaluates vision tasks only at high-level image classification, omitting mid-level and low-level vision tasks such as semantic segmentation.", "claim_type": "experimental", "paper_id": "iynRvVVAmH", "paper_title": "Parameter-Efficient Multi-Task Model Fusion with Partial Linearization", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "aBi4FrnLpB", "reviewer": "Reviewer_tr5D", "review_text": "Summary: Efficient finetuning on the pretrained large model has been an important topic. In this work, a partial linearization method (L-Lora) is proposed under the context of PERF(parameter-efficient finetuning).  The key idea is applying linearization to adapter modules and applies task arithmetic over the linearized adapters. In practice, first-order Tayler expansion is used to linearize the model dynamics at time $t$. Based on the derivation from a neural tangent kernel theory, the hypothesis is that partial linearization of a subset of model parameters during fine-tuning can also improve weight disentanglement compared to full non-linear fine-tuning. CLIP and Flan-T5 are used to verify the hypothesis in vision-language and language domains.\n\nStrengths: - Evaluations are conducted on both vision-language and language tasks. \n- The proposed method achieved significant performance improvement, compared with the standard LoRA strategy.\n\nWeaknesses: - In vision domain, only the high-level vision task like image classification tasks evaluated, the mid-level and low level task are missing, for example, semantic segmentation.\n\nQuestions: - From table 1, seems the L-LoRA method are outperforming full-finetuning under some model fusion settings, do we have some possible illustrations?\n- Is it possible to evaluate L-LoRA on image segmentation foundational models like SAM?", "labeling_timestamp": "2026-01-11T16:37:00.663813", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper reports experiments on image classification and NLP (GLUE) tasks and shows loss-visualizations for image classification (CLIP), but contains no mention or evaluation of mid- or low-level vision tasks such as semantic segmentation.", "evidence": "\"Our experiments on image classification and natural language processing tasks demonstrate that our partial linearization technique enables more effective model fusion...\"; \"We apply our method to the LoRA modules to construct Linearized LoRA (L-LoRA) modules and conduct extensive experiments on seven tasks from the GLUE benchmark to demonstrate that our method is effective in improving the multi-task fusion capability of fine-tuned task-specific models.\"; \"Figure 1: Loss landscape visualization. Here, we visualize the loss landscape ... for CLIP model on combinations of three downstream image classification tasks...\"", "section": "Introduction / Abstract / Contributions / Figure 1 (loss visualization)"}
{"claim": "The authors do not evaluate L-LoRA on image segmentation foundational models like SAM, leaving segmentation applicability untested.", "claim_type": "experimental", "paper_id": "iynRvVVAmH", "paper_title": "Parameter-Efficient Multi-Task Model Fusion with Partial Linearization", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "aBi4FrnLpB", "reviewer": "Reviewer_tr5D", "review_text": "Summary: Efficient finetuning on the pretrained large model has been an important topic. In this work, a partial linearization method (L-Lora) is proposed under the context of PERF(parameter-efficient finetuning).  The key idea is applying linearization to adapter modules and applies task arithmetic over the linearized adapters. In practice, first-order Tayler expansion is used to linearize the model dynamics at time $t$. Based on the derivation from a neural tangent kernel theory, the hypothesis is that partial linearization of a subset of model parameters during fine-tuning can also improve weight disentanglement compared to full non-linear fine-tuning. CLIP and Flan-T5 are used to verify the hypothesis in vision-language and language domains.\n\nStrengths: - Evaluations are conducted on both vision-language and language tasks. \n- The proposed method achieved significant performance improvement, compared with the standard LoRA strategy.\n\nWeaknesses: - In vision domain, only the high-level vision task like image classification tasks evaluated, the mid-level and low level task are missing, for example, semantic segmentation.\n\nQuestions: - From table 1, seems the L-LoRA method are outperforming full-finetuning under some model fusion settings, do we have some possible illustrations?\n- Is it possible to evaluate L-LoRA on image segmentation foundational models like SAM?", "labeling_timestamp": "2026-01-11T16:37:22.530493", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes experiments on image classification and GLUE (NLP) tasks and explicitly states evaluation on seven GLUE tasks for L-LoRA. The manuscript does not mention SAM or any image segmentation foundational models, so segmentation applicability is not evaluated in the paper.", "evidence": "\"We apply our method to the LoRA modules to construct Linearized LoRA (L-LoRA) modules and conduct extensive experiments on seven tasks from the GLUE benchmark to demonstrate that our method is effective in improving the multi-task fusion capability of fine-tuned task-specific models.\" and \"Our experiments on image classification and natural language processing tasks demonstrate that our partial linearization technique enables more effective model fusion...\"", "section": "Contributions (end of Introduction) and Abstract"}
{"claim": "Table 1 appears to show L-LoRA outperforming full fine-tuning in some model fusion settings, but the paper provides no illustrations or explanations for this behavior.", "claim_type": "experimental", "paper_id": "iynRvVVAmH", "paper_title": "Parameter-Efficient Multi-Task Model Fusion with Partial Linearization", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "aBi4FrnLpB", "reviewer": "Reviewer_tr5D", "review_text": "Summary: Efficient finetuning on the pretrained large model has been an important topic. In this work, a partial linearization method (L-Lora) is proposed under the context of PERF(parameter-efficient finetuning).  The key idea is applying linearization to adapter modules and applies task arithmetic over the linearized adapters. In practice, first-order Tayler expansion is used to linearize the model dynamics at time $t$. Based on the derivation from a neural tangent kernel theory, the hypothesis is that partial linearization of a subset of model parameters during fine-tuning can also improve weight disentanglement compared to full non-linear fine-tuning. CLIP and Flan-T5 are used to verify the hypothesis in vision-language and language domains.\n\nStrengths: - Evaluations are conducted on both vision-language and language tasks. \n- The proposed method achieved significant performance improvement, compared with the standard LoRA strategy.\n\nWeaknesses: - In vision domain, only the high-level vision task like image classification tasks evaluated, the mid-level and low level task are missing, for example, semantic segmentation.\n\nQuestions: - From table 1, seems the L-LoRA method are outperforming full-finetuning under some model fusion settings, do we have some possible illustrations?\n- Is it possible to evaluate L-LoRA on image segmentation foundational models like SAM?", "labeling_timestamp": "2026-01-11T16:37:16.663822", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not contain Table 1 or its results, so we cannot verify the claim that Table 1 shows L-LoRA outperforming full fine-tuning. The paper does state that it includes visualizations and analyses of weight disentanglement, but without the table or the experimental sections in the provided content, there is insufficient information to determine whether the paper omits illustrations/explanations for the specific behavior noted in Table 1.", "evidence": "1) \"In some cases, our proposed method is even comparable to full fine-tuning.\" 2) \"In addition to the direct comparison of multi-task model fusion performance, we also visualize the weight disentanglement gain of our method on different downstream task pairs.\" 3) \"We apply our method to the LoRA modules to construct Linearized LoRA (L-LoRA) modules and conduct extensive experiments on seven tasks from the GLUE benchmark to demonstrate that our method is effective in improving the multi-task fusion capability of fine-tuned task-specific models.\"", "section": "1 INTRODUCTION"}
{"claim": "The paper lacks details on how the image encoder accepts increased numbers of input views.", "claim_type": "methodology", "paper_id": "2dfBpyqh0A", "paper_title": "Gaussian Graph Network: Learning Efficient and Generalizable Gaussian Representations from Multi-view Images", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "J9ZvJzmWl3", "reviewer": "Reviewer_LnYN", "review_text": "Comment: Thanks the authors for the detailed response. Could the authors share more details about how the image encoder accepts more input views since I guess the multi-view epipolar attention in both pixelSplat and MVSplat would cost a lot of time and memory when the number of views increase? Did the author perform standard multi-view epipolar attention (for example, pair-wise) or is there any modification made here? Thanks.", "labeling_timestamp": "2026-01-11T16:37:29.972162", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper only states that it takes N input images and uses a 2D backbone (following prior works) to extract per-view features, but it does not describe concrete architectural or training details for how the image encoder handles increased numbers of input views or variable N beyond this high-level description.", "evidence": "\"Given N input images I = { I i } ∈ R N × H × W × 3 and their corresponding camera parameters C = { c i } , we follow the instructions of pixelSplat [4] and MVSplat [6] to extract image features: ... where Φ_image is a 2D backbone. We predict the means and features of pixel-aligned Gaussians: ... where Φ_depth and Φ_feat stand for neural networks to predict depth maps and Gaussian features, and ψ_unproj is the unprojection operation.\"", "section": "3.1 Building Gaussian Graphs"}
{"claim": "Multi-view epipolar attention in pixelSplat and MVSplat likely incurs high time and memory costs as the number of views increases.", "claim_type": "experimental", "paper_id": "2dfBpyqh0A", "paper_title": "Gaussian Graph Network: Learning Efficient and Generalizable Gaussian Representations from Multi-view Images", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "J9ZvJzmWl3", "reviewer": "Reviewer_LnYN", "review_text": "Comment: Thanks the authors for the detailed response. Could the authors share more details about how the image encoder accepts more input views since I guess the multi-view epipolar attention in both pixelSplat and MVSplat would cost a lot of time and memory when the number of views increase? Did the author perform standard multi-view epipolar attention (for example, pair-wise) or is there any modification made here? Thanks.", "labeling_timestamp": "2026-01-11T16:37:44.323230", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly states that pixelSplat and MVSplat combine pixel-aligned Gaussians from multiple views leading to extra memory cost and produce several times as many Gaussians (which implies higher resource usage). The paper also claims previous methods' performance declines as input views increase and contrasts this with its higher rendering speed, implying prior methods are slower with more views. However, the paper does not explicitly analyze or attribute these costs specifically to \"multi-view epipolar attention\" nor provide a detailed time-complexity analysis as views increase.", "evidence": "these methods simply combine pixel-aligned Gaussians from multiple views as scene representations, thereby leading to artifacts and extra memory cost\nAs shown in Figure 1a, pixelSplat [4] and MVSplat [6] suffer from artifacts with several times as many Gaussians as ours\nWhile the performance of previous methods declines as the number of input views increases, our method can benefit from more input views\nCompared to the state-of-the-art methods, our model uses fewer Gaussians and achieves better image quality with higher rendering speed", "section": "Abstract; Introduction"}
{"claim": "The paper does not state whether it uses standard pair-wise multi-view epipolar attention.", "claim_type": "methodology", "paper_id": "2dfBpyqh0A", "paper_title": "Gaussian Graph Network: Learning Efficient and Generalizable Gaussian Representations from Multi-view Images", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "J9ZvJzmWl3", "reviewer": "Reviewer_LnYN", "review_text": "Comment: Thanks the authors for the detailed response. Could the authors share more details about how the image encoder accepts more input views since I guess the multi-view epipolar attention in both pixelSplat and MVSplat would cost a lot of time and memory when the number of views increase? Did the author perform standard multi-view epipolar attention (for example, pair-wise) or is there any modification made here? Thanks.", "labeling_timestamp": "2026-01-11T16:37:35.417847", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper mentions prior works that use epipolar transformers (Introduction) and says it \"follows the instructions of pixelSplat and MVSplat to extract image features\" (Sec. 3.1), but it does not explicitly state whether it employs standard pair-wise multi-view epipolar attention or describes that component in its own architecture.", "evidence": "1) \"Typically, these methods [6, 4] generate pixel-aligned Gaussians with U-Net architectures, epipolar transformers or cost volume representations for depth estimation and parameter predictions, and directly combine Gaussian groups obtained from different views as scene representations.\" 2) \"we follow the instructions of pixelSplat [4] and MVSplat [6] to extract image features:\" (followed by formulas)", "section": "Introduction; 3.1 Building Gaussian Graphs"}
{"claim": "The paper does not describe any modifications to multi-view epipolar attention to mitigate time and memory scaling.", "claim_type": "methodology", "paper_id": "2dfBpyqh0A", "paper_title": "Gaussian Graph Network: Learning Efficient and Generalizable Gaussian Representations from Multi-view Images", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "J9ZvJzmWl3", "reviewer": "Reviewer_LnYN", "review_text": "Comment: Thanks the authors for the detailed response. Could the authors share more details about how the image encoder accepts more input views since I guess the multi-view epipolar attention in both pixelSplat and MVSplat would cost a lot of time and memory when the number of views increase? Did the author perform standard multi-view epipolar attention (for example, pair-wise) or is there any modification made here? Thanks.", "labeling_timestamp": "2026-01-11T16:37:48.369637", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper references epipolar transformers as used by prior work but presents a graph-based approach (Gaussian Graphs/Graph Network) and graph pruning to limit complexity. It does not describe any modifications to multi-view epipolar attention or epipolar transformers to address time or memory scaling.", "evidence": "1) \"Typically, these methods [6, 4] generate pixel-aligned Gaussians with U-Net architectures, epipolar transformers or cost volume representations for depth estimation and parameter predictions, and directly combine Gaussian groups obtained from different views as scene representations.\" 2) \"To limit further computational complexity, we prune the graph by preserving edges with top n weights and ignore other possible edges.\" 3) \"we extend the scalar weight a_sk of an edge to a matrix E_{s->k} = [ e_{s->k}^{ij} ]_{HW×HW}, which depicts the detailed relations at Gaussian-level between v_s and v_k.\"", "section": "Introduction; 3.1 Building Gaussian Graphs; 3.2 Gaussian Graph Network"}
{"claim": "The paper applies the detection algorithm from [1] without changes and fails to give proper credit to that prior work.", "claim_type": "methodology", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:37:48.700525", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper describes FedGT as a novel framework inspired by group testing and explicitly discusses related prior work (So et al., Pillutla et al., Zhang et al.). It presents its own grouping/assignment matrix and decoding viewpoint (parity-check matrix / conventional decoding), rather than stating it directly applies a specific prior detection algorithm unchanged or hiding credit. The Related Work section cites and contrasts closest existing methods, so the claim that the paper simply applies the detection algorithm from [1] without changes and fails to credit it is contradicted by the text.", "evidence": "1) \"In this paper, we propose FedGT, a novel framework for identifying malicious clients in FL with secure aggregation.\" (Introduction)\n\n2) \"To the best of our knowledge, only the works (So et al., 2021; Pillutla et al., 2022; Zhang et al., 2021) address resiliency against poisoning attacks in conjunction with secure aggregation... The approach in Zhang et al. (2021) is the method closest to ours as it relies on dividing clients into subgroups and on testing the group aggregates. However, contrary to FedGT, it is unable to identify malicious clients and to leverage the information of overlapping groups.\" (Related Work)\n\n3) \"Thus, a suitable choice for A is the parity-check matrix of a powerful error-correcting code, i.e., with good distance properties. Furthermore, d can be inferred by applying conventional decoding techniques.\" (Section 4)", "section": "Introduction; Related Work; 4 FEDGT"}
{"claim": "Section 5 largely re-derives results from [1], wasting space and compromising the explanation of the decoding method.", "claim_type": "presentation", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:37:47.675165", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper content stops at Section 4 and contains no Section 5 or citations labeled [1], so it is impossible to verify the reviewer’s claim about Section 5 re-deriving results from [1].", "evidence": "## 4 FEDGT: GROUP TESTING FOR FL WITH SECURE AGGREGATION\n\nWe consider a population of n clients, n_m of which are malicious. We define the defective vector d = (d_1, d_2, . . . , d_n) ...", "section": "Section 4 (no Section 5 present in the provided content)"}
{"claim": "The component using optimal coding techniques is mostly based on prior coding theory work and provides little to no technical novelty.", "claim_type": "novelty", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:37:53.843445", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly states that the assignment matrix A is analogous to a parity-check matrix and that conventional decoding techniques from coding theory can be applied to infer malicious clients, indicating this component builds on prior coding theory rather than introducing new coding-theoretic methods.", "evidence": "\"The design of the assignment matrix A and the corresponding inference problem is akin to an error-correcting coding problem, where the assignment matrix A can seen as the parity-check matrix of a code, and the inference problem corresponds to a decoding operation based on A and t. Thus, a suitable choice for A is the parity-check matrix of a powerful error-correcting code, i.e., with good distance properties. Furthermore, d can be inferred by applying conventional decoding techniques.\"", "section": "4 FEDGT: GROUP TESTING FOR FL WITH SECURE AGGREGATION"}
{"claim": "The paper fails to explain which coding scheme is used, what specific test is used, and how Q(t|s) is chosen for the federated setting.", "claim_type": "methodology", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:38:28.289717", "model": "gpt-5-mini", "label": "Supported", "justification": "Section 4 describes the design choices only at a high level and explicitly leaves them unspecified: the assignment matrix A is suggested in general to be a parity-check matrix of a code (no specific code named), the test t is described abstractly and the framework is said to be applicable to any test, and the test noise distribution Q(t|s) is stated to be (unknown). Thus the paper does not explain which specific coding scheme, which specific test, nor how Q(t|s) is chosen for the federated setting.", "evidence": "“A suitable choice for A is the parity-check matrix of a powerful error-correcting code, i.e., with good distance properties. Furthermore, d can be inferred by applying conventional decoding techniques.”; “The central server then applies a test t : u →{0,1} to the aggregate model for each test group ... We remark that the proposed framework is general and can be applied to any test on the test group aggregates.”; “In general, the (noisy) test vector t is statistically dependent on the syndrome vector s according to an (unknown) probability distribution Q(t|s).”", "section": "4 FEDGT: GROUP TESTING FOR FL WITH SECURE AGGREGATION"}
{"claim": "The authors assume the defender knows δ, an assumption that is unrealistic in practice and not justified in experiments.", "claim_type": "methodology", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:38:10.483137", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper states the number of malicious clients as n_m and that the defective vector d is unknown, but it does not explicitly state that the server (defender) knows δ (or n_m), nor does the provided excerpts describe experiments that assume or justify knowledge of δ. There is insufficient information to confirm the reviewer's claim.", "evidence": "“We consider a cross-silo scenario with an honest-but-curious server and n clients out of which n_m are compromised (referred to as malicious clients).”\n\n“Note that d is unknown, i.e., we do not know a priori which clients are malicious.”\n\n“We consider a population of n clients, n_m of which are malicious. We define the defective vector d = (d1, d2, ..., dn) ... It follows that ∑_{j=1}^n d_j = n_m.”", "section": "Section 3 (Attack model) and Section 4 (FedGT: Group testing for FL with secure aggregation)"}
{"claim": "The paper models the threshold Λ as an uninterpretable tunable parameter, making tuning difficult and sensitive compared to using Δ'.", "claim_type": "methodology", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:38:25.343575", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper text does not mention any threshold denoted \"Λ\" or \"Δ'\", nor does it describe modeling a threshold as an uninterpretable tunable parameter or compare tuning sensitivity between such a Λ and a Δ'. Therefore the claim cannot be verified from the paper.", "evidence": "In FedGT, for each test group, a secure aggregation mechanism is employed to reveal only the aggregate of the client models in the test group to the server. Let u_i, i ∈ [m], be the aggregate model of test group i. The central server then applies a test t : u → {0,1} to the aggregate model for each test group to be used for the identification of malicious clients in the group. Let t_i = t(u_i) ∈ {0,1} be the result of the test for test group i, where t_i = 1 if the test is positive and t_i = 0 if the test is negative.", "section": "Section 4 (FEDGT: GROUP TESTING FOR FL WITH SECURE AGGREGATION)"}
{"claim": "The Q(t|s) model depends only on a single error probability p, which is overly simplistic and unlikely to reflect individual test failures.", "claim_type": "methodology", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:38:16.174707", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states the test-noise model Q(t|s) is an (unknown) probability distribution and does not assume it depends on a single error probability p. Thus the claim that Q(t|s) depends only on a single p is not supported and is contradicted by the paper's description.", "evidence": "In general, the (noisy) test vector t is statistically dependent on the syndrome vector s according to an (unknown) probability distribution Q ( t | s ).", "section": "4 FEDGT: GROUP TESTING FOR FL WITH SECURE AGGREGATION"}
{"claim": "Hyperparameter ρ is likely dependent on heterogeneity and problem complexity and will therefore be hard to tune in practical deployments.", "claim_type": "experimental", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:38:16.989798", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does not mention a hyperparameter named ρ nor discuss its dependence on data heterogeneity or problem complexity or the practical difficulty of tuning it. The paper describes design choices (group size, number, overlap, assignment matrix) but provides no statements about a ρ hyperparameter or its tuning behavior in deployments.", "evidence": "\"By choosing the size, number, and overlap between groups, FedGT strikes a balance between privacy and security.\" (Abstract)\n\n\"Matrix A -and hence the corresponding bipartite graph-is a design choice that may be decided offline, analogous to the model architecture, and shared with the clients for transparency.\" (Section 4)", "section": "Abstract; Section 4"}
{"claim": "Choosing Λ heuristically requires multiple retrainings, making hyperparameter tuning impractical for realistic federated learning workflows.", "claim_type": "methodology", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:38:55.550992", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does not mention a parameter named Λ nor discuss heuristic selection of such a parameter requiring multiple retrainings or making hyperparameter tuning impractical. It describes the assignment matrix A (group design) as a design choice that “may be decided offline,” which neither supports nor substantiates the reviewer’s claim about repeated retrainings for tuning Λ.", "evidence": "“Matrix A -and hence the corresponding bipartite graph-is a design choice that may be decided offline, analogous to the model architecture, and shared with the clients for transparency.”", "section": "Section 4 (FEDGT: GROUP TESTING FOR FL WITH SECURE AGGREGATION)"}
{"claim": "Experiments retain the known target assumption for targeted attacks, which unrealistically advantages the defender and inflates reported performance.", "claim_type": "methodology", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:38:35.819646", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper mentions evaluation under targeted data-poisoning attacks (e.g., 'targeted label-flipping attack') but does not state in the provided text whether experiments assume the defender knows the attack target. There is insufficient information in the supplied sections to confirm the 'known target' assumption.", "evidence": "Abstract: \"...under different data-poisoning attacks... in the presence of targeted attacks (CIFAR-10 and ISIC2019).\"; Introduction: \"Fig. 1 illustrates the performance of FedGT for a scenario with 15 clients and a targeted label-flipping attack.\"", "section": "Abstract; 1 Introduction"}
{"claim": "The paper does not provide experiments where the defender does not know the targets for targeted attacks, leaving a key scenario untested.", "claim_type": "experimental", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:38:35.466313", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper mentions experiments under targeted and untargeted data-poisoning attacks (Abstract, Introduction) but does not state whether the defender (server) is assumed to know the attack targets or whether experiments include the scenario where the defender does not know the targets. The provided sections do not address this specific experimental condition.", "evidence": "Abstract: \"We showcase FedGT's effectiveness in identifying malicious clients through experiments on the MNIST, CIFAR-10, and ISIC2019 datasets under both targeted and untargeted data-poisoning attacks.\"; Introduction: \"We showcase FedGT's effectiveness in identifying malicious clients through experiments on the MNIST, CIFAR-10, and ISIC2019 datasets under both targeted and untargeted data-poisoning attacks.\"", "section": "Abstract; 1 INTRODUCTION"}
{"claim": "The method often fails to outperform the baseline [2], particularly on simple settings like MNIST as shown in Figures 4 and 5 in the appendix.", "claim_type": "baseline", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:38:53.292055", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper reports experiments on MNIST, CIFAR-10, and ISIC2019 and explicitly states that FedGT significantly outperforms the baseline (RFA) on CIFAR-10 and ISIC2019, but the provided content does not present the appendix Figures 4 and 5 nor any explicit claim that FedGT often fails to outperform RFA on MNIST. Therefore there is insufficient information in the supplied paper content to verify the reviewer's specific assertion about failing on MNIST (Figures 4 and 5).", "evidence": "“The effectiveness of FedGT is demonstrated through extensive experiments on the MNIST, CIFAR-10, and ISIC2019 datasets ... We further show that FedGT significantly outperforms the private robust aggregation approach based on the geometric median recently proposed by Pillutla et al. on heterogeneous client data (ISIC2019) and in the presence of targeted attacks (CIFAR-10 and ISIC2019).”", "section": "Abstract"}
{"claim": "The paper does not report how many clients are securely aggregated by the baseline [2], preventing fair apples-to-apples comparisons.", "claim_type": "baseline", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:38:49.457410", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper discusses and compares to the baseline of Pillutla et al. (RFA) in the Abstract and Related Work, but nowhere in the provided content does it report how many clients the baseline securely aggregates (i.e., the group size or aggregation configuration), so the claim that this information is not reported is true according to the paper.", "evidence": "Abstract: \"We further show that FedGT significantly outperforms the private robust aggregation approach based on the geometric median recently proposed by Pillutla et al. on heterogeneous client data (ISIC2019) and in the presence of targeted attacks (CIFAR-10 and ISIC2019).\"\nRelated Work: \"In (Pillutla et al., 2022), a robust aggregation protocol (dubbed RFA) based on an approximate geometric median (computed exploiting secure aggregation) is proposed. However, this protocol lacks the capability to identify malicious clients and is known to be inferior to other robust aggregation techniques, especially when dealing with heterogeneous client data (Li et al., 2023).\"", "section": "Abstract; 2 RELATED WORK"}
{"claim": "If baseline [2] aggregates all clients, the comparison is unfavorable to the proposed method and the authors do not discuss this possibility.", "claim_type": "baseline", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:39:02.552152", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states that secure aggregation corresponds to group size equal to the total number of clients and directly compares FedGT to the RFA baseline (Pillutla et al.), claiming FedGT outperforms RFA in heterogeneous data and targeted attacks. Thus the reviewer's assertion that such a baseline aggregation would make the comparison unfavorable and is not discussed is contradicted by the paper.", "evidence": "Abstract: \"vanilla federated learning and secure aggregation correspond to the extreme cases of FedGT with group size equal to one and the total number of clients, respectively.\" Abstract: \"We further show that FedGT significantly outperforms the private robust aggregation approach based on the geometric median recently proposed by Pillutla et al. on heterogeneous client data (ISIC2019) and in the presence of targeted attacks (CIFAR-10 and ISIC2019).\" Related Work: \"In (Pillutla et al., 2022), a robust aggregation protocol (dubbed RFA) based on an approximate geometric median (computed exploiting secure aggregation) is proposed.\"", "section": "Abstract; Related Work"}
{"claim": "The method often reduces clean accuracy more than baseline [2] and the no-defense case, potentially due to excessive false alarm rates.", "claim_type": "baseline", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:39:04.646370", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper reports that FedGT achieves high model utility and explicitly states it attains low false-alarm (and misdetection) probabilities and outperforms the private robust-aggregation baseline (Pillutla et al.). Thus the claim that the method often reduces clean accuracy more than baseline [2] and no-defense due to excessive false alarms contradicts the paper's reported results and statements.", "evidence": "1) \"These experiments showcase FedGT's ability to identify malicious clients, resulting in high model utility.\" 2) \"Remarkably, FedGT enables the identification and removal of malicious clients with low misdetection and false alarm probabilities. This leads to a substantially reduced attack accuracy—significantly outperforming the recently-proposed robust federated aggregation (RFA) protocol based on the geometric median (Pillutla et al., 2022) for the CIFAR-10 and ISIC2019 datasets...\"", "section": "ABSTRACT; 1 INTRODUCTION"}
{"claim": "Experiments fail to report false alarm (FA) and missed detection (MD) rates for all evaluations, omitting key defense performance metrics.", "claim_type": "experimental", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:39:22.768330", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper defines misdetection and false-alarm probabilities and asserts that FedGT achieves low values (Introduction), but the provided content does not include experimental tables, numbers, or plots explicitly reporting FA/MD rates across the evaluations (MNIST, CIFAR-10, ISIC2019). Therefore, from the supplied text it is not possible to confirm the reviewer's claim that the experiments fail to report these metrics for all evaluations.", "evidence": "1) \"Remarkably, FedGT enables the identification and removal of malicious clients with low misdetection and false alarm probabilities.\" (Introduction/Abstract). 2) \"The performance of FedGT, measured in terms of the utility of the model, is affected by two quantities: the misdetection probability, i.e., the probability that a malicious client is flagged as non-malicious, and the false-alarm probability, i.e., the probability that a non-malicious client is flagged as malicious, defined as\" (Section 4).", "section": "Introduction and Section 4"}
{"claim": "The authors do not provide experiments that vary Λ to show its effects on FA and MD rates, attacker accuracy, and model clean accuracy.", "claim_type": "experimental", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:39:21.874247", "model": "gpt-5-mini", "label": "Supported", "justification": "The provided paper text describes that group size/overlap are design choices and defines misdetection/false-alarm metrics, but no experiments are shown that vary a parameter named Λ (or explicitly vary group-design parameter to report its effect) to measure FA/MD, attacker accuracy, and clean accuracy. The only experimental figure mentioned varies number of malicious clients, not Λ.", "evidence": "1) \"By choosing the size, number, and overlap between groups, FedGT strikes a balance between privacy and security.\" (ABSTRACT)\n2) \"Figure 1: Attack accuracy MNIST (left), CIFAR10 (mid), and ISIC2019 (right) datasets for varying number of malicious clients.\" (INTRODUCTION / Fig. 1 caption)\n3) \"The performance of FedGT, measured in terms of the utility of the model, is affected by two quantities: the misdetection probability... and the false-alarm probability...\" (Section 4)", "section": "ABSTRACT / INTRODUCTION / Section 4 (FEDGT)"}
{"claim": "The paper does not quantify the communication complexity of the chosen coding strategies in the main text, relegating details to the appendix.", "claim_type": "quantitative", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:39:26.877099", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The main text (Abstract, Introduction, and Sections 3–4 provided) asserts that FedGT achieves lower communication complexity but does not present any quantified communication-cost expressions or detailed complexity analysis in those sections. However, the claim that such details are 'relegated to the appendix' cannot be confirmed from the provided content because the appendix is not included.", "evidence": "\"...significantly outperforming the recently-proposed robust federated aggregation (RFA) protocol based on the geometric median (Pillutla et al., 2022) for the CIFAR-10 and ISIC2019 datasets, while achieving a lower communication complexity.\"", "section": "Abstract (no quantified communication-complexity expressions appear in Introduction, Sec. 3–4 of the provided main text)"}
{"claim": "Communication complexity details are only available in the appendix, so main-paper readers cannot assess communication costs without consulting supplementary material.", "claim_type": "methodology", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:39:25.423984", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided main-paper content (Abstract, Introduction, Related Work, Preliminaries, start of Section 4) refers to achieving \"lower communication complexity\" but contains no detailed communication-complexity analysis or formulas. The claim asserts those details are only in the appendix, but the appendix is not provided here, and the main text shown does not explicitly state that full communication-complexity details are deferred to the appendix. Therefore we cannot verify the reviewer’s assertion from the available content.", "evidence": "“This leads to a substantially reduced attack accuracy-significantly outperforming the recently-proposed robust federated aggregation (RFA) protocol based on the geometric median (Pillutla et al., 2022) for the CIFAR-10 and ISIC2019 datasets, while achieving a lower communication complexity.”", "section": "ABSTRACT"}
{"claim": "The authors claim cross-silo FL supports up to 100 devices but evaluate only 15 clients and fail to scale beyond 31 with more precise encodings.", "claim_type": "experimental", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:39:50.515505", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly states the cross-silo setting is 'moderate (up to 100)' and presents an experiment scenario with 15 clients (Fig. 1), so the first two parts of the claim are supported by the paper. However, the paper does not mention any failure to scale beyond 31 clients nor any discussion of 'more precise encodings'; that part of the reviewer claim is not supported or addressed in the paper.", "evidence": ["Our focus is specifically on the cross-silo scenario, wherein the number of clients is moderate (up to 100 ) and data-poisoning is the predominant attack vector (Shejwalkar et al., 2022).", "Fig. 1 illustrates the performance of FedGT for a scenario with 15 clients and a targeted label-flipping attack."], "section": "Introduction"}
{"claim": "The method appears computationally expensive on the server side when more than a handful of clients are available, jeopardizing practical applicability.", "claim_type": "experimental", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:40:01.526600", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does not provide an analysis or measurements of server-side computational cost as the number of clients grows. It explicitly focuses on a cross-silo setting with a moderate number of clients (up to 100) and emphasizes lower communication complexity, but it does not state or quantify that the method is computationally expensive on the server for larger client populations.", "evidence": "\"Our focus is specifically on the cross-silo scenario, wherein the number of clients is moderate (up to 100 ) and data-poisoning is the predominant attack vector (Shejwalkar et al., 2022).\" \n\n\"This leads to a substantially reduced attack accuracy—significantly outperforming the recently-proposed robust federated aggregation (RFA) protocol based on the geometric median (Pillutla et al., 2022) for the CIFAR-10 and ISIC2019 datasets, while achieving a lower communication complexity.\"", "section": "1 INTRODUCTION"}
{"claim": "The paper does not provide runtime measurements for different client counts and encoding choices, nor worst-case or average-case big-O runtime analysis.", "claim_type": "experimental", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:39:42.252078", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes experimental evaluations of FedGT (accuracy, misdetection/false-alarm, communication complexity) and discusses using conventional decoding techniques, but it does not include any empirical runtime measurements across varying client counts or encoding/grouping choices, nor does it present worst-case or average-case Big-O runtime analysis anywhere in the provided sections (Abstract, Introduction, Related Work, Preliminaries, Section 4).", "evidence": "\"d can be inferred by applying conventional decoding techniques.\"", "section": "4 FEDGT: GROUP TESTING FOR FL WITH SECURE AGGREGATION"}
{"claim": "Section 5 lacks justification for the forward and backward formulas used in the decoding approach, leaving key equations unexplained.", "claim_type": "methodology", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:40:19.048489", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper content ends at Section 4 and does not include Section 5, so there is insufficient information to verify whether Section 5 lacks justification for the forward and backward formulas. Section 4 mentions that decoding can be done with conventional techniques but does not show the detailed formulas; however, that does not confirm whether Section 5 (which might contain those justifications) is missing or inadequate.", "evidence": "1) \"Given the test results t and the assignment matrix A , the goal of FedGT is to identify the malicious clients, i.e., infer the defective vector d .\" 2) \"d can be inferred by applying conventional decoding techniques.\" 3) \"In general, the (noisy) test vector t is statistically dependent on the syndrome vector s according to an (unknown) probability distribution Q ( t | s ) .\"", "section": "Sections 1–4 (Section 5 not included in the provided content)"}
{"claim": "The paper takes the simplest instantiation choices from [1] (simple test, tunable Λ, assumed δ), failing to adapt or justify them for the federated setting.", "claim_type": "methodology", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:40:02.991174", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper text describes a general FedGT framework and states that it can use any test and that the assignment matrix A is a design choice, but the excerpts do not show the specific instantiation choices (e.g., the \"simple test\", tunable Λ, assumed δ) nor any justification or adaptation decisions. Therefore there is insufficient evidence in the given content to confirm the reviewer’s claim.", "evidence": "\"We remark that the proposed framework is general and can be applied to any test on the test group aggregates.\"", "section": "4 FEDGT: GROUP TESTING FOR FL WITH SECURE AGGREGATION"}
{"claim": "The authors do not explain what guarantees the chosen coding strategies provide for the federated learning problem being solved.", "claim_type": "methodology", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:40:16.040667", "model": "gpt-5-mini", "label": "Supported", "justification": "Section 4 frames the assignment matrix A as analogous to a parity-check matrix and recommends using a 'powerful error-correcting code' but provides no formal guarantees, bounds, or analysis tying code choices to detection/false-alarm performance for the federated learning problem. While misdetection and false-alarm probabilities are defined, the paper does not derive guarantees relating them to coding design.", "evidence": "“The design of the assignment matrix A and the corresponding inference problem is akin to an error-correcting coding problem, where the assignment matrix A can seen as the parity-check matrix of a code, and the inference problem corresponds to a decoding operation based on A and t. Thus, a suitable choice for A is the parity-check matrix of a powerful error-correcting code, i.e., with good distance properties. Furthermore, d can be inferred by applying conventional decoding techniques.”\n\n“ The performance of FedGT, measured in terms of the utility of the model, is affected by two quantities: the misdetection probability, i.e., the probability that a malicious client is flagged as non-malicious, and the false-alarm probability, i.e., the probability that a non-malicious client is flagged as malicious, defined as”", "section": "4 FEDGT: GROUP TESTING FOR FL WITH SECURE AGGREGATION"}
{"claim": "The paper does not justify the reasonableness of tests based on additional validation data and known attack parameters such as targeted classes and malicious client counts.", "claim_type": "experimental", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:40:42.306585", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes a generic test function t on group aggregates and treats the test error distribution as unknown, but does not present or justify concrete tests that rely on additional validation data or on known attack parameters (e.g., targeted classes or known malicious client counts). While experiments mention targeted attacks, the method section leaves test design general and does not justify the reasonableness of tests based on validation data or assumed attack parameters.", "evidence": "1) \"The central server then applies a test t : u →{ 0 , 1 } to the aggregate model for each test group to be used for the identification of malicious clients in the group. ... We remark that the proposed framework is general and can be applied to any test on the test group aggregates.\" 2) \"Note that the result of a test may be erroneous, i.e., the result of the test may be t_i = 1 even if no malicious clients are present or t_i = 0 even if malicious clients are present. In general, the (noisy) test vector t is statistically dependent on the syndrome vector s according to an (unknown) probability distribution Q(t | s).\" 3) \"We consider a cross-silo scenario with an honest-but-curious server and n clients out of which n_m are compromised (referred to as malicious clients). ... In this paper, we focus on data-poisoning attacks, which constitute the most realistic type of attack for cross-silo FL (Shejwalkar et al., 2022).\"", "section": "Section 4 (FedGT: Group Testing for FL with Secure Aggregation); Section 3 (Preliminaries)"}
{"claim": "Figure 4 uses a logarithmic scale for attacker accuracy, which can obscure interpretation and make non-log-distributed results hard to read.", "claim_type": "presentation", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:40:38.483163", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper content does not contain Figure 4 nor any statement that a logarithmic scale is used for attacker accuracy. The document only shows Figure 1 (attack accuracy) and Figure 2 (bipartite graph), so there is insufficient information to verify the reviewer’s claim about Figure 4.", "evidence": "Figure 1: Attack accuracy MNIST (left), CIFAR10 (mid), and ISIC2019 (right) datasets for varying number of malicious clients.\n\nFigure 2: Bipartite graph (left) and trellis (right) representation of the assignment matrix A in Example 1.", "section": "1 INTRODUCTION (Figure 1) and 4 FEDGT (Figure 2)"}
{"claim": "CIFAR10 attack success decreases even without defense in Figure 4, and the authors do not explain this unexpected behavior.", "claim_type": "experimental", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:40:22.913622", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states that when no defense is in place the attack success increases with the number of malicious clients (including CIFAR-10), so the reviewer's claim that CIFAR-10 attack success decreases without defense is contrary to the paper. The paper therefore does not exhibit the unexpected decrease the reviewer describes.", "evidence": "\"When no defense mechanism is in place, the attack success significantly increases as the number of malicious clients grows.\"", "section": "1 INTRODUCTION"}
{"claim": "The notation s = d ∨ A^T is introduced without explaining the nonstandard matrix multiplication semantics, confusing readers unfamiliar with group testing.", "claim_type": "presentation", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:40:49.099502", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly defines the syndrome vector component-wise (s_i = 1 if at least one client in group i is malicious, else 0) and states that ∨ is logical disjunction, so the boolean/matrix semantics are explained in Section 4. The exact notation 's = d ∨ A^T' does not appear in the provided text.", "evidence": "We define the syndrome vector s = ( s1 , . . . , s m ) , where s i = 1 if at least one client participating in test group i is malicious and s i = 0 if no client participating in test group i is malicious, i.e., <!-- formula-not-decoded --> where ∨ is the logical disjunction.", "section": "4 FEDGT: GROUP TESTING FOR FL WITH SECURE AGGREGATION"}
{"claim": "The last paragraph on page 3 fails to define when a test is considered positive, leaving ambiguity about whether positivity indicates presence or absence of malicious clients.", "claim_type": "methodology", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:40:42.443975", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly defines what a positive test means (t_i = 1) and links test outcomes to the presence of malicious clients via the syndrome s, stating that for perfect tests t = s. Thus there is no ambiguity as claimed.", "evidence": "\"Let t_i = t(u_i) ∈ {0,1} be the result of the test for test group i, where t_i = 1 if the test is positive and t_i = 0 if the test is negative.\" \"We define the syndrome vector s = (s_1, . . . , s_m), where s_i = 1 if at least one client participating in test group i is malicious and s_i = 0 if no client participating in test group i is malicious... For perfect (non-noisy) test results, it follows that t = s.\"", "section": "4 FEDGT: GROUP TESTING FOR FL WITH SECURE AGGREGATION"}
{"claim": "The authors give the defender extra information in experiments that may favor their method relative to baselines, but do not justify these experimental advantages.", "claim_type": "baseline", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:40:55.040784", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper text describes experiments comparing FedGT to baselines (Abstract, Intro) but does not include experimental-detail statements that the defender was given extra information that could favor FedGT, nor any discussion justifying such experimental advantages. The excerpt lacks the experimental protocol and training/baseline setup needed to confirm or refute the reviewer's claim.", "evidence": "Abstract: \"The effectiveness of FedGT is demonstrated through extensive experiments on the MNIST, CIFAR-10, and ISIC2019 datasets in a cross-silo setting under different data-poisoning attacks. These experiments showcase FedGT's ability to identify malicious clients, resulting in high model utility. We further show that FedGT significantly outperforms the private robust aggregation approach based on the geometric median recently proposed by Pillutla et al. on heterogeneous client data (ISIC2019) and in the presence of targeted attacks (CIFAR-10 and ISIC2019).\"", "section": "Abstract"}
{"claim": "The baseline [2] cannot exploit the security-versus-privacy trade-off described, and the authors do not account for this fundamental comparison asymmetry.", "claim_type": "baseline", "paper_id": "rDgw3yX2aO", "paper_title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "LjGi6U2LkK", "reviewer": "Reviewer_CiYQ", "review_text": "Summary: The paper proposes a new scheme for defending against poisoning attacks in cross-silo FL under secure aggregation. The scheme uses optimal coding techniques to split the federated learning clients into possibly overlapping groups. Each group's models are securely aggregated into a single model the server observes. Each group's aggregated model is tested by the server separately on auxiliary data for maliciousness. This allows the server to detect the presence of malicious or severely out-of-distribution clients in each group and, from that, deduce the identity of the malicious clients based on their group participation patterns cleverly chosen by the server. Finally, the server removes the perceived malicious clients from the federated process. The scheme tradeoffs privacy and security by allowing different sizes of groups to be chosen by the server, which, thanks to the optimal coding scheme used, also corresponds to the effective secure aggregation size of the FL process. The authors show experiments on different datasets and models, demonstrating that the scheme can be effective at defending against malicious clients while ensuring the privacy of benign clients.\n\nStrengths: - Solves an important problem\n- The proposed method is an interesting application of group testing theory.\n- Encouraging results that such a method can work in this setting despite the simplistic instantiation of it\n- The method can work even when applied only at a single communication round.\n- Interesting connection between coding theory and privacy of secure aggregation\n\nWeaknesses: - **Structure and Novelty of the submission:**   \nThere are three major components to this paper. First, the paper suggests the use of group-testing strategies to determine which clients are malicious in the context of the cross-silo FL setup with secure aggregation. In this context, the authors prove how overlapping test groups affect the overall client privacy, as secure aggregation is not commonly applied over overlapping groups. This part of the paper is novel and constitutes the biggest technical contribution of the paper. Then, the authors use optimal codes to determine an efficient way to group clients such that a chosen level of privacy is ensured while the decoding ( aka malicious client identification ) can work well. This is mostly based on prior work from coding theory and contains little to no technical novelty. Finally, the paper then applies the optimal detection strategy from [1] without any changes to the problem of malicious client identification. The paper does not give proper credit to [1], given that it essentially just applies the algorithm presented there and instead re-derives most of the work in Section 5. To this end, there is little to no novel technical contribution in that part of the paper either. One area where the paper could have expanded on [1] and could have provided a real technical contribution is instantiating the many choices of [1] to the FL problem considered. Unfortunately, the paper does very little of that as it takes the absolute simplest choice for a test, models the threshold $\\Lambda$ as a tunable hyperparameter ( pretty much independent of the proposed FA/MD trade-off ), assumes $\\delta$ is known ( something that even the authors agree is not a reasonable assumption ), and use embarrassingly simple $Q(t|s)$ model. All in all, therefore, the paper provides very little novel technical contribution, which is sad, as the idea of applying group testing to this problem seems interesting and promising.   \nStructurally, the paper wastes Section 5 by re-deriving [1] without giving [1] the proper credit and also compromising the explanation of the decoding, as the space is not enough for a full explanation of the decoding method. This space would have been much better spent on explaining the instantiation of [1] to the federated setting, e.g., what coding scheme is used, what test is used, how is $Q(t|s)$ chosen, etc., which is currently done all over the place. Further, it would have been good if the paper explained the properties of this instantiation and why it makes sense in the federated case. For example, the paper could have explained what the chosen coding strategies guarantee in the context of the FL problem being solved, what their communication complexity is, and how reasonable a test based on additional validation data and known attack parameters ( such as the targetted classes and number of malicious clients ) is. \n- **The defense relies on unreasonable assumptions and introduces too many hyperparameters:**   \nWhile the experiments provided in the paper suggest the proposed group testing strategies have the potential to constitute a good defense, the particular instantiation is unsatisfactory.   \nIn particular, in the main paper, the authors assume unrealistic knowledge by the defender, such as known $\\delta$ and known target class in the targetted version of the attack. While $\\delta$ is shown to be not that important in Figure 9, wrong $\\delta$ should be the default setting in which all experiments are carried out, as it is the only way to carry out the attack in the first place. Providing experiments with known $\\delta$ is fine for ablation purposes but nothing more. Moreover, the known target is also a big assumption, as if the test is carried out for multiple targets, this can introduce additional noise in the test and lower the presented results.  \nFurther, the added hyperparameters $\\Lambda$, $p$, and $\\rho$ are hard to tune in practice. In particular, choosing $\\Lambda$ heuristically depending on the setting, is hard and only possible through several re-trainings of the model. This is caused by the author's choice to tune $\\Lambda$, which is an uninterpretable parameter in $(-\\infty,\\infty)$ to which the results are very sensitive, instead of $\\Delta'$, which can be interpreted as a tradeoff between FA and MD. Further, I find the chosen model of $Q(t|s)$ which only depends on a single $p$ (the probability of error), too simplistic and hard to guess. I would suggest the authors instead at least choose $Q(t|s)$ as a probability distribution of the hamming distance between $t$ and $s$. This will at least account for the fact that individual tests can fail separately. Finally, $\\rho$ is likely to be dependent on the heterogeneity level and complexity of the problem and might also be hard to tune.\n- **Current experimental results are not strong enough:**   \nDespite the multiple hyperparameters introduced by the authors, the method still produces results that, in many cases, do not beat the baseline the authors themselves provide( [2] ). This is especially visible in Figures 4 and 5 in the Appendix, where the authors' method consistently performs worse in simple settings like MNIST. It seems that the proposed model, as it is, is mostly capable of beating [2] in heterogeneous settings where targetted attacks with known targets are applied. While this is an important setting that is worth solving separately, this is a clear drawback of the method that is not discussed by the authors. If this is where the method really works better than alternatives, the whole paper needs to focus on this setting and explain why this setting is favorable to their method ( or unfavorable for other methods ). Further, to allow realistic apples-to-apples comparisons, as mentioned above, authors should remove the known target and $\\delta$ assumptions (which I assume are not needed by [2]) and discuss how many clients are being securely aggregated by [2]. If the experiments for [2] aggregate all clients ( which I suspect is the case ), then the numbers are even worse for the proposed method, as [2] is in the \"most private\" setting where security is harder to achieve and yet still beats the proposed method regularly.\nFinally, another concern regarding the results is the fact the method seems to affect more clean accuracy compared to [2] and even the no defense ( according to Tables 1-3 ). This might suggest that good defense properties demonstrated in the paper are currently achieved on the back of too-high FA rates, tanking the clean performance. \n- **Runtime concerns:**\nThe authors say in the main paper repeatedly they focus on the cross-silo FL setting which they define to be up to 100 devices. Yet, in their experiments, the authors only use 15, and in the scaling experiment in Appendix J, they claim they do not even scale to 31 when some more precise encodings are used. This needs to be explicitly discussed in the main paper, but also severely jeopardizes the applicability of the proposed algorithm.\n- **Other:**\n1. The discussion on communication complexity needs to be at least partially moved to the main paper. The authors keep claiming their communication complexity as an important contribution, but unless one reads the appendix, they do not even know what the communication complexity of this and baseline models are.\n2. In all experiments, provide information on what the FA and MD rates are. Further, provide experiments where you change $\\Lambda$ and show how this affects FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy.\n3. Make explicit in **the main paper** that the current methodology works only for data poisoning and not model poisoning. ( Essentially bring the \"**Beyond data poisoning**\" point from Appendix L to the main paper )\n4. If the authors plan on keeping Sec. 5, which I do not recommend, they should at least provide some justification for the forward and backward formulas. \n- **Nits:**\n1. The last paragraph on page 3 does not explain when a test is positive ( i.e. is it when the group has a malicious client or when there isn't ). Add this to the definition\n2. The $s=d \\vee A^T$ notation, when first introduced, is confusing if the reader is unfamiliar with the group testing literature. Maybe explain that this is matrix multiplication where the dot product uses \"ands\" for multiplication and \"ors\" for additions.\n\nQuestions: - Can the authors provide an additional experiment where they change $\\Lambda$ and show how this affects the FA and MD rates and how it affects the attacker's accuracy and the model's clean accuracy? Which is more detrimental for the performance of FedGT - high FA or high MD ratios?\n- Can the authors explain why the CIFAR10 attack success goes down even without defense in Figure 4 ?\n- Can the authors provide a linear scale version of Figure 4? Log scale in the attacker's accuracy both makes it hard to read and also obfuscates the results, which are not naturally log distributed. \n- Can the authors provide experiments where the defender does not know the targets for the targeted attacks?\n- Can the authors explain how many clients are securely aggregated by [2] in their experiments?\n- Can the authors provide runtimes of the method for different numbers of clients and encodings? Can the authors provide a worst-case or average-case runtime analysis (in terms of big-O notation)?\n\nAll in all, this paper has the potential to introduce a very interesting defense mechanism with good performance and interesting properties. However, it does not do any of this. It does not properly instantiate the algorithms used in the federated setting, as it chooses super basic tests and sets everything else to impossible to obtain in practice hyperparameters. Possibly due to this, the results on the defense side are underwhelming, even when the comparison is done in a very favorable way to the proposed method by giving more information to the defender than typically assumed while also comparing to a method that cannot benefit from the trade-off described in this paper in terms of security-vs-privacy. Further, the method seems to be very computationally expensive to the server in cases where more than a handful of clients are available, and the clean accuracy is sometimes severely penalized possibly due to a high FA ratio.\n\n\n[1] Gianluigi Liva, Enrico Paolini, and Marco Chiani. Optimum detection of defective elements in non-adaptive group testing. In Annu. Conf. Information Sciences and Systems (CISS), Baltimore, MD, 2021.   \n[2] Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022.", "labeling_timestamp": "2026-01-11T16:41:12.224048", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly presents FedGT as a mechanism that trades off privacy and security by varying group size, and contrasts this with secure-aggregation-based methods. This supports the reviewer’s claim that the baseline in [2] (Pillutla et al.) cannot exploit the same privacy–security trade-off. However, the paper does account for this asymmetry: it explicitly frames the trade-off, positions RFA as a private (secure-aggregation) approach, and directly compares FedGT to the Pillutla et al. baseline in experiments and discussion. Thus the first subclaim is supported while the second (that the authors do not account for the asymmetry) is contradicted.", "evidence": "“The server learns the aggregated model of the clients in each group—vanilla federated learning and secure aggregation correspond to the extreme cases of FedGT with group size equal to one and the total amount of clients, respectively.” (Abstract)\n\n“secure aggregation ... enhance clients' privacy at the expense of camouflaging adversaries ... Hence, there is a fundamental trade-off between security and privacy.” (Introduction)\n\n“We further show that FedGT significantly outperforms the private robust aggregation approach based on the geometric median recently proposed by Pillutla et al. on heterogeneous client data (ISIC2019) and in the presence of targeted attacks (CIFAR-10 and ISIC2019).” (Abstract)\n\n“In (Pillutla et al., 2022), a robust aggregation protocol (dubbed RFA) based on an approximate geometric median (computed exploiting secure aggregation) is proposed. However, this protocol lacks the capability to identify malicious clients ...” (Related Work)", "section": "Abstract; Introduction; Related Work"}
{"claim": "The paper does not specify which model is used to generate prompts within the DeNEVIL framework.", "claim_type": "methodology", "paper_id": "m3RRWWFaVe", "paper_title": "DENEVIL: TOWARDS DECIPHERING AND NAVIGATING THE ETHICAL VALUES OF LARGE LANGUAGE MODELS VIA INSTRUCTION LEARNING", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "3cfg9Wjv21", "reviewer": "Reviewer_AiAD", "review_text": "Summary: This paper studies the ethical values in LLMs based on moral foundation theory. Instead of just trying to \"know\" whether there is ethical issues in LLMs, they want to understand how LLMs deal with value conformity. They propose a DeNEVIL framework that dynamically generates and refines the prompts so that these prompts can induce LLMs to produce completions violating specified ethical values. They found most LLMs are not good at obeying ethical values under DeNEVIL. To improve LLMs' value conformity, they propose VILMO which generates value instructions to intervene in LLMs to generate output that follows the ethical values.\n\nStrengths: The authors have summarized two challenges in discriminative evaluations and tried to propose a new framework to address them. They have proposed new methodologies along with detailed analysis of different LLMs to support their claim. The research question is important and the authors did a great job to introduce their solution step by step.\n\nWeaknesses: Some details might be missing from the main paper which could potentially cause some unsmoothness in reading.\n\nQuestions: - What is the model used in DeNEVIL? Additionally, for your results in Fig.2, ChatGPT has the lowest misalignment behavior, could it be because the moral prompt dataset is generated using it?\n- Related to the above question, for DeNEVIL, it seems we are generating the most \"aggressive\" prompt (to induce LLMs to generate harmful output as best as we can). I'm wondering if different LLMs should have different most \"aggressive\" prompts. \n- In Fig.3(c), since the goal of DeNEVIL is to probe the issues in LLMs, shouldn't we use LLaMA-70B model? And, what model is being evaluated for this figure?\n- For VILMO warning, have you considered a baseline as a templated prompt with certain values? For example, ``Please ensure that your completion does not violate \"[value]\".''", "labeling_timestamp": "2026-01-11T16:41:18.639665", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes DeNEVIL in generic terms using p_θ (the target LLM) and lists p_θ as an input to the algorithm, and only gives example models (e.g., ChatGPT, LLaMA) rather than specifying a single model used to generate prompts.", "evidence": "Input: ¬ v , β , τ 0 , T , K , M , p θ , p ω , the initial candidate sets X 0 = { x 0 } and Y 0 = { y 0 } Output: The optimized provocative prompt x ∗\n\nFor LLMs with strong instruction following abilities, e.g., ChatGPT, we directly provide ¬ v as an instruction in the prompt. For vanilla but open-source LLMs like LLaMA (Touvron et al., 2023), we transform the sampling in Eq.(1) to inference-time controllable decoding (Yang & Klein, 2021; Krause et al., 2021), regarding ¬ v as a classifier-based condition, with p ω introduced before, then: ...", "section": "3.1 DENEVIL FOR VALUES DECIPHERMENT"}
{"claim": "Figure 2's finding that ChatGPT has the lowest misalignment may be biased because the moral prompt dataset could have been generated with ChatGPT.", "claim_type": "methodology", "paper_id": "m3RRWWFaVe", "paper_title": "DENEVIL: TOWARDS DECIPHERING AND NAVIGATING THE ETHICAL VALUES OF LARGE LANGUAGE MODELS VIA INSTRUCTION LEARNING", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "3cfg9Wjv21", "reviewer": "Reviewer_AiAD", "review_text": "Summary: This paper studies the ethical values in LLMs based on moral foundation theory. Instead of just trying to \"know\" whether there is ethical issues in LLMs, they want to understand how LLMs deal with value conformity. They propose a DeNEVIL framework that dynamically generates and refines the prompts so that these prompts can induce LLMs to produce completions violating specified ethical values. They found most LLMs are not good at obeying ethical values under DeNEVIL. To improve LLMs' value conformity, they propose VILMO which generates value instructions to intervene in LLMs to generate output that follows the ethical values.\n\nStrengths: The authors have summarized two challenges in discriminative evaluations and tried to propose a new framework to address them. They have proposed new methodologies along with detailed analysis of different LLMs to support their claim. The research question is important and the authors did a great job to introduce their solution step by step.\n\nWeaknesses: Some details might be missing from the main paper which could potentially cause some unsmoothness in reading.\n\nQuestions: - What is the model used in DeNEVIL? Additionally, for your results in Fig.2, ChatGPT has the lowest misalignment behavior, could it be because the moral prompt dataset is generated using it?\n- Related to the above question, for DeNEVIL, it seems we are generating the most \"aggressive\" prompt (to induce LLMs to generate harmful output as best as we can). I'm wondering if different LLMs should have different most \"aggressive\" prompts. \n- In Fig.3(c), since the goal of DeNEVIL is to probe the issues in LLMs, shouldn't we use LLaMA-70B model? And, what model is being evaluated for this figure?\n- For VILMO warning, have you considered a baseline as a templated prompt with certain values? For example, ``Please ensure that your completion does not violate \"[value]\".''", "labeling_timestamp": "2026-01-11T16:41:11.667092", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper states MoralPrompt was created by instantiating DeNEVIL, which 'dynamically probes the value vulnerabilities in each model' and 'creates novel and tailored prompts co-evolving with LLMs' to avoid data leakage. Thus the dataset is presented as generated per-target-model rather than produced solely by ChatGPT, so the specific bias alleged is contradicted by the paper's described methodology.", "evidence": "1) \"DeNEVIL dynamically probes the value vulnerabilities in each model and then creates novel and tailored prompts co-evolving with LLMs, avoiding test data leakage (address Challenge 1).\" 2) \"Then we instantiate DeNEVIL with the Moral Foundations Theory ... to construct MoralPrompt, a dataset containing 2,397 prompts ... and benchmarked 27 LLMs across diverse architectures and scales.\" 3) \"DeNEVIL framework The key challenge ... we propose the DeNEVIL framework to explore each LLM's value vulnerabilities dynamically, and hence find the most provocative scenarios (prompts) x.\"", "section": "Introduction; 3.1 DENEVIL FOR VALUES DECIPHERMENT"}
{"claim": "DeNEVIL appears to generate a single 'most aggressive' prompt and may not consider that different LLMs require different aggressive prompts.", "claim_type": "methodology", "paper_id": "m3RRWWFaVe", "paper_title": "DENEVIL: TOWARDS DECIPHERING AND NAVIGATING THE ETHICAL VALUES OF LARGE LANGUAGE MODELS VIA INSTRUCTION LEARNING", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "3cfg9Wjv21", "reviewer": "Reviewer_AiAD", "review_text": "Summary: This paper studies the ethical values in LLMs based on moral foundation theory. Instead of just trying to \"know\" whether there is ethical issues in LLMs, they want to understand how LLMs deal with value conformity. They propose a DeNEVIL framework that dynamically generates and refines the prompts so that these prompts can induce LLMs to produce completions violating specified ethical values. They found most LLMs are not good at obeying ethical values under DeNEVIL. To improve LLMs' value conformity, they propose VILMO which generates value instructions to intervene in LLMs to generate output that follows the ethical values.\n\nStrengths: The authors have summarized two challenges in discriminative evaluations and tried to propose a new framework to address them. They have proposed new methodologies along with detailed analysis of different LLMs to support their claim. The research question is important and the authors did a great job to introduce their solution step by step.\n\nWeaknesses: Some details might be missing from the main paper which could potentially cause some unsmoothness in reading.\n\nQuestions: - What is the model used in DeNEVIL? Additionally, for your results in Fig.2, ChatGPT has the lowest misalignment behavior, could it be because the moral prompt dataset is generated using it?\n- Related to the above question, for DeNEVIL, it seems we are generating the most \"aggressive\" prompt (to induce LLMs to generate harmful output as best as we can). I'm wondering if different LLMs should have different most \"aggressive\" prompts. \n- In Fig.3(c), since the goal of DeNEVIL is to probe the issues in LLMs, shouldn't we use LLaMA-70B model? And, what model is being evaluated for this figure?\n- For VILMO warning, have you considered a baseline as a templated prompt with certain values? For example, ``Please ensure that your completion does not violate \"[value]\".''", "labeling_timestamp": "2026-01-11T16:41:15.437245", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper states DeNEVIL dynamically probes each LLM's value vulnerabilities and optimizes prompts per model (it formulates x* = argmax_x log p_theta(¬v | x)), so it does not generate a single global prompt without considering model-specific differences.", "evidence": "“we propose the DeNEVIL framework to explore each LLM's value vulnerabilities dynamically, and hence find the most provocative scenarios (prompts) x, where the LLM would potentially violate values.”; “we consider the inverse value statement, ¬v, … and resort to the Variational Expectation Maximization (EM) algorithm … to obtain the prompts that make the LLM violate v to the maximum extent via x* = argmax_x log p_theta(¬v | x).”", "section": "3.1 DENEVIL FOR VALUES DECIPHERMENT / DeNEVIL Framework"}
{"claim": "Figure 3(c) does not report which model is being evaluated, leaving the reader uncertain about the experimental setup.", "claim_type": "experimental", "paper_id": "m3RRWWFaVe", "paper_title": "DENEVIL: TOWARDS DECIPHERING AND NAVIGATING THE ETHICAL VALUES OF LARGE LANGUAGE MODELS VIA INSTRUCTION LEARNING", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "3cfg9Wjv21", "reviewer": "Reviewer_AiAD", "review_text": "Summary: This paper studies the ethical values in LLMs based on moral foundation theory. Instead of just trying to \"know\" whether there is ethical issues in LLMs, they want to understand how LLMs deal with value conformity. They propose a DeNEVIL framework that dynamically generates and refines the prompts so that these prompts can induce LLMs to produce completions violating specified ethical values. They found most LLMs are not good at obeying ethical values under DeNEVIL. To improve LLMs' value conformity, they propose VILMO which generates value instructions to intervene in LLMs to generate output that follows the ethical values.\n\nStrengths: The authors have summarized two challenges in discriminative evaluations and tried to propose a new framework to address them. They have proposed new methodologies along with detailed analysis of different LLMs to support their claim. The research question is important and the authors did a great job to introduce their solution step by step.\n\nWeaknesses: Some details might be missing from the main paper which could potentially cause some unsmoothness in reading.\n\nQuestions: - What is the model used in DeNEVIL? Additionally, for your results in Fig.2, ChatGPT has the lowest misalignment behavior, could it be because the moral prompt dataset is generated using it?\n- Related to the above question, for DeNEVIL, it seems we are generating the most \"aggressive\" prompt (to induce LLMs to generate harmful output as best as we can). I'm wondering if different LLMs should have different most \"aggressive\" prompts. \n- In Fig.3(c), since the goal of DeNEVIL is to probe the issues in LLMs, shouldn't we use LLaMA-70B model? And, what model is being evaluated for this figure?\n- For VILMO warning, have you considered a baseline as a templated prompt with certain values? For example, ``Please ensure that your completion does not violate \"[value]\".''", "labeling_timestamp": "2026-01-11T16:41:24.677110", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper content includes Figure 1 (with subfigures a–c) but does not contain or describe any Figure 3 or Figure 3(c), so I cannot verify the reviewer's claim from the available text.", "evidence": "\"Figure 1: (a) Examples of discriminative and generative evaluations. (b) Illustration of our generative evaluation framework, DeNEVIL. (c) Depiction of our in-context alignment method, VILMO.\"", "section": "Introduction"}
{"claim": "Figure 3(c) omits evaluation on LLaMA-70B, which seems relevant given DeNEVIL's goal of probing large LLM vulnerabilities.", "claim_type": "experimental", "paper_id": "m3RRWWFaVe", "paper_title": "DENEVIL: TOWARDS DECIPHERING AND NAVIGATING THE ETHICAL VALUES OF LARGE LANGUAGE MODELS VIA INSTRUCTION LEARNING", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "3cfg9Wjv21", "reviewer": "Reviewer_AiAD", "review_text": "Summary: This paper studies the ethical values in LLMs based on moral foundation theory. Instead of just trying to \"know\" whether there is ethical issues in LLMs, they want to understand how LLMs deal with value conformity. They propose a DeNEVIL framework that dynamically generates and refines the prompts so that these prompts can induce LLMs to produce completions violating specified ethical values. They found most LLMs are not good at obeying ethical values under DeNEVIL. To improve LLMs' value conformity, they propose VILMO which generates value instructions to intervene in LLMs to generate output that follows the ethical values.\n\nStrengths: The authors have summarized two challenges in discriminative evaluations and tried to propose a new framework to address them. They have proposed new methodologies along with detailed analysis of different LLMs to support their claim. The research question is important and the authors did a great job to introduce their solution step by step.\n\nWeaknesses: Some details might be missing from the main paper which could potentially cause some unsmoothness in reading.\n\nQuestions: - What is the model used in DeNEVIL? Additionally, for your results in Fig.2, ChatGPT has the lowest misalignment behavior, could it be because the moral prompt dataset is generated using it?\n- Related to the above question, for DeNEVIL, it seems we are generating the most \"aggressive\" prompt (to induce LLMs to generate harmful output as best as we can). I'm wondering if different LLMs should have different most \"aggressive\" prompts. \n- In Fig.3(c), since the goal of DeNEVIL is to probe the issues in LLMs, shouldn't we use LLaMA-70B model? And, what model is being evaluated for this figure?\n- For VILMO warning, have you considered a baseline as a templated prompt with certain values? For example, ``Please ensure that your completion does not violate \"[value]\".''", "labeling_timestamp": "2026-01-11T16:42:03.586825", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not include Figure 3 or its subfigure 3(c), so we cannot verify whether that figure omits evaluation on LLaMA-70B. The paper does state they benchmarked many models and mentions LLaMA generally, but it does not list model sizes (e.g., LLaMA-70B) in the provided text.", "evidence": "1) \"...and benchmarked 27 LLMs across diverse architectures and scales.\" 2) \"For vanilla but open-source LLMs like LLaMA (Touvron et al., 2023), we transform the sampling in Eq.(1) to inference-time controllable decoding...\"", "section": "Introduction; 3.1 DENEVIL FOR VALUES DECIPHERMENT"}
{"claim": "The VILMO evaluation lacks a baseline comparison to a simple templated instruction such as 'Please ensure that your completion does not violate \"[value]\"'.", "claim_type": "baseline", "paper_id": "m3RRWWFaVe", "paper_title": "DENEVIL: TOWARDS DECIPHERING AND NAVIGATING THE ETHICAL VALUES OF LARGE LANGUAGE MODELS VIA INSTRUCTION LEARNING", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "3cfg9Wjv21", "reviewer": "Reviewer_AiAD", "review_text": "Summary: This paper studies the ethical values in LLMs based on moral foundation theory. Instead of just trying to \"know\" whether there is ethical issues in LLMs, they want to understand how LLMs deal with value conformity. They propose a DeNEVIL framework that dynamically generates and refines the prompts so that these prompts can induce LLMs to produce completions violating specified ethical values. They found most LLMs are not good at obeying ethical values under DeNEVIL. To improve LLMs' value conformity, they propose VILMO which generates value instructions to intervene in LLMs to generate output that follows the ethical values.\n\nStrengths: The authors have summarized two challenges in discriminative evaluations and tried to propose a new framework to address them. They have proposed new methodologies along with detailed analysis of different LLMs to support their claim. The research question is important and the authors did a great job to introduce their solution step by step.\n\nWeaknesses: Some details might be missing from the main paper which could potentially cause some unsmoothness in reading.\n\nQuestions: - What is the model used in DeNEVIL? Additionally, for your results in Fig.2, ChatGPT has the lowest misalignment behavior, could it be because the moral prompt dataset is generated using it?\n- Related to the above question, for DeNEVIL, it seems we are generating the most \"aggressive\" prompt (to induce LLMs to generate harmful output as best as we can). I'm wondering if different LLMs should have different most \"aggressive\" prompts. \n- In Fig.3(c), since the goal of DeNEVIL is to probe the issues in LLMs, shouldn't we use LLaMA-70B model? And, what model is being evaluated for this figure?\n- For VILMO warning, have you considered a baseline as a templated prompt with certain values? For example, ``Please ensure that your completion does not violate \"[value]\".''", "labeling_timestamp": "2026-01-11T16:41:50.476237", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper describes VILMO as an in-context alignment method and claims it outperforms existing competitors, but the provided content does not show experimental details or whether a simple templated-instruction baseline (e.g., 'Please ensure that your completion does not violate \"[value]\"') was included in evaluations. Therefore there is insufficient information to confirm the reviewer's claim.", "evidence": "Abstract: \"In response, we develop VILMO, an in-context alignment method that enhances the value compliance of LLM outputs by learning to generate appropriate value instructions, outperforming existing competitors.\";\nIntroduction/Sec. 3 overview: \"...and then present our in-context alignment method VILMO in Sec. 3.3.\"", "section": "Abstract; Introduction; Sec. 3 (overview)"}
{"claim": "Several methodological or experimental details are missing from the main paper, causing potential unsmoothness and difficulty for reproducibility and interpretation.", "claim_type": "methodology", "paper_id": "m3RRWWFaVe", "paper_title": "DENEVIL: TOWARDS DECIPHERING AND NAVIGATING THE ETHICAL VALUES OF LARGE LANGUAGE MODELS VIA INSTRUCTION LEARNING", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "3cfg9Wjv21", "reviewer": "Reviewer_AiAD", "review_text": "Summary: This paper studies the ethical values in LLMs based on moral foundation theory. Instead of just trying to \"know\" whether there is ethical issues in LLMs, they want to understand how LLMs deal with value conformity. They propose a DeNEVIL framework that dynamically generates and refines the prompts so that these prompts can induce LLMs to produce completions violating specified ethical values. They found most LLMs are not good at obeying ethical values under DeNEVIL. To improve LLMs' value conformity, they propose VILMO which generates value instructions to intervene in LLMs to generate output that follows the ethical values.\n\nStrengths: The authors have summarized two challenges in discriminative evaluations and tried to propose a new framework to address them. They have proposed new methodologies along with detailed analysis of different LLMs to support their claim. The research question is important and the authors did a great job to introduce their solution step by step.\n\nWeaknesses: Some details might be missing from the main paper which could potentially cause some unsmoothness in reading.\n\nQuestions: - What is the model used in DeNEVIL? Additionally, for your results in Fig.2, ChatGPT has the lowest misalignment behavior, could it be because the moral prompt dataset is generated using it?\n- Related to the above question, for DeNEVIL, it seems we are generating the most \"aggressive\" prompt (to induce LLMs to generate harmful output as best as we can). I'm wondering if different LLMs should have different most \"aggressive\" prompts. \n- In Fig.3(c), since the goal of DeNEVIL is to probe the issues in LLMs, shouldn't we use LLaMA-70B model? And, what model is being evaluated for this figure?\n- For VILMO warning, have you considered a baseline as a templated prompt with certain values? For example, ``Please ensure that your completion does not violate \"[value]\".''", "labeling_timestamp": "2026-01-11T16:41:44.490417", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper defers several detailed definitions and metrics to the appendix (e.g., Appendix B.2.1) and some algorithmic steps and formulas are not fully presented in the main text, which supports the reviewer's point that not all methodological/experimental details appear in the main paper. At the same time, the main paper includes algorithm pseudocode, equations, dataset statistics, and a code link, which mitigates complete lack of reproducibility information.", "evidence": "Refer to Appendix. B.2.1 for more detailed metrics we designed to reflect the frequency and degree of value conformity for LLMs.\n\nIn practice, we sample more completions but keep the top K\n\nSource codes: https://valuecompass.github.io .", "section": "3.1 DENEVIL FOR VALUES DECIPHERMENT; Abstract"}
{"claim": "The paper lacks comparison or explanation of computational cost and parameter count, raising concerns about the method's practicality and deployability.", "claim_type": "quantitative", "paper_id": "paobkszgIA", "paper_title": "End-to-End Video Semantic Segmentation in Adverse Weather using Fusion Blocks and Temporal-Spatial Teacher-Student Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BHq8fT4eAr", "reviewer": "Reviewer_L8p7", "review_text": "Summary: In this paper, a end-to-end domain-adaptive video semantic segmentation method without optical flow estimation is proposed to address the problem of video frame quality degradation under adverse weather conditions. The proposed method uses the temporal information of adjacent frames through fusion blocks and spatiotemporal teacher models to enhance the model's robustness to video semantic segmentation under adverse weather conditions. The fusion block combines information by matching and fusing relevant information pixels from adjacent frames. The spatiotemporal teacher model includes a temporal teacher and a spatial teacher to guide the student model from the temporal dimension and the spatial dimension, respectively.\n\nStrengths: 1. For the first time, an end-to-end video semantic segmentation method without optical flow estimation is proposed, which is suitable for adverse weather conditions.\n2. The model's adaptability to real scenarios is enhanced by simulating dynamic weather degradation in consecutive frames.\n3. The article achieves significant performance improvements on multiple datasets, surpassing existing state-of-the-art methods.\n\nWeaknesses: 1. The paper does not have any comparison or explanation on the amount of calculation and the number of parameters, which makes me worry about the practicality of the method.\n2. There are few baselines selected for visual comparison, which makes it difficult to reflect the effectiveness of the proposed method.\n3. The paper may lack in-depth discussion and justification of the theoretical basis of the proposed method.\n4. Is there some newer sota method that can be compared? The method in the table doesn't seem to be up to date.\n5. The work in this article is carried out under severe weather degradation conditions. Therefore, I think that the article should add a section on image restoration in the related works for discussion.\n\nQuestions: Please refer to the weaknesses.", "labeling_timestamp": "2026-01-11T16:41:38.573059", "model": "gpt-5-mini", "label": "Supported", "justification": "Across the Abstract, Introduction, contributions, and Proposed Method sections provided, the paper reports accuracy (mIoU) improvements and describes the fusion block and teacher-student training, but contains no discussion, comparison, or quantitative measurements of computational cost, runtime, FLOPs, or parameter count.", "evidence": "Our method achieves a performance of 25.4% and 33.0% mIoU on the adaptation from VIPER [28] and Synthia [29] to MVSS [18], respectively, representing an improvement of 4.3% and 5.8% mIoU over the existing state-of-the-art method.", "section": "Abstract"}
{"claim": "The paper uses too few baselines for visual comparison, which hinders evaluation of the proposed method's effectiveness and relative performance.", "claim_type": "baseline", "paper_id": "paobkszgIA", "paper_title": "End-to-End Video Semantic Segmentation in Adverse Weather using Fusion Blocks and Temporal-Spatial Teacher-Student Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BHq8fT4eAr", "reviewer": "Reviewer_L8p7", "review_text": "Summary: In this paper, a end-to-end domain-adaptive video semantic segmentation method without optical flow estimation is proposed to address the problem of video frame quality degradation under adverse weather conditions. The proposed method uses the temporal information of adjacent frames through fusion blocks and spatiotemporal teacher models to enhance the model's robustness to video semantic segmentation under adverse weather conditions. The fusion block combines information by matching and fusing relevant information pixels from adjacent frames. The spatiotemporal teacher model includes a temporal teacher and a spatial teacher to guide the student model from the temporal dimension and the spatial dimension, respectively.\n\nStrengths: 1. For the first time, an end-to-end video semantic segmentation method without optical flow estimation is proposed, which is suitable for adverse weather conditions.\n2. The model's adaptability to real scenarios is enhanced by simulating dynamic weather degradation in consecutive frames.\n3. The article achieves significant performance improvements on multiple datasets, surpassing existing state-of-the-art methods.\n\nWeaknesses: 1. The paper does not have any comparison or explanation on the amount of calculation and the number of parameters, which makes me worry about the practicality of the method.\n2. There are few baselines selected for visual comparison, which makes it difficult to reflect the effectiveness of the proposed method.\n3. The paper may lack in-depth discussion and justification of the theoretical basis of the proposed method.\n4. Is there some newer sota method that can be compared? The method in the table doesn't seem to be up to date.\n5. The work in this article is carried out under severe weather degradation conditions. Therefore, I think that the article should add a section on image restoration in the related works for discussion.\n\nQuestions: Please refer to the weaknesses.", "labeling_timestamp": "2026-01-11T16:41:55.046482", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt only explicitly mentions a visual comparison with TPS (Figure 1) but does not include the experiments/qualitative results section or a comprehensive list of visual baselines. Therefore there is insufficient information to confirm the reviewer’s claim that the paper uses too few baselines for visual comparison.", "evidence": "“Fig. 1 compares our method with TPS [36], illustrating our method's enhanced robustness in adverse weather conditions, achieved independently of pretrained optical flow.”", "section": "Introduction / Figure 1"}
{"claim": "The paper lacks in-depth discussion and theoretical justification for the proposed method, providing insufficient explanation of its underlying principles.", "claim_type": "methodology", "paper_id": "paobkszgIA", "paper_title": "End-to-End Video Semantic Segmentation in Adverse Weather using Fusion Blocks and Temporal-Spatial Teacher-Student Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BHq8fT4eAr", "reviewer": "Reviewer_L8p7", "review_text": "Summary: In this paper, a end-to-end domain-adaptive video semantic segmentation method without optical flow estimation is proposed to address the problem of video frame quality degradation under adverse weather conditions. The proposed method uses the temporal information of adjacent frames through fusion blocks and spatiotemporal teacher models to enhance the model's robustness to video semantic segmentation under adverse weather conditions. The fusion block combines information by matching and fusing relevant information pixels from adjacent frames. The spatiotemporal teacher model includes a temporal teacher and a spatial teacher to guide the student model from the temporal dimension and the spatial dimension, respectively.\n\nStrengths: 1. For the first time, an end-to-end video semantic segmentation method without optical flow estimation is proposed, which is suitable for adverse weather conditions.\n2. The model's adaptability to real scenarios is enhanced by simulating dynamic weather degradation in consecutive frames.\n3. The article achieves significant performance improvements on multiple datasets, surpassing existing state-of-the-art methods.\n\nWeaknesses: 1. The paper does not have any comparison or explanation on the amount of calculation and the number of parameters, which makes me worry about the practicality of the method.\n2. There are few baselines selected for visual comparison, which makes it difficult to reflect the effectiveness of the proposed method.\n3. The paper may lack in-depth discussion and justification of the theoretical basis of the proposed method.\n4. Is there some newer sota method that can be compared? The method in the table doesn't seem to be up to date.\n5. The work in this article is carried out under severe weather degradation conditions. Therefore, I think that the article should add a section on image restoration in the related works for discussion.\n\nQuestions: Please refer to the weaknesses.", "labeling_timestamp": "2026-01-11T16:42:07.726139", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes the fusion block, temporal-spatial teacher-student scheme, and temporal augmentation at a conceptual and architectural level, but contains no detailed theoretical analysis, formal derivations, or in-depth justification of why the components should work (e.g., no theoretical guarantees, proofs, or analytical discussion). The provided sections focus on design intuition and implementation rather than theoretical principles.", "evidence": "Abstract: \"The key idea of our fusion block is to offer the model a way to merge information from consecutive frames by matching and merging relevant pixels from those frames.\"  1 Introduction: \"We have developed a temporal-spatial teacher-student learning approach to effectively train the fusion block and enhance the quality of pseudo-labels.\"  3 Proposed method: \"Subsequently, the network processes the two inputs individually, producing separate sets of feature maps for each frame. These feature maps are then fused by the fusion block, resulting in the final prediction for the current frame.\"", "section": "Abstract; 1 Introduction; 3 Proposed method"}
{"claim": "The paper fails to compare against recent state-of-the-art methods, and the reported baselines in the results table appear outdated.", "claim_type": "baseline", "paper_id": "paobkszgIA", "paper_title": "End-to-End Video Semantic Segmentation in Adverse Weather using Fusion Blocks and Temporal-Spatial Teacher-Student Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BHq8fT4eAr", "reviewer": "Reviewer_L8p7", "review_text": "Summary: In this paper, a end-to-end domain-adaptive video semantic segmentation method without optical flow estimation is proposed to address the problem of video frame quality degradation under adverse weather conditions. The proposed method uses the temporal information of adjacent frames through fusion blocks and spatiotemporal teacher models to enhance the model's robustness to video semantic segmentation under adverse weather conditions. The fusion block combines information by matching and fusing relevant information pixels from adjacent frames. The spatiotemporal teacher model includes a temporal teacher and a spatial teacher to guide the student model from the temporal dimension and the spatial dimension, respectively.\n\nStrengths: 1. For the first time, an end-to-end video semantic segmentation method without optical flow estimation is proposed, which is suitable for adverse weather conditions.\n2. The model's adaptability to real scenarios is enhanced by simulating dynamic weather degradation in consecutive frames.\n3. The article achieves significant performance improvements on multiple datasets, surpassing existing state-of-the-art methods.\n\nWeaknesses: 1. The paper does not have any comparison or explanation on the amount of calculation and the number of parameters, which makes me worry about the practicality of the method.\n2. There are few baselines selected for visual comparison, which makes it difficult to reflect the effectiveness of the proposed method.\n3. The paper may lack in-depth discussion and justification of the theoretical basis of the proposed method.\n4. Is there some newer sota method that can be compared? The method in the table doesn't seem to be up to date.\n5. The work in this article is carried out under severe weather degradation conditions. Therefore, I think that the article should add a section on image restoration in the related works for discussion.\n\nQuestions: Please refer to the weaknesses.", "labeling_timestamp": "2026-01-11T16:42:19.411512", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states comparisons to recent video UDA methods (e.g., TPS, DA-VSN, VAT-VST, SFC) and claims quantitative improvements over the existing state-of-the-art. Thus the reviewer's claim that the paper fails to compare against recent SOTA and uses outdated baselines is contradicted by the paper content.", "evidence": "1) \"Our method achieves a performance of 25.4% and 33.0% mIoU on the adaptation from VIPER [28] and Synthia [29] to MVSS [18], respectively, representing an improvement of 4.3% and 5.8% mIoU over the existing state-of-the-art method.\" 2) \"Among these methods, DA-VSN [10] ... V AT-VST [30] ... TPS [36] ... SFC [9] ... It's important to highlight that all these methods depend on pretrained optical flow estimations\"", "section": "Abstract; Related work"}
{"claim": "The manuscript omits discussion of image restoration literature, which is relevant given experiments conducted under severe weather degradation scenarios.", "claim_type": "subjective", "paper_id": "paobkszgIA", "paper_title": "End-to-End Video Semantic Segmentation in Adverse Weather using Fusion Blocks and Temporal-Spatial Teacher-Student Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BHq8fT4eAr", "reviewer": "Reviewer_L8p7", "review_text": "Summary: In this paper, a end-to-end domain-adaptive video semantic segmentation method without optical flow estimation is proposed to address the problem of video frame quality degradation under adverse weather conditions. The proposed method uses the temporal information of adjacent frames through fusion blocks and spatiotemporal teacher models to enhance the model's robustness to video semantic segmentation under adverse weather conditions. The fusion block combines information by matching and fusing relevant information pixels from adjacent frames. The spatiotemporal teacher model includes a temporal teacher and a spatial teacher to guide the student model from the temporal dimension and the spatial dimension, respectively.\n\nStrengths: 1. For the first time, an end-to-end video semantic segmentation method without optical flow estimation is proposed, which is suitable for adverse weather conditions.\n2. The model's adaptability to real scenarios is enhanced by simulating dynamic weather degradation in consecutive frames.\n3. The article achieves significant performance improvements on multiple datasets, surpassing existing state-of-the-art methods.\n\nWeaknesses: 1. The paper does not have any comparison or explanation on the amount of calculation and the number of parameters, which makes me worry about the practicality of the method.\n2. There are few baselines selected for visual comparison, which makes it difficult to reflect the effectiveness of the proposed method.\n3. The paper may lack in-depth discussion and justification of the theoretical basis of the proposed method.\n4. Is there some newer sota method that can be compared? The method in the table doesn't seem to be up to date.\n5. The work in this article is carried out under severe weather degradation conditions. Therefore, I think that the article should add a section on image restoration in the related works for discussion.\n\nQuestions: Please refer to the weaknesses.", "labeling_timestamp": "2026-01-11T16:42:18.325286", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper discusses adverse weather degradations and UDA/video segmentation literature but does not cite or discuss image restoration/enhancement work (e.g., dehazing, deraining, low-light enhancement) that would be relevant to severe weather scenarios. The Related Work and Adverse Weather paragraphs describe low-level degradations and UDA approaches but contain no discussion of image restoration methods.", "evidence": "Adverse weather degradation Current video-based UDA methods are mainly developed for adapting models from synthetic to real-world datasets under ideal conditions. However, they fall short in adverse weather conditions. This limitation is largely due to two key factors in the domain gap between synthetic and real-world scenes under such conditions: style-related differences, and significant low-level degradations [25, 21, 19, 3, 5].", "section": "Related work (Adverse weather degradation)"}
{"claim": "The paper does not provide convincing evidence that the proposed model can scale up model capacity as claimed.", "claim_type": "experimental", "paper_id": "wYvuY60SdD", "paper_title": "Mixture of Weak and Strong Experts on Graphs", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "48z2642NVy", "reviewer": "Reviewer_jetB", "review_text": "Comment: Dear Authors,\n\nThanks for your detailed response, which have addressed many of my concerns. The revision of your paper looks better than the original version. However, based on your response, I am still not convinced about the ability to scale up the model capacity of the proposed model, which is the main goal of this work. I agree that the accuracies for the 2-expert Mowst model and its variant are slightly improved compared to previous methods, but we cannot guarantee that these accuracies will be significantly improved when the number of experts increases. Therefore, I suggest that the authors should study the multi-expert Mowst more extensively, and conduct more experiments on that model to strengthen the paper. For those reasons, I decide to keep my score unchanged. \n\nThank you,\n\nReviewer jetB", "labeling_timestamp": "2026-01-11T16:42:29.581588", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper asserts increased expressive power and reports empirical accuracy gains (Abstract, Introduction), and references sections analyzing expressive power and many-expert generalization (§2.6, §2.7). However, the provided excerpt does not contain explicit experiments or analyses that directly demonstrate scaling up model capacity (e.g., results showing capacity growth with increased experts or parameters). Because the relevant sections (§2.6, §2.7 and experimental details) are referenced but not present here, there is insufficient information to confirm the reviewer's claim.", "evidence": "1) \"Mowst is easy to optimize and achieves strong expressive power, with a computation cost comparable to a single GNN.\" 2) \"In experiments, we extensively evaluate Mowst on 4 types of GNN experts and 6 standard benchmarks ... We show consistent accuracy improvements over state-of-the-art baselines.\" 3) \"§2.7 shows the many-expert generalization.\"", "section": "Abstract / Introduction (references to §2.6 and §2.7)"}
{"claim": "The reported accuracies for the 2-expert Mowst and its variant are only slight improvements compared to previous methods.", "claim_type": "baseline", "paper_id": "wYvuY60SdD", "paper_title": "Mixture of Weak and Strong Experts on Graphs", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "48z2642NVy", "reviewer": "Reviewer_jetB", "review_text": "Comment: Dear Authors,\n\nThanks for your detailed response, which have addressed many of my concerns. The revision of your paper looks better than the original version. However, based on your response, I am still not convinced about the ability to scale up the model capacity of the proposed model, which is the main goal of this work. I agree that the accuracies for the 2-expert Mowst model and its variant are slightly improved compared to previous methods, but we cannot guarantee that these accuracies will be significantly improved when the number of experts increases. Therefore, I suggest that the authors should study the multi-expert Mowst more extensively, and conduct more experiments on that model to strengthen the paper. For those reasons, I decide to keep my score unchanged. \n\nThank you,\n\nReviewer jetB", "labeling_timestamp": "2026-01-11T16:42:31.825235", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states that Mowst achieves \"significant accuracy improvement\" on benchmarks and \"consistent accuracy improvements over state-of-the-art baselines,\" which contradicts the reviewer's claim that reported accuracies are only slight improvements.", "evidence": "Abstract: \"Mowst on 4 backbone GNN architectures show significant accuracy improvement on 6 standard node classification benchmarks...\"; Contributions/Introduction: \"We show consistent accuracy improvements over state-of-the-art baselines.\"", "section": "Abstract; Contributions (Introduction)"}
{"claim": "The paper provides no evidence that increasing the number of experts will lead to significant accuracy improvements.", "claim_type": "experimental", "paper_id": "wYvuY60SdD", "paper_title": "Mixture of Weak and Strong Experts on Graphs", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "48z2642NVy", "reviewer": "Reviewer_jetB", "review_text": "Comment: Dear Authors,\n\nThanks for your detailed response, which have addressed many of my concerns. The revision of your paper looks better than the original version. However, based on your response, I am still not convinced about the ability to scale up the model capacity of the proposed model, which is the main goal of this work. I agree that the accuracies for the 2-expert Mowst model and its variant are slightly improved compared to previous methods, but we cannot guarantee that these accuracies will be significantly improved when the number of experts increases. Therefore, I suggest that the authors should study the multi-expert Mowst more extensively, and conduct more experiments on that model to strengthen the paper. For those reasons, I decide to keep my score unchanged. \n\nThank you,\n\nReviewer jetB", "labeling_timestamp": "2026-01-11T16:42:36.587916", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly frames and evaluates the 2-expert Mowst setup and reports accuracy improvements for that setting. While it mentions a \"many-expert generalization\" (§2.7), the provided content contains no empirical results or evidence showing that increasing the number of experts yields additional accuracy improvements.", "evidence": "\"Our discussion mainly focuses on the 2-expert Mowst while §2.7 shows the many-expert generalization.\" (Section 2) \n\"In experiments, Mowst on 4 backbone GNN architectures show significant accuracy improvement on 6 standard node classification benchmarks, including both homophilous and heterophilous graphs\" (Abstract)", "section": "Section 2 (Mowst); Abstract"}
{"claim": "The evaluation lacks extensive experiments on multi-expert Mowst to support the central scalability claim.", "claim_type": "experimental", "paper_id": "wYvuY60SdD", "paper_title": "Mixture of Weak and Strong Experts on Graphs", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "48z2642NVy", "reviewer": "Reviewer_jetB", "review_text": "Comment: Dear Authors,\n\nThanks for your detailed response, which have addressed many of my concerns. The revision of your paper looks better than the original version. However, based on your response, I am still not convinced about the ability to scale up the model capacity of the proposed model, which is the main goal of this work. I agree that the accuracies for the 2-expert Mowst model and its variant are slightly improved compared to previous methods, but we cannot guarantee that these accuracies will be significantly improved when the number of experts increases. Therefore, I suggest that the authors should study the multi-expert Mowst more extensively, and conduct more experiments on that model to strengthen the paper. For those reasons, I decide to keep my score unchanged. \n\nThank you,\n\nReviewer jetB", "labeling_timestamp": "2026-01-11T16:42:49.878240", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly states it focuses on the 2-expert Mowst and reports experiments across 4 GNN backbones and 6 benchmarks, but the provided content does not show extensive empirical evaluation of multi-expert (many-expert) settings—only a mention that §2.7 discusses many-expert generalization. Thus the claim that the evaluation lacks extensive multi-expert experiments is supported by the paper text.", "evidence": "1) \"Our discussion mainly focuses on the 2-expert Mowst while §2.7 shows the many-expert generalization.\" 2) \"In experiments, we extensively evaluate Mowst on 4 types of GNN experts and 6 standard benchmarks covering both homophilous and heterophilous graphs.\"", "section": "Section 2 (Mowst) and Abstract"}
{"claim": "Assumption A1 appears to contradict common understanding that more training samples generally improve generalization.", "claim_type": "methodology", "paper_id": "BxHgpC6FNv", "paper_title": "Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "2KLOTeJSAm", "reviewer": "Reviewer_mTpn", "review_text": "Summary: This paper delves into the exploration of benign overfitting and \"grokking\" in two-layer ReLU neural networks when trained on XOR cluster data with noisy labels. The authors demonstrate that networks can perfectly fit noisy training data (benign overfitting) and, after a period, transition from harmful overfitting to a stage where they generalize near-optimally (\"grokking\"). Through rigorous theoretical analysis and proofs, the study reveals that these surprising phenomena are evident in the networks' training trajectories, providing a nuanced understanding of overfitting and generalization in neural network models.\n\nThe paper's contributions lie in providing the first theoretical insights into benign overfitting in non-linearly separable data distributions, unraveling the feature learning dynamics under gradient descent. These findings illuminate the pathways through which neural networks navigate the complexities of noisy data, offering a fresh perspective on their capacity to generalize despite apparent overfitting.\n\nStrengths: * The paper offers a new theoretical examination of benign overfitting and \"grokking\" in two-layer ReLU neural networks. It focuses on XOR cluster data with noisy labels, giving a detailed exploration and proofs related to these phenomena. The authors use existing concepts and new theories to better explain the behavior of neural networks with noisy training data.\n\n* The paper is structured and clear, effectively communicating the authors’ work and results. It has a logical organization that makes it easy for readers to follow the ideas and analyses. The presentation of definitions, explanations, and proofs is mostly straightforward, helping readers understand the complex concepts and findings.\n\n* The paper is important because it helps understand overfitting and generalization in neural networks better. It explains the concepts of benign overfitting and \"grokking\" in one framework.\n\nWeaknesses: * The assumption made in A1 seems to contradict common understanding. Generally, increasing the number of samples, even with limited noisy labels, tends to enhance the generalization capability of neural networks. However, in Assumption A1, having a larger number of training samples seems to adversely affect the model, as indicated by its presence on the right-hand side of the inequality. This aspect might require further clarification or justification within the context of the study.\n\n* The mechanism of overfitting, once the neural network learns the directions of $u_1$ and $u_2$. is not explicitly clear. The paper mentions that post the initial gradient step, positive neurons learn $u_1$ while negative neurons learn $u_2$. However, the explanation seems lacking in how the network overfits to samples with noisy (flipped) labels in this condition. A more detailed discussion or clarification on this would be beneficial.\n\n* The role of Lemma 4.6 in the paper is unclear. A clearer explanation of how it relates to other parts of the paper and how it contributes to the overall arguments and conclusions is needed for better understanding. \n\n* The paper seems to lack a comparative discussion with some relevant works, specifically references [1,2].\n\n\n[1] Meng, Xuran, Difan Zou, and Yuan Cao. \"Benign Overfitting in Two-Layer ReLU Convolutional Neural Networks for XOR Data.\" arXiv preprint arXiv:2310.01975 (2023).\n\n[2] Glasgow, Margalit. \"SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem.\" arXiv preprint arXiv:2309.15111 (2023).\n\nQuestions: * Could you provide more insights or justification regarding Assumption A1? Specifically, could you clarify why an increase in the number of training samples seems to negatively influence the model, contrary to the common understanding that more samples generally improve a model's generalization capability?\n\n* Could you elaborate on the mechanism of overfitting after the neural network learns the directions of $u_1$ and $u_2$.\n\n* Could you clarify the role and significance of Lemma 4.6 in the context of the paper's objectives and findings?\n\n* Could you make a comparsion your results and techniques with [1] and [2]?\n\n* Have you considered variations in the network architecture, such as not fixing the second layer, and if so, how do these variations influence the results?\n\n* In the model settings used in the paper, there doesn’t appear to be an upper bound on the network width. If an extremely wide network setting were used, would the findings align with the \"lazy training\" regime? Could you discuss how the results might be influenced by varying the width of the network to such extremes?\n\nI would increase my score if the authors could clarify my concerns demonstrated in the above questions.\n\n--------------------------------------\nI increase my score to 6 after rebuttal.", "labeling_timestamp": "2026-01-11T16:42:57.743992", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper indeed contains Assumption A1 which requires the norm of the cluster mean to scale with n (the displayed inequality shows ∥µ∥ must grow with n), so the reviewer’s observation that this assumption appears counter to the usual intuition that more samples reduce the required signal strength is a reasonable interpretation. However, the paper does not explicitly state that this contradicts common understanding nor discuss this point, so the reviewer’s claim is an interpretation rather than a documented contradiction in the paper.", "evidence": "“Given a large enough universal constant C, we make the following assumptions: ... (A1) The norm of the mean satisfies ∥ µ ∥ 2 ≥ Cn 0 . 51 √ p .”", "section": "3 MAIN RESULTS"}
{"claim": "Assumption A1 places the number of training samples on the right-hand side of an inequality, implying more samples adversely affect performance, which lacks justification.", "claim_type": "methodology", "paper_id": "BxHgpC6FNv", "paper_title": "Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "2KLOTeJSAm", "reviewer": "Reviewer_mTpn", "review_text": "Summary: This paper delves into the exploration of benign overfitting and \"grokking\" in two-layer ReLU neural networks when trained on XOR cluster data with noisy labels. The authors demonstrate that networks can perfectly fit noisy training data (benign overfitting) and, after a period, transition from harmful overfitting to a stage where they generalize near-optimally (\"grokking\"). Through rigorous theoretical analysis and proofs, the study reveals that these surprising phenomena are evident in the networks' training trajectories, providing a nuanced understanding of overfitting and generalization in neural network models.\n\nThe paper's contributions lie in providing the first theoretical insights into benign overfitting in non-linearly separable data distributions, unraveling the feature learning dynamics under gradient descent. These findings illuminate the pathways through which neural networks navigate the complexities of noisy data, offering a fresh perspective on their capacity to generalize despite apparent overfitting.\n\nStrengths: * The paper offers a new theoretical examination of benign overfitting and \"grokking\" in two-layer ReLU neural networks. It focuses on XOR cluster data with noisy labels, giving a detailed exploration and proofs related to these phenomena. The authors use existing concepts and new theories to better explain the behavior of neural networks with noisy training data.\n\n* The paper is structured and clear, effectively communicating the authors’ work and results. It has a logical organization that makes it easy for readers to follow the ideas and analyses. The presentation of definitions, explanations, and proofs is mostly straightforward, helping readers understand the complex concepts and findings.\n\n* The paper is important because it helps understand overfitting and generalization in neural networks better. It explains the concepts of benign overfitting and \"grokking\" in one framework.\n\nWeaknesses: * The assumption made in A1 seems to contradict common understanding. Generally, increasing the number of samples, even with limited noisy labels, tends to enhance the generalization capability of neural networks. However, in Assumption A1, having a larger number of training samples seems to adversely affect the model, as indicated by its presence on the right-hand side of the inequality. This aspect might require further clarification or justification within the context of the study.\n\n* The mechanism of overfitting, once the neural network learns the directions of $u_1$ and $u_2$. is not explicitly clear. The paper mentions that post the initial gradient step, positive neurons learn $u_1$ while negative neurons learn $u_2$. However, the explanation seems lacking in how the network overfits to samples with noisy (flipped) labels in this condition. A more detailed discussion or clarification on this would be beneficial.\n\n* The role of Lemma 4.6 in the paper is unclear. A clearer explanation of how it relates to other parts of the paper and how it contributes to the overall arguments and conclusions is needed for better understanding. \n\n* The paper seems to lack a comparative discussion with some relevant works, specifically references [1,2].\n\n\n[1] Meng, Xuran, Difan Zou, and Yuan Cao. \"Benign Overfitting in Two-Layer ReLU Convolutional Neural Networks for XOR Data.\" arXiv preprint arXiv:2310.01975 (2023).\n\n[2] Glasgow, Margalit. \"SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem.\" arXiv preprint arXiv:2309.15111 (2023).\n\nQuestions: * Could you provide more insights or justification regarding Assumption A1? Specifically, could you clarify why an increase in the number of training samples seems to negatively influence the model, contrary to the common understanding that more samples generally improve a model's generalization capability?\n\n* Could you elaborate on the mechanism of overfitting after the neural network learns the directions of $u_1$ and $u_2$.\n\n* Could you clarify the role and significance of Lemma 4.6 in the context of the paper's objectives and findings?\n\n* Could you make a comparsion your results and techniques with [1] and [2]?\n\n* Have you considered variations in the network architecture, such as not fixing the second layer, and if so, how do these variations influence the results?\n\n* In the model settings used in the paper, there doesn’t appear to be an upper bound on the network width. If an extremely wide network setting were used, would the findings align with the \"lazy training\" regime? Could you discuss how the results might be influenced by varying the width of the network to such extremes?\n\nI would increase my score if the authors could clarify my concerns demonstrated in the above questions.\n\n--------------------------------------\nI increase my score to 6 after rebuttal.", "labeling_timestamp": "2026-01-11T16:43:17.549975", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper's Assumption (A1) explicitly includes the sample size n on the right-hand side of the inequality (a lower bound on the mean norm that grows with n). The provided text does not offer a rationale for why the required signal strength should increase with n, so the reviewer's observation that this placement suggests more samples make the assumption harder to satisfy (and that justification is lacking) is accurate according to the paper content shown.", "evidence": "Given a large enough universal constant C, we make the following assumptions: - (A1) The norm of the mean satisfies ∥ µ ∥ 2 ≥ Cn 0 . 51 √ p .", "section": "3 MAIN RESULTS"}
{"claim": "The mechanism by which the network overfits after learning the directions u1 and u2 is not clearly explained.", "claim_type": "presentation", "paper_id": "BxHgpC6FNv", "paper_title": "Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "2KLOTeJSAm", "reviewer": "Reviewer_mTpn", "review_text": "Summary: This paper delves into the exploration of benign overfitting and \"grokking\" in two-layer ReLU neural networks when trained on XOR cluster data with noisy labels. The authors demonstrate that networks can perfectly fit noisy training data (benign overfitting) and, after a period, transition from harmful overfitting to a stage where they generalize near-optimally (\"grokking\"). Through rigorous theoretical analysis and proofs, the study reveals that these surprising phenomena are evident in the networks' training trajectories, providing a nuanced understanding of overfitting and generalization in neural network models.\n\nThe paper's contributions lie in providing the first theoretical insights into benign overfitting in non-linearly separable data distributions, unraveling the feature learning dynamics under gradient descent. These findings illuminate the pathways through which neural networks navigate the complexities of noisy data, offering a fresh perspective on their capacity to generalize despite apparent overfitting.\n\nStrengths: * The paper offers a new theoretical examination of benign overfitting and \"grokking\" in two-layer ReLU neural networks. It focuses on XOR cluster data with noisy labels, giving a detailed exploration and proofs related to these phenomena. The authors use existing concepts and new theories to better explain the behavior of neural networks with noisy training data.\n\n* The paper is structured and clear, effectively communicating the authors’ work and results. It has a logical organization that makes it easy for readers to follow the ideas and analyses. The presentation of definitions, explanations, and proofs is mostly straightforward, helping readers understand the complex concepts and findings.\n\n* The paper is important because it helps understand overfitting and generalization in neural networks better. It explains the concepts of benign overfitting and \"grokking\" in one framework.\n\nWeaknesses: * The assumption made in A1 seems to contradict common understanding. Generally, increasing the number of samples, even with limited noisy labels, tends to enhance the generalization capability of neural networks. However, in Assumption A1, having a larger number of training samples seems to adversely affect the model, as indicated by its presence on the right-hand side of the inequality. This aspect might require further clarification or justification within the context of the study.\n\n* The mechanism of overfitting, once the neural network learns the directions of $u_1$ and $u_2$. is not explicitly clear. The paper mentions that post the initial gradient step, positive neurons learn $u_1$ while negative neurons learn $u_2$. However, the explanation seems lacking in how the network overfits to samples with noisy (flipped) labels in this condition. A more detailed discussion or clarification on this would be beneficial.\n\n* The role of Lemma 4.6 in the paper is unclear. A clearer explanation of how it relates to other parts of the paper and how it contributes to the overall arguments and conclusions is needed for better understanding. \n\n* The paper seems to lack a comparative discussion with some relevant works, specifically references [1,2].\n\n\n[1] Meng, Xuran, Difan Zou, and Yuan Cao. \"Benign Overfitting in Two-Layer ReLU Convolutional Neural Networks for XOR Data.\" arXiv preprint arXiv:2310.01975 (2023).\n\n[2] Glasgow, Margalit. \"SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem.\" arXiv preprint arXiv:2309.15111 (2023).\n\nQuestions: * Could you provide more insights or justification regarding Assumption A1? Specifically, could you clarify why an increase in the number of training samples seems to negatively influence the model, contrary to the common understanding that more samples generally improve a model's generalization capability?\n\n* Could you elaborate on the mechanism of overfitting after the neural network learns the directions of $u_1$ and $u_2$.\n\n* Could you clarify the role and significance of Lemma 4.6 in the context of the paper's objectives and findings?\n\n* Could you make a comparsion your results and techniques with [1] and [2]?\n\n* Have you considered variations in the network architecture, such as not fixing the second layer, and if so, how do these variations influence the results?\n\n* In the model settings used in the paper, there doesn’t appear to be an upper bound on the network width. If an extremely wide network setting were used, would the findings align with the \"lazy training\" regime? Could you discuss how the results might be influenced by varying the width of the network to such extremes?\n\nI would increase my score if the authors could clarify my concerns demonstrated in the above questions.\n\n--------------------------------------\nI increase my score to 6 after rebuttal.", "labeling_timestamp": "2026-01-11T16:43:08.724658", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states and summarizes the mechanism: it proves that after one gradient step the network implements a non-generalizable linear classifier (causing catastrophic overfitting) and that with further training neurons align with the core features ±µ1 and ±µ2, which yields generalization. Thus the mechanism is described in the paper (Abstract and Introduction) and is the basis of their proofs.", "evidence": "Abstract: \"Our proofs rely on analyzing the feature learning process under GD, which reveals that the network implements a non-generalizable linear classifier after one step and gradually learns generalizable features in later steps.\"; Introduction: \"After one training step, we prove that the network approximately implements a linear classifier over the underlying data distribution, which is able to overfit all the training datapoints but unable to generalize. Upon further training, the neurons gradually align with the core features ±µ1 and ±µ2, which is sufficient for generalization.\"", "section": "Abstract; 1 Introduction"}
{"claim": "The paper does not explain how the network overfits specifically to samples with noisy flipped labels after initial feature learning.", "claim_type": "methodology", "paper_id": "BxHgpC6FNv", "paper_title": "Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "2KLOTeJSAm", "reviewer": "Reviewer_mTpn", "review_text": "Summary: This paper delves into the exploration of benign overfitting and \"grokking\" in two-layer ReLU neural networks when trained on XOR cluster data with noisy labels. The authors demonstrate that networks can perfectly fit noisy training data (benign overfitting) and, after a period, transition from harmful overfitting to a stage where they generalize near-optimally (\"grokking\"). Through rigorous theoretical analysis and proofs, the study reveals that these surprising phenomena are evident in the networks' training trajectories, providing a nuanced understanding of overfitting and generalization in neural network models.\n\nThe paper's contributions lie in providing the first theoretical insights into benign overfitting in non-linearly separable data distributions, unraveling the feature learning dynamics under gradient descent. These findings illuminate the pathways through which neural networks navigate the complexities of noisy data, offering a fresh perspective on their capacity to generalize despite apparent overfitting.\n\nStrengths: * The paper offers a new theoretical examination of benign overfitting and \"grokking\" in two-layer ReLU neural networks. It focuses on XOR cluster data with noisy labels, giving a detailed exploration and proofs related to these phenomena. The authors use existing concepts and new theories to better explain the behavior of neural networks with noisy training data.\n\n* The paper is structured and clear, effectively communicating the authors’ work and results. It has a logical organization that makes it easy for readers to follow the ideas and analyses. The presentation of definitions, explanations, and proofs is mostly straightforward, helping readers understand the complex concepts and findings.\n\n* The paper is important because it helps understand overfitting and generalization in neural networks better. It explains the concepts of benign overfitting and \"grokking\" in one framework.\n\nWeaknesses: * The assumption made in A1 seems to contradict common understanding. Generally, increasing the number of samples, even with limited noisy labels, tends to enhance the generalization capability of neural networks. However, in Assumption A1, having a larger number of training samples seems to adversely affect the model, as indicated by its presence on the right-hand side of the inequality. This aspect might require further clarification or justification within the context of the study.\n\n* The mechanism of overfitting, once the neural network learns the directions of $u_1$ and $u_2$. is not explicitly clear. The paper mentions that post the initial gradient step, positive neurons learn $u_1$ while negative neurons learn $u_2$. However, the explanation seems lacking in how the network overfits to samples with noisy (flipped) labels in this condition. A more detailed discussion or clarification on this would be beneficial.\n\n* The role of Lemma 4.6 in the paper is unclear. A clearer explanation of how it relates to other parts of the paper and how it contributes to the overall arguments and conclusions is needed for better understanding. \n\n* The paper seems to lack a comparative discussion with some relevant works, specifically references [1,2].\n\n\n[1] Meng, Xuran, Difan Zou, and Yuan Cao. \"Benign Overfitting in Two-Layer ReLU Convolutional Neural Networks for XOR Data.\" arXiv preprint arXiv:2310.01975 (2023).\n\n[2] Glasgow, Margalit. \"SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem.\" arXiv preprint arXiv:2309.15111 (2023).\n\nQuestions: * Could you provide more insights or justification regarding Assumption A1? Specifically, could you clarify why an increase in the number of training samples seems to negatively influence the model, contrary to the common understanding that more samples generally improve a model's generalization capability?\n\n* Could you elaborate on the mechanism of overfitting after the neural network learns the directions of $u_1$ and $u_2$.\n\n* Could you clarify the role and significance of Lemma 4.6 in the context of the paper's objectives and findings?\n\n* Could you make a comparsion your results and techniques with [1] and [2]?\n\n* Have you considered variations in the network architecture, such as not fixing the second layer, and if so, how do these variations influence the results?\n\n* In the model settings used in the paper, there doesn’t appear to be an upper bound on the network width. If an extremely wide network setting were used, would the findings align with the \"lazy training\" regime? Could you discuss how the results might be influenced by varying the width of the network to such extremes?\n\nI would increase my score if the authors could clarify my concerns demonstrated in the above questions.\n\n--------------------------------------\nI increase my score to 6 after rebuttal.", "labeling_timestamp": "2026-01-11T16:43:06.447672", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly analyzes and explains the mechanism by which the network overfits after the initial step: it shows that after one gradient step the network implements a non-generalizable linear classifier that fits all training points (including flipped labels), and later feature learning produces generalizable features. Thus the paper does explain how overfitting to noisy labels occurs after initial feature learning.", "evidence": "“Our proofs rely on analyzing the feature learning process under GD, which reveals that the network implements a non-generalizable linear classifier after one step and gradually learns generalizable features in later steps.”\n\n“After one training step, we prove that the network approximately implements a linear classifier over the underlying data distribution, which is able to overfit all the training datapoints but unable to generalize.”", "section": "Abstract; Introduction (Section 1)"}
{"claim": "The role and significance of Lemma 4.6 are unclear and not explicitly connected to the paper's main arguments or conclusions.", "claim_type": "presentation", "paper_id": "BxHgpC6FNv", "paper_title": "Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "2KLOTeJSAm", "reviewer": "Reviewer_mTpn", "review_text": "Summary: This paper delves into the exploration of benign overfitting and \"grokking\" in two-layer ReLU neural networks when trained on XOR cluster data with noisy labels. The authors demonstrate that networks can perfectly fit noisy training data (benign overfitting) and, after a period, transition from harmful overfitting to a stage where they generalize near-optimally (\"grokking\"). Through rigorous theoretical analysis and proofs, the study reveals that these surprising phenomena are evident in the networks' training trajectories, providing a nuanced understanding of overfitting and generalization in neural network models.\n\nThe paper's contributions lie in providing the first theoretical insights into benign overfitting in non-linearly separable data distributions, unraveling the feature learning dynamics under gradient descent. These findings illuminate the pathways through which neural networks navigate the complexities of noisy data, offering a fresh perspective on their capacity to generalize despite apparent overfitting.\n\nStrengths: * The paper offers a new theoretical examination of benign overfitting and \"grokking\" in two-layer ReLU neural networks. It focuses on XOR cluster data with noisy labels, giving a detailed exploration and proofs related to these phenomena. The authors use existing concepts and new theories to better explain the behavior of neural networks with noisy training data.\n\n* The paper is structured and clear, effectively communicating the authors’ work and results. It has a logical organization that makes it easy for readers to follow the ideas and analyses. The presentation of definitions, explanations, and proofs is mostly straightforward, helping readers understand the complex concepts and findings.\n\n* The paper is important because it helps understand overfitting and generalization in neural networks better. It explains the concepts of benign overfitting and \"grokking\" in one framework.\n\nWeaknesses: * The assumption made in A1 seems to contradict common understanding. Generally, increasing the number of samples, even with limited noisy labels, tends to enhance the generalization capability of neural networks. However, in Assumption A1, having a larger number of training samples seems to adversely affect the model, as indicated by its presence on the right-hand side of the inequality. This aspect might require further clarification or justification within the context of the study.\n\n* The mechanism of overfitting, once the neural network learns the directions of $u_1$ and $u_2$. is not explicitly clear. The paper mentions that post the initial gradient step, positive neurons learn $u_1$ while negative neurons learn $u_2$. However, the explanation seems lacking in how the network overfits to samples with noisy (flipped) labels in this condition. A more detailed discussion or clarification on this would be beneficial.\n\n* The role of Lemma 4.6 in the paper is unclear. A clearer explanation of how it relates to other parts of the paper and how it contributes to the overall arguments and conclusions is needed for better understanding. \n\n* The paper seems to lack a comparative discussion with some relevant works, specifically references [1,2].\n\n\n[1] Meng, Xuran, Difan Zou, and Yuan Cao. \"Benign Overfitting in Two-Layer ReLU Convolutional Neural Networks for XOR Data.\" arXiv preprint arXiv:2310.01975 (2023).\n\n[2] Glasgow, Margalit. \"SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem.\" arXiv preprint arXiv:2309.15111 (2023).\n\nQuestions: * Could you provide more insights or justification regarding Assumption A1? Specifically, could you clarify why an increase in the number of training samples seems to negatively influence the model, contrary to the common understanding that more samples generally improve a model's generalization capability?\n\n* Could you elaborate on the mechanism of overfitting after the neural network learns the directions of $u_1$ and $u_2$.\n\n* Could you clarify the role and significance of Lemma 4.6 in the context of the paper's objectives and findings?\n\n* Could you make a comparsion your results and techniques with [1] and [2]?\n\n* Have you considered variations in the network architecture, such as not fixing the second layer, and if so, how do these variations influence the results?\n\n* In the model settings used in the paper, there doesn’t appear to be an upper bound on the network width. If an extremely wide network setting were used, would the findings align with the \"lazy training\" regime? Could you discuss how the results might be influenced by varying the width of the network to such extremes?\n\nI would increase my score if the authors could clarify my concerns demonstrated in the above questions.\n\n--------------------------------------\nI increase my score to 6 after rebuttal.", "labeling_timestamp": "2026-01-11T16:43:11.835408", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not contain or reference Lemma 4.6, so there is insufficient information to judge whether its role and significance are unclear or unconnected to the main arguments.", "evidence": "“Our proofs rely on analyzing the feature learning process under GD, which reveals that the network implements a non-generalizable linear classifier after one step and gradually learns generalizable features in later steps.”", "section": "Abstract / Introduction (provided excerpt)"}
{"claim": "The paper lacks a comparative discussion with recent relevant works, specifically the referenced papers [1] and [2].", "claim_type": "baseline", "paper_id": "BxHgpC6FNv", "paper_title": "Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "2KLOTeJSAm", "reviewer": "Reviewer_mTpn", "review_text": "Summary: This paper delves into the exploration of benign overfitting and \"grokking\" in two-layer ReLU neural networks when trained on XOR cluster data with noisy labels. The authors demonstrate that networks can perfectly fit noisy training data (benign overfitting) and, after a period, transition from harmful overfitting to a stage where they generalize near-optimally (\"grokking\"). Through rigorous theoretical analysis and proofs, the study reveals that these surprising phenomena are evident in the networks' training trajectories, providing a nuanced understanding of overfitting and generalization in neural network models.\n\nThe paper's contributions lie in providing the first theoretical insights into benign overfitting in non-linearly separable data distributions, unraveling the feature learning dynamics under gradient descent. These findings illuminate the pathways through which neural networks navigate the complexities of noisy data, offering a fresh perspective on their capacity to generalize despite apparent overfitting.\n\nStrengths: * The paper offers a new theoretical examination of benign overfitting and \"grokking\" in two-layer ReLU neural networks. It focuses on XOR cluster data with noisy labels, giving a detailed exploration and proofs related to these phenomena. The authors use existing concepts and new theories to better explain the behavior of neural networks with noisy training data.\n\n* The paper is structured and clear, effectively communicating the authors’ work and results. It has a logical organization that makes it easy for readers to follow the ideas and analyses. The presentation of definitions, explanations, and proofs is mostly straightforward, helping readers understand the complex concepts and findings.\n\n* The paper is important because it helps understand overfitting and generalization in neural networks better. It explains the concepts of benign overfitting and \"grokking\" in one framework.\n\nWeaknesses: * The assumption made in A1 seems to contradict common understanding. Generally, increasing the number of samples, even with limited noisy labels, tends to enhance the generalization capability of neural networks. However, in Assumption A1, having a larger number of training samples seems to adversely affect the model, as indicated by its presence on the right-hand side of the inequality. This aspect might require further clarification or justification within the context of the study.\n\n* The mechanism of overfitting, once the neural network learns the directions of $u_1$ and $u_2$. is not explicitly clear. The paper mentions that post the initial gradient step, positive neurons learn $u_1$ while negative neurons learn $u_2$. However, the explanation seems lacking in how the network overfits to samples with noisy (flipped) labels in this condition. A more detailed discussion or clarification on this would be beneficial.\n\n* The role of Lemma 4.6 in the paper is unclear. A clearer explanation of how it relates to other parts of the paper and how it contributes to the overall arguments and conclusions is needed for better understanding. \n\n* The paper seems to lack a comparative discussion with some relevant works, specifically references [1,2].\n\n\n[1] Meng, Xuran, Difan Zou, and Yuan Cao. \"Benign Overfitting in Two-Layer ReLU Convolutional Neural Networks for XOR Data.\" arXiv preprint arXiv:2310.01975 (2023).\n\n[2] Glasgow, Margalit. \"SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem.\" arXiv preprint arXiv:2309.15111 (2023).\n\nQuestions: * Could you provide more insights or justification regarding Assumption A1? Specifically, could you clarify why an increase in the number of training samples seems to negatively influence the model, contrary to the common understanding that more samples generally improve a model's generalization capability?\n\n* Could you elaborate on the mechanism of overfitting after the neural network learns the directions of $u_1$ and $u_2$.\n\n* Could you clarify the role and significance of Lemma 4.6 in the context of the paper's objectives and findings?\n\n* Could you make a comparsion your results and techniques with [1] and [2]?\n\n* Have you considered variations in the network architecture, such as not fixing the second layer, and if so, how do these variations influence the results?\n\n* In the model settings used in the paper, there doesn’t appear to be an upper bound on the network width. If an extremely wide network setting were used, would the findings align with the \"lazy training\" regime? Could you discuss how the results might be influenced by varying the width of the network to such extremes?\n\nI would increase my score if the authors could clarify my concerns demonstrated in the above questions.\n\n--------------------------------------\nI increase my score to 6 after rebuttal.", "labeling_timestamp": "2026-01-11T16:43:38.410080", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper contains an explicit Related Work section (Section 1.1) that cites and discusses multiple recent relevant works on benign overfitting and grokking, and it explicitly compares its findings to prior papers (e.g., Frei et al. (2022a,b), Xu & Gu (2023), Kou et al. (2023)), so the claim that the paper lacks comparative discussion is contradicted.", "evidence": "“Frei et al. (2022b) showed that two-layer networks with smooth leaky ReLU activations trained by gradient descent (GD) exhibit benign overfitting when trained on a high-dimensional binary cluster distribution. Xu & Gu (2023) extended their results to more general activations like ReLU. Cao et al. (2022) showed that two-layer convolutional networks with polynomial-ReLU activations trained by GD exhibit benign overfitting for image-patch data; Kou et al. (2023) extended their results to allow for label-flipping noise and standard ReLU activations.”\n\n“Most related to this work, Frei et al. (2022a) characterized the dynamics of GD in ReLU networks in the same distributional setting we consider here, namely the XOR cluster with label-flipping noise. They showed that by early-stopping, the neural network achieves perfect (clean) test accuracy although the training error is close to the label noise rate; in particular, their network achieved optimal generalization without overfitting, which is fundamentally different from our result. By contrast, we show that the network first exhibits catastrophic overfitting before transitioning to benign overfitting later in training.”", "section": "1.1 ADDITONAL RELATED WORK"}
{"claim": "The paper does not consider variations in network architecture, such as not fixing the second layer, leaving unclear how these variations influence results.", "claim_type": "methodology", "paper_id": "BxHgpC6FNv", "paper_title": "Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "2KLOTeJSAm", "reviewer": "Reviewer_mTpn", "review_text": "Summary: This paper delves into the exploration of benign overfitting and \"grokking\" in two-layer ReLU neural networks when trained on XOR cluster data with noisy labels. The authors demonstrate that networks can perfectly fit noisy training data (benign overfitting) and, after a period, transition from harmful overfitting to a stage where they generalize near-optimally (\"grokking\"). Through rigorous theoretical analysis and proofs, the study reveals that these surprising phenomena are evident in the networks' training trajectories, providing a nuanced understanding of overfitting and generalization in neural network models.\n\nThe paper's contributions lie in providing the first theoretical insights into benign overfitting in non-linearly separable data distributions, unraveling the feature learning dynamics under gradient descent. These findings illuminate the pathways through which neural networks navigate the complexities of noisy data, offering a fresh perspective on their capacity to generalize despite apparent overfitting.\n\nStrengths: * The paper offers a new theoretical examination of benign overfitting and \"grokking\" in two-layer ReLU neural networks. It focuses on XOR cluster data with noisy labels, giving a detailed exploration and proofs related to these phenomena. The authors use existing concepts and new theories to better explain the behavior of neural networks with noisy training data.\n\n* The paper is structured and clear, effectively communicating the authors’ work and results. It has a logical organization that makes it easy for readers to follow the ideas and analyses. The presentation of definitions, explanations, and proofs is mostly straightforward, helping readers understand the complex concepts and findings.\n\n* The paper is important because it helps understand overfitting and generalization in neural networks better. It explains the concepts of benign overfitting and \"grokking\" in one framework.\n\nWeaknesses: * The assumption made in A1 seems to contradict common understanding. Generally, increasing the number of samples, even with limited noisy labels, tends to enhance the generalization capability of neural networks. However, in Assumption A1, having a larger number of training samples seems to adversely affect the model, as indicated by its presence on the right-hand side of the inequality. This aspect might require further clarification or justification within the context of the study.\n\n* The mechanism of overfitting, once the neural network learns the directions of $u_1$ and $u_2$. is not explicitly clear. The paper mentions that post the initial gradient step, positive neurons learn $u_1$ while negative neurons learn $u_2$. However, the explanation seems lacking in how the network overfits to samples with noisy (flipped) labels in this condition. A more detailed discussion or clarification on this would be beneficial.\n\n* The role of Lemma 4.6 in the paper is unclear. A clearer explanation of how it relates to other parts of the paper and how it contributes to the overall arguments and conclusions is needed for better understanding. \n\n* The paper seems to lack a comparative discussion with some relevant works, specifically references [1,2].\n\n\n[1] Meng, Xuran, Difan Zou, and Yuan Cao. \"Benign Overfitting in Two-Layer ReLU Convolutional Neural Networks for XOR Data.\" arXiv preprint arXiv:2310.01975 (2023).\n\n[2] Glasgow, Margalit. \"SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem.\" arXiv preprint arXiv:2309.15111 (2023).\n\nQuestions: * Could you provide more insights or justification regarding Assumption A1? Specifically, could you clarify why an increase in the number of training samples seems to negatively influence the model, contrary to the common understanding that more samples generally improve a model's generalization capability?\n\n* Could you elaborate on the mechanism of overfitting after the neural network learns the directions of $u_1$ and $u_2$.\n\n* Could you clarify the role and significance of Lemma 4.6 in the context of the paper's objectives and findings?\n\n* Could you make a comparsion your results and techniques with [1] and [2]?\n\n* Have you considered variations in the network architecture, such as not fixing the second layer, and if so, how do these variations influence the results?\n\n* In the model settings used in the paper, there doesn’t appear to be an upper bound on the network width. If an extremely wide network setting were used, would the findings align with the \"lazy training\" regime? Could you discuss how the results might be influenced by varying the width of the network to such extremes?\n\nI would increase my score if the authors could clarify my concerns demonstrated in the above questions.\n\n--------------------------------------\nI increase my score to 6 after rebuttal.", "labeling_timestamp": "2026-01-11T16:43:27.743305", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly fixes the second-layer weights and only analyzes training of the first-layer weights; it does not study alternative architectures or the effect of training the second layer, so the claim that variations (e.g., not fixing the second layer) are not considered is accurate.", "evidence": "We assume the second-layer weights are sampled according to a_j i.i.d. ∼ Unif{±1/√m} and are fixed during the training process.", "section": "2.3 NEURAL NETWORK, LOSS FUNCTION, AND TRAINING PROCEDURE"}
{"claim": "The model settings do not specify an upper bound on the network width.", "claim_type": "methodology", "paper_id": "BxHgpC6FNv", "paper_title": "Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "2KLOTeJSAm", "reviewer": "Reviewer_mTpn", "review_text": "Summary: This paper delves into the exploration of benign overfitting and \"grokking\" in two-layer ReLU neural networks when trained on XOR cluster data with noisy labels. The authors demonstrate that networks can perfectly fit noisy training data (benign overfitting) and, after a period, transition from harmful overfitting to a stage where they generalize near-optimally (\"grokking\"). Through rigorous theoretical analysis and proofs, the study reveals that these surprising phenomena are evident in the networks' training trajectories, providing a nuanced understanding of overfitting and generalization in neural network models.\n\nThe paper's contributions lie in providing the first theoretical insights into benign overfitting in non-linearly separable data distributions, unraveling the feature learning dynamics under gradient descent. These findings illuminate the pathways through which neural networks navigate the complexities of noisy data, offering a fresh perspective on their capacity to generalize despite apparent overfitting.\n\nStrengths: * The paper offers a new theoretical examination of benign overfitting and \"grokking\" in two-layer ReLU neural networks. It focuses on XOR cluster data with noisy labels, giving a detailed exploration and proofs related to these phenomena. The authors use existing concepts and new theories to better explain the behavior of neural networks with noisy training data.\n\n* The paper is structured and clear, effectively communicating the authors’ work and results. It has a logical organization that makes it easy for readers to follow the ideas and analyses. The presentation of definitions, explanations, and proofs is mostly straightforward, helping readers understand the complex concepts and findings.\n\n* The paper is important because it helps understand overfitting and generalization in neural networks better. It explains the concepts of benign overfitting and \"grokking\" in one framework.\n\nWeaknesses: * The assumption made in A1 seems to contradict common understanding. Generally, increasing the number of samples, even with limited noisy labels, tends to enhance the generalization capability of neural networks. However, in Assumption A1, having a larger number of training samples seems to adversely affect the model, as indicated by its presence on the right-hand side of the inequality. This aspect might require further clarification or justification within the context of the study.\n\n* The mechanism of overfitting, once the neural network learns the directions of $u_1$ and $u_2$. is not explicitly clear. The paper mentions that post the initial gradient step, positive neurons learn $u_1$ while negative neurons learn $u_2$. However, the explanation seems lacking in how the network overfits to samples with noisy (flipped) labels in this condition. A more detailed discussion or clarification on this would be beneficial.\n\n* The role of Lemma 4.6 in the paper is unclear. A clearer explanation of how it relates to other parts of the paper and how it contributes to the overall arguments and conclusions is needed for better understanding. \n\n* The paper seems to lack a comparative discussion with some relevant works, specifically references [1,2].\n\n\n[1] Meng, Xuran, Difan Zou, and Yuan Cao. \"Benign Overfitting in Two-Layer ReLU Convolutional Neural Networks for XOR Data.\" arXiv preprint arXiv:2310.01975 (2023).\n\n[2] Glasgow, Margalit. \"SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem.\" arXiv preprint arXiv:2309.15111 (2023).\n\nQuestions: * Could you provide more insights or justification regarding Assumption A1? Specifically, could you clarify why an increase in the number of training samples seems to negatively influence the model, contrary to the common understanding that more samples generally improve a model's generalization capability?\n\n* Could you elaborate on the mechanism of overfitting after the neural network learns the directions of $u_1$ and $u_2$.\n\n* Could you clarify the role and significance of Lemma 4.6 in the context of the paper's objectives and findings?\n\n* Could you make a comparsion your results and techniques with [1] and [2]?\n\n* Have you considered variations in the network architecture, such as not fixing the second layer, and if so, how do these variations influence the results?\n\n* In the model settings used in the paper, there doesn’t appear to be an upper bound on the network width. If an extremely wide network setting were used, would the findings align with the \"lazy training\" regime? Could you discuss how the results might be influenced by varying the width of the network to such extremes?\n\nI would increase my score if the authors could clarify my concerns demonstrated in the above questions.\n\n--------------------------------------\nI increase my score to 6 after rebuttal.", "labeling_timestamp": "2026-01-11T16:43:40.807592", "model": "gpt-5-mini", "label": "Not Determined", "justification": "In the provided excerpts the network width m is introduced but no explicit upper bound on m is given; however the main results/assumptions section is truncated and may contain bounds on m later, so there is insufficient information to conclude the paper does not specify an upper bound.", "evidence": "\"We consider a two-layer neural network of width m of the form\"; \"We denote W = [ w_1, . . . , w_m ] ∈ R^{p×m} and a = [ a_1, . . . , a_m ]^⊤ ∈ R^m.\"; \"Given a large enough universal constant C, we make the following assumptions:\"", "section": "2.3 NEURAL NETWORK, LOSS FUNCTION, AND TRAINING PROCEDURE; 3 MAIN RESULTS (assumptions begin)"}
{"claim": "The paper does not analyze whether extremely wide networks would fall into the lazy training regime or how varying width influences the reported phenomena.", "claim_type": "experimental", "paper_id": "BxHgpC6FNv", "paper_title": "Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "2KLOTeJSAm", "reviewer": "Reviewer_mTpn", "review_text": "Summary: This paper delves into the exploration of benign overfitting and \"grokking\" in two-layer ReLU neural networks when trained on XOR cluster data with noisy labels. The authors demonstrate that networks can perfectly fit noisy training data (benign overfitting) and, after a period, transition from harmful overfitting to a stage where they generalize near-optimally (\"grokking\"). Through rigorous theoretical analysis and proofs, the study reveals that these surprising phenomena are evident in the networks' training trajectories, providing a nuanced understanding of overfitting and generalization in neural network models.\n\nThe paper's contributions lie in providing the first theoretical insights into benign overfitting in non-linearly separable data distributions, unraveling the feature learning dynamics under gradient descent. These findings illuminate the pathways through which neural networks navigate the complexities of noisy data, offering a fresh perspective on their capacity to generalize despite apparent overfitting.\n\nStrengths: * The paper offers a new theoretical examination of benign overfitting and \"grokking\" in two-layer ReLU neural networks. It focuses on XOR cluster data with noisy labels, giving a detailed exploration and proofs related to these phenomena. The authors use existing concepts and new theories to better explain the behavior of neural networks with noisy training data.\n\n* The paper is structured and clear, effectively communicating the authors’ work and results. It has a logical organization that makes it easy for readers to follow the ideas and analyses. The presentation of definitions, explanations, and proofs is mostly straightforward, helping readers understand the complex concepts and findings.\n\n* The paper is important because it helps understand overfitting and generalization in neural networks better. It explains the concepts of benign overfitting and \"grokking\" in one framework.\n\nWeaknesses: * The assumption made in A1 seems to contradict common understanding. Generally, increasing the number of samples, even with limited noisy labels, tends to enhance the generalization capability of neural networks. However, in Assumption A1, having a larger number of training samples seems to adversely affect the model, as indicated by its presence on the right-hand side of the inequality. This aspect might require further clarification or justification within the context of the study.\n\n* The mechanism of overfitting, once the neural network learns the directions of $u_1$ and $u_2$. is not explicitly clear. The paper mentions that post the initial gradient step, positive neurons learn $u_1$ while negative neurons learn $u_2$. However, the explanation seems lacking in how the network overfits to samples with noisy (flipped) labels in this condition. A more detailed discussion or clarification on this would be beneficial.\n\n* The role of Lemma 4.6 in the paper is unclear. A clearer explanation of how it relates to other parts of the paper and how it contributes to the overall arguments and conclusions is needed for better understanding. \n\n* The paper seems to lack a comparative discussion with some relevant works, specifically references [1,2].\n\n\n[1] Meng, Xuran, Difan Zou, and Yuan Cao. \"Benign Overfitting in Two-Layer ReLU Convolutional Neural Networks for XOR Data.\" arXiv preprint arXiv:2310.01975 (2023).\n\n[2] Glasgow, Margalit. \"SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem.\" arXiv preprint arXiv:2309.15111 (2023).\n\nQuestions: * Could you provide more insights or justification regarding Assumption A1? Specifically, could you clarify why an increase in the number of training samples seems to negatively influence the model, contrary to the common understanding that more samples generally improve a model's generalization capability?\n\n* Could you elaborate on the mechanism of overfitting after the neural network learns the directions of $u_1$ and $u_2$.\n\n* Could you clarify the role and significance of Lemma 4.6 in the context of the paper's objectives and findings?\n\n* Could you make a comparsion your results and techniques with [1] and [2]?\n\n* Have you considered variations in the network architecture, such as not fixing the second layer, and if so, how do these variations influence the results?\n\n* In the model settings used in the paper, there doesn’t appear to be an upper bound on the network width. If an extremely wide network setting were used, would the findings align with the \"lazy training\" regime? Could you discuss how the results might be influenced by varying the width of the network to such extremes?\n\nI would increase my score if the authors could clarify my concerns demonstrated in the above questions.\n\n--------------------------------------\nI increase my score to 6 after rebuttal.", "labeling_timestamp": "2026-01-11T16:43:39.017687", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper focuses on feature learning in two-layer ReLU networks of width m and proves one-step catastrophic overfitting and later grokking via neuron alignment, but it contains no analysis of extremely wide networks, the lazy/NTK regime, or how varying the network width m affects the phenomena.", "evidence": ["Our proofs rely on analyzing the feature learning behavior of individual neurons over the gradient descent trajectory, which reveals that the network implements a non-generalizable linear classifier after one step and gradually learns generalizable features in later steps.", "We consider a two-layer neural network of width m of the form ...", "We define W = [w_1, ..., w_m] ∈ R^{p×m} and a = [a_1, ..., a_m]^⊤ ∈ R^m. We assume the second-layer weights are sampled according to a_j i.i.d. ∼ Unif{±1/√m} and are fixed during the training process."], "section": "Abstract; Introduction; Section 2.3 (Neural network, loss function, and training procedure)"}
{"claim": "The proposed algorithms rely on standard techniques like pessimism and maximum likelihood estimation, and the paper does not clearly establish technical novelty.", "claim_type": "novelty", "paper_id": "XmkuQfWZAB", "paper_title": "On Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "pg25m3wQGF", "reviewer": "Reviewer_5Lzq", "review_text": "Summary: This paper develops a theoretical comparison between these human feedback approaches in offline contextual bandits and shows how human bias and uncertainty in feedback modeling can affect the theoretical guarantees of these approaches. The proposed results seek to provide a theoretical explanation for the empirical successes of preference-based methods from a modeling perspective.\n\nStrengths: 1.\tThe studied problem, i.e., contextual bandits with human feedback, is very well-motivated and finds important applications such as large language models.\n2.\tThe authors propose algorithms based on pessimism with suboptimality guarantees.\n\nWeaknesses: 1.\tIt seems that the proposed algorithms are designed based on standard techniques, such as pessimism and MLE. The authors should elaborate more on their technical novelty. This is my main concern.\n2.\tIt would be more clear to present conditions 1, 2 and 3 as assumptions. The authors should justify more on these assumptions. For example, why is condition 1 reasonable? Why the noise never changes the human preference? \n3.\tIn Theorem 1, the setup $C^*=2$ seems too specific. Can the result be extended to the one that allows general $C^*$ and depends on $C^*$?\n\n**-------After Rebuttal-------**\n\nThank the authors for their rebuttal. I read the authors' response and other reviewers' comments. \n\nI agree with the comments of Reviewer jnsn, i.e., the logic of this paper is not very reasonable. Specifically, the authors prove that the LCB algorithm with *biased* rating feedback is not sample efficient in Section 4, the pessimistic MLE algorithm with *unbiased* human feedback is sample efficient in Section 5.1, and furthermore, the pessimistic MLE algorithm with *biased* human feedback is also not sample efficient in Section 5.2. Then, without any experiments on the studied settings and designed algorithms, the authors directly come to a conclusion --- the reason that human feedback works well in practical LLMs is probably due to less bias. I do not think the theoretical results in this paper prove (or provide sufficient supports for) this conclusion.\n\nIn my opinion, the theoretical analysis in this paper is standard in the bandit and offline RL literature. The authors replied that the main purpose of this paper is to propose a theoretical explanation for the empirical phenomenon. However, with the current theoretical results under the restrictive and biased settings and assumptions, and without any experiments to connect with their settings and algorithms, I do not think the presented theory can effectively explain why human feedback works well in practical LLMs.\n\nCurrently I tend to keep my score 3, and will listen to the opinions of other reviewers and AC.\n\nQuestions: Please see the weaknesses above.", "labeling_timestamp": "2026-01-11T16:43:50.122142", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper indeed builds on standard techniques (pessimism, LCB, pessimistic MLE), so the first part of the reviewer claim is supported. However, the paper explicitly asserts novel contributions (new rating and preference models and the 'first known suboptimality analysis' for rating-based reward engineering), so the claim that the paper does not clearly establish technical novelty is contradicted by the authors' stated claims of novelty.", "evidence": [{"quote": "Recently, [80; 78] proved the optimal policy can be learned from preference data in the offline setting with pessimism and maximum likelihood estimation (MLE) and analyzed the suboptimality.", "section": "Introduction"}, {"quote": "In this spirit, the algorithms we study in this work also use human feedback with pessimism.", "section": "1.1 RELATED WORKS"}, {"quote": "we first consider a new model for human rating data and analyze the suboptimality guarantees of the standard LCB algorithm under it.", "section": "Introduction"}, {"quote": "Through our models, we provide the first known suboptimality analysis for reward engineering with human rating in bandit problems and shed light on how human bias and uncertainty can adversely impact policy learning.", "section": "Introduction"}, {"quote": "we also consider a new model for human preference with human bias and compare the sample complexity of pessimistic MLE under this new model with the results for human rating.", "section": "Introduction"}], "section": ""}
{"claim": "Conditions 1, 2, and 3 are presented as conditions rather than formal assumptions, and the paper fails to justify this presentation choice.", "claim_type": "methodology", "paper_id": "XmkuQfWZAB", "paper_title": "On Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "pg25m3wQGF", "reviewer": "Reviewer_5Lzq", "review_text": "Summary: This paper develops a theoretical comparison between these human feedback approaches in offline contextual bandits and shows how human bias and uncertainty in feedback modeling can affect the theoretical guarantees of these approaches. The proposed results seek to provide a theoretical explanation for the empirical successes of preference-based methods from a modeling perspective.\n\nStrengths: 1.\tThe studied problem, i.e., contextual bandits with human feedback, is very well-motivated and finds important applications such as large language models.\n2.\tThe authors propose algorithms based on pessimism with suboptimality guarantees.\n\nWeaknesses: 1.\tIt seems that the proposed algorithms are designed based on standard techniques, such as pessimism and MLE. The authors should elaborate more on their technical novelty. This is my main concern.\n2.\tIt would be more clear to present conditions 1, 2 and 3 as assumptions. The authors should justify more on these assumptions. For example, why is condition 1 reasonable? Why the noise never changes the human preference? \n3.\tIn Theorem 1, the setup $C^*=2$ seems too specific. Can the result be extended to the one that allows general $C^*$ and depends on $C^*$?\n\n**-------After Rebuttal-------**\n\nThank the authors for their rebuttal. I read the authors' response and other reviewers' comments. \n\nI agree with the comments of Reviewer jnsn, i.e., the logic of this paper is not very reasonable. Specifically, the authors prove that the LCB algorithm with *biased* rating feedback is not sample efficient in Section 4, the pessimistic MLE algorithm with *unbiased* human feedback is sample efficient in Section 5.1, and furthermore, the pessimistic MLE algorithm with *biased* human feedback is also not sample efficient in Section 5.2. Then, without any experiments on the studied settings and designed algorithms, the authors directly come to a conclusion --- the reason that human feedback works well in practical LLMs is probably due to less bias. I do not think the theoretical results in this paper prove (or provide sufficient supports for) this conclusion.\n\nIn my opinion, the theoretical analysis in this paper is standard in the bandit and offline RL literature. The authors replied that the main purpose of this paper is to propose a theoretical explanation for the empirical phenomenon. However, with the current theoretical results under the restrictive and biased settings and assumptions, and without any experiments to connect with their settings and algorithms, I do not think the presented theory can effectively explain why human feedback works well in practical LLMs.\n\nCurrently I tend to keep my score 3, and will listen to the opinions of other reviewers and AC.\n\nQuestions: Please see the weaknesses above.", "labeling_timestamp": "2026-01-11T16:44:01.560705", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not contain or show \"Conditions 1, 2, and 3\" nor any discussion about presenting them as conditions versus formal assumptions, so there is insufficient information to verify the reviewer's claim.", "evidence": "In the following, we present this model in the single annotator case under the contextual bandit setting.", "section": "3 HUMAN RATING MODELS"}
{"claim": "The paper does not justify why condition 1 is a reasonable assumption for the studied human feedback models.", "claim_type": "methodology", "paper_id": "XmkuQfWZAB", "paper_title": "On Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "pg25m3wQGF", "reviewer": "Reviewer_5Lzq", "review_text": "Summary: This paper develops a theoretical comparison between these human feedback approaches in offline contextual bandits and shows how human bias and uncertainty in feedback modeling can affect the theoretical guarantees of these approaches. The proposed results seek to provide a theoretical explanation for the empirical successes of preference-based methods from a modeling perspective.\n\nStrengths: 1.\tThe studied problem, i.e., contextual bandits with human feedback, is very well-motivated and finds important applications such as large language models.\n2.\tThe authors propose algorithms based on pessimism with suboptimality guarantees.\n\nWeaknesses: 1.\tIt seems that the proposed algorithms are designed based on standard techniques, such as pessimism and MLE. The authors should elaborate more on their technical novelty. This is my main concern.\n2.\tIt would be more clear to present conditions 1, 2 and 3 as assumptions. The authors should justify more on these assumptions. For example, why is condition 1 reasonable? Why the noise never changes the human preference? \n3.\tIn Theorem 1, the setup $C^*=2$ seems too specific. Can the result be extended to the one that allows general $C^*$ and depends on $C^*$?\n\n**-------After Rebuttal-------**\n\nThank the authors for their rebuttal. I read the authors' response and other reviewers' comments. \n\nI agree with the comments of Reviewer jnsn, i.e., the logic of this paper is not very reasonable. Specifically, the authors prove that the LCB algorithm with *biased* rating feedback is not sample efficient in Section 4, the pessimistic MLE algorithm with *unbiased* human feedback is sample efficient in Section 5.1, and furthermore, the pessimistic MLE algorithm with *biased* human feedback is also not sample efficient in Section 5.2. Then, without any experiments on the studied settings and designed algorithms, the authors directly come to a conclusion --- the reason that human feedback works well in practical LLMs is probably due to less bias. I do not think the theoretical results in this paper prove (or provide sufficient supports for) this conclusion.\n\nIn my opinion, the theoretical analysis in this paper is standard in the bandit and offline RL literature. The authors replied that the main purpose of this paper is to propose a theoretical explanation for the empirical phenomenon. However, with the current theoretical results under the restrictive and biased settings and assumptions, and without any experiments to connect with their settings and algorithms, I do not think the presented theory can effectively explain why human feedback works well in practical LLMs.\n\nCurrently I tend to keep my score 3, and will listen to the opinions of other reviewers and AC.\n\nQuestions: Please see the weaknesses above.", "labeling_timestamp": "2026-01-11T16:44:00.394915", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not contain or reference any item labeled 'Condition 1' nor any explicit justification for such a condition. Therefore, from the available text it is impossible to verify whether the paper justifies why 'Condition 1' is a reasonable assumption.", "evidence": "Specifically, we first consider a new model for human rating data and analyze the suboptimality guarantees of the standard LCB algorithm under it.", "section": "1 INTRODUCTION"}
{"claim": "The assumption that noise never changes human preference is stated without justification or empirical support.", "claim_type": "methodology", "paper_id": "XmkuQfWZAB", "paper_title": "On Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "pg25m3wQGF", "reviewer": "Reviewer_5Lzq", "review_text": "Summary: This paper develops a theoretical comparison between these human feedback approaches in offline contextual bandits and shows how human bias and uncertainty in feedback modeling can affect the theoretical guarantees of these approaches. The proposed results seek to provide a theoretical explanation for the empirical successes of preference-based methods from a modeling perspective.\n\nStrengths: 1.\tThe studied problem, i.e., contextual bandits with human feedback, is very well-motivated and finds important applications such as large language models.\n2.\tThe authors propose algorithms based on pessimism with suboptimality guarantees.\n\nWeaknesses: 1.\tIt seems that the proposed algorithms are designed based on standard techniques, such as pessimism and MLE. The authors should elaborate more on their technical novelty. This is my main concern.\n2.\tIt would be more clear to present conditions 1, 2 and 3 as assumptions. The authors should justify more on these assumptions. For example, why is condition 1 reasonable? Why the noise never changes the human preference? \n3.\tIn Theorem 1, the setup $C^*=2$ seems too specific. Can the result be extended to the one that allows general $C^*$ and depends on $C^*$?\n\n**-------After Rebuttal-------**\n\nThank the authors for their rebuttal. I read the authors' response and other reviewers' comments. \n\nI agree with the comments of Reviewer jnsn, i.e., the logic of this paper is not very reasonable. Specifically, the authors prove that the LCB algorithm with *biased* rating feedback is not sample efficient in Section 4, the pessimistic MLE algorithm with *unbiased* human feedback is sample efficient in Section 5.1, and furthermore, the pessimistic MLE algorithm with *biased* human feedback is also not sample efficient in Section 5.2. Then, without any experiments on the studied settings and designed algorithms, the authors directly come to a conclusion --- the reason that human feedback works well in practical LLMs is probably due to less bias. I do not think the theoretical results in this paper prove (or provide sufficient supports for) this conclusion.\n\nIn my opinion, the theoretical analysis in this paper is standard in the bandit and offline RL literature. The authors replied that the main purpose of this paper is to propose a theoretical explanation for the empirical phenomenon. However, with the current theoretical results under the restrictive and biased settings and assumptions, and without any experiments to connect with their settings and algorithms, I do not think the presented theory can effectively explain why human feedback works well in practical LLMs.\n\nCurrently I tend to keep my score 3, and will listen to the opinions of other reviewers and AC.\n\nQuestions: Please see the weaknesses above.", "labeling_timestamp": "2026-01-11T16:44:07.949023", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly assumes a monotonic rating model that preserves reward ordering (i.e., noise does not change preference). It offers a conceptual rationale for this assumption (saying monotonicity captures observed bias while maintaining ordering) but does not provide empirical validation or experiments supporting the assumption in the provided content.", "evidence": "\"Our rating model is based on a general class of monotone functions that can account for both human bias and uncertainty with general forms. By incorporating the concept of monotonicity, our model captures the bias observed in real-world human rating and maintains the correct reward ordering.\"", "section": "Abstract / Section 3 (Human Rating Models)"}
{"claim": "Theorem 1 is proved only for the specific setting C^* = 2 and the paper does not provide an extension or dependence on general C^*.", "claim_type": "methodology", "paper_id": "XmkuQfWZAB", "paper_title": "On Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "pg25m3wQGF", "reviewer": "Reviewer_5Lzq", "review_text": "Summary: This paper develops a theoretical comparison between these human feedback approaches in offline contextual bandits and shows how human bias and uncertainty in feedback modeling can affect the theoretical guarantees of these approaches. The proposed results seek to provide a theoretical explanation for the empirical successes of preference-based methods from a modeling perspective.\n\nStrengths: 1.\tThe studied problem, i.e., contextual bandits with human feedback, is very well-motivated and finds important applications such as large language models.\n2.\tThe authors propose algorithms based on pessimism with suboptimality guarantees.\n\nWeaknesses: 1.\tIt seems that the proposed algorithms are designed based on standard techniques, such as pessimism and MLE. The authors should elaborate more on their technical novelty. This is my main concern.\n2.\tIt would be more clear to present conditions 1, 2 and 3 as assumptions. The authors should justify more on these assumptions. For example, why is condition 1 reasonable? Why the noise never changes the human preference? \n3.\tIn Theorem 1, the setup $C^*=2$ seems too specific. Can the result be extended to the one that allows general $C^*$ and depends on $C^*$?\n\n**-------After Rebuttal-------**\n\nThank the authors for their rebuttal. I read the authors' response and other reviewers' comments. \n\nI agree with the comments of Reviewer jnsn, i.e., the logic of this paper is not very reasonable. Specifically, the authors prove that the LCB algorithm with *biased* rating feedback is not sample efficient in Section 4, the pessimistic MLE algorithm with *unbiased* human feedback is sample efficient in Section 5.1, and furthermore, the pessimistic MLE algorithm with *biased* human feedback is also not sample efficient in Section 5.2. Then, without any experiments on the studied settings and designed algorithms, the authors directly come to a conclusion --- the reason that human feedback works well in practical LLMs is probably due to less bias. I do not think the theoretical results in this paper prove (or provide sufficient supports for) this conclusion.\n\nIn my opinion, the theoretical analysis in this paper is standard in the bandit and offline RL literature. The authors replied that the main purpose of this paper is to propose a theoretical explanation for the empirical phenomenon. However, with the current theoretical results under the restrictive and biased settings and assumptions, and without any experiments to connect with their settings and algorithms, I do not think the presented theory can effectively explain why human feedback works well in practical LLMs.\n\nCurrently I tend to keep my score 3, and will listen to the opinions of other reviewers and AC.\n\nQuestions: Please see the weaknesses above.", "labeling_timestamp": "2026-01-11T16:44:18.414749", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt (Abstract, Introduction, Related Works, Preliminaries, and the start of Section 3) does not contain Theorem 1 nor any mention of a parameter C^*, so there is insufficient information to decide whether Theorem 1 is proved only for C^* = 2 or whether extensions/dependence on general C^* are provided elsewhere in the paper.", "evidence": "\"In the following, we present this model in the single annotator case under the contextual bandit setting. For any fixed state-action pair ( s↪ a ) ∈ S × A with true reward r ( s↪ a ) , a rating from human annotator ˜ r ( s↪ a ) can be written as ˜ r ( s↪ a ) = r ( s↪ a ) + ∆ ( s↪a ) + /epsilon1 glyph[triangleright] (2)\"", "section": "3 HUMAN RATING MODELS"}
{"claim": "There are no experiments connecting the theoretical settings or the designed algorithms to practical large language model implementations.", "claim_type": "experimental", "paper_id": "XmkuQfWZAB", "paper_title": "On Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "pg25m3wQGF", "reviewer": "Reviewer_5Lzq", "review_text": "Summary: This paper develops a theoretical comparison between these human feedback approaches in offline contextual bandits and shows how human bias and uncertainty in feedback modeling can affect the theoretical guarantees of these approaches. The proposed results seek to provide a theoretical explanation for the empirical successes of preference-based methods from a modeling perspective.\n\nStrengths: 1.\tThe studied problem, i.e., contextual bandits with human feedback, is very well-motivated and finds important applications such as large language models.\n2.\tThe authors propose algorithms based on pessimism with suboptimality guarantees.\n\nWeaknesses: 1.\tIt seems that the proposed algorithms are designed based on standard techniques, such as pessimism and MLE. The authors should elaborate more on their technical novelty. This is my main concern.\n2.\tIt would be more clear to present conditions 1, 2 and 3 as assumptions. The authors should justify more on these assumptions. For example, why is condition 1 reasonable? Why the noise never changes the human preference? \n3.\tIn Theorem 1, the setup $C^*=2$ seems too specific. Can the result be extended to the one that allows general $C^*$ and depends on $C^*$?\n\n**-------After Rebuttal-------**\n\nThank the authors for their rebuttal. I read the authors' response and other reviewers' comments. \n\nI agree with the comments of Reviewer jnsn, i.e., the logic of this paper is not very reasonable. Specifically, the authors prove that the LCB algorithm with *biased* rating feedback is not sample efficient in Section 4, the pessimistic MLE algorithm with *unbiased* human feedback is sample efficient in Section 5.1, and furthermore, the pessimistic MLE algorithm with *biased* human feedback is also not sample efficient in Section 5.2. Then, without any experiments on the studied settings and designed algorithms, the authors directly come to a conclusion --- the reason that human feedback works well in practical LLMs is probably due to less bias. I do not think the theoretical results in this paper prove (or provide sufficient supports for) this conclusion.\n\nIn my opinion, the theoretical analysis in this paper is standard in the bandit and offline RL literature. The authors replied that the main purpose of this paper is to propose a theoretical explanation for the empirical phenomenon. However, with the current theoretical results under the restrictive and biased settings and assumptions, and without any experiments to connect with their settings and algorithms, I do not think the presented theory can effectively explain why human feedback works well in practical LLMs.\n\nCurrently I tend to keep my score 3, and will listen to the opinions of other reviewers and AC.\n\nQuestions: Please see the weaknesses above.", "labeling_timestamp": "2026-01-11T16:44:24.620668", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper presents theoretical analysis in the offline tabular contextual bandit setting (developing models and suboptimality bounds) and frames its contribution as a theoretical explanation for empirical successes; it contains no experimental section or experiments linking the proposed theory/algorithms to practical LLM implementations.", "evidence": "\"In this work, we develop a theoretical comparison between these human feedback approaches in offline contextual bandits and show how human bias and uncertainty in feedback modelings can affect the theoretical guarantees of these approaches.\" (Abstract) \"To align with recent applications [13; 38], we focus on tabular contextual bandits in the offline setting.\" (Introduction)", "section": "Abstract; Introduction"}
{"claim": "The paper concludes that less bias explains human feedback success in practical LLMs, but the theoretical results do not sufficiently support this conclusion.", "claim_type": "subjective", "paper_id": "XmkuQfWZAB", "paper_title": "On Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "pg25m3wQGF", "reviewer": "Reviewer_5Lzq", "review_text": "Summary: This paper develops a theoretical comparison between these human feedback approaches in offline contextual bandits and shows how human bias and uncertainty in feedback modeling can affect the theoretical guarantees of these approaches. The proposed results seek to provide a theoretical explanation for the empirical successes of preference-based methods from a modeling perspective.\n\nStrengths: 1.\tThe studied problem, i.e., contextual bandits with human feedback, is very well-motivated and finds important applications such as large language models.\n2.\tThe authors propose algorithms based on pessimism with suboptimality guarantees.\n\nWeaknesses: 1.\tIt seems that the proposed algorithms are designed based on standard techniques, such as pessimism and MLE. The authors should elaborate more on their technical novelty. This is my main concern.\n2.\tIt would be more clear to present conditions 1, 2 and 3 as assumptions. The authors should justify more on these assumptions. For example, why is condition 1 reasonable? Why the noise never changes the human preference? \n3.\tIn Theorem 1, the setup $C^*=2$ seems too specific. Can the result be extended to the one that allows general $C^*$ and depends on $C^*$?\n\n**-------After Rebuttal-------**\n\nThank the authors for their rebuttal. I read the authors' response and other reviewers' comments. \n\nI agree with the comments of Reviewer jnsn, i.e., the logic of this paper is not very reasonable. Specifically, the authors prove that the LCB algorithm with *biased* rating feedback is not sample efficient in Section 4, the pessimistic MLE algorithm with *unbiased* human feedback is sample efficient in Section 5.1, and furthermore, the pessimistic MLE algorithm with *biased* human feedback is also not sample efficient in Section 5.2. Then, without any experiments on the studied settings and designed algorithms, the authors directly come to a conclusion --- the reason that human feedback works well in practical LLMs is probably due to less bias. I do not think the theoretical results in this paper prove (or provide sufficient supports for) this conclusion.\n\nIn my opinion, the theoretical analysis in this paper is standard in the bandit and offline RL literature. The authors replied that the main purpose of this paper is to propose a theoretical explanation for the empirical phenomenon. However, with the current theoretical results under the restrictive and biased settings and assumptions, and without any experiments to connect with their settings and algorithms, I do not think the presented theory can effectively explain why human feedback works well in practical LLMs.\n\nCurrently I tend to keep my score 3, and will listen to the opinions of other reviewers and AC.\n\nQuestions: Please see the weaknesses above.", "labeling_timestamp": "2026-01-11T16:44:41.416418", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly concludes that the empirical success of preference-based methods can be explained by preference data having milder bias/uncertainty (supported). However, the paper also presents theoretical suboptimality comparisons and claims that these results explain when and why preference feedback helps, so the reviewer's assertion that the theoretical results do not sufficiently support the conclusion is not consistent with the paper (contradicted). Because the claim contains one true and one false component, the correct overall judgment is ‘Partially Supported.’", "evidence": "1) \"Through this, our results seek to provide a theoretical explanation for the empirical successes of preference-based methods from a modeling perspective.\" 2) \"Altogether, our theory shows the advantage of the preference-based approach can be largely attributed to its modeling with mild human bias and uncertainty, which makes it reasonable to believe the great empirical success of preference-based methods is because human preference data is subject to less bias and uncertainty in practice.\" 3) \"The comparison reveals that the preference-based approach enjoys lower suboptimality than the rating-based approach when human bias is extreme in human rating.\" ", "section": "Abstract; 1 INTRODUCTION"}
{"claim": "Theoretical analysis presented appears standard relative to existing bandit and offline reinforcement learning literature, undermining claims of novelty.", "claim_type": "novelty", "paper_id": "XmkuQfWZAB", "paper_title": "On Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "pg25m3wQGF", "reviewer": "Reviewer_5Lzq", "review_text": "Summary: This paper develops a theoretical comparison between these human feedback approaches in offline contextual bandits and shows how human bias and uncertainty in feedback modeling can affect the theoretical guarantees of these approaches. The proposed results seek to provide a theoretical explanation for the empirical successes of preference-based methods from a modeling perspective.\n\nStrengths: 1.\tThe studied problem, i.e., contextual bandits with human feedback, is very well-motivated and finds important applications such as large language models.\n2.\tThe authors propose algorithms based on pessimism with suboptimality guarantees.\n\nWeaknesses: 1.\tIt seems that the proposed algorithms are designed based on standard techniques, such as pessimism and MLE. The authors should elaborate more on their technical novelty. This is my main concern.\n2.\tIt would be more clear to present conditions 1, 2 and 3 as assumptions. The authors should justify more on these assumptions. For example, why is condition 1 reasonable? Why the noise never changes the human preference? \n3.\tIn Theorem 1, the setup $C^*=2$ seems too specific. Can the result be extended to the one that allows general $C^*$ and depends on $C^*$?\n\n**-------After Rebuttal-------**\n\nThank the authors for their rebuttal. I read the authors' response and other reviewers' comments. \n\nI agree with the comments of Reviewer jnsn, i.e., the logic of this paper is not very reasonable. Specifically, the authors prove that the LCB algorithm with *biased* rating feedback is not sample efficient in Section 4, the pessimistic MLE algorithm with *unbiased* human feedback is sample efficient in Section 5.1, and furthermore, the pessimistic MLE algorithm with *biased* human feedback is also not sample efficient in Section 5.2. Then, without any experiments on the studied settings and designed algorithms, the authors directly come to a conclusion --- the reason that human feedback works well in practical LLMs is probably due to less bias. I do not think the theoretical results in this paper prove (or provide sufficient supports for) this conclusion.\n\nIn my opinion, the theoretical analysis in this paper is standard in the bandit and offline RL literature. The authors replied that the main purpose of this paper is to propose a theoretical explanation for the empirical phenomenon. However, with the current theoretical results under the restrictive and biased settings and assumptions, and without any experiments to connect with their settings and algorithms, I do not think the presented theory can effectively explain why human feedback works well in practical LLMs.\n\nCurrently I tend to keep my score 3, and will listen to the opinions of other reviewers and AC.\n\nQuestions: Please see the weaknesses above.", "labeling_timestamp": "2026-01-11T16:44:44.219247", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly claims novel theoretical contributions and new models (e.g., a new monotone rating model and \"the first known suboptimality analysis\" for rating-based reward engineering), which contradicts the reviewer's assertion that the analysis is merely standard and thus undermines novelty. These claims appear in the Abstract and Introduction where the authors frame their results as new compared to prior work.", "evidence": "Furthermore, we provide the first known suboptimality analysis for reward engineering with human rating in bandit problems and shed light on how human bias and uncertainty can adversely impact policy learning.", "section": "1 INTRODUCTION"}
{"claim": "Theoretical results rely on restrictive and biased settings and assumptions, limiting their relevance to practical human feedback scenarios.", "claim_type": "methodology", "paper_id": "XmkuQfWZAB", "paper_title": "On Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "pg25m3wQGF", "reviewer": "Reviewer_5Lzq", "review_text": "Summary: This paper develops a theoretical comparison between these human feedback approaches in offline contextual bandits and shows how human bias and uncertainty in feedback modeling can affect the theoretical guarantees of these approaches. The proposed results seek to provide a theoretical explanation for the empirical successes of preference-based methods from a modeling perspective.\n\nStrengths: 1.\tThe studied problem, i.e., contextual bandits with human feedback, is very well-motivated and finds important applications such as large language models.\n2.\tThe authors propose algorithms based on pessimism with suboptimality guarantees.\n\nWeaknesses: 1.\tIt seems that the proposed algorithms are designed based on standard techniques, such as pessimism and MLE. The authors should elaborate more on their technical novelty. This is my main concern.\n2.\tIt would be more clear to present conditions 1, 2 and 3 as assumptions. The authors should justify more on these assumptions. For example, why is condition 1 reasonable? Why the noise never changes the human preference? \n3.\tIn Theorem 1, the setup $C^*=2$ seems too specific. Can the result be extended to the one that allows general $C^*$ and depends on $C^*$?\n\n**-------After Rebuttal-------**\n\nThank the authors for their rebuttal. I read the authors' response and other reviewers' comments. \n\nI agree with the comments of Reviewer jnsn, i.e., the logic of this paper is not very reasonable. Specifically, the authors prove that the LCB algorithm with *biased* rating feedback is not sample efficient in Section 4, the pessimistic MLE algorithm with *unbiased* human feedback is sample efficient in Section 5.1, and furthermore, the pessimistic MLE algorithm with *biased* human feedback is also not sample efficient in Section 5.2. Then, without any experiments on the studied settings and designed algorithms, the authors directly come to a conclusion --- the reason that human feedback works well in practical LLMs is probably due to less bias. I do not think the theoretical results in this paper prove (or provide sufficient supports for) this conclusion.\n\nIn my opinion, the theoretical analysis in this paper is standard in the bandit and offline RL literature. The authors replied that the main purpose of this paper is to propose a theoretical explanation for the empirical phenomenon. However, with the current theoretical results under the restrictive and biased settings and assumptions, and without any experiments to connect with their settings and algorithms, I do not think the presented theory can effectively explain why human feedback works well in practical LLMs.\n\nCurrently I tend to keep my score 3, and will listen to the opinions of other reviewers and AC.\n\nQuestions: Please see the weaknesses above.", "labeling_timestamp": "2026-01-11T16:44:36.825338", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper does adopt restrictive technical settings (tabular, offline contextual bandits; deterministic reward) and analyzes specific feedback models, which supports the reviewer's claim that assumptions limit practical relevance. However, the paper explicitly argues that it relaxes prior restrictive rating models by introducing a more general monotone rating model that captures bias and uncertainty, and it provides comparative results for preference models too. Thus the claim is only partially true: some restrictive assumptions remain, but the authors also claim to address and generalize prior restrictive modeling of human ratings.", "evidence": "Abstract: \"To align with recent applications [13; 38], we focus on tabular contextual bandits in the offline setting.\"; Preliminaries: \"The function r : S \\times A \\to [0 \\dash R] represents the true reward function, which is assumed to be deterministic and unknown in this paper.\"; Section 3: \"our rating model is based on a general class of monotone functions that can account for both human bias and uncertainty with general forms.\"; Introduction: \"These existing models, however, fall short on two aspects: (i) they model the uncertainty noise in a simple, isolated form, and (ii) their modeling of the bias is also restrictive...\"", "section": "Abstract; 2 PRELIMINARIES; 3 HUMAN RATING MODELS; 1 INTRODUCTION"}
{"claim": "The presented theorems produce mixed outcomes that do not coherently support the paper's high-level claim about why human feedback works well in LLMs.", "claim_type": "subjective", "paper_id": "XmkuQfWZAB", "paper_title": "On Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "pg25m3wQGF", "reviewer": "Reviewer_5Lzq", "review_text": "Summary: This paper develops a theoretical comparison between these human feedback approaches in offline contextual bandits and shows how human bias and uncertainty in feedback modeling can affect the theoretical guarantees of these approaches. The proposed results seek to provide a theoretical explanation for the empirical successes of preference-based methods from a modeling perspective.\n\nStrengths: 1.\tThe studied problem, i.e., contextual bandits with human feedback, is very well-motivated and finds important applications such as large language models.\n2.\tThe authors propose algorithms based on pessimism with suboptimality guarantees.\n\nWeaknesses: 1.\tIt seems that the proposed algorithms are designed based on standard techniques, such as pessimism and MLE. The authors should elaborate more on their technical novelty. This is my main concern.\n2.\tIt would be more clear to present conditions 1, 2 and 3 as assumptions. The authors should justify more on these assumptions. For example, why is condition 1 reasonable? Why the noise never changes the human preference? \n3.\tIn Theorem 1, the setup $C^*=2$ seems too specific. Can the result be extended to the one that allows general $C^*$ and depends on $C^*$?\n\n**-------After Rebuttal-------**\n\nThank the authors for their rebuttal. I read the authors' response and other reviewers' comments. \n\nI agree with the comments of Reviewer jnsn, i.e., the logic of this paper is not very reasonable. Specifically, the authors prove that the LCB algorithm with *biased* rating feedback is not sample efficient in Section 4, the pessimistic MLE algorithm with *unbiased* human feedback is sample efficient in Section 5.1, and furthermore, the pessimistic MLE algorithm with *biased* human feedback is also not sample efficient in Section 5.2. Then, without any experiments on the studied settings and designed algorithms, the authors directly come to a conclusion --- the reason that human feedback works well in practical LLMs is probably due to less bias. I do not think the theoretical results in this paper prove (or provide sufficient supports for) this conclusion.\n\nIn my opinion, the theoretical analysis in this paper is standard in the bandit and offline RL literature. The authors replied that the main purpose of this paper is to propose a theoretical explanation for the empirical phenomenon. However, with the current theoretical results under the restrictive and biased settings and assumptions, and without any experiments to connect with their settings and algorithms, I do not think the presented theory can effectively explain why human feedback works well in practical LLMs.\n\nCurrently I tend to keep my score 3, and will listen to the opinions of other reviewers and AC.\n\nQuestions: Please see the weaknesses above.", "labeling_timestamp": "2026-01-11T16:44:53.117818", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states that its theorems yield different outcomes in different regimes and uses those results to explain when and why preference-based feedback helps. It shows preference methods have lower suboptimality under extreme rating bias, but no provable advantage when bias/uncertainty are equal—then draws the high-level conclusion that preference's empirical success is attributable to milder bias/uncertainty in practice. Thus the theorems are presented coherently to support the high-level claim.", "evidence": "1) \"The comparison reveals that the preference-based approach enjoys lower suboptimality than the rating-based approach when human bias is extreme in human rating.\" 2) \"This comparison shows when human bias and uncertainty are equally strong in both types of human feedback, the preference-based approach has no provable advantage over the rating-based one.\" 3) \"Altogether, our theory shows the advantage of the preference-based approach can be largely attributed to its modeling with mild human bias and uncertainty, which makes it reasonable to believe the great empirical success of preference-based methods is because human preference data is subject to less bias and uncertainty in practice.\"", "section": "Introduction (also summarized in Abstract)"}
{"claim": "The differences in architecture and learning algorithm between the paper's studied setting and practical deep learning are substantial and should be explicitly addressed in the main text.", "claim_type": "presentation", "paper_id": "3hcn0UxP72", "paper_title": "Topological obstruction to the training of shallow ReLU neural networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "TCqYuDFZZx", "reviewer": "Reviewer_JxYP", "review_text": "Comment: ### Framing\n\n**5. On the position of the limitations section:** I think the differences of architecture and learning algorithm between the setting studied in this paper and settings of relevance in deep learning practice are substantial and deserve to be explicitly addressed within the main text of this paper if it is to be accepted.\n\nI think the authors have acknowledged these differences and their appendix F does a good job of summarising them. I would repeat the suggestion from my review, that the authors should consider promoting this material from appendix F into the main text.\n\nSince my review the authors have also presented a page worth of additional figures, and, also accounting for the space required to describe these experiments, they may face a shortage of space if they were to attempt to put all of this material into a 10-page version of the submission along with the discussion of limitations. However, I still think it is worth considering making a prominent place for a brief explicit acknowledgement of these limitations (e.g. in a named section or subsection) because this is one of the most important considerations for readers of the submitted work to be made aware of. (Of course, the authors are welcome to use their judgement to decide how to organise the final version of the paper, if accepted.)\n\n**6. Making explicit the open question of topological obstructions:** I questioned the authors' framing around topological obstructions, given that their results imply that these appear in quite restricted circumstances. I appreciate the authors' response which was as follows:\n\n> We agree that, from this first set of results, it seems that the topological obstruction induced by symmetries is a restricted phenomenon. However, the fact that there is the possibility of a fundamental obstruction to training is not something that is commonly known or expected by the community. Therefore, we believe that the framing we gave to the paper is justified, as it serves to highlight this possibility. Moreover, while our analysis clearly identifies obstructions in some cases, it is, in principle, hard to exclude that extending the results to more general settings will not uncover other obstructions of a similar kind.\n\nI agree that the finding of topological obstructions in a restricted setting is worth presenting to the community and worth further investigation.\n\nUpon taking another glance at the paper I can't fault the authors for anything they have said. They consistently carefully qualify every claim so as to avoid overstating their results. They have been upfront about the limitations of the direct implications of their results in their paper and in their rebuttals. In my opinion, the paper is a masterclass in precise communication of theoretical results.\n\nHowever I am left with a distinct impression that while the authors have not said anything wrong, something important remains *unsaid*---namely that the central motivation of their work is the hypothesis that topological obstructions may hinder training in practical deep learning. If I am not mistaken, this is never stated in the paper, even though it has got to be the strongest reason to share this work with the NeurIPS community (right?).\n\nI think the authors don't need to be afraid to make this claim, even though the answer lies in future work. Their results and experiments have comprehensively and elegantly answered this question in the limited setting of shallow ReLU networks. More than a toy model, their analysis made to look effortless with the powerful tools of topology has cut out an example for future work on this topic to follow.\n\nI therefore ask:\n\n1. Do the authors agree that the reason people should care about their work is that topological obstructions may be relevant to deep learning with practical algorithms and architectures (or not, whatever the case may be, but it would be nice to know either way)?\n2. Are the authors willing to state their answer to question (1) prominently in their paper (for example in the introduction or conclusion, for example alongside the discussion of limitations as per my previous point)?\n\n[Part 2 of 2]", "labeling_timestamp": "2026-01-11T16:44:50.234123", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper does acknowledge key differences (it studies shallow, bias-free two-layer ReLU networks and uses continuous-time gradient flow) and briefly notes that gradient flow vs finite-step GD alters trajectories, and that biases are treated in an appendix. However, it does not provide a detailed, explicit discussion in the main text contrasting these modeling choices with typical deep architectures and discrete optimization used in practice.", "evidence": ["Abstract: \"...topological obstruction in the loss landscape of shallow ReLU neural networks trained using gradient flow.\"", "Section 3.1: \"Notice that we consider a network with no biases, as it allows us a discussion with lighter notation. The case with biases is discussed in Appendix E.\"", "Related work: \"The transition from gradient flow to finite step size gradient descent breaks the conservation laws, resulting in altered trajectories [14, 2, 27, 44].\""], "section": "Abstract; Section 3.1 (Setup and preliminaries); Section 2 (Related work)"}
{"claim": "Appendix F summarizes these differences but the authors should promote that material into the main paper rather than leaving it only in the appendix.", "claim_type": "presentation", "paper_id": "3hcn0UxP72", "paper_title": "Topological obstruction to the training of shallow ReLU neural networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "TCqYuDFZZx", "reviewer": "Reviewer_JxYP", "review_text": "Comment: ### Framing\n\n**5. On the position of the limitations section:** I think the differences of architecture and learning algorithm between the setting studied in this paper and settings of relevance in deep learning practice are substantial and deserve to be explicitly addressed within the main text of this paper if it is to be accepted.\n\nI think the authors have acknowledged these differences and their appendix F does a good job of summarising them. I would repeat the suggestion from my review, that the authors should consider promoting this material from appendix F into the main text.\n\nSince my review the authors have also presented a page worth of additional figures, and, also accounting for the space required to describe these experiments, they may face a shortage of space if they were to attempt to put all of this material into a 10-page version of the submission along with the discussion of limitations. However, I still think it is worth considering making a prominent place for a brief explicit acknowledgement of these limitations (e.g. in a named section or subsection) because this is one of the most important considerations for readers of the submitted work to be made aware of. (Of course, the authors are welcome to use their judgement to decide how to organise the final version of the paper, if accepted.)\n\n**6. Making explicit the open question of topological obstructions:** I questioned the authors' framing around topological obstructions, given that their results imply that these appear in quite restricted circumstances. I appreciate the authors' response which was as follows:\n\n> We agree that, from this first set of results, it seems that the topological obstruction induced by symmetries is a restricted phenomenon. However, the fact that there is the possibility of a fundamental obstruction to training is not something that is commonly known or expected by the community. Therefore, we believe that the framing we gave to the paper is justified, as it serves to highlight this possibility. Moreover, while our analysis clearly identifies obstructions in some cases, it is, in principle, hard to exclude that extending the results to more general settings will not uncover other obstructions of a similar kind.\n\nI agree that the finding of topological obstructions in a restricted setting is worth presenting to the community and worth further investigation.\n\nUpon taking another glance at the paper I can't fault the authors for anything they have said. They consistently carefully qualify every claim so as to avoid overstating their results. They have been upfront about the limitations of the direct implications of their results in their paper and in their rebuttals. In my opinion, the paper is a masterclass in precise communication of theoretical results.\n\nHowever I am left with a distinct impression that while the authors have not said anything wrong, something important remains *unsaid*---namely that the central motivation of their work is the hypothesis that topological obstructions may hinder training in practical deep learning. If I am not mistaken, this is never stated in the paper, even though it has got to be the strongest reason to share this work with the NeurIPS community (right?).\n\nI think the authors don't need to be afraid to make this claim, even though the answer lies in future work. Their results and experiments have comprehensively and elegantly answered this question in the limited setting of shallow ReLU networks. More than a toy model, their analysis made to look effortless with the powerful tools of topology has cut out an example for future work on this topic to follow.\n\nI therefore ask:\n\n1. Do the authors agree that the reason people should care about their work is that topological obstructions may be relevant to deep learning with practical algorithms and architectures (or not, whatever the case may be, but it would be nice to know either way)?\n2. Are the authors willing to state their answer to question (1) prominently in their paper (for example in the introduction or conclusion, for example alongside the discussion of limitations as per my previous point)?\n\n[Part 2 of 2]", "labeling_timestamp": "2026-01-11T16:45:04.166954", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper text references several appendices (A, E and an unnamed appendix for Lemma 3) but does not show or mention Appendix F or any summary of 'these differences'. There is insufficient information in the supplied content to confirm that Appendix F summarizes differences or that that material is currently only in the appendix.", "evidence": ["Details of the proof are provided in Appendix A.", "The case with biases is discussed in Appendix E.", "…thanks to Lemma 3 in the Appendix…"], "section": "Sections 3.1–3.3 (Appendix references)"}
{"claim": "The paper lacks a prominent, brief explicit acknowledgement of its limitations in the main text, for example a named section or subsection.", "claim_type": "presentation", "paper_id": "3hcn0UxP72", "paper_title": "Topological obstruction to the training of shallow ReLU neural networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "TCqYuDFZZx", "reviewer": "Reviewer_JxYP", "review_text": "Comment: ### Framing\n\n**5. On the position of the limitations section:** I think the differences of architecture and learning algorithm between the setting studied in this paper and settings of relevance in deep learning practice are substantial and deserve to be explicitly addressed within the main text of this paper if it is to be accepted.\n\nI think the authors have acknowledged these differences and their appendix F does a good job of summarising them. I would repeat the suggestion from my review, that the authors should consider promoting this material from appendix F into the main text.\n\nSince my review the authors have also presented a page worth of additional figures, and, also accounting for the space required to describe these experiments, they may face a shortage of space if they were to attempt to put all of this material into a 10-page version of the submission along with the discussion of limitations. However, I still think it is worth considering making a prominent place for a brief explicit acknowledgement of these limitations (e.g. in a named section or subsection) because this is one of the most important considerations for readers of the submitted work to be made aware of. (Of course, the authors are welcome to use their judgement to decide how to organise the final version of the paper, if accepted.)\n\n**6. Making explicit the open question of topological obstructions:** I questioned the authors' framing around topological obstructions, given that their results imply that these appear in quite restricted circumstances. I appreciate the authors' response which was as follows:\n\n> We agree that, from this first set of results, it seems that the topological obstruction induced by symmetries is a restricted phenomenon. However, the fact that there is the possibility of a fundamental obstruction to training is not something that is commonly known or expected by the community. Therefore, we believe that the framing we gave to the paper is justified, as it serves to highlight this possibility. Moreover, while our analysis clearly identifies obstructions in some cases, it is, in principle, hard to exclude that extending the results to more general settings will not uncover other obstructions of a similar kind.\n\nI agree that the finding of topological obstructions in a restricted setting is worth presenting to the community and worth further investigation.\n\nUpon taking another glance at the paper I can't fault the authors for anything they have said. They consistently carefully qualify every claim so as to avoid overstating their results. They have been upfront about the limitations of the direct implications of their results in their paper and in their rebuttals. In my opinion, the paper is a masterclass in precise communication of theoretical results.\n\nHowever I am left with a distinct impression that while the authors have not said anything wrong, something important remains *unsaid*---namely that the central motivation of their work is the hypothesis that topological obstructions may hinder training in practical deep learning. If I am not mistaken, this is never stated in the paper, even though it has got to be the strongest reason to share this work with the NeurIPS community (right?).\n\nI think the authors don't need to be afraid to make this claim, even though the answer lies in future work. Their results and experiments have comprehensively and elegantly answered this question in the limited setting of shallow ReLU networks. More than a toy model, their analysis made to look effortless with the powerful tools of topology has cut out an example for future work on this topic to follow.\n\nI therefore ask:\n\n1. Do the authors agree that the reason people should care about their work is that topological obstructions may be relevant to deep learning with practical algorithms and architectures (or not, whatever the case may be, but it would be nice to know either way)?\n2. Are the authors willing to state their answer to question (1) prominently in their paper (for example in the introduction or conclusion, for example alongside the discussion of limitations as per my previous point)?\n\n[Part 2 of 2]", "labeling_timestamp": "2026-01-11T16:44:57.280733", "model": "gpt-5-mini", "label": "Supported", "justification": "The main text states assumptions and defers discussion of extensions (e.g. biases) to an appendix, but does not contain a dedicated, prominent 'Limitations' section or subsection acknowledging limitations in brief in the main body.", "evidence": "\"The case with biases is discussed in Appendix E.\"", "section": "3.1 One-hidden layer neural network"}
{"claim": "Because the authors added additional figures, including appendix material into a 10-page submission may create a space shortage.", "claim_type": "presentation", "paper_id": "3hcn0UxP72", "paper_title": "Topological obstruction to the training of shallow ReLU neural networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "TCqYuDFZZx", "reviewer": "Reviewer_JxYP", "review_text": "Comment: ### Framing\n\n**5. On the position of the limitations section:** I think the differences of architecture and learning algorithm between the setting studied in this paper and settings of relevance in deep learning practice are substantial and deserve to be explicitly addressed within the main text of this paper if it is to be accepted.\n\nI think the authors have acknowledged these differences and their appendix F does a good job of summarising them. I would repeat the suggestion from my review, that the authors should consider promoting this material from appendix F into the main text.\n\nSince my review the authors have also presented a page worth of additional figures, and, also accounting for the space required to describe these experiments, they may face a shortage of space if they were to attempt to put all of this material into a 10-page version of the submission along with the discussion of limitations. However, I still think it is worth considering making a prominent place for a brief explicit acknowledgement of these limitations (e.g. in a named section or subsection) because this is one of the most important considerations for readers of the submitted work to be made aware of. (Of course, the authors are welcome to use their judgement to decide how to organise the final version of the paper, if accepted.)\n\n**6. Making explicit the open question of topological obstructions:** I questioned the authors' framing around topological obstructions, given that their results imply that these appear in quite restricted circumstances. I appreciate the authors' response which was as follows:\n\n> We agree that, from this first set of results, it seems that the topological obstruction induced by symmetries is a restricted phenomenon. However, the fact that there is the possibility of a fundamental obstruction to training is not something that is commonly known or expected by the community. Therefore, we believe that the framing we gave to the paper is justified, as it serves to highlight this possibility. Moreover, while our analysis clearly identifies obstructions in some cases, it is, in principle, hard to exclude that extending the results to more general settings will not uncover other obstructions of a similar kind.\n\nI agree that the finding of topological obstructions in a restricted setting is worth presenting to the community and worth further investigation.\n\nUpon taking another glance at the paper I can't fault the authors for anything they have said. They consistently carefully qualify every claim so as to avoid overstating their results. They have been upfront about the limitations of the direct implications of their results in their paper and in their rebuttals. In my opinion, the paper is a masterclass in precise communication of theoretical results.\n\nHowever I am left with a distinct impression that while the authors have not said anything wrong, something important remains *unsaid*---namely that the central motivation of their work is the hypothesis that topological obstructions may hinder training in practical deep learning. If I am not mistaken, this is never stated in the paper, even though it has got to be the strongest reason to share this work with the NeurIPS community (right?).\n\nI think the authors don't need to be afraid to make this claim, even though the answer lies in future work. Their results and experiments have comprehensively and elegantly answered this question in the limited setting of shallow ReLU networks. More than a toy model, their analysis made to look effortless with the powerful tools of topology has cut out an example for future work on this topic to follow.\n\nI therefore ask:\n\n1. Do the authors agree that the reason people should care about their work is that topological obstructions may be relevant to deep learning with practical algorithms and architectures (or not, whatever the case may be, but it would be nice to know either way)?\n2. Are the authors willing to state their answer to question (1) prominently in their paper (for example in the introduction or conclusion, for example alongside the discussion of limitations as per my previous point)?\n\n[Part 2 of 2]", "labeling_timestamp": "2026-01-11T16:45:09.662148", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does refer to figures and multiple appendices (e.g., Appendix A and E, and Figure 1), but it does not discuss submission page limits, a 10-page submission, or any space/formatting constraints. Therefore the paper does not provide information to confirm or refute the reviewer's claim about creating a space shortage.", "evidence": [{"quote": "Details of the proof are provided in Appendix A.", "section": "3.1 One-hidden layer neural network"}, {"quote": "The case with biases is discussed in Appendix E.", "section": "3.1 One-hidden layer neural network"}, {"quote": "Figure 1: a. Depiction of the two group actions acting on the space of the network's parameters: the neuron rescaling of Equation (2) (top) and the neuron permutation of Equation (4) (bottom). b. Depiction of the geometry of the parameter space induced by the rescaling invariance of ReLU networks.", "section": "3.2 Symmetries and observationally equivalent networks"}], "section": ""}
{"claim": "The paper's framing around topological obstructions is questionable because the results show obstructions only in quite restricted circumstances.", "claim_type": "experimental", "paper_id": "3hcn0UxP72", "paper_title": "Topological obstruction to the training of shallow ReLU neural networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "TCqYuDFZZx", "reviewer": "Reviewer_JxYP", "review_text": "Comment: ### Framing\n\n**5. On the position of the limitations section:** I think the differences of architecture and learning algorithm between the setting studied in this paper and settings of relevance in deep learning practice are substantial and deserve to be explicitly addressed within the main text of this paper if it is to be accepted.\n\nI think the authors have acknowledged these differences and their appendix F does a good job of summarising them. I would repeat the suggestion from my review, that the authors should consider promoting this material from appendix F into the main text.\n\nSince my review the authors have also presented a page worth of additional figures, and, also accounting for the space required to describe these experiments, they may face a shortage of space if they were to attempt to put all of this material into a 10-page version of the submission along with the discussion of limitations. However, I still think it is worth considering making a prominent place for a brief explicit acknowledgement of these limitations (e.g. in a named section or subsection) because this is one of the most important considerations for readers of the submitted work to be made aware of. (Of course, the authors are welcome to use their judgement to decide how to organise the final version of the paper, if accepted.)\n\n**6. Making explicit the open question of topological obstructions:** I questioned the authors' framing around topological obstructions, given that their results imply that these appear in quite restricted circumstances. I appreciate the authors' response which was as follows:\n\n> We agree that, from this first set of results, it seems that the topological obstruction induced by symmetries is a restricted phenomenon. However, the fact that there is the possibility of a fundamental obstruction to training is not something that is commonly known or expected by the community. Therefore, we believe that the framing we gave to the paper is justified, as it serves to highlight this possibility. Moreover, while our analysis clearly identifies obstructions in some cases, it is, in principle, hard to exclude that extending the results to more general settings will not uncover other obstructions of a similar kind.\n\nI agree that the finding of topological obstructions in a restricted setting is worth presenting to the community and worth further investigation.\n\nUpon taking another glance at the paper I can't fault the authors for anything they have said. They consistently carefully qualify every claim so as to avoid overstating their results. They have been upfront about the limitations of the direct implications of their results in their paper and in their rebuttals. In my opinion, the paper is a masterclass in precise communication of theoretical results.\n\nHowever I am left with a distinct impression that while the authors have not said anything wrong, something important remains *unsaid*---namely that the central motivation of their work is the hypothesis that topological obstructions may hinder training in practical deep learning. If I am not mistaken, this is never stated in the paper, even though it has got to be the strongest reason to share this work with the NeurIPS community (right?).\n\nI think the authors don't need to be afraid to make this claim, even though the answer lies in future work. Their results and experiments have comprehensively and elegantly answered this question in the limited setting of shallow ReLU networks. More than a toy model, their analysis made to look effortless with the powerful tools of topology has cut out an example for future work on this topic to follow.\n\nI therefore ask:\n\n1. Do the authors agree that the reason people should care about their work is that topological obstructions may be relevant to deep learning with practical algorithms and architectures (or not, whatever the case may be, but it would be nice to know either way)?\n2. Are the authors willing to state their answer to question (1) prominently in their paper (for example in the introduction or conclusion, for example alongside the discussion of limitations as per my previous point)?\n\n[Part 2 of 2]", "labeling_timestamp": "2026-01-11T16:45:23.112982", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly derives topological obstructions only for a restricted setting: two-layer (shallow) ReLU networks trained with gradient flow, focusing especially on the scalar-output case (output dimension = 1) and without biases; it also states the obstruction depends on initialization and 'pathological' neurons. Thus the reviewer's claim that obstructions appear only in quite restricted circumstances is accurate.", "evidence": "Abstract: \"When the neural network's output is a single scalar, we prove that these quadrics can have multiple connected components, limiting the set of reachable parameters during training.\"; Main contributions (3): \"We find that the invariant set can be disconnected when the network's output dimension is 1, leading to a clear topological obstruction.\"; Main contributions (4): \"We find that the obstruction is caused by 'pathological' neurons that cannot change the sign of their output weights when trained with gradient flow.\"; Section 3.1: \"Notice that we consider a network with no biases, as it allows us a discussion with lighter notation.\"; Section 3.3: \"In this work ... we consider the continuous time version of the gradient descent (GD) algorithm ... named gradient flow (GF)\".", "section": "Abstract; Main contributions (Section 1); Setup and preliminaries (Section 3.1); Conserved quantities and the invariant hyperquadrics (Section 3.3)"}
{"claim": "The authors do not explicitly state the central motivation that topological obstructions may hinder training in practical deep learning.", "claim_type": "presentation", "paper_id": "3hcn0UxP72", "paper_title": "Topological obstruction to the training of shallow ReLU neural networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "TCqYuDFZZx", "reviewer": "Reviewer_JxYP", "review_text": "Comment: ### Framing\n\n**5. On the position of the limitations section:** I think the differences of architecture and learning algorithm between the setting studied in this paper and settings of relevance in deep learning practice are substantial and deserve to be explicitly addressed within the main text of this paper if it is to be accepted.\n\nI think the authors have acknowledged these differences and their appendix F does a good job of summarising them. I would repeat the suggestion from my review, that the authors should consider promoting this material from appendix F into the main text.\n\nSince my review the authors have also presented a page worth of additional figures, and, also accounting for the space required to describe these experiments, they may face a shortage of space if they were to attempt to put all of this material into a 10-page version of the submission along with the discussion of limitations. However, I still think it is worth considering making a prominent place for a brief explicit acknowledgement of these limitations (e.g. in a named section or subsection) because this is one of the most important considerations for readers of the submitted work to be made aware of. (Of course, the authors are welcome to use their judgement to decide how to organise the final version of the paper, if accepted.)\n\n**6. Making explicit the open question of topological obstructions:** I questioned the authors' framing around topological obstructions, given that their results imply that these appear in quite restricted circumstances. I appreciate the authors' response which was as follows:\n\n> We agree that, from this first set of results, it seems that the topological obstruction induced by symmetries is a restricted phenomenon. However, the fact that there is the possibility of a fundamental obstruction to training is not something that is commonly known or expected by the community. Therefore, we believe that the framing we gave to the paper is justified, as it serves to highlight this possibility. Moreover, while our analysis clearly identifies obstructions in some cases, it is, in principle, hard to exclude that extending the results to more general settings will not uncover other obstructions of a similar kind.\n\nI agree that the finding of topological obstructions in a restricted setting is worth presenting to the community and worth further investigation.\n\nUpon taking another glance at the paper I can't fault the authors for anything they have said. They consistently carefully qualify every claim so as to avoid overstating their results. They have been upfront about the limitations of the direct implications of their results in their paper and in their rebuttals. In my opinion, the paper is a masterclass in precise communication of theoretical results.\n\nHowever I am left with a distinct impression that while the authors have not said anything wrong, something important remains *unsaid*---namely that the central motivation of their work is the hypothesis that topological obstructions may hinder training in practical deep learning. If I am not mistaken, this is never stated in the paper, even though it has got to be the strongest reason to share this work with the NeurIPS community (right?).\n\nI think the authors don't need to be afraid to make this claim, even though the answer lies in future work. Their results and experiments have comprehensively and elegantly answered this question in the limited setting of shallow ReLU networks. More than a toy model, their analysis made to look effortless with the powerful tools of topology has cut out an example for future work on this topic to follow.\n\nI therefore ask:\n\n1. Do the authors agree that the reason people should care about their work is that topological obstructions may be relevant to deep learning with practical algorithms and architectures (or not, whatever the case may be, but it would be nice to know either way)?\n2. Are the authors willing to state their answer to question (1) prominently in their paper (for example in the introduction or conclusion, for example alongside the discussion of limitations as per my previous point)?\n\n[Part 2 of 2]", "labeling_timestamp": "2026-01-11T16:45:27.704881", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states the motivation to identify and quantify topological obstructions that can prevent training from reaching an optimal parameter; it claims such obstructions can make the global optimum unreachable and frames this as the main goal of the work.", "evidence": "In the Abstract: \"In this simple setting, we find that the non-connectedness results in a topological obstruction, which, depending on the initialization, can make the global optimum unreachable.\"  In the Introduction: \"Our main goal is to find and quantify in topological terms the impossibility of the training trajectories to freely explore the parameter space and get from any initialization to an optimal parameter.\"  Main contributions (point 3): \"We find that the invariant set can be disconnected when the network's output dimension is 1, leading to a clear topological obstruction.\"", "section": "Abstract; Introduction (Main contributions)"}
{"claim": "The paper's results identify obstructions only for shallow ReLU networks, limiting direct implications for more general architectures and practical algorithms.", "claim_type": "methodology", "paper_id": "3hcn0UxP72", "paper_title": "Topological obstruction to the training of shallow ReLU neural networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "TCqYuDFZZx", "reviewer": "Reviewer_JxYP", "review_text": "Comment: ### Framing\n\n**5. On the position of the limitations section:** I think the differences of architecture and learning algorithm between the setting studied in this paper and settings of relevance in deep learning practice are substantial and deserve to be explicitly addressed within the main text of this paper if it is to be accepted.\n\nI think the authors have acknowledged these differences and their appendix F does a good job of summarising them. I would repeat the suggestion from my review, that the authors should consider promoting this material from appendix F into the main text.\n\nSince my review the authors have also presented a page worth of additional figures, and, also accounting for the space required to describe these experiments, they may face a shortage of space if they were to attempt to put all of this material into a 10-page version of the submission along with the discussion of limitations. However, I still think it is worth considering making a prominent place for a brief explicit acknowledgement of these limitations (e.g. in a named section or subsection) because this is one of the most important considerations for readers of the submitted work to be made aware of. (Of course, the authors are welcome to use their judgement to decide how to organise the final version of the paper, if accepted.)\n\n**6. Making explicit the open question of topological obstructions:** I questioned the authors' framing around topological obstructions, given that their results imply that these appear in quite restricted circumstances. I appreciate the authors' response which was as follows:\n\n> We agree that, from this first set of results, it seems that the topological obstruction induced by symmetries is a restricted phenomenon. However, the fact that there is the possibility of a fundamental obstruction to training is not something that is commonly known or expected by the community. Therefore, we believe that the framing we gave to the paper is justified, as it serves to highlight this possibility. Moreover, while our analysis clearly identifies obstructions in some cases, it is, in principle, hard to exclude that extending the results to more general settings will not uncover other obstructions of a similar kind.\n\nI agree that the finding of topological obstructions in a restricted setting is worth presenting to the community and worth further investigation.\n\nUpon taking another glance at the paper I can't fault the authors for anything they have said. They consistently carefully qualify every claim so as to avoid overstating their results. They have been upfront about the limitations of the direct implications of their results in their paper and in their rebuttals. In my opinion, the paper is a masterclass in precise communication of theoretical results.\n\nHowever I am left with a distinct impression that while the authors have not said anything wrong, something important remains *unsaid*---namely that the central motivation of their work is the hypothesis that topological obstructions may hinder training in practical deep learning. If I am not mistaken, this is never stated in the paper, even though it has got to be the strongest reason to share this work with the NeurIPS community (right?).\n\nI think the authors don't need to be afraid to make this claim, even though the answer lies in future work. Their results and experiments have comprehensively and elegantly answered this question in the limited setting of shallow ReLU networks. More than a toy model, their analysis made to look effortless with the powerful tools of topology has cut out an example for future work on this topic to follow.\n\nI therefore ask:\n\n1. Do the authors agree that the reason people should care about their work is that topological obstructions may be relevant to deep learning with practical algorithms and architectures (or not, whatever the case may be, but it would be nice to know either way)?\n2. Are the authors willing to state their answer to question (1) prominently in their paper (for example in the introduction or conclusion, for example alongside the discussion of limitations as per my previous point)?\n\n[Part 2 of 2]", "labeling_timestamp": "2026-01-11T16:45:33.070909", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly studies two-layer (shallow) ReLU networks trained with gradient flow and states the key decomposition/result only holds for two-layer networks; it also notes that gradient flow conservation laws do not necessarily hold for finite-step gradient descent, indicating limited direct implications for deeper architectures and practical algorithms.", "evidence": ["Abstract: \"This paper reveals the presence of topological obstruction in the loss landscape of shallow ReLU neural networks trained using gradient flow.\"", "Section 3.1 (Proposition 1): \"Interestingly, the decomposition of Proposition 1 only holds for two-layer neural networks and will be crucial to the formulations of this paper's results.\"", "Related work / limitations: \"The transition from gradient flow to finite step size gradient descent breaks the conservation laws, resulting in altered trajectories [14, 2, 27, 44].\"", "Main contributions (3): \"We find that the invariant set can be disconnected when the network's output dimension is 1, leading to a clear topological obstruction.\""], "section": "Abstract; Section 3.1 (Proposition 1); Related work"}
{"claim": "The paper does not answer whether topological obstructions are relevant to deep learning with practical algorithms and architectures.", "claim_type": "experimental", "paper_id": "3hcn0UxP72", "paper_title": "Topological obstruction to the training of shallow ReLU neural networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "TCqYuDFZZx", "reviewer": "Reviewer_JxYP", "review_text": "Comment: ### Framing\n\n**5. On the position of the limitations section:** I think the differences of architecture and learning algorithm between the setting studied in this paper and settings of relevance in deep learning practice are substantial and deserve to be explicitly addressed within the main text of this paper if it is to be accepted.\n\nI think the authors have acknowledged these differences and their appendix F does a good job of summarising them. I would repeat the suggestion from my review, that the authors should consider promoting this material from appendix F into the main text.\n\nSince my review the authors have also presented a page worth of additional figures, and, also accounting for the space required to describe these experiments, they may face a shortage of space if they were to attempt to put all of this material into a 10-page version of the submission along with the discussion of limitations. However, I still think it is worth considering making a prominent place for a brief explicit acknowledgement of these limitations (e.g. in a named section or subsection) because this is one of the most important considerations for readers of the submitted work to be made aware of. (Of course, the authors are welcome to use their judgement to decide how to organise the final version of the paper, if accepted.)\n\n**6. Making explicit the open question of topological obstructions:** I questioned the authors' framing around topological obstructions, given that their results imply that these appear in quite restricted circumstances. I appreciate the authors' response which was as follows:\n\n> We agree that, from this first set of results, it seems that the topological obstruction induced by symmetries is a restricted phenomenon. However, the fact that there is the possibility of a fundamental obstruction to training is not something that is commonly known or expected by the community. Therefore, we believe that the framing we gave to the paper is justified, as it serves to highlight this possibility. Moreover, while our analysis clearly identifies obstructions in some cases, it is, in principle, hard to exclude that extending the results to more general settings will not uncover other obstructions of a similar kind.\n\nI agree that the finding of topological obstructions in a restricted setting is worth presenting to the community and worth further investigation.\n\nUpon taking another glance at the paper I can't fault the authors for anything they have said. They consistently carefully qualify every claim so as to avoid overstating their results. They have been upfront about the limitations of the direct implications of their results in their paper and in their rebuttals. In my opinion, the paper is a masterclass in precise communication of theoretical results.\n\nHowever I am left with a distinct impression that while the authors have not said anything wrong, something important remains *unsaid*---namely that the central motivation of their work is the hypothesis that topological obstructions may hinder training in practical deep learning. If I am not mistaken, this is never stated in the paper, even though it has got to be the strongest reason to share this work with the NeurIPS community (right?).\n\nI think the authors don't need to be afraid to make this claim, even though the answer lies in future work. Their results and experiments have comprehensively and elegantly answered this question in the limited setting of shallow ReLU networks. More than a toy model, their analysis made to look effortless with the powerful tools of topology has cut out an example for future work on this topic to follow.\n\nI therefore ask:\n\n1. Do the authors agree that the reason people should care about their work is that topological obstructions may be relevant to deep learning with practical algorithms and architectures (or not, whatever the case may be, but it would be nice to know either way)?\n2. Are the authors willing to state their answer to question (1) prominently in their paper (for example in the introduction or conclusion, for example alongside the discussion of limitations as per my previous point)?\n\n[Part 2 of 2]", "labeling_timestamp": "2026-01-11T16:45:28.531488", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper studies topological obstructions only for shallow (two-layer) ReLU networks trained with continuous-time gradient flow and validates results on controlled toy scenarios; it does not analyze deeper networks or practical discrete optimization algorithms, so it does not answer relevance to practical deep-learning architectures/algorithms.", "evidence": "Abstract: \"topological obstruction in the loss landscape of shallow ReLU neural networks trained using gradient flow.\"; Main contributions: \"1. We find that, for two-layer neural networks, the gradient flow trajectories lie on an invariant set...\" and \"6. We perform numerical validations on controlled toy scenarios, displaying the effect of obstruction in practice.\"; Related work: \"The transition from gradient flow to finite step size gradient descent breaks the conservation laws, resulting in altered trajectories [14, 2, 27, 44].\"", "section": "Abstract; Main contributions (Introduction); Related work; Section 3 (Setup and preliminaries)"}
{"claim": "The paper does not prominently state the authors' stance on whether topological obstructions are relevant; this should appear in the introduction or conclusion.", "claim_type": "presentation", "paper_id": "3hcn0UxP72", "paper_title": "Topological obstruction to the training of shallow ReLU neural networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "TCqYuDFZZx", "reviewer": "Reviewer_JxYP", "review_text": "Comment: ### Framing\n\n**5. On the position of the limitations section:** I think the differences of architecture and learning algorithm between the setting studied in this paper and settings of relevance in deep learning practice are substantial and deserve to be explicitly addressed within the main text of this paper if it is to be accepted.\n\nI think the authors have acknowledged these differences and their appendix F does a good job of summarising them. I would repeat the suggestion from my review, that the authors should consider promoting this material from appendix F into the main text.\n\nSince my review the authors have also presented a page worth of additional figures, and, also accounting for the space required to describe these experiments, they may face a shortage of space if they were to attempt to put all of this material into a 10-page version of the submission along with the discussion of limitations. However, I still think it is worth considering making a prominent place for a brief explicit acknowledgement of these limitations (e.g. in a named section or subsection) because this is one of the most important considerations for readers of the submitted work to be made aware of. (Of course, the authors are welcome to use their judgement to decide how to organise the final version of the paper, if accepted.)\n\n**6. Making explicit the open question of topological obstructions:** I questioned the authors' framing around topological obstructions, given that their results imply that these appear in quite restricted circumstances. I appreciate the authors' response which was as follows:\n\n> We agree that, from this first set of results, it seems that the topological obstruction induced by symmetries is a restricted phenomenon. However, the fact that there is the possibility of a fundamental obstruction to training is not something that is commonly known or expected by the community. Therefore, we believe that the framing we gave to the paper is justified, as it serves to highlight this possibility. Moreover, while our analysis clearly identifies obstructions in some cases, it is, in principle, hard to exclude that extending the results to more general settings will not uncover other obstructions of a similar kind.\n\nI agree that the finding of topological obstructions in a restricted setting is worth presenting to the community and worth further investigation.\n\nUpon taking another glance at the paper I can't fault the authors for anything they have said. They consistently carefully qualify every claim so as to avoid overstating their results. They have been upfront about the limitations of the direct implications of their results in their paper and in their rebuttals. In my opinion, the paper is a masterclass in precise communication of theoretical results.\n\nHowever I am left with a distinct impression that while the authors have not said anything wrong, something important remains *unsaid*---namely that the central motivation of their work is the hypothesis that topological obstructions may hinder training in practical deep learning. If I am not mistaken, this is never stated in the paper, even though it has got to be the strongest reason to share this work with the NeurIPS community (right?).\n\nI think the authors don't need to be afraid to make this claim, even though the answer lies in future work. Their results and experiments have comprehensively and elegantly answered this question in the limited setting of shallow ReLU networks. More than a toy model, their analysis made to look effortless with the powerful tools of topology has cut out an example for future work on this topic to follow.\n\nI therefore ask:\n\n1. Do the authors agree that the reason people should care about their work is that topological obstructions may be relevant to deep learning with practical algorithms and architectures (or not, whatever the case may be, but it would be nice to know either way)?\n2. Are the authors willing to state their answer to question (1) prominently in their paper (for example in the introduction or conclusion, for example alongside the discussion of limitations as per my previous point)?\n\n[Part 2 of 2]", "labeling_timestamp": "2026-01-11T16:45:36.753937", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly and prominently states that topological obstructions exist and can be relevant (e.g., can make the global optimum unreachable). This is asserted in the Abstract and reiterated in the Introduction and Main contributions.", "evidence": "Abstract: 'This paper reveals the presence of topological obstruction in the loss landscape of shallow ReLU neural networks trained using gradient flow.'\nMain contributions (point 3): 'We find that the invariant set can be disconnected when the network's output dimension is 1, leading to a clear topological obstruction.'\nAbstract (later): '...we find that the non-connectedness results in a topological obstruction, which, depending on the initialization, can make the global optimum unreachable.'", "section": "Abstract; Introduction (Main contributions)"}
{"claim": "The paper reports higher performance than transformer-based methods without explaining why.", "claim_type": "baseline", "paper_id": "ehfCxpDsrw", "paper_title": "LinNet: Linear Network for Efficient Point Cloud Representation Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "V28l1766Ei", "reviewer": "Reviewer_aBR6", "review_text": "Summary: In this work, the authors propose an efficient learning framework for point cloud representation learning. For the computational intensive local aggregation operation, this work proposes Disassembled set abstraction (DSA) to aggregate local features in terms of the spatial distributions of points in a simple and efficient manner. This work also proposes a Linearization sampling strategy and hash query operation to accelerate the sampling and neighbor searching processes. Experiments on classification and semantic segmentation demonstrate the effectiveness of the proposed method.\n\nStrengths: 1. The proposed local aggregation operation seems to be simple, efficient, and effective;\n\n2. The point searching strategy including the Linearization sampling strategy and hash query can indeed improve the effciency while keeping the overall performances;\n\nWeaknesses: My major concern about this work is the higher performances than transformer-based methods. It may be a little hard to understand why the proposed DSA and hash-based searching operations can improve the performances so greatly. Because these operations are more like the approximation of corresponding operations in PointNet++.\n\nQuestions: 1. Why do we introduce batch normalization to the aggregated features in Eq.4? As the addition of features and neighborhood features have already introduce the spatial characteristics, I am not sure if this BN is necessary here, or other simple components may also work;\n\n2. In the Hash query part, will the points in a same local grid share the same neighbors? I do not quite get the calculation of complexity for each point.\n\n3. Is the hash grid is pre-constructed before training? Or created repeatedly during training?\n\n3. From the results in Table 1 and Table 3, the proposed method even outperforms transformer-based methods in a more efficiency way. Could the author analyze the reasons behind this? As the improvements of this framework seem to be efficient simplification of existing PointNet++ framework, I am curious why it can improve the performances so obviously.\n\nPlease check the grammar also, e.g., In Line 101, the $x_i$ might actually be $p_i$;", "labeling_timestamp": "2026-01-11T16:45:47.157253", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper does report outperforming prior methods (including attention-based approaches) but it also provides explicit explanations for these gains: it introduces the Disassembled Set Abstraction (DSA) module, describes the linearization sampling and hash-query strategy mapping 3D points to 1D space-filling curves for linear-time search, and discusses weight initialization benefits. Thus the claim that the paper reports higher performance without explaining why is contradicted.", "evidence": "1) \"LinNet, as a purely point-based method, outperforms most previous methods in both indoor and outdoor scenes without any extra attention, and sparse convolution but merely relying on a simple MLP.\"  (Abstract)\n2) \"we first propose the disassembled set abstraction (DSA) module...\" and \"by mapping 3D point clouds onto 1D space-filling curves, we enable parallelization of downsampling and neighborhood queries on GPUs with linear complexity.\" (Introduction)\n3) \"In this section, we introduce the DSA modules...\" and detailed discussion of separating spatial and channel anisotropy and weight initialization. (Section 3.2 Disassembled Set Abstraction)", "section": "Abstract; Introduction; 3.2 Disassembled Set Abstraction"}
{"claim": "The paper does not explain why DSA and hash-based searching improve performance significantly.", "claim_type": "methodology", "paper_id": "ehfCxpDsrw", "paper_title": "LinNet: Linear Network for Efficient Point Cloud Representation Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "V28l1766Ei", "reviewer": "Reviewer_aBR6", "review_text": "Summary: In this work, the authors propose an efficient learning framework for point cloud representation learning. For the computational intensive local aggregation operation, this work proposes Disassembled set abstraction (DSA) to aggregate local features in terms of the spatial distributions of points in a simple and efficient manner. This work also proposes a Linearization sampling strategy and hash query operation to accelerate the sampling and neighbor searching processes. Experiments on classification and semantic segmentation demonstrate the effectiveness of the proposed method.\n\nStrengths: 1. The proposed local aggregation operation seems to be simple, efficient, and effective;\n\n2. The point searching strategy including the Linearization sampling strategy and hash query can indeed improve the effciency while keeping the overall performances;\n\nWeaknesses: My major concern about this work is the higher performances than transformer-based methods. It may be a little hard to understand why the proposed DSA and hash-based searching operations can improve the performances so greatly. Because these operations are more like the approximation of corresponding operations in PointNet++.\n\nQuestions: 1. Why do we introduce batch normalization to the aggregated features in Eq.4? As the addition of features and neighborhood features have already introduce the spatial characteristics, I am not sure if this BN is necessary here, or other simple components may also work;\n\n2. In the Hash query part, will the points in a same local grid share the same neighbors? I do not quite get the calculation of complexity for each point.\n\n3. Is the hash grid is pre-constructed before training? Or created repeatedly during training?\n\n3. From the results in Table 1 and Table 3, the proposed method even outperforms transformer-based methods in a more efficiency way. Could the author analyze the reasons behind this? As the improvements of this framework seem to be efficient simplification of existing PointNet++ framework, I am curious why it can improve the performances so obviously.\n\nPlease check the grammar also, e.g., In Line 101, the $x_i$ might actually be $p_i$;", "labeling_timestamp": "2026-01-11T16:46:00.882231", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly explains why DSA improves effectiveness (separating spatial and channel anisotropy, assigning geometric features as biases, and reducing FLOPs and presenting weight-initialization benefits) and why the hash/linearization search speeds up computation (mapping to 1D space-filling curves, bucketed hash queries, reducing search range to linear complexity and enabling GPU parallelism). Thus the claim that the paper does not explain these improvements is false.", "evidence": "DSA: \"we first use two independent MLPs to separately learn depth-wise geometric features of the neighborhood and point-wise semantic features. Then, the geometric features are assigned as biases to the queried neighbors' semantic features, achieving spatially anisotropic neighborhood aggregation. Since the learning of high-dimensional semantic features is point-wise and does not involve the neighborhood, the required floating-point operations (FLOPs) are significantly lower than those needed for SA.\" (Introduction / 3.2 Disassembled Set Abstraction)\n\nHash-based searching / linearization: \"by mapping 3D point clouds onto 1D space-filling curves, we enable parallelization of downsampling and neighborhood queries on GPUs with linear complexity... For neighborhood queries, we store each segmented curve as a bucket in a hash table. When querying the neighborhood, we only need to search in the buckets corresponding to the neighboring curves, which drastically cuts down the search range... The method reduces the time complexity to be linear and supports GPU parallelism, resulting in very fast sampling.\" (Introduction)", "section": "Introduction; 3.2 Disassembled Set Abstraction"}
{"claim": "The proposed DSA and hash-based operations are essentially approximations of PointNet++ operations, raising doubt about their claimed performance gains.", "claim_type": "methodology", "paper_id": "ehfCxpDsrw", "paper_title": "LinNet: Linear Network for Efficient Point Cloud Representation Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "V28l1766Ei", "reviewer": "Reviewer_aBR6", "review_text": "Summary: In this work, the authors propose an efficient learning framework for point cloud representation learning. For the computational intensive local aggregation operation, this work proposes Disassembled set abstraction (DSA) to aggregate local features in terms of the spatial distributions of points in a simple and efficient manner. This work also proposes a Linearization sampling strategy and hash query operation to accelerate the sampling and neighbor searching processes. Experiments on classification and semantic segmentation demonstrate the effectiveness of the proposed method.\n\nStrengths: 1. The proposed local aggregation operation seems to be simple, efficient, and effective;\n\n2. The point searching strategy including the Linearization sampling strategy and hash query can indeed improve the effciency while keeping the overall performances;\n\nWeaknesses: My major concern about this work is the higher performances than transformer-based methods. It may be a little hard to understand why the proposed DSA and hash-based searching operations can improve the performances so greatly. Because these operations are more like the approximation of corresponding operations in PointNet++.\n\nQuestions: 1. Why do we introduce batch normalization to the aggregated features in Eq.4? As the addition of features and neighborhood features have already introduce the spatial characteristics, I am not sure if this BN is necessary here, or other simple components may also work;\n\n2. In the Hash query part, will the points in a same local grid share the same neighbors? I do not quite get the calculation of complexity for each point.\n\n3. Is the hash grid is pre-constructed before training? Or created repeatedly during training?\n\n3. From the results in Table 1 and Table 3, the proposed method even outperforms transformer-based methods in a more efficiency way. Could the author analyze the reasons behind this? As the improvements of this framework seem to be efficient simplification of existing PointNet++ framework, I am curious why it can improve the performances so obviously.\n\nPlease check the grammar also, e.g., In Line 101, the $x_i$ might actually be $p_i$;", "labeling_timestamp": "2026-01-11T16:46:02.823186", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper describes DSA and the hash/linearization search as novel, structurally different mechanisms from PointNet++ set abstraction (SA) and FPS/KNN: DSA separates channel and spatial anisotropy with independent MLPs and assigns geometric features as biases (reducing FLOPs), and the hash + space-filling-curve linearization is presented as a distinct, linear-complexity GPU-parallel neighborhood/query strategy rather than merely an approximation of PointNet++ operations.", "evidence": ["\"we first propose the disassembled set abstraction (DSA) module, which is more effective than the previous version of set abstraction.\"", "\"we first use two independent MLPs to separately learn depth-wise geometric features of the neighborhood and point-wise semantic features. Then, the geometric features are assigned as biases to the queried neighbors' semantic features, achieving spatially anisotropic neighborhood aggregation.\"", "\"the required floating-point operations (FLOPs) are significantly lower than those needed for SA.\"", "\"a hash query and linearization sampling strategy are proposed for speeding up point searching. The core of our method is to map the 3D search space onto a segmented curve for acceleration.\"", "\"When querying the neighborhood, we only need to search in the buckets corresponding to the neighboring curves, which drastically cuts down the search range. Linear sampling ensures uniform sampling by taking the point closest to the origin within each grid as the new sampling point. The method reduces the time complexity to be linear and supports GPU parallelism, resulting in very fast sampling.\"", "\"After separation, this complexity is reduced to c^2.\""], "section": "Abstract; Introduction; 3.2 Disassembled Set Abstraction"}
{"claim": "The paper uses batch normalization after aggregation in Eq.4 without justification.", "claim_type": "methodology", "paper_id": "ehfCxpDsrw", "paper_title": "LinNet: Linear Network for Efficient Point Cloud Representation Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "V28l1766Ei", "reviewer": "Reviewer_aBR6", "review_text": "Summary: In this work, the authors propose an efficient learning framework for point cloud representation learning. For the computational intensive local aggregation operation, this work proposes Disassembled set abstraction (DSA) to aggregate local features in terms of the spatial distributions of points in a simple and efficient manner. This work also proposes a Linearization sampling strategy and hash query operation to accelerate the sampling and neighbor searching processes. Experiments on classification and semantic segmentation demonstrate the effectiveness of the proposed method.\n\nStrengths: 1. The proposed local aggregation operation seems to be simple, efficient, and effective;\n\n2. The point searching strategy including the Linearization sampling strategy and hash query can indeed improve the effciency while keeping the overall performances;\n\nWeaknesses: My major concern about this work is the higher performances than transformer-based methods. It may be a little hard to understand why the proposed DSA and hash-based searching operations can improve the performances so greatly. Because these operations are more like the approximation of corresponding operations in PointNet++.\n\nQuestions: 1. Why do we introduce batch normalization to the aggregated features in Eq.4? As the addition of features and neighborhood features have already introduce the spatial characteristics, I am not sure if this BN is necessary here, or other simple components may also work;\n\n2. In the Hash query part, will the points in a same local grid share the same neighbors? I do not quite get the calculation of complexity for each point.\n\n3. Is the hash grid is pre-constructed before training? Or created repeatedly during training?\n\n3. From the results in Table 1 and Table 3, the proposed method even outperforms transformer-based methods in a more efficiency way. Could the author analyze the reasons behind this? As the improvements of this framework seem to be efficient simplification of existing PointNet++ framework, I am curious why it can improve the performances so obviously.\n\nPlease check the grammar also, e.g., In Line 101, the $x_i$ might actually be $p_i$;", "labeling_timestamp": "2026-01-11T16:45:56.735652", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper states that the MLP used for local aggregation (PWConv) includes a batch normalization layer, but it does not provide a specific justification for using batch normalization after the aggregation operation (no discussion or rationale for this design choice is given in the cited section). The claim's precise wording about \"after aggregation in Eq.4\" is more specific than the text explicitly shows, so the reviewer claim is only partially supported by the paper content.", "evidence": "\"PWConv 3+ c ↦→ c : R 3+ c ↦→ R c is an MLP that consists of pointwise convolution, batch normalization layer, and ReLU activation function.\"", "section": "3.2 Disassembled Set Abstraction"}
{"claim": "Batch normalization may be unnecessary because addition of features and neighborhood features already introduces spatial characteristics.", "claim_type": "methodology", "paper_id": "ehfCxpDsrw", "paper_title": "LinNet: Linear Network for Efficient Point Cloud Representation Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "V28l1766Ei", "reviewer": "Reviewer_aBR6", "review_text": "Summary: In this work, the authors propose an efficient learning framework for point cloud representation learning. For the computational intensive local aggregation operation, this work proposes Disassembled set abstraction (DSA) to aggregate local features in terms of the spatial distributions of points in a simple and efficient manner. This work also proposes a Linearization sampling strategy and hash query operation to accelerate the sampling and neighbor searching processes. Experiments on classification and semantic segmentation demonstrate the effectiveness of the proposed method.\n\nStrengths: 1. The proposed local aggregation operation seems to be simple, efficient, and effective;\n\n2. The point searching strategy including the Linearization sampling strategy and hash query can indeed improve the effciency while keeping the overall performances;\n\nWeaknesses: My major concern about this work is the higher performances than transformer-based methods. It may be a little hard to understand why the proposed DSA and hash-based searching operations can improve the performances so greatly. Because these operations are more like the approximation of corresponding operations in PointNet++.\n\nQuestions: 1. Why do we introduce batch normalization to the aggregated features in Eq.4? As the addition of features and neighborhood features have already introduce the spatial characteristics, I am not sure if this BN is necessary here, or other simple components may also work;\n\n2. In the Hash query part, will the points in a same local grid share the same neighbors? I do not quite get the calculation of complexity for each point.\n\n3. Is the hash grid is pre-constructed before training? Or created repeatedly during training?\n\n3. From the results in Table 1 and Table 3, the proposed method even outperforms transformer-based methods in a more efficiency way. Could the author analyze the reasons behind this? As the improvements of this framework seem to be efficient simplification of existing PointNet++ framework, I am curious why it can improve the performances so obviously.\n\nPlease check the grammar also, e.g., In Line 101, the $x_i$ might actually be $p_i$;", "labeling_timestamp": "2026-01-11T16:46:01.347448", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does not state that batch normalization is unnecessary. It explicitly includes a batch normalization layer as part of PWConv in its feature extractor and does not discuss removing batch normalization or claim that adding features and neighborhood features makes batch normalization redundant.", "evidence": "\"PWConv ... is an MLP that consists of pointwise convolution, batch normalization layer, and ReLU activation function.\"", "section": "3.2 Disassembled Set Abstraction"}
{"claim": "The authors do not evaluate simpler alternatives to batch normalization in the aggregation step.", "claim_type": "experimental", "paper_id": "ehfCxpDsrw", "paper_title": "LinNet: Linear Network for Efficient Point Cloud Representation Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "V28l1766Ei", "reviewer": "Reviewer_aBR6", "review_text": "Summary: In this work, the authors propose an efficient learning framework for point cloud representation learning. For the computational intensive local aggregation operation, this work proposes Disassembled set abstraction (DSA) to aggregate local features in terms of the spatial distributions of points in a simple and efficient manner. This work also proposes a Linearization sampling strategy and hash query operation to accelerate the sampling and neighbor searching processes. Experiments on classification and semantic segmentation demonstrate the effectiveness of the proposed method.\n\nStrengths: 1. The proposed local aggregation operation seems to be simple, efficient, and effective;\n\n2. The point searching strategy including the Linearization sampling strategy and hash query can indeed improve the effciency while keeping the overall performances;\n\nWeaknesses: My major concern about this work is the higher performances than transformer-based methods. It may be a little hard to understand why the proposed DSA and hash-based searching operations can improve the performances so greatly. Because these operations are more like the approximation of corresponding operations in PointNet++.\n\nQuestions: 1. Why do we introduce batch normalization to the aggregated features in Eq.4? As the addition of features and neighborhood features have already introduce the spatial characteristics, I am not sure if this BN is necessary here, or other simple components may also work;\n\n2. In the Hash query part, will the points in a same local grid share the same neighbors? I do not quite get the calculation of complexity for each point.\n\n3. Is the hash grid is pre-constructed before training? Or created repeatedly during training?\n\n3. From the results in Table 1 and Table 3, the proposed method even outperforms transformer-based methods in a more efficiency way. Could the author analyze the reasons behind this? As the improvements of this framework seem to be efficient simplification of existing PointNet++ framework, I am curious why it can improve the performances so obviously.\n\nPlease check the grammar also, e.g., In Line 101, the $x_i$ might actually be $p_i$;", "labeling_timestamp": "2026-01-11T16:46:15.403647", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper only specifies the use of batch normalization inside the PWConv (MLP) in the aggregation module and discusses weight initialization, but it contains no experimental evaluation or ablation comparing batch normalization to simpler alternatives (e.g., no norm, LayerNorm, GroupNorm) in the aggregation step.", "evidence": "\"PWConv ... is an MLP that consists of pointwise convolution, batch normalization layer, and ReLU activation function.\" \n\n\"We analyze the feature aggregation of the vanilla SA module and introduce a novel efficient and effective DSA module... Moreover, we discuss the superiority of this method from the perspective of weight initialization...\"", "section": "3.2 Disassembled Set Abstraction"}
{"claim": "It is unclear whether points in the same local hash grid share the same neighbors.", "claim_type": "methodology", "paper_id": "ehfCxpDsrw", "paper_title": "LinNet: Linear Network for Efficient Point Cloud Representation Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "V28l1766Ei", "reviewer": "Reviewer_aBR6", "review_text": "Summary: In this work, the authors propose an efficient learning framework for point cloud representation learning. For the computational intensive local aggregation operation, this work proposes Disassembled set abstraction (DSA) to aggregate local features in terms of the spatial distributions of points in a simple and efficient manner. This work also proposes a Linearization sampling strategy and hash query operation to accelerate the sampling and neighbor searching processes. Experiments on classification and semantic segmentation demonstrate the effectiveness of the proposed method.\n\nStrengths: 1. The proposed local aggregation operation seems to be simple, efficient, and effective;\n\n2. The point searching strategy including the Linearization sampling strategy and hash query can indeed improve the effciency while keeping the overall performances;\n\nWeaknesses: My major concern about this work is the higher performances than transformer-based methods. It may be a little hard to understand why the proposed DSA and hash-based searching operations can improve the performances so greatly. Because these operations are more like the approximation of corresponding operations in PointNet++.\n\nQuestions: 1. Why do we introduce batch normalization to the aggregated features in Eq.4? As the addition of features and neighborhood features have already introduce the spatial characteristics, I am not sure if this BN is necessary here, or other simple components may also work;\n\n2. In the Hash query part, will the points in a same local grid share the same neighbors? I do not quite get the calculation of complexity for each point.\n\n3. Is the hash grid is pre-constructed before training? Or created repeatedly during training?\n\n3. From the results in Table 1 and Table 3, the proposed method even outperforms transformer-based methods in a more efficiency way. Could the author analyze the reasons behind this? As the improvements of this framework seem to be efficient simplification of existing PointNet++ framework, I am curious why it can improve the performances so obviously.\n\nPlease check the grammar also, e.g., In Line 101, the $x_i$ might actually be $p_i$;", "labeling_timestamp": "2026-01-11T16:46:22.748150", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes the hash-query and linearization sampling strategy (mapping points to buckets/curves and taking one point per grid) but does not state whether points that fall in the same local hash grid (bucket) share the same neighbor sets. The description focuses on buckets and which buckets to search, without clarifying neighbor-sharing within a grid.", "evidence": "“For neighborhood queries, we store each segmented curve as a bucket in a hash table. When querying the neighborhood, we only need to search in the buckets corresponding to the neighboring curves, which drastically cuts down the search range. Linear sampling ensures uniform sampling by taking the point closest to the origin within each grid as the new sampling point.”", "section": "Introduction"}
{"claim": "The complexity calculation per point for the hash query is not clearly explained.", "claim_type": "presentation", "paper_id": "ehfCxpDsrw", "paper_title": "LinNet: Linear Network for Efficient Point Cloud Representation Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "V28l1766Ei", "reviewer": "Reviewer_aBR6", "review_text": "Summary: In this work, the authors propose an efficient learning framework for point cloud representation learning. For the computational intensive local aggregation operation, this work proposes Disassembled set abstraction (DSA) to aggregate local features in terms of the spatial distributions of points in a simple and efficient manner. This work also proposes a Linearization sampling strategy and hash query operation to accelerate the sampling and neighbor searching processes. Experiments on classification and semantic segmentation demonstrate the effectiveness of the proposed method.\n\nStrengths: 1. The proposed local aggregation operation seems to be simple, efficient, and effective;\n\n2. The point searching strategy including the Linearization sampling strategy and hash query can indeed improve the effciency while keeping the overall performances;\n\nWeaknesses: My major concern about this work is the higher performances than transformer-based methods. It may be a little hard to understand why the proposed DSA and hash-based searching operations can improve the performances so greatly. Because these operations are more like the approximation of corresponding operations in PointNet++.\n\nQuestions: 1. Why do we introduce batch normalization to the aggregated features in Eq.4? As the addition of features and neighborhood features have already introduce the spatial characteristics, I am not sure if this BN is necessary here, or other simple components may also work;\n\n2. In the Hash query part, will the points in a same local grid share the same neighbors? I do not quite get the calculation of complexity for each point.\n\n3. Is the hash grid is pre-constructed before training? Or created repeatedly during training?\n\n3. From the results in Table 1 and Table 3, the proposed method even outperforms transformer-based methods in a more efficiency way. Could the author analyze the reasons behind this? As the improvements of this framework seem to be efficient simplification of existing PointNet++ framework, I am curious why it can improve the performances so obviously.\n\nPlease check the grammar also, e.g., In Line 101, the $x_i$ might actually be $p_i$;", "labeling_timestamp": "2026-01-11T16:46:23.949836", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes a hash query and linearization sampling approach and claims linear overall complexity, but it does not provide an explicit per-point complexity calculation or detailed derivation for the hash query; thus the reviewer's claim that the per-point complexity is not clearly explained is accurate.", "evidence": "\"a hash query and linearization sampling strategy are proposed for speeding up point searching. The core of our method is to map the 3D search space onto a segmented curve for acceleration. A sparse point cloud is ordered on that curve, and points adjacent to each other in the curve are also adjacent in space. For neighborhood queries, we store each segmented curve as a bucket in a hash table. When querying the neighborhood, we only need to search in the buckets corresponding to the neighboring curves, which drastically cuts down the search range. Linear sampling ensures uniform sampling by taking the point closest to the origin within each grid as the new sampling point. The method reduces the time complexity to be linear and supports GPU parallelism, resulting in very fast sampling.\"", "section": "Introduction (Sec. 1)"}
{"claim": "The paper does not specify whether the hash grid is pre-constructed before training or recreated during training.", "claim_type": "methodology", "paper_id": "ehfCxpDsrw", "paper_title": "LinNet: Linear Network for Efficient Point Cloud Representation Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "V28l1766Ei", "reviewer": "Reviewer_aBR6", "review_text": "Summary: In this work, the authors propose an efficient learning framework for point cloud representation learning. For the computational intensive local aggregation operation, this work proposes Disassembled set abstraction (DSA) to aggregate local features in terms of the spatial distributions of points in a simple and efficient manner. This work also proposes a Linearization sampling strategy and hash query operation to accelerate the sampling and neighbor searching processes. Experiments on classification and semantic segmentation demonstrate the effectiveness of the proposed method.\n\nStrengths: 1. The proposed local aggregation operation seems to be simple, efficient, and effective;\n\n2. The point searching strategy including the Linearization sampling strategy and hash query can indeed improve the effciency while keeping the overall performances;\n\nWeaknesses: My major concern about this work is the higher performances than transformer-based methods. It may be a little hard to understand why the proposed DSA and hash-based searching operations can improve the performances so greatly. Because these operations are more like the approximation of corresponding operations in PointNet++.\n\nQuestions: 1. Why do we introduce batch normalization to the aggregated features in Eq.4? As the addition of features and neighborhood features have already introduce the spatial characteristics, I am not sure if this BN is necessary here, or other simple components may also work;\n\n2. In the Hash query part, will the points in a same local grid share the same neighbors? I do not quite get the calculation of complexity for each point.\n\n3. Is the hash grid is pre-constructed before training? Or created repeatedly during training?\n\n3. From the results in Table 1 and Table 3, the proposed method even outperforms transformer-based methods in a more efficiency way. Could the author analyze the reasons behind this? As the improvements of this framework seem to be efficient simplification of existing PointNet++ framework, I am curious why it can improve the performances so obviously.\n\nPlease check the grammar also, e.g., In Line 101, the $x_i$ might actually be $p_i$;", "labeling_timestamp": "2026-01-11T16:46:25.887738", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes using a hash table/buckets for the segmented curves (hash query) but does not state whether this hash grid/table is constructed once beforehand or rebuilt during training/inference. The cited passages describe the method but give no timing or construction-frequency details.", "evidence": "\"For neighborhood queries, we store each segmented curve as a bucket in a hash table. When querying the neighborhood, we only need to search in the buckets corresponding to the neighboring curves, which drastically cuts down the search range.\"", "section": "Introduction"}
{"claim": "The authors do not analyze reasons why their method outperforms transformer-based methods despite being an efficient simplification of PointNet++.", "claim_type": "baseline", "paper_id": "ehfCxpDsrw", "paper_title": "LinNet: Linear Network for Efficient Point Cloud Representation Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "V28l1766Ei", "reviewer": "Reviewer_aBR6", "review_text": "Summary: In this work, the authors propose an efficient learning framework for point cloud representation learning. For the computational intensive local aggregation operation, this work proposes Disassembled set abstraction (DSA) to aggregate local features in terms of the spatial distributions of points in a simple and efficient manner. This work also proposes a Linearization sampling strategy and hash query operation to accelerate the sampling and neighbor searching processes. Experiments on classification and semantic segmentation demonstrate the effectiveness of the proposed method.\n\nStrengths: 1. The proposed local aggregation operation seems to be simple, efficient, and effective;\n\n2. The point searching strategy including the Linearization sampling strategy and hash query can indeed improve the effciency while keeping the overall performances;\n\nWeaknesses: My major concern about this work is the higher performances than transformer-based methods. It may be a little hard to understand why the proposed DSA and hash-based searching operations can improve the performances so greatly. Because these operations are more like the approximation of corresponding operations in PointNet++.\n\nQuestions: 1. Why do we introduce batch normalization to the aggregated features in Eq.4? As the addition of features and neighborhood features have already introduce the spatial characteristics, I am not sure if this BN is necessary here, or other simple components may also work;\n\n2. In the Hash query part, will the points in a same local grid share the same neighbors? I do not quite get the calculation of complexity for each point.\n\n3. Is the hash grid is pre-constructed before training? Or created repeatedly during training?\n\n3. From the results in Table 1 and Table 3, the proposed method even outperforms transformer-based methods in a more efficiency way. Could the author analyze the reasons behind this? As the improvements of this framework seem to be efficient simplification of existing PointNet++ framework, I am curious why it can improve the performances so obviously.\n\nPlease check the grammar also, e.g., In Line 101, the $x_i$ might actually be $p_i$;", "labeling_timestamp": "2026-01-11T16:46:33.504488", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explains why LinNet outperforms prior point-based methods (e.g., via the DSA module, separable aggregation, linearization sampling and hash query) but contains no analysis explaining why it outperforms transformer-based or attention-based methods. The abstract notes performance 'without any extra attention' but the provided sections do not include analysis comparing mechanisms or reasons relative to transformer architectures.", "evidence": "'LinNet, as a purely point-based method, outperforms most previous methods in both indoor and outdoor scenes without any extra attention, and sparse convolution but merely relying on a simple MLP.'\n\n'We analyze the feature aggregation of the vanilla SA module and introduce a novel efficient and effective DSA module.'", "section": "Abstract; Section 3 (Disassembled Set Abstraction)"}
{"claim": "There is a possible notation error where x_i might actually be p_i in Line 101.", "claim_type": "methodology", "paper_id": "ehfCxpDsrw", "paper_title": "LinNet: Linear Network for Efficient Point Cloud Representation Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "V28l1766Ei", "reviewer": "Reviewer_aBR6", "review_text": "Summary: In this work, the authors propose an efficient learning framework for point cloud representation learning. For the computational intensive local aggregation operation, this work proposes Disassembled set abstraction (DSA) to aggregate local features in terms of the spatial distributions of points in a simple and efficient manner. This work also proposes a Linearization sampling strategy and hash query operation to accelerate the sampling and neighbor searching processes. Experiments on classification and semantic segmentation demonstrate the effectiveness of the proposed method.\n\nStrengths: 1. The proposed local aggregation operation seems to be simple, efficient, and effective;\n\n2. The point searching strategy including the Linearization sampling strategy and hash query can indeed improve the effciency while keeping the overall performances;\n\nWeaknesses: My major concern about this work is the higher performances than transformer-based methods. It may be a little hard to understand why the proposed DSA and hash-based searching operations can improve the performances so greatly. Because these operations are more like the approximation of corresponding operations in PointNet++.\n\nQuestions: 1. Why do we introduce batch normalization to the aggregated features in Eq.4? As the addition of features and neighborhood features have already introduce the spatial characteristics, I am not sure if this BN is necessary here, or other simple components may also work;\n\n2. In the Hash query part, will the points in a same local grid share the same neighbors? I do not quite get the calculation of complexity for each point.\n\n3. Is the hash grid is pre-constructed before training? Or created repeatedly during training?\n\n3. From the results in Table 1 and Table 3, the proposed method even outperforms transformer-based methods in a more efficiency way. Could the author analyze the reasons behind this? As the improvements of this framework seem to be efficient simplification of existing PointNet++ framework, I am curious why it can improve the performances so obviously.\n\nPlease check the grammar also, e.g., In Line 101, the $x_i$ might actually be $p_i$;", "labeling_timestamp": "2026-01-11T16:47:00.280283", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper shows inconsistent notation around x_i: Section 3.1 defines x_i = (p_i, f_i) (so x_i is a point with coordinates p_i), but Section 3.2 then uses the phrase \"Given a point cloud x_i\", which is likely a misuse (should be 'point x_i' or refer to p_i). This indicates a notation slip, but the paper does not clearly show an instance where x_i must be replaced by p_i everywhere, so the reviewer's specific claim is only partially correct.", "evidence": "1) \"Given a 3D point cloud V = ( P , F ) consisting of n points x . For the i -th point x i = ( p i , f i ) , p i ∈ R 3 and f i ∈ R c are the space coordinates and features, respectively.\" 2) \"Given a point cloud x i , a typical local aggregation in 3D vision can be formulated as:\"", "section": "3.1 Problem Formulation; 3.2 Disassembled Set Abstraction"}
{"claim": "There is a significant gap between the paper's theoretical analysis and the experimental settings used in the evaluations.", "claim_type": "methodology", "paper_id": "TlyiaPXaVN", "paper_title": "Generative Adversarial Equilibrium Solvers", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "rCNKi8T07D", "reviewer": "Reviewer_Wcqa", "review_text": "Summary: The paper introduces generative adversarial equilibrium solvers (GAES), a GAN that learns to map pseudo-games to their generalized Nash equilibria from a sample of problem instances. In particular, they provide a formulation that makes the problem amenable to standard stochastic first-order methods. They use GAES to compute in a scalable way competitive equilibria in exchange economies and an environmental economic model of the Kyoto mechanism, outperforming earlier methods.\n\nStrengths: Pseudo-games are very general game-theoretic models with a number of applications, most notably Arrow-Debreu competitive economies. Yet, there is a lack of scalable techniques for computing generalized Nash equilibria in such settings. This paper makes a concrete contribution in that direction by providing a method with promising performance across a number of benchmarks. The proposed method is natural and the experimental results are overall convincing and quite thorough. Indeed, the paper appears to attain state of the art performance in a number of important applications, and could have significant impact in this area.\n\nWeaknesses: There are some soundness issues that the authors have to address. First, there appears to be a significant gap between the theoretical analysis and the experimental settings. Specifically, it is not clear how a stationary point in the sense of Theorem 4.1 translates to a GNE. If stationary points are not necessarily GNE, the narrative of the paper has to be restructured. In particular, it is often claimed that the method maps pseudo-games to GNE, and it is not clear whether that claim is theoretically sound. Of course, computing GNE is intractable, but it is alluded (for example in the abstract) that under a distribution over problem instances the problem could be easier. Theorem 4.1 also makes a strong concavity assumption which appears to be violated in all settings of interest. It should be the case that a \"small\" regularizer can always be incorporated without affecting the equilibria by much, but I think this should be discussed in more detail. \n\nI am also confused about Footnote 4. It is claimed that the method obtains the state of the art $O(1/\\epsilon^3)$ complexity, a major improvement over $O(1/\\epsilon^6)$, which the authors claim was the previous state of the art. The authors have to explain more precisely the class of problems this applies to; there are many variants of the PL condition studied in the literature. In particular, the following papers seem to obtain a much better dependency: \"Faster single-loop algorithms for minimax optimization without strong concavity,\" \"faster stochastic algoritms for minimax optimization under Polyak-Lojasiewicz Conditions\" and \"Doubly smoothed GDA for constrained nonconvex-nonconcave minimax optimization.\"\n\nBesides the issues above, the algorithmic approach is very close to the paper \"Exploitability minimization in games and beyond,\" which limits to some extent the algorithmic contribution of the present paper. The authors have to highlight the comparisons in more detail.\n\nQuestions: Some minor comments for the authors:\n\n1. The title of the submission document is the default one.\n2. The references have to be polished. There are many papers that are published many years ago and only the arXiv version is cited. There is also an issue with consistency: sometimes URLs are included, sometimes they are not. Please fix those issues.\n3. There are underfull equations in Observation 1 and immediately below.\n4. I don't understand how the paper of Daskalakis et al. (2009) is relevant in the context of Footnote 4 about min-max optimization.\n5. The appendix has many overfull equations that need to be formatted properly.", "labeling_timestamp": "2026-01-11T16:46:44.965882", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper presents both theoretical results (complexity and convergence bounds under specific assumptions) and empirical evaluations (Section 5), but the provided content does not state whether the experimental settings satisfy the theoretical assumptions or identify any mismatch. Therefore there is insufficient information in the paper excerpt to confirm a significant gap between theory and experiments.", "evidence": "Abstract: \"We provide computational and sample complexity bounds for Lipschitz-smooth function approximators in a large class of concave pseudo-games, and apply the framework to finding Nash equilibria...\"; Contributions: \"We prove polynomial-time convergence of our training algorithm for arbitrary Lipschitz-smooth function approximators with the joint action space dimensions larger than parameter space dimensions (Theorem 4.1, Section 4). and provide generalization bounds for arbitrary function approximators (Theorem 4.2, Section 4). Finally, we provide empirical evidence that GAES outperforms state of the art baselines in Arrow-Debreu competitive economies, and show that GAES can replicate existing qualitative analyses for pseudo-games... (Section 5).\"", "section": "Abstract; Contributions (Introduction)"}
{"claim": "Theorem 4.1's stationary points may not correspond to generalized Nash equilibria, calling the theoretical guarantees into question.", "claim_type": "methodology", "paper_id": "TlyiaPXaVN", "paper_title": "Generative Adversarial Equilibrium Solvers", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "rCNKi8T07D", "reviewer": "Reviewer_Wcqa", "review_text": "Summary: The paper introduces generative adversarial equilibrium solvers (GAES), a GAN that learns to map pseudo-games to their generalized Nash equilibria from a sample of problem instances. In particular, they provide a formulation that makes the problem amenable to standard stochastic first-order methods. They use GAES to compute in a scalable way competitive equilibria in exchange economies and an environmental economic model of the Kyoto mechanism, outperforming earlier methods.\n\nStrengths: Pseudo-games are very general game-theoretic models with a number of applications, most notably Arrow-Debreu competitive economies. Yet, there is a lack of scalable techniques for computing generalized Nash equilibria in such settings. This paper makes a concrete contribution in that direction by providing a method with promising performance across a number of benchmarks. The proposed method is natural and the experimental results are overall convincing and quite thorough. Indeed, the paper appears to attain state of the art performance in a number of important applications, and could have significant impact in this area.\n\nWeaknesses: There are some soundness issues that the authors have to address. First, there appears to be a significant gap between the theoretical analysis and the experimental settings. Specifically, it is not clear how a stationary point in the sense of Theorem 4.1 translates to a GNE. If stationary points are not necessarily GNE, the narrative of the paper has to be restructured. In particular, it is often claimed that the method maps pseudo-games to GNE, and it is not clear whether that claim is theoretically sound. Of course, computing GNE is intractable, but it is alluded (for example in the abstract) that under a distribution over problem instances the problem could be easier. Theorem 4.1 also makes a strong concavity assumption which appears to be violated in all settings of interest. It should be the case that a \"small\" regularizer can always be incorporated without affecting the equilibria by much, but I think this should be discussed in more detail. \n\nI am also confused about Footnote 4. It is claimed that the method obtains the state of the art $O(1/\\epsilon^3)$ complexity, a major improvement over $O(1/\\epsilon^6)$, which the authors claim was the previous state of the art. The authors have to explain more precisely the class of problems this applies to; there are many variants of the PL condition studied in the literature. In particular, the following papers seem to obtain a much better dependency: \"Faster single-loop algorithms for minimax optimization without strong concavity,\" \"faster stochastic algoritms for minimax optimization under Polyak-Lojasiewicz Conditions\" and \"Doubly smoothed GDA for constrained nonconvex-nonconcave minimax optimization.\"\n\nBesides the issues above, the algorithmic approach is very close to the paper \"Exploitability minimization in games and beyond,\" which limits to some extent the algorithmic contribution of the present paper. The authors have to highlight the comparisons in more detail.\n\nQuestions: Some minor comments for the authors:\n\n1. The title of the submission document is the default one.\n2. The references have to be polished. There are many papers that are published many years ago and only the arXiv version is cited. There is also an issue with consistency: sometimes URLs are included, sometimes they are not. Please fix those issues.\n3. There are underfull equations in Observation 1 and immediately below.\n4. I don't understand how the paper of Daskalakis et al. (2009) is relevant in the context of Footnote 4 about min-max optimization.\n5. The appendix has many overfull equations that need to be formatted properly.", "labeling_timestamp": "2026-01-11T16:46:49.216463", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt references Theorem 4.1 and claims polynomial-time convergence (see Contributions), but the statement and proof of Theorem 4.1 are not included in the supplied content. Without the theorem text or surrounding analysis, we cannot verify whether its stationary points correspond to generalized Nash equilibria or whether the theoretical guarantees are affected.", "evidence": "We prove polynomial-time convergence of our training algorithm for arbitrary Lipschitz-smooth function approximators with the joint action space dimensions larger than parameter space dimensions (Theorem 4.1, Section 4).", "section": "Contributions / Introduction"}
{"claim": "The paper frequently claims the method maps pseudo-games to GNE without providing clear theoretical justification for that mapping.", "claim_type": "methodology", "paper_id": "TlyiaPXaVN", "paper_title": "Generative Adversarial Equilibrium Solvers", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "rCNKi8T07D", "reviewer": "Reviewer_Wcqa", "review_text": "Summary: The paper introduces generative adversarial equilibrium solvers (GAES), a GAN that learns to map pseudo-games to their generalized Nash equilibria from a sample of problem instances. In particular, they provide a formulation that makes the problem amenable to standard stochastic first-order methods. They use GAES to compute in a scalable way competitive equilibria in exchange economies and an environmental economic model of the Kyoto mechanism, outperforming earlier methods.\n\nStrengths: Pseudo-games are very general game-theoretic models with a number of applications, most notably Arrow-Debreu competitive economies. Yet, there is a lack of scalable techniques for computing generalized Nash equilibria in such settings. This paper makes a concrete contribution in that direction by providing a method with promising performance across a number of benchmarks. The proposed method is natural and the experimental results are overall convincing and quite thorough. Indeed, the paper appears to attain state of the art performance in a number of important applications, and could have significant impact in this area.\n\nWeaknesses: There are some soundness issues that the authors have to address. First, there appears to be a significant gap between the theoretical analysis and the experimental settings. Specifically, it is not clear how a stationary point in the sense of Theorem 4.1 translates to a GNE. If stationary points are not necessarily GNE, the narrative of the paper has to be restructured. In particular, it is often claimed that the method maps pseudo-games to GNE, and it is not clear whether that claim is theoretically sound. Of course, computing GNE is intractable, but it is alluded (for example in the abstract) that under a distribution over problem instances the problem could be easier. Theorem 4.1 also makes a strong concavity assumption which appears to be violated in all settings of interest. It should be the case that a \"small\" regularizer can always be incorporated without affecting the equilibria by much, but I think this should be discussed in more detail. \n\nI am also confused about Footnote 4. It is claimed that the method obtains the state of the art $O(1/\\epsilon^3)$ complexity, a major improvement over $O(1/\\epsilon^6)$, which the authors claim was the previous state of the art. The authors have to explain more precisely the class of problems this applies to; there are many variants of the PL condition studied in the literature. In particular, the following papers seem to obtain a much better dependency: \"Faster single-loop algorithms for minimax optimization without strong concavity,\" \"faster stochastic algoritms for minimax optimization under Polyak-Lojasiewicz Conditions\" and \"Doubly smoothed GDA for constrained nonconvex-nonconcave minimax optimization.\"\n\nBesides the issues above, the algorithmic approach is very close to the paper \"Exploitability minimization in games and beyond,\" which limits to some extent the algorithmic contribution of the present paper. The authors have to highlight the comparisons in more detail.\n\nQuestions: Some minor comments for the authors:\n\n1. The title of the submission document is the default one.\n2. The references have to be polished. There are many papers that are published many years ago and only the arXiv version is cited. There is also an issue with consistency: sometimes URLs are included, sometimes they are not. Please fix those issues.\n3. There are underfull equations in Observation 1 and immediately below.\n4. I don't understand how the paper of Daskalakis et al. (2009) is relevant in the context of Footnote 4 about min-max optimization.\n5. The appendix has many overfull equations that need to be formatted properly.", "labeling_timestamp": "2026-01-11T16:46:54.700681", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper does make claims that GAES learns/maps pseudo-games to GNE, but it also provides explicit theoretical justification: computational and sample complexity bounds, and proven convergence and generalization results (Theorems 4.1 and 4.2), and discusses conditions (Lipschitz continuity, bounded gradients) under which the training objective is well-behaved. Thus the reviewer's assertion that the paper frequently claims the mapping without clear theoretical justification is contradicted by the paper.", "evidence": "Abstract: \"We introduce Generative Adversarial Equilibrium Solvers (GAES): a family of generative adversarial neural networks that can learn GNE and CE from only a sample of problem instances.\" Introduction (Contributions): \"We prove polynomial-time convergence of our training algorithm for arbitrary Lipschitz-smooth function approximators with the joint action space dimensions larger than parameter space dimensions (Theorem 4.1, Section 4). and provide generalization bounds for arbitrary function approximators (Theorem 4.2, Section 4).\" Introduction: \"By delegating the task of computing a best-response to a discriminator, our method circumvents the issue of solving a convex program, yielding a training problem given by a min-max optimization problem whose objective is Lipschitz-continuous, for which gradients can be guaranteed to be bounded under standard assumptions on the discriminator and the payoffs of players.\"", "section": "Abstract; Introduction (Contributions)"}
{"claim": "Theorem 4.1 assumes strong concavity, an assumption that appears to be violated in all experimental settings of interest.", "claim_type": "methodology", "paper_id": "TlyiaPXaVN", "paper_title": "Generative Adversarial Equilibrium Solvers", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "rCNKi8T07D", "reviewer": "Reviewer_Wcqa", "review_text": "Summary: The paper introduces generative adversarial equilibrium solvers (GAES), a GAN that learns to map pseudo-games to their generalized Nash equilibria from a sample of problem instances. In particular, they provide a formulation that makes the problem amenable to standard stochastic first-order methods. They use GAES to compute in a scalable way competitive equilibria in exchange economies and an environmental economic model of the Kyoto mechanism, outperforming earlier methods.\n\nStrengths: Pseudo-games are very general game-theoretic models with a number of applications, most notably Arrow-Debreu competitive economies. Yet, there is a lack of scalable techniques for computing generalized Nash equilibria in such settings. This paper makes a concrete contribution in that direction by providing a method with promising performance across a number of benchmarks. The proposed method is natural and the experimental results are overall convincing and quite thorough. Indeed, the paper appears to attain state of the art performance in a number of important applications, and could have significant impact in this area.\n\nWeaknesses: There are some soundness issues that the authors have to address. First, there appears to be a significant gap between the theoretical analysis and the experimental settings. Specifically, it is not clear how a stationary point in the sense of Theorem 4.1 translates to a GNE. If stationary points are not necessarily GNE, the narrative of the paper has to be restructured. In particular, it is often claimed that the method maps pseudo-games to GNE, and it is not clear whether that claim is theoretically sound. Of course, computing GNE is intractable, but it is alluded (for example in the abstract) that under a distribution over problem instances the problem could be easier. Theorem 4.1 also makes a strong concavity assumption which appears to be violated in all settings of interest. It should be the case that a \"small\" regularizer can always be incorporated without affecting the equilibria by much, but I think this should be discussed in more detail. \n\nI am also confused about Footnote 4. It is claimed that the method obtains the state of the art $O(1/\\epsilon^3)$ complexity, a major improvement over $O(1/\\epsilon^6)$, which the authors claim was the previous state of the art. The authors have to explain more precisely the class of problems this applies to; there are many variants of the PL condition studied in the literature. In particular, the following papers seem to obtain a much better dependency: \"Faster single-loop algorithms for minimax optimization without strong concavity,\" \"faster stochastic algoritms for minimax optimization under Polyak-Lojasiewicz Conditions\" and \"Doubly smoothed GDA for constrained nonconvex-nonconcave minimax optimization.\"\n\nBesides the issues above, the algorithmic approach is very close to the paper \"Exploitability minimization in games and beyond,\" which limits to some extent the algorithmic contribution of the present paper. The authors have to highlight the comparisons in more detail.\n\nQuestions: Some minor comments for the authors:\n\n1. The title of the submission document is the default one.\n2. The references have to be polished. There are many papers that are published many years ago and only the arXiv version is cited. There is also an issue with consistency: sometimes URLs are included, sometimes they are not. Please fix those issues.\n3. There are underfull equations in Observation 1 and immediately below.\n4. I don't understand how the paper of Daskalakis et al. (2009) is relevant in the context of Footnote 4 about min-max optimization.\n5. The appendix has many overfull equations that need to be formatted properly.", "labeling_timestamp": "2026-01-11T16:46:54.764227", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt references Theorem 4.1 and states convergence results for concave pseudo-games and Lipschitz-smooth function approximators, but does not include Theorem 4.1 itself nor an explicit list of its assumptions (e.g., whether it requires strong concavity). Therefore, from the given text we cannot verify the claim that Theorem 4.1 assumes strong concavity or that this assumption is violated in the reported experiments.", "evidence": "1) \"We prove polynomial-time convergence of our training algorithm for arbitrary Lipschitz-smooth function approximators with the joint action space dimensions larger than parameter space dimensions (Theorem 4.1, Section 4).\" 2) \"We provide computational and sample complexity bounds for Lipschitz-smooth function approximators in a large class of concave pseudo-games, and apply the framework...\"", "section": "Contributions / Introduction (reference to Theorem 4.1 in Section 4)"}
{"claim": "The authors do not sufficiently justify that adding a small regularizer preserves equilibria or quantify its practical effect.", "claim_type": "methodology", "paper_id": "TlyiaPXaVN", "paper_title": "Generative Adversarial Equilibrium Solvers", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "rCNKi8T07D", "reviewer": "Reviewer_Wcqa", "review_text": "Summary: The paper introduces generative adversarial equilibrium solvers (GAES), a GAN that learns to map pseudo-games to their generalized Nash equilibria from a sample of problem instances. In particular, they provide a formulation that makes the problem amenable to standard stochastic first-order methods. They use GAES to compute in a scalable way competitive equilibria in exchange economies and an environmental economic model of the Kyoto mechanism, outperforming earlier methods.\n\nStrengths: Pseudo-games are very general game-theoretic models with a number of applications, most notably Arrow-Debreu competitive economies. Yet, there is a lack of scalable techniques for computing generalized Nash equilibria in such settings. This paper makes a concrete contribution in that direction by providing a method with promising performance across a number of benchmarks. The proposed method is natural and the experimental results are overall convincing and quite thorough. Indeed, the paper appears to attain state of the art performance in a number of important applications, and could have significant impact in this area.\n\nWeaknesses: There are some soundness issues that the authors have to address. First, there appears to be a significant gap between the theoretical analysis and the experimental settings. Specifically, it is not clear how a stationary point in the sense of Theorem 4.1 translates to a GNE. If stationary points are not necessarily GNE, the narrative of the paper has to be restructured. In particular, it is often claimed that the method maps pseudo-games to GNE, and it is not clear whether that claim is theoretically sound. Of course, computing GNE is intractable, but it is alluded (for example in the abstract) that under a distribution over problem instances the problem could be easier. Theorem 4.1 also makes a strong concavity assumption which appears to be violated in all settings of interest. It should be the case that a \"small\" regularizer can always be incorporated without affecting the equilibria by much, but I think this should be discussed in more detail. \n\nI am also confused about Footnote 4. It is claimed that the method obtains the state of the art $O(1/\\epsilon^3)$ complexity, a major improvement over $O(1/\\epsilon^6)$, which the authors claim was the previous state of the art. The authors have to explain more precisely the class of problems this applies to; there are many variants of the PL condition studied in the literature. In particular, the following papers seem to obtain a much better dependency: \"Faster single-loop algorithms for minimax optimization without strong concavity,\" \"faster stochastic algoritms for minimax optimization under Polyak-Lojasiewicz Conditions\" and \"Doubly smoothed GDA for constrained nonconvex-nonconcave minimax optimization.\"\n\nBesides the issues above, the algorithmic approach is very close to the paper \"Exploitability minimization in games and beyond,\" which limits to some extent the algorithmic contribution of the present paper. The authors have to highlight the comparisons in more detail.\n\nQuestions: Some minor comments for the authors:\n\n1. The title of the submission document is the default one.\n2. The references have to be polished. There are many papers that are published many years ago and only the arXiv version is cited. There is also an issue with consistency: sometimes URLs are included, sometimes they are not. Please fix those issues.\n3. There are underfull equations in Observation 1 and immediately below.\n4. I don't understand how the paper of Daskalakis et al. (2009) is relevant in the context of Footnote 4 about min-max optimization.\n5. The appendix has many overfull equations that need to be formatted properly.", "labeling_timestamp": "2026-01-11T16:47:23.430764", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not discuss adding a small regularizer or analyze whether such a regularizer preserves equilibria or quantify its practical effect. The authors describe making the objective Lipschitz-continuous and bounding gradients via their GAN formulation, but do not mention a small regularizer in the shown sections, so there is insufficient information in the supplied content to confirm or refute the reviewer's claim.", "evidence": "“By delegating the task of computing a best-response to a discriminator, our method circumvents the issue of solving a convex program, yielding a training problem given by a min-max optimization problem whose objective is Lipschitz-continuous, for which gradients can be guaranteed to be bounded under standard assumptions on the discriminator and the payoffs of players.”\n\n“These methods become inefficient in pseudo-games, since in contrast to regular games, the exploitability in pseudo-games (1) requires solving a non-linear optimization problem, (2) is not Lipschitz-continuous, in turn making it hard to learn from samples, and (3) has unbounded gradients, which might lead to exploding gradients in neighborhoods of GNE. Our GAN formulation circumvents all three of these issues.”", "section": "Introduction / Contributions (Section 1)"}
{"claim": "The claim of O(1/ε^3) complexity lacks a precise specification of the problem class or assumptions under which that bound holds.", "claim_type": "methodology", "paper_id": "TlyiaPXaVN", "paper_title": "Generative Adversarial Equilibrium Solvers", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "rCNKi8T07D", "reviewer": "Reviewer_Wcqa", "review_text": "Summary: The paper introduces generative adversarial equilibrium solvers (GAES), a GAN that learns to map pseudo-games to their generalized Nash equilibria from a sample of problem instances. In particular, they provide a formulation that makes the problem amenable to standard stochastic first-order methods. They use GAES to compute in a scalable way competitive equilibria in exchange economies and an environmental economic model of the Kyoto mechanism, outperforming earlier methods.\n\nStrengths: Pseudo-games are very general game-theoretic models with a number of applications, most notably Arrow-Debreu competitive economies. Yet, there is a lack of scalable techniques for computing generalized Nash equilibria in such settings. This paper makes a concrete contribution in that direction by providing a method with promising performance across a number of benchmarks. The proposed method is natural and the experimental results are overall convincing and quite thorough. Indeed, the paper appears to attain state of the art performance in a number of important applications, and could have significant impact in this area.\n\nWeaknesses: There are some soundness issues that the authors have to address. First, there appears to be a significant gap between the theoretical analysis and the experimental settings. Specifically, it is not clear how a stationary point in the sense of Theorem 4.1 translates to a GNE. If stationary points are not necessarily GNE, the narrative of the paper has to be restructured. In particular, it is often claimed that the method maps pseudo-games to GNE, and it is not clear whether that claim is theoretically sound. Of course, computing GNE is intractable, but it is alluded (for example in the abstract) that under a distribution over problem instances the problem could be easier. Theorem 4.1 also makes a strong concavity assumption which appears to be violated in all settings of interest. It should be the case that a \"small\" regularizer can always be incorporated without affecting the equilibria by much, but I think this should be discussed in more detail. \n\nI am also confused about Footnote 4. It is claimed that the method obtains the state of the art $O(1/\\epsilon^3)$ complexity, a major improvement over $O(1/\\epsilon^6)$, which the authors claim was the previous state of the art. The authors have to explain more precisely the class of problems this applies to; there are many variants of the PL condition studied in the literature. In particular, the following papers seem to obtain a much better dependency: \"Faster single-loop algorithms for minimax optimization without strong concavity,\" \"faster stochastic algoritms for minimax optimization under Polyak-Lojasiewicz Conditions\" and \"Doubly smoothed GDA for constrained nonconvex-nonconcave minimax optimization.\"\n\nBesides the issues above, the algorithmic approach is very close to the paper \"Exploitability minimization in games and beyond,\" which limits to some extent the algorithmic contribution of the present paper. The authors have to highlight the comparisons in more detail.\n\nQuestions: Some minor comments for the authors:\n\n1. The title of the submission document is the default one.\n2. The references have to be polished. There are many papers that are published many years ago and only the arXiv version is cited. There is also an issue with consistency: sometimes URLs are included, sometimes they are not. Please fix those issues.\n3. There are underfull equations in Observation 1 and immediately below.\n4. I don't understand how the paper of Daskalakis et al. (2009) is relevant in the context of Footnote 4 about min-max optimization.\n5. The appendix has many overfull equations that need to be formatted properly.", "labeling_timestamp": "2026-01-11T16:47:17.627324", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided excerpt references a polynomial-time convergence result (Theorem 4.1, Section 4) and generalization bounds, but the excerpt does not contain the claimed O(1/ε^3) complexity statement nor the detailed assumptions or problem-class specification under which such a bound would hold. Therefore, based on the supplied paper content, it is impossible to verify whether the O(1/ε^3) claim appears and whether it lacks precise specification.", "evidence": "\"We prove polynomial-time convergence of our training algorithm for arbitrary Lipschitz-smooth function approximators with the joint action space dimensions larger than parameter space dimensions (Theorem 4.1, Section 4). and provide generalization bounds for arbitrary function approximators (Theorem 4.2, Section 4).\"", "section": "Introduction (references to Theorem 4.1 and Section 4)"}
{"claim": "Several prior works report better complexity dependencies than the claimed O(1/ε^3), but those references and comparisons are not discussed.", "claim_type": "baseline", "paper_id": "TlyiaPXaVN", "paper_title": "Generative Adversarial Equilibrium Solvers", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "rCNKi8T07D", "reviewer": "Reviewer_Wcqa", "review_text": "Summary: The paper introduces generative adversarial equilibrium solvers (GAES), a GAN that learns to map pseudo-games to their generalized Nash equilibria from a sample of problem instances. In particular, they provide a formulation that makes the problem amenable to standard stochastic first-order methods. They use GAES to compute in a scalable way competitive equilibria in exchange economies and an environmental economic model of the Kyoto mechanism, outperforming earlier methods.\n\nStrengths: Pseudo-games are very general game-theoretic models with a number of applications, most notably Arrow-Debreu competitive economies. Yet, there is a lack of scalable techniques for computing generalized Nash equilibria in such settings. This paper makes a concrete contribution in that direction by providing a method with promising performance across a number of benchmarks. The proposed method is natural and the experimental results are overall convincing and quite thorough. Indeed, the paper appears to attain state of the art performance in a number of important applications, and could have significant impact in this area.\n\nWeaknesses: There are some soundness issues that the authors have to address. First, there appears to be a significant gap between the theoretical analysis and the experimental settings. Specifically, it is not clear how a stationary point in the sense of Theorem 4.1 translates to a GNE. If stationary points are not necessarily GNE, the narrative of the paper has to be restructured. In particular, it is often claimed that the method maps pseudo-games to GNE, and it is not clear whether that claim is theoretically sound. Of course, computing GNE is intractable, but it is alluded (for example in the abstract) that under a distribution over problem instances the problem could be easier. Theorem 4.1 also makes a strong concavity assumption which appears to be violated in all settings of interest. It should be the case that a \"small\" regularizer can always be incorporated without affecting the equilibria by much, but I think this should be discussed in more detail. \n\nI am also confused about Footnote 4. It is claimed that the method obtains the state of the art $O(1/\\epsilon^3)$ complexity, a major improvement over $O(1/\\epsilon^6)$, which the authors claim was the previous state of the art. The authors have to explain more precisely the class of problems this applies to; there are many variants of the PL condition studied in the literature. In particular, the following papers seem to obtain a much better dependency: \"Faster single-loop algorithms for minimax optimization without strong concavity,\" \"faster stochastic algoritms for minimax optimization under Polyak-Lojasiewicz Conditions\" and \"Doubly smoothed GDA for constrained nonconvex-nonconcave minimax optimization.\"\n\nBesides the issues above, the algorithmic approach is very close to the paper \"Exploitability minimization in games and beyond,\" which limits to some extent the algorithmic contribution of the present paper. The authors have to highlight the comparisons in more detail.\n\nQuestions: Some minor comments for the authors:\n\n1. The title of the submission document is the default one.\n2. The references have to be polished. There are many papers that are published many years ago and only the arXiv version is cited. There is also an issue with consistency: sometimes URLs are included, sometimes they are not. Please fix those issues.\n3. There are underfull equations in Observation 1 and immediately below.\n4. I don't understand how the paper of Daskalakis et al. (2009) is relevant in the context of Footnote 4 about min-max optimization.\n5. The appendix has many overfull equations that need to be formatted properly.", "labeling_timestamp": "2026-01-11T16:47:20.185935", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not state the O(1/ε^3) complexity bound nor discuss prior works with better complexity dependencies. The paper only claims polynomial-time convergence (Theorem 4.1) and provides generalization bounds, but the exact dependency O(1/ε^3) and any comparisons to prior results with better rates are not present in the supplied content, so the reviewer's assertion cannot be verified from the given text.", "evidence": "We prove polynomial-time convergence of our training algorithm for arbitrary Lipschitz-smooth function approximators with the joint action space dimensions larger than parameter space dimensions (Theorem 4.1, Section 4).", "section": "Contributions / Introduction"}
{"claim": "The algorithmic approach is very similar to 'Exploitability minimization in games and beyond', which reduces the paper's algorithmic novelty.", "claim_type": "novelty", "paper_id": "TlyiaPXaVN", "paper_title": "Generative Adversarial Equilibrium Solvers", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "rCNKi8T07D", "reviewer": "Reviewer_Wcqa", "review_text": "Summary: The paper introduces generative adversarial equilibrium solvers (GAES), a GAN that learns to map pseudo-games to their generalized Nash equilibria from a sample of problem instances. In particular, they provide a formulation that makes the problem amenable to standard stochastic first-order methods. They use GAES to compute in a scalable way competitive equilibria in exchange economies and an environmental economic model of the Kyoto mechanism, outperforming earlier methods.\n\nStrengths: Pseudo-games are very general game-theoretic models with a number of applications, most notably Arrow-Debreu competitive economies. Yet, there is a lack of scalable techniques for computing generalized Nash equilibria in such settings. This paper makes a concrete contribution in that direction by providing a method with promising performance across a number of benchmarks. The proposed method is natural and the experimental results are overall convincing and quite thorough. Indeed, the paper appears to attain state of the art performance in a number of important applications, and could have significant impact in this area.\n\nWeaknesses: There are some soundness issues that the authors have to address. First, there appears to be a significant gap between the theoretical analysis and the experimental settings. Specifically, it is not clear how a stationary point in the sense of Theorem 4.1 translates to a GNE. If stationary points are not necessarily GNE, the narrative of the paper has to be restructured. In particular, it is often claimed that the method maps pseudo-games to GNE, and it is not clear whether that claim is theoretically sound. Of course, computing GNE is intractable, but it is alluded (for example in the abstract) that under a distribution over problem instances the problem could be easier. Theorem 4.1 also makes a strong concavity assumption which appears to be violated in all settings of interest. It should be the case that a \"small\" regularizer can always be incorporated without affecting the equilibria by much, but I think this should be discussed in more detail. \n\nI am also confused about Footnote 4. It is claimed that the method obtains the state of the art $O(1/\\epsilon^3)$ complexity, a major improvement over $O(1/\\epsilon^6)$, which the authors claim was the previous state of the art. The authors have to explain more precisely the class of problems this applies to; there are many variants of the PL condition studied in the literature. In particular, the following papers seem to obtain a much better dependency: \"Faster single-loop algorithms for minimax optimization without strong concavity,\" \"faster stochastic algoritms for minimax optimization under Polyak-Lojasiewicz Conditions\" and \"Doubly smoothed GDA for constrained nonconvex-nonconcave minimax optimization.\"\n\nBesides the issues above, the algorithmic approach is very close to the paper \"Exploitability minimization in games and beyond,\" which limits to some extent the algorithmic contribution of the present paper. The authors have to highlight the comparisons in more detail.\n\nQuestions: Some minor comments for the authors:\n\n1. The title of the submission document is the default one.\n2. The references have to be polished. There are many papers that are published many years ago and only the arXiv version is cited. There is also an issue with consistency: sometimes URLs are included, sometimes they are not. Please fix those issues.\n3. There are underfull equations in Observation 1 and immediately below.\n4. I don't understand how the paper of Daskalakis et al. (2009) is relevant in the context of Footnote 4 about min-max optimization.\n5. The appendix has many overfull equations that need to be formatted properly.", "labeling_timestamp": "2026-01-11T16:47:19.215544", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly states it is inspired by prior exploitability-minimization methods (Duan et al., Marris et al.) and also minimizes expected exploitability, so there is methodological similarity. However, the authors claim key algorithmic differences and novel contributions: they replace direct exploitability computation with a GAN-style discriminator that computes best-responses (circumventing non‑Lipschitz/unbounded-gradient issues in pseudo-games) and provide new theoretical guarantees, so the reviewer’s claim that the novelty is reduced is only partially accurate.", "evidence": "“Our approach is inspired by previous methods that cast the computation of an equilibrium in normalform games as an unsupervised learning problem (Duan et al., 2021a; Marris et al., 2022). These methods train a network to predict a strategy profile that minimizes the exploitability ... These methods become inefficient in pseudo-games, since ... the exploitability in pseudo-games (1) requires solving a non-linear optimization problem, (2) is not Lipschitz-continuous, ... and (3) has unbounded gradients ... By delegating the task of computing a best-response to a discriminator, our method circumvents the issue of solving a convex program, yielding a training problem given by a min-max optimization problem whose objective is Lipschitz-continuous, for which gradients can be guaranteed to be bounded under standard assumptions ... We prove polynomial-time convergence of our training algorithm ... and provide generalization bounds ...”", "section": "1 INTRODUCTION"}
{"claim": "The paper does not provide a detailed empirical or theoretical comparison with the 'Exploitability minimization' algorithm.", "claim_type": "baseline", "paper_id": "TlyiaPXaVN", "paper_title": "Generative Adversarial Equilibrium Solvers", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "rCNKi8T07D", "reviewer": "Reviewer_Wcqa", "review_text": "Summary: The paper introduces generative adversarial equilibrium solvers (GAES), a GAN that learns to map pseudo-games to their generalized Nash equilibria from a sample of problem instances. In particular, they provide a formulation that makes the problem amenable to standard stochastic first-order methods. They use GAES to compute in a scalable way competitive equilibria in exchange economies and an environmental economic model of the Kyoto mechanism, outperforming earlier methods.\n\nStrengths: Pseudo-games are very general game-theoretic models with a number of applications, most notably Arrow-Debreu competitive economies. Yet, there is a lack of scalable techniques for computing generalized Nash equilibria in such settings. This paper makes a concrete contribution in that direction by providing a method with promising performance across a number of benchmarks. The proposed method is natural and the experimental results are overall convincing and quite thorough. Indeed, the paper appears to attain state of the art performance in a number of important applications, and could have significant impact in this area.\n\nWeaknesses: There are some soundness issues that the authors have to address. First, there appears to be a significant gap between the theoretical analysis and the experimental settings. Specifically, it is not clear how a stationary point in the sense of Theorem 4.1 translates to a GNE. If stationary points are not necessarily GNE, the narrative of the paper has to be restructured. In particular, it is often claimed that the method maps pseudo-games to GNE, and it is not clear whether that claim is theoretically sound. Of course, computing GNE is intractable, but it is alluded (for example in the abstract) that under a distribution over problem instances the problem could be easier. Theorem 4.1 also makes a strong concavity assumption which appears to be violated in all settings of interest. It should be the case that a \"small\" regularizer can always be incorporated without affecting the equilibria by much, but I think this should be discussed in more detail. \n\nI am also confused about Footnote 4. It is claimed that the method obtains the state of the art $O(1/\\epsilon^3)$ complexity, a major improvement over $O(1/\\epsilon^6)$, which the authors claim was the previous state of the art. The authors have to explain more precisely the class of problems this applies to; there are many variants of the PL condition studied in the literature. In particular, the following papers seem to obtain a much better dependency: \"Faster single-loop algorithms for minimax optimization without strong concavity,\" \"faster stochastic algoritms for minimax optimization under Polyak-Lojasiewicz Conditions\" and \"Doubly smoothed GDA for constrained nonconvex-nonconcave minimax optimization.\"\n\nBesides the issues above, the algorithmic approach is very close to the paper \"Exploitability minimization in games and beyond,\" which limits to some extent the algorithmic contribution of the present paper. The authors have to highlight the comparisons in more detail.\n\nQuestions: Some minor comments for the authors:\n\n1. The title of the submission document is the default one.\n2. The references have to be polished. There are many papers that are published many years ago and only the arXiv version is cited. There is also an issue with consistency: sometimes URLs are included, sometimes they are not. Please fix those issues.\n3. There are underfull equations in Observation 1 and immediately below.\n4. I don't understand how the paper of Daskalakis et al. (2009) is relevant in the context of Footnote 4 about min-max optimization.\n5. The appendix has many overfull equations that need to be formatted properly.", "labeling_timestamp": "2026-01-11T16:47:27.457509", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper does discuss and theoretically critique prior 'exploitability minimization' approaches (citing Duan et al. (2021a) and Marris et al. (2022)) and explains why those methods fail to extend to pseudo-games (non-Lipschitz exploitability, unbounded gradients). However, the provided content does not show a detailed empirical comparison specifically against an 'Exploitability minimization' algorithm—only a general statement that GAES \"outperforms state of the art baselines\" without naming or detailing that specific baseline—so the claim is only partially accurate.", "evidence": "“Earlier approaches Duan et al. (2021a); Marris et al. (2022) do not extend even to continuous (non pseudo-)games, since evaluating the expected exploitability and its gradient over a distribution of pseudo-games requires solving as many convex programs as examples in the data set. Additionally, in pseudogames, the exploitability is not Lipschitzcontinuous, and thus its gradient is unbounded (Appendix D), hindering the use of standard ...”\n\n“Finally, we provide empirical evidence that GAES outperforms state of the art baselines in Arrow-Debreu competitive economies, and show that GAES can replicate existing qualitative analyses for pseudo-games...”", "section": "1 INTRODUCTION (Contributions paragraph)"}
{"claim": "The suggestion that computing GNE is easier under a distribution over problem instances is asserted without sufficient justification.", "claim_type": "subjective", "paper_id": "TlyiaPXaVN", "paper_title": "Generative Adversarial Equilibrium Solvers", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "rCNKi8T07D", "reviewer": "Reviewer_Wcqa", "review_text": "Summary: The paper introduces generative adversarial equilibrium solvers (GAES), a GAN that learns to map pseudo-games to their generalized Nash equilibria from a sample of problem instances. In particular, they provide a formulation that makes the problem amenable to standard stochastic first-order methods. They use GAES to compute in a scalable way competitive equilibria in exchange economies and an environmental economic model of the Kyoto mechanism, outperforming earlier methods.\n\nStrengths: Pseudo-games are very general game-theoretic models with a number of applications, most notably Arrow-Debreu competitive economies. Yet, there is a lack of scalable techniques for computing generalized Nash equilibria in such settings. This paper makes a concrete contribution in that direction by providing a method with promising performance across a number of benchmarks. The proposed method is natural and the experimental results are overall convincing and quite thorough. Indeed, the paper appears to attain state of the art performance in a number of important applications, and could have significant impact in this area.\n\nWeaknesses: There are some soundness issues that the authors have to address. First, there appears to be a significant gap between the theoretical analysis and the experimental settings. Specifically, it is not clear how a stationary point in the sense of Theorem 4.1 translates to a GNE. If stationary points are not necessarily GNE, the narrative of the paper has to be restructured. In particular, it is often claimed that the method maps pseudo-games to GNE, and it is not clear whether that claim is theoretically sound. Of course, computing GNE is intractable, but it is alluded (for example in the abstract) that under a distribution over problem instances the problem could be easier. Theorem 4.1 also makes a strong concavity assumption which appears to be violated in all settings of interest. It should be the case that a \"small\" regularizer can always be incorporated without affecting the equilibria by much, but I think this should be discussed in more detail. \n\nI am also confused about Footnote 4. It is claimed that the method obtains the state of the art $O(1/\\epsilon^3)$ complexity, a major improvement over $O(1/\\epsilon^6)$, which the authors claim was the previous state of the art. The authors have to explain more precisely the class of problems this applies to; there are many variants of the PL condition studied in the literature. In particular, the following papers seem to obtain a much better dependency: \"Faster single-loop algorithms for minimax optimization without strong concavity,\" \"faster stochastic algoritms for minimax optimization under Polyak-Lojasiewicz Conditions\" and \"Doubly smoothed GDA for constrained nonconvex-nonconcave minimax optimization.\"\n\nBesides the issues above, the algorithmic approach is very close to the paper \"Exploitability minimization in games and beyond,\" which limits to some extent the algorithmic contribution of the present paper. The authors have to highlight the comparisons in more detail.\n\nQuestions: Some minor comments for the authors:\n\n1. The title of the submission document is the default one.\n2. The references have to be polished. There are many papers that are published many years ago and only the arXiv version is cited. There is also an issue with consistency: sometimes URLs are included, sometimes they are not. Please fix those issues.\n3. There are underfull equations in Observation 1 and immediately below.\n4. I don't understand how the paper of Daskalakis et al. (2009) is relevant in the context of Footnote 4 about min-max optimization.\n5. The appendix has many overfull equations that need to be formatted properly.", "labeling_timestamp": "2026-01-11T16:47:44.479687", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly acknowledges worst-case intractability but provides theoretical justification for solving GNE in expectation over a distribution: it states computational and sample complexity bounds and cites Theorems 4.1 and 4.2 proving polynomial-time convergence and generalization bounds under stated assumptions. Thus the claim that this suggestion is asserted without sufficient justification is contradicted by the paper.", "evidence": "1) \"Although the computation of GNE and CE is intractable in the worst-case, i.e., PPAD-hard, in practice, many applications only require solutions with high accuracy in expectation over a distribution of problem instances.\" (Abstract). 2) \"We provide computational and sample complexity bounds for Lipschitz-smooth function approximators in a large class of concave pseudo-games...\" (Abstract). 3) \"We prove polynomial-time convergence of our training algorithm for arbitrary Lipschitz-smooth function approximators with the joint action space dimensions larger than parameter space dimensions (Theorem 4.1, Section 4). and provide generalization bounds for arbitrary function approximators (Theorem 4.2, Section 4).\" (Contributions, 1 INTRODUCTION).", "section": "Abstract; 1 INTRODUCTION (Contributions)"}
{"claim": "The authors do not clarify which variant of the Polyak-Łojasiewicz (PL) condition or related assumptions their complexity result relies on.", "claim_type": "methodology", "paper_id": "TlyiaPXaVN", "paper_title": "Generative Adversarial Equilibrium Solvers", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "rCNKi8T07D", "reviewer": "Reviewer_Wcqa", "review_text": "Summary: The paper introduces generative adversarial equilibrium solvers (GAES), a GAN that learns to map pseudo-games to their generalized Nash equilibria from a sample of problem instances. In particular, they provide a formulation that makes the problem amenable to standard stochastic first-order methods. They use GAES to compute in a scalable way competitive equilibria in exchange economies and an environmental economic model of the Kyoto mechanism, outperforming earlier methods.\n\nStrengths: Pseudo-games are very general game-theoretic models with a number of applications, most notably Arrow-Debreu competitive economies. Yet, there is a lack of scalable techniques for computing generalized Nash equilibria in such settings. This paper makes a concrete contribution in that direction by providing a method with promising performance across a number of benchmarks. The proposed method is natural and the experimental results are overall convincing and quite thorough. Indeed, the paper appears to attain state of the art performance in a number of important applications, and could have significant impact in this area.\n\nWeaknesses: There are some soundness issues that the authors have to address. First, there appears to be a significant gap between the theoretical analysis and the experimental settings. Specifically, it is not clear how a stationary point in the sense of Theorem 4.1 translates to a GNE. If stationary points are not necessarily GNE, the narrative of the paper has to be restructured. In particular, it is often claimed that the method maps pseudo-games to GNE, and it is not clear whether that claim is theoretically sound. Of course, computing GNE is intractable, but it is alluded (for example in the abstract) that under a distribution over problem instances the problem could be easier. Theorem 4.1 also makes a strong concavity assumption which appears to be violated in all settings of interest. It should be the case that a \"small\" regularizer can always be incorporated without affecting the equilibria by much, but I think this should be discussed in more detail. \n\nI am also confused about Footnote 4. It is claimed that the method obtains the state of the art $O(1/\\epsilon^3)$ complexity, a major improvement over $O(1/\\epsilon^6)$, which the authors claim was the previous state of the art. The authors have to explain more precisely the class of problems this applies to; there are many variants of the PL condition studied in the literature. In particular, the following papers seem to obtain a much better dependency: \"Faster single-loop algorithms for minimax optimization without strong concavity,\" \"faster stochastic algoritms for minimax optimization under Polyak-Lojasiewicz Conditions\" and \"Doubly smoothed GDA for constrained nonconvex-nonconcave minimax optimization.\"\n\nBesides the issues above, the algorithmic approach is very close to the paper \"Exploitability minimization in games and beyond,\" which limits to some extent the algorithmic contribution of the present paper. The authors have to highlight the comparisons in more detail.\n\nQuestions: Some minor comments for the authors:\n\n1. The title of the submission document is the default one.\n2. The references have to be polished. There are many papers that are published many years ago and only the arXiv version is cited. There is also an issue with consistency: sometimes URLs are included, sometimes they are not. Please fix those issues.\n3. There are underfull equations in Observation 1 and immediately below.\n4. I don't understand how the paper of Daskalakis et al. (2009) is relevant in the context of Footnote 4 about min-max optimization.\n5. The appendix has many overfull equations that need to be formatted properly.", "labeling_timestamp": "2026-01-11T16:48:22.184413", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper states polynomial-time convergence under Lipschitz-smooth and bounded-gradient assumptions (see Contributions / Introduction) but does not mention the Polyak-Łojasiewicz condition or specify any variant of PL or related PL-type assumptions for the complexity result. The referenced theorem (Theorem 4.1) is cited but no PL condition is clarified in the provided text.", "evidence": "“We prove polynomial-time convergence of our training algorithm for arbitrary Lipschitz-smooth function approximators with the joint action space dimensions larger than parameter space dimensions (Theorem 4.1, Section 4).” || “...yielding a training problem given by a min-max optimization problem whose objective is Lipschitz-continuous, for which gradients can be guaranteed to be bounded under standard assumptions on the discriminator and the payoffs of players.”", "section": "Introduction (Contributions)"}
{"claim": "The relevance of Daskalakis et al. (2009) to the min-max optimization discussion in Footnote 4 is unclear and unexplained.", "claim_type": "presentation", "paper_id": "TlyiaPXaVN", "paper_title": "Generative Adversarial Equilibrium Solvers", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "rCNKi8T07D", "reviewer": "Reviewer_Wcqa", "review_text": "Summary: The paper introduces generative adversarial equilibrium solvers (GAES), a GAN that learns to map pseudo-games to their generalized Nash equilibria from a sample of problem instances. In particular, they provide a formulation that makes the problem amenable to standard stochastic first-order methods. They use GAES to compute in a scalable way competitive equilibria in exchange economies and an environmental economic model of the Kyoto mechanism, outperforming earlier methods.\n\nStrengths: Pseudo-games are very general game-theoretic models with a number of applications, most notably Arrow-Debreu competitive economies. Yet, there is a lack of scalable techniques for computing generalized Nash equilibria in such settings. This paper makes a concrete contribution in that direction by providing a method with promising performance across a number of benchmarks. The proposed method is natural and the experimental results are overall convincing and quite thorough. Indeed, the paper appears to attain state of the art performance in a number of important applications, and could have significant impact in this area.\n\nWeaknesses: There are some soundness issues that the authors have to address. First, there appears to be a significant gap between the theoretical analysis and the experimental settings. Specifically, it is not clear how a stationary point in the sense of Theorem 4.1 translates to a GNE. If stationary points are not necessarily GNE, the narrative of the paper has to be restructured. In particular, it is often claimed that the method maps pseudo-games to GNE, and it is not clear whether that claim is theoretically sound. Of course, computing GNE is intractable, but it is alluded (for example in the abstract) that under a distribution over problem instances the problem could be easier. Theorem 4.1 also makes a strong concavity assumption which appears to be violated in all settings of interest. It should be the case that a \"small\" regularizer can always be incorporated without affecting the equilibria by much, but I think this should be discussed in more detail. \n\nI am also confused about Footnote 4. It is claimed that the method obtains the state of the art $O(1/\\epsilon^3)$ complexity, a major improvement over $O(1/\\epsilon^6)$, which the authors claim was the previous state of the art. The authors have to explain more precisely the class of problems this applies to; there are many variants of the PL condition studied in the literature. In particular, the following papers seem to obtain a much better dependency: \"Faster single-loop algorithms for minimax optimization without strong concavity,\" \"faster stochastic algoritms for minimax optimization under Polyak-Lojasiewicz Conditions\" and \"Doubly smoothed GDA for constrained nonconvex-nonconcave minimax optimization.\"\n\nBesides the issues above, the algorithmic approach is very close to the paper \"Exploitability minimization in games and beyond,\" which limits to some extent the algorithmic contribution of the present paper. The authors have to highlight the comparisons in more detail.\n\nQuestions: Some minor comments for the authors:\n\n1. The title of the submission document is the default one.\n2. The references have to be polished. There are many papers that are published many years ago and only the arXiv version is cited. There is also an issue with consistency: sometimes URLs are included, sometimes they are not. Please fix those issues.\n3. There are underfull equations in Observation 1 and immediately below.\n4. I don't understand how the paper of Daskalakis et al. (2009) is relevant in the context of Footnote 4 about min-max optimization.\n5. The appendix has many overfull equations that need to be formatted properly.", "labeling_timestamp": "2026-01-11T16:47:50.300033", "model": "gpt-5-mini", "label": "Supported", "justification": "In the Introduction the paper cites Daskalakis et al. (2009) only as part of a list of worst-case hardness results for computing GNE/CE, while the min–max/GAN training discussion appears separately and there is no explanation or footnote connecting Daskalakis et al. (2009) to the min–max optimization point. Thus the reviewer’s observation that the relevance is unclear and unexplained is accurate.", "evidence": "\"Although the computation of GNE is intractable in the worst-case (Chen & Deng, 2006; Daskalakis et al., 2009; Chen & Teng, 2009; Vazirani & Yannakakis, 2011; Garg et al., 2017), in practice, applications may only require a solver that gives solutions with high accuracy in expectation over a realistic distribution of problem instances.\" \n\n\"By delegating the task of computing a best-response to a discriminator, our method circumvents the issue of solving a convex program, yielding a training problem given by a min-max optimization problem whose objective is Lipschitz-continuous, for which gradients can be guaranteed to be bounded under standard assumptions on the discriminator and the payoffs of players.\"", "section": "1 INTRODUCTION"}
{"claim": "The submission document uses the default title, indicating the manuscript title was not updated before submission.", "claim_type": "presentation", "paper_id": "TlyiaPXaVN", "paper_title": "Generative Adversarial Equilibrium Solvers", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "rCNKi8T07D", "reviewer": "Reviewer_Wcqa", "review_text": "Summary: The paper introduces generative adversarial equilibrium solvers (GAES), a GAN that learns to map pseudo-games to their generalized Nash equilibria from a sample of problem instances. In particular, they provide a formulation that makes the problem amenable to standard stochastic first-order methods. They use GAES to compute in a scalable way competitive equilibria in exchange economies and an environmental economic model of the Kyoto mechanism, outperforming earlier methods.\n\nStrengths: Pseudo-games are very general game-theoretic models with a number of applications, most notably Arrow-Debreu competitive economies. Yet, there is a lack of scalable techniques for computing generalized Nash equilibria in such settings. This paper makes a concrete contribution in that direction by providing a method with promising performance across a number of benchmarks. The proposed method is natural and the experimental results are overall convincing and quite thorough. Indeed, the paper appears to attain state of the art performance in a number of important applications, and could have significant impact in this area.\n\nWeaknesses: There are some soundness issues that the authors have to address. First, there appears to be a significant gap between the theoretical analysis and the experimental settings. Specifically, it is not clear how a stationary point in the sense of Theorem 4.1 translates to a GNE. If stationary points are not necessarily GNE, the narrative of the paper has to be restructured. In particular, it is often claimed that the method maps pseudo-games to GNE, and it is not clear whether that claim is theoretically sound. Of course, computing GNE is intractable, but it is alluded (for example in the abstract) that under a distribution over problem instances the problem could be easier. Theorem 4.1 also makes a strong concavity assumption which appears to be violated in all settings of interest. It should be the case that a \"small\" regularizer can always be incorporated without affecting the equilibria by much, but I think this should be discussed in more detail. \n\nI am also confused about Footnote 4. It is claimed that the method obtains the state of the art $O(1/\\epsilon^3)$ complexity, a major improvement over $O(1/\\epsilon^6)$, which the authors claim was the previous state of the art. The authors have to explain more precisely the class of problems this applies to; there are many variants of the PL condition studied in the literature. In particular, the following papers seem to obtain a much better dependency: \"Faster single-loop algorithms for minimax optimization without strong concavity,\" \"faster stochastic algoritms for minimax optimization under Polyak-Lojasiewicz Conditions\" and \"Doubly smoothed GDA for constrained nonconvex-nonconcave minimax optimization.\"\n\nBesides the issues above, the algorithmic approach is very close to the paper \"Exploitability minimization in games and beyond,\" which limits to some extent the algorithmic contribution of the present paper. The authors have to highlight the comparisons in more detail.\n\nQuestions: Some minor comments for the authors:\n\n1. The title of the submission document is the default one.\n2. The references have to be polished. There are many papers that are published many years ago and only the arXiv version is cited. There is also an issue with consistency: sometimes URLs are included, sometimes they are not. Please fix those issues.\n3. There are underfull equations in Observation 1 and immediately below.\n4. I don't understand how the paper of Daskalakis et al. (2009) is relevant in the context of Footnote 4 about min-max optimization.\n5. The appendix has many overfull equations that need to be formatted properly.", "labeling_timestamp": "2026-01-11T16:47:35.633165", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper clearly displays a specific, non-default title at the start: 'GENERATIVE ADVERSARIAL EQUILIBRIUM SOLVERS', so the claim that a default title was used is false according to the submission.", "evidence": "## GENERATIVE ADVERSARIAL EQUILIBRIUM SOLVERS", "section": "Title / Header (beginning of document)"}
{"claim": "The references are inconsistent and include many older papers cited only in arXiv form without uniform citation formatting or URLs.", "claim_type": "presentation", "paper_id": "TlyiaPXaVN", "paper_title": "Generative Adversarial Equilibrium Solvers", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "rCNKi8T07D", "reviewer": "Reviewer_Wcqa", "review_text": "Summary: The paper introduces generative adversarial equilibrium solvers (GAES), a GAN that learns to map pseudo-games to their generalized Nash equilibria from a sample of problem instances. In particular, they provide a formulation that makes the problem amenable to standard stochastic first-order methods. They use GAES to compute in a scalable way competitive equilibria in exchange economies and an environmental economic model of the Kyoto mechanism, outperforming earlier methods.\n\nStrengths: Pseudo-games are very general game-theoretic models with a number of applications, most notably Arrow-Debreu competitive economies. Yet, there is a lack of scalable techniques for computing generalized Nash equilibria in such settings. This paper makes a concrete contribution in that direction by providing a method with promising performance across a number of benchmarks. The proposed method is natural and the experimental results are overall convincing and quite thorough. Indeed, the paper appears to attain state of the art performance in a number of important applications, and could have significant impact in this area.\n\nWeaknesses: There are some soundness issues that the authors have to address. First, there appears to be a significant gap between the theoretical analysis and the experimental settings. Specifically, it is not clear how a stationary point in the sense of Theorem 4.1 translates to a GNE. If stationary points are not necessarily GNE, the narrative of the paper has to be restructured. In particular, it is often claimed that the method maps pseudo-games to GNE, and it is not clear whether that claim is theoretically sound. Of course, computing GNE is intractable, but it is alluded (for example in the abstract) that under a distribution over problem instances the problem could be easier. Theorem 4.1 also makes a strong concavity assumption which appears to be violated in all settings of interest. It should be the case that a \"small\" regularizer can always be incorporated without affecting the equilibria by much, but I think this should be discussed in more detail. \n\nI am also confused about Footnote 4. It is claimed that the method obtains the state of the art $O(1/\\epsilon^3)$ complexity, a major improvement over $O(1/\\epsilon^6)$, which the authors claim was the previous state of the art. The authors have to explain more precisely the class of problems this applies to; there are many variants of the PL condition studied in the literature. In particular, the following papers seem to obtain a much better dependency: \"Faster single-loop algorithms for minimax optimization without strong concavity,\" \"faster stochastic algoritms for minimax optimization under Polyak-Lojasiewicz Conditions\" and \"Doubly smoothed GDA for constrained nonconvex-nonconcave minimax optimization.\"\n\nBesides the issues above, the algorithmic approach is very close to the paper \"Exploitability minimization in games and beyond,\" which limits to some extent the algorithmic contribution of the present paper. The authors have to highlight the comparisons in more detail.\n\nQuestions: Some minor comments for the authors:\n\n1. The title of the submission document is the default one.\n2. The references have to be polished. There are many papers that are published many years ago and only the arXiv version is cited. There is also an issue with consistency: sometimes URLs are included, sometimes they are not. Please fix those issues.\n3. There are underfull equations in Observation 1 and immediately below.\n4. I don't understand how the paper of Daskalakis et al. (2009) is relevant in the context of Footnote 4 about min-max optimization.\n5. The appendix has many overfull equations that need to be formatted properly.", "labeling_timestamp": "2026-01-11T16:47:51.009812", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt contains inline citations (e.g., 'Han et al., 2011; Pang et al., 2008') and some formatting artifacts in the author block, but the full references list is not included in the excerpt. Therefore there is insufficient information to verify whether references are inconsistent or whether many older papers are cited only in arXiv form without uniform formatting or URLs.", "evidence": "“Economic models and equilibrium concepts ... (Han et al., 2011; Pang et al., 2008), energy resource allocation (Hobbs & Pang, 2007; Jing-Yuan & Smeers, 1999), and cloud computing (Gutman & Nisan, 2012; Lai et al., 2005; Zahedi et al., 2018; Ardagna et al., 2017).”", "section": "Introduction"}
{"claim": "Observation 1 and the immediately following equations contain underfull equation formatting that needs correction.", "claim_type": "presentation", "paper_id": "TlyiaPXaVN", "paper_title": "Generative Adversarial Equilibrium Solvers", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "rCNKi8T07D", "reviewer": "Reviewer_Wcqa", "review_text": "Summary: The paper introduces generative adversarial equilibrium solvers (GAES), a GAN that learns to map pseudo-games to their generalized Nash equilibria from a sample of problem instances. In particular, they provide a formulation that makes the problem amenable to standard stochastic first-order methods. They use GAES to compute in a scalable way competitive equilibria in exchange economies and an environmental economic model of the Kyoto mechanism, outperforming earlier methods.\n\nStrengths: Pseudo-games are very general game-theoretic models with a number of applications, most notably Arrow-Debreu competitive economies. Yet, there is a lack of scalable techniques for computing generalized Nash equilibria in such settings. This paper makes a concrete contribution in that direction by providing a method with promising performance across a number of benchmarks. The proposed method is natural and the experimental results are overall convincing and quite thorough. Indeed, the paper appears to attain state of the art performance in a number of important applications, and could have significant impact in this area.\n\nWeaknesses: There are some soundness issues that the authors have to address. First, there appears to be a significant gap between the theoretical analysis and the experimental settings. Specifically, it is not clear how a stationary point in the sense of Theorem 4.1 translates to a GNE. If stationary points are not necessarily GNE, the narrative of the paper has to be restructured. In particular, it is often claimed that the method maps pseudo-games to GNE, and it is not clear whether that claim is theoretically sound. Of course, computing GNE is intractable, but it is alluded (for example in the abstract) that under a distribution over problem instances the problem could be easier. Theorem 4.1 also makes a strong concavity assumption which appears to be violated in all settings of interest. It should be the case that a \"small\" regularizer can always be incorporated without affecting the equilibria by much, but I think this should be discussed in more detail. \n\nI am also confused about Footnote 4. It is claimed that the method obtains the state of the art $O(1/\\epsilon^3)$ complexity, a major improvement over $O(1/\\epsilon^6)$, which the authors claim was the previous state of the art. The authors have to explain more precisely the class of problems this applies to; there are many variants of the PL condition studied in the literature. In particular, the following papers seem to obtain a much better dependency: \"Faster single-loop algorithms for minimax optimization without strong concavity,\" \"faster stochastic algoritms for minimax optimization under Polyak-Lojasiewicz Conditions\" and \"Doubly smoothed GDA for constrained nonconvex-nonconcave minimax optimization.\"\n\nBesides the issues above, the algorithmic approach is very close to the paper \"Exploitability minimization in games and beyond,\" which limits to some extent the algorithmic contribution of the present paper. The authors have to highlight the comparisons in more detail.\n\nQuestions: Some minor comments for the authors:\n\n1. The title of the submission document is the default one.\n2. The references have to be polished. There are many papers that are published many years ago and only the arXiv version is cited. There is also an issue with consistency: sometimes URLs are included, sometimes they are not. Please fix those issues.\n3. There are underfull equations in Observation 1 and immediately below.\n4. I don't understand how the paper of Daskalakis et al. (2009) is relevant in the context of Footnote 4 about min-max optimization.\n5. The appendix has many overfull equations that need to be formatted properly.", "labeling_timestamp": "2026-01-11T16:47:51.809610", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The supplied paper excerpt does not contain an \"Observation 1\" or the immediately following equations, so it is not possible to verify the reviewer's specific claim about underfull equation formatting for that item. While the excerpt exhibits other formatting/glyph issues (suggesting formatting problems elsewhere), the exact location referenced by the reviewer is absent from the provided content.", "evidence": "We denote the set of integers ¶ 0 ↪ glyph[triangleright] glyph[triangleright] glyph[triangleright] ↪ n -1 ♦ by [ n ] , the set of natural numbers by N , the set of real numbers by R , and the positive and strictly positive elements of a set by a subscript + and ++ , e.g., R + and R ++ .", "section": "2 PRELIMINARIES / Notation"}
{"claim": "The appendix contains many overfull equations that require proper formatting and layout fixes.", "claim_type": "presentation", "paper_id": "TlyiaPXaVN", "paper_title": "Generative Adversarial Equilibrium Solvers", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "rCNKi8T07D", "reviewer": "Reviewer_Wcqa", "review_text": "Summary: The paper introduces generative adversarial equilibrium solvers (GAES), a GAN that learns to map pseudo-games to their generalized Nash equilibria from a sample of problem instances. In particular, they provide a formulation that makes the problem amenable to standard stochastic first-order methods. They use GAES to compute in a scalable way competitive equilibria in exchange economies and an environmental economic model of the Kyoto mechanism, outperforming earlier methods.\n\nStrengths: Pseudo-games are very general game-theoretic models with a number of applications, most notably Arrow-Debreu competitive economies. Yet, there is a lack of scalable techniques for computing generalized Nash equilibria in such settings. This paper makes a concrete contribution in that direction by providing a method with promising performance across a number of benchmarks. The proposed method is natural and the experimental results are overall convincing and quite thorough. Indeed, the paper appears to attain state of the art performance in a number of important applications, and could have significant impact in this area.\n\nWeaknesses: There are some soundness issues that the authors have to address. First, there appears to be a significant gap between the theoretical analysis and the experimental settings. Specifically, it is not clear how a stationary point in the sense of Theorem 4.1 translates to a GNE. If stationary points are not necessarily GNE, the narrative of the paper has to be restructured. In particular, it is often claimed that the method maps pseudo-games to GNE, and it is not clear whether that claim is theoretically sound. Of course, computing GNE is intractable, but it is alluded (for example in the abstract) that under a distribution over problem instances the problem could be easier. Theorem 4.1 also makes a strong concavity assumption which appears to be violated in all settings of interest. It should be the case that a \"small\" regularizer can always be incorporated without affecting the equilibria by much, but I think this should be discussed in more detail. \n\nI am also confused about Footnote 4. It is claimed that the method obtains the state of the art $O(1/\\epsilon^3)$ complexity, a major improvement over $O(1/\\epsilon^6)$, which the authors claim was the previous state of the art. The authors have to explain more precisely the class of problems this applies to; there are many variants of the PL condition studied in the literature. In particular, the following papers seem to obtain a much better dependency: \"Faster single-loop algorithms for minimax optimization without strong concavity,\" \"faster stochastic algoritms for minimax optimization under Polyak-Lojasiewicz Conditions\" and \"Doubly smoothed GDA for constrained nonconvex-nonconcave minimax optimization.\"\n\nBesides the issues above, the algorithmic approach is very close to the paper \"Exploitability minimization in games and beyond,\" which limits to some extent the algorithmic contribution of the present paper. The authors have to highlight the comparisons in more detail.\n\nQuestions: Some minor comments for the authors:\n\n1. The title of the submission document is the default one.\n2. The references have to be polished. There are many papers that are published many years ago and only the arXiv version is cited. There is also an issue with consistency: sometimes URLs are included, sometimes they are not. Please fix those issues.\n3. There are underfull equations in Observation 1 and immediately below.\n4. I don't understand how the paper of Daskalakis et al. (2009) is relevant in the context of Footnote 4 about min-max optimization.\n5. The appendix has many overfull equations that need to be formatted properly.", "labeling_timestamp": "2026-01-11T16:47:57.777101", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt only refers to multiple appendices (A–E) but does not include the appendix content itself. Therefore there is insufficient information to verify whether the appendix contains overfull equations or needs formatting/layout fixes.", "evidence": "“We refer the reader to Appendix A for a mathematical example.”; “We refer the reader to Appendix B for a survey of methods to compute GNEs, and to Appendix C for a survey of applications of GNE.”", "section": "1 INTRODUCTION"}
{"claim": "It is unclear whether M and N are learned feature maps; the paper should visualize them to clarify what these maps look like.", "claim_type": "methodology", "paper_id": "yURca4wi2L", "paper_title": "Temporally Consistent Atmospheric Turbulence Mitigation with Neural Representations", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Gu6qnKVtKj", "reviewer": "Reviewer_DNNj", "review_text": "Comment: Thank you for the rebuttal, it satisfied most of my concerns. \n\nOne remaining question: I am still confused about M and N. Are they learned feature maps (optimized)? Perhaps you can visualize them as you did C, so the reader has a better understanding of what these maps look like (probably good for supplemental material). \n\nA few follow-ups for the final camera-ready paper (if accepted): \n\n1) I think it is important that the paper acknowledge that the Hadamard product decomposition is inspired by other papers (such as [1-3]) by explicitly putting those references and sentence into a revised version of the paper. \n\n2) Visualizing C in the main paper greatly assists the reader in understanding what this map is. I would endeavor to include it in one of your figures, or as a small additional figure. Your text description should also be incorporated so that the reader can more clearly understand C. Also if M and N are able to be visualized (see question above), it would be good to point readers to understand those feature maps as well. \n\n3) If the loss function only adds 0.15dB, please revise the language in that section to say a \"modest\" improvement. It is not a major difference, and do not want to overclaim the contributions of that one loss function. If you want to keep the original language, then I still think a qualitative figure with and without the loss function is needed to convince readers the 0.15dB is significant. \n\n4) For the wall-clock latency time, I think it would be good to breakdown how much time was due to DATUM (which is not your method), and how much was due to your optimizations. That gives the readers some understanding that if they add your module to their base ATM method, this is the expected overhead. Please add this discussion to either the main paper or the supplemental material.", "labeling_timestamp": "2026-01-11T16:48:16.164380", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly refers to a \"Canonical Spatial Feature Map (C)\" and calls it a \"trainable feature map\", and it describes regularizing the temporal feature map, which implies these maps are learned. However, the excerpt does not mention symbols M and N anywhere, nor does it include any visualization of these feature maps. Thus the reviewer's point that it is unclear (and that visualization would help) is partially true: the paper indicates the maps are learned but does not provide the requested visual clarification.", "evidence": "1) \"S field queries the Canonical Spatial Feature Map ( C ) at the modified location ( x +∆ x, y +∆ y ) to retrieve the corresponding feature from a trainable feature map.\" 2) \"Regularization is applied by constraining the dimensions of the Temporal Feature Map. Similarly, reducing the size of the deformation MLP serves as additional regularization to promote temporal consistency.\"", "section": "3.1 Overview of the Pipeline (and Figure 4 caption)"}
{"claim": "The paper fails to explicitly acknowledge prior work that inspired the Hadamard product decomposition and should add the appropriate citations.", "claim_type": "subjective", "paper_id": "yURca4wi2L", "paper_title": "Temporally Consistent Atmospheric Turbulence Mitigation with Neural Representations", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Gu6qnKVtKj", "reviewer": "Reviewer_DNNj", "review_text": "Comment: Thank you for the rebuttal, it satisfied most of my concerns. \n\nOne remaining question: I am still confused about M and N. Are they learned feature maps (optimized)? Perhaps you can visualize them as you did C, so the reader has a better understanding of what these maps look like (probably good for supplemental material). \n\nA few follow-ups for the final camera-ready paper (if accepted): \n\n1) I think it is important that the paper acknowledge that the Hadamard product decomposition is inspired by other papers (such as [1-3]) by explicitly putting those references and sentence into a revised version of the paper. \n\n2) Visualizing C in the main paper greatly assists the reader in understanding what this map is. I would endeavor to include it in one of your figures, or as a small additional figure. Your text description should also be incorporated so that the reader can more clearly understand C. Also if M and N are able to be visualized (see question above), it would be good to point readers to understand those feature maps as well. \n\n3) If the loss function only adds 0.15dB, please revise the language in that section to say a \"modest\" improvement. It is not a major difference, and do not want to overclaim the contributions of that one loss function. If you want to keep the original language, then I still think a qualitative figure with and without the loss function is needed to convince readers the 0.15dB is significant. \n\n4) For the wall-clock latency time, I think it would be good to breakdown how much time was due to DATUM (which is not your method), and how much was due to your optimizations. That gives the readers some understanding that if they add your module to their base ATM method, this is the expected overhead. Please add this discussion to either the main paper or the supplemental material.", "labeling_timestamp": "2026-01-11T16:48:05.663971", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper text does not mention a 'Hadamard product decomposition' specifically. It does state the method was 'inspired by a series of works on tensor decomposition' and cites related works (33; 58; 59), but there is no explicit discussion of a Hadamard decomposition or which prior work inspired such a decomposition, so the claim cannot be confirmed from the available content.", "evidence": "Design Inspiration. As discussed in Section 1.2, the core design logic of ConVRT is to apply temporal-wise regularization in video representation learning. For the representation, our method is inspired by a series of works on tensor decomposition, commonly used to parameterize 3D volumes in implicit neural representations (INR). These approaches enhance the ability to represent 3D signals while reducing the number of required parameters (33; 58; 59).", "section": "3.1 Overview of the Pipeline (Design Inspiration); 2 Related Work (Implicit neural representations / Neural video representation)"}
{"claim": "The main paper does not include a visualization of C, which hinders reader understanding and should be added to a figure.", "claim_type": "presentation", "paper_id": "yURca4wi2L", "paper_title": "Temporally Consistent Atmospheric Turbulence Mitigation with Neural Representations", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Gu6qnKVtKj", "reviewer": "Reviewer_DNNj", "review_text": "Comment: Thank you for the rebuttal, it satisfied most of my concerns. \n\nOne remaining question: I am still confused about M and N. Are they learned feature maps (optimized)? Perhaps you can visualize them as you did C, so the reader has a better understanding of what these maps look like (probably good for supplemental material). \n\nA few follow-ups for the final camera-ready paper (if accepted): \n\n1) I think it is important that the paper acknowledge that the Hadamard product decomposition is inspired by other papers (such as [1-3]) by explicitly putting those references and sentence into a revised version of the paper. \n\n2) Visualizing C in the main paper greatly assists the reader in understanding what this map is. I would endeavor to include it in one of your figures, or as a small additional figure. Your text description should also be incorporated so that the reader can more clearly understand C. Also if M and N are able to be visualized (see question above), it would be good to point readers to understand those feature maps as well. \n\n3) If the loss function only adds 0.15dB, please revise the language in that section to say a \"modest\" improvement. It is not a major difference, and do not want to overclaim the contributions of that one loss function. If you want to keep the original language, then I still think a qualitative figure with and without the loss function is needed to convince readers the 0.15dB is significant. \n\n4) For the wall-clock latency time, I think it would be good to breakdown how much time was due to DATUM (which is not your method), and how much was due to your optimizations. That gives the readers some understanding that if they add your module to their base ATM method, this is the expected overhead. Please add this discussion to either the main paper or the supplemental material.", "labeling_timestamp": "2026-01-11T16:48:21.692962", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper references a Canonical Spatial Feature Map (C) in the method description (Section 3.1 / Figure 4) but the provided content does not include any figure or explicit visualization of C, supporting the claim that such a visualization is missing from the main paper.", "evidence": "“S field queries the Canonical Spatial Feature Map ( C ) at the modified location ( x +∆ x, y +∆ y ) to retrieve the corresponding feature from a trainable feature map.”", "section": "3.1 Overview of the Pipeline (Figure 4 description)"}
{"claim": "The paper lacks visualizations of M and N; if they are learnable feature maps, the authors should provide visual examples or supplemental figures.", "claim_type": "methodology", "paper_id": "yURca4wi2L", "paper_title": "Temporally Consistent Atmospheric Turbulence Mitigation with Neural Representations", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Gu6qnKVtKj", "reviewer": "Reviewer_DNNj", "review_text": "Comment: Thank you for the rebuttal, it satisfied most of my concerns. \n\nOne remaining question: I am still confused about M and N. Are they learned feature maps (optimized)? Perhaps you can visualize them as you did C, so the reader has a better understanding of what these maps look like (probably good for supplemental material). \n\nA few follow-ups for the final camera-ready paper (if accepted): \n\n1) I think it is important that the paper acknowledge that the Hadamard product decomposition is inspired by other papers (such as [1-3]) by explicitly putting those references and sentence into a revised version of the paper. \n\n2) Visualizing C in the main paper greatly assists the reader in understanding what this map is. I would endeavor to include it in one of your figures, or as a small additional figure. Your text description should also be incorporated so that the reader can more clearly understand C. Also if M and N are able to be visualized (see question above), it would be good to point readers to understand those feature maps as well. \n\n3) If the loss function only adds 0.15dB, please revise the language in that section to say a \"modest\" improvement. It is not a major difference, and do not want to overclaim the contributions of that one loss function. If you want to keep the original language, then I still think a qualitative figure with and without the loss function is needed to convince readers the 0.15dB is significant. \n\n4) For the wall-clock latency time, I think it would be good to breakdown how much time was due to DATUM (which is not your method), and how much was due to your optimizations. That gives the readers some understanding that if they add your module to their base ATM method, this is the expected overhead. Please add this discussion to either the main paper or the supplemental material.", "labeling_timestamp": "2026-01-11T16:48:30.228665", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly describes learnable feature maps (a 'Canonical Spatial Feature Map (C)' and a 'Temporal Feature Map') in Section 3.1 but the provided content contains no visualizations or figures showing these maps or their learned contents. Thus the reviewer's claim that visualizations of these feature maps are lacking is true according to the paper.", "evidence": "“S field queries the Canonical Spatial Feature Map ( C ) at the modified location ( x +∆ x, y +∆ y ) to retrieve the corresponding feature from a trainable feature map.”\n\n“Regularization is applied by constraining the dimensions of the Temporal Feature Map. Similarly, reducing the size of the deformation MLP serves as additional regularization to promote temporal consistency.”", "section": "3.1 Overview of the Pipeline (Video Representation) and Figure 4 caption"}
{"claim": "The textual description of C is not properly incorporated into the main figures or text and should be included to improve reader comprehension.", "claim_type": "presentation", "paper_id": "yURca4wi2L", "paper_title": "Temporally Consistent Atmospheric Turbulence Mitigation with Neural Representations", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Gu6qnKVtKj", "reviewer": "Reviewer_DNNj", "review_text": "Comment: Thank you for the rebuttal, it satisfied most of my concerns. \n\nOne remaining question: I am still confused about M and N. Are they learned feature maps (optimized)? Perhaps you can visualize them as you did C, so the reader has a better understanding of what these maps look like (probably good for supplemental material). \n\nA few follow-ups for the final camera-ready paper (if accepted): \n\n1) I think it is important that the paper acknowledge that the Hadamard product decomposition is inspired by other papers (such as [1-3]) by explicitly putting those references and sentence into a revised version of the paper. \n\n2) Visualizing C in the main paper greatly assists the reader in understanding what this map is. I would endeavor to include it in one of your figures, or as a small additional figure. Your text description should also be incorporated so that the reader can more clearly understand C. Also if M and N are able to be visualized (see question above), it would be good to point readers to understand those feature maps as well. \n\n3) If the loss function only adds 0.15dB, please revise the language in that section to say a \"modest\" improvement. It is not a major difference, and do not want to overclaim the contributions of that one loss function. If you want to keep the original language, then I still think a qualitative figure with and without the loss function is needed to convince readers the 0.15dB is significant. \n\n4) For the wall-clock latency time, I think it would be good to breakdown how much time was due to DATUM (which is not your method), and how much was due to your optimizations. That gives the readers some understanding that if they add your module to their base ATM method, this is the expected overhead. Please add this discussion to either the main paper or the supplemental material.", "labeling_timestamp": "2026-01-11T16:48:41.009486", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly defines and describes the Canonical Spatial Feature Map (C) in the main text of Section 3.1, explaining how the S field queries C at (x+Δx, y+Δy) to retrieve features. Thus the claim that C's textual description is not properly incorporated into the main text/figures is false.", "evidence": "\"S field queries the Canonical Spatial Feature Map ( C ) at the modified location ( x +∆ x, y +∆ y ) to retrieve the corresponding feature from a trainable feature map.\"", "section": "3.1 Overview of the Pipeline"}
{"claim": "The reported loss function improvement is only 0.15 dB, which is a modest improvement and should not be described as a major difference.", "claim_type": "quantitative", "paper_id": "yURca4wi2L", "paper_title": "Temporally Consistent Atmospheric Turbulence Mitigation with Neural Representations", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Gu6qnKVtKj", "reviewer": "Reviewer_DNNj", "review_text": "Comment: Thank you for the rebuttal, it satisfied most of my concerns. \n\nOne remaining question: I am still confused about M and N. Are they learned feature maps (optimized)? Perhaps you can visualize them as you did C, so the reader has a better understanding of what these maps look like (probably good for supplemental material). \n\nA few follow-ups for the final camera-ready paper (if accepted): \n\n1) I think it is important that the paper acknowledge that the Hadamard product decomposition is inspired by other papers (such as [1-3]) by explicitly putting those references and sentence into a revised version of the paper. \n\n2) Visualizing C in the main paper greatly assists the reader in understanding what this map is. I would endeavor to include it in one of your figures, or as a small additional figure. Your text description should also be incorporated so that the reader can more clearly understand C. Also if M and N are able to be visualized (see question above), it would be good to point readers to understand those feature maps as well. \n\n3) If the loss function only adds 0.15dB, please revise the language in that section to say a \"modest\" improvement. It is not a major difference, and do not want to overclaim the contributions of that one loss function. If you want to keep the original language, then I still think a qualitative figure with and without the loss function is needed to convince readers the 0.15dB is significant. \n\n4) For the wall-clock latency time, I think it would be good to breakdown how much time was due to DATUM (which is not your method), and how much was due to your optimizations. That gives the readers some understanding that if they add your module to their base ATM method, this is the expected overhead. Please add this discussion to either the main paper or the supplemental material.", "labeling_timestamp": "2026-01-11T16:48:34.411074", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper mentions a \"marginal\" per-frame improvement but does not report a numeric improvement of 0.15 dB anywhere in the provided content, so the reviewer's specific numeric claim cannot be verified from the paper.", "evidence": "\"Through extensive evaluations, we demonstrate that ConVRT substantially improves temporally consistency while also marginally improving per-frame restoration quality.\"", "section": "1.2 Motivation and Contribution"}
{"claim": "The paper does not provide a qualitative figure comparing results with and without the loss function to justify whether the 0.15 dB improvement is significant.", "claim_type": "experimental", "paper_id": "yURca4wi2L", "paper_title": "Temporally Consistent Atmospheric Turbulence Mitigation with Neural Representations", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Gu6qnKVtKj", "reviewer": "Reviewer_DNNj", "review_text": "Comment: Thank you for the rebuttal, it satisfied most of my concerns. \n\nOne remaining question: I am still confused about M and N. Are they learned feature maps (optimized)? Perhaps you can visualize them as you did C, so the reader has a better understanding of what these maps look like (probably good for supplemental material). \n\nA few follow-ups for the final camera-ready paper (if accepted): \n\n1) I think it is important that the paper acknowledge that the Hadamard product decomposition is inspired by other papers (such as [1-3]) by explicitly putting those references and sentence into a revised version of the paper. \n\n2) Visualizing C in the main paper greatly assists the reader in understanding what this map is. I would endeavor to include it in one of your figures, or as a small additional figure. Your text description should also be incorporated so that the reader can more clearly understand C. Also if M and N are able to be visualized (see question above), it would be good to point readers to understand those feature maps as well. \n\n3) If the loss function only adds 0.15dB, please revise the language in that section to say a \"modest\" improvement. It is not a major difference, and do not want to overclaim the contributions of that one loss function. If you want to keep the original language, then I still think a qualitative figure with and without the loss function is needed to convince readers the 0.15dB is significant. \n\n4) For the wall-clock latency time, I think it would be good to breakdown how much time was due to DATUM (which is not your method), and how much was due to your optimizations. That gives the readers some understanding that if they add your module to their base ATM method, this is the expected overhead. Please add this discussion to either the main paper or the supplemental material.", "labeling_timestamp": "2026-01-11T16:48:38.632158", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper content does not include the experiments/figures or an ablation comparing results with and without the loss function, nor does it report a specific 0.15 dB improvement in the sections shown. Therefore we cannot confirm the reviewer’s claim from the supplied excerpts.", "evidence": "\"Through extensive evaluations, we demonstrate that ConVRT substantially improves temporally consistency while also marginally improving per-frame restoration quality.\"", "section": "1.2 Motivation and Contribution"}
{"claim": "The wall-clock latency results do not break down how much runtime is due to DATUM versus the authors' optimizations.", "claim_type": "experimental", "paper_id": "yURca4wi2L", "paper_title": "Temporally Consistent Atmospheric Turbulence Mitigation with Neural Representations", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Gu6qnKVtKj", "reviewer": "Reviewer_DNNj", "review_text": "Comment: Thank you for the rebuttal, it satisfied most of my concerns. \n\nOne remaining question: I am still confused about M and N. Are they learned feature maps (optimized)? Perhaps you can visualize them as you did C, so the reader has a better understanding of what these maps look like (probably good for supplemental material). \n\nA few follow-ups for the final camera-ready paper (if accepted): \n\n1) I think it is important that the paper acknowledge that the Hadamard product decomposition is inspired by other papers (such as [1-3]) by explicitly putting those references and sentence into a revised version of the paper. \n\n2) Visualizing C in the main paper greatly assists the reader in understanding what this map is. I would endeavor to include it in one of your figures, or as a small additional figure. Your text description should also be incorporated so that the reader can more clearly understand C. Also if M and N are able to be visualized (see question above), it would be good to point readers to understand those feature maps as well. \n\n3) If the loss function only adds 0.15dB, please revise the language in that section to say a \"modest\" improvement. It is not a major difference, and do not want to overclaim the contributions of that one loss function. If you want to keep the original language, then I still think a qualitative figure with and without the loss function is needed to convince readers the 0.15dB is significant. \n\n4) For the wall-clock latency time, I think it would be good to breakdown how much time was due to DATUM (which is not your method), and how much was due to your optimizations. That gives the readers some understanding that if they add your module to their base ATM method, this is the expected overhead. Please add this discussion to either the main paper or the supplemental material.", "labeling_timestamp": "2026-01-11T16:48:51.474357", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt (Introduction, Related Work, and Method) does not include any wall-clock latency results or an experimental/runtime breakdown. Because the Experiments/Results section (where latency numbers and breakdowns would appear) is not present in the supplied content, there is insufficient information to confirm the reviewer's claim about whether the paper fails to break down runtime between DATUM and the authors' optimizations.", "evidence": "\"Through extensive evaluations, we demonstrate that ConVRT substantially improves temporally consistency while also marginally improving per-frame restoration quality.\"", "section": "1.2 Motivation and Contribution"}
{"claim": "Without a latency breakdown, readers cannot estimate the expected overhead of adding the authors' module to a base ATM method.", "claim_type": "experimental", "paper_id": "yURca4wi2L", "paper_title": "Temporally Consistent Atmospheric Turbulence Mitigation with Neural Representations", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Gu6qnKVtKj", "reviewer": "Reviewer_DNNj", "review_text": "Comment: Thank you for the rebuttal, it satisfied most of my concerns. \n\nOne remaining question: I am still confused about M and N. Are they learned feature maps (optimized)? Perhaps you can visualize them as you did C, so the reader has a better understanding of what these maps look like (probably good for supplemental material). \n\nA few follow-ups for the final camera-ready paper (if accepted): \n\n1) I think it is important that the paper acknowledge that the Hadamard product decomposition is inspired by other papers (such as [1-3]) by explicitly putting those references and sentence into a revised version of the paper. \n\n2) Visualizing C in the main paper greatly assists the reader in understanding what this map is. I would endeavor to include it in one of your figures, or as a small additional figure. Your text description should also be incorporated so that the reader can more clearly understand C. Also if M and N are able to be visualized (see question above), it would be good to point readers to understand those feature maps as well. \n\n3) If the loss function only adds 0.15dB, please revise the language in that section to say a \"modest\" improvement. It is not a major difference, and do not want to overclaim the contributions of that one loss function. If you want to keep the original language, then I still think a qualitative figure with and without the loss function is needed to convince readers the 0.15dB is significant. \n\n4) For the wall-clock latency time, I think it would be good to breakdown how much time was due to DATUM (which is not your method), and how much was due to your optimizations. That gives the readers some understanding that if they add your module to their base ATM method, this is the expected overhead. Please add this discussion to either the main paper or the supplemental material.", "labeling_timestamp": "2026-01-11T16:48:53.913649", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper does not provide any latency or runtime breakdown, only qualitative statements about computational cost and architectural choices. Because no timing, FLOPs, or runtime-overhead numbers are given for adding the proposed module to a base ATM method, readers cannot estimate the expected overhead from the paper text.", "evidence": "1) \"To address this temporal inconsistency issue, in addition to better simulators, enlarging the dataset and model capacity are necessary, which substantially increases the computational costs of training.\" 2) \"Regularization is applied by constraining the dimensions of the Temporal Feature Map. Similarly, reducing the size of the deformation MLP serves as additional regularization to promote temporal consistency.\"", "section": "1.1 Current State-of-the-Art in Atmospheric Turbulence Mitigation; 3.1 Overview of the Pipeline"}
{"claim": "The paper does not compare HydraLoRA with LoRA at higher ranks (r=16 or r=32), which would be a fairer baseline.", "claim_type": "baseline", "paper_id": "qEpi8uWX3N", "paper_title": "HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning", "paper_venue": "neurips2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "hA9JXs1a2I", "reviewer": "Reviewer_6P2g", "review_text": "Comment: I believe a fair comparison is with LoRA (r=16 or r=32), that's why the improvements are incremental. Since r=8 only has half of the HydraLoRA parameters used for finetuning. Or comparing under the same inference latency/compute budget.", "labeling_timestamp": "2026-01-11T16:49:03.439609", "model": "gpt-5-mini", "label": "Supported", "justification": "Table 1 (in Section 2.3) shows LoRA results at r=16 and r=32 but does not include any HydraLoRA rows or a direct comparison of HydraLoRA vs LoRA at those higher ranks in the provided content, so the claim that such a comparison is missing is accurate based on the shown material.", "evidence": "Table 1: Performance on instruction tuning with Dolly-15K [8] and evaluated with MMLU [16] with different ranks. ... | LoRA | 16 × 1 | 45.45 | 0.124 |  | LoRA | 32 × 1 | 46.59 | 0.248 |  | LoRA (Split) | 16 × 2 | 46.82 | 0.248 | ...", "section": "2.3 Observations / Table 1"}
{"claim": "The reported improvements over LoRA are only incremental, indicating insufficient performance gains.", "claim_type": "baseline", "paper_id": "qEpi8uWX3N", "paper_title": "HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning", "paper_venue": "neurips2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "hA9JXs1a2I", "reviewer": "Reviewer_6P2g", "review_text": "Comment: I believe a fair comparison is with LoRA (r=16 or r=32), that's why the improvements are incremental. Since r=8 only has half of the HydraLoRA parameters used for finetuning. Or comparing under the same inference latency/compute budget.", "labeling_timestamp": "2026-01-11T16:49:03.835902", "model": "gpt-5-mini", "label": "Supported", "justification": "Table 1 in the paper reports only small absolute MMLU improvements (on the order of ~0.2–0.4 points) for split/multi-head LoRA variants versus monolithic LoRA at the same parameter budget, which supports the reviewer's characterization of the gains as incremental.", "evidence": "Table 1: \"LoRA | 32 × 1 | 46.59 | 0.248\"; \"LoRA (Split) | 16 × 2 | 46.82 | 0.248\"; \"LoRA (Split) | 8 × 4 | 46.94 | 0.248\"", "section": "2.3 Observations / Table 1"}
{"claim": "Using LoRA with r=8 is unfair because it uses only half the parameters compared to HydraLoRA finetuning.", "claim_type": "quantitative", "paper_id": "qEpi8uWX3N", "paper_title": "HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning", "paper_venue": "neurips2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "hA9JXs1a2I", "reviewer": "Reviewer_6P2g", "review_text": "Comment: I believe a fair comparison is with LoRA (r=16 or r=32), that's why the improvements are incremental. Since r=8 only has half of the HydraLoRA parameters used for finetuning. Or comparing under the same inference latency/compute budget.", "labeling_timestamp": "2026-01-11T16:49:17.911996", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper reports parameter percentages for various LoRA ranks (e.g., r=8 uses 0.062% of parameters) but does not provide an explicit parameter-count or percentage for HydraLoRA in the provided text, so the claim that LoRA r=8 uses \"only half the parameters compared to HydraLoRA\" cannot be verified from the paper content shown.", "evidence": "Table 1: \"LoRA | 8 × 1 | 43.22 | 0.062\"; and \"From the architecture perspective, unlike LoRA's symmetric structure, HydraLoRA has an asymmetric structure that has a shared A matrix and multiple B matrices (see Figure 1(c)).\" Also: \"The matrics B_i ∈ R d × r and shared A ∈ R r × k . The hyper-parameter N denotes the number of B matrices.\"", "section": "Section 2.3 (Table 1) and Section 3.1 Asymmetric LoRA architecture"}
{"claim": "The evaluation lacks comparisons under equal inference latency or compute budget, which may bias the reported results.", "claim_type": "baseline", "paper_id": "qEpi8uWX3N", "paper_title": "HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning", "paper_venue": "neurips2024", "paper_decision": "Accept (oral)", "decision": "Oral", "review_id": "hA9JXs1a2I", "reviewer": "Reviewer_6P2g", "review_text": "Comment: I believe a fair comparison is with LoRA (r=16 or r=32), that's why the improvements are incremental. Since r=8 only has half of the HydraLoRA parameters used for finetuning. Or comparing under the same inference latency/compute budget.", "labeling_timestamp": "2026-01-11T16:49:14.928110", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper's reported evaluation (e.g., Table 1 and surrounding text) shows accuracy (MMLU) and percentage of parameters but does not present or compare inference latency or compute-budget (e.g., FLOPs, latency, or time) under equal budgets. The abstract and experiments claim superior performance versus other PEFT methods, but no equal-inference-latency or equal-compute-budget comparisons are provided in the shown sections.", "evidence": "Abstract: \"Our experiments demonstrate that HydraLoRA outperforms other PEFT approaches, even those that rely on domain knowledge during the training and inference phases.\" Table 1: \"Performance on instruction tuning with Dolly-15K [8] and evaluated with MMLU [16] with different ranks.\" (columns shown: \"MMLU ↑\" and \"% Parameter\")", "section": "Abstract; Section 2.3 (Table 1)"}
{"claim": "The paper's claim that C-RLFT is superior to RLFT is supported only by results from the openchat-13b model.", "claim_type": "experimental", "paper_id": "AOJyfhWYHf", "paper_title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "B8hxyc42tI", "reviewer": "Reviewer_U8fK", "review_text": "Summary: This paper proposes OpenChat, a framework for fine-tuning open-source language models with mixed-quality data. It proposes Conditioned-RLFT: A novel method that leverages coarse-grained rewards and class-conditioned policies to align the language model with human goals. This method is simple, RL-free, and does not require costly human feedback. Using RLFT, they finetuned OpenChat-13b, a language model based on LLaMA-2-13b and the ShareGPT dataset, which consists of conversations from GPT-4 and GPT-3.5. This model achieves the highest average performance on three standard benchmarks for instruction following ability among all 13b open-source language models.\n\nStrengths: - Originality: The paper introduces a new method, C-RLFT, that leverages coarse-grained rewards and class-conditioned policies to align the language model with human goals. The paper also provides a theoretical analysis and derivation of the optimal policy for C-RLFT. The resulting framework is a conditioned SFT with weighted loss, which is easy to implement and more stable than RLHF.\n- Quality: The paper presents evaluation results on three benchmarks to assess instruction following ability of the proposed model. The baselines include popular open and closed LLMs. The paper also performs ablation studies, representation visualization, prompt token effects, evaluator consistency, and data size effects to validate the effectiveness and robustness of OpenChat.\n- Clarity: The paper is well-written and organized.\n- Significance: The paper addresses an important and challenging problem of fine-tuning open-source language models with mixed-quality data. The paper demonstrates that OpenChat can achieve superior performance and generalization with simple and RL-free training, minimal reward quality requirements, and easily collectible data.\n\nWeaknesses: - The paper claims the superiority of C-RLFT over RLFT. But the claim is only supported by openchat-13b, which distilled its knowledge from two RLFT models: GPT-4 and GPT-3.5. So, it is only fair to say C-RLFT is better than SFT when distilling from GPT models, which is based on the fact the openchat-13b is better than vicuna-13b-1.5. However, one cannot say C-RLFT is better than RLFT, because openchat-13b learns from models trained by RLFT.\n- It is not clear if the \"low quality data\" is even useful at all. (see my question below)\n\nQuestions: - What is the performance if the llama model is only trained on GPT-4 data? I feel it should be another ablated model to demonstrate the value of \"low quality data\".", "labeling_timestamp": "2026-01-11T16:49:22.749881", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper reports experimental results for C-RLFT specifically on the openchat-13b model (fine-tuned from llama-2-13b) and claims superiority based on those results. The manuscript states they used llama-2-13b as the base model and present performance claims for openchat-13b; no other base models or model sizes are reported as carrying the C-RLFT results in the provided content.", "evidence": "\"Through extensive experiments on three standard benchmarks, our openchat-13b fine-tuned with C-RLFT achieves the highest average performance among all 13b open-source language models.\"", "section": "Abstract"}
{"claim": "Openchat-13b was distilled from GPT-4 and GPT-3.5, which themselves were trained using RL-based fine-tuning (RLFT).", "claim_type": "methodology", "paper_id": "AOJyfhWYHf", "paper_title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "B8hxyc42tI", "reviewer": "Reviewer_U8fK", "review_text": "Summary: This paper proposes OpenChat, a framework for fine-tuning open-source language models with mixed-quality data. It proposes Conditioned-RLFT: A novel method that leverages coarse-grained rewards and class-conditioned policies to align the language model with human goals. This method is simple, RL-free, and does not require costly human feedback. Using RLFT, they finetuned OpenChat-13b, a language model based on LLaMA-2-13b and the ShareGPT dataset, which consists of conversations from GPT-4 and GPT-3.5. This model achieves the highest average performance on three standard benchmarks for instruction following ability among all 13b open-source language models.\n\nStrengths: - Originality: The paper introduces a new method, C-RLFT, that leverages coarse-grained rewards and class-conditioned policies to align the language model with human goals. The paper also provides a theoretical analysis and derivation of the optimal policy for C-RLFT. The resulting framework is a conditioned SFT with weighted loss, which is easy to implement and more stable than RLHF.\n- Quality: The paper presents evaluation results on three benchmarks to assess instruction following ability of the proposed model. The baselines include popular open and closed LLMs. The paper also performs ablation studies, representation visualization, prompt token effects, evaluator consistency, and data size effects to validate the effectiveness and robustness of OpenChat.\n- Clarity: The paper is well-written and organized.\n- Significance: The paper addresses an important and challenging problem of fine-tuning open-source language models with mixed-quality data. The paper demonstrates that OpenChat can achieve superior performance and generalization with simple and RL-free training, minimal reward quality requirements, and easily collectible data.\n\nWeaknesses: - The paper claims the superiority of C-RLFT over RLFT. But the claim is only supported by openchat-13b, which distilled its knowledge from two RLFT models: GPT-4 and GPT-3.5. So, it is only fair to say C-RLFT is better than SFT when distilling from GPT models, which is based on the fact the openchat-13b is better than vicuna-13b-1.5. However, one cannot say C-RLFT is better than RLFT, because openchat-13b learns from models trained by RLFT.\n- It is not clear if the \"low quality data\" is even useful at all. (see my question below)\n\nQuestions: - What is the performance if the llama model is only trained on GPT-4 data? I feel it should be another ablated model to demonstrate the value of \"low quality data\".", "labeling_timestamp": "2026-01-11T16:50:01.752863", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper states openchat-13b was fine-tuned from llama-2-13b using ShareGPT data (which includes outputs from GPT-4 and GPT-3.5 as data sources). It does not claim openchat-13b was distilled from GPT-4/GPT-3.5, nor does it state that GPT-4 or GPT-3.5 were trained using RL-based fine-tuning; therefore the reviewer's claim is contradicted by the paper.", "evidence": "“we leverage the ShareGPT conversations dataset ... and use llama-2-13b as the base model.”; “the distinct data sources from GPT-4 and GPT-3.5 can be regarded as D_exp and D_sub, as the overall quality of GPT-3.5 conversations generally falls short when compared to that of GPT-4 conversations”", "section": "Abstract; Section 3 (OPENCHAT)"}
{"claim": "Because openchat-13b learned from models trained with RLFT, the experiments cannot demonstrate that C-RLFT outperforms RLFT directly.", "claim_type": "baseline", "paper_id": "AOJyfhWYHf", "paper_title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "B8hxyc42tI", "reviewer": "Reviewer_U8fK", "review_text": "Summary: This paper proposes OpenChat, a framework for fine-tuning open-source language models with mixed-quality data. It proposes Conditioned-RLFT: A novel method that leverages coarse-grained rewards and class-conditioned policies to align the language model with human goals. This method is simple, RL-free, and does not require costly human feedback. Using RLFT, they finetuned OpenChat-13b, a language model based on LLaMA-2-13b and the ShareGPT dataset, which consists of conversations from GPT-4 and GPT-3.5. This model achieves the highest average performance on three standard benchmarks for instruction following ability among all 13b open-source language models.\n\nStrengths: - Originality: The paper introduces a new method, C-RLFT, that leverages coarse-grained rewards and class-conditioned policies to align the language model with human goals. The paper also provides a theoretical analysis and derivation of the optimal policy for C-RLFT. The resulting framework is a conditioned SFT with weighted loss, which is easy to implement and more stable than RLHF.\n- Quality: The paper presents evaluation results on three benchmarks to assess instruction following ability of the proposed model. The baselines include popular open and closed LLMs. The paper also performs ablation studies, representation visualization, prompt token effects, evaluator consistency, and data size effects to validate the effectiveness and robustness of OpenChat.\n- Clarity: The paper is well-written and organized.\n- Significance: The paper addresses an important and challenging problem of fine-tuning open-source language models with mixed-quality data. The paper demonstrates that OpenChat can achieve superior performance and generalization with simple and RL-free training, minimal reward quality requirements, and easily collectible data.\n\nWeaknesses: - The paper claims the superiority of C-RLFT over RLFT. But the claim is only supported by openchat-13b, which distilled its knowledge from two RLFT models: GPT-4 and GPT-3.5. So, it is only fair to say C-RLFT is better than SFT when distilling from GPT models, which is based on the fact the openchat-13b is better than vicuna-13b-1.5. However, one cannot say C-RLFT is better than RLFT, because openchat-13b learns from models trained by RLFT.\n- It is not clear if the \"low quality data\" is even useful at all. (see my question below)\n\nQuestions: - What is the performance if the llama model is only trained on GPT-4 data? I feel it should be another ablated model to demonstrate the value of \"low quality data\".", "labeling_timestamp": "2026-01-11T16:49:45.674112", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper states it used the ShareGPT dataset (with GPT-4 and GPT-3.5 sources) and llama-2-13b as the base model and emphasizes that C-RLFT is an RL-free, single-stage supervised method. It does not state that openchat-13b was learned from models trained with RLFT, nor does it claim that this would prevent direct comparison to RLFT methods. Therefore there is insufficient information in the paper to confirm the reviewer's assertion.", "evidence": "“Specifically, we leverage the ShareGPT conversations dataset … and use llama-2-13b as the base model.”; “Interestingly, the optimal policy in C-RLFT can be easily solved through single-stage, RL-free supervised learning, which is lightweight and avoids costly human preference labeling.”; “Taking the most popular SFT dataset ShareGPT used in Vicuna … the distinct data sources from GPT-4 and GPT-3.5 can be regarded as D_exp and D_sub…”", "section": "Abstract; Section 3 (OPENCHAT)"}
{"claim": "The evidence only supports that C-RLFT outperforms supervised fine-tuning when distilling from GPT models, not that it outperforms RLFT.", "claim_type": "baseline", "paper_id": "AOJyfhWYHf", "paper_title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "B8hxyc42tI", "reviewer": "Reviewer_U8fK", "review_text": "Summary: This paper proposes OpenChat, a framework for fine-tuning open-source language models with mixed-quality data. It proposes Conditioned-RLFT: A novel method that leverages coarse-grained rewards and class-conditioned policies to align the language model with human goals. This method is simple, RL-free, and does not require costly human feedback. Using RLFT, they finetuned OpenChat-13b, a language model based on LLaMA-2-13b and the ShareGPT dataset, which consists of conversations from GPT-4 and GPT-3.5. This model achieves the highest average performance on three standard benchmarks for instruction following ability among all 13b open-source language models.\n\nStrengths: - Originality: The paper introduces a new method, C-RLFT, that leverages coarse-grained rewards and class-conditioned policies to align the language model with human goals. The paper also provides a theoretical analysis and derivation of the optimal policy for C-RLFT. The resulting framework is a conditioned SFT with weighted loss, which is easy to implement and more stable than RLHF.\n- Quality: The paper presents evaluation results on three benchmarks to assess instruction following ability of the proposed model. The baselines include popular open and closed LLMs. The paper also performs ablation studies, representation visualization, prompt token effects, evaluator consistency, and data size effects to validate the effectiveness and robustness of OpenChat.\n- Clarity: The paper is well-written and organized.\n- Significance: The paper addresses an important and challenging problem of fine-tuning open-source language models with mixed-quality data. The paper demonstrates that OpenChat can achieve superior performance and generalization with simple and RL-free training, minimal reward quality requirements, and easily collectible data.\n\nWeaknesses: - The paper claims the superiority of C-RLFT over RLFT. But the claim is only supported by openchat-13b, which distilled its knowledge from two RLFT models: GPT-4 and GPT-3.5. So, it is only fair to say C-RLFT is better than SFT when distilling from GPT models, which is based on the fact the openchat-13b is better than vicuna-13b-1.5. However, one cannot say C-RLFT is better than RLFT, because openchat-13b learns from models trained by RLFT.\n- It is not clear if the \"low quality data\" is even useful at all. (see my question below)\n\nQuestions: - What is the performance if the llama model is only trained on GPT-4 data? I feel it should be another ablated model to demonstrate the value of \"low quality data\".", "labeling_timestamp": "2026-01-11T16:50:05.343080", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper evaluates C-RLFT on benchmarks using mixed-quality data distilled from GPT models (ShareGPT, i.e., GPT-4 and GPT-3.5) and reports that openchat-13b outperforms previous 13b open-source models and even GPT-3.5. The paper emphasizes that C-RLFT is an RL-free, single-stage supervised method. Nowhere in the provided content does the paper present experimental comparisons showing C-RLFT outperforming prior RLFT (RLHF or other RL-based fine-tuning) baselines, so the evidence supports the reviewer’s statement.", "evidence": "Abstract: \"Interestingly, the optimal policy in C-RLFT can be easily solved through single-stage, RL-free supervised learning, which is lightweight and avoids costly human preference labeling. Through extensive experiments on three standard benchmarks, our openchat-13b fine-tuned with C-RLFT achieves the highest average performance among all 13b open-source language models.\"; Section 3: \"Taking the most popular SFT dataset ShareGPT ... the distinct data sources from GPT-4 and GPT-3.5 can be regarded as D_exp and D_sub, ... we aim to fine-tune an LLM policy π_θ based on π_0 using only data from D_exp ⋃ D_sub.\" ", "section": "Abstract; 3 OPENCHAT"}
{"claim": "It is unclear whether the low-quality portion of the mixed dataset provides any benefit to model performance.", "claim_type": "experimental", "paper_id": "AOJyfhWYHf", "paper_title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "B8hxyc42tI", "reviewer": "Reviewer_U8fK", "review_text": "Summary: This paper proposes OpenChat, a framework for fine-tuning open-source language models with mixed-quality data. It proposes Conditioned-RLFT: A novel method that leverages coarse-grained rewards and class-conditioned policies to align the language model with human goals. This method is simple, RL-free, and does not require costly human feedback. Using RLFT, they finetuned OpenChat-13b, a language model based on LLaMA-2-13b and the ShareGPT dataset, which consists of conversations from GPT-4 and GPT-3.5. This model achieves the highest average performance on three standard benchmarks for instruction following ability among all 13b open-source language models.\n\nStrengths: - Originality: The paper introduces a new method, C-RLFT, that leverages coarse-grained rewards and class-conditioned policies to align the language model with human goals. The paper also provides a theoretical analysis and derivation of the optimal policy for C-RLFT. The resulting framework is a conditioned SFT with weighted loss, which is easy to implement and more stable than RLHF.\n- Quality: The paper presents evaluation results on three benchmarks to assess instruction following ability of the proposed model. The baselines include popular open and closed LLMs. The paper also performs ablation studies, representation visualization, prompt token effects, evaluator consistency, and data size effects to validate the effectiveness and robustness of OpenChat.\n- Clarity: The paper is well-written and organized.\n- Significance: The paper addresses an important and challenging problem of fine-tuning open-source language models with mixed-quality data. The paper demonstrates that OpenChat can achieve superior performance and generalization with simple and RL-free training, minimal reward quality requirements, and easily collectible data.\n\nWeaknesses: - The paper claims the superiority of C-RLFT over RLFT. But the claim is only supported by openchat-13b, which distilled its knowledge from two RLFT models: GPT-4 and GPT-3.5. So, it is only fair to say C-RLFT is better than SFT when distilling from GPT models, which is based on the fact the openchat-13b is better than vicuna-13b-1.5. However, one cannot say C-RLFT is better than RLFT, because openchat-13b learns from models trained by RLFT.\n- It is not clear if the \"low quality data\" is even useful at all. (see my question below)\n\nQuestions: - What is the performance if the llama model is only trained on GPT-4 data? I feel it should be another ablated model to demonstrate the value of \"low quality data\".", "labeling_timestamp": "2026-01-11T16:49:49.096088", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes using mixed-quality data and claims overall improvements from the proposed C-RLFT, but in the provided content it does not present a clear, isolated analysis showing whether the low-quality (sub-optimal) portion specifically provides a benefit to performance. The text mentions ablation studies but does not show results that attribute gains to the low-quality data alone.", "evidence": "Abstract: \"we consider the general SFT training data, consisting of a small amount of expert data mixed with a large proportion of sub-optimal data, without any preference labels.\" Introduction: \"it is not advisable to indiscriminately feed all these mixed conversations to the base model, as the low-quality data are likely to negatively impact learning ... However, this is largely neglected in previous methods, which often treat all training data equally.\" Section 3: \"Obviously, it is not possible to derive accurate and fine-grained reward signals solely based on D_exp and D_sub. However, it should be noted that the quality difference between D_exp and D_sub itself can serve as implicit or weak reward signals.\"", "section": "Abstract; Introduction; Section 3 (Class-conditioned dataset and rewards)"}
{"claim": "The paper lacks an ablation experiment training the LLaMA-based model solely on GPT-4 data to isolate the effect of low-quality data.", "claim_type": "experimental", "paper_id": "AOJyfhWYHf", "paper_title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "B8hxyc42tI", "reviewer": "Reviewer_U8fK", "review_text": "Summary: This paper proposes OpenChat, a framework for fine-tuning open-source language models with mixed-quality data. It proposes Conditioned-RLFT: A novel method that leverages coarse-grained rewards and class-conditioned policies to align the language model with human goals. This method is simple, RL-free, and does not require costly human feedback. Using RLFT, they finetuned OpenChat-13b, a language model based on LLaMA-2-13b and the ShareGPT dataset, which consists of conversations from GPT-4 and GPT-3.5. This model achieves the highest average performance on three standard benchmarks for instruction following ability among all 13b open-source language models.\n\nStrengths: - Originality: The paper introduces a new method, C-RLFT, that leverages coarse-grained rewards and class-conditioned policies to align the language model with human goals. The paper also provides a theoretical analysis and derivation of the optimal policy for C-RLFT. The resulting framework is a conditioned SFT with weighted loss, which is easy to implement and more stable than RLHF.\n- Quality: The paper presents evaluation results on three benchmarks to assess instruction following ability of the proposed model. The baselines include popular open and closed LLMs. The paper also performs ablation studies, representation visualization, prompt token effects, evaluator consistency, and data size effects to validate the effectiveness and robustness of OpenChat.\n- Clarity: The paper is well-written and organized.\n- Significance: The paper addresses an important and challenging problem of fine-tuning open-source language models with mixed-quality data. The paper demonstrates that OpenChat can achieve superior performance and generalization with simple and RL-free training, minimal reward quality requirements, and easily collectible data.\n\nWeaknesses: - The paper claims the superiority of C-RLFT over RLFT. But the claim is only supported by openchat-13b, which distilled its knowledge from two RLFT models: GPT-4 and GPT-3.5. So, it is only fair to say C-RLFT is better than SFT when distilling from GPT models, which is based on the fact the openchat-13b is better than vicuna-13b-1.5. However, one cannot say C-RLFT is better than RLFT, because openchat-13b learns from models trained by RLFT.\n- It is not clear if the \"low quality data\" is even useful at all. (see my question below)\n\nQuestions: - What is the performance if the llama model is only trained on GPT-4 data? I feel it should be another ablated model to demonstrate the value of \"low quality data\".", "labeling_timestamp": "2026-01-11T16:49:44.020301", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt states that ablation studies were conducted (Abstract) and describes using GPT-4 as expert data and GPT-3.5 as suboptimal data (Sec. 3). However, the excerpt does not include experimental details or results that explicitly show an ablation where the LLaMA-based model is trained solely on GPT-4 data to isolate the effect of low-quality data, so this specific claim cannot be verified from the provided content.", "evidence": "Abstract: \"Finally, we design a series of ablation studies and analyses to validate the contribution of different components, and performance consistency...\"; Sec. 3: \"Taking the most popular SFT dataset ShareGPT used in Vicuna ... the distinct data sources from GPT-4 and GPT-3.5 can be regarded as D_exp and D_sub, as the overall quality of GPT-3.5 conversations generally falls short when compared to that of GPT-4 conversations...\"", "section": "Abstract; Section 3 (OPENCHAT)"}
{"claim": "The reviewer asks for reported performance when the model is trained only on GPT-4 data, indicating a missing comparison to evaluate low-quality data value.", "claim_type": "baseline", "paper_id": "AOJyfhWYHf", "paper_title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "B8hxyc42tI", "reviewer": "Reviewer_U8fK", "review_text": "Summary: This paper proposes OpenChat, a framework for fine-tuning open-source language models with mixed-quality data. It proposes Conditioned-RLFT: A novel method that leverages coarse-grained rewards and class-conditioned policies to align the language model with human goals. This method is simple, RL-free, and does not require costly human feedback. Using RLFT, they finetuned OpenChat-13b, a language model based on LLaMA-2-13b and the ShareGPT dataset, which consists of conversations from GPT-4 and GPT-3.5. This model achieves the highest average performance on three standard benchmarks for instruction following ability among all 13b open-source language models.\n\nStrengths: - Originality: The paper introduces a new method, C-RLFT, that leverages coarse-grained rewards and class-conditioned policies to align the language model with human goals. The paper also provides a theoretical analysis and derivation of the optimal policy for C-RLFT. The resulting framework is a conditioned SFT with weighted loss, which is easy to implement and more stable than RLHF.\n- Quality: The paper presents evaluation results on three benchmarks to assess instruction following ability of the proposed model. The baselines include popular open and closed LLMs. The paper also performs ablation studies, representation visualization, prompt token effects, evaluator consistency, and data size effects to validate the effectiveness and robustness of OpenChat.\n- Clarity: The paper is well-written and organized.\n- Significance: The paper addresses an important and challenging problem of fine-tuning open-source language models with mixed-quality data. The paper demonstrates that OpenChat can achieve superior performance and generalization with simple and RL-free training, minimal reward quality requirements, and easily collectible data.\n\nWeaknesses: - The paper claims the superiority of C-RLFT over RLFT. But the claim is only supported by openchat-13b, which distilled its knowledge from two RLFT models: GPT-4 and GPT-3.5. So, it is only fair to say C-RLFT is better than SFT when distilling from GPT models, which is based on the fact the openchat-13b is better than vicuna-13b-1.5. However, one cannot say C-RLFT is better than RLFT, because openchat-13b learns from models trained by RLFT.\n- It is not clear if the \"low quality data\" is even useful at all. (see my question below)\n\nQuestions: - What is the performance if the llama model is only trained on GPT-4 data? I feel it should be another ablated model to demonstrate the value of \"low quality data\".", "labeling_timestamp": "2026-01-11T16:50:18.246013", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper frames the fine-tuning setting as using a mixture of expert (GPT-4) and sub-optimal (GPT-3.5) data and describes experiments on mixed ShareGPT data, but it does not report results for a model trained solely on GPT-4 data (i.e., a GPT-4-only ablation/comparison is not presented).", "evidence": "“we consider the setting where we are given a pre-trained LLM π0, a small set of high-quality/expert conversation data Dexp, and a larger medium-quality or sub-optimal conversation dataset Dsub, we aim to fine-tune an LLM policy πθ based on π0 using only data from Dexp ⋃ Dsub.”", "section": "3 OPENCHAT"}
{"claim": "The superiority claim depends on openchat-13b outperforming vicuna-13b-1.5, which may conflate distillation source quality with the proposed method's effect.", "claim_type": "baseline", "paper_id": "AOJyfhWYHf", "paper_title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "B8hxyc42tI", "reviewer": "Reviewer_U8fK", "review_text": "Summary: This paper proposes OpenChat, a framework for fine-tuning open-source language models with mixed-quality data. It proposes Conditioned-RLFT: A novel method that leverages coarse-grained rewards and class-conditioned policies to align the language model with human goals. This method is simple, RL-free, and does not require costly human feedback. Using RLFT, they finetuned OpenChat-13b, a language model based on LLaMA-2-13b and the ShareGPT dataset, which consists of conversations from GPT-4 and GPT-3.5. This model achieves the highest average performance on three standard benchmarks for instruction following ability among all 13b open-source language models.\n\nStrengths: - Originality: The paper introduces a new method, C-RLFT, that leverages coarse-grained rewards and class-conditioned policies to align the language model with human goals. The paper also provides a theoretical analysis and derivation of the optimal policy for C-RLFT. The resulting framework is a conditioned SFT with weighted loss, which is easy to implement and more stable than RLHF.\n- Quality: The paper presents evaluation results on three benchmarks to assess instruction following ability of the proposed model. The baselines include popular open and closed LLMs. The paper also performs ablation studies, representation visualization, prompt token effects, evaluator consistency, and data size effects to validate the effectiveness and robustness of OpenChat.\n- Clarity: The paper is well-written and organized.\n- Significance: The paper addresses an important and challenging problem of fine-tuning open-source language models with mixed-quality data. The paper demonstrates that OpenChat can achieve superior performance and generalization with simple and RL-free training, minimal reward quality requirements, and easily collectible data.\n\nWeaknesses: - The paper claims the superiority of C-RLFT over RLFT. But the claim is only supported by openchat-13b, which distilled its knowledge from two RLFT models: GPT-4 and GPT-3.5. So, it is only fair to say C-RLFT is better than SFT when distilling from GPT models, which is based on the fact the openchat-13b is better than vicuna-13b-1.5. However, one cannot say C-RLFT is better than RLFT, because openchat-13b learns from models trained by RLFT.\n- It is not clear if the \"low quality data\" is even useful at all. (see my question below)\n\nQuestions: - What is the performance if the llama model is only trained on GPT-4 data? I feel it should be another ablated model to demonstrate the value of \"low quality data\".", "labeling_timestamp": "2026-01-11T16:50:39.417779", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper states it uses the ShareGPT dataset “following Vicuna” and reports that openchat-13b surpasses prior 13b open-source models, but the provided excerpts do not show explicit comparisons to vicuna-13b-1.5 or any analysis that disentangles effects of distillation/source quality from the proposed method. The paper points to a detailed comparison in Sec. 5.1 (not included here), so based on the available text we cannot confirm the reviewer’s specific claim about dependence on vicuna-13b-1.5 or conflation with distillation source quality.", "evidence": "\"we leverage the ShareGPT conversations dataset following Vicuna (Chiang et al., 2023) and use llama-2-13b as the base model.\"; \"the results demonstrate that openchat-13b significantly surpasses previous 13b open-source language models and can even outperform gpt-3.5-turbo in all three benchmarks.\"; \"Taking the most popular SFT dataset ShareGPT used in Vicuna (Chiang et al., 2023) as an example, the distinct data sources from GPT-4 and GPT-3.5 can be regarded as D_exp and D_sub, ... where detailed comparison can be found in Sec. 5.1.\"", "section": "Introduction / Section 3"}
{"claim": "The experimental comparison is limited, primarily featuring methods from Tang et al., 2022, lacking other weakly supervised baselines.", "claim_type": "baseline", "paper_id": "fW7DOHDQvF", "paper_title": "Consistent Multi-Class Classification from Multiple Unlabeled Datasets", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "VC9qQQQ5cR", "reviewer": "Reviewer_XSqD", "review_text": "Summary: The paper introduces a novel approach to weakly supervised learning, specifically targeting multi-class classification from multiple unlabeled datasets with class priors. It addresses limitations identified in Tang et al., 2022, and offers two distinct approaches: a classifier-consistent method using a probability transition matrix and a risk-consistent approach employing importance weighting. However, certain areas require further clarification and empirical investigation.\n\nStrengths: - The paper successfully addresses limitations observed in the MCMU method and presents an alternative solution, expanding the landscape of weakly supervised learning.\n- The introduction of a novel approach focusing on empirical risk minimization (ERM) at the instance level, treating individual data points, stands out as a significant strength.\n\nWeaknesses: - The experimental comparison is somewhat limited, primarily featuring methods from Tang et al., 2022. While justified due to MCMU's novelty, including additional weakly supervised methods could provide valuable reference points. Moreover, presenting accuracies for fully supervised learning on the same train/test splits would enhance the evaluation.\n- The paper lacks a comprehensive study of the method's limitations, particularly with regards to constraints on parameters such as m (the number of unlabeled sets) and n_i (the number of data points in each set), which are crucial for practical applicability.\n- Theoretical results like Theorem 3.5, which establishes an upper bound for the difference between true and empirical risk, would benefit from discussion regarding their practical implications and their relationship to the actual classification task involving class labels.\n\nQuestions: 1. The paper could further investigate the impact of small m (few unlabeled sets) and significant variations in label proportions across sets on the generalizability of its results with respect to m and n_i.\n   \n2. A clarification regarding the difference between Lemma 3.1 in the paper and Theorem 1 in Lu et al., ICML 2021, would be valuable for readers.\n\n3. Given that label proportions in real-world scenarios may only slightly differ, the paper should outline constraints on parameters m and n for practical significance, especially considering that a large m may be necessary for the method's effectiveness.\n\n4. A discussion on the practical relevance of the derived upper bound in Theorem 3.5 and its connection to the actual classification task involving class labels would provide valuable insights.\n\n5. The observation that the proposed approach performs significantly worse under certain settings, such as a random class prior matrix and non-constrained m, should be supported by clearer details on how test accuracies were computed, whether test data were balanced, and if class size variations in datasets played a role.\n\nOther suggestions:\n\n\"In MCMU, given the class priors, the data points in the same set are independent from each other, while in LLP, given the label proportions, the data points in\nthe same set are dependent from each other.\"\n\nThe second part of the sentence should clearly state whether data points are \"independent from each other\" or \"dependent on each other.\" If it implies independence, it raises a significant ambiguity regarding how MCMU and LLP fundamentally differ, as class priors and label proportions are practically equivalent concepts. Moreover, there appears to be a conceptual misuse in the paragraph. Data points within each class are typically assumed to be independent, conditioned on their class labels. The statement in question uses \"class priors\" in place of \"class labels\", which could introduce confusion and should be clarified. Overall, given that label proportions and class priors are practically the same thing in the context studied by the papers it is hard to see any difference from a dependence perspective between the two approaches. \n\nI think using the term \"label proportions\" when referring to class priors within each set would be more appropriate. In fact what the paper denotes as the j-th class prior of the i-th unlabeled set can be denoted as the label proportion of class j in dataset i. This would distinguish label proportions from class priors estimated using data from all available unlabeled sets.\n\n\nIn Section 4.1. \"The ni data points contained in i-th unlabeled set were randomly generated according to ...\" Isn't this supposed to be \"randomly sampled\" or \"drawn\".  Data already exists... nothing is supposed to be generated.\n\nIt would greatly enhance the paper if, within the related work section, an early explanation were provided regarding the reasons behind the possibility of negative empirical risk in these methods (Lu et al., 2019; Tsai & Lin, 2020; Tang et al., 2022). This proactive approach would help readers anticipate and better understand the issues related to negative empirical risk that the paper seeks to address.\n\nIn page 4 i is used for indexing both the sets and data points. It may cause confusions. \n\nIn Theorem 3.6 the paper defines this probability p(y=j|\\bar{y},x)as the probability of x in \\bar{y}-th unlabeled set whose ground-truth label is j. Isn't this supposed to be the probability of x in \\bar{y}-th unlabeled set belonging to class j? x may or may not belong to class j. x's ground truth label is not necessarily j. \n\nTheorem 3.6 also considers p(x|y=j)=p(x|y=j,\\bar{y}) as a fact. However, this is not necessarily true. The first one is the conditional distribution of class j estimated using all data belonging to class j whereas the second one is the conditional distribution of class j estimated using only data from unlabeled set \\bar{y}. So, from an empirical standpoint these distributions are not equal. Please clarify.\n\n\nAfter Rebuttal:\n\nThanks for taking the time to do those extra comparisons I mentioned. It’s helpful to see how your method holds up with different 'm' values and varying levels of class imbalance. Based on these new results,  I'm happy to bump up my score by one.", "labeling_timestamp": "2026-01-11T16:50:15.636841", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt mentions extensive experiments and cites Tang et al. (2022) as the only prior work on MCMU, and Table 1 compares related methods conceptually. However, the excerpt does not contain the experimental section or a concrete list of baselines used in the empirical comparisons, so we cannot verify whether the experiments primarily featured Tang et al. (2022) or omitted other weakly supervised baselines.", "evidence": "Abstract: \"Experimental results on multiple benchmark datasets across various settings demonstrate the superiority of our proposed methods.\"; Introduction: \"Tang et al. (2022) proposed an unbiased risk estimator for MCMU so that the expected classification risk of fully supervised data can be unbiasedly estimated by given training data, which is also the sole work on MCMU to the best of our knowledge.\"; End of Introduction: \"We conduct extensive experiments on benchmark datasets with various settings. Experimental results demonstrate that CCM works well but RCM consistently outperforms CCM.\"", "section": "Abstract / Introduction"}
{"claim": "The paper does not report fully supervised accuracies on the same train/test splits, which would enhance evaluation.", "claim_type": "experimental", "paper_id": "fW7DOHDQvF", "paper_title": "Consistent Multi-Class Classification from Multiple Unlabeled Datasets", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "VC9qQQQ5cR", "reviewer": "Reviewer_XSqD", "review_text": "Summary: The paper introduces a novel approach to weakly supervised learning, specifically targeting multi-class classification from multiple unlabeled datasets with class priors. It addresses limitations identified in Tang et al., 2022, and offers two distinct approaches: a classifier-consistent method using a probability transition matrix and a risk-consistent approach employing importance weighting. However, certain areas require further clarification and empirical investigation.\n\nStrengths: - The paper successfully addresses limitations observed in the MCMU method and presents an alternative solution, expanding the landscape of weakly supervised learning.\n- The introduction of a novel approach focusing on empirical risk minimization (ERM) at the instance level, treating individual data points, stands out as a significant strength.\n\nWeaknesses: - The experimental comparison is somewhat limited, primarily featuring methods from Tang et al., 2022. While justified due to MCMU's novelty, including additional weakly supervised methods could provide valuable reference points. Moreover, presenting accuracies for fully supervised learning on the same train/test splits would enhance the evaluation.\n- The paper lacks a comprehensive study of the method's limitations, particularly with regards to constraints on parameters such as m (the number of unlabeled sets) and n_i (the number of data points in each set), which are crucial for practical applicability.\n- Theoretical results like Theorem 3.5, which establishes an upper bound for the difference between true and empirical risk, would benefit from discussion regarding their practical implications and their relationship to the actual classification task involving class labels.\n\nQuestions: 1. The paper could further investigate the impact of small m (few unlabeled sets) and significant variations in label proportions across sets on the generalizability of its results with respect to m and n_i.\n   \n2. A clarification regarding the difference between Lemma 3.1 in the paper and Theorem 1 in Lu et al., ICML 2021, would be valuable for readers.\n\n3. Given that label proportions in real-world scenarios may only slightly differ, the paper should outline constraints on parameters m and n for practical significance, especially considering that a large m may be necessary for the method's effectiveness.\n\n4. A discussion on the practical relevance of the derived upper bound in Theorem 3.5 and its connection to the actual classification task involving class labels would provide valuable insights.\n\n5. The observation that the proposed approach performs significantly worse under certain settings, such as a random class prior matrix and non-constrained m, should be supported by clearer details on how test accuracies were computed, whether test data were balanced, and if class size variations in datasets played a role.\n\nOther suggestions:\n\n\"In MCMU, given the class priors, the data points in the same set are independent from each other, while in LLP, given the label proportions, the data points in\nthe same set are dependent from each other.\"\n\nThe second part of the sentence should clearly state whether data points are \"independent from each other\" or \"dependent on each other.\" If it implies independence, it raises a significant ambiguity regarding how MCMU and LLP fundamentally differ, as class priors and label proportions are practically equivalent concepts. Moreover, there appears to be a conceptual misuse in the paragraph. Data points within each class are typically assumed to be independent, conditioned on their class labels. The statement in question uses \"class priors\" in place of \"class labels\", which could introduce confusion and should be clarified. Overall, given that label proportions and class priors are practically the same thing in the context studied by the papers it is hard to see any difference from a dependence perspective between the two approaches. \n\nI think using the term \"label proportions\" when referring to class priors within each set would be more appropriate. In fact what the paper denotes as the j-th class prior of the i-th unlabeled set can be denoted as the label proportion of class j in dataset i. This would distinguish label proportions from class priors estimated using data from all available unlabeled sets.\n\n\nIn Section 4.1. \"The ni data points contained in i-th unlabeled set were randomly generated according to ...\" Isn't this supposed to be \"randomly sampled\" or \"drawn\".  Data already exists... nothing is supposed to be generated.\n\nIt would greatly enhance the paper if, within the related work section, an early explanation were provided regarding the reasons behind the possibility of negative empirical risk in these methods (Lu et al., 2019; Tsai & Lin, 2020; Tang et al., 2022). This proactive approach would help readers anticipate and better understand the issues related to negative empirical risk that the paper seeks to address.\n\nIn page 4 i is used for indexing both the sets and data points. It may cause confusions. \n\nIn Theorem 3.6 the paper defines this probability p(y=j|\\bar{y},x)as the probability of x in \\bar{y}-th unlabeled set whose ground-truth label is j. Isn't this supposed to be the probability of x in \\bar{y}-th unlabeled set belonging to class j? x may or may not belong to class j. x's ground truth label is not necessarily j. \n\nTheorem 3.6 also considers p(x|y=j)=p(x|y=j,\\bar{y}) as a fact. However, this is not necessarily true. The first one is the conditional distribution of class j estimated using all data belonging to class j whereas the second one is the conditional distribution of class j estimated using only data from unlabeled set \\bar{y}. So, from an empirical standpoint these distributions are not equal. Please clarify.\n\n\nAfter Rebuttal:\n\nThanks for taking the time to do those extra comparisons I mentioned. It’s helpful to see how your method holds up with different 'm' values and varying levels of class imbalance. Based on these new results,  I'm happy to bump up my score by one.", "labeling_timestamp": "2026-01-11T16:50:19.898705", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpts describe experiments generally (Abstract and Contributions) but do not include the experimental details, tables, or mention of fully supervised baselines evaluated using the same train/test splits. Therefore there is insufficient information to confirm the reviewer claim from the given content.", "evidence": [{"quote": "Experimental results on multiple benchmark datasets across various settings demonstrate the superiority of our proposed methods.", "section": "Abstract"}, {"quote": "We conduct extensive experiments on benchmark datasets with various settings. Experimental results demonstrate that CCM works well but RCM consistently outperforms CCM.", "section": "Main contributions"}], "section": ""}
{"claim": "The paper lacks a comprehensive study of the method's limitations regarding constraints on m (number of unlabeled sets) and n_i (number of data points per set).", "claim_type": "experimental", "paper_id": "fW7DOHDQvF", "paper_title": "Consistent Multi-Class Classification from Multiple Unlabeled Datasets", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "VC9qQQQ5cR", "reviewer": "Reviewer_XSqD", "review_text": "Summary: The paper introduces a novel approach to weakly supervised learning, specifically targeting multi-class classification from multiple unlabeled datasets with class priors. It addresses limitations identified in Tang et al., 2022, and offers two distinct approaches: a classifier-consistent method using a probability transition matrix and a risk-consistent approach employing importance weighting. However, certain areas require further clarification and empirical investigation.\n\nStrengths: - The paper successfully addresses limitations observed in the MCMU method and presents an alternative solution, expanding the landscape of weakly supervised learning.\n- The introduction of a novel approach focusing on empirical risk minimization (ERM) at the instance level, treating individual data points, stands out as a significant strength.\n\nWeaknesses: - The experimental comparison is somewhat limited, primarily featuring methods from Tang et al., 2022. While justified due to MCMU's novelty, including additional weakly supervised methods could provide valuable reference points. Moreover, presenting accuracies for fully supervised learning on the same train/test splits would enhance the evaluation.\n- The paper lacks a comprehensive study of the method's limitations, particularly with regards to constraints on parameters such as m (the number of unlabeled sets) and n_i (the number of data points in each set), which are crucial for practical applicability.\n- Theoretical results like Theorem 3.5, which establishes an upper bound for the difference between true and empirical risk, would benefit from discussion regarding their practical implications and their relationship to the actual classification task involving class labels.\n\nQuestions: 1. The paper could further investigate the impact of small m (few unlabeled sets) and significant variations in label proportions across sets on the generalizability of its results with respect to m and n_i.\n   \n2. A clarification regarding the difference between Lemma 3.1 in the paper and Theorem 1 in Lu et al., ICML 2021, would be valuable for readers.\n\n3. Given that label proportions in real-world scenarios may only slightly differ, the paper should outline constraints on parameters m and n for practical significance, especially considering that a large m may be necessary for the method's effectiveness.\n\n4. A discussion on the practical relevance of the derived upper bound in Theorem 3.5 and its connection to the actual classification task involving class labels would provide valuable insights.\n\n5. The observation that the proposed approach performs significantly worse under certain settings, such as a random class prior matrix and non-constrained m, should be supported by clearer details on how test accuracies were computed, whether test data were balanced, and if class size variations in datasets played a role.\n\nOther suggestions:\n\n\"In MCMU, given the class priors, the data points in the same set are independent from each other, while in LLP, given the label proportions, the data points in\nthe same set are dependent from each other.\"\n\nThe second part of the sentence should clearly state whether data points are \"independent from each other\" or \"dependent on each other.\" If it implies independence, it raises a significant ambiguity regarding how MCMU and LLP fundamentally differ, as class priors and label proportions are practically equivalent concepts. Moreover, there appears to be a conceptual misuse in the paragraph. Data points within each class are typically assumed to be independent, conditioned on their class labels. The statement in question uses \"class priors\" in place of \"class labels\", which could introduce confusion and should be clarified. Overall, given that label proportions and class priors are practically the same thing in the context studied by the papers it is hard to see any difference from a dependence perspective between the two approaches. \n\nI think using the term \"label proportions\" when referring to class priors within each set would be more appropriate. In fact what the paper denotes as the j-th class prior of the i-th unlabeled set can be denoted as the label proportion of class j in dataset i. This would distinguish label proportions from class priors estimated using data from all available unlabeled sets.\n\n\nIn Section 4.1. \"The ni data points contained in i-th unlabeled set were randomly generated according to ...\" Isn't this supposed to be \"randomly sampled\" or \"drawn\".  Data already exists... nothing is supposed to be generated.\n\nIt would greatly enhance the paper if, within the related work section, an early explanation were provided regarding the reasons behind the possibility of negative empirical risk in these methods (Lu et al., 2019; Tsai & Lin, 2020; Tang et al., 2022). This proactive approach would help readers anticipate and better understand the issues related to negative empirical risk that the paper seeks to address.\n\nIn page 4 i is used for indexing both the sets and data points. It may cause confusions. \n\nIn Theorem 3.6 the paper defines this probability p(y=j|\\bar{y},x)as the probability of x in \\bar{y}-th unlabeled set whose ground-truth label is j. Isn't this supposed to be the probability of x in \\bar{y}-th unlabeled set belonging to class j? x may or may not belong to class j. x's ground truth label is not necessarily j. \n\nTheorem 3.6 also considers p(x|y=j)=p(x|y=j,\\bar{y}) as a fact. However, this is not necessarily true. The first one is the conditional distribution of class j estimated using all data belonging to class j whereas the second one is the conditional distribution of class j estimated using only data from unlabeled set \\bar{y}. So, from an empirical standpoint these distributions are not equal. Please clarify.\n\n\nAfter Rebuttal:\n\nThanks for taking the time to do those extra comparisons I mentioned. It’s helpful to see how your method holds up with different 'm' values and varying levels of class imbalance. Based on these new results,  I'm happy to bump up my score by one.", "labeling_timestamp": "2026-01-11T16:50:42.909800", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper imposes assumptions that constrain m (e.g., m ≥ 2 and θ must be full column rank) and gives only a formulaic way to estimate quantities involving n_i, but it does not provide a systematic or detailed analysis of how performance or identifiability depends on the number of unlabeled sets m or the per-set sample sizes n_i (no sample-complexity bounds or empirical sensitivity study are presented).", "evidence": "1) \"Given m ( m ě 2 ) sets of unlabeled data D : ' Ť m i ' 1 U i where U i ' t x i,r u n i r ' 1 is a collection of n i data points...\" 2) \"We assume that these class priors form a full column rank matrix θ : ' p θ ij q P r 0 , 1 s m ˆ k with the constraint ř k j ' 1 θ ij ' 1.\" 3) \"Throughout the paper, we assume that the class priors of each unlabeled set are given, which means θ is accessible. Then, π j and ρ j could be estimated by ř m i ' 1 n i ¨ θ ij { ř m i ' i n i and n j { ř m i ' 1 n i respectively.\"", "section": "2.4 MULTI-CLASS CLASSIFICATION FROM MULTIPLE UNLABELED DATASETS"}
{"claim": "Theorem 3.5 lacks discussion of practical implications and connection to the actual classification task involving class labels.", "claim_type": "presentation", "paper_id": "fW7DOHDQvF", "paper_title": "Consistent Multi-Class Classification from Multiple Unlabeled Datasets", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "VC9qQQQ5cR", "reviewer": "Reviewer_XSqD", "review_text": "Summary: The paper introduces a novel approach to weakly supervised learning, specifically targeting multi-class classification from multiple unlabeled datasets with class priors. It addresses limitations identified in Tang et al., 2022, and offers two distinct approaches: a classifier-consistent method using a probability transition matrix and a risk-consistent approach employing importance weighting. However, certain areas require further clarification and empirical investigation.\n\nStrengths: - The paper successfully addresses limitations observed in the MCMU method and presents an alternative solution, expanding the landscape of weakly supervised learning.\n- The introduction of a novel approach focusing on empirical risk minimization (ERM) at the instance level, treating individual data points, stands out as a significant strength.\n\nWeaknesses: - The experimental comparison is somewhat limited, primarily featuring methods from Tang et al., 2022. While justified due to MCMU's novelty, including additional weakly supervised methods could provide valuable reference points. Moreover, presenting accuracies for fully supervised learning on the same train/test splits would enhance the evaluation.\n- The paper lacks a comprehensive study of the method's limitations, particularly with regards to constraints on parameters such as m (the number of unlabeled sets) and n_i (the number of data points in each set), which are crucial for practical applicability.\n- Theoretical results like Theorem 3.5, which establishes an upper bound for the difference between true and empirical risk, would benefit from discussion regarding their practical implications and their relationship to the actual classification task involving class labels.\n\nQuestions: 1. The paper could further investigate the impact of small m (few unlabeled sets) and significant variations in label proportions across sets on the generalizability of its results with respect to m and n_i.\n   \n2. A clarification regarding the difference between Lemma 3.1 in the paper and Theorem 1 in Lu et al., ICML 2021, would be valuable for readers.\n\n3. Given that label proportions in real-world scenarios may only slightly differ, the paper should outline constraints on parameters m and n for practical significance, especially considering that a large m may be necessary for the method's effectiveness.\n\n4. A discussion on the practical relevance of the derived upper bound in Theorem 3.5 and its connection to the actual classification task involving class labels would provide valuable insights.\n\n5. The observation that the proposed approach performs significantly worse under certain settings, such as a random class prior matrix and non-constrained m, should be supported by clearer details on how test accuracies were computed, whether test data were balanced, and if class size variations in datasets played a role.\n\nOther suggestions:\n\n\"In MCMU, given the class priors, the data points in the same set are independent from each other, while in LLP, given the label proportions, the data points in\nthe same set are dependent from each other.\"\n\nThe second part of the sentence should clearly state whether data points are \"independent from each other\" or \"dependent on each other.\" If it implies independence, it raises a significant ambiguity regarding how MCMU and LLP fundamentally differ, as class priors and label proportions are practically equivalent concepts. Moreover, there appears to be a conceptual misuse in the paragraph. Data points within each class are typically assumed to be independent, conditioned on their class labels. The statement in question uses \"class priors\" in place of \"class labels\", which could introduce confusion and should be clarified. Overall, given that label proportions and class priors are practically the same thing in the context studied by the papers it is hard to see any difference from a dependence perspective between the two approaches. \n\nI think using the term \"label proportions\" when referring to class priors within each set would be more appropriate. In fact what the paper denotes as the j-th class prior of the i-th unlabeled set can be denoted as the label proportion of class j in dataset i. This would distinguish label proportions from class priors estimated using data from all available unlabeled sets.\n\n\nIn Section 4.1. \"The ni data points contained in i-th unlabeled set were randomly generated according to ...\" Isn't this supposed to be \"randomly sampled\" or \"drawn\".  Data already exists... nothing is supposed to be generated.\n\nIt would greatly enhance the paper if, within the related work section, an early explanation were provided regarding the reasons behind the possibility of negative empirical risk in these methods (Lu et al., 2019; Tsai & Lin, 2020; Tang et al., 2022). This proactive approach would help readers anticipate and better understand the issues related to negative empirical risk that the paper seeks to address.\n\nIn page 4 i is used for indexing both the sets and data points. It may cause confusions. \n\nIn Theorem 3.6 the paper defines this probability p(y=j|\\bar{y},x)as the probability of x in \\bar{y}-th unlabeled set whose ground-truth label is j. Isn't this supposed to be the probability of x in \\bar{y}-th unlabeled set belonging to class j? x may or may not belong to class j. x's ground truth label is not necessarily j. \n\nTheorem 3.6 also considers p(x|y=j)=p(x|y=j,\\bar{y}) as a fact. However, this is not necessarily true. The first one is the conditional distribution of class j estimated using all data belonging to class j whereas the second one is the conditional distribution of class j estimated using only data from unlabeled set \\bar{y}. So, from an empirical standpoint these distributions are not equal. Please clarify.\n\n\nAfter Rebuttal:\n\nThanks for taking the time to do those extra comparisons I mentioned. It’s helpful to see how your method holds up with different 'm' values and varying levels of class imbalance. Based on these new results,  I'm happy to bump up my score by one.", "labeling_timestamp": "2026-01-11T16:50:33.180737", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not include Theorem 3.5 or any surrounding text for that theorem, so it is impossible to verify whether Theorem 3.5 lacks discussion of practical implications and connection to the classification task involving class labels.", "evidence": "“In this section, we present our two statistically consistent methods, i.e., the classifier-consistent method (CCM) and the risk-consistent method (RCM).”\n\n“We provide comprehensive theoretical analyses for our methods to demonstrate the statistical consistency.”", "section": "Abstract and Section 3 (excerpt)"}
{"claim": "The paper does not investigate the impact of small m and large variations in label proportions across sets on generalizability with respect to m and n_i.", "claim_type": "experimental", "paper_id": "fW7DOHDQvF", "paper_title": "Consistent Multi-Class Classification from Multiple Unlabeled Datasets", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "VC9qQQQ5cR", "reviewer": "Reviewer_XSqD", "review_text": "Summary: The paper introduces a novel approach to weakly supervised learning, specifically targeting multi-class classification from multiple unlabeled datasets with class priors. It addresses limitations identified in Tang et al., 2022, and offers two distinct approaches: a classifier-consistent method using a probability transition matrix and a risk-consistent approach employing importance weighting. However, certain areas require further clarification and empirical investigation.\n\nStrengths: - The paper successfully addresses limitations observed in the MCMU method and presents an alternative solution, expanding the landscape of weakly supervised learning.\n- The introduction of a novel approach focusing on empirical risk minimization (ERM) at the instance level, treating individual data points, stands out as a significant strength.\n\nWeaknesses: - The experimental comparison is somewhat limited, primarily featuring methods from Tang et al., 2022. While justified due to MCMU's novelty, including additional weakly supervised methods could provide valuable reference points. Moreover, presenting accuracies for fully supervised learning on the same train/test splits would enhance the evaluation.\n- The paper lacks a comprehensive study of the method's limitations, particularly with regards to constraints on parameters such as m (the number of unlabeled sets) and n_i (the number of data points in each set), which are crucial for practical applicability.\n- Theoretical results like Theorem 3.5, which establishes an upper bound for the difference between true and empirical risk, would benefit from discussion regarding their practical implications and their relationship to the actual classification task involving class labels.\n\nQuestions: 1. The paper could further investigate the impact of small m (few unlabeled sets) and significant variations in label proportions across sets on the generalizability of its results with respect to m and n_i.\n   \n2. A clarification regarding the difference between Lemma 3.1 in the paper and Theorem 1 in Lu et al., ICML 2021, would be valuable for readers.\n\n3. Given that label proportions in real-world scenarios may only slightly differ, the paper should outline constraints on parameters m and n for practical significance, especially considering that a large m may be necessary for the method's effectiveness.\n\n4. A discussion on the practical relevance of the derived upper bound in Theorem 3.5 and its connection to the actual classification task involving class labels would provide valuable insights.\n\n5. The observation that the proposed approach performs significantly worse under certain settings, such as a random class prior matrix and non-constrained m, should be supported by clearer details on how test accuracies were computed, whether test data were balanced, and if class size variations in datasets played a role.\n\nOther suggestions:\n\n\"In MCMU, given the class priors, the data points in the same set are independent from each other, while in LLP, given the label proportions, the data points in\nthe same set are dependent from each other.\"\n\nThe second part of the sentence should clearly state whether data points are \"independent from each other\" or \"dependent on each other.\" If it implies independence, it raises a significant ambiguity regarding how MCMU and LLP fundamentally differ, as class priors and label proportions are practically equivalent concepts. Moreover, there appears to be a conceptual misuse in the paragraph. Data points within each class are typically assumed to be independent, conditioned on their class labels. The statement in question uses \"class priors\" in place of \"class labels\", which could introduce confusion and should be clarified. Overall, given that label proportions and class priors are practically the same thing in the context studied by the papers it is hard to see any difference from a dependence perspective between the two approaches. \n\nI think using the term \"label proportions\" when referring to class priors within each set would be more appropriate. In fact what the paper denotes as the j-th class prior of the i-th unlabeled set can be denoted as the label proportion of class j in dataset i. This would distinguish label proportions from class priors estimated using data from all available unlabeled sets.\n\n\nIn Section 4.1. \"The ni data points contained in i-th unlabeled set were randomly generated according to ...\" Isn't this supposed to be \"randomly sampled\" or \"drawn\".  Data already exists... nothing is supposed to be generated.\n\nIt would greatly enhance the paper if, within the related work section, an early explanation were provided regarding the reasons behind the possibility of negative empirical risk in these methods (Lu et al., 2019; Tsai & Lin, 2020; Tang et al., 2022). This proactive approach would help readers anticipate and better understand the issues related to negative empirical risk that the paper seeks to address.\n\nIn page 4 i is used for indexing both the sets and data points. It may cause confusions. \n\nIn Theorem 3.6 the paper defines this probability p(y=j|\\bar{y},x)as the probability of x in \\bar{y}-th unlabeled set whose ground-truth label is j. Isn't this supposed to be the probability of x in \\bar{y}-th unlabeled set belonging to class j? x may or may not belong to class j. x's ground truth label is not necessarily j. \n\nTheorem 3.6 also considers p(x|y=j)=p(x|y=j,\\bar{y}) as a fact. However, this is not necessarily true. The first one is the conditional distribution of class j estimated using all data belonging to class j whereas the second one is the conditional distribution of class j estimated using only data from unlabeled set \\bar{y}. So, from an empirical standpoint these distributions are not equal. Please clarify.\n\n\nAfter Rebuttal:\n\nThanks for taking the time to do those extra comparisons I mentioned. It’s helpful to see how your method holds up with different 'm' values and varying levels of class imbalance. Based on these new results,  I'm happy to bump up my score by one.", "labeling_timestamp": "2026-01-11T16:50:46.207641", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper mentions that MCMU can learn from an extremely small number of unlabeled sets and assumes class priors (θ) are given and form a full-column-rank matrix, but it provides no analysis or experiments that explicitly study how small m or large variations in label proportions across sets (or varying n_i) affect generalization. Theoretical assumptions constrain m (via rank of θ) but do not constitute an investigation of impact on generalizability.", "evidence": "\"These differences make our studied MCMU more applicable than LLP. For example, MCMU can learn from an extremely small number of unlabeled sets\"; \"we assume that these class priors form a full column rank matrix θ : ' p θ_{ij} q P [0,1]^{m×k} ...\"; \"Experimental results on multiple benchmark datasets across various settings demonstrate the superiority of our proposed methods.\"", "section": "Introduction; Section 2.4; Abstract"}
{"claim": "The paper does not clarify the difference between its Lemma 3.1 and Theorem 1 of Lu et al., ICML 2021.", "claim_type": "methodology", "paper_id": "fW7DOHDQvF", "paper_title": "Consistent Multi-Class Classification from Multiple Unlabeled Datasets", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "VC9qQQQ5cR", "reviewer": "Reviewer_XSqD", "review_text": "Summary: The paper introduces a novel approach to weakly supervised learning, specifically targeting multi-class classification from multiple unlabeled datasets with class priors. It addresses limitations identified in Tang et al., 2022, and offers two distinct approaches: a classifier-consistent method using a probability transition matrix and a risk-consistent approach employing importance weighting. However, certain areas require further clarification and empirical investigation.\n\nStrengths: - The paper successfully addresses limitations observed in the MCMU method and presents an alternative solution, expanding the landscape of weakly supervised learning.\n- The introduction of a novel approach focusing on empirical risk minimization (ERM) at the instance level, treating individual data points, stands out as a significant strength.\n\nWeaknesses: - The experimental comparison is somewhat limited, primarily featuring methods from Tang et al., 2022. While justified due to MCMU's novelty, including additional weakly supervised methods could provide valuable reference points. Moreover, presenting accuracies for fully supervised learning on the same train/test splits would enhance the evaluation.\n- The paper lacks a comprehensive study of the method's limitations, particularly with regards to constraints on parameters such as m (the number of unlabeled sets) and n_i (the number of data points in each set), which are crucial for practical applicability.\n- Theoretical results like Theorem 3.5, which establishes an upper bound for the difference between true and empirical risk, would benefit from discussion regarding their practical implications and their relationship to the actual classification task involving class labels.\n\nQuestions: 1. The paper could further investigate the impact of small m (few unlabeled sets) and significant variations in label proportions across sets on the generalizability of its results with respect to m and n_i.\n   \n2. A clarification regarding the difference between Lemma 3.1 in the paper and Theorem 1 in Lu et al., ICML 2021, would be valuable for readers.\n\n3. Given that label proportions in real-world scenarios may only slightly differ, the paper should outline constraints on parameters m and n for practical significance, especially considering that a large m may be necessary for the method's effectiveness.\n\n4. A discussion on the practical relevance of the derived upper bound in Theorem 3.5 and its connection to the actual classification task involving class labels would provide valuable insights.\n\n5. The observation that the proposed approach performs significantly worse under certain settings, such as a random class prior matrix and non-constrained m, should be supported by clearer details on how test accuracies were computed, whether test data were balanced, and if class size variations in datasets played a role.\n\nOther suggestions:\n\n\"In MCMU, given the class priors, the data points in the same set are independent from each other, while in LLP, given the label proportions, the data points in\nthe same set are dependent from each other.\"\n\nThe second part of the sentence should clearly state whether data points are \"independent from each other\" or \"dependent on each other.\" If it implies independence, it raises a significant ambiguity regarding how MCMU and LLP fundamentally differ, as class priors and label proportions are practically equivalent concepts. Moreover, there appears to be a conceptual misuse in the paragraph. Data points within each class are typically assumed to be independent, conditioned on their class labels. The statement in question uses \"class priors\" in place of \"class labels\", which could introduce confusion and should be clarified. Overall, given that label proportions and class priors are practically the same thing in the context studied by the papers it is hard to see any difference from a dependence perspective between the two approaches. \n\nI think using the term \"label proportions\" when referring to class priors within each set would be more appropriate. In fact what the paper denotes as the j-th class prior of the i-th unlabeled set can be denoted as the label proportion of class j in dataset i. This would distinguish label proportions from class priors estimated using data from all available unlabeled sets.\n\n\nIn Section 4.1. \"The ni data points contained in i-th unlabeled set were randomly generated according to ...\" Isn't this supposed to be \"randomly sampled\" or \"drawn\".  Data already exists... nothing is supposed to be generated.\n\nIt would greatly enhance the paper if, within the related work section, an early explanation were provided regarding the reasons behind the possibility of negative empirical risk in these methods (Lu et al., 2019; Tsai & Lin, 2020; Tang et al., 2022). This proactive approach would help readers anticipate and better understand the issues related to negative empirical risk that the paper seeks to address.\n\nIn page 4 i is used for indexing both the sets and data points. It may cause confusions. \n\nIn Theorem 3.6 the paper defines this probability p(y=j|\\bar{y},x)as the probability of x in \\bar{y}-th unlabeled set whose ground-truth label is j. Isn't this supposed to be the probability of x in \\bar{y}-th unlabeled set belonging to class j? x may or may not belong to class j. x's ground truth label is not necessarily j. \n\nTheorem 3.6 also considers p(x|y=j)=p(x|y=j,\\bar{y}) as a fact. However, this is not necessarily true. The first one is the conditional distribution of class j estimated using all data belonging to class j whereas the second one is the conditional distribution of class j estimated using only data from unlabeled set \\bar{y}. So, from an empirical standpoint these distributions are not equal. Please clarify.\n\n\nAfter Rebuttal:\n\nThanks for taking the time to do those extra comparisons I mentioned. It’s helpful to see how your method holds up with different 'm' values and varying levels of class imbalance. Based on these new results,  I'm happy to bump up my score by one.", "labeling_timestamp": "2026-01-11T16:50:48.620402", "model": "gpt-5-mini", "label": "Supported", "justification": "Throughout the provided paper text the authors only state that CCM is a multi-class extension of Lu et al. (2021) and describe their probability-transition approach, but there is no explicit discussion distinguishing their Lemma 3.1 from Theorem 1 of Lu et al. (ICML 2021). The manuscript does not clarify the difference between the two results in the shown sections.", "evidence": "“CCM can be viewed as a multi-class extension of Lu et al. (2021), following which we also consider the MCMU problem as a surrogate set classification problem... Then we theoretically showed that we can transform the posterior probabilities of ordinary labels to the posterior probabilities of surrogate sets, via a probability transition function.”\n\n“We propose a classifier-consistent method (CCM) based on a probability transition function, which can be considered as a multi-class extension of Lu et al. (2021).”", "section": "Main contributions; 3 THE PROPOSED CONSISTENT LEARNING METHODS"}
{"claim": "The paper fails to outline constraints on m and n required for practical significance, given that small label proportion differences may necessitate a large m.", "claim_type": "methodology", "paper_id": "fW7DOHDQvF", "paper_title": "Consistent Multi-Class Classification from Multiple Unlabeled Datasets", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "VC9qQQQ5cR", "reviewer": "Reviewer_XSqD", "review_text": "Summary: The paper introduces a novel approach to weakly supervised learning, specifically targeting multi-class classification from multiple unlabeled datasets with class priors. It addresses limitations identified in Tang et al., 2022, and offers two distinct approaches: a classifier-consistent method using a probability transition matrix and a risk-consistent approach employing importance weighting. However, certain areas require further clarification and empirical investigation.\n\nStrengths: - The paper successfully addresses limitations observed in the MCMU method and presents an alternative solution, expanding the landscape of weakly supervised learning.\n- The introduction of a novel approach focusing on empirical risk minimization (ERM) at the instance level, treating individual data points, stands out as a significant strength.\n\nWeaknesses: - The experimental comparison is somewhat limited, primarily featuring methods from Tang et al., 2022. While justified due to MCMU's novelty, including additional weakly supervised methods could provide valuable reference points. Moreover, presenting accuracies for fully supervised learning on the same train/test splits would enhance the evaluation.\n- The paper lacks a comprehensive study of the method's limitations, particularly with regards to constraints on parameters such as m (the number of unlabeled sets) and n_i (the number of data points in each set), which are crucial for practical applicability.\n- Theoretical results like Theorem 3.5, which establishes an upper bound for the difference between true and empirical risk, would benefit from discussion regarding their practical implications and their relationship to the actual classification task involving class labels.\n\nQuestions: 1. The paper could further investigate the impact of small m (few unlabeled sets) and significant variations in label proportions across sets on the generalizability of its results with respect to m and n_i.\n   \n2. A clarification regarding the difference between Lemma 3.1 in the paper and Theorem 1 in Lu et al., ICML 2021, would be valuable for readers.\n\n3. Given that label proportions in real-world scenarios may only slightly differ, the paper should outline constraints on parameters m and n for practical significance, especially considering that a large m may be necessary for the method's effectiveness.\n\n4. A discussion on the practical relevance of the derived upper bound in Theorem 3.5 and its connection to the actual classification task involving class labels would provide valuable insights.\n\n5. The observation that the proposed approach performs significantly worse under certain settings, such as a random class prior matrix and non-constrained m, should be supported by clearer details on how test accuracies were computed, whether test data were balanced, and if class size variations in datasets played a role.\n\nOther suggestions:\n\n\"In MCMU, given the class priors, the data points in the same set are independent from each other, while in LLP, given the label proportions, the data points in\nthe same set are dependent from each other.\"\n\nThe second part of the sentence should clearly state whether data points are \"independent from each other\" or \"dependent on each other.\" If it implies independence, it raises a significant ambiguity regarding how MCMU and LLP fundamentally differ, as class priors and label proportions are practically equivalent concepts. Moreover, there appears to be a conceptual misuse in the paragraph. Data points within each class are typically assumed to be independent, conditioned on their class labels. The statement in question uses \"class priors\" in place of \"class labels\", which could introduce confusion and should be clarified. Overall, given that label proportions and class priors are practically the same thing in the context studied by the papers it is hard to see any difference from a dependence perspective between the two approaches. \n\nI think using the term \"label proportions\" when referring to class priors within each set would be more appropriate. In fact what the paper denotes as the j-th class prior of the i-th unlabeled set can be denoted as the label proportion of class j in dataset i. This would distinguish label proportions from class priors estimated using data from all available unlabeled sets.\n\n\nIn Section 4.1. \"The ni data points contained in i-th unlabeled set were randomly generated according to ...\" Isn't this supposed to be \"randomly sampled\" or \"drawn\".  Data already exists... nothing is supposed to be generated.\n\nIt would greatly enhance the paper if, within the related work section, an early explanation were provided regarding the reasons behind the possibility of negative empirical risk in these methods (Lu et al., 2019; Tsai & Lin, 2020; Tang et al., 2022). This proactive approach would help readers anticipate and better understand the issues related to negative empirical risk that the paper seeks to address.\n\nIn page 4 i is used for indexing both the sets and data points. It may cause confusions. \n\nIn Theorem 3.6 the paper defines this probability p(y=j|\\bar{y},x)as the probability of x in \\bar{y}-th unlabeled set whose ground-truth label is j. Isn't this supposed to be the probability of x in \\bar{y}-th unlabeled set belonging to class j? x may or may not belong to class j. x's ground truth label is not necessarily j. \n\nTheorem 3.6 also considers p(x|y=j)=p(x|y=j,\\bar{y}) as a fact. However, this is not necessarily true. The first one is the conditional distribution of class j estimated using all data belonging to class j whereas the second one is the conditional distribution of class j estimated using only data from unlabeled set \\bar{y}. So, from an empirical standpoint these distributions are not equal. Please clarify.\n\n\nAfter Rebuttal:\n\nThanks for taking the time to do those extra comparisons I mentioned. It’s helpful to see how your method holds up with different 'm' values and varying levels of class imbalance. Based on these new results,  I'm happy to bump up my score by one.", "labeling_timestamp": "2026-01-11T16:51:48.191976", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper does impose a theoretical constraint on the number of unlabeled sets via the assumption that the class-prior matrix θ is full column rank (which implies a relation between m and k), but it does not provide practical guidance or explicit constraints on m or the per-set sample sizes n (e.g., sample complexity or how large m must be when class-prior differences are small). Thus the reviewer’s statement is partly correct: a theoretical requirement exists, but practical requirements on m and n are not outlined.", "evidence": "\"We assume that these class priors form a full column rank matrix θ ...\"; \"Throughout the paper, we assume that the class priors of each unlabeled set are given, which means θ is accessible.\"; \"For example, MCMU can learn from an extremely small number of unlabeled sets, while most previous LLP methods that treat each set as a training unit cannot work well in this case.\"", "section": "Introduction; 2.4 MULTI-CLASS CLASSIFICATION FROM MULTIPLE UNLABELED DATASETS"}
{"claim": "The proposed approach performs significantly worse under certain settings, such as a random class prior matrix and non-constrained m.", "claim_type": "experimental", "paper_id": "fW7DOHDQvF", "paper_title": "Consistent Multi-Class Classification from Multiple Unlabeled Datasets", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "VC9qQQQ5cR", "reviewer": "Reviewer_XSqD", "review_text": "Summary: The paper introduces a novel approach to weakly supervised learning, specifically targeting multi-class classification from multiple unlabeled datasets with class priors. It addresses limitations identified in Tang et al., 2022, and offers two distinct approaches: a classifier-consistent method using a probability transition matrix and a risk-consistent approach employing importance weighting. However, certain areas require further clarification and empirical investigation.\n\nStrengths: - The paper successfully addresses limitations observed in the MCMU method and presents an alternative solution, expanding the landscape of weakly supervised learning.\n- The introduction of a novel approach focusing on empirical risk minimization (ERM) at the instance level, treating individual data points, stands out as a significant strength.\n\nWeaknesses: - The experimental comparison is somewhat limited, primarily featuring methods from Tang et al., 2022. While justified due to MCMU's novelty, including additional weakly supervised methods could provide valuable reference points. Moreover, presenting accuracies for fully supervised learning on the same train/test splits would enhance the evaluation.\n- The paper lacks a comprehensive study of the method's limitations, particularly with regards to constraints on parameters such as m (the number of unlabeled sets) and n_i (the number of data points in each set), which are crucial for practical applicability.\n- Theoretical results like Theorem 3.5, which establishes an upper bound for the difference between true and empirical risk, would benefit from discussion regarding their practical implications and their relationship to the actual classification task involving class labels.\n\nQuestions: 1. The paper could further investigate the impact of small m (few unlabeled sets) and significant variations in label proportions across sets on the generalizability of its results with respect to m and n_i.\n   \n2. A clarification regarding the difference between Lemma 3.1 in the paper and Theorem 1 in Lu et al., ICML 2021, would be valuable for readers.\n\n3. Given that label proportions in real-world scenarios may only slightly differ, the paper should outline constraints on parameters m and n for practical significance, especially considering that a large m may be necessary for the method's effectiveness.\n\n4. A discussion on the practical relevance of the derived upper bound in Theorem 3.5 and its connection to the actual classification task involving class labels would provide valuable insights.\n\n5. The observation that the proposed approach performs significantly worse under certain settings, such as a random class prior matrix and non-constrained m, should be supported by clearer details on how test accuracies were computed, whether test data were balanced, and if class size variations in datasets played a role.\n\nOther suggestions:\n\n\"In MCMU, given the class priors, the data points in the same set are independent from each other, while in LLP, given the label proportions, the data points in\nthe same set are dependent from each other.\"\n\nThe second part of the sentence should clearly state whether data points are \"independent from each other\" or \"dependent on each other.\" If it implies independence, it raises a significant ambiguity regarding how MCMU and LLP fundamentally differ, as class priors and label proportions are practically equivalent concepts. Moreover, there appears to be a conceptual misuse in the paragraph. Data points within each class are typically assumed to be independent, conditioned on their class labels. The statement in question uses \"class priors\" in place of \"class labels\", which could introduce confusion and should be clarified. Overall, given that label proportions and class priors are practically the same thing in the context studied by the papers it is hard to see any difference from a dependence perspective between the two approaches. \n\nI think using the term \"label proportions\" when referring to class priors within each set would be more appropriate. In fact what the paper denotes as the j-th class prior of the i-th unlabeled set can be denoted as the label proportion of class j in dataset i. This would distinguish label proportions from class priors estimated using data from all available unlabeled sets.\n\n\nIn Section 4.1. \"The ni data points contained in i-th unlabeled set were randomly generated according to ...\" Isn't this supposed to be \"randomly sampled\" or \"drawn\".  Data already exists... nothing is supposed to be generated.\n\nIt would greatly enhance the paper if, within the related work section, an early explanation were provided regarding the reasons behind the possibility of negative empirical risk in these methods (Lu et al., 2019; Tsai & Lin, 2020; Tang et al., 2022). This proactive approach would help readers anticipate and better understand the issues related to negative empirical risk that the paper seeks to address.\n\nIn page 4 i is used for indexing both the sets and data points. It may cause confusions. \n\nIn Theorem 3.6 the paper defines this probability p(y=j|\\bar{y},x)as the probability of x in \\bar{y}-th unlabeled set whose ground-truth label is j. Isn't this supposed to be the probability of x in \\bar{y}-th unlabeled set belonging to class j? x may or may not belong to class j. x's ground truth label is not necessarily j. \n\nTheorem 3.6 also considers p(x|y=j)=p(x|y=j,\\bar{y}) as a fact. However, this is not necessarily true. The first one is the conditional distribution of class j estimated using all data belonging to class j whereas the second one is the conditional distribution of class j estimated using only data from unlabeled set \\bar{y}. So, from an empirical standpoint these distributions are not equal. Please clarify.\n\n\nAfter Rebuttal:\n\nThanks for taking the time to do those extra comparisons I mentioned. It’s helpful to see how your method holds up with different 'm' values and varying levels of class imbalance. Based on these new results,  I'm happy to bump up my score by one.", "labeling_timestamp": "2026-01-11T16:51:02.131563", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does not report any empirical or theoretical statement that the proposed methods perform significantly worse under a random class-prior matrix or when m is unconstrained. Instead, the paper assumes the class-prior matrix θ is given and has full column rank (a constraint), and reports that experiments across various settings demonstrate superiority. There is no evidence in the provided content evaluating performance specifically for 'random class prior matrix' or 'non-constrained m'.", "evidence": "1) \"We assume that these class priors form a full column rank matrix θ : ' p θ ij q P r 0 , 1 s m ˆ k with the constraint ř k j ' 1 θ ij ' 1 .\" (Section 2.4)\n2) \"Experimental results on multiple benchmark datasets across various settings demonstrate the superiority of our proposed methods.\" (Abstract)", "section": "Abstract; Section 2.4"}
{"claim": "The paper does not clearly state how test accuracies were computed.", "claim_type": "methodology", "paper_id": "fW7DOHDQvF", "paper_title": "Consistent Multi-Class Classification from Multiple Unlabeled Datasets", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "VC9qQQQ5cR", "reviewer": "Reviewer_XSqD", "review_text": "Summary: The paper introduces a novel approach to weakly supervised learning, specifically targeting multi-class classification from multiple unlabeled datasets with class priors. It addresses limitations identified in Tang et al., 2022, and offers two distinct approaches: a classifier-consistent method using a probability transition matrix and a risk-consistent approach employing importance weighting. However, certain areas require further clarification and empirical investigation.\n\nStrengths: - The paper successfully addresses limitations observed in the MCMU method and presents an alternative solution, expanding the landscape of weakly supervised learning.\n- The introduction of a novel approach focusing on empirical risk minimization (ERM) at the instance level, treating individual data points, stands out as a significant strength.\n\nWeaknesses: - The experimental comparison is somewhat limited, primarily featuring methods from Tang et al., 2022. While justified due to MCMU's novelty, including additional weakly supervised methods could provide valuable reference points. Moreover, presenting accuracies for fully supervised learning on the same train/test splits would enhance the evaluation.\n- The paper lacks a comprehensive study of the method's limitations, particularly with regards to constraints on parameters such as m (the number of unlabeled sets) and n_i (the number of data points in each set), which are crucial for practical applicability.\n- Theoretical results like Theorem 3.5, which establishes an upper bound for the difference between true and empirical risk, would benefit from discussion regarding their practical implications and their relationship to the actual classification task involving class labels.\n\nQuestions: 1. The paper could further investigate the impact of small m (few unlabeled sets) and significant variations in label proportions across sets on the generalizability of its results with respect to m and n_i.\n   \n2. A clarification regarding the difference between Lemma 3.1 in the paper and Theorem 1 in Lu et al., ICML 2021, would be valuable for readers.\n\n3. Given that label proportions in real-world scenarios may only slightly differ, the paper should outline constraints on parameters m and n for practical significance, especially considering that a large m may be necessary for the method's effectiveness.\n\n4. A discussion on the practical relevance of the derived upper bound in Theorem 3.5 and its connection to the actual classification task involving class labels would provide valuable insights.\n\n5. The observation that the proposed approach performs significantly worse under certain settings, such as a random class prior matrix and non-constrained m, should be supported by clearer details on how test accuracies were computed, whether test data were balanced, and if class size variations in datasets played a role.\n\nOther suggestions:\n\n\"In MCMU, given the class priors, the data points in the same set are independent from each other, while in LLP, given the label proportions, the data points in\nthe same set are dependent from each other.\"\n\nThe second part of the sentence should clearly state whether data points are \"independent from each other\" or \"dependent on each other.\" If it implies independence, it raises a significant ambiguity regarding how MCMU and LLP fundamentally differ, as class priors and label proportions are practically equivalent concepts. Moreover, there appears to be a conceptual misuse in the paragraph. Data points within each class are typically assumed to be independent, conditioned on their class labels. The statement in question uses \"class priors\" in place of \"class labels\", which could introduce confusion and should be clarified. Overall, given that label proportions and class priors are practically the same thing in the context studied by the papers it is hard to see any difference from a dependence perspective between the two approaches. \n\nI think using the term \"label proportions\" when referring to class priors within each set would be more appropriate. In fact what the paper denotes as the j-th class prior of the i-th unlabeled set can be denoted as the label proportion of class j in dataset i. This would distinguish label proportions from class priors estimated using data from all available unlabeled sets.\n\n\nIn Section 4.1. \"The ni data points contained in i-th unlabeled set were randomly generated according to ...\" Isn't this supposed to be \"randomly sampled\" or \"drawn\".  Data already exists... nothing is supposed to be generated.\n\nIt would greatly enhance the paper if, within the related work section, an early explanation were provided regarding the reasons behind the possibility of negative empirical risk in these methods (Lu et al., 2019; Tsai & Lin, 2020; Tang et al., 2022). This proactive approach would help readers anticipate and better understand the issues related to negative empirical risk that the paper seeks to address.\n\nIn page 4 i is used for indexing both the sets and data points. It may cause confusions. \n\nIn Theorem 3.6 the paper defines this probability p(y=j|\\bar{y},x)as the probability of x in \\bar{y}-th unlabeled set whose ground-truth label is j. Isn't this supposed to be the probability of x in \\bar{y}-th unlabeled set belonging to class j? x may or may not belong to class j. x's ground truth label is not necessarily j. \n\nTheorem 3.6 also considers p(x|y=j)=p(x|y=j,\\bar{y}) as a fact. However, this is not necessarily true. The first one is the conditional distribution of class j estimated using all data belonging to class j whereas the second one is the conditional distribution of class j estimated using only data from unlabeled set \\bar{y}. So, from an empirical standpoint these distributions are not equal. Please clarify.\n\n\nAfter Rebuttal:\n\nThanks for taking the time to do those extra comparisons I mentioned. It’s helpful to see how your method holds up with different 'm' values and varying levels of class imbalance. Based on these new results,  I'm happy to bump up my score by one.", "labeling_timestamp": "2026-01-11T16:51:26.502106", "model": "gpt-5-mini", "label": "Supported", "justification": "The provided paper content mentions experimental results but contains no description of evaluation metrics or a procedure for computing test accuracies (no experimental setup, metrics, or evaluation protocol is given in the supplied sections). Therefore the claim that the paper does not clearly state how test accuracies were computed is supported by the content.", "evidence": "Abstract: \"Experimental results on multiple benchmark datasets across various settings demonstrate the superiority of our proposed methods.\"; Main contributions: \"We conduct extensive experiments on benchmark datasets with various settings. Experimental results demonstrate that CCM works well but RCM consistently outperforms CCM.\"", "section": "Abstract / Main contributions (Introduction)"}
{"claim": "The paper does not state whether test data were balanced across classes when reporting accuracies.", "claim_type": "experimental", "paper_id": "fW7DOHDQvF", "paper_title": "Consistent Multi-Class Classification from Multiple Unlabeled Datasets", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "VC9qQQQ5cR", "reviewer": "Reviewer_XSqD", "review_text": "Summary: The paper introduces a novel approach to weakly supervised learning, specifically targeting multi-class classification from multiple unlabeled datasets with class priors. It addresses limitations identified in Tang et al., 2022, and offers two distinct approaches: a classifier-consistent method using a probability transition matrix and a risk-consistent approach employing importance weighting. However, certain areas require further clarification and empirical investigation.\n\nStrengths: - The paper successfully addresses limitations observed in the MCMU method and presents an alternative solution, expanding the landscape of weakly supervised learning.\n- The introduction of a novel approach focusing on empirical risk minimization (ERM) at the instance level, treating individual data points, stands out as a significant strength.\n\nWeaknesses: - The experimental comparison is somewhat limited, primarily featuring methods from Tang et al., 2022. While justified due to MCMU's novelty, including additional weakly supervised methods could provide valuable reference points. Moreover, presenting accuracies for fully supervised learning on the same train/test splits would enhance the evaluation.\n- The paper lacks a comprehensive study of the method's limitations, particularly with regards to constraints on parameters such as m (the number of unlabeled sets) and n_i (the number of data points in each set), which are crucial for practical applicability.\n- Theoretical results like Theorem 3.5, which establishes an upper bound for the difference between true and empirical risk, would benefit from discussion regarding their practical implications and their relationship to the actual classification task involving class labels.\n\nQuestions: 1. The paper could further investigate the impact of small m (few unlabeled sets) and significant variations in label proportions across sets on the generalizability of its results with respect to m and n_i.\n   \n2. A clarification regarding the difference between Lemma 3.1 in the paper and Theorem 1 in Lu et al., ICML 2021, would be valuable for readers.\n\n3. Given that label proportions in real-world scenarios may only slightly differ, the paper should outline constraints on parameters m and n for practical significance, especially considering that a large m may be necessary for the method's effectiveness.\n\n4. A discussion on the practical relevance of the derived upper bound in Theorem 3.5 and its connection to the actual classification task involving class labels would provide valuable insights.\n\n5. The observation that the proposed approach performs significantly worse under certain settings, such as a random class prior matrix and non-constrained m, should be supported by clearer details on how test accuracies were computed, whether test data were balanced, and if class size variations in datasets played a role.\n\nOther suggestions:\n\n\"In MCMU, given the class priors, the data points in the same set are independent from each other, while in LLP, given the label proportions, the data points in\nthe same set are dependent from each other.\"\n\nThe second part of the sentence should clearly state whether data points are \"independent from each other\" or \"dependent on each other.\" If it implies independence, it raises a significant ambiguity regarding how MCMU and LLP fundamentally differ, as class priors and label proportions are practically equivalent concepts. Moreover, there appears to be a conceptual misuse in the paragraph. Data points within each class are typically assumed to be independent, conditioned on their class labels. The statement in question uses \"class priors\" in place of \"class labels\", which could introduce confusion and should be clarified. Overall, given that label proportions and class priors are practically the same thing in the context studied by the papers it is hard to see any difference from a dependence perspective between the two approaches. \n\nI think using the term \"label proportions\" when referring to class priors within each set would be more appropriate. In fact what the paper denotes as the j-th class prior of the i-th unlabeled set can be denoted as the label proportion of class j in dataset i. This would distinguish label proportions from class priors estimated using data from all available unlabeled sets.\n\n\nIn Section 4.1. \"The ni data points contained in i-th unlabeled set were randomly generated according to ...\" Isn't this supposed to be \"randomly sampled\" or \"drawn\".  Data already exists... nothing is supposed to be generated.\n\nIt would greatly enhance the paper if, within the related work section, an early explanation were provided regarding the reasons behind the possibility of negative empirical risk in these methods (Lu et al., 2019; Tsai & Lin, 2020; Tang et al., 2022). This proactive approach would help readers anticipate and better understand the issues related to negative empirical risk that the paper seeks to address.\n\nIn page 4 i is used for indexing both the sets and data points. It may cause confusions. \n\nIn Theorem 3.6 the paper defines this probability p(y=j|\\bar{y},x)as the probability of x in \\bar{y}-th unlabeled set whose ground-truth label is j. Isn't this supposed to be the probability of x in \\bar{y}-th unlabeled set belonging to class j? x may or may not belong to class j. x's ground truth label is not necessarily j. \n\nTheorem 3.6 also considers p(x|y=j)=p(x|y=j,\\bar{y}) as a fact. However, this is not necessarily true. The first one is the conditional distribution of class j estimated using all data belonging to class j whereas the second one is the conditional distribution of class j estimated using only data from unlabeled set \\bar{y}. So, from an empirical standpoint these distributions are not equal. Please clarify.\n\n\nAfter Rebuttal:\n\nThanks for taking the time to do those extra comparisons I mentioned. It’s helpful to see how your method holds up with different 'm' values and varying levels of class imbalance. Based on these new results,  I'm happy to bump up my score by one.", "labeling_timestamp": "2026-01-11T16:51:15.241282", "model": "gpt-5-mini", "label": "Supported", "justification": "The provided paper content (including the Abstract and Introduction) mentions experimental results but provides no information about whether test sets were balanced across classes when reporting accuracies. There is no statement describing test-data class balance or distribution in the included sections.", "evidence": "Experimental results on multiple benchmark datasets across various settings demonstrate the superiority of our proposed methods.", "section": "Abstract"}
{"claim": "The paper does not analyze whether class size variations in datasets influenced the reported performance differences.", "claim_type": "experimental", "paper_id": "fW7DOHDQvF", "paper_title": "Consistent Multi-Class Classification from Multiple Unlabeled Datasets", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "VC9qQQQ5cR", "reviewer": "Reviewer_XSqD", "review_text": "Summary: The paper introduces a novel approach to weakly supervised learning, specifically targeting multi-class classification from multiple unlabeled datasets with class priors. It addresses limitations identified in Tang et al., 2022, and offers two distinct approaches: a classifier-consistent method using a probability transition matrix and a risk-consistent approach employing importance weighting. However, certain areas require further clarification and empirical investigation.\n\nStrengths: - The paper successfully addresses limitations observed in the MCMU method and presents an alternative solution, expanding the landscape of weakly supervised learning.\n- The introduction of a novel approach focusing on empirical risk minimization (ERM) at the instance level, treating individual data points, stands out as a significant strength.\n\nWeaknesses: - The experimental comparison is somewhat limited, primarily featuring methods from Tang et al., 2022. While justified due to MCMU's novelty, including additional weakly supervised methods could provide valuable reference points. Moreover, presenting accuracies for fully supervised learning on the same train/test splits would enhance the evaluation.\n- The paper lacks a comprehensive study of the method's limitations, particularly with regards to constraints on parameters such as m (the number of unlabeled sets) and n_i (the number of data points in each set), which are crucial for practical applicability.\n- Theoretical results like Theorem 3.5, which establishes an upper bound for the difference between true and empirical risk, would benefit from discussion regarding their practical implications and their relationship to the actual classification task involving class labels.\n\nQuestions: 1. The paper could further investigate the impact of small m (few unlabeled sets) and significant variations in label proportions across sets on the generalizability of its results with respect to m and n_i.\n   \n2. A clarification regarding the difference between Lemma 3.1 in the paper and Theorem 1 in Lu et al., ICML 2021, would be valuable for readers.\n\n3. Given that label proportions in real-world scenarios may only slightly differ, the paper should outline constraints on parameters m and n for practical significance, especially considering that a large m may be necessary for the method's effectiveness.\n\n4. A discussion on the practical relevance of the derived upper bound in Theorem 3.5 and its connection to the actual classification task involving class labels would provide valuable insights.\n\n5. The observation that the proposed approach performs significantly worse under certain settings, such as a random class prior matrix and non-constrained m, should be supported by clearer details on how test accuracies were computed, whether test data were balanced, and if class size variations in datasets played a role.\n\nOther suggestions:\n\n\"In MCMU, given the class priors, the data points in the same set are independent from each other, while in LLP, given the label proportions, the data points in\nthe same set are dependent from each other.\"\n\nThe second part of the sentence should clearly state whether data points are \"independent from each other\" or \"dependent on each other.\" If it implies independence, it raises a significant ambiguity regarding how MCMU and LLP fundamentally differ, as class priors and label proportions are practically equivalent concepts. Moreover, there appears to be a conceptual misuse in the paragraph. Data points within each class are typically assumed to be independent, conditioned on their class labels. The statement in question uses \"class priors\" in place of \"class labels\", which could introduce confusion and should be clarified. Overall, given that label proportions and class priors are practically the same thing in the context studied by the papers it is hard to see any difference from a dependence perspective between the two approaches. \n\nI think using the term \"label proportions\" when referring to class priors within each set would be more appropriate. In fact what the paper denotes as the j-th class prior of the i-th unlabeled set can be denoted as the label proportion of class j in dataset i. This would distinguish label proportions from class priors estimated using data from all available unlabeled sets.\n\n\nIn Section 4.1. \"The ni data points contained in i-th unlabeled set were randomly generated according to ...\" Isn't this supposed to be \"randomly sampled\" or \"drawn\".  Data already exists... nothing is supposed to be generated.\n\nIt would greatly enhance the paper if, within the related work section, an early explanation were provided regarding the reasons behind the possibility of negative empirical risk in these methods (Lu et al., 2019; Tsai & Lin, 2020; Tang et al., 2022). This proactive approach would help readers anticipate and better understand the issues related to negative empirical risk that the paper seeks to address.\n\nIn page 4 i is used for indexing both the sets and data points. It may cause confusions. \n\nIn Theorem 3.6 the paper defines this probability p(y=j|\\bar{y},x)as the probability of x in \\bar{y}-th unlabeled set whose ground-truth label is j. Isn't this supposed to be the probability of x in \\bar{y}-th unlabeled set belonging to class j? x may or may not belong to class j. x's ground truth label is not necessarily j. \n\nTheorem 3.6 also considers p(x|y=j)=p(x|y=j,\\bar{y}) as a fact. However, this is not necessarily true. The first one is the conditional distribution of class j estimated using all data belonging to class j whereas the second one is the conditional distribution of class j estimated using only data from unlabeled set \\bar{y}. So, from an empirical standpoint these distributions are not equal. Please clarify.\n\n\nAfter Rebuttal:\n\nThanks for taking the time to do those extra comparisons I mentioned. It’s helpful to see how your method holds up with different 'm' values and varying levels of class imbalance. Based on these new results,  I'm happy to bump up my score by one.", "labeling_timestamp": "2026-01-11T16:51:11.114335", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper discusses class priors and reports experimental superiority but the provided content contains no analysis or ablation examining how variations in class sizes (class imbalance) across datasets affected the reported performance differences.", "evidence": "\"Throughout the paper, we assume that the class priors of each unlabeled set are given, which means θ is accessible.\"; \"Experimental results on multiple benchmark datasets across various settings demonstrate the superiority of our proposed methods.\"", "section": "Section 2.4 (class priors assumption) and Abstract (experimental results)"}
{"claim": "The sentence conflating class priors with class labels introduces confusion, since data points are typically assumed independent conditioned on their class labels.", "claim_type": "methodology", "paper_id": "fW7DOHDQvF", "paper_title": "Consistent Multi-Class Classification from Multiple Unlabeled Datasets", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "VC9qQQQ5cR", "reviewer": "Reviewer_XSqD", "review_text": "Summary: The paper introduces a novel approach to weakly supervised learning, specifically targeting multi-class classification from multiple unlabeled datasets with class priors. It addresses limitations identified in Tang et al., 2022, and offers two distinct approaches: a classifier-consistent method using a probability transition matrix and a risk-consistent approach employing importance weighting. However, certain areas require further clarification and empirical investigation.\n\nStrengths: - The paper successfully addresses limitations observed in the MCMU method and presents an alternative solution, expanding the landscape of weakly supervised learning.\n- The introduction of a novel approach focusing on empirical risk minimization (ERM) at the instance level, treating individual data points, stands out as a significant strength.\n\nWeaknesses: - The experimental comparison is somewhat limited, primarily featuring methods from Tang et al., 2022. While justified due to MCMU's novelty, including additional weakly supervised methods could provide valuable reference points. Moreover, presenting accuracies for fully supervised learning on the same train/test splits would enhance the evaluation.\n- The paper lacks a comprehensive study of the method's limitations, particularly with regards to constraints on parameters such as m (the number of unlabeled sets) and n_i (the number of data points in each set), which are crucial for practical applicability.\n- Theoretical results like Theorem 3.5, which establishes an upper bound for the difference between true and empirical risk, would benefit from discussion regarding their practical implications and their relationship to the actual classification task involving class labels.\n\nQuestions: 1. The paper could further investigate the impact of small m (few unlabeled sets) and significant variations in label proportions across sets on the generalizability of its results with respect to m and n_i.\n   \n2. A clarification regarding the difference between Lemma 3.1 in the paper and Theorem 1 in Lu et al., ICML 2021, would be valuable for readers.\n\n3. Given that label proportions in real-world scenarios may only slightly differ, the paper should outline constraints on parameters m and n for practical significance, especially considering that a large m may be necessary for the method's effectiveness.\n\n4. A discussion on the practical relevance of the derived upper bound in Theorem 3.5 and its connection to the actual classification task involving class labels would provide valuable insights.\n\n5. The observation that the proposed approach performs significantly worse under certain settings, such as a random class prior matrix and non-constrained m, should be supported by clearer details on how test accuracies were computed, whether test data were balanced, and if class size variations in datasets played a role.\n\nOther suggestions:\n\n\"In MCMU, given the class priors, the data points in the same set are independent from each other, while in LLP, given the label proportions, the data points in\nthe same set are dependent from each other.\"\n\nThe second part of the sentence should clearly state whether data points are \"independent from each other\" or \"dependent on each other.\" If it implies independence, it raises a significant ambiguity regarding how MCMU and LLP fundamentally differ, as class priors and label proportions are practically equivalent concepts. Moreover, there appears to be a conceptual misuse in the paragraph. Data points within each class are typically assumed to be independent, conditioned on their class labels. The statement in question uses \"class priors\" in place of \"class labels\", which could introduce confusion and should be clarified. Overall, given that label proportions and class priors are practically the same thing in the context studied by the papers it is hard to see any difference from a dependence perspective between the two approaches. \n\nI think using the term \"label proportions\" when referring to class priors within each set would be more appropriate. In fact what the paper denotes as the j-th class prior of the i-th unlabeled set can be denoted as the label proportion of class j in dataset i. This would distinguish label proportions from class priors estimated using data from all available unlabeled sets.\n\n\nIn Section 4.1. \"The ni data points contained in i-th unlabeled set were randomly generated according to ...\" Isn't this supposed to be \"randomly sampled\" or \"drawn\".  Data already exists... nothing is supposed to be generated.\n\nIt would greatly enhance the paper if, within the related work section, an early explanation were provided regarding the reasons behind the possibility of negative empirical risk in these methods (Lu et al., 2019; Tsai & Lin, 2020; Tang et al., 2022). This proactive approach would help readers anticipate and better understand the issues related to negative empirical risk that the paper seeks to address.\n\nIn page 4 i is used for indexing both the sets and data points. It may cause confusions. \n\nIn Theorem 3.6 the paper defines this probability p(y=j|\\bar{y},x)as the probability of x in \\bar{y}-th unlabeled set whose ground-truth label is j. Isn't this supposed to be the probability of x in \\bar{y}-th unlabeled set belonging to class j? x may or may not belong to class j. x's ground truth label is not necessarily j. \n\nTheorem 3.6 also considers p(x|y=j)=p(x|y=j,\\bar{y}) as a fact. However, this is not necessarily true. The first one is the conditional distribution of class j estimated using all data belonging to class j whereas the second one is the conditional distribution of class j estimated using only data from unlabeled set \\bar{y}. So, from an empirical standpoint these distributions are not equal. Please clarify.\n\n\nAfter Rebuttal:\n\nThanks for taking the time to do those extra comparisons I mentioned. It’s helpful to see how your method holds up with different 'm' values and varying levels of class imbalance. Based on these new results,  I'm happy to bump up my score by one.", "labeling_timestamp": "2026-01-11T16:51:43.123159", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper contains the sentence describing the generation process as \"class priors → ground truth label y → data point x\" and explicitly states that in MCMU data points are independent given the class priors. These statements conflate the role of class priors with labels and assert independence conditioned on priors rather than on class labels, matching the reviewer's point.", "evidence": "“From the generation perspective : In MCMU, the generation process of a data point is: class priors Ñ ground truth label y Ñ data point x … From the dependence perspective : In MCMU, given the class priors, the data points in the same set are independent on each other, while in LLP, given the label proportions, the data points in the same set are dependent from each other.”", "section": "2.2 LEARNING FROM LABEL PROPORTIONS"}
{"claim": "Label proportions and class priors are practically equivalent in this context, so there is no clear dependence-based difference between MCMU and LLP.", "claim_type": "methodology", "paper_id": "fW7DOHDQvF", "paper_title": "Consistent Multi-Class Classification from Multiple Unlabeled Datasets", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "VC9qQQQ5cR", "reviewer": "Reviewer_XSqD", "review_text": "Summary: The paper introduces a novel approach to weakly supervised learning, specifically targeting multi-class classification from multiple unlabeled datasets with class priors. It addresses limitations identified in Tang et al., 2022, and offers two distinct approaches: a classifier-consistent method using a probability transition matrix and a risk-consistent approach employing importance weighting. However, certain areas require further clarification and empirical investigation.\n\nStrengths: - The paper successfully addresses limitations observed in the MCMU method and presents an alternative solution, expanding the landscape of weakly supervised learning.\n- The introduction of a novel approach focusing on empirical risk minimization (ERM) at the instance level, treating individual data points, stands out as a significant strength.\n\nWeaknesses: - The experimental comparison is somewhat limited, primarily featuring methods from Tang et al., 2022. While justified due to MCMU's novelty, including additional weakly supervised methods could provide valuable reference points. Moreover, presenting accuracies for fully supervised learning on the same train/test splits would enhance the evaluation.\n- The paper lacks a comprehensive study of the method's limitations, particularly with regards to constraints on parameters such as m (the number of unlabeled sets) and n_i (the number of data points in each set), which are crucial for practical applicability.\n- Theoretical results like Theorem 3.5, which establishes an upper bound for the difference between true and empirical risk, would benefit from discussion regarding their practical implications and their relationship to the actual classification task involving class labels.\n\nQuestions: 1. The paper could further investigate the impact of small m (few unlabeled sets) and significant variations in label proportions across sets on the generalizability of its results with respect to m and n_i.\n   \n2. A clarification regarding the difference between Lemma 3.1 in the paper and Theorem 1 in Lu et al., ICML 2021, would be valuable for readers.\n\n3. Given that label proportions in real-world scenarios may only slightly differ, the paper should outline constraints on parameters m and n for practical significance, especially considering that a large m may be necessary for the method's effectiveness.\n\n4. A discussion on the practical relevance of the derived upper bound in Theorem 3.5 and its connection to the actual classification task involving class labels would provide valuable insights.\n\n5. The observation that the proposed approach performs significantly worse under certain settings, such as a random class prior matrix and non-constrained m, should be supported by clearer details on how test accuracies were computed, whether test data were balanced, and if class size variations in datasets played a role.\n\nOther suggestions:\n\n\"In MCMU, given the class priors, the data points in the same set are independent from each other, while in LLP, given the label proportions, the data points in\nthe same set are dependent from each other.\"\n\nThe second part of the sentence should clearly state whether data points are \"independent from each other\" or \"dependent on each other.\" If it implies independence, it raises a significant ambiguity regarding how MCMU and LLP fundamentally differ, as class priors and label proportions are practically equivalent concepts. Moreover, there appears to be a conceptual misuse in the paragraph. Data points within each class are typically assumed to be independent, conditioned on their class labels. The statement in question uses \"class priors\" in place of \"class labels\", which could introduce confusion and should be clarified. Overall, given that label proportions and class priors are practically the same thing in the context studied by the papers it is hard to see any difference from a dependence perspective between the two approaches. \n\nI think using the term \"label proportions\" when referring to class priors within each set would be more appropriate. In fact what the paper denotes as the j-th class prior of the i-th unlabeled set can be denoted as the label proportion of class j in dataset i. This would distinguish label proportions from class priors estimated using data from all available unlabeled sets.\n\n\nIn Section 4.1. \"The ni data points contained in i-th unlabeled set were randomly generated according to ...\" Isn't this supposed to be \"randomly sampled\" or \"drawn\".  Data already exists... nothing is supposed to be generated.\n\nIt would greatly enhance the paper if, within the related work section, an early explanation were provided regarding the reasons behind the possibility of negative empirical risk in these methods (Lu et al., 2019; Tsai & Lin, 2020; Tang et al., 2022). This proactive approach would help readers anticipate and better understand the issues related to negative empirical risk that the paper seeks to address.\n\nIn page 4 i is used for indexing both the sets and data points. It may cause confusions. \n\nIn Theorem 3.6 the paper defines this probability p(y=j|\\bar{y},x)as the probability of x in \\bar{y}-th unlabeled set whose ground-truth label is j. Isn't this supposed to be the probability of x in \\bar{y}-th unlabeled set belonging to class j? x may or may not belong to class j. x's ground truth label is not necessarily j. \n\nTheorem 3.6 also considers p(x|y=j)=p(x|y=j,\\bar{y}) as a fact. However, this is not necessarily true. The first one is the conditional distribution of class j estimated using all data belonging to class j whereas the second one is the conditional distribution of class j estimated using only data from unlabeled set \\bar{y}. So, from an empirical standpoint these distributions are not equal. Please clarify.\n\n\nAfter Rebuttal:\n\nThanks for taking the time to do those extra comparisons I mentioned. It’s helpful to see how your method holds up with different 'm' values and varying levels of class imbalance. Based on these new results,  I'm happy to bump up my score by one.", "labeling_timestamp": "2026-01-11T16:51:29.724130", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly distinguishes MCMU from LLP and states a dependence-based difference: in MCMU data points in the same set are independent given class priors, whereas in LLP they are dependent given label proportions. Thus the claim that there is no clear dependence-based difference is contradicted by the paper.", "evidence": "From the dependence perspective : In MCMU, given the class priors, the data points in the same set are independent on each other, while in LLP, given the label proportions, the data points in the same set are dependent from each other.", "section": "2.2 LEARNING FROM LABEL PROPORTIONS"}
{"claim": "The paper should use the term 'label proportions' instead of 'class priors' within each set to avoid terminological confusion.", "claim_type": "presentation", "paper_id": "fW7DOHDQvF", "paper_title": "Consistent Multi-Class Classification from Multiple Unlabeled Datasets", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "VC9qQQQ5cR", "reviewer": "Reviewer_XSqD", "review_text": "Summary: The paper introduces a novel approach to weakly supervised learning, specifically targeting multi-class classification from multiple unlabeled datasets with class priors. It addresses limitations identified in Tang et al., 2022, and offers two distinct approaches: a classifier-consistent method using a probability transition matrix and a risk-consistent approach employing importance weighting. However, certain areas require further clarification and empirical investigation.\n\nStrengths: - The paper successfully addresses limitations observed in the MCMU method and presents an alternative solution, expanding the landscape of weakly supervised learning.\n- The introduction of a novel approach focusing on empirical risk minimization (ERM) at the instance level, treating individual data points, stands out as a significant strength.\n\nWeaknesses: - The experimental comparison is somewhat limited, primarily featuring methods from Tang et al., 2022. While justified due to MCMU's novelty, including additional weakly supervised methods could provide valuable reference points. Moreover, presenting accuracies for fully supervised learning on the same train/test splits would enhance the evaluation.\n- The paper lacks a comprehensive study of the method's limitations, particularly with regards to constraints on parameters such as m (the number of unlabeled sets) and n_i (the number of data points in each set), which are crucial for practical applicability.\n- Theoretical results like Theorem 3.5, which establishes an upper bound for the difference between true and empirical risk, would benefit from discussion regarding their practical implications and their relationship to the actual classification task involving class labels.\n\nQuestions: 1. The paper could further investigate the impact of small m (few unlabeled sets) and significant variations in label proportions across sets on the generalizability of its results with respect to m and n_i.\n   \n2. A clarification regarding the difference between Lemma 3.1 in the paper and Theorem 1 in Lu et al., ICML 2021, would be valuable for readers.\n\n3. Given that label proportions in real-world scenarios may only slightly differ, the paper should outline constraints on parameters m and n for practical significance, especially considering that a large m may be necessary for the method's effectiveness.\n\n4. A discussion on the practical relevance of the derived upper bound in Theorem 3.5 and its connection to the actual classification task involving class labels would provide valuable insights.\n\n5. The observation that the proposed approach performs significantly worse under certain settings, such as a random class prior matrix and non-constrained m, should be supported by clearer details on how test accuracies were computed, whether test data were balanced, and if class size variations in datasets played a role.\n\nOther suggestions:\n\n\"In MCMU, given the class priors, the data points in the same set are independent from each other, while in LLP, given the label proportions, the data points in\nthe same set are dependent from each other.\"\n\nThe second part of the sentence should clearly state whether data points are \"independent from each other\" or \"dependent on each other.\" If it implies independence, it raises a significant ambiguity regarding how MCMU and LLP fundamentally differ, as class priors and label proportions are practically equivalent concepts. Moreover, there appears to be a conceptual misuse in the paragraph. Data points within each class are typically assumed to be independent, conditioned on their class labels. The statement in question uses \"class priors\" in place of \"class labels\", which could introduce confusion and should be clarified. Overall, given that label proportions and class priors are practically the same thing in the context studied by the papers it is hard to see any difference from a dependence perspective between the two approaches. \n\nI think using the term \"label proportions\" when referring to class priors within each set would be more appropriate. In fact what the paper denotes as the j-th class prior of the i-th unlabeled set can be denoted as the label proportion of class j in dataset i. This would distinguish label proportions from class priors estimated using data from all available unlabeled sets.\n\n\nIn Section 4.1. \"The ni data points contained in i-th unlabeled set were randomly generated according to ...\" Isn't this supposed to be \"randomly sampled\" or \"drawn\".  Data already exists... nothing is supposed to be generated.\n\nIt would greatly enhance the paper if, within the related work section, an early explanation were provided regarding the reasons behind the possibility of negative empirical risk in these methods (Lu et al., 2019; Tsai & Lin, 2020; Tang et al., 2022). This proactive approach would help readers anticipate and better understand the issues related to negative empirical risk that the paper seeks to address.\n\nIn page 4 i is used for indexing both the sets and data points. It may cause confusions. \n\nIn Theorem 3.6 the paper defines this probability p(y=j|\\bar{y},x)as the probability of x in \\bar{y}-th unlabeled set whose ground-truth label is j. Isn't this supposed to be the probability of x in \\bar{y}-th unlabeled set belonging to class j? x may or may not belong to class j. x's ground truth label is not necessarily j. \n\nTheorem 3.6 also considers p(x|y=j)=p(x|y=j,\\bar{y}) as a fact. However, this is not necessarily true. The first one is the conditional distribution of class j estimated using all data belonging to class j whereas the second one is the conditional distribution of class j estimated using only data from unlabeled set \\bar{y}. So, from an empirical standpoint these distributions are not equal. Please clarify.\n\n\nAfter Rebuttal:\n\nThanks for taking the time to do those extra comparisons I mentioned. It’s helpful to see how your method holds up with different 'm' values and varying levels of class imbalance. Based on these new results,  I'm happy to bump up my score by one.", "labeling_timestamp": "2026-01-11T16:51:52.983973", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper already uses and explicitly equates 'class priors' with the proportion of each class and also discusses 'label proportions' in the LLP context, so the suggested terminology change is unnecessary and the paper does not exhibit terminological confusion.", "evidence": "Abstract: \"...multiple sets of unlabeled data and their class priors (i.e., the proportion of each class) are provided...\"; Section 2.2: \"This supervision provides similar information compared with class priors.\"; Section 2.2: \"the supervision is the proportion of instances from each class.\"", "section": "Abstract; 2.2 LEARNING FROM LABEL PROPORTIONS"}
{"claim": "Section 4.1 incorrectly uses 'randomly generated' when it should say 'randomly sampled' because data is drawn from existing distributions.", "claim_type": "methodology", "paper_id": "fW7DOHDQvF", "paper_title": "Consistent Multi-Class Classification from Multiple Unlabeled Datasets", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "VC9qQQQ5cR", "reviewer": "Reviewer_XSqD", "review_text": "Summary: The paper introduces a novel approach to weakly supervised learning, specifically targeting multi-class classification from multiple unlabeled datasets with class priors. It addresses limitations identified in Tang et al., 2022, and offers two distinct approaches: a classifier-consistent method using a probability transition matrix and a risk-consistent approach employing importance weighting. However, certain areas require further clarification and empirical investigation.\n\nStrengths: - The paper successfully addresses limitations observed in the MCMU method and presents an alternative solution, expanding the landscape of weakly supervised learning.\n- The introduction of a novel approach focusing on empirical risk minimization (ERM) at the instance level, treating individual data points, stands out as a significant strength.\n\nWeaknesses: - The experimental comparison is somewhat limited, primarily featuring methods from Tang et al., 2022. While justified due to MCMU's novelty, including additional weakly supervised methods could provide valuable reference points. Moreover, presenting accuracies for fully supervised learning on the same train/test splits would enhance the evaluation.\n- The paper lacks a comprehensive study of the method's limitations, particularly with regards to constraints on parameters such as m (the number of unlabeled sets) and n_i (the number of data points in each set), which are crucial for practical applicability.\n- Theoretical results like Theorem 3.5, which establishes an upper bound for the difference between true and empirical risk, would benefit from discussion regarding their practical implications and their relationship to the actual classification task involving class labels.\n\nQuestions: 1. The paper could further investigate the impact of small m (few unlabeled sets) and significant variations in label proportions across sets on the generalizability of its results with respect to m and n_i.\n   \n2. A clarification regarding the difference between Lemma 3.1 in the paper and Theorem 1 in Lu et al., ICML 2021, would be valuable for readers.\n\n3. Given that label proportions in real-world scenarios may only slightly differ, the paper should outline constraints on parameters m and n for practical significance, especially considering that a large m may be necessary for the method's effectiveness.\n\n4. A discussion on the practical relevance of the derived upper bound in Theorem 3.5 and its connection to the actual classification task involving class labels would provide valuable insights.\n\n5. The observation that the proposed approach performs significantly worse under certain settings, such as a random class prior matrix and non-constrained m, should be supported by clearer details on how test accuracies were computed, whether test data were balanced, and if class size variations in datasets played a role.\n\nOther suggestions:\n\n\"In MCMU, given the class priors, the data points in the same set are independent from each other, while in LLP, given the label proportions, the data points in\nthe same set are dependent from each other.\"\n\nThe second part of the sentence should clearly state whether data points are \"independent from each other\" or \"dependent on each other.\" If it implies independence, it raises a significant ambiguity regarding how MCMU and LLP fundamentally differ, as class priors and label proportions are practically equivalent concepts. Moreover, there appears to be a conceptual misuse in the paragraph. Data points within each class are typically assumed to be independent, conditioned on their class labels. The statement in question uses \"class priors\" in place of \"class labels\", which could introduce confusion and should be clarified. Overall, given that label proportions and class priors are practically the same thing in the context studied by the papers it is hard to see any difference from a dependence perspective between the two approaches. \n\nI think using the term \"label proportions\" when referring to class priors within each set would be more appropriate. In fact what the paper denotes as the j-th class prior of the i-th unlabeled set can be denoted as the label proportion of class j in dataset i. This would distinguish label proportions from class priors estimated using data from all available unlabeled sets.\n\n\nIn Section 4.1. \"The ni data points contained in i-th unlabeled set were randomly generated according to ...\" Isn't this supposed to be \"randomly sampled\" or \"drawn\".  Data already exists... nothing is supposed to be generated.\n\nIt would greatly enhance the paper if, within the related work section, an early explanation were provided regarding the reasons behind the possibility of negative empirical risk in these methods (Lu et al., 2019; Tsai & Lin, 2020; Tang et al., 2022). This proactive approach would help readers anticipate and better understand the issues related to negative empirical risk that the paper seeks to address.\n\nIn page 4 i is used for indexing both the sets and data points. It may cause confusions. \n\nIn Theorem 3.6 the paper defines this probability p(y=j|\\bar{y},x)as the probability of x in \\bar{y}-th unlabeled set whose ground-truth label is j. Isn't this supposed to be the probability of x in \\bar{y}-th unlabeled set belonging to class j? x may or may not belong to class j. x's ground truth label is not necessarily j. \n\nTheorem 3.6 also considers p(x|y=j)=p(x|y=j,\\bar{y}) as a fact. However, this is not necessarily true. The first one is the conditional distribution of class j estimated using all data belonging to class j whereas the second one is the conditional distribution of class j estimated using only data from unlabeled set \\bar{y}. So, from an empirical standpoint these distributions are not equal. Please clarify.\n\n\nAfter Rebuttal:\n\nThanks for taking the time to do those extra comparisons I mentioned. It’s helpful to see how your method holds up with different 'm' values and varying levels of class imbalance. Based on these new results,  I'm happy to bump up my score by one.", "labeling_timestamp": "2026-01-11T16:51:42.758634", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not include Section 4.1, so I cannot verify whether that section uses the phrase 'randomly generated' or whether that usage is incorrect. The paper does, however, state elsewhere that unlabeled sets and data points are 'sampled' or 'drawn' from distributions, which would align with using 'sampled' rather than 'randomly generated'.", "evidence": ["In MCMU, the unlabeled sets are sampled from distributions with different class priors,", "U_i = (x_{i,r})_{r=1}^{n_i} is a collection of n_i data points drawn from a mixture of class-conditional densities:"], "section": "Section 2.2 and Section 2.4 (no Section 4.1 provided in the excerpt)"}
{"claim": "The related work section lacks an early explanation for why negative empirical risk occurs in prior methods, which would help readers anticipate those issues.", "claim_type": "presentation", "paper_id": "fW7DOHDQvF", "paper_title": "Consistent Multi-Class Classification from Multiple Unlabeled Datasets", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "VC9qQQQ5cR", "reviewer": "Reviewer_XSqD", "review_text": "Summary: The paper introduces a novel approach to weakly supervised learning, specifically targeting multi-class classification from multiple unlabeled datasets with class priors. It addresses limitations identified in Tang et al., 2022, and offers two distinct approaches: a classifier-consistent method using a probability transition matrix and a risk-consistent approach employing importance weighting. However, certain areas require further clarification and empirical investigation.\n\nStrengths: - The paper successfully addresses limitations observed in the MCMU method and presents an alternative solution, expanding the landscape of weakly supervised learning.\n- The introduction of a novel approach focusing on empirical risk minimization (ERM) at the instance level, treating individual data points, stands out as a significant strength.\n\nWeaknesses: - The experimental comparison is somewhat limited, primarily featuring methods from Tang et al., 2022. While justified due to MCMU's novelty, including additional weakly supervised methods could provide valuable reference points. Moreover, presenting accuracies for fully supervised learning on the same train/test splits would enhance the evaluation.\n- The paper lacks a comprehensive study of the method's limitations, particularly with regards to constraints on parameters such as m (the number of unlabeled sets) and n_i (the number of data points in each set), which are crucial for practical applicability.\n- Theoretical results like Theorem 3.5, which establishes an upper bound for the difference between true and empirical risk, would benefit from discussion regarding their practical implications and their relationship to the actual classification task involving class labels.\n\nQuestions: 1. The paper could further investigate the impact of small m (few unlabeled sets) and significant variations in label proportions across sets on the generalizability of its results with respect to m and n_i.\n   \n2. A clarification regarding the difference between Lemma 3.1 in the paper and Theorem 1 in Lu et al., ICML 2021, would be valuable for readers.\n\n3. Given that label proportions in real-world scenarios may only slightly differ, the paper should outline constraints on parameters m and n for practical significance, especially considering that a large m may be necessary for the method's effectiveness.\n\n4. A discussion on the practical relevance of the derived upper bound in Theorem 3.5 and its connection to the actual classification task involving class labels would provide valuable insights.\n\n5. The observation that the proposed approach performs significantly worse under certain settings, such as a random class prior matrix and non-constrained m, should be supported by clearer details on how test accuracies were computed, whether test data were balanced, and if class size variations in datasets played a role.\n\nOther suggestions:\n\n\"In MCMU, given the class priors, the data points in the same set are independent from each other, while in LLP, given the label proportions, the data points in\nthe same set are dependent from each other.\"\n\nThe second part of the sentence should clearly state whether data points are \"independent from each other\" or \"dependent on each other.\" If it implies independence, it raises a significant ambiguity regarding how MCMU and LLP fundamentally differ, as class priors and label proportions are practically equivalent concepts. Moreover, there appears to be a conceptual misuse in the paragraph. Data points within each class are typically assumed to be independent, conditioned on their class labels. The statement in question uses \"class priors\" in place of \"class labels\", which could introduce confusion and should be clarified. Overall, given that label proportions and class priors are practically the same thing in the context studied by the papers it is hard to see any difference from a dependence perspective between the two approaches. \n\nI think using the term \"label proportions\" when referring to class priors within each set would be more appropriate. In fact what the paper denotes as the j-th class prior of the i-th unlabeled set can be denoted as the label proportion of class j in dataset i. This would distinguish label proportions from class priors estimated using data from all available unlabeled sets.\n\n\nIn Section 4.1. \"The ni data points contained in i-th unlabeled set were randomly generated according to ...\" Isn't this supposed to be \"randomly sampled\" or \"drawn\".  Data already exists... nothing is supposed to be generated.\n\nIt would greatly enhance the paper if, within the related work section, an early explanation were provided regarding the reasons behind the possibility of negative empirical risk in these methods (Lu et al., 2019; Tsai & Lin, 2020; Tang et al., 2022). This proactive approach would help readers anticipate and better understand the issues related to negative empirical risk that the paper seeks to address.\n\nIn page 4 i is used for indexing both the sets and data points. It may cause confusions. \n\nIn Theorem 3.6 the paper defines this probability p(y=j|\\bar{y},x)as the probability of x in \\bar{y}-th unlabeled set whose ground-truth label is j. Isn't this supposed to be the probability of x in \\bar{y}-th unlabeled set belonging to class j? x may or may not belong to class j. x's ground truth label is not necessarily j. \n\nTheorem 3.6 also considers p(x|y=j)=p(x|y=j,\\bar{y}) as a fact. However, this is not necessarily true. The first one is the conditional distribution of class j estimated using all data belonging to class j whereas the second one is the conditional distribution of class j estimated using only data from unlabeled set \\bar{y}. So, from an empirical standpoint these distributions are not equal. Please clarify.\n\n\nAfter Rebuttal:\n\nThanks for taking the time to do those extra comparisons I mentioned. It’s helpful to see how your method holds up with different 'm' values and varying levels of class imbalance. Based on these new results,  I'm happy to bump up my score by one.", "labeling_timestamp": "2026-01-11T16:52:00.295044", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper's related-work/materials mention the 'negative empirical risk' issue (what it is and that it poses a challenge) but do not provide an early explanation of why prior methods produce negative empirical risk or the mechanism causing it. Thus the reviewer's claim that the related work lacks an early explanation for why negative empirical risk occurs is supported.", "evidence": "From Section 2.3: \"Some methods would encounter the negative empirical risk issue (Lu et al., 2019; Tsai & Lin, 2020; Tang et al., 2022), i.e., the risk could empirically become negative during the training process. This poses a serious challenge to the optimization process since it is problematic to minimize an objective that can be unbounded from below.\" Also from the Introduction: \"However, this method results in an unreasonable training objective caused by the negative empirical risk. Although a partial-risk regularization term is further proposed to alleviate the negative empirical risk issue, the unbiasedness of the risk estimator is actually broken, hence the theoretical guarantee cannot hold, and thus the performance is still suboptimal.\"", "section": "2.3 PREVIOUS STUDIES ON CLASSIFICATION FROM UNLABELED DATASETS (and Introduction)"}
{"claim": "The paper uses the index i both for unlabeled sets and for data points, creating potential notational confusion for readers.", "claim_type": "methodology", "paper_id": "fW7DOHDQvF", "paper_title": "Consistent Multi-Class Classification from Multiple Unlabeled Datasets", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "VC9qQQQ5cR", "reviewer": "Reviewer_XSqD", "review_text": "Summary: The paper introduces a novel approach to weakly supervised learning, specifically targeting multi-class classification from multiple unlabeled datasets with class priors. It addresses limitations identified in Tang et al., 2022, and offers two distinct approaches: a classifier-consistent method using a probability transition matrix and a risk-consistent approach employing importance weighting. However, certain areas require further clarification and empirical investigation.\n\nStrengths: - The paper successfully addresses limitations observed in the MCMU method and presents an alternative solution, expanding the landscape of weakly supervised learning.\n- The introduction of a novel approach focusing on empirical risk minimization (ERM) at the instance level, treating individual data points, stands out as a significant strength.\n\nWeaknesses: - The experimental comparison is somewhat limited, primarily featuring methods from Tang et al., 2022. While justified due to MCMU's novelty, including additional weakly supervised methods could provide valuable reference points. Moreover, presenting accuracies for fully supervised learning on the same train/test splits would enhance the evaluation.\n- The paper lacks a comprehensive study of the method's limitations, particularly with regards to constraints on parameters such as m (the number of unlabeled sets) and n_i (the number of data points in each set), which are crucial for practical applicability.\n- Theoretical results like Theorem 3.5, which establishes an upper bound for the difference between true and empirical risk, would benefit from discussion regarding their practical implications and their relationship to the actual classification task involving class labels.\n\nQuestions: 1. The paper could further investigate the impact of small m (few unlabeled sets) and significant variations in label proportions across sets on the generalizability of its results with respect to m and n_i.\n   \n2. A clarification regarding the difference between Lemma 3.1 in the paper and Theorem 1 in Lu et al., ICML 2021, would be valuable for readers.\n\n3. Given that label proportions in real-world scenarios may only slightly differ, the paper should outline constraints on parameters m and n for practical significance, especially considering that a large m may be necessary for the method's effectiveness.\n\n4. A discussion on the practical relevance of the derived upper bound in Theorem 3.5 and its connection to the actual classification task involving class labels would provide valuable insights.\n\n5. The observation that the proposed approach performs significantly worse under certain settings, such as a random class prior matrix and non-constrained m, should be supported by clearer details on how test accuracies were computed, whether test data were balanced, and if class size variations in datasets played a role.\n\nOther suggestions:\n\n\"In MCMU, given the class priors, the data points in the same set are independent from each other, while in LLP, given the label proportions, the data points in\nthe same set are dependent from each other.\"\n\nThe second part of the sentence should clearly state whether data points are \"independent from each other\" or \"dependent on each other.\" If it implies independence, it raises a significant ambiguity regarding how MCMU and LLP fundamentally differ, as class priors and label proportions are practically equivalent concepts. Moreover, there appears to be a conceptual misuse in the paragraph. Data points within each class are typically assumed to be independent, conditioned on their class labels. The statement in question uses \"class priors\" in place of \"class labels\", which could introduce confusion and should be clarified. Overall, given that label proportions and class priors are practically the same thing in the context studied by the papers it is hard to see any difference from a dependence perspective between the two approaches. \n\nI think using the term \"label proportions\" when referring to class priors within each set would be more appropriate. In fact what the paper denotes as the j-th class prior of the i-th unlabeled set can be denoted as the label proportion of class j in dataset i. This would distinguish label proportions from class priors estimated using data from all available unlabeled sets.\n\n\nIn Section 4.1. \"The ni data points contained in i-th unlabeled set were randomly generated according to ...\" Isn't this supposed to be \"randomly sampled\" or \"drawn\".  Data already exists... nothing is supposed to be generated.\n\nIt would greatly enhance the paper if, within the related work section, an early explanation were provided regarding the reasons behind the possibility of negative empirical risk in these methods (Lu et al., 2019; Tsai & Lin, 2020; Tang et al., 2022). This proactive approach would help readers anticipate and better understand the issues related to negative empirical risk that the paper seeks to address.\n\nIn page 4 i is used for indexing both the sets and data points. It may cause confusions. \n\nIn Theorem 3.6 the paper defines this probability p(y=j|\\bar{y},x)as the probability of x in \\bar{y}-th unlabeled set whose ground-truth label is j. Isn't this supposed to be the probability of x in \\bar{y}-th unlabeled set belonging to class j? x may or may not belong to class j. x's ground truth label is not necessarily j. \n\nTheorem 3.6 also considers p(x|y=j)=p(x|y=j,\\bar{y}) as a fact. However, this is not necessarily true. The first one is the conditional distribution of class j estimated using all data belonging to class j whereas the second one is the conditional distribution of class j estimated using only data from unlabeled set \\bar{y}. So, from an empirical standpoint these distributions are not equal. Please clarify.\n\n\nAfter Rebuttal:\n\nThanks for taking the time to do those extra comparisons I mentioned. It’s helpful to see how your method holds up with different 'm' values and varying levels of class imbalance. Based on these new results,  I'm happy to bump up my score by one.", "labeling_timestamp": "2026-01-11T16:52:11.606780", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper consistently uses i to index unlabeled sets and uses a separate index r for data points within a set; it does not use i to index individual data points, so the claimed notational conflation does not occur.", "evidence": "Given m ( m ě 2 ) sets of unlabeled data D : ' Ť m i ' 1 U i where U i ' t x i,r u n i r ' 1 is a collection of n i data points drawn from a mixture of class-conditional densities:", "section": "2.4 MULTI-CLASS CLASSIFICATION FROM MULTIPLE UNLABELED DATASETS"}
{"claim": "Theorem 3.6 defines p(y=j|\\bar{y},x) as the probability that x's ground-truth label is j, which is misleading; it should denote the probability x belongs to class j.", "claim_type": "methodology", "paper_id": "fW7DOHDQvF", "paper_title": "Consistent Multi-Class Classification from Multiple Unlabeled Datasets", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "VC9qQQQ5cR", "reviewer": "Reviewer_XSqD", "review_text": "Summary: The paper introduces a novel approach to weakly supervised learning, specifically targeting multi-class classification from multiple unlabeled datasets with class priors. It addresses limitations identified in Tang et al., 2022, and offers two distinct approaches: a classifier-consistent method using a probability transition matrix and a risk-consistent approach employing importance weighting. However, certain areas require further clarification and empirical investigation.\n\nStrengths: - The paper successfully addresses limitations observed in the MCMU method and presents an alternative solution, expanding the landscape of weakly supervised learning.\n- The introduction of a novel approach focusing on empirical risk minimization (ERM) at the instance level, treating individual data points, stands out as a significant strength.\n\nWeaknesses: - The experimental comparison is somewhat limited, primarily featuring methods from Tang et al., 2022. While justified due to MCMU's novelty, including additional weakly supervised methods could provide valuable reference points. Moreover, presenting accuracies for fully supervised learning on the same train/test splits would enhance the evaluation.\n- The paper lacks a comprehensive study of the method's limitations, particularly with regards to constraints on parameters such as m (the number of unlabeled sets) and n_i (the number of data points in each set), which are crucial for practical applicability.\n- Theoretical results like Theorem 3.5, which establishes an upper bound for the difference between true and empirical risk, would benefit from discussion regarding their practical implications and their relationship to the actual classification task involving class labels.\n\nQuestions: 1. The paper could further investigate the impact of small m (few unlabeled sets) and significant variations in label proportions across sets on the generalizability of its results with respect to m and n_i.\n   \n2. A clarification regarding the difference between Lemma 3.1 in the paper and Theorem 1 in Lu et al., ICML 2021, would be valuable for readers.\n\n3. Given that label proportions in real-world scenarios may only slightly differ, the paper should outline constraints on parameters m and n for practical significance, especially considering that a large m may be necessary for the method's effectiveness.\n\n4. A discussion on the practical relevance of the derived upper bound in Theorem 3.5 and its connection to the actual classification task involving class labels would provide valuable insights.\n\n5. The observation that the proposed approach performs significantly worse under certain settings, such as a random class prior matrix and non-constrained m, should be supported by clearer details on how test accuracies were computed, whether test data were balanced, and if class size variations in datasets played a role.\n\nOther suggestions:\n\n\"In MCMU, given the class priors, the data points in the same set are independent from each other, while in LLP, given the label proportions, the data points in\nthe same set are dependent from each other.\"\n\nThe second part of the sentence should clearly state whether data points are \"independent from each other\" or \"dependent on each other.\" If it implies independence, it raises a significant ambiguity regarding how MCMU and LLP fundamentally differ, as class priors and label proportions are practically equivalent concepts. Moreover, there appears to be a conceptual misuse in the paragraph. Data points within each class are typically assumed to be independent, conditioned on their class labels. The statement in question uses \"class priors\" in place of \"class labels\", which could introduce confusion and should be clarified. Overall, given that label proportions and class priors are practically the same thing in the context studied by the papers it is hard to see any difference from a dependence perspective between the two approaches. \n\nI think using the term \"label proportions\" when referring to class priors within each set would be more appropriate. In fact what the paper denotes as the j-th class prior of the i-th unlabeled set can be denoted as the label proportion of class j in dataset i. This would distinguish label proportions from class priors estimated using data from all available unlabeled sets.\n\n\nIn Section 4.1. \"The ni data points contained in i-th unlabeled set were randomly generated according to ...\" Isn't this supposed to be \"randomly sampled\" or \"drawn\".  Data already exists... nothing is supposed to be generated.\n\nIt would greatly enhance the paper if, within the related work section, an early explanation were provided regarding the reasons behind the possibility of negative empirical risk in these methods (Lu et al., 2019; Tsai & Lin, 2020; Tang et al., 2022). This proactive approach would help readers anticipate and better understand the issues related to negative empirical risk that the paper seeks to address.\n\nIn page 4 i is used for indexing both the sets and data points. It may cause confusions. \n\nIn Theorem 3.6 the paper defines this probability p(y=j|\\bar{y},x)as the probability of x in \\bar{y}-th unlabeled set whose ground-truth label is j. Isn't this supposed to be the probability of x in \\bar{y}-th unlabeled set belonging to class j? x may or may not belong to class j. x's ground truth label is not necessarily j. \n\nTheorem 3.6 also considers p(x|y=j)=p(x|y=j,\\bar{y}) as a fact. However, this is not necessarily true. The first one is the conditional distribution of class j estimated using all data belonging to class j whereas the second one is the conditional distribution of class j estimated using only data from unlabeled set \\bar{y}. So, from an empirical standpoint these distributions are not equal. Please clarify.\n\n\nAfter Rebuttal:\n\nThanks for taking the time to do those extra comparisons I mentioned. It’s helpful to see how your method holds up with different 'm' values and varying levels of class imbalance. Based on these new results,  I'm happy to bump up my score by one.", "labeling_timestamp": "2026-01-11T16:52:23.132061", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper defines y as the true label (class membership) and ¯y as the index of the unlabeled set; therefore p(y = j | ¯y, x) naturally denotes the probability that x's true label is j (i.e., that x belongs to class j). The reviewer's claim that this is misleading is not supported by the paper's notation and definitions.", "evidence": "\"Let x P X be an instance and y P Y be a label, and each example p x , y q P X ˆ Y is assumed to be drawn from an underlying joint distribution with probability density p p x , y q .\"; \"where θ P R m ˆ k and 0 ď θ ij ď 1 denotes the j -th class prior of the i -th unlabeled set, and ¯ y P t 1 , 2 , . . . , m u denotes the index of the m sets of unlabeled instances.\"; \"Let π denote the original class priors (the class priors over all m unlabeled sets), i.e., π j ' p p y ' j q.\"", "section": "Sections 2.1 and 2.4 (Problem setup and notation)"}
{"claim": "Theorem 3.6 assumes p(x|y=j)=p(x|y=j,\\bar{y}), but conditional distributions estimated from all class j data and from a single unlabeled set may differ in practice.", "claim_type": "methodology", "paper_id": "fW7DOHDQvF", "paper_title": "Consistent Multi-Class Classification from Multiple Unlabeled Datasets", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "VC9qQQQ5cR", "reviewer": "Reviewer_XSqD", "review_text": "Summary: The paper introduces a novel approach to weakly supervised learning, specifically targeting multi-class classification from multiple unlabeled datasets with class priors. It addresses limitations identified in Tang et al., 2022, and offers two distinct approaches: a classifier-consistent method using a probability transition matrix and a risk-consistent approach employing importance weighting. However, certain areas require further clarification and empirical investigation.\n\nStrengths: - The paper successfully addresses limitations observed in the MCMU method and presents an alternative solution, expanding the landscape of weakly supervised learning.\n- The introduction of a novel approach focusing on empirical risk minimization (ERM) at the instance level, treating individual data points, stands out as a significant strength.\n\nWeaknesses: - The experimental comparison is somewhat limited, primarily featuring methods from Tang et al., 2022. While justified due to MCMU's novelty, including additional weakly supervised methods could provide valuable reference points. Moreover, presenting accuracies for fully supervised learning on the same train/test splits would enhance the evaluation.\n- The paper lacks a comprehensive study of the method's limitations, particularly with regards to constraints on parameters such as m (the number of unlabeled sets) and n_i (the number of data points in each set), which are crucial for practical applicability.\n- Theoretical results like Theorem 3.5, which establishes an upper bound for the difference between true and empirical risk, would benefit from discussion regarding their practical implications and their relationship to the actual classification task involving class labels.\n\nQuestions: 1. The paper could further investigate the impact of small m (few unlabeled sets) and significant variations in label proportions across sets on the generalizability of its results with respect to m and n_i.\n   \n2. A clarification regarding the difference between Lemma 3.1 in the paper and Theorem 1 in Lu et al., ICML 2021, would be valuable for readers.\n\n3. Given that label proportions in real-world scenarios may only slightly differ, the paper should outline constraints on parameters m and n for practical significance, especially considering that a large m may be necessary for the method's effectiveness.\n\n4. A discussion on the practical relevance of the derived upper bound in Theorem 3.5 and its connection to the actual classification task involving class labels would provide valuable insights.\n\n5. The observation that the proposed approach performs significantly worse under certain settings, such as a random class prior matrix and non-constrained m, should be supported by clearer details on how test accuracies were computed, whether test data were balanced, and if class size variations in datasets played a role.\n\nOther suggestions:\n\n\"In MCMU, given the class priors, the data points in the same set are independent from each other, while in LLP, given the label proportions, the data points in\nthe same set are dependent from each other.\"\n\nThe second part of the sentence should clearly state whether data points are \"independent from each other\" or \"dependent on each other.\" If it implies independence, it raises a significant ambiguity regarding how MCMU and LLP fundamentally differ, as class priors and label proportions are practically equivalent concepts. Moreover, there appears to be a conceptual misuse in the paragraph. Data points within each class are typically assumed to be independent, conditioned on their class labels. The statement in question uses \"class priors\" in place of \"class labels\", which could introduce confusion and should be clarified. Overall, given that label proportions and class priors are practically the same thing in the context studied by the papers it is hard to see any difference from a dependence perspective between the two approaches. \n\nI think using the term \"label proportions\" when referring to class priors within each set would be more appropriate. In fact what the paper denotes as the j-th class prior of the i-th unlabeled set can be denoted as the label proportion of class j in dataset i. This would distinguish label proportions from class priors estimated using data from all available unlabeled sets.\n\n\nIn Section 4.1. \"The ni data points contained in i-th unlabeled set were randomly generated according to ...\" Isn't this supposed to be \"randomly sampled\" or \"drawn\".  Data already exists... nothing is supposed to be generated.\n\nIt would greatly enhance the paper if, within the related work section, an early explanation were provided regarding the reasons behind the possibility of negative empirical risk in these methods (Lu et al., 2019; Tsai & Lin, 2020; Tang et al., 2022). This proactive approach would help readers anticipate and better understand the issues related to negative empirical risk that the paper seeks to address.\n\nIn page 4 i is used for indexing both the sets and data points. It may cause confusions. \n\nIn Theorem 3.6 the paper defines this probability p(y=j|\\bar{y},x)as the probability of x in \\bar{y}-th unlabeled set whose ground-truth label is j. Isn't this supposed to be the probability of x in \\bar{y}-th unlabeled set belonging to class j? x may or may not belong to class j. x's ground truth label is not necessarily j. \n\nTheorem 3.6 also considers p(x|y=j)=p(x|y=j,\\bar{y}) as a fact. However, this is not necessarily true. The first one is the conditional distribution of class j estimated using all data belonging to class j whereas the second one is the conditional distribution of class j estimated using only data from unlabeled set \\bar{y}. So, from an empirical standpoint these distributions are not equal. Please clarify.\n\n\nAfter Rebuttal:\n\nThanks for taking the time to do those extra comparisons I mentioned. It’s helpful to see how your method holds up with different 'm' values and varying levels of class imbalance. Based on these new results,  I'm happy to bump up my score by one.", "labeling_timestamp": "2026-01-11T16:52:25.528147", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper models each unlabeled set as a mixture of the same class-conditional densities (p(x|y=j)), i.e. x is generated from y and the class-conditionals do not depend on the set index ¯y. Thus Theorem 3.6 (in section 3) would be operating under this modeling assumption (p(x|y=j)=p(x|y=j,¯y)). The paper does not discuss deviations from this assumption in practical datasets.", "evidence": "From Section 2.4: \"U_i ... is a collection of n_i data points drawn from a mixture of class-conditional densities: [formula not decoded] where θ ∈ R^{m×k} and 0 ≤ θ_{ij} ≤ 1 denotes the j-th class prior of the i-th unlabeled set, and ¯y ∈ {1,2,...,m} denotes the index of the m sets of unlabeled instances.\" From Section 2.2: \"In MCMU, the generation process of a data point is: class priors → ground truth label y → data point x.\"", "section": "2.4 MULTI-CLASS CLASSIFICATION FROM MULTIPLE UNLABELED DATASETS; 2.2 LEARNING FROM LABEL PROPORTIONS"}
{"claim": "The paper lacks justification for the testing framework used for Python and Java functions in BioCoder, with no detailed description of its design or rationale.", "claim_type": "methodology", "paper_id": "JbOsMrwjZ3", "paper_title": "BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "zqRMRjR9Zo", "reviewer": "Reviewer_r6TX", "review_text": "Summary: This paper presents a large-scale benchmark, BioCoder, which is devoted to assessing the capability of LLMs regarding code generation, specifically in the field of bioinformatics. The authors collect 1026 functions and 1243 methods in two programming languages (Python and Java) from GitHub and 253 examples from the Rosalind project to form a relatively intricate dataset to evaluate the LLMs' code generation abilities from various aspects. The authors conduct multiple steps to ensure the validity and unbiasedness of the constructed benchmark. In particular, 10 different LLMs (including the fine-tuned one) are evaluated on BioCoder, and the experiments reveal what factors can potentially affect the performance of LLMs while tackling challenging code generation tasks.\n\nStrengths: 1. Timely and vital problem.\n2. A valuable large-scale benchmark for code generation, including specialized domain knowledge.\n3. Comprehensive evaluation with multiple LLMs.\n4. The presentation is in a good manner, and the paper is easy to follow.\n\nWeaknesses: I appreciate that this paper has provided a valuable benchmark to the communities, as the new benchmark can potentially help researchers and practitioners in this direction. However, there are several concerns regarding the methodology and evaluation of this paper, which I will elaborate on below:\n\n\n1. My biggest concern is the lack of justifications for the testing framework for Python and Java functions in BioCoder. The authors state, \"... we employ a custom syntax to indicate the insertion points for custom randomly generated test cases.\" How are the test cases \"randomly generated\" and inserted into the context files? I did not find any detailed explanations in the main paper or in Appendix L and Y. In particular, Appendix L only briefly introduces the pipeline of the testing framework, how does such an approach deliver \"a secure and efficient testing framework, promising robustness in the evaluation of generated code\"? The authors need to clarify more about the generation of the test cases.\n\n2. In addition to the previous point, for Rosalind functions, the authors mentioned, \"...the output of this execution is compared with the cached golden code output.\" Why and how are the generated codes compared with the gold code outputs? I do not find any experiment results that illustrate the comparison outcomes.\n\n3. Another concern is the implementation of correction mechanisms which rectify minor syntax and style errors. What kind of syntax and style errors can be considered \"minor\" with no impact on the functionality of the generated programs? As the authors take invalid syntax and runtime error as two major failure reasons in the following error distribution analysis, I recommend further justifying such correction mechanisms, which may affect the validity of the analysis results.\n\n4. Table 4 summarizes the performance of the studied LLMs on BioCoder w.r.t 4 different types of prompt formats. However, the explanations of the different prompt versions are placed in Appendix I, which makes Table 4 hard to understand. Moreover, Appendix I only gives explanations with examples of the prompts in each version; nevertheless, I am looking for some high-level guidelines for the prompt design. Namely, how the five prompt versions are proposed? Are they from existing lectures or experimental experience? What are the characteristics of different prompt formats?\n\n5. The discussion of the experiment results seems shallow to me. In section 5, the authors consider there is an inverse relationship between the length of the input prompts and the performance of the generated codes. However, from Table 4 and Appendix I, the Necessary only prompts have relatively shorter prompts but lower passing rates compared to uncommented and Summary at Top/Bottom in most of the studied LLMs. The author may elaborate more on the perspectives of prompt structures and contents instead of just the length of the prompts.\n\n\nMinor Comments\n\n1. The \"Summary At Bottom\" results illustrated in Appendix U seem incomplete (no row for GPT-4). \n\n2. From section 3.4, \"Our testing framework starts with a manual review of selected functions, leading to the creation of a context file and a golden code file for each problem (see Figure 3)\". I do not find how Figure 3 is correlated with the testing framework, Figure 17 in Appendix R may be a better example.\n\nQuestions: 1. The details of the testing framework and the corresponding effectiveness should be discussed.\n\n2. For Rosalind functions, why and how are the generated codes compared with the gold code outputs?\n\n3. What are the guidelines while designing the 5 different prompt styles for the subject LLMs?", "labeling_timestamp": "2026-01-11T16:52:45.313900", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly states it uses a fuzzing / dockerized testing framework and gives a high-level rationale (execution-based evaluation is standard and scalable), but the provided content does not include a detailed design description or in-depth justification for the testing framework specific to the Python and Java functions (no architecture, algorithms, test-generation detail, or appendix reference for testing is given).", "evidence": "\"BIOCODER incorporates a fuzz-testing framework for evaluation.\"; \"(4) We provide a fuzzer testing tool capable of scaling to handle substantial datasets. Our benchmark results, derived from 1000 iterations, are particularly reliable, indicating the Pass@K rate.\"; \"context and test case creation and a massively dockerized testing framework.\"; \"Both our approach and CoderEval ... employ Docker-based testing.\"", "section": "Abstract; Introduction (key highlights); Figure 2 caption; Section 2.2"}
{"claim": "The process by which test cases are described as 'randomly generated' and inserted into context files is not explained in the main paper or appendices L and Y.", "claim_type": "methodology", "paper_id": "JbOsMrwjZ3", "paper_title": "BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "zqRMRjR9Zo", "reviewer": "Reviewer_r6TX", "review_text": "Summary: This paper presents a large-scale benchmark, BioCoder, which is devoted to assessing the capability of LLMs regarding code generation, specifically in the field of bioinformatics. The authors collect 1026 functions and 1243 methods in two programming languages (Python and Java) from GitHub and 253 examples from the Rosalind project to form a relatively intricate dataset to evaluate the LLMs' code generation abilities from various aspects. The authors conduct multiple steps to ensure the validity and unbiasedness of the constructed benchmark. In particular, 10 different LLMs (including the fine-tuned one) are evaluated on BioCoder, and the experiments reveal what factors can potentially affect the performance of LLMs while tackling challenging code generation tasks.\n\nStrengths: 1. Timely and vital problem.\n2. A valuable large-scale benchmark for code generation, including specialized domain knowledge.\n3. Comprehensive evaluation with multiple LLMs.\n4. The presentation is in a good manner, and the paper is easy to follow.\n\nWeaknesses: I appreciate that this paper has provided a valuable benchmark to the communities, as the new benchmark can potentially help researchers and practitioners in this direction. However, there are several concerns regarding the methodology and evaluation of this paper, which I will elaborate on below:\n\n\n1. My biggest concern is the lack of justifications for the testing framework for Python and Java functions in BioCoder. The authors state, \"... we employ a custom syntax to indicate the insertion points for custom randomly generated test cases.\" How are the test cases \"randomly generated\" and inserted into the context files? I did not find any detailed explanations in the main paper or in Appendix L and Y. In particular, Appendix L only briefly introduces the pipeline of the testing framework, how does such an approach deliver \"a secure and efficient testing framework, promising robustness in the evaluation of generated code\"? The authors need to clarify more about the generation of the test cases.\n\n2. In addition to the previous point, for Rosalind functions, the authors mentioned, \"...the output of this execution is compared with the cached golden code output.\" Why and how are the generated codes compared with the gold code outputs? I do not find any experiment results that illustrate the comparison outcomes.\n\n3. Another concern is the implementation of correction mechanisms which rectify minor syntax and style errors. What kind of syntax and style errors can be considered \"minor\" with no impact on the functionality of the generated programs? As the authors take invalid syntax and runtime error as two major failure reasons in the following error distribution analysis, I recommend further justifying such correction mechanisms, which may affect the validity of the analysis results.\n\n4. Table 4 summarizes the performance of the studied LLMs on BioCoder w.r.t 4 different types of prompt formats. However, the explanations of the different prompt versions are placed in Appendix I, which makes Table 4 hard to understand. Moreover, Appendix I only gives explanations with examples of the prompts in each version; nevertheless, I am looking for some high-level guidelines for the prompt design. Namely, how the five prompt versions are proposed? Are they from existing lectures or experimental experience? What are the characteristics of different prompt formats?\n\n5. The discussion of the experiment results seems shallow to me. In section 5, the authors consider there is an inverse relationship between the length of the input prompts and the performance of the generated codes. However, from Table 4 and Appendix I, the Necessary only prompts have relatively shorter prompts but lower passing rates compared to uncommented and Summary at Top/Bottom in most of the studied LLMs. The author may elaborate more on the perspectives of prompt structures and contents instead of just the length of the prompts.\n\n\nMinor Comments\n\n1. The \"Summary At Bottom\" results illustrated in Appendix U seem incomplete (no row for GPT-4). \n\n2. From section 3.4, \"Our testing framework starts with a manual review of selected functions, leading to the creation of a context file and a golden code file for each problem (see Figure 3)\". I do not find how Figure 3 is correlated with the testing framework, Figure 17 in Appendix R may be a better example.\n\nQuestions: 1. The details of the testing framework and the corresponding effectiveness should be discussed.\n\n2. For Rosalind functions, why and how are the generated codes compared with the gold code outputs?\n\n3. What are the guidelines while designing the 5 different prompt styles for the subject LLMs?", "labeling_timestamp": "2026-01-11T16:52:40.317020", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper only mentions a fuzz-testing framework and test-case creation at a high level (Abstract, Introduction, Figure 2) but provides no detailed description of the process for 'randomly generated' test cases or how they are inserted into context files in the main paper. The provided content also does not include or reference appendices L or Y with such details.", "evidence": ["BIOCODER incorporates a fuzz-testing framework for evaluation.", "We offer a fuzzer testing tool capable of scaling to handle substantial datasets.", "Figure 2: A diagram of the BIOCODER construction process involving custom GitHub repository cleaning, parsing, and function selection, as well as context and test case creation and a massively dockerized testing framework."], "section": "Abstract; 1 INTRODUCTION; Figure 2 caption"}
{"claim": "Appendix L only briefly outlines the testing pipeline and does not explain how the approach provides a 'secure and efficient testing framework' or guarantees evaluation robustness.", "claim_type": "methodology", "paper_id": "JbOsMrwjZ3", "paper_title": "BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "zqRMRjR9Zo", "reviewer": "Reviewer_r6TX", "review_text": "Summary: This paper presents a large-scale benchmark, BioCoder, which is devoted to assessing the capability of LLMs regarding code generation, specifically in the field of bioinformatics. The authors collect 1026 functions and 1243 methods in two programming languages (Python and Java) from GitHub and 253 examples from the Rosalind project to form a relatively intricate dataset to evaluate the LLMs' code generation abilities from various aspects. The authors conduct multiple steps to ensure the validity and unbiasedness of the constructed benchmark. In particular, 10 different LLMs (including the fine-tuned one) are evaluated on BioCoder, and the experiments reveal what factors can potentially affect the performance of LLMs while tackling challenging code generation tasks.\n\nStrengths: 1. Timely and vital problem.\n2. A valuable large-scale benchmark for code generation, including specialized domain knowledge.\n3. Comprehensive evaluation with multiple LLMs.\n4. The presentation is in a good manner, and the paper is easy to follow.\n\nWeaknesses: I appreciate that this paper has provided a valuable benchmark to the communities, as the new benchmark can potentially help researchers and practitioners in this direction. However, there are several concerns regarding the methodology and evaluation of this paper, which I will elaborate on below:\n\n\n1. My biggest concern is the lack of justifications for the testing framework for Python and Java functions in BioCoder. The authors state, \"... we employ a custom syntax to indicate the insertion points for custom randomly generated test cases.\" How are the test cases \"randomly generated\" and inserted into the context files? I did not find any detailed explanations in the main paper or in Appendix L and Y. In particular, Appendix L only briefly introduces the pipeline of the testing framework, how does such an approach deliver \"a secure and efficient testing framework, promising robustness in the evaluation of generated code\"? The authors need to clarify more about the generation of the test cases.\n\n2. In addition to the previous point, for Rosalind functions, the authors mentioned, \"...the output of this execution is compared with the cached golden code output.\" Why and how are the generated codes compared with the gold code outputs? I do not find any experiment results that illustrate the comparison outcomes.\n\n3. Another concern is the implementation of correction mechanisms which rectify minor syntax and style errors. What kind of syntax and style errors can be considered \"minor\" with no impact on the functionality of the generated programs? As the authors take invalid syntax and runtime error as two major failure reasons in the following error distribution analysis, I recommend further justifying such correction mechanisms, which may affect the validity of the analysis results.\n\n4. Table 4 summarizes the performance of the studied LLMs on BioCoder w.r.t 4 different types of prompt formats. However, the explanations of the different prompt versions are placed in Appendix I, which makes Table 4 hard to understand. Moreover, Appendix I only gives explanations with examples of the prompts in each version; nevertheless, I am looking for some high-level guidelines for the prompt design. Namely, how the five prompt versions are proposed? Are they from existing lectures or experimental experience? What are the characteristics of different prompt formats?\n\n5. The discussion of the experiment results seems shallow to me. In section 5, the authors consider there is an inverse relationship between the length of the input prompts and the performance of the generated codes. However, from Table 4 and Appendix I, the Necessary only prompts have relatively shorter prompts but lower passing rates compared to uncommented and Summary at Top/Bottom in most of the studied LLMs. The author may elaborate more on the perspectives of prompt structures and contents instead of just the length of the prompts.\n\n\nMinor Comments\n\n1. The \"Summary At Bottom\" results illustrated in Appendix U seem incomplete (no row for GPT-4). \n\n2. From section 3.4, \"Our testing framework starts with a manual review of selected functions, leading to the creation of a context file and a golden code file for each problem (see Figure 3)\". I do not find how Figure 3 is correlated with the testing framework, Figure 17 in Appendix R may be a better example.\n\nQuestions: 1. The details of the testing framework and the corresponding effectiveness should be discussed.\n\n2. For Rosalind functions, why and how are the generated codes compared with the gold code outputs?\n\n3. What are the guidelines while designing the 5 different prompt styles for the subject LLMs?", "labeling_timestamp": "2026-01-11T16:53:02.987519", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does mention a fuzz-testing tool and a dockerized testing framework (Abstract, Figure 2, and Introduction), but it does not include Appendix L or details of its contents. Because Appendix L is not present in the supplied content, I cannot verify whether Appendix L only briefly outlines the testing pipeline or whether it omits explanations about providing a 'secure and efficient testing framework' or guarantees of evaluation robustness.", "evidence": "1) \"BIOCODER incorporates a fuzz-testing framework for evaluation.\" (Abstract)\n2) \"Figure 2: A diagram of the BIOCODER construction process ... as well as context and test case creation and a massively dockerized testing framework.\" (Section 2 / Figure 2 caption)\n3) \"We provide a fuzzer testing tool capable of scaling to handle substantial datasets. Our benchmark results, derived from 1000 iterations, are particularly reliable, indicating the Pass@K rate.\" (Introduction / end of Section 1)", "section": "Abstract / Introduction / Figure 2 caption"}
{"claim": "For Rosalind functions, the authors do not explain why or how generated code outputs are compared with cached golden outputs, leaving the comparison procedure unclear.", "claim_type": "methodology", "paper_id": "JbOsMrwjZ3", "paper_title": "BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "zqRMRjR9Zo", "reviewer": "Reviewer_r6TX", "review_text": "Summary: This paper presents a large-scale benchmark, BioCoder, which is devoted to assessing the capability of LLMs regarding code generation, specifically in the field of bioinformatics. The authors collect 1026 functions and 1243 methods in two programming languages (Python and Java) from GitHub and 253 examples from the Rosalind project to form a relatively intricate dataset to evaluate the LLMs' code generation abilities from various aspects. The authors conduct multiple steps to ensure the validity and unbiasedness of the constructed benchmark. In particular, 10 different LLMs (including the fine-tuned one) are evaluated on BioCoder, and the experiments reveal what factors can potentially affect the performance of LLMs while tackling challenging code generation tasks.\n\nStrengths: 1. Timely and vital problem.\n2. A valuable large-scale benchmark for code generation, including specialized domain knowledge.\n3. Comprehensive evaluation with multiple LLMs.\n4. The presentation is in a good manner, and the paper is easy to follow.\n\nWeaknesses: I appreciate that this paper has provided a valuable benchmark to the communities, as the new benchmark can potentially help researchers and practitioners in this direction. However, there are several concerns regarding the methodology and evaluation of this paper, which I will elaborate on below:\n\n\n1. My biggest concern is the lack of justifications for the testing framework for Python and Java functions in BioCoder. The authors state, \"... we employ a custom syntax to indicate the insertion points for custom randomly generated test cases.\" How are the test cases \"randomly generated\" and inserted into the context files? I did not find any detailed explanations in the main paper or in Appendix L and Y. In particular, Appendix L only briefly introduces the pipeline of the testing framework, how does such an approach deliver \"a secure and efficient testing framework, promising robustness in the evaluation of generated code\"? The authors need to clarify more about the generation of the test cases.\n\n2. In addition to the previous point, for Rosalind functions, the authors mentioned, \"...the output of this execution is compared with the cached golden code output.\" Why and how are the generated codes compared with the gold code outputs? I do not find any experiment results that illustrate the comparison outcomes.\n\n3. Another concern is the implementation of correction mechanisms which rectify minor syntax and style errors. What kind of syntax and style errors can be considered \"minor\" with no impact on the functionality of the generated programs? As the authors take invalid syntax and runtime error as two major failure reasons in the following error distribution analysis, I recommend further justifying such correction mechanisms, which may affect the validity of the analysis results.\n\n4. Table 4 summarizes the performance of the studied LLMs on BioCoder w.r.t 4 different types of prompt formats. However, the explanations of the different prompt versions are placed in Appendix I, which makes Table 4 hard to understand. Moreover, Appendix I only gives explanations with examples of the prompts in each version; nevertheless, I am looking for some high-level guidelines for the prompt design. Namely, how the five prompt versions are proposed? Are they from existing lectures or experimental experience? What are the characteristics of different prompt formats?\n\n5. The discussion of the experiment results seems shallow to me. In section 5, the authors consider there is an inverse relationship between the length of the input prompts and the performance of the generated codes. However, from Table 4 and Appendix I, the Necessary only prompts have relatively shorter prompts but lower passing rates compared to uncommented and Summary at Top/Bottom in most of the studied LLMs. The author may elaborate more on the perspectives of prompt structures and contents instead of just the length of the prompts.\n\n\nMinor Comments\n\n1. The \"Summary At Bottom\" results illustrated in Appendix U seem incomplete (no row for GPT-4). \n\n2. From section 3.4, \"Our testing framework starts with a manual review of selected functions, leading to the creation of a context file and a golden code file for each problem (see Figure 3)\". I do not find how Figure 3 is correlated with the testing framework, Figure 17 in Appendix R may be a better example.\n\nQuestions: 1. The details of the testing framework and the corresponding effectiveness should be discussed.\n\n2. For Rosalind functions, why and how are the generated codes compared with the gold code outputs?\n\n3. What are the guidelines while designing the 5 different prompt styles for the subject LLMs?", "labeling_timestamp": "2026-01-11T16:52:51.327219", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper states that it includes 253 Rosalind examples and describes a fuzz-testing / execution-based evaluation framework, but it does not describe any procedure for comparing generated outputs to cached golden outputs or explain why such cached outputs would be used. There is no text in the provided sections that details how Rosalind function outputs are compared to reference outputs.", "evidence": "1) \"...we included an additional 253 questions from the Rosalind project.\" (Abstract / 1 INTRODUCTION)\n2) \"BIOCODER incorporates a fuzz-testing framework for evaluation.\" (Abstract)\n3) \"execution-based evaluation approaches... execute tests on the generated code to verify its functional correctness, ensuring unbiased evaluations irrespective of implementation method or style variations.\" (2.2 CODE GENERATION DATASETS AND BENCHMARKS)\n", "section": "Abstract / 1 INTRODUCTION / 2.2 CODE GENERATION DATASETS AND BENCHMARKS"}
{"claim": "The paper provides no experimental results or examples that illustrate the outcomes of comparing generated code outputs to golden outputs for Rosalind problems.", "claim_type": "experimental", "paper_id": "JbOsMrwjZ3", "paper_title": "BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "zqRMRjR9Zo", "reviewer": "Reviewer_r6TX", "review_text": "Summary: This paper presents a large-scale benchmark, BioCoder, which is devoted to assessing the capability of LLMs regarding code generation, specifically in the field of bioinformatics. The authors collect 1026 functions and 1243 methods in two programming languages (Python and Java) from GitHub and 253 examples from the Rosalind project to form a relatively intricate dataset to evaluate the LLMs' code generation abilities from various aspects. The authors conduct multiple steps to ensure the validity and unbiasedness of the constructed benchmark. In particular, 10 different LLMs (including the fine-tuned one) are evaluated on BioCoder, and the experiments reveal what factors can potentially affect the performance of LLMs while tackling challenging code generation tasks.\n\nStrengths: 1. Timely and vital problem.\n2. A valuable large-scale benchmark for code generation, including specialized domain knowledge.\n3. Comprehensive evaluation with multiple LLMs.\n4. The presentation is in a good manner, and the paper is easy to follow.\n\nWeaknesses: I appreciate that this paper has provided a valuable benchmark to the communities, as the new benchmark can potentially help researchers and practitioners in this direction. However, there are several concerns regarding the methodology and evaluation of this paper, which I will elaborate on below:\n\n\n1. My biggest concern is the lack of justifications for the testing framework for Python and Java functions in BioCoder. The authors state, \"... we employ a custom syntax to indicate the insertion points for custom randomly generated test cases.\" How are the test cases \"randomly generated\" and inserted into the context files? I did not find any detailed explanations in the main paper or in Appendix L and Y. In particular, Appendix L only briefly introduces the pipeline of the testing framework, how does such an approach deliver \"a secure and efficient testing framework, promising robustness in the evaluation of generated code\"? The authors need to clarify more about the generation of the test cases.\n\n2. In addition to the previous point, for Rosalind functions, the authors mentioned, \"...the output of this execution is compared with the cached golden code output.\" Why and how are the generated codes compared with the gold code outputs? I do not find any experiment results that illustrate the comparison outcomes.\n\n3. Another concern is the implementation of correction mechanisms which rectify minor syntax and style errors. What kind of syntax and style errors can be considered \"minor\" with no impact on the functionality of the generated programs? As the authors take invalid syntax and runtime error as two major failure reasons in the following error distribution analysis, I recommend further justifying such correction mechanisms, which may affect the validity of the analysis results.\n\n4. Table 4 summarizes the performance of the studied LLMs on BioCoder w.r.t 4 different types of prompt formats. However, the explanations of the different prompt versions are placed in Appendix I, which makes Table 4 hard to understand. Moreover, Appendix I only gives explanations with examples of the prompts in each version; nevertheless, I am looking for some high-level guidelines for the prompt design. Namely, how the five prompt versions are proposed? Are they from existing lectures or experimental experience? What are the characteristics of different prompt formats?\n\n5. The discussion of the experiment results seems shallow to me. In section 5, the authors consider there is an inverse relationship between the length of the input prompts and the performance of the generated codes. However, from Table 4 and Appendix I, the Necessary only prompts have relatively shorter prompts but lower passing rates compared to uncommented and Summary at Top/Bottom in most of the studied LLMs. The author may elaborate more on the perspectives of prompt structures and contents instead of just the length of the prompts.\n\n\nMinor Comments\n\n1. The \"Summary At Bottom\" results illustrated in Appendix U seem incomplete (no row for GPT-4). \n\n2. From section 3.4, \"Our testing framework starts with a manual review of selected functions, leading to the creation of a context file and a golden code file for each problem (see Figure 3)\". I do not find how Figure 3 is correlated with the testing framework, Figure 17 in Appendix R may be a better example.\n\nQuestions: 1. The details of the testing framework and the corresponding effectiveness should be discussed.\n\n2. For Rosalind functions, why and how are the generated codes compared with the gold code outputs?\n\n3. What are the guidelines while designing the 5 different prompt styles for the subject LLMs?", "labeling_timestamp": "2026-01-11T16:53:02.776346", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper states that it includes 253 Rosalind questions but, in the provided content, presents only overall benchmark evaluations (e.g., Pass@K across models) without any experimental results or example comparisons specifically reporting generated vs. golden outputs for the Rosalind subset.", "evidence": "\"we included an additional 253 questions from the Rosalind project. This project specializes in generating Python functions addressing key bioinformatics topics such as genetic sequencing and DNA/RNA analysis.\" || \"We have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT5+, GPT-3.5, and GPT-4.\"", "section": "Introduction; Abstract"}
{"claim": "The implemented correction mechanisms for rectifying minor syntax and style errors are not defined, so it is unclear which errors are considered 'minor' and harmless to functionality.", "claim_type": "methodology", "paper_id": "JbOsMrwjZ3", "paper_title": "BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "zqRMRjR9Zo", "reviewer": "Reviewer_r6TX", "review_text": "Summary: This paper presents a large-scale benchmark, BioCoder, which is devoted to assessing the capability of LLMs regarding code generation, specifically in the field of bioinformatics. The authors collect 1026 functions and 1243 methods in two programming languages (Python and Java) from GitHub and 253 examples from the Rosalind project to form a relatively intricate dataset to evaluate the LLMs' code generation abilities from various aspects. The authors conduct multiple steps to ensure the validity and unbiasedness of the constructed benchmark. In particular, 10 different LLMs (including the fine-tuned one) are evaluated on BioCoder, and the experiments reveal what factors can potentially affect the performance of LLMs while tackling challenging code generation tasks.\n\nStrengths: 1. Timely and vital problem.\n2. A valuable large-scale benchmark for code generation, including specialized domain knowledge.\n3. Comprehensive evaluation with multiple LLMs.\n4. The presentation is in a good manner, and the paper is easy to follow.\n\nWeaknesses: I appreciate that this paper has provided a valuable benchmark to the communities, as the new benchmark can potentially help researchers and practitioners in this direction. However, there are several concerns regarding the methodology and evaluation of this paper, which I will elaborate on below:\n\n\n1. My biggest concern is the lack of justifications for the testing framework for Python and Java functions in BioCoder. The authors state, \"... we employ a custom syntax to indicate the insertion points for custom randomly generated test cases.\" How are the test cases \"randomly generated\" and inserted into the context files? I did not find any detailed explanations in the main paper or in Appendix L and Y. In particular, Appendix L only briefly introduces the pipeline of the testing framework, how does such an approach deliver \"a secure and efficient testing framework, promising robustness in the evaluation of generated code\"? The authors need to clarify more about the generation of the test cases.\n\n2. In addition to the previous point, for Rosalind functions, the authors mentioned, \"...the output of this execution is compared with the cached golden code output.\" Why and how are the generated codes compared with the gold code outputs? I do not find any experiment results that illustrate the comparison outcomes.\n\n3. Another concern is the implementation of correction mechanisms which rectify minor syntax and style errors. What kind of syntax and style errors can be considered \"minor\" with no impact on the functionality of the generated programs? As the authors take invalid syntax and runtime error as two major failure reasons in the following error distribution analysis, I recommend further justifying such correction mechanisms, which may affect the validity of the analysis results.\n\n4. Table 4 summarizes the performance of the studied LLMs on BioCoder w.r.t 4 different types of prompt formats. However, the explanations of the different prompt versions are placed in Appendix I, which makes Table 4 hard to understand. Moreover, Appendix I only gives explanations with examples of the prompts in each version; nevertheless, I am looking for some high-level guidelines for the prompt design. Namely, how the five prompt versions are proposed? Are they from existing lectures or experimental experience? What are the characteristics of different prompt formats?\n\n5. The discussion of the experiment results seems shallow to me. In section 5, the authors consider there is an inverse relationship between the length of the input prompts and the performance of the generated codes. However, from Table 4 and Appendix I, the Necessary only prompts have relatively shorter prompts but lower passing rates compared to uncommented and Summary at Top/Bottom in most of the studied LLMs. The author may elaborate more on the perspectives of prompt structures and contents instead of just the length of the prompts.\n\n\nMinor Comments\n\n1. The \"Summary At Bottom\" results illustrated in Appendix U seem incomplete (no row for GPT-4). \n\n2. From section 3.4, \"Our testing framework starts with a manual review of selected functions, leading to the creation of a context file and a golden code file for each problem (see Figure 3)\". I do not find how Figure 3 is correlated with the testing framework, Figure 17 in Appendix R may be a better example.\n\nQuestions: 1. The details of the testing framework and the corresponding effectiveness should be discussed.\n\n2. For Rosalind functions, why and how are the generated codes compared with the gold code outputs?\n\n3. What are the guidelines while designing the 5 different prompt styles for the subject LLMs?", "labeling_timestamp": "2026-01-11T16:53:16.255958", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes a fuzz-testing / execution-based evaluation setup (Abstract, Sec. 1, Sec. 2.2) but does not specify any correction or normalization mechanisms for handling minor syntax or style errors or define which errors are considered 'minor' and harmless. There is no text in the provided excerpts that defines error-correction rules or tolerances for non-functional deviations.", "evidence": "“BIOCODER incorporates a fuzz-testing framework for evaluation.”; “We provide a fuzzer testing tool capable of scaling to handle substantial datasets.”; “Early work on code generation benchmarks used lexical exact match, data flow, and abstract syntax tree (AST) methods. However, these measures proved to be unreliable ... In response, execution-based evaluation approaches have become more prevalent ... These approaches execute tests on the generated code to verify its functional correctness, ensuring unbiased evaluations irrespective of implementation method or style variations.”", "section": "Abstract; Introduction (Sec. 1); Related Work (Sec. 2.2)"}
{"claim": "Applying unspecified correction mechanisms may bias the reported error distribution, undermining the validity of treating syntax and runtime errors as major failure reasons.", "claim_type": "methodology", "paper_id": "JbOsMrwjZ3", "paper_title": "BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "zqRMRjR9Zo", "reviewer": "Reviewer_r6TX", "review_text": "Summary: This paper presents a large-scale benchmark, BioCoder, which is devoted to assessing the capability of LLMs regarding code generation, specifically in the field of bioinformatics. The authors collect 1026 functions and 1243 methods in two programming languages (Python and Java) from GitHub and 253 examples from the Rosalind project to form a relatively intricate dataset to evaluate the LLMs' code generation abilities from various aspects. The authors conduct multiple steps to ensure the validity and unbiasedness of the constructed benchmark. In particular, 10 different LLMs (including the fine-tuned one) are evaluated on BioCoder, and the experiments reveal what factors can potentially affect the performance of LLMs while tackling challenging code generation tasks.\n\nStrengths: 1. Timely and vital problem.\n2. A valuable large-scale benchmark for code generation, including specialized domain knowledge.\n3. Comprehensive evaluation with multiple LLMs.\n4. The presentation is in a good manner, and the paper is easy to follow.\n\nWeaknesses: I appreciate that this paper has provided a valuable benchmark to the communities, as the new benchmark can potentially help researchers and practitioners in this direction. However, there are several concerns regarding the methodology and evaluation of this paper, which I will elaborate on below:\n\n\n1. My biggest concern is the lack of justifications for the testing framework for Python and Java functions in BioCoder. The authors state, \"... we employ a custom syntax to indicate the insertion points for custom randomly generated test cases.\" How are the test cases \"randomly generated\" and inserted into the context files? I did not find any detailed explanations in the main paper or in Appendix L and Y. In particular, Appendix L only briefly introduces the pipeline of the testing framework, how does such an approach deliver \"a secure and efficient testing framework, promising robustness in the evaluation of generated code\"? The authors need to clarify more about the generation of the test cases.\n\n2. In addition to the previous point, for Rosalind functions, the authors mentioned, \"...the output of this execution is compared with the cached golden code output.\" Why and how are the generated codes compared with the gold code outputs? I do not find any experiment results that illustrate the comparison outcomes.\n\n3. Another concern is the implementation of correction mechanisms which rectify minor syntax and style errors. What kind of syntax and style errors can be considered \"minor\" with no impact on the functionality of the generated programs? As the authors take invalid syntax and runtime error as two major failure reasons in the following error distribution analysis, I recommend further justifying such correction mechanisms, which may affect the validity of the analysis results.\n\n4. Table 4 summarizes the performance of the studied LLMs on BioCoder w.r.t 4 different types of prompt formats. However, the explanations of the different prompt versions are placed in Appendix I, which makes Table 4 hard to understand. Moreover, Appendix I only gives explanations with examples of the prompts in each version; nevertheless, I am looking for some high-level guidelines for the prompt design. Namely, how the five prompt versions are proposed? Are they from existing lectures or experimental experience? What are the characteristics of different prompt formats?\n\n5. The discussion of the experiment results seems shallow to me. In section 5, the authors consider there is an inverse relationship between the length of the input prompts and the performance of the generated codes. However, from Table 4 and Appendix I, the Necessary only prompts have relatively shorter prompts but lower passing rates compared to uncommented and Summary at Top/Bottom in most of the studied LLMs. The author may elaborate more on the perspectives of prompt structures and contents instead of just the length of the prompts.\n\n\nMinor Comments\n\n1. The \"Summary At Bottom\" results illustrated in Appendix U seem incomplete (no row for GPT-4). \n\n2. From section 3.4, \"Our testing framework starts with a manual review of selected functions, leading to the creation of a context file and a golden code file for each problem (see Figure 3)\". I do not find how Figure 3 is correlated with the testing framework, Figure 17 in Appendix R may be a better example.\n\nQuestions: 1. The details of the testing framework and the corresponding effectiveness should be discussed.\n\n2. For Rosalind functions, why and how are the generated codes compared with the gold code outputs?\n\n3. What are the guidelines while designing the 5 different prompt styles for the subject LLMs?", "labeling_timestamp": "2026-01-11T16:53:27.158163", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper describes using an execution-based, fuzz-testing evaluation (with 1000 iterations) but does not state that it applies any unspecified correction mechanisms to error counts, nor does it discuss how such corrections might bias error distributions or the categorization of failures (syntax vs runtime). Therefore the reviewer's claim about unspecified correction mechanisms biasing reported error distributions cannot be verified from the provided text.", "evidence": "“BIOCODER incorporates a fuzz-testing framework for evaluation.”\n“ Our benchmark results, derived from 1000 iterations, are particularly reliable, indicating the Pass@K rate.”\n“Early work on code generation benchmarks used lexical exact match, data flow, and abstract syntax tree (AST) methods. ... In response, execution-based evaluation approaches have become more prevalent.”", "section": "Abstract / 2.2 CODE GENERATION DATASETS AND BENCHMARKS"}
{"claim": "Table 4’s prompt format explanations are relegated to Appendix I, making the table hard to interpret in isolation within the main paper.", "claim_type": "presentation", "paper_id": "JbOsMrwjZ3", "paper_title": "BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "zqRMRjR9Zo", "reviewer": "Reviewer_r6TX", "review_text": "Summary: This paper presents a large-scale benchmark, BioCoder, which is devoted to assessing the capability of LLMs regarding code generation, specifically in the field of bioinformatics. The authors collect 1026 functions and 1243 methods in two programming languages (Python and Java) from GitHub and 253 examples from the Rosalind project to form a relatively intricate dataset to evaluate the LLMs' code generation abilities from various aspects. The authors conduct multiple steps to ensure the validity and unbiasedness of the constructed benchmark. In particular, 10 different LLMs (including the fine-tuned one) are evaluated on BioCoder, and the experiments reveal what factors can potentially affect the performance of LLMs while tackling challenging code generation tasks.\n\nStrengths: 1. Timely and vital problem.\n2. A valuable large-scale benchmark for code generation, including specialized domain knowledge.\n3. Comprehensive evaluation with multiple LLMs.\n4. The presentation is in a good manner, and the paper is easy to follow.\n\nWeaknesses: I appreciate that this paper has provided a valuable benchmark to the communities, as the new benchmark can potentially help researchers and practitioners in this direction. However, there are several concerns regarding the methodology and evaluation of this paper, which I will elaborate on below:\n\n\n1. My biggest concern is the lack of justifications for the testing framework for Python and Java functions in BioCoder. The authors state, \"... we employ a custom syntax to indicate the insertion points for custom randomly generated test cases.\" How are the test cases \"randomly generated\" and inserted into the context files? I did not find any detailed explanations in the main paper or in Appendix L and Y. In particular, Appendix L only briefly introduces the pipeline of the testing framework, how does such an approach deliver \"a secure and efficient testing framework, promising robustness in the evaluation of generated code\"? The authors need to clarify more about the generation of the test cases.\n\n2. In addition to the previous point, for Rosalind functions, the authors mentioned, \"...the output of this execution is compared with the cached golden code output.\" Why and how are the generated codes compared with the gold code outputs? I do not find any experiment results that illustrate the comparison outcomes.\n\n3. Another concern is the implementation of correction mechanisms which rectify minor syntax and style errors. What kind of syntax and style errors can be considered \"minor\" with no impact on the functionality of the generated programs? As the authors take invalid syntax and runtime error as two major failure reasons in the following error distribution analysis, I recommend further justifying such correction mechanisms, which may affect the validity of the analysis results.\n\n4. Table 4 summarizes the performance of the studied LLMs on BioCoder w.r.t 4 different types of prompt formats. However, the explanations of the different prompt versions are placed in Appendix I, which makes Table 4 hard to understand. Moreover, Appendix I only gives explanations with examples of the prompts in each version; nevertheless, I am looking for some high-level guidelines for the prompt design. Namely, how the five prompt versions are proposed? Are they from existing lectures or experimental experience? What are the characteristics of different prompt formats?\n\n5. The discussion of the experiment results seems shallow to me. In section 5, the authors consider there is an inverse relationship between the length of the input prompts and the performance of the generated codes. However, from Table 4 and Appendix I, the Necessary only prompts have relatively shorter prompts but lower passing rates compared to uncommented and Summary at Top/Bottom in most of the studied LLMs. The author may elaborate more on the perspectives of prompt structures and contents instead of just the length of the prompts.\n\n\nMinor Comments\n\n1. The \"Summary At Bottom\" results illustrated in Appendix U seem incomplete (no row for GPT-4). \n\n2. From section 3.4, \"Our testing framework starts with a manual review of selected functions, leading to the creation of a context file and a golden code file for each problem (see Figure 3)\". I do not find how Figure 3 is correlated with the testing framework, Figure 17 in Appendix R may be a better example.\n\nQuestions: 1. The details of the testing framework and the corresponding effectiveness should be discussed.\n\n2. For Rosalind functions, why and how are the generated codes compared with the gold code outputs?\n\n3. What are the guidelines while designing the 5 different prompt styles for the subject LLMs?", "labeling_timestamp": "2026-01-11T16:53:16.724515", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not contain Table 4 or Appendix I, so it is impossible to verify the reviewer's claim about where Table 4's prompt format explanations are placed or whether that placement makes the table hard to interpret in isolation.", "evidence": "The paper references other appendices (e.g., \"see Appendix o for more details...\", \"see Appendix n\" and Table 2 notes \"(Appendix z)\"), but does not include any mention of Appendix I or Table 4 in the supplied content.", "section": "Introduction / Table 2 (references to appendices)"}
{"claim": "Appendix I gives prompt examples but lacks high-level guidelines explaining how the five prompt versions were designed and what distinguishes their characteristics.", "claim_type": "methodology", "paper_id": "JbOsMrwjZ3", "paper_title": "BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "zqRMRjR9Zo", "reviewer": "Reviewer_r6TX", "review_text": "Summary: This paper presents a large-scale benchmark, BioCoder, which is devoted to assessing the capability of LLMs regarding code generation, specifically in the field of bioinformatics. The authors collect 1026 functions and 1243 methods in two programming languages (Python and Java) from GitHub and 253 examples from the Rosalind project to form a relatively intricate dataset to evaluate the LLMs' code generation abilities from various aspects. The authors conduct multiple steps to ensure the validity and unbiasedness of the constructed benchmark. In particular, 10 different LLMs (including the fine-tuned one) are evaluated on BioCoder, and the experiments reveal what factors can potentially affect the performance of LLMs while tackling challenging code generation tasks.\n\nStrengths: 1. Timely and vital problem.\n2. A valuable large-scale benchmark for code generation, including specialized domain knowledge.\n3. Comprehensive evaluation with multiple LLMs.\n4. The presentation is in a good manner, and the paper is easy to follow.\n\nWeaknesses: I appreciate that this paper has provided a valuable benchmark to the communities, as the new benchmark can potentially help researchers and practitioners in this direction. However, there are several concerns regarding the methodology and evaluation of this paper, which I will elaborate on below:\n\n\n1. My biggest concern is the lack of justifications for the testing framework for Python and Java functions in BioCoder. The authors state, \"... we employ a custom syntax to indicate the insertion points for custom randomly generated test cases.\" How are the test cases \"randomly generated\" and inserted into the context files? I did not find any detailed explanations in the main paper or in Appendix L and Y. In particular, Appendix L only briefly introduces the pipeline of the testing framework, how does such an approach deliver \"a secure and efficient testing framework, promising robustness in the evaluation of generated code\"? The authors need to clarify more about the generation of the test cases.\n\n2. In addition to the previous point, for Rosalind functions, the authors mentioned, \"...the output of this execution is compared with the cached golden code output.\" Why and how are the generated codes compared with the gold code outputs? I do not find any experiment results that illustrate the comparison outcomes.\n\n3. Another concern is the implementation of correction mechanisms which rectify minor syntax and style errors. What kind of syntax and style errors can be considered \"minor\" with no impact on the functionality of the generated programs? As the authors take invalid syntax and runtime error as two major failure reasons in the following error distribution analysis, I recommend further justifying such correction mechanisms, which may affect the validity of the analysis results.\n\n4. Table 4 summarizes the performance of the studied LLMs on BioCoder w.r.t 4 different types of prompt formats. However, the explanations of the different prompt versions are placed in Appendix I, which makes Table 4 hard to understand. Moreover, Appendix I only gives explanations with examples of the prompts in each version; nevertheless, I am looking for some high-level guidelines for the prompt design. Namely, how the five prompt versions are proposed? Are they from existing lectures or experimental experience? What are the characteristics of different prompt formats?\n\n5. The discussion of the experiment results seems shallow to me. In section 5, the authors consider there is an inverse relationship between the length of the input prompts and the performance of the generated codes. However, from Table 4 and Appendix I, the Necessary only prompts have relatively shorter prompts but lower passing rates compared to uncommented and Summary at Top/Bottom in most of the studied LLMs. The author may elaborate more on the perspectives of prompt structures and contents instead of just the length of the prompts.\n\n\nMinor Comments\n\n1. The \"Summary At Bottom\" results illustrated in Appendix U seem incomplete (no row for GPT-4). \n\n2. From section 3.4, \"Our testing framework starts with a manual review of selected functions, leading to the creation of a context file and a golden code file for each problem (see Figure 3)\". I do not find how Figure 3 is correlated with the testing framework, Figure 17 in Appendix R may be a better example.\n\nQuestions: 1. The details of the testing framework and the corresponding effectiveness should be discussed.\n\n2. For Rosalind functions, why and how are the generated codes compared with the gold code outputs?\n\n3. What are the guidelines while designing the 5 different prompt styles for the subject LLMs?", "labeling_timestamp": "2026-01-11T16:53:28.214221", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not mention Appendix I or describe the five prompt versions; it references other appendices (o, n, z) but gives no information to confirm whether Appendix I contains prompt examples or whether high-level guidelines for five prompt versions are missing. Therefore there is insufficient information to verify the reviewer’s claim.", "evidence": "\"see Appendix o for more details with the topic modeling and statistics regarding the overall topic coverage of the dataset\"; \"we performed ablation studies to determine whether the models are strictly memorizing the solutions rather than being proficient at generating code (see Appendix n).\"; \"Similar data is a subset of the hidden data, mimicing the distribution of the public data (Appendix z).\"", "section": "1 INTRODUCTION"}
{"claim": "The paper claims an inverse relationship between prompt length and code generation performance without reconciling contradictory evidence from Table 4 showing shorter 'Necessary only' prompts often have lower pass rates.", "claim_type": "experimental", "paper_id": "JbOsMrwjZ3", "paper_title": "BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "zqRMRjR9Zo", "reviewer": "Reviewer_r6TX", "review_text": "Summary: This paper presents a large-scale benchmark, BioCoder, which is devoted to assessing the capability of LLMs regarding code generation, specifically in the field of bioinformatics. The authors collect 1026 functions and 1243 methods in two programming languages (Python and Java) from GitHub and 253 examples from the Rosalind project to form a relatively intricate dataset to evaluate the LLMs' code generation abilities from various aspects. The authors conduct multiple steps to ensure the validity and unbiasedness of the constructed benchmark. In particular, 10 different LLMs (including the fine-tuned one) are evaluated on BioCoder, and the experiments reveal what factors can potentially affect the performance of LLMs while tackling challenging code generation tasks.\n\nStrengths: 1. Timely and vital problem.\n2. A valuable large-scale benchmark for code generation, including specialized domain knowledge.\n3. Comprehensive evaluation with multiple LLMs.\n4. The presentation is in a good manner, and the paper is easy to follow.\n\nWeaknesses: I appreciate that this paper has provided a valuable benchmark to the communities, as the new benchmark can potentially help researchers and practitioners in this direction. However, there are several concerns regarding the methodology and evaluation of this paper, which I will elaborate on below:\n\n\n1. My biggest concern is the lack of justifications for the testing framework for Python and Java functions in BioCoder. The authors state, \"... we employ a custom syntax to indicate the insertion points for custom randomly generated test cases.\" How are the test cases \"randomly generated\" and inserted into the context files? I did not find any detailed explanations in the main paper or in Appendix L and Y. In particular, Appendix L only briefly introduces the pipeline of the testing framework, how does such an approach deliver \"a secure and efficient testing framework, promising robustness in the evaluation of generated code\"? The authors need to clarify more about the generation of the test cases.\n\n2. In addition to the previous point, for Rosalind functions, the authors mentioned, \"...the output of this execution is compared with the cached golden code output.\" Why and how are the generated codes compared with the gold code outputs? I do not find any experiment results that illustrate the comparison outcomes.\n\n3. Another concern is the implementation of correction mechanisms which rectify minor syntax and style errors. What kind of syntax and style errors can be considered \"minor\" with no impact on the functionality of the generated programs? As the authors take invalid syntax and runtime error as two major failure reasons in the following error distribution analysis, I recommend further justifying such correction mechanisms, which may affect the validity of the analysis results.\n\n4. Table 4 summarizes the performance of the studied LLMs on BioCoder w.r.t 4 different types of prompt formats. However, the explanations of the different prompt versions are placed in Appendix I, which makes Table 4 hard to understand. Moreover, Appendix I only gives explanations with examples of the prompts in each version; nevertheless, I am looking for some high-level guidelines for the prompt design. Namely, how the five prompt versions are proposed? Are they from existing lectures or experimental experience? What are the characteristics of different prompt formats?\n\n5. The discussion of the experiment results seems shallow to me. In section 5, the authors consider there is an inverse relationship between the length of the input prompts and the performance of the generated codes. However, from Table 4 and Appendix I, the Necessary only prompts have relatively shorter prompts but lower passing rates compared to uncommented and Summary at Top/Bottom in most of the studied LLMs. The author may elaborate more on the perspectives of prompt structures and contents instead of just the length of the prompts.\n\n\nMinor Comments\n\n1. The \"Summary At Bottom\" results illustrated in Appendix U seem incomplete (no row for GPT-4). \n\n2. From section 3.4, \"Our testing framework starts with a manual review of selected functions, leading to the creation of a context file and a golden code file for each problem (see Figure 3)\". I do not find how Figure 3 is correlated with the testing framework, Figure 17 in Appendix R may be a better example.\n\nQuestions: 1. The details of the testing framework and the corresponding effectiveness should be discussed.\n\n2. For Rosalind functions, why and how are the generated codes compared with the gold code outputs?\n\n3. What are the guidelines while designing the 5 different prompt styles for the subject LLMs?", "labeling_timestamp": "2026-01-11T16:53:30.175744", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper excerpt explicitly states that successful models 'accommodate a long prompt (> 2600 tokens)', so it does make a claim about prompt length affecting performance. However, the provided content does not include Table 4 or any mention of 'Necessary only' prompts or the specific pass-rate numbers the reviewer cites, so there is insufficient information to determine whether the paper asserted an (inverse) relationship as the reviewer describes or whether it reconciles any contradictory evidence from Table 4.", "evidence": "\"(1) Successful models accommodate a long prompt (> 2600 tokens) with full context, for functional dependencies.\"", "section": "ABSTRACT"}
{"claim": "The discussion of prompt effects is shallow and focuses on prompt length rather than analyzing prompt structure and content differences that may influence performance.", "claim_type": "methodology", "paper_id": "JbOsMrwjZ3", "paper_title": "BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "zqRMRjR9Zo", "reviewer": "Reviewer_r6TX", "review_text": "Summary: This paper presents a large-scale benchmark, BioCoder, which is devoted to assessing the capability of LLMs regarding code generation, specifically in the field of bioinformatics. The authors collect 1026 functions and 1243 methods in two programming languages (Python and Java) from GitHub and 253 examples from the Rosalind project to form a relatively intricate dataset to evaluate the LLMs' code generation abilities from various aspects. The authors conduct multiple steps to ensure the validity and unbiasedness of the constructed benchmark. In particular, 10 different LLMs (including the fine-tuned one) are evaluated on BioCoder, and the experiments reveal what factors can potentially affect the performance of LLMs while tackling challenging code generation tasks.\n\nStrengths: 1. Timely and vital problem.\n2. A valuable large-scale benchmark for code generation, including specialized domain knowledge.\n3. Comprehensive evaluation with multiple LLMs.\n4. The presentation is in a good manner, and the paper is easy to follow.\n\nWeaknesses: I appreciate that this paper has provided a valuable benchmark to the communities, as the new benchmark can potentially help researchers and practitioners in this direction. However, there are several concerns regarding the methodology and evaluation of this paper, which I will elaborate on below:\n\n\n1. My biggest concern is the lack of justifications for the testing framework for Python and Java functions in BioCoder. The authors state, \"... we employ a custom syntax to indicate the insertion points for custom randomly generated test cases.\" How are the test cases \"randomly generated\" and inserted into the context files? I did not find any detailed explanations in the main paper or in Appendix L and Y. In particular, Appendix L only briefly introduces the pipeline of the testing framework, how does such an approach deliver \"a secure and efficient testing framework, promising robustness in the evaluation of generated code\"? The authors need to clarify more about the generation of the test cases.\n\n2. In addition to the previous point, for Rosalind functions, the authors mentioned, \"...the output of this execution is compared with the cached golden code output.\" Why and how are the generated codes compared with the gold code outputs? I do not find any experiment results that illustrate the comparison outcomes.\n\n3. Another concern is the implementation of correction mechanisms which rectify minor syntax and style errors. What kind of syntax and style errors can be considered \"minor\" with no impact on the functionality of the generated programs? As the authors take invalid syntax and runtime error as two major failure reasons in the following error distribution analysis, I recommend further justifying such correction mechanisms, which may affect the validity of the analysis results.\n\n4. Table 4 summarizes the performance of the studied LLMs on BioCoder w.r.t 4 different types of prompt formats. However, the explanations of the different prompt versions are placed in Appendix I, which makes Table 4 hard to understand. Moreover, Appendix I only gives explanations with examples of the prompts in each version; nevertheless, I am looking for some high-level guidelines for the prompt design. Namely, how the five prompt versions are proposed? Are they from existing lectures or experimental experience? What are the characteristics of different prompt formats?\n\n5. The discussion of the experiment results seems shallow to me. In section 5, the authors consider there is an inverse relationship between the length of the input prompts and the performance of the generated codes. However, from Table 4 and Appendix I, the Necessary only prompts have relatively shorter prompts but lower passing rates compared to uncommented and Summary at Top/Bottom in most of the studied LLMs. The author may elaborate more on the perspectives of prompt structures and contents instead of just the length of the prompts.\n\n\nMinor Comments\n\n1. The \"Summary At Bottom\" results illustrated in Appendix U seem incomplete (no row for GPT-4). \n\n2. From section 3.4, \"Our testing framework starts with a manual review of selected functions, leading to the creation of a context file and a golden code file for each problem (see Figure 3)\". I do not find how Figure 3 is correlated with the testing framework, Figure 17 in Appendix R may be a better example.\n\nQuestions: 1. The details of the testing framework and the corresponding effectiveness should be discussed.\n\n2. For Rosalind functions, why and how are the generated codes compared with the gold code outputs?\n\n3. What are the guidelines while designing the 5 different prompt styles for the subject LLMs?", "labeling_timestamp": "2026-01-11T16:53:59.866678", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper's main explicit claim about prompt effects emphasizes prompt length and having the full context (e.g., >2600 tokens). While it notes that context influences performance and that BIOCODER includes contextual elements (imports, class declarations, globals), the provided sections do not present an analysis of prompt structure or content differences that might affect performance—only general statements and pointers to ablation appendices without detailed discussion in the main text.", "evidence": "Abstract: \"Successful models accommodate a long prompt (> 2600 tokens) with full context, for functional dependencies.\"  Introduction: \"BIOCODER assures the inclusion of all potential external packages and code that could be utilized by the generated program... hence, we included all potentially required class declarations in the input.\"  Section 2.2: \"In addition, the context supplied greatly influences the performance of existing LLMs (Wang et al., 2022a).\"", "section": "Abstract; 1 INTRODUCTION; 2.2 CODE GENERATION DATASETS AND BENCHMARKS"}
{"claim": "Appendix U’s 'Summary At Bottom' results appear incomplete because the table lacks a row for GPT-4, suggesting missing reported results.", "claim_type": "baseline", "paper_id": "JbOsMrwjZ3", "paper_title": "BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "zqRMRjR9Zo", "reviewer": "Reviewer_r6TX", "review_text": "Summary: This paper presents a large-scale benchmark, BioCoder, which is devoted to assessing the capability of LLMs regarding code generation, specifically in the field of bioinformatics. The authors collect 1026 functions and 1243 methods in two programming languages (Python and Java) from GitHub and 253 examples from the Rosalind project to form a relatively intricate dataset to evaluate the LLMs' code generation abilities from various aspects. The authors conduct multiple steps to ensure the validity and unbiasedness of the constructed benchmark. In particular, 10 different LLMs (including the fine-tuned one) are evaluated on BioCoder, and the experiments reveal what factors can potentially affect the performance of LLMs while tackling challenging code generation tasks.\n\nStrengths: 1. Timely and vital problem.\n2. A valuable large-scale benchmark for code generation, including specialized domain knowledge.\n3. Comprehensive evaluation with multiple LLMs.\n4. The presentation is in a good manner, and the paper is easy to follow.\n\nWeaknesses: I appreciate that this paper has provided a valuable benchmark to the communities, as the new benchmark can potentially help researchers and practitioners in this direction. However, there are several concerns regarding the methodology and evaluation of this paper, which I will elaborate on below:\n\n\n1. My biggest concern is the lack of justifications for the testing framework for Python and Java functions in BioCoder. The authors state, \"... we employ a custom syntax to indicate the insertion points for custom randomly generated test cases.\" How are the test cases \"randomly generated\" and inserted into the context files? I did not find any detailed explanations in the main paper or in Appendix L and Y. In particular, Appendix L only briefly introduces the pipeline of the testing framework, how does such an approach deliver \"a secure and efficient testing framework, promising robustness in the evaluation of generated code\"? The authors need to clarify more about the generation of the test cases.\n\n2. In addition to the previous point, for Rosalind functions, the authors mentioned, \"...the output of this execution is compared with the cached golden code output.\" Why and how are the generated codes compared with the gold code outputs? I do not find any experiment results that illustrate the comparison outcomes.\n\n3. Another concern is the implementation of correction mechanisms which rectify minor syntax and style errors. What kind of syntax and style errors can be considered \"minor\" with no impact on the functionality of the generated programs? As the authors take invalid syntax and runtime error as two major failure reasons in the following error distribution analysis, I recommend further justifying such correction mechanisms, which may affect the validity of the analysis results.\n\n4. Table 4 summarizes the performance of the studied LLMs on BioCoder w.r.t 4 different types of prompt formats. However, the explanations of the different prompt versions are placed in Appendix I, which makes Table 4 hard to understand. Moreover, Appendix I only gives explanations with examples of the prompts in each version; nevertheless, I am looking for some high-level guidelines for the prompt design. Namely, how the five prompt versions are proposed? Are they from existing lectures or experimental experience? What are the characteristics of different prompt formats?\n\n5. The discussion of the experiment results seems shallow to me. In section 5, the authors consider there is an inverse relationship between the length of the input prompts and the performance of the generated codes. However, from Table 4 and Appendix I, the Necessary only prompts have relatively shorter prompts but lower passing rates compared to uncommented and Summary at Top/Bottom in most of the studied LLMs. The author may elaborate more on the perspectives of prompt structures and contents instead of just the length of the prompts.\n\n\nMinor Comments\n\n1. The \"Summary At Bottom\" results illustrated in Appendix U seem incomplete (no row for GPT-4). \n\n2. From section 3.4, \"Our testing framework starts with a manual review of selected functions, leading to the creation of a context file and a golden code file for each problem (see Figure 3)\". I do not find how Figure 3 is correlated with the testing framework, Figure 17 in Appendix R may be a better example.\n\nQuestions: 1. The details of the testing framework and the corresponding effectiveness should be discussed.\n\n2. For Rosalind functions, why and how are the generated codes compared with the gold code outputs?\n\n3. What are the guidelines while designing the 5 different prompt styles for the subject LLMs?", "labeling_timestamp": "2026-01-11T16:53:31.690456", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper text states that GPT-4 was evaluated (so reporting GPT-4 results is expected), but the submission excerpts do not include Appendix U or the described 'Summary At Bottom' table, so it is impossible to confirm from the supplied content whether that appendix omits a GPT-4 row.", "evidence": "We have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT5+, GPT-3.5, and GPT-4.", "section": "ABSTRACT"}
{"claim": "Figure 3 is not clearly correlated with the described testing framework in section 3.4, and the reviewer suggests Figure 17 in Appendix R as a better example.", "claim_type": "presentation", "paper_id": "JbOsMrwjZ3", "paper_title": "BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "zqRMRjR9Zo", "reviewer": "Reviewer_r6TX", "review_text": "Summary: This paper presents a large-scale benchmark, BioCoder, which is devoted to assessing the capability of LLMs regarding code generation, specifically in the field of bioinformatics. The authors collect 1026 functions and 1243 methods in two programming languages (Python and Java) from GitHub and 253 examples from the Rosalind project to form a relatively intricate dataset to evaluate the LLMs' code generation abilities from various aspects. The authors conduct multiple steps to ensure the validity and unbiasedness of the constructed benchmark. In particular, 10 different LLMs (including the fine-tuned one) are evaluated on BioCoder, and the experiments reveal what factors can potentially affect the performance of LLMs while tackling challenging code generation tasks.\n\nStrengths: 1. Timely and vital problem.\n2. A valuable large-scale benchmark for code generation, including specialized domain knowledge.\n3. Comprehensive evaluation with multiple LLMs.\n4. The presentation is in a good manner, and the paper is easy to follow.\n\nWeaknesses: I appreciate that this paper has provided a valuable benchmark to the communities, as the new benchmark can potentially help researchers and practitioners in this direction. However, there are several concerns regarding the methodology and evaluation of this paper, which I will elaborate on below:\n\n\n1. My biggest concern is the lack of justifications for the testing framework for Python and Java functions in BioCoder. The authors state, \"... we employ a custom syntax to indicate the insertion points for custom randomly generated test cases.\" How are the test cases \"randomly generated\" and inserted into the context files? I did not find any detailed explanations in the main paper or in Appendix L and Y. In particular, Appendix L only briefly introduces the pipeline of the testing framework, how does such an approach deliver \"a secure and efficient testing framework, promising robustness in the evaluation of generated code\"? The authors need to clarify more about the generation of the test cases.\n\n2. In addition to the previous point, for Rosalind functions, the authors mentioned, \"...the output of this execution is compared with the cached golden code output.\" Why and how are the generated codes compared with the gold code outputs? I do not find any experiment results that illustrate the comparison outcomes.\n\n3. Another concern is the implementation of correction mechanisms which rectify minor syntax and style errors. What kind of syntax and style errors can be considered \"minor\" with no impact on the functionality of the generated programs? As the authors take invalid syntax and runtime error as two major failure reasons in the following error distribution analysis, I recommend further justifying such correction mechanisms, which may affect the validity of the analysis results.\n\n4. Table 4 summarizes the performance of the studied LLMs on BioCoder w.r.t 4 different types of prompt formats. However, the explanations of the different prompt versions are placed in Appendix I, which makes Table 4 hard to understand. Moreover, Appendix I only gives explanations with examples of the prompts in each version; nevertheless, I am looking for some high-level guidelines for the prompt design. Namely, how the five prompt versions are proposed? Are they from existing lectures or experimental experience? What are the characteristics of different prompt formats?\n\n5. The discussion of the experiment results seems shallow to me. In section 5, the authors consider there is an inverse relationship between the length of the input prompts and the performance of the generated codes. However, from Table 4 and Appendix I, the Necessary only prompts have relatively shorter prompts but lower passing rates compared to uncommented and Summary at Top/Bottom in most of the studied LLMs. The author may elaborate more on the perspectives of prompt structures and contents instead of just the length of the prompts.\n\n\nMinor Comments\n\n1. The \"Summary At Bottom\" results illustrated in Appendix U seem incomplete (no row for GPT-4). \n\n2. From section 3.4, \"Our testing framework starts with a manual review of selected functions, leading to the creation of a context file and a golden code file for each problem (see Figure 3)\". I do not find how Figure 3 is correlated with the testing framework, Figure 17 in Appendix R may be a better example.\n\nQuestions: 1. The details of the testing framework and the corresponding effectiveness should be discussed.\n\n2. For Rosalind functions, why and how are the generated codes compared with the gold code outputs?\n\n3. What are the guidelines while designing the 5 different prompt styles for the subject LLMs?", "labeling_timestamp": "2026-01-11T16:53:55.677581", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not include Section 3.4, Figure 3, or Appendix R / Figure 17, so there is insufficient information to assess whether Figure 3 is or is not clearly correlated with the testing framework described in Section 3.4 or whether Figure 17 would be a better example. The excerpt only mentions that a fuzz-testing / dockerized testing framework exists but does not contain the specific figures or section referenced by the reviewer.", "evidence": "1) \"BIOCODER incorporates a fuzz-testing framework for evaluation.\" (ABSTRACT)\n2) \"Figure 2: A diagram of the BIOCODER construction process involving custom GitHub repository cleaning, parsing, and function selection, as well as context and test case creation and a massively dockerized testing framework.\" (2 RELATED WORK / Figure 2 caption)", "section": "ABSTRACT and 2 RELATED WORK"}
{"claim": "The paper does not state whether the five prompt styles were derived from prior literature, lectures, or experimental experience, leaving their provenance unspecified.", "claim_type": "methodology", "paper_id": "JbOsMrwjZ3", "paper_title": "BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "zqRMRjR9Zo", "reviewer": "Reviewer_r6TX", "review_text": "Summary: This paper presents a large-scale benchmark, BioCoder, which is devoted to assessing the capability of LLMs regarding code generation, specifically in the field of bioinformatics. The authors collect 1026 functions and 1243 methods in two programming languages (Python and Java) from GitHub and 253 examples from the Rosalind project to form a relatively intricate dataset to evaluate the LLMs' code generation abilities from various aspects. The authors conduct multiple steps to ensure the validity and unbiasedness of the constructed benchmark. In particular, 10 different LLMs (including the fine-tuned one) are evaluated on BioCoder, and the experiments reveal what factors can potentially affect the performance of LLMs while tackling challenging code generation tasks.\n\nStrengths: 1. Timely and vital problem.\n2. A valuable large-scale benchmark for code generation, including specialized domain knowledge.\n3. Comprehensive evaluation with multiple LLMs.\n4. The presentation is in a good manner, and the paper is easy to follow.\n\nWeaknesses: I appreciate that this paper has provided a valuable benchmark to the communities, as the new benchmark can potentially help researchers and practitioners in this direction. However, there are several concerns regarding the methodology and evaluation of this paper, which I will elaborate on below:\n\n\n1. My biggest concern is the lack of justifications for the testing framework for Python and Java functions in BioCoder. The authors state, \"... we employ a custom syntax to indicate the insertion points for custom randomly generated test cases.\" How are the test cases \"randomly generated\" and inserted into the context files? I did not find any detailed explanations in the main paper or in Appendix L and Y. In particular, Appendix L only briefly introduces the pipeline of the testing framework, how does such an approach deliver \"a secure and efficient testing framework, promising robustness in the evaluation of generated code\"? The authors need to clarify more about the generation of the test cases.\n\n2. In addition to the previous point, for Rosalind functions, the authors mentioned, \"...the output of this execution is compared with the cached golden code output.\" Why and how are the generated codes compared with the gold code outputs? I do not find any experiment results that illustrate the comparison outcomes.\n\n3. Another concern is the implementation of correction mechanisms which rectify minor syntax and style errors. What kind of syntax and style errors can be considered \"minor\" with no impact on the functionality of the generated programs? As the authors take invalid syntax and runtime error as two major failure reasons in the following error distribution analysis, I recommend further justifying such correction mechanisms, which may affect the validity of the analysis results.\n\n4. Table 4 summarizes the performance of the studied LLMs on BioCoder w.r.t 4 different types of prompt formats. However, the explanations of the different prompt versions are placed in Appendix I, which makes Table 4 hard to understand. Moreover, Appendix I only gives explanations with examples of the prompts in each version; nevertheless, I am looking for some high-level guidelines for the prompt design. Namely, how the five prompt versions are proposed? Are they from existing lectures or experimental experience? What are the characteristics of different prompt formats?\n\n5. The discussion of the experiment results seems shallow to me. In section 5, the authors consider there is an inverse relationship between the length of the input prompts and the performance of the generated codes. However, from Table 4 and Appendix I, the Necessary only prompts have relatively shorter prompts but lower passing rates compared to uncommented and Summary at Top/Bottom in most of the studied LLMs. The author may elaborate more on the perspectives of prompt structures and contents instead of just the length of the prompts.\n\n\nMinor Comments\n\n1. The \"Summary At Bottom\" results illustrated in Appendix U seem incomplete (no row for GPT-4). \n\n2. From section 3.4, \"Our testing framework starts with a manual review of selected functions, leading to the creation of a context file and a golden code file for each problem (see Figure 3)\". I do not find how Figure 3 is correlated with the testing framework, Figure 17 in Appendix R may be a better example.\n\nQuestions: 1. The details of the testing framework and the corresponding effectiveness should be discussed.\n\n2. For Rosalind functions, why and how are the generated codes compared with the gold code outputs?\n\n3. What are the guidelines while designing the 5 different prompt styles for the subject LLMs?", "labeling_timestamp": "2026-01-11T16:53:50.587516", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper discusses prompt length and context (e.g., long prompts >2600 tokens) but does not describe any ‘five prompt styles’ or state whether prompt styles were derived from prior literature, lectures, or experimental experience. There is no provenance information for multiple prompt styles in the provided text.", "evidence": "\"(1) Successful models accommodate a long prompt (> 2600 tokens) with full context, for functional dependencies.\"", "section": "Abstract"}
{"claim": "The paper does not justify why model editing is necessary given widespread use of APIs and their capabilities.", "claim_type": "subjective", "paper_id": "m0DS4OOmSY", "paper_title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Ys6GEBpHKH", "reviewer": "Reviewer_2vUP", "review_text": "Comment: Dear Authors,\n\nThank you for your thorough rebuttal. I will update my review to raise my score.\n\nHowever, I have some lingering questions that you might consider addressing in your paper to provide more clarity and understanding: Is model editing necessary in the age of APIs? If so, who is going to perform model editing at all? In what situations should model editing be even considered aside from FT or RAGs?", "labeling_timestamp": "2026-01-11T16:53:52.410378", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper motivates model editing primarily by arguing that fine-tuning is prohibitively expensive and that editing is an efficient alternative (Introduction), but it does not discuss or justify why editing is necessary in light of widely used API-based services or their capabilities — no discussion of APIs as an alternative is provided.", "evidence": "While directly fine-tuning an LLM on calibrated knowledge can help mitigate this problem, which is prohibitive due to hardware constraints and resource budget [9; 10; 11]. To this end, model editing [12; 13; 14] has been proposed to efficiently update knowledge within LLM.", "section": "1 Introduction"}
{"claim": "The paper fails to specify who (which users or organizations) would be responsible for performing model editing in practice.", "claim_type": "other", "paper_id": "m0DS4OOmSY", "paper_title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Ys6GEBpHKH", "reviewer": "Reviewer_2vUP", "review_text": "Comment: Dear Authors,\n\nThank you for your thorough rebuttal. I will update my review to raise my score.\n\nHowever, I have some lingering questions that you might consider addressing in your paper to provide more clarity and understanding: Is model editing necessary in the age of APIs? If so, who is going to perform model editing at all? In what situations should model editing be even considered aside from FT or RAGs?", "labeling_timestamp": "2026-01-11T16:53:54.853664", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper discusses deployment, operational efficiency, and ‘practical use in production’ but does not identify or assign responsibility to any users, roles, or organizations who would perform model editing in practice. There is no passage specifying which parties would be responsible for performing edits.", "evidence": "“Technically, we conducted an in-depth analysis of the side effects, operational efficiency, and deployment of edited LLM, and discussed their practical use in production.”", "section": "1 Introduction"}
{"claim": "The paper does not identify scenarios where model editing is preferable to fine-tuning or retrieval-augmented generation (RAG) approaches.", "claim_type": "baseline", "paper_id": "m0DS4OOmSY", "paper_title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Ys6GEBpHKH", "reviewer": "Reviewer_2vUP", "review_text": "Comment: Dear Authors,\n\nThank you for your thorough rebuttal. I will update my review to raise my score.\n\nHowever, I have some lingering questions that you might consider addressing in your paper to provide more clarity and understanding: Is model editing necessary in the age of APIs? If so, who is going to perform model editing at all? In what situations should model editing be even considered aside from FT or RAGs?", "labeling_timestamp": "2026-01-11T16:54:26.178748", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper does identify a scenario where editing is preferable to fine-tuning (fine-tuning is described as prohibitive due to hardware/resource constraints), but it does not discuss or identify scenarios where editing is preferable to retrieval-augmented generation (RAG) approaches.", "evidence": "\"While directly fine-tuning an LLM on calibrated knowledge can help mitigate this problem, which is prohibitive due to hardware constraints and resource budget [9; 10; 11]. To this end, model editing [12; 13; 14] has been proposed to efficiently update knowledge within LLM.\" \n\n\"Model Editing Methods. ... (3) retrival based methods: SERAC [18], (4) extra parameters based methods: GRACE [38].\"", "section": "Introduction; Section 3.1 Experimental Setups"}
{"claim": "The paper does not provide the algorithmic complexity in terms of input dimension, output dimension, batch size, and number of parameters.", "claim_type": "methodology", "paper_id": "jrNlWfor7q", "paper_title": "Kronecker-Factored Approximate Curvature for Physics-Informed Neural Networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "fjswa13z9o", "reviewer": "Reviewer_TQnM", "review_text": "Summary: This work considers the problem of optimising partial differential equations (PDE) with neural networks, in particular second-order optimization.\nEven if simple models (multi-layer perceptrons) are used, for which KFAC approximations of the curvature matrix are well-known, it needs to derive new approximations because the loss function used for solving PDEs is different than previous work (square loss or cross entropy), in particular it includes derivatives of the model with respect to the input, which is interpreted as a larger model with weight sharing. Therefore, it uses the previously (but recently) developed technique of KFAC with weight sharing. Experiments show that the new method is successful when applied to a few example PDEs.\n\nStrengths: The paper is clearly written and seems correct, both the theory and experiment. It seems to be the first application of KFAC to PDEs and has the potential of a high impact.\n\nWeaknesses: Even if experiments with a relatively large number of parameters are shown (10^5), the paper does not provide the complexity of the algorithm, in terms of relevant quantities, for example the input dimension, output dimension, batch size, number of parameters, etc. For example, does the method scale poorly with the input dimension? I would expect quadratic scaling with input dimension. I have not read the forward laplacian paper, but I have the feeling that improves scaling by only a constant factor. https://arxiv.org/abs/1206.6464 argued that it may not exist an algorithm for computing the diagonal of the Hessian that is linear in the dimension using automatic differentiation. Also, as with other KFAC approximations, it remains unclear how general is the approach, e.g. how to apply it to higher order PDEs or other neural network models.\n\nQuestions: NA", "labeling_timestamp": "2026-01-11T16:54:24.276890", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper discusses high computational cost and qualitative scaling improvements of the proposed KFAC method (Abstract, Introduction, Sections 2–3) but does not present explicit algorithmic complexity formulas or Big-O expressions parameterized by input dimension, output dimension, batch size, or number of parameters.", "evidence": "Abstract: \"While promising, the proposed methods only scale to networks with a few thousand parameters due to the high computational cost to evaluate, store, and invert the curvature matrix. We propose Kronecker-factored approximate curvature (KFAC) for PINN losses that greatly reduces the computational cost and allows scaling to much larger networks.\" Introduction: \"However, these methods require solving a linear system in the network's high-dimensional parameter space at cubic computational iteration cost, which prohibits scaling such approaches.\" Section 2.2: \"It is cheap to store and invert by inverting the two Kronecker factors.\"", "section": "Abstract; 1 Introduction; 2.2 Kronecker-factored Approximate Curvature"}
{"claim": "The paper does not analyze how the proposed method's computational cost scales with input dimension.", "claim_type": "methodology", "paper_id": "jrNlWfor7q", "paper_title": "Kronecker-Factored Approximate Curvature for Physics-Informed Neural Networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "fjswa13z9o", "reviewer": "Reviewer_TQnM", "review_text": "Summary: This work considers the problem of optimising partial differential equations (PDE) with neural networks, in particular second-order optimization.\nEven if simple models (multi-layer perceptrons) are used, for which KFAC approximations of the curvature matrix are well-known, it needs to derive new approximations because the loss function used for solving PDEs is different than previous work (square loss or cross entropy), in particular it includes derivatives of the model with respect to the input, which is interpreted as a larger model with weight sharing. Therefore, it uses the previously (but recently) developed technique of KFAC with weight sharing. Experiments show that the new method is successful when applied to a few example PDEs.\n\nStrengths: The paper is clearly written and seems correct, both the theory and experiment. It seems to be the first application of KFAC to PDEs and has the potential of a high impact.\n\nWeaknesses: Even if experiments with a relatively large number of parameters are shown (10^5), the paper does not provide the complexity of the algorithm, in terms of relevant quantities, for example the input dimension, output dimension, batch size, number of parameters, etc. For example, does the method scale poorly with the input dimension? I would expect quadratic scaling with input dimension. I have not read the forward laplacian paper, but I have the feeling that improves scaling by only a constant factor. https://arxiv.org/abs/1206.6464 argued that it may not exist an algorithm for computing the diagonal of the Hessian that is linear in the dimension using automatic differentiation. Also, as with other KFAC approximations, it remains unclear how general is the approach, e.g. how to apply it to higher order PDEs or other neural network models.\n\nQuestions: NA", "labeling_timestamp": "2026-01-11T16:54:22.874180", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes algorithmic techniques (Taylor-mode AD, KFAC with weight sharing) and reports empirical scaling results, but contains no theoretical or explicit analysis of how computational cost grows with the input dimension d (no complexity formulas or scaling bounds are provided). The manuscript only makes empirical claims about favorable scaling (§Abstract, §4) and describes per-derivative forward-propagation rules (§3.1) without deriving overall cost as a function of input dimension.", "evidence": "Abstract: \"Empirically, we find that our KFAC-based optimizers are competitive with expensive second-order methods on small problems, scale more favorably to higher-dimensional neural networks and PDEs, and consistently outperform first-order methods and LBFGS.\"  §3.1: \"Our goal is to evaluate first-and second-order partial derivatives of the form ∂_{x_i} u, ∂^2_{x_i,x_j} u for i, j = 1, . . . , d.\"", "section": "Abstract; §3.1"}
{"claim": "The reviewer expects the method's computational cost to scale quadratically with the input dimension.", "claim_type": "quantitative", "paper_id": "jrNlWfor7q", "paper_title": "Kronecker-Factored Approximate Curvature for Physics-Informed Neural Networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "fjswa13z9o", "reviewer": "Reviewer_TQnM", "review_text": "Summary: This work considers the problem of optimising partial differential equations (PDE) with neural networks, in particular second-order optimization.\nEven if simple models (multi-layer perceptrons) are used, for which KFAC approximations of the curvature matrix are well-known, it needs to derive new approximations because the loss function used for solving PDEs is different than previous work (square loss or cross entropy), in particular it includes derivatives of the model with respect to the input, which is interpreted as a larger model with weight sharing. Therefore, it uses the previously (but recently) developed technique of KFAC with weight sharing. Experiments show that the new method is successful when applied to a few example PDEs.\n\nStrengths: The paper is clearly written and seems correct, both the theory and experiment. It seems to be the first application of KFAC to PDEs and has the potential of a high impact.\n\nWeaknesses: Even if experiments with a relatively large number of parameters are shown (10^5), the paper does not provide the complexity of the algorithm, in terms of relevant quantities, for example the input dimension, output dimension, batch size, number of parameters, etc. For example, does the method scale poorly with the input dimension? I would expect quadratic scaling with input dimension. I have not read the forward laplacian paper, but I have the feeling that improves scaling by only a constant factor. https://arxiv.org/abs/1206.6464 argued that it may not exist an algorithm for computing the diagonal of the Hessian that is linear in the dimension using automatic differentiation. Also, as with other KFAC approximations, it remains unclear how general is the approach, e.g. how to apply it to higher order PDEs or other neural network models.\n\nQuestions: NA", "labeling_timestamp": "2026-01-11T16:54:24.177587", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explains that Taylor-mode must propagate first- and second-order directional derivatives ∂_{x_i}u and ∂^2_{x_i,x_j}u for i,j=1..d, which implies work over O(d^2) second-order directions in the general case. However, the authors also state that for differential operators with special structure (e.g. the Laplacian) this can be fused to a more efficient computation, reducing the cost, so quadratic scaling in input dimension is not always incurred.", "evidence": "\"Our goal is to evaluate first-and second-order partial derivatives of the form ∂ x_i u, ∂^2_{x_i,x_j} u for i, j = 1 , . . . , d .\" and \"For differential operators of special structure, we can fuse the Taylor-mode forward propagation of individual directional derivatives in Equations (5) and (6) and obtain a more efficient computation. E.g., to comp...\"", "section": "3.1 Higher-order Forward Mode Automatic Differentiation as Weight Sharing"}
{"claim": "The forward Laplacian approach likely improves scaling only by a constant factor.", "claim_type": "quantitative", "paper_id": "jrNlWfor7q", "paper_title": "Kronecker-Factored Approximate Curvature for Physics-Informed Neural Networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "fjswa13z9o", "reviewer": "Reviewer_TQnM", "review_text": "Summary: This work considers the problem of optimising partial differential equations (PDE) with neural networks, in particular second-order optimization.\nEven if simple models (multi-layer perceptrons) are used, for which KFAC approximations of the curvature matrix are well-known, it needs to derive new approximations because the loss function used for solving PDEs is different than previous work (square loss or cross entropy), in particular it includes derivatives of the model with respect to the input, which is interpreted as a larger model with weight sharing. Therefore, it uses the previously (but recently) developed technique of KFAC with weight sharing. Experiments show that the new method is successful when applied to a few example PDEs.\n\nStrengths: The paper is clearly written and seems correct, both the theory and experiment. It seems to be the first application of KFAC to PDEs and has the potential of a high impact.\n\nWeaknesses: Even if experiments with a relatively large number of parameters are shown (10^5), the paper does not provide the complexity of the algorithm, in terms of relevant quantities, for example the input dimension, output dimension, batch size, number of parameters, etc. For example, does the method scale poorly with the input dimension? I would expect quadratic scaling with input dimension. I have not read the forward laplacian paper, but I have the feeling that improves scaling by only a constant factor. https://arxiv.org/abs/1206.6464 argued that it may not exist an algorithm for computing the diagonal of the Hessian that is linear in the dimension using automatic differentiation. Also, as with other KFAC approximations, it remains unclear how general is the approach, e.g. how to apply it to higher order PDEs or other neural network models.\n\nQuestions: NA", "labeling_timestamp": "2026-01-11T16:55:06.424464", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper states that the forward Laplacian reduces weight sharing and yields a more efficient scheme (Intro and §3.1), but it does not quantify the scaling improvement or claim that the benefit is only a constant-factor speedup. There is insufficient information to conclude that the improvement is only by a constant factor.", "evidence": "“We show that, for specific differential operators, the weight sharing in Taylor-mode can be further reduced by absorbing the reduction of partial derivatives into the forward propagation, producing a more efficient scheme. For the prominent example of the Laplace operator, this recovers and generalizes the forward Laplacian framework [29] (§3.2 and eq. (9)).”\n\n“Forward Laplacian For differential operators of special structure, we can fuse the Taylor-mode forward propagation of individual directional derivatives in Equations (5) and (6) and obtain a more efficient computation.”", "section": "Introduction; 3.1 Higher-order Forward Mode Automatic Differentiation"}
{"claim": "Prior work argued there may be no algorithm to compute the diagonal of the Hessian that is linear in dimension using automatic differentiation.", "claim_type": "methodology", "paper_id": "jrNlWfor7q", "paper_title": "Kronecker-Factored Approximate Curvature for Physics-Informed Neural Networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "fjswa13z9o", "reviewer": "Reviewer_TQnM", "review_text": "Summary: This work considers the problem of optimising partial differential equations (PDE) with neural networks, in particular second-order optimization.\nEven if simple models (multi-layer perceptrons) are used, for which KFAC approximations of the curvature matrix are well-known, it needs to derive new approximations because the loss function used for solving PDEs is different than previous work (square loss or cross entropy), in particular it includes derivatives of the model with respect to the input, which is interpreted as a larger model with weight sharing. Therefore, it uses the previously (but recently) developed technique of KFAC with weight sharing. Experiments show that the new method is successful when applied to a few example PDEs.\n\nStrengths: The paper is clearly written and seems correct, both the theory and experiment. It seems to be the first application of KFAC to PDEs and has the potential of a high impact.\n\nWeaknesses: Even if experiments with a relatively large number of parameters are shown (10^5), the paper does not provide the complexity of the algorithm, in terms of relevant quantities, for example the input dimension, output dimension, batch size, number of parameters, etc. For example, does the method scale poorly with the input dimension? I would expect quadratic scaling with input dimension. I have not read the forward laplacian paper, but I have the feeling that improves scaling by only a constant factor. https://arxiv.org/abs/1206.6464 argued that it may not exist an algorithm for computing the diagonal of the Hessian that is linear in the dimension using automatic differentiation. Also, as with other KFAC approximations, it remains unclear how general is the approach, e.g. how to apply it to higher order PDEs or other neural network models.\n\nQuestions: NA", "labeling_timestamp": "2026-01-11T16:54:38.784387", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper discusses using Taylor-mode (forward-mode) automatic differentiation to express PDE differential-operator computations as forward passes with weight sharing and applies KFAC to Gauss-Newton matrices, but it does not mention or evaluate prior claims about the (im)possibility of computing the Hessian diagonal in linear time with automatic differentiation. Therefore the reviewer claim is not addressed by the paper.", "evidence": "Abstract: \"To establish KFAC for such losses, we use Taylor-mode automatic differentiation to describe the differential operator's computation graph as a forward network with shared weights.\" \n\nSection 3.1: \"Here, we review higher-order forward mode, also known as Taylor-mode, automatic differentiation...\"", "section": "Abstract; 3.1 (Higher-order Forward Mode Automatic Differentiation)"}
{"claim": "It remains unclear how general the KFAC weight-sharing approach is, for example how to apply it to higher-order PDEs or other neural network models.", "claim_type": "novelty", "paper_id": "jrNlWfor7q", "paper_title": "Kronecker-Factored Approximate Curvature for Physics-Informed Neural Networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "fjswa13z9o", "reviewer": "Reviewer_TQnM", "review_text": "Summary: This work considers the problem of optimising partial differential equations (PDE) with neural networks, in particular second-order optimization.\nEven if simple models (multi-layer perceptrons) are used, for which KFAC approximations of the curvature matrix are well-known, it needs to derive new approximations because the loss function used for solving PDEs is different than previous work (square loss or cross entropy), in particular it includes derivatives of the model with respect to the input, which is interpreted as a larger model with weight sharing. Therefore, it uses the previously (but recently) developed technique of KFAC with weight sharing. Experiments show that the new method is successful when applied to a few example PDEs.\n\nStrengths: The paper is clearly written and seems correct, both the theory and experiment. It seems to be the first application of KFAC to PDEs and has the potential of a high impact.\n\nWeaknesses: Even if experiments with a relatively large number of parameters are shown (10^5), the paper does not provide the complexity of the algorithm, in terms of relevant quantities, for example the input dimension, output dimension, batch size, number of parameters, etc. For example, does the method scale poorly with the input dimension? I would expect quadratic scaling with input dimension. I have not read the forward laplacian paper, but I have the feeling that improves scaling by only a constant factor. https://arxiv.org/abs/1206.6464 argued that it may not exist an algorithm for computing the diagonal of the Hessian that is linear in the dimension using automatic differentiation. Also, as with other KFAC approximations, it remains unclear how general is the approach, e.g. how to apply it to higher order PDEs or other neural network models.\n\nQuestions: NA", "labeling_timestamp": "2026-01-11T16:54:53.299067", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper asserts that the method is general (mentions applicability to higher-order PDEs and arbitrary architectures) but only develops details for MLPs and second-order Taylor-mode. It states higher-order PDEs and other layer types can be handled 'analogously' but provides no worked examples or concrete procedures for those cases, so the reviewer's claim that it is unclear how general the approach is is partially true.", "evidence": [{"quote": "For simplicity, we present our approach for multi-layer perceptrons (MLPs) consisting of fullyconnected and element-wise activation layers. However, the generality of Taylor-mode automatic differentiation and KFAC for linear layers with weight sharing allows our KFAC to be applied to such layers (e.g. fully-connected, convolution, attention) in arbitrary neural network architectures.", "section": "Section 2 (Background)"}, {"quote": "Many PDEs only incorporate first- and second-order partial derivatives and we focus our discussion on second-order Taylor-mode for MLPs to keep the presentation light. However, one can treat higher-order PDEs and arbitrary network architectures completely analogously.", "section": "Section 3.1 (Higher-order Forward Mode Automatic Differentiation as Weight Sharing)"}, {"quote": "Thanks to the generality of Taylor-mode and KFAC for weight sharing layers [17], our approach is widely applicable.", "section": "Introduction / Abstract"}], "section": ""}
{"claim": "The paper omits critical details of the phylogenetic inference setup, specifically not specifying what the states are.", "claim_type": "methodology", "paper_id": "VJDFhkwQg6", "paper_title": "Federated contrastive GFlowNets", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "Om8tHTwSEN", "reviewer": "Reviewer_Xj4h", "review_text": "Summary: An algorithm is proposed for training a generative flow network (GFlowNet) to match a product of distributions, each of which is sampled by a \"client\" GFlowNet. A training objective is stated, its correctness is proved, and bounds relating error of clients to that of the centralized model are derived. Experiments are done on federated versions of several existing tasks and on a new domain for GFlowNets (a toy case of Bayesian phylogenetic inference), where it performs well compared to a non-GFlowNet baseline.\n\nStrengths: - Excellent exposition. I have no complaints on the presentation or on the math. I easily grasped the main idea on the first reading.\n  - But see one of the weaknesses on why two of the proofs could be one-line reductions to existing results.\n- Beautiful and original idea. Sampling from a product of GFlowNets is natural, but the application to federated learning is creative. \n  - A reference to consider adding is \"Compositional sculpting of iterative generative processes\" [Garipov et al., NeurIPS 2023, to appear], which considers a different kind of modular combination of samplers and could probably be used in a federated setting as well. (I am well aware that its omission is not a weakness as this paper appeared on arXiv on 28 September!)\n- Application to phylogenetic inference is also a new contribution that could be expanded; I wonder how it would scale.\n\nWeaknesses: - Critical missing details for phylogenetic inference. What are the states? What are the actions?\n- It should be noted that an alternative approach to the problem in the Bayesian posterior setting is to train a single GFlowNet with a *stochastic* reward, as done in [Deleu et al., UAI 2022] and [Deleu et al., NeurIPS 2023].\n- The experimental validation is a little thin (esp. given the next point). \n  - This is not a major weakness for me given that there may be no natural baselines, and it is made up for by the good idea and diversity of experiments.\n  - However, there are still comparisons to be made, such as using different training objectives for the client models (TB or CB).\n- The \"contrastive loss\" is claimed as original, but in fact it is not. I suggest that the authors revise the discussion on this in section 3.3 and in the claimed contributions. \n  - It is well known that the expected square difference between two independent samples from a distribution is the same as twice the variance. So the loss in section 3.3 is equivalently optimizing variance of $\\log P_F(\\tau)-\\log R(x)-\\log P_B(\\tau\\mid x)$.\n  - This variance loss is described in \"GFlowNets and VI\" [Malkin et al., ICLR 2023] (\"local baseline\"). It was independently discovered and tested in \"Robust scheduling\" [Zhang et al., ICLR 2023]. Such a gradient variance reduction method is originally proposed in \"VarGrad\" [Richter et al., NeurIPS 2020].\n  - This leads me to wonder whether differences between CL and TB (Figure 6) are only due to insufficiently high learning rate on logZ for TB, or differently tuned learning rates for the policies in both algorithms. \n    - Figure 7, which is hidden in the Appendix, shows that on other domains, CB performs similarly to TB/DB.\n    - It seems a little misleading not to point to Figure 7 in the main text. In my opinion, it would not make the paper weaker to show all four plots in the main text and to say that first steps are made towards understanding when CB is helpful and when it is not.\n  - Related to this, the proofs of all the results in section 3.3 have one-line reductions to existing results (the TB training theorem and the TB gradient analysis theorem). The current proofs are quite obfuscated.\n- The bound in Theorem 2 is nice but may not be very useful in practice. If one of the models is missing a mode, which is quite possible with reverse-KL objectives like those used here, $\\alpha_n$ will be very small, and the final bound is not useful. Therefore, I wonder if one can obtain similar bounds on other divergences that are less sensitive than the Jeffrey divergence to mode-dropping.\n\nI am happy to increase the score from 5 to 6 or even 8 if these are satisfactorily addressed.\n\nQuestions: I like the motivation paragraph on the first page, but what do you see as the main future applications: Privacy-preserving distributed training? Large-scale Bayesian inference where the full reward is expensive to compute on a single client? Modular combination of pretrained GFlowNets?\n\nIn particular, I am not sure to understand the private-rewards application. If each GFlowNet fits (close to) perfectly and can share its policies, then each client's reward density can be recovered as the ratio of its forward and backward policies.", "labeling_timestamp": "2026-01-11T16:54:43.188711", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper text references Bayesian phylogenetic inference as one of the empirical tasks (Abstract and Introduction), but the supplied content does not include the experimental/methods sections or any description of the phylogenetic inference setup (e.g., definition of states). Therefore, from the given content we cannot verify whether the paper omits those details.", "evidence": "Abstract: \"We empirically attest the performance of FC-GFlowNets in four settings, including grid-world, sequence, multiset generation, and Bayesian phylogenetic inference.\"; Introduction (Contributions): \"... illustrate the potential of GFlowNets in a novel application: Bayesian phylogenetic inference.\"", "section": "Abstract; 1 INTRODUCTION"}
{"claim": "The paper omits critical details of the phylogenetic inference setup, specifically not specifying what the actions are.", "claim_type": "methodology", "paper_id": "VJDFhkwQg6", "paper_title": "Federated contrastive GFlowNets", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "Om8tHTwSEN", "reviewer": "Reviewer_Xj4h", "review_text": "Summary: An algorithm is proposed for training a generative flow network (GFlowNet) to match a product of distributions, each of which is sampled by a \"client\" GFlowNet. A training objective is stated, its correctness is proved, and bounds relating error of clients to that of the centralized model are derived. Experiments are done on federated versions of several existing tasks and on a new domain for GFlowNets (a toy case of Bayesian phylogenetic inference), where it performs well compared to a non-GFlowNet baseline.\n\nStrengths: - Excellent exposition. I have no complaints on the presentation or on the math. I easily grasped the main idea on the first reading.\n  - But see one of the weaknesses on why two of the proofs could be one-line reductions to existing results.\n- Beautiful and original idea. Sampling from a product of GFlowNets is natural, but the application to federated learning is creative. \n  - A reference to consider adding is \"Compositional sculpting of iterative generative processes\" [Garipov et al., NeurIPS 2023, to appear], which considers a different kind of modular combination of samplers and could probably be used in a federated setting as well. (I am well aware that its omission is not a weakness as this paper appeared on arXiv on 28 September!)\n- Application to phylogenetic inference is also a new contribution that could be expanded; I wonder how it would scale.\n\nWeaknesses: - Critical missing details for phylogenetic inference. What are the states? What are the actions?\n- It should be noted that an alternative approach to the problem in the Bayesian posterior setting is to train a single GFlowNet with a *stochastic* reward, as done in [Deleu et al., UAI 2022] and [Deleu et al., NeurIPS 2023].\n- The experimental validation is a little thin (esp. given the next point). \n  - This is not a major weakness for me given that there may be no natural baselines, and it is made up for by the good idea and diversity of experiments.\n  - However, there are still comparisons to be made, such as using different training objectives for the client models (TB or CB).\n- The \"contrastive loss\" is claimed as original, but in fact it is not. I suggest that the authors revise the discussion on this in section 3.3 and in the claimed contributions. \n  - It is well known that the expected square difference between two independent samples from a distribution is the same as twice the variance. So the loss in section 3.3 is equivalently optimizing variance of $\\log P_F(\\tau)-\\log R(x)-\\log P_B(\\tau\\mid x)$.\n  - This variance loss is described in \"GFlowNets and VI\" [Malkin et al., ICLR 2023] (\"local baseline\"). It was independently discovered and tested in \"Robust scheduling\" [Zhang et al., ICLR 2023]. Such a gradient variance reduction method is originally proposed in \"VarGrad\" [Richter et al., NeurIPS 2020].\n  - This leads me to wonder whether differences between CL and TB (Figure 6) are only due to insufficiently high learning rate on logZ for TB, or differently tuned learning rates for the policies in both algorithms. \n    - Figure 7, which is hidden in the Appendix, shows that on other domains, CB performs similarly to TB/DB.\n    - It seems a little misleading not to point to Figure 7 in the main text. In my opinion, it would not make the paper weaker to show all four plots in the main text and to say that first steps are made towards understanding when CB is helpful and when it is not.\n  - Related to this, the proofs of all the results in section 3.3 have one-line reductions to existing results (the TB training theorem and the TB gradient analysis theorem). The current proofs are quite obfuscated.\n- The bound in Theorem 2 is nice but may not be very useful in practice. If one of the models is missing a mode, which is quite possible with reverse-KL objectives like those used here, $\\alpha_n$ will be very small, and the final bound is not useful. Therefore, I wonder if one can obtain similar bounds on other divergences that are less sensitive than the Jeffrey divergence to mode-dropping.\n\nI am happy to increase the score from 5 to 6 or even 8 if these are satisfactorily addressed.\n\nQuestions: I like the motivation paragraph on the first page, but what do you see as the main future applications: Privacy-preserving distributed training? Large-scale Bayesian inference where the full reward is expensive to compute on a single client? Modular combination of pretrained GFlowNets?\n\nIn particular, I am not sure to understand the private-rewards application. If each GFlowNet fits (close to) perfectly and can share its policies, then each client's reward density can be recovered as the ratio of its forward and backward policies.", "labeling_timestamp": "2026-01-11T16:54:41.303031", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper mentions Bayesian phylogenetic inference as one of the experimental tasks (Abstract, Introduction) but the provided content contains no description of the phylogenetic inference setup and does not specify the action space or what the actions are. There is no methods or experiment subsection in the supplied text that defines actions for the phylogenetic task.", "evidence": "Abstract: \"We empirically attest the performance of FC-GFlowNets in four settings, including grid-world, sequence, multiset generation, and Bayesian phylogenetic inference.\"", "section": "Abstract / Introduction"}
{"claim": "The paper fails to mention the alternative approach of training a single GFlowNet with a stochastic reward as used in prior work.", "claim_type": "other", "paper_id": "VJDFhkwQg6", "paper_title": "Federated contrastive GFlowNets", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "Om8tHTwSEN", "reviewer": "Reviewer_Xj4h", "review_text": "Summary: An algorithm is proposed for training a generative flow network (GFlowNet) to match a product of distributions, each of which is sampled by a \"client\" GFlowNet. A training objective is stated, its correctness is proved, and bounds relating error of clients to that of the centralized model are derived. Experiments are done on federated versions of several existing tasks and on a new domain for GFlowNets (a toy case of Bayesian phylogenetic inference), where it performs well compared to a non-GFlowNet baseline.\n\nStrengths: - Excellent exposition. I have no complaints on the presentation or on the math. I easily grasped the main idea on the first reading.\n  - But see one of the weaknesses on why two of the proofs could be one-line reductions to existing results.\n- Beautiful and original idea. Sampling from a product of GFlowNets is natural, but the application to federated learning is creative. \n  - A reference to consider adding is \"Compositional sculpting of iterative generative processes\" [Garipov et al., NeurIPS 2023, to appear], which considers a different kind of modular combination of samplers and could probably be used in a federated setting as well. (I am well aware that its omission is not a weakness as this paper appeared on arXiv on 28 September!)\n- Application to phylogenetic inference is also a new contribution that could be expanded; I wonder how it would scale.\n\nWeaknesses: - Critical missing details for phylogenetic inference. What are the states? What are the actions?\n- It should be noted that an alternative approach to the problem in the Bayesian posterior setting is to train a single GFlowNet with a *stochastic* reward, as done in [Deleu et al., UAI 2022] and [Deleu et al., NeurIPS 2023].\n- The experimental validation is a little thin (esp. given the next point). \n  - This is not a major weakness for me given that there may be no natural baselines, and it is made up for by the good idea and diversity of experiments.\n  - However, there are still comparisons to be made, such as using different training objectives for the client models (TB or CB).\n- The \"contrastive loss\" is claimed as original, but in fact it is not. I suggest that the authors revise the discussion on this in section 3.3 and in the claimed contributions. \n  - It is well known that the expected square difference between two independent samples from a distribution is the same as twice the variance. So the loss in section 3.3 is equivalently optimizing variance of $\\log P_F(\\tau)-\\log R(x)-\\log P_B(\\tau\\mid x)$.\n  - This variance loss is described in \"GFlowNets and VI\" [Malkin et al., ICLR 2023] (\"local baseline\"). It was independently discovered and tested in \"Robust scheduling\" [Zhang et al., ICLR 2023]. Such a gradient variance reduction method is originally proposed in \"VarGrad\" [Richter et al., NeurIPS 2020].\n  - This leads me to wonder whether differences between CL and TB (Figure 6) are only due to insufficiently high learning rate on logZ for TB, or differently tuned learning rates for the policies in both algorithms. \n    - Figure 7, which is hidden in the Appendix, shows that on other domains, CB performs similarly to TB/DB.\n    - It seems a little misleading not to point to Figure 7 in the main text. In my opinion, it would not make the paper weaker to show all four plots in the main text and to say that first steps are made towards understanding when CB is helpful and when it is not.\n  - Related to this, the proofs of all the results in section 3.3 have one-line reductions to existing results (the TB training theorem and the TB gradient analysis theorem). The current proofs are quite obfuscated.\n- The bound in Theorem 2 is nice but may not be very useful in practice. If one of the models is missing a mode, which is quite possible with reverse-KL objectives like those used here, $\\alpha_n$ will be very small, and the final bound is not useful. Therefore, I wonder if one can obtain similar bounds on other divergences that are less sensitive than the Jeffrey divergence to mode-dropping.\n\nI am happy to increase the score from 5 to 6 or even 8 if these are satisfactorily addressed.\n\nQuestions: I like the motivation paragraph on the first page, but what do you see as the main future applications: Privacy-preserving distributed training? Large-scale Bayesian inference where the full reward is expensive to compute on a single client? Modular combination of pretrained GFlowNets?\n\nIn particular, I am not sure to understand the private-rewards application. If each GFlowNet fits (close to) perfectly and can share its policies, then each client's reward density can be recovered as the ratio of its forward and backward policies.", "labeling_timestamp": "2026-01-11T16:55:13.903353", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper consistently describes and advocates a divide-and-conquer federation scheme where each client trains a local GFlowNet and sends policies to a server for a single aggregation step; the main text does not describe or mention the alternative of training a single GFlowNet with a stochastic reward. Related work is deferred to an appendix, so the main paper does not present that alternative approach.", "evidence": "Introduction: \"This paper extends the theory of GFlowNets to develop a simple divide-and-conquer algorithm for federated learning of GFlowNets, in which clients learn locally GFlowNets to sample from their individual rewards and send their policies to a server for aggregation. Importantly, this procedure requires a single round of communication between the client and server sides.\" \n\nSection 3.1: \"To circumvent the restrictions imposed by the problem statement, we propose a simple divide-and-conquer strategy. First, each user n trains independently a GFlowNet with forward/backward policies p^(n)_F and p^(n)_B to sample from R_n. Then, the users send their policies to a server for a single aggregation step, in which the server combines the local GFlowNets into a new one...\" \n\nProblem statement: \"For related works, see Appendix D.\"", "section": "Introduction / Problem statement / Section 3.1 (FEDERATED GFLOWNETS)"}
{"claim": "The experimental validation is thin and does not provide comprehensive empirical support for the proposed method.", "claim_type": "subjective", "paper_id": "VJDFhkwQg6", "paper_title": "Federated contrastive GFlowNets", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "Om8tHTwSEN", "reviewer": "Reviewer_Xj4h", "review_text": "Summary: An algorithm is proposed for training a generative flow network (GFlowNet) to match a product of distributions, each of which is sampled by a \"client\" GFlowNet. A training objective is stated, its correctness is proved, and bounds relating error of clients to that of the centralized model are derived. Experiments are done on federated versions of several existing tasks and on a new domain for GFlowNets (a toy case of Bayesian phylogenetic inference), where it performs well compared to a non-GFlowNet baseline.\n\nStrengths: - Excellent exposition. I have no complaints on the presentation or on the math. I easily grasped the main idea on the first reading.\n  - But see one of the weaknesses on why two of the proofs could be one-line reductions to existing results.\n- Beautiful and original idea. Sampling from a product of GFlowNets is natural, but the application to federated learning is creative. \n  - A reference to consider adding is \"Compositional sculpting of iterative generative processes\" [Garipov et al., NeurIPS 2023, to appear], which considers a different kind of modular combination of samplers and could probably be used in a federated setting as well. (I am well aware that its omission is not a weakness as this paper appeared on arXiv on 28 September!)\n- Application to phylogenetic inference is also a new contribution that could be expanded; I wonder how it would scale.\n\nWeaknesses: - Critical missing details for phylogenetic inference. What are the states? What are the actions?\n- It should be noted that an alternative approach to the problem in the Bayesian posterior setting is to train a single GFlowNet with a *stochastic* reward, as done in [Deleu et al., UAI 2022] and [Deleu et al., NeurIPS 2023].\n- The experimental validation is a little thin (esp. given the next point). \n  - This is not a major weakness for me given that there may be no natural baselines, and it is made up for by the good idea and diversity of experiments.\n  - However, there are still comparisons to be made, such as using different training objectives for the client models (TB or CB).\n- The \"contrastive loss\" is claimed as original, but in fact it is not. I suggest that the authors revise the discussion on this in section 3.3 and in the claimed contributions. \n  - It is well known that the expected square difference between two independent samples from a distribution is the same as twice the variance. So the loss in section 3.3 is equivalently optimizing variance of $\\log P_F(\\tau)-\\log R(x)-\\log P_B(\\tau\\mid x)$.\n  - This variance loss is described in \"GFlowNets and VI\" [Malkin et al., ICLR 2023] (\"local baseline\"). It was independently discovered and tested in \"Robust scheduling\" [Zhang et al., ICLR 2023]. Such a gradient variance reduction method is originally proposed in \"VarGrad\" [Richter et al., NeurIPS 2020].\n  - This leads me to wonder whether differences between CL and TB (Figure 6) are only due to insufficiently high learning rate on logZ for TB, or differently tuned learning rates for the policies in both algorithms. \n    - Figure 7, which is hidden in the Appendix, shows that on other domains, CB performs similarly to TB/DB.\n    - It seems a little misleading not to point to Figure 7 in the main text. In my opinion, it would not make the paper weaker to show all four plots in the main text and to say that first steps are made towards understanding when CB is helpful and when it is not.\n  - Related to this, the proofs of all the results in section 3.3 have one-line reductions to existing results (the TB training theorem and the TB gradient analysis theorem). The current proofs are quite obfuscated.\n- The bound in Theorem 2 is nice but may not be very useful in practice. If one of the models is missing a mode, which is quite possible with reverse-KL objectives like those used here, $\\alpha_n$ will be very small, and the final bound is not useful. Therefore, I wonder if one can obtain similar bounds on other divergences that are less sensitive than the Jeffrey divergence to mode-dropping.\n\nI am happy to increase the score from 5 to 6 or even 8 if these are satisfactorily addressed.\n\nQuestions: I like the motivation paragraph on the first page, but what do you see as the main future applications: Privacy-preserving distributed training? Large-scale Bayesian inference where the full reward is expensive to compute on a single client? Modular combination of pretrained GFlowNets?\n\nIn particular, I am not sure to understand the private-rewards application. If each GFlowNet fits (close to) perfectly and can share its policies, then each client's reward density can be recovered as the ratio of its forward and backward policies.", "labeling_timestamp": "2026-01-11T16:55:43.913181", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper states that it includes experiments across four diverse tasks (grid-world, sequence, multiset generation, Bayesian phylogenetic inference) and claims empirical support, but the provided excerpts contain only high-level descriptions and no experimental details (datasets, baselines, metrics, results). Therefore there is insufficient information in the paper content given here to determine whether the experimental validation is 'thin' or 'comprehensive.'", "evidence": "Abstract: \"We empirically attest the performance of FC-GFlowNets in four settings, including grid-world, sequence, multiset generation, and Bayesian phylogenetic inference.\"; Introduction / Contributions: \"We substantiate our methodological contributions with experiments on four different tasks. Notably, our empirical results i ) demonstrate the accuracy of our federated framework for GFlowNets; ii ) show that, in some cases, using the contrastive balance as a training criterion leads to faster convergence rates compared to using trajectory and detailed balances; iii) illustrate the potential of GFlowNets in a novel application: Bayesian phylogenetic inference.\"", "section": "Abstract; Introduction / Contributions"}
{"claim": "The paper does not compare client training objectives, such as using TB versus CB, as experimental baselines.", "claim_type": "baseline", "paper_id": "VJDFhkwQg6", "paper_title": "Federated contrastive GFlowNets", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "Om8tHTwSEN", "reviewer": "Reviewer_Xj4h", "review_text": "Summary: An algorithm is proposed for training a generative flow network (GFlowNet) to match a product of distributions, each of which is sampled by a \"client\" GFlowNet. A training objective is stated, its correctness is proved, and bounds relating error of clients to that of the centralized model are derived. Experiments are done on federated versions of several existing tasks and on a new domain for GFlowNets (a toy case of Bayesian phylogenetic inference), where it performs well compared to a non-GFlowNet baseline.\n\nStrengths: - Excellent exposition. I have no complaints on the presentation or on the math. I easily grasped the main idea on the first reading.\n  - But see one of the weaknesses on why two of the proofs could be one-line reductions to existing results.\n- Beautiful and original idea. Sampling from a product of GFlowNets is natural, but the application to federated learning is creative. \n  - A reference to consider adding is \"Compositional sculpting of iterative generative processes\" [Garipov et al., NeurIPS 2023, to appear], which considers a different kind of modular combination of samplers and could probably be used in a federated setting as well. (I am well aware that its omission is not a weakness as this paper appeared on arXiv on 28 September!)\n- Application to phylogenetic inference is also a new contribution that could be expanded; I wonder how it would scale.\n\nWeaknesses: - Critical missing details for phylogenetic inference. What are the states? What are the actions?\n- It should be noted that an alternative approach to the problem in the Bayesian posterior setting is to train a single GFlowNet with a *stochastic* reward, as done in [Deleu et al., UAI 2022] and [Deleu et al., NeurIPS 2023].\n- The experimental validation is a little thin (esp. given the next point). \n  - This is not a major weakness for me given that there may be no natural baselines, and it is made up for by the good idea and diversity of experiments.\n  - However, there are still comparisons to be made, such as using different training objectives for the client models (TB or CB).\n- The \"contrastive loss\" is claimed as original, but in fact it is not. I suggest that the authors revise the discussion on this in section 3.3 and in the claimed contributions. \n  - It is well known that the expected square difference between two independent samples from a distribution is the same as twice the variance. So the loss in section 3.3 is equivalently optimizing variance of $\\log P_F(\\tau)-\\log R(x)-\\log P_B(\\tau\\mid x)$.\n  - This variance loss is described in \"GFlowNets and VI\" [Malkin et al., ICLR 2023] (\"local baseline\"). It was independently discovered and tested in \"Robust scheduling\" [Zhang et al., ICLR 2023]. Such a gradient variance reduction method is originally proposed in \"VarGrad\" [Richter et al., NeurIPS 2020].\n  - This leads me to wonder whether differences between CL and TB (Figure 6) are only due to insufficiently high learning rate on logZ for TB, or differently tuned learning rates for the policies in both algorithms. \n    - Figure 7, which is hidden in the Appendix, shows that on other domains, CB performs similarly to TB/DB.\n    - It seems a little misleading not to point to Figure 7 in the main text. In my opinion, it would not make the paper weaker to show all four plots in the main text and to say that first steps are made towards understanding when CB is helpful and when it is not.\n  - Related to this, the proofs of all the results in section 3.3 have one-line reductions to existing results (the TB training theorem and the TB gradient analysis theorem). The current proofs are quite obfuscated.\n- The bound in Theorem 2 is nice but may not be very useful in practice. If one of the models is missing a mode, which is quite possible with reverse-KL objectives like those used here, $\\alpha_n$ will be very small, and the final bound is not useful. Therefore, I wonder if one can obtain similar bounds on other divergences that are less sensitive than the Jeffrey divergence to mode-dropping.\n\nI am happy to increase the score from 5 to 6 or even 8 if these are satisfactorily addressed.\n\nQuestions: I like the motivation paragraph on the first page, but what do you see as the main future applications: Privacy-preserving distributed training? Large-scale Bayesian inference where the full reward is expensive to compute on a single client? Modular combination of pretrained GFlowNets?\n\nIn particular, I am not sure to understand the private-rewards application. If each GFlowNet fits (close to) perfectly and can share its policies, then each client's reward density can be recovered as the ratio of its forward and backward policies.", "labeling_timestamp": "2026-01-11T16:55:15.938604", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states that it compares the contrastive balance (CB) training criterion to trajectory balance (TB) and detailed balance (DB), reporting faster convergence in some cases; therefore the reviewer's claim that it does not compare these training objectives is false.", "evidence": "1) Abstract: \"Experiments also suggest that, in some cases, enforcing the contrastive balance can accelerate the training of conventional GFlowNets.\" 2) Contributions: \"iii) show that, in some cases, using the contrastive balance as a training criterion leads to faster convergence rates compared to using trajectory and detailed balances;\"", "section": "Abstract; Introduction / Contributions"}
{"claim": "The contrastive loss is incorrectly claimed as original despite equivalent variance-based formulations in prior literature.", "claim_type": "novelty", "paper_id": "VJDFhkwQg6", "paper_title": "Federated contrastive GFlowNets", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "Om8tHTwSEN", "reviewer": "Reviewer_Xj4h", "review_text": "Summary: An algorithm is proposed for training a generative flow network (GFlowNet) to match a product of distributions, each of which is sampled by a \"client\" GFlowNet. A training objective is stated, its correctness is proved, and bounds relating error of clients to that of the centralized model are derived. Experiments are done on federated versions of several existing tasks and on a new domain for GFlowNets (a toy case of Bayesian phylogenetic inference), where it performs well compared to a non-GFlowNet baseline.\n\nStrengths: - Excellent exposition. I have no complaints on the presentation or on the math. I easily grasped the main idea on the first reading.\n  - But see one of the weaknesses on why two of the proofs could be one-line reductions to existing results.\n- Beautiful and original idea. Sampling from a product of GFlowNets is natural, but the application to federated learning is creative. \n  - A reference to consider adding is \"Compositional sculpting of iterative generative processes\" [Garipov et al., NeurIPS 2023, to appear], which considers a different kind of modular combination of samplers and could probably be used in a federated setting as well. (I am well aware that its omission is not a weakness as this paper appeared on arXiv on 28 September!)\n- Application to phylogenetic inference is also a new contribution that could be expanded; I wonder how it would scale.\n\nWeaknesses: - Critical missing details for phylogenetic inference. What are the states? What are the actions?\n- It should be noted that an alternative approach to the problem in the Bayesian posterior setting is to train a single GFlowNet with a *stochastic* reward, as done in [Deleu et al., UAI 2022] and [Deleu et al., NeurIPS 2023].\n- The experimental validation is a little thin (esp. given the next point). \n  - This is not a major weakness for me given that there may be no natural baselines, and it is made up for by the good idea and diversity of experiments.\n  - However, there are still comparisons to be made, such as using different training objectives for the client models (TB or CB).\n- The \"contrastive loss\" is claimed as original, but in fact it is not. I suggest that the authors revise the discussion on this in section 3.3 and in the claimed contributions. \n  - It is well known that the expected square difference between two independent samples from a distribution is the same as twice the variance. So the loss in section 3.3 is equivalently optimizing variance of $\\log P_F(\\tau)-\\log R(x)-\\log P_B(\\tau\\mid x)$.\n  - This variance loss is described in \"GFlowNets and VI\" [Malkin et al., ICLR 2023] (\"local baseline\"). It was independently discovered and tested in \"Robust scheduling\" [Zhang et al., ICLR 2023]. Such a gradient variance reduction method is originally proposed in \"VarGrad\" [Richter et al., NeurIPS 2020].\n  - This leads me to wonder whether differences between CL and TB (Figure 6) are only due to insufficiently high learning rate on logZ for TB, or differently tuned learning rates for the policies in both algorithms. \n    - Figure 7, which is hidden in the Appendix, shows that on other domains, CB performs similarly to TB/DB.\n    - It seems a little misleading not to point to Figure 7 in the main text. In my opinion, it would not make the paper weaker to show all four plots in the main text and to say that first steps are made towards understanding when CB is helpful and when it is not.\n  - Related to this, the proofs of all the results in section 3.3 have one-line reductions to existing results (the TB training theorem and the TB gradient analysis theorem). The current proofs are quite obfuscated.\n- The bound in Theorem 2 is nice but may not be very useful in practice. If one of the models is missing a mode, which is quite possible with reverse-KL objectives like those used here, $\\alpha_n$ will be very small, and the final bound is not useful. Therefore, I wonder if one can obtain similar bounds on other divergences that are less sensitive than the Jeffrey divergence to mode-dropping.\n\nI am happy to increase the score from 5 to 6 or even 8 if these are satisfactorily addressed.\n\nQuestions: I like the motivation paragraph on the first page, but what do you see as the main future applications: Privacy-preserving distributed training? Large-scale Bayesian inference where the full reward is expensive to compute on a single client? Modular combination of pretrained GFlowNets?\n\nIn particular, I am not sure to understand the private-rewards application. If each GFlowNet fits (close to) perfectly and can share its policies, then each client's reward density can be recovered as the ratio of its forward and backward policies.", "labeling_timestamp": "2026-01-11T16:55:15.908451", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states the contrastive balance (CB) and its induced loss are equivalent in expectation to the variance of a log-partition estimator, and it cites prior work (Zhang et al. 2023a) as having first proposed minimizing that variance. Thus the paper does not claim the variance-based formulation is novel without acknowledgement; the reviewer's claim that the loss is 'incorrectly claimed as original' is contradicted by the paper's text.", "evidence": "Enforcing the CB induces a loss that, in expectation, is equivalent to the variance of an estimator of the log-partition, whose minimization was first proposed by Zhang et al. (2023a).", "section": "Abstract / Introduction"}
{"claim": "The loss in section 3.3 is mathematically equivalent to optimizing the variance of log P_F(τ) - log R(x) - log P_B(τ | x).", "claim_type": "methodology", "paper_id": "VJDFhkwQg6", "paper_title": "Federated contrastive GFlowNets", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "Om8tHTwSEN", "reviewer": "Reviewer_Xj4h", "review_text": "Summary: An algorithm is proposed for training a generative flow network (GFlowNet) to match a product of distributions, each of which is sampled by a \"client\" GFlowNet. A training objective is stated, its correctness is proved, and bounds relating error of clients to that of the centralized model are derived. Experiments are done on federated versions of several existing tasks and on a new domain for GFlowNets (a toy case of Bayesian phylogenetic inference), where it performs well compared to a non-GFlowNet baseline.\n\nStrengths: - Excellent exposition. I have no complaints on the presentation or on the math. I easily grasped the main idea on the first reading.\n  - But see one of the weaknesses on why two of the proofs could be one-line reductions to existing results.\n- Beautiful and original idea. Sampling from a product of GFlowNets is natural, but the application to federated learning is creative. \n  - A reference to consider adding is \"Compositional sculpting of iterative generative processes\" [Garipov et al., NeurIPS 2023, to appear], which considers a different kind of modular combination of samplers and could probably be used in a federated setting as well. (I am well aware that its omission is not a weakness as this paper appeared on arXiv on 28 September!)\n- Application to phylogenetic inference is also a new contribution that could be expanded; I wonder how it would scale.\n\nWeaknesses: - Critical missing details for phylogenetic inference. What are the states? What are the actions?\n- It should be noted that an alternative approach to the problem in the Bayesian posterior setting is to train a single GFlowNet with a *stochastic* reward, as done in [Deleu et al., UAI 2022] and [Deleu et al., NeurIPS 2023].\n- The experimental validation is a little thin (esp. given the next point). \n  - This is not a major weakness for me given that there may be no natural baselines, and it is made up for by the good idea and diversity of experiments.\n  - However, there are still comparisons to be made, such as using different training objectives for the client models (TB or CB).\n- The \"contrastive loss\" is claimed as original, but in fact it is not. I suggest that the authors revise the discussion on this in section 3.3 and in the claimed contributions. \n  - It is well known that the expected square difference between two independent samples from a distribution is the same as twice the variance. So the loss in section 3.3 is equivalently optimizing variance of $\\log P_F(\\tau)-\\log R(x)-\\log P_B(\\tau\\mid x)$.\n  - This variance loss is described in \"GFlowNets and VI\" [Malkin et al., ICLR 2023] (\"local baseline\"). It was independently discovered and tested in \"Robust scheduling\" [Zhang et al., ICLR 2023]. Such a gradient variance reduction method is originally proposed in \"VarGrad\" [Richter et al., NeurIPS 2020].\n  - This leads me to wonder whether differences between CL and TB (Figure 6) are only due to insufficiently high learning rate on logZ for TB, or differently tuned learning rates for the policies in both algorithms. \n    - Figure 7, which is hidden in the Appendix, shows that on other domains, CB performs similarly to TB/DB.\n    - It seems a little misleading not to point to Figure 7 in the main text. In my opinion, it would not make the paper weaker to show all four plots in the main text and to say that first steps are made towards understanding when CB is helpful and when it is not.\n  - Related to this, the proofs of all the results in section 3.3 have one-line reductions to existing results (the TB training theorem and the TB gradient analysis theorem). The current proofs are quite obfuscated.\n- The bound in Theorem 2 is nice but may not be very useful in practice. If one of the models is missing a mode, which is quite possible with reverse-KL objectives like those used here, $\\alpha_n$ will be very small, and the final bound is not useful. Therefore, I wonder if one can obtain similar bounds on other divergences that are less sensitive than the Jeffrey divergence to mode-dropping.\n\nI am happy to increase the score from 5 to 6 or even 8 if these are satisfactorily addressed.\n\nQuestions: I like the motivation paragraph on the first page, but what do you see as the main future applications: Privacy-preserving distributed training? Large-scale Bayesian inference where the full reward is expensive to compute on a single client? Modular combination of pretrained GFlowNets?\n\nIn particular, I am not sure to understand the private-rewards application. If each GFlowNet fits (close to) perfectly and can share its policies, then each client's reward density can be recovered as the ratio of its forward and backward policies.", "labeling_timestamp": "2026-01-11T16:55:32.195204", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper (Abstract/Introduction) states that the contrastive-balance (CB) loss is, in expectation, equivalent to the variance of an estimator of the log-partition. However, the provided excerpt does not include Section 3.3 or the precise loss formula, so we cannot verify whether the loss in Section 3.3 is mathematically equivalent to the variance of the specific quantity log P_F(τ) - log R(x) - log P_B(τ | x).", "evidence": "“Enforcing the CB induces a loss that, in expectation, is equivalent to the variance of an estimator of the log-partition, whose minimization was first proposed by Zhang et al. (2023a).”", "section": "Introduction / Abstract"}
{"claim": "Prior works (Malkin et al. ICLR 2023, Zhang et al. ICLR 2023, Richter et al. NeurIPS 2020) already described similar variance-reduction losses or VarGrad methods.", "claim_type": "novelty", "paper_id": "VJDFhkwQg6", "paper_title": "Federated contrastive GFlowNets", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "Om8tHTwSEN", "reviewer": "Reviewer_Xj4h", "review_text": "Summary: An algorithm is proposed for training a generative flow network (GFlowNet) to match a product of distributions, each of which is sampled by a \"client\" GFlowNet. A training objective is stated, its correctness is proved, and bounds relating error of clients to that of the centralized model are derived. Experiments are done on federated versions of several existing tasks and on a new domain for GFlowNets (a toy case of Bayesian phylogenetic inference), where it performs well compared to a non-GFlowNet baseline.\n\nStrengths: - Excellent exposition. I have no complaints on the presentation or on the math. I easily grasped the main idea on the first reading.\n  - But see one of the weaknesses on why two of the proofs could be one-line reductions to existing results.\n- Beautiful and original idea. Sampling from a product of GFlowNets is natural, but the application to federated learning is creative. \n  - A reference to consider adding is \"Compositional sculpting of iterative generative processes\" [Garipov et al., NeurIPS 2023, to appear], which considers a different kind of modular combination of samplers and could probably be used in a federated setting as well. (I am well aware that its omission is not a weakness as this paper appeared on arXiv on 28 September!)\n- Application to phylogenetic inference is also a new contribution that could be expanded; I wonder how it would scale.\n\nWeaknesses: - Critical missing details for phylogenetic inference. What are the states? What are the actions?\n- It should be noted that an alternative approach to the problem in the Bayesian posterior setting is to train a single GFlowNet with a *stochastic* reward, as done in [Deleu et al., UAI 2022] and [Deleu et al., NeurIPS 2023].\n- The experimental validation is a little thin (esp. given the next point). \n  - This is not a major weakness for me given that there may be no natural baselines, and it is made up for by the good idea and diversity of experiments.\n  - However, there are still comparisons to be made, such as using different training objectives for the client models (TB or CB).\n- The \"contrastive loss\" is claimed as original, but in fact it is not. I suggest that the authors revise the discussion on this in section 3.3 and in the claimed contributions. \n  - It is well known that the expected square difference between two independent samples from a distribution is the same as twice the variance. So the loss in section 3.3 is equivalently optimizing variance of $\\log P_F(\\tau)-\\log R(x)-\\log P_B(\\tau\\mid x)$.\n  - This variance loss is described in \"GFlowNets and VI\" [Malkin et al., ICLR 2023] (\"local baseline\"). It was independently discovered and tested in \"Robust scheduling\" [Zhang et al., ICLR 2023]. Such a gradient variance reduction method is originally proposed in \"VarGrad\" [Richter et al., NeurIPS 2020].\n  - This leads me to wonder whether differences between CL and TB (Figure 6) are only due to insufficiently high learning rate on logZ for TB, or differently tuned learning rates for the policies in both algorithms. \n    - Figure 7, which is hidden in the Appendix, shows that on other domains, CB performs similarly to TB/DB.\n    - It seems a little misleading not to point to Figure 7 in the main text. In my opinion, it would not make the paper weaker to show all four plots in the main text and to say that first steps are made towards understanding when CB is helpful and when it is not.\n  - Related to this, the proofs of all the results in section 3.3 have one-line reductions to existing results (the TB training theorem and the TB gradient analysis theorem). The current proofs are quite obfuscated.\n- The bound in Theorem 2 is nice but may not be very useful in practice. If one of the models is missing a mode, which is quite possible with reverse-KL objectives like those used here, $\\alpha_n$ will be very small, and the final bound is not useful. Therefore, I wonder if one can obtain similar bounds on other divergences that are less sensitive than the Jeffrey divergence to mode-dropping.\n\nI am happy to increase the score from 5 to 6 or even 8 if these are satisfactorily addressed.\n\nQuestions: I like the motivation paragraph on the first page, but what do you see as the main future applications: Privacy-preserving distributed training? Large-scale Bayesian inference where the full reward is expensive to compute on a single client? Modular combination of pretrained GFlowNets?\n\nIn particular, I am not sure to understand the private-rewards application. If each GFlowNet fits (close to) perfectly and can share its policies, then each client's reward density can be recovered as the ratio of its forward and backward policies.", "labeling_timestamp": "2026-01-11T16:55:41.926456", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly credits Zhang et al. (2023a) with proposing minimization of the variance of an estimator of the log-partition, but it does not attribute similar variance-reduction or VarGrad methods to Malkin et al. or Richter et al. Malkin et al. are cited regarding the trajectory-balance/backward policy but not as proposing variance-reduction losses; Richter et al. is not mentioned in the provided text.", "evidence": "1) \"Enforcing the CB induces a loss that, in expectation, is equivalent to the variance of an estimator of the log-partition, whose minimization was first proposed by Zhang et al. (2023a).\"  (Abstract)\n\n2) \"Minimizing Equation 2 enforces the TB condition: ... This is the most widely used training scheme for GFlowNets. In practice, some works set p_B as a uniform distribution to avoid learning ϕ_B, as suggested by Malkin et al. (2022).\" (Section 2 PRELIMINARIES)", "section": "Abstract; Section 2 PRELIMINARIES"}
{"claim": "Observed differences between the contrastive loss (CL) and TB in Figure 6 may be due to learning rate choices rather than intrinsic algorithmic differences.", "claim_type": "experimental", "paper_id": "VJDFhkwQg6", "paper_title": "Federated contrastive GFlowNets", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "Om8tHTwSEN", "reviewer": "Reviewer_Xj4h", "review_text": "Summary: An algorithm is proposed for training a generative flow network (GFlowNet) to match a product of distributions, each of which is sampled by a \"client\" GFlowNet. A training objective is stated, its correctness is proved, and bounds relating error of clients to that of the centralized model are derived. Experiments are done on federated versions of several existing tasks and on a new domain for GFlowNets (a toy case of Bayesian phylogenetic inference), where it performs well compared to a non-GFlowNet baseline.\n\nStrengths: - Excellent exposition. I have no complaints on the presentation or on the math. I easily grasped the main idea on the first reading.\n  - But see one of the weaknesses on why two of the proofs could be one-line reductions to existing results.\n- Beautiful and original idea. Sampling from a product of GFlowNets is natural, but the application to federated learning is creative. \n  - A reference to consider adding is \"Compositional sculpting of iterative generative processes\" [Garipov et al., NeurIPS 2023, to appear], which considers a different kind of modular combination of samplers and could probably be used in a federated setting as well. (I am well aware that its omission is not a weakness as this paper appeared on arXiv on 28 September!)\n- Application to phylogenetic inference is also a new contribution that could be expanded; I wonder how it would scale.\n\nWeaknesses: - Critical missing details for phylogenetic inference. What are the states? What are the actions?\n- It should be noted that an alternative approach to the problem in the Bayesian posterior setting is to train a single GFlowNet with a *stochastic* reward, as done in [Deleu et al., UAI 2022] and [Deleu et al., NeurIPS 2023].\n- The experimental validation is a little thin (esp. given the next point). \n  - This is not a major weakness for me given that there may be no natural baselines, and it is made up for by the good idea and diversity of experiments.\n  - However, there are still comparisons to be made, such as using different training objectives for the client models (TB or CB).\n- The \"contrastive loss\" is claimed as original, but in fact it is not. I suggest that the authors revise the discussion on this in section 3.3 and in the claimed contributions. \n  - It is well known that the expected square difference between two independent samples from a distribution is the same as twice the variance. So the loss in section 3.3 is equivalently optimizing variance of $\\log P_F(\\tau)-\\log R(x)-\\log P_B(\\tau\\mid x)$.\n  - This variance loss is described in \"GFlowNets and VI\" [Malkin et al., ICLR 2023] (\"local baseline\"). It was independently discovered and tested in \"Robust scheduling\" [Zhang et al., ICLR 2023]. Such a gradient variance reduction method is originally proposed in \"VarGrad\" [Richter et al., NeurIPS 2020].\n  - This leads me to wonder whether differences between CL and TB (Figure 6) are only due to insufficiently high learning rate on logZ for TB, or differently tuned learning rates for the policies in both algorithms. \n    - Figure 7, which is hidden in the Appendix, shows that on other domains, CB performs similarly to TB/DB.\n    - It seems a little misleading not to point to Figure 7 in the main text. In my opinion, it would not make the paper weaker to show all four plots in the main text and to say that first steps are made towards understanding when CB is helpful and when it is not.\n  - Related to this, the proofs of all the results in section 3.3 have one-line reductions to existing results (the TB training theorem and the TB gradient analysis theorem). The current proofs are quite obfuscated.\n- The bound in Theorem 2 is nice but may not be very useful in practice. If one of the models is missing a mode, which is quite possible with reverse-KL objectives like those used here, $\\alpha_n$ will be very small, and the final bound is not useful. Therefore, I wonder if one can obtain similar bounds on other divergences that are less sensitive than the Jeffrey divergence to mode-dropping.\n\nI am happy to increase the score from 5 to 6 or even 8 if these are satisfactorily addressed.\n\nQuestions: I like the motivation paragraph on the first page, but what do you see as the main future applications: Privacy-preserving distributed training? Large-scale Bayesian inference where the full reward is expensive to compute on a single client? Modular combination of pretrained GFlowNets?\n\nIn particular, I am not sure to understand the private-rewards application. If each GFlowNet fits (close to) perfectly and can share its policies, then each client's reward density can be recovered as the ratio of its forward and backward policies.", "labeling_timestamp": "2026-01-11T16:55:34.958670", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper reports that contrastive balance (CL) can accelerate training compared to trajectory/detailed balance (TB) (Abstract, Introduction), but it does not analyze or attribute these observed differences to learning rate choices or provide experiments/ablation on learning rates. Therefore the claim that differences in Figure 6 may be due to learning rate choices is not addressed by the paper.", "evidence": "1) \"Experiments also suggest that, in some cases, enforcing the contrastive balance can accelerate the training of conventional GFlowNets.\" 2) \"ii ) show that, in some cases, using the contrastive balance as a training criterion leads to faster convergence rates compared to using trajectory and detailed balances;\"", "section": "ABSTRACT and INTRODUCTION (Contributions)"}
{"claim": "The authors do not reference Figure 7 in the main text, which shows CB performing similarly to TB/DB on other domains.", "claim_type": "experimental", "paper_id": "VJDFhkwQg6", "paper_title": "Federated contrastive GFlowNets", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "Om8tHTwSEN", "reviewer": "Reviewer_Xj4h", "review_text": "Summary: An algorithm is proposed for training a generative flow network (GFlowNet) to match a product of distributions, each of which is sampled by a \"client\" GFlowNet. A training objective is stated, its correctness is proved, and bounds relating error of clients to that of the centralized model are derived. Experiments are done on federated versions of several existing tasks and on a new domain for GFlowNets (a toy case of Bayesian phylogenetic inference), where it performs well compared to a non-GFlowNet baseline.\n\nStrengths: - Excellent exposition. I have no complaints on the presentation or on the math. I easily grasped the main idea on the first reading.\n  - But see one of the weaknesses on why two of the proofs could be one-line reductions to existing results.\n- Beautiful and original idea. Sampling from a product of GFlowNets is natural, but the application to federated learning is creative. \n  - A reference to consider adding is \"Compositional sculpting of iterative generative processes\" [Garipov et al., NeurIPS 2023, to appear], which considers a different kind of modular combination of samplers and could probably be used in a federated setting as well. (I am well aware that its omission is not a weakness as this paper appeared on arXiv on 28 September!)\n- Application to phylogenetic inference is also a new contribution that could be expanded; I wonder how it would scale.\n\nWeaknesses: - Critical missing details for phylogenetic inference. What are the states? What are the actions?\n- It should be noted that an alternative approach to the problem in the Bayesian posterior setting is to train a single GFlowNet with a *stochastic* reward, as done in [Deleu et al., UAI 2022] and [Deleu et al., NeurIPS 2023].\n- The experimental validation is a little thin (esp. given the next point). \n  - This is not a major weakness for me given that there may be no natural baselines, and it is made up for by the good idea and diversity of experiments.\n  - However, there are still comparisons to be made, such as using different training objectives for the client models (TB or CB).\n- The \"contrastive loss\" is claimed as original, but in fact it is not. I suggest that the authors revise the discussion on this in section 3.3 and in the claimed contributions. \n  - It is well known that the expected square difference between two independent samples from a distribution is the same as twice the variance. So the loss in section 3.3 is equivalently optimizing variance of $\\log P_F(\\tau)-\\log R(x)-\\log P_B(\\tau\\mid x)$.\n  - This variance loss is described in \"GFlowNets and VI\" [Malkin et al., ICLR 2023] (\"local baseline\"). It was independently discovered and tested in \"Robust scheduling\" [Zhang et al., ICLR 2023]. Such a gradient variance reduction method is originally proposed in \"VarGrad\" [Richter et al., NeurIPS 2020].\n  - This leads me to wonder whether differences between CL and TB (Figure 6) are only due to insufficiently high learning rate on logZ for TB, or differently tuned learning rates for the policies in both algorithms. \n    - Figure 7, which is hidden in the Appendix, shows that on other domains, CB performs similarly to TB/DB.\n    - It seems a little misleading not to point to Figure 7 in the main text. In my opinion, it would not make the paper weaker to show all four plots in the main text and to say that first steps are made towards understanding when CB is helpful and when it is not.\n  - Related to this, the proofs of all the results in section 3.3 have one-line reductions to existing results (the TB training theorem and the TB gradient analysis theorem). The current proofs are quite obfuscated.\n- The bound in Theorem 2 is nice but may not be very useful in practice. If one of the models is missing a mode, which is quite possible with reverse-KL objectives like those used here, $\\alpha_n$ will be very small, and the final bound is not useful. Therefore, I wonder if one can obtain similar bounds on other divergences that are less sensitive than the Jeffrey divergence to mode-dropping.\n\nI am happy to increase the score from 5 to 6 or even 8 if these are satisfactorily addressed.\n\nQuestions: I like the motivation paragraph on the first page, but what do you see as the main future applications: Privacy-preserving distributed training? Large-scale Bayesian inference where the full reward is expensive to compute on a single client? Modular combination of pretrained GFlowNets?\n\nIn particular, I am not sure to understand the private-rewards application. If each GFlowNet fits (close to) perfectly and can share its policies, then each client's reward density can be recovered as the ratio of its forward and backward policies.", "labeling_timestamp": "2026-01-11T16:55:50.490896", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "In the provided main-text content there is no mention or citation of 'Figure 7', so the reviewer's first assertion (that the authors do not reference Figure 7 in the main text) is supported by the paper excerpt. However, the paper content given does not include Figure 7 itself, its caption, or any detailed description of its results, so the second assertion (that Figure 7 shows CB performing similarly to TB/DB on other domains) cannot be verified from the provided material.", "evidence": "We substantiate our methodological contributions with experiments on four different tasks. Notably, our empirical results i ) demonstrate the accuracy of our federated framework for GFlowNets; ii ) show that, in some cases, using the contrastive balance as a training criterion leads to faster convergence rates compared to using trajectory and detailed balances; iii) illustrate the potential of GFlowNets in a novel application: Bayesian phylogenetic inference.", "section": "Abstract / Introduction (no occurrences of 'Figure 7' in provided main text)"}
{"claim": "The proofs in section 3.3 appear to reduce in one line to existing TB training and TB gradient analysis theorems, making the current proofs redundant.", "claim_type": "novelty", "paper_id": "VJDFhkwQg6", "paper_title": "Federated contrastive GFlowNets", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "Om8tHTwSEN", "reviewer": "Reviewer_Xj4h", "review_text": "Summary: An algorithm is proposed for training a generative flow network (GFlowNet) to match a product of distributions, each of which is sampled by a \"client\" GFlowNet. A training objective is stated, its correctness is proved, and bounds relating error of clients to that of the centralized model are derived. Experiments are done on federated versions of several existing tasks and on a new domain for GFlowNets (a toy case of Bayesian phylogenetic inference), where it performs well compared to a non-GFlowNet baseline.\n\nStrengths: - Excellent exposition. I have no complaints on the presentation or on the math. I easily grasped the main idea on the first reading.\n  - But see one of the weaknesses on why two of the proofs could be one-line reductions to existing results.\n- Beautiful and original idea. Sampling from a product of GFlowNets is natural, but the application to federated learning is creative. \n  - A reference to consider adding is \"Compositional sculpting of iterative generative processes\" [Garipov et al., NeurIPS 2023, to appear], which considers a different kind of modular combination of samplers and could probably be used in a federated setting as well. (I am well aware that its omission is not a weakness as this paper appeared on arXiv on 28 September!)\n- Application to phylogenetic inference is also a new contribution that could be expanded; I wonder how it would scale.\n\nWeaknesses: - Critical missing details for phylogenetic inference. What are the states? What are the actions?\n- It should be noted that an alternative approach to the problem in the Bayesian posterior setting is to train a single GFlowNet with a *stochastic* reward, as done in [Deleu et al., UAI 2022] and [Deleu et al., NeurIPS 2023].\n- The experimental validation is a little thin (esp. given the next point). \n  - This is not a major weakness for me given that there may be no natural baselines, and it is made up for by the good idea and diversity of experiments.\n  - However, there are still comparisons to be made, such as using different training objectives for the client models (TB or CB).\n- The \"contrastive loss\" is claimed as original, but in fact it is not. I suggest that the authors revise the discussion on this in section 3.3 and in the claimed contributions. \n  - It is well known that the expected square difference between two independent samples from a distribution is the same as twice the variance. So the loss in section 3.3 is equivalently optimizing variance of $\\log P_F(\\tau)-\\log R(x)-\\log P_B(\\tau\\mid x)$.\n  - This variance loss is described in \"GFlowNets and VI\" [Malkin et al., ICLR 2023] (\"local baseline\"). It was independently discovered and tested in \"Robust scheduling\" [Zhang et al., ICLR 2023]. Such a gradient variance reduction method is originally proposed in \"VarGrad\" [Richter et al., NeurIPS 2020].\n  - This leads me to wonder whether differences between CL and TB (Figure 6) are only due to insufficiently high learning rate on logZ for TB, or differently tuned learning rates for the policies in both algorithms. \n    - Figure 7, which is hidden in the Appendix, shows that on other domains, CB performs similarly to TB/DB.\n    - It seems a little misleading not to point to Figure 7 in the main text. In my opinion, it would not make the paper weaker to show all four plots in the main text and to say that first steps are made towards understanding when CB is helpful and when it is not.\n  - Related to this, the proofs of all the results in section 3.3 have one-line reductions to existing results (the TB training theorem and the TB gradient analysis theorem). The current proofs are quite obfuscated.\n- The bound in Theorem 2 is nice but may not be very useful in practice. If one of the models is missing a mode, which is quite possible with reverse-KL objectives like those used here, $\\alpha_n$ will be very small, and the final bound is not useful. Therefore, I wonder if one can obtain similar bounds on other divergences that are less sensitive than the Jeffrey divergence to mode-dropping.\n\nI am happy to increase the score from 5 to 6 or even 8 if these are satisfactorily addressed.\n\nQuestions: I like the motivation paragraph on the first page, but what do you see as the main future applications: Privacy-preserving distributed training? Large-scale Bayesian inference where the full reward is expensive to compute on a single client? Modular combination of pretrained GFlowNets?\n\nIn particular, I am not sure to understand the private-rewards application. If each GFlowNet fits (close to) perfectly and can share its policies, then each client's reward density can be recovered as the ratio of its forward and backward policies.", "labeling_timestamp": "2026-01-11T16:55:54.498221", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper text describes the trajectory balance (TB) objective in Section 2 and introduces the contrastive and federated balance conditions in Section 3.1, but the content of Section 3.3 (and the proofs the reviewer refers to) is not included. Therefore there is insufficient information to verify whether those proofs reduce to existing TB training/gradient results or are redundant.", "evidence": "\"Minimizing Equation 2 enforces the TB condition: p_F(τ;ϕ_F) = Z^{-1}_{ϕ_Z} R(x) ∏ p_B(s',s;ϕ_B), which implies Equation 1 if valid for all τ ∈ T.\" (Preliminaries, Section 2)\n\n\"This section derives a provably correct framework for federated GFlowNets (Section 3.1). Towards this end, we introduce the contrastive balance condition, a new balance condition ... Additionally, Section 3.2 further explores this condition as a general objective for training conventional GFlowNets...\" (Section 3 introduction)\n\n\"Theorem 1 (Federated balance condition)...\" (Section 3.1)", "section": "Preliminaries (Section 2) and Section 3.1"}
{"claim": "The current proofs in section 3.3 are obfuscated and should be simplified to their one-line reductions to existing results.", "claim_type": "presentation", "paper_id": "VJDFhkwQg6", "paper_title": "Federated contrastive GFlowNets", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "Om8tHTwSEN", "reviewer": "Reviewer_Xj4h", "review_text": "Summary: An algorithm is proposed for training a generative flow network (GFlowNet) to match a product of distributions, each of which is sampled by a \"client\" GFlowNet. A training objective is stated, its correctness is proved, and bounds relating error of clients to that of the centralized model are derived. Experiments are done on federated versions of several existing tasks and on a new domain for GFlowNets (a toy case of Bayesian phylogenetic inference), where it performs well compared to a non-GFlowNet baseline.\n\nStrengths: - Excellent exposition. I have no complaints on the presentation or on the math. I easily grasped the main idea on the first reading.\n  - But see one of the weaknesses on why two of the proofs could be one-line reductions to existing results.\n- Beautiful and original idea. Sampling from a product of GFlowNets is natural, but the application to federated learning is creative. \n  - A reference to consider adding is \"Compositional sculpting of iterative generative processes\" [Garipov et al., NeurIPS 2023, to appear], which considers a different kind of modular combination of samplers and could probably be used in a federated setting as well. (I am well aware that its omission is not a weakness as this paper appeared on arXiv on 28 September!)\n- Application to phylogenetic inference is also a new contribution that could be expanded; I wonder how it would scale.\n\nWeaknesses: - Critical missing details for phylogenetic inference. What are the states? What are the actions?\n- It should be noted that an alternative approach to the problem in the Bayesian posterior setting is to train a single GFlowNet with a *stochastic* reward, as done in [Deleu et al., UAI 2022] and [Deleu et al., NeurIPS 2023].\n- The experimental validation is a little thin (esp. given the next point). \n  - This is not a major weakness for me given that there may be no natural baselines, and it is made up for by the good idea and diversity of experiments.\n  - However, there are still comparisons to be made, such as using different training objectives for the client models (TB or CB).\n- The \"contrastive loss\" is claimed as original, but in fact it is not. I suggest that the authors revise the discussion on this in section 3.3 and in the claimed contributions. \n  - It is well known that the expected square difference between two independent samples from a distribution is the same as twice the variance. So the loss in section 3.3 is equivalently optimizing variance of $\\log P_F(\\tau)-\\log R(x)-\\log P_B(\\tau\\mid x)$.\n  - This variance loss is described in \"GFlowNets and VI\" [Malkin et al., ICLR 2023] (\"local baseline\"). It was independently discovered and tested in \"Robust scheduling\" [Zhang et al., ICLR 2023]. Such a gradient variance reduction method is originally proposed in \"VarGrad\" [Richter et al., NeurIPS 2020].\n  - This leads me to wonder whether differences between CL and TB (Figure 6) are only due to insufficiently high learning rate on logZ for TB, or differently tuned learning rates for the policies in both algorithms. \n    - Figure 7, which is hidden in the Appendix, shows that on other domains, CB performs similarly to TB/DB.\n    - It seems a little misleading not to point to Figure 7 in the main text. In my opinion, it would not make the paper weaker to show all four plots in the main text and to say that first steps are made towards understanding when CB is helpful and when it is not.\n  - Related to this, the proofs of all the results in section 3.3 have one-line reductions to existing results (the TB training theorem and the TB gradient analysis theorem). The current proofs are quite obfuscated.\n- The bound in Theorem 2 is nice but may not be very useful in practice. If one of the models is missing a mode, which is quite possible with reverse-KL objectives like those used here, $\\alpha_n$ will be very small, and the final bound is not useful. Therefore, I wonder if one can obtain similar bounds on other divergences that are less sensitive than the Jeffrey divergence to mode-dropping.\n\nI am happy to increase the score from 5 to 6 or even 8 if these are satisfactorily addressed.\n\nQuestions: I like the motivation paragraph on the first page, but what do you see as the main future applications: Privacy-preserving distributed training? Large-scale Bayesian inference where the full reward is expensive to compute on a single client? Modular combination of pretrained GFlowNets?\n\nIn particular, I am not sure to understand the private-rewards application. If each GFlowNet fits (close to) perfectly and can share its policies, then each client's reward density can be recovered as the ratio of its forward and backward policies.", "labeling_timestamp": "2026-01-11T16:56:04.463855", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not include Section 3.3 or its proofs, only Sections 3.1 and a mention of 3.2. Therefore there is insufficient information in the paper content given to evaluate the reviewer's claim about obfuscation or possible one-line reductions.", "evidence": "This section derives a provably correct framework for federated GFlowNets (Section 3.1). Towards this end, we introduce the contrastive balance condition , a new balance condition that requires mini-\n\nAdditionally, Section 3.2 further explores this condition as a general objective for training conventional GFlowNets, which stems naturally from our theoretical developments.", "section": "Section 3 (Method) / 3.1"}
{"claim": "The bound in Theorem 2 may be practically unhelpful when models drop modes under reverse-KL objectives because the parameter alpha_n can become very small.", "claim_type": "methodology", "paper_id": "VJDFhkwQg6", "paper_title": "Federated contrastive GFlowNets", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "Om8tHTwSEN", "reviewer": "Reviewer_Xj4h", "review_text": "Summary: An algorithm is proposed for training a generative flow network (GFlowNet) to match a product of distributions, each of which is sampled by a \"client\" GFlowNet. A training objective is stated, its correctness is proved, and bounds relating error of clients to that of the centralized model are derived. Experiments are done on federated versions of several existing tasks and on a new domain for GFlowNets (a toy case of Bayesian phylogenetic inference), where it performs well compared to a non-GFlowNet baseline.\n\nStrengths: - Excellent exposition. I have no complaints on the presentation or on the math. I easily grasped the main idea on the first reading.\n  - But see one of the weaknesses on why two of the proofs could be one-line reductions to existing results.\n- Beautiful and original idea. Sampling from a product of GFlowNets is natural, but the application to federated learning is creative. \n  - A reference to consider adding is \"Compositional sculpting of iterative generative processes\" [Garipov et al., NeurIPS 2023, to appear], which considers a different kind of modular combination of samplers and could probably be used in a federated setting as well. (I am well aware that its omission is not a weakness as this paper appeared on arXiv on 28 September!)\n- Application to phylogenetic inference is also a new contribution that could be expanded; I wonder how it would scale.\n\nWeaknesses: - Critical missing details for phylogenetic inference. What are the states? What are the actions?\n- It should be noted that an alternative approach to the problem in the Bayesian posterior setting is to train a single GFlowNet with a *stochastic* reward, as done in [Deleu et al., UAI 2022] and [Deleu et al., NeurIPS 2023].\n- The experimental validation is a little thin (esp. given the next point). \n  - This is not a major weakness for me given that there may be no natural baselines, and it is made up for by the good idea and diversity of experiments.\n  - However, there are still comparisons to be made, such as using different training objectives for the client models (TB or CB).\n- The \"contrastive loss\" is claimed as original, but in fact it is not. I suggest that the authors revise the discussion on this in section 3.3 and in the claimed contributions. \n  - It is well known that the expected square difference between two independent samples from a distribution is the same as twice the variance. So the loss in section 3.3 is equivalently optimizing variance of $\\log P_F(\\tau)-\\log R(x)-\\log P_B(\\tau\\mid x)$.\n  - This variance loss is described in \"GFlowNets and VI\" [Malkin et al., ICLR 2023] (\"local baseline\"). It was independently discovered and tested in \"Robust scheduling\" [Zhang et al., ICLR 2023]. Such a gradient variance reduction method is originally proposed in \"VarGrad\" [Richter et al., NeurIPS 2020].\n  - This leads me to wonder whether differences between CL and TB (Figure 6) are only due to insufficiently high learning rate on logZ for TB, or differently tuned learning rates for the policies in both algorithms. \n    - Figure 7, which is hidden in the Appendix, shows that on other domains, CB performs similarly to TB/DB.\n    - It seems a little misleading not to point to Figure 7 in the main text. In my opinion, it would not make the paper weaker to show all four plots in the main text and to say that first steps are made towards understanding when CB is helpful and when it is not.\n  - Related to this, the proofs of all the results in section 3.3 have one-line reductions to existing results (the TB training theorem and the TB gradient analysis theorem). The current proofs are quite obfuscated.\n- The bound in Theorem 2 is nice but may not be very useful in practice. If one of the models is missing a mode, which is quite possible with reverse-KL objectives like those used here, $\\alpha_n$ will be very small, and the final bound is not useful. Therefore, I wonder if one can obtain similar bounds on other divergences that are less sensitive than the Jeffrey divergence to mode-dropping.\n\nI am happy to increase the score from 5 to 6 or even 8 if these are satisfactorily addressed.\n\nQuestions: I like the motivation paragraph on the first page, but what do you see as the main future applications: Privacy-preserving distributed training? Large-scale Bayesian inference where the full reward is expensive to compute on a single client? Modular combination of pretrained GFlowNets?\n\nIn particular, I am not sure to understand the private-rewards application. If each GFlowNet fits (close to) perfectly and can share its policies, then each client's reward density can be recovered as the ratio of its forward and backward policies.", "labeling_timestamp": "2026-01-11T16:56:04.223109", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not contain Theorem 2, any bound involving a parameter alpha_n, nor discussion of reverse-KL objectives or mode dropping. Therefore the claim cannot be verified from the available content.", "evidence": "“Theorem 1 (Federated balance condition) . Let ( p (1) F , p (1) F ) , . . . , ( p ( N ) F , p ( N ) F ) : V 2 → R + be pairs of forward and backward policies from N GFlowNets sampling respectively proportionally to R 1 , . . . , R N : X → R + . Then, another GFlowNet with forward and backward policies p F , p B ∈ V 2 → R + samples proportionally to R ( x ) := ∏ N n =1 R ( x ) if and only if the following condition holds for”", "section": "3.1 FEDERATED GFLOWNETS"}
{"claim": "The paper does not derive similar error bounds for divergences that are less sensitive to mode-dropping than the Jeffrey divergence.", "claim_type": "methodology", "paper_id": "VJDFhkwQg6", "paper_title": "Federated contrastive GFlowNets", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "Om8tHTwSEN", "reviewer": "Reviewer_Xj4h", "review_text": "Summary: An algorithm is proposed for training a generative flow network (GFlowNet) to match a product of distributions, each of which is sampled by a \"client\" GFlowNet. A training objective is stated, its correctness is proved, and bounds relating error of clients to that of the centralized model are derived. Experiments are done on federated versions of several existing tasks and on a new domain for GFlowNets (a toy case of Bayesian phylogenetic inference), where it performs well compared to a non-GFlowNet baseline.\n\nStrengths: - Excellent exposition. I have no complaints on the presentation or on the math. I easily grasped the main idea on the first reading.\n  - But see one of the weaknesses on why two of the proofs could be one-line reductions to existing results.\n- Beautiful and original idea. Sampling from a product of GFlowNets is natural, but the application to federated learning is creative. \n  - A reference to consider adding is \"Compositional sculpting of iterative generative processes\" [Garipov et al., NeurIPS 2023, to appear], which considers a different kind of modular combination of samplers and could probably be used in a federated setting as well. (I am well aware that its omission is not a weakness as this paper appeared on arXiv on 28 September!)\n- Application to phylogenetic inference is also a new contribution that could be expanded; I wonder how it would scale.\n\nWeaknesses: - Critical missing details for phylogenetic inference. What are the states? What are the actions?\n- It should be noted that an alternative approach to the problem in the Bayesian posterior setting is to train a single GFlowNet with a *stochastic* reward, as done in [Deleu et al., UAI 2022] and [Deleu et al., NeurIPS 2023].\n- The experimental validation is a little thin (esp. given the next point). \n  - This is not a major weakness for me given that there may be no natural baselines, and it is made up for by the good idea and diversity of experiments.\n  - However, there are still comparisons to be made, such as using different training objectives for the client models (TB or CB).\n- The \"contrastive loss\" is claimed as original, but in fact it is not. I suggest that the authors revise the discussion on this in section 3.3 and in the claimed contributions. \n  - It is well known that the expected square difference between two independent samples from a distribution is the same as twice the variance. So the loss in section 3.3 is equivalently optimizing variance of $\\log P_F(\\tau)-\\log R(x)-\\log P_B(\\tau\\mid x)$.\n  - This variance loss is described in \"GFlowNets and VI\" [Malkin et al., ICLR 2023] (\"local baseline\"). It was independently discovered and tested in \"Robust scheduling\" [Zhang et al., ICLR 2023]. Such a gradient variance reduction method is originally proposed in \"VarGrad\" [Richter et al., NeurIPS 2020].\n  - This leads me to wonder whether differences between CL and TB (Figure 6) are only due to insufficiently high learning rate on logZ for TB, or differently tuned learning rates for the policies in both algorithms. \n    - Figure 7, which is hidden in the Appendix, shows that on other domains, CB performs similarly to TB/DB.\n    - It seems a little misleading not to point to Figure 7 in the main text. In my opinion, it would not make the paper weaker to show all four plots in the main text and to say that first steps are made towards understanding when CB is helpful and when it is not.\n  - Related to this, the proofs of all the results in section 3.3 have one-line reductions to existing results (the TB training theorem and the TB gradient analysis theorem). The current proofs are quite obfuscated.\n- The bound in Theorem 2 is nice but may not be very useful in practice. If one of the models is missing a mode, which is quite possible with reverse-KL objectives like those used here, $\\alpha_n$ will be very small, and the final bound is not useful. Therefore, I wonder if one can obtain similar bounds on other divergences that are less sensitive than the Jeffrey divergence to mode-dropping.\n\nI am happy to increase the score from 5 to 6 or even 8 if these are satisfactorily addressed.\n\nQuestions: I like the motivation paragraph on the first page, but what do you see as the main future applications: Privacy-preserving distributed training? Large-scale Bayesian inference where the full reward is expensive to compute on a single client? Modular combination of pretrained GFlowNets?\n\nIn particular, I am not sure to understand the private-rewards application. If each GFlowNet fits (close to) perfectly and can share its policies, then each client's reward density can be recovered as the ratio of its forward and backward policies.", "labeling_timestamp": "2026-01-11T16:56:06.082967", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper's provided content discusses theoretical guarantees (federated balance, contrastive balance) and robustness to estimation errors but contains no derivation or mention of error bounds for divergences (nor any reference to the Jeffrey divergence or alternative divergences). Thus the reviewer's claim that such bounds are not derived is supported by the paper text.", "evidence": "\"we provide a theory that guarantees its correctness, and also analyze its robustness to errors in the estimation of local GFlowNets;\" (Contributions) \n\n\"Additionally, our theoretical analysis builds on the idea of contrastive balance , that imposes necessary and sufficient conditions for the correctness of general (nonfederated) GFlowNets.\" (Abstract)\n\nNo occurrences of \"Jeffrey\" or derivations of error bounds for other divergences appear in the provided content.", "section": "Abstract / Contributions / Introduction"}
{"claim": "The authors omitted showing all four relevant plots in the main text, which would better clarify when CB is helpful versus when it is not.", "claim_type": "presentation", "paper_id": "VJDFhkwQg6", "paper_title": "Federated contrastive GFlowNets", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "Om8tHTwSEN", "reviewer": "Reviewer_Xj4h", "review_text": "Summary: An algorithm is proposed for training a generative flow network (GFlowNet) to match a product of distributions, each of which is sampled by a \"client\" GFlowNet. A training objective is stated, its correctness is proved, and bounds relating error of clients to that of the centralized model are derived. Experiments are done on federated versions of several existing tasks and on a new domain for GFlowNets (a toy case of Bayesian phylogenetic inference), where it performs well compared to a non-GFlowNet baseline.\n\nStrengths: - Excellent exposition. I have no complaints on the presentation or on the math. I easily grasped the main idea on the first reading.\n  - But see one of the weaknesses on why two of the proofs could be one-line reductions to existing results.\n- Beautiful and original idea. Sampling from a product of GFlowNets is natural, but the application to federated learning is creative. \n  - A reference to consider adding is \"Compositional sculpting of iterative generative processes\" [Garipov et al., NeurIPS 2023, to appear], which considers a different kind of modular combination of samplers and could probably be used in a federated setting as well. (I am well aware that its omission is not a weakness as this paper appeared on arXiv on 28 September!)\n- Application to phylogenetic inference is also a new contribution that could be expanded; I wonder how it would scale.\n\nWeaknesses: - Critical missing details for phylogenetic inference. What are the states? What are the actions?\n- It should be noted that an alternative approach to the problem in the Bayesian posterior setting is to train a single GFlowNet with a *stochastic* reward, as done in [Deleu et al., UAI 2022] and [Deleu et al., NeurIPS 2023].\n- The experimental validation is a little thin (esp. given the next point). \n  - This is not a major weakness for me given that there may be no natural baselines, and it is made up for by the good idea and diversity of experiments.\n  - However, there are still comparisons to be made, such as using different training objectives for the client models (TB or CB).\n- The \"contrastive loss\" is claimed as original, but in fact it is not. I suggest that the authors revise the discussion on this in section 3.3 and in the claimed contributions. \n  - It is well known that the expected square difference between two independent samples from a distribution is the same as twice the variance. So the loss in section 3.3 is equivalently optimizing variance of $\\log P_F(\\tau)-\\log R(x)-\\log P_B(\\tau\\mid x)$.\n  - This variance loss is described in \"GFlowNets and VI\" [Malkin et al., ICLR 2023] (\"local baseline\"). It was independently discovered and tested in \"Robust scheduling\" [Zhang et al., ICLR 2023]. Such a gradient variance reduction method is originally proposed in \"VarGrad\" [Richter et al., NeurIPS 2020].\n  - This leads me to wonder whether differences between CL and TB (Figure 6) are only due to insufficiently high learning rate on logZ for TB, or differently tuned learning rates for the policies in both algorithms. \n    - Figure 7, which is hidden in the Appendix, shows that on other domains, CB performs similarly to TB/DB.\n    - It seems a little misleading not to point to Figure 7 in the main text. In my opinion, it would not make the paper weaker to show all four plots in the main text and to say that first steps are made towards understanding when CB is helpful and when it is not.\n  - Related to this, the proofs of all the results in section 3.3 have one-line reductions to existing results (the TB training theorem and the TB gradient analysis theorem). The current proofs are quite obfuscated.\n- The bound in Theorem 2 is nice but may not be very useful in practice. If one of the models is missing a mode, which is quite possible with reverse-KL objectives like those used here, $\\alpha_n$ will be very small, and the final bound is not useful. Therefore, I wonder if one can obtain similar bounds on other divergences that are less sensitive than the Jeffrey divergence to mode-dropping.\n\nI am happy to increase the score from 5 to 6 or even 8 if these are satisfactorily addressed.\n\nQuestions: I like the motivation paragraph on the first page, but what do you see as the main future applications: Privacy-preserving distributed training? Large-scale Bayesian inference where the full reward is expensive to compute on a single client? Modular combination of pretrained GFlowNets?\n\nIn particular, I am not sure to understand the private-rewards application. If each GFlowNet fits (close to) perfectly and can share its policies, then each client's reward density can be recovered as the ratio of its forward and backward policies.", "labeling_timestamp": "2026-01-11T16:56:17.563901", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt states that experiments were performed in four settings and that contrastive balance (CB) can sometimes accelerate training, but the excerpt does not include the figures or indicate which plots are placed in the main text versus appendix. Therefore there is insufficient information to verify the reviewer's claim that 'all four relevant plots' were omitted from the main text.", "evidence": "“We empirically attest the performance of FC-GFlowNets in four settings, including grid-world, sequence, multiset generation, and Bayesian phylogenetic inference. Experiments also suggest that, in some cases, enforcing the contrastive balance can accelerate the training of conventional GFlowNets.”", "section": "ABSTRACT / Summary of contributions (Introduction)"}
{"claim": "The private-rewards application is unclear because sharing policies could allow recovering each client's reward density as the ratio of forward and backward policies.", "claim_type": "methodology", "paper_id": "VJDFhkwQg6", "paper_title": "Federated contrastive GFlowNets", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "Om8tHTwSEN", "reviewer": "Reviewer_Xj4h", "review_text": "Summary: An algorithm is proposed for training a generative flow network (GFlowNet) to match a product of distributions, each of which is sampled by a \"client\" GFlowNet. A training objective is stated, its correctness is proved, and bounds relating error of clients to that of the centralized model are derived. Experiments are done on federated versions of several existing tasks and on a new domain for GFlowNets (a toy case of Bayesian phylogenetic inference), where it performs well compared to a non-GFlowNet baseline.\n\nStrengths: - Excellent exposition. I have no complaints on the presentation or on the math. I easily grasped the main idea on the first reading.\n  - But see one of the weaknesses on why two of the proofs could be one-line reductions to existing results.\n- Beautiful and original idea. Sampling from a product of GFlowNets is natural, but the application to federated learning is creative. \n  - A reference to consider adding is \"Compositional sculpting of iterative generative processes\" [Garipov et al., NeurIPS 2023, to appear], which considers a different kind of modular combination of samplers and could probably be used in a federated setting as well. (I am well aware that its omission is not a weakness as this paper appeared on arXiv on 28 September!)\n- Application to phylogenetic inference is also a new contribution that could be expanded; I wonder how it would scale.\n\nWeaknesses: - Critical missing details for phylogenetic inference. What are the states? What are the actions?\n- It should be noted that an alternative approach to the problem in the Bayesian posterior setting is to train a single GFlowNet with a *stochastic* reward, as done in [Deleu et al., UAI 2022] and [Deleu et al., NeurIPS 2023].\n- The experimental validation is a little thin (esp. given the next point). \n  - This is not a major weakness for me given that there may be no natural baselines, and it is made up for by the good idea and diversity of experiments.\n  - However, there are still comparisons to be made, such as using different training objectives for the client models (TB or CB).\n- The \"contrastive loss\" is claimed as original, but in fact it is not. I suggest that the authors revise the discussion on this in section 3.3 and in the claimed contributions. \n  - It is well known that the expected square difference between two independent samples from a distribution is the same as twice the variance. So the loss in section 3.3 is equivalently optimizing variance of $\\log P_F(\\tau)-\\log R(x)-\\log P_B(\\tau\\mid x)$.\n  - This variance loss is described in \"GFlowNets and VI\" [Malkin et al., ICLR 2023] (\"local baseline\"). It was independently discovered and tested in \"Robust scheduling\" [Zhang et al., ICLR 2023]. Such a gradient variance reduction method is originally proposed in \"VarGrad\" [Richter et al., NeurIPS 2020].\n  - This leads me to wonder whether differences between CL and TB (Figure 6) are only due to insufficiently high learning rate on logZ for TB, or differently tuned learning rates for the policies in both algorithms. \n    - Figure 7, which is hidden in the Appendix, shows that on other domains, CB performs similarly to TB/DB.\n    - It seems a little misleading not to point to Figure 7 in the main text. In my opinion, it would not make the paper weaker to show all four plots in the main text and to say that first steps are made towards understanding when CB is helpful and when it is not.\n  - Related to this, the proofs of all the results in section 3.3 have one-line reductions to existing results (the TB training theorem and the TB gradient analysis theorem). The current proofs are quite obfuscated.\n- The bound in Theorem 2 is nice but may not be very useful in practice. If one of the models is missing a mode, which is quite possible with reverse-KL objectives like those used here, $\\alpha_n$ will be very small, and the final bound is not useful. Therefore, I wonder if one can obtain similar bounds on other divergences that are less sensitive than the Jeffrey divergence to mode-dropping.\n\nI am happy to increase the score from 5 to 6 or even 8 if these are satisfactorily addressed.\n\nQuestions: I like the motivation paragraph on the first page, but what do you see as the main future applications: Privacy-preserving distributed training? Large-scale Bayesian inference where the full reward is expensive to compute on a single client? Modular combination of pretrained GFlowNets?\n\nIn particular, I am not sure to understand the private-rewards application. If each GFlowNet fits (close to) perfectly and can share its policies, then each client's reward density can be recovered as the ratio of its forward and backward policies.", "labeling_timestamp": "2026-01-11T16:56:30.591107", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper states the trajectory balance (TB) relation that expresses p_F(τ) in terms of R(x), Z, and the product of backward policies, so given a client's forward and backward policies one can recover R_n(x) up to the partition constant. The paper also explicitly describes that clients send their forward/backward policies to the server, implying that sharing these policies could reveal information about the local rewards.", "evidence": "Minimizing Equation 2 enforces the TB condition: p_F(τ; ϕ_F) = Z_ϕZ^{-1} R(x) ∏ p_B(s', s; ϕ_B), which implies Equation 1 if valid for all τ ∈ T. \n\nFirst, each user n trains independently a GFlowNet with forward/backward policies p^(n)_F and p^(n)_B to sample from R_n. Then, the users send their policies to a server for a single aggregation step, in which the server combines the local GFlowNets into a new one...", "section": "2 PRELIMINARIES; 3.1 FEDERATED GFLOWNETS"}
{"claim": "It is unclear how the proposed phylogenetic inference experiments would scale to larger, realistic phylogenetic problems.", "claim_type": "experimental", "paper_id": "VJDFhkwQg6", "paper_title": "Federated contrastive GFlowNets", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "Om8tHTwSEN", "reviewer": "Reviewer_Xj4h", "review_text": "Summary: An algorithm is proposed for training a generative flow network (GFlowNet) to match a product of distributions, each of which is sampled by a \"client\" GFlowNet. A training objective is stated, its correctness is proved, and bounds relating error of clients to that of the centralized model are derived. Experiments are done on federated versions of several existing tasks and on a new domain for GFlowNets (a toy case of Bayesian phylogenetic inference), where it performs well compared to a non-GFlowNet baseline.\n\nStrengths: - Excellent exposition. I have no complaints on the presentation or on the math. I easily grasped the main idea on the first reading.\n  - But see one of the weaknesses on why two of the proofs could be one-line reductions to existing results.\n- Beautiful and original idea. Sampling from a product of GFlowNets is natural, but the application to federated learning is creative. \n  - A reference to consider adding is \"Compositional sculpting of iterative generative processes\" [Garipov et al., NeurIPS 2023, to appear], which considers a different kind of modular combination of samplers and could probably be used in a federated setting as well. (I am well aware that its omission is not a weakness as this paper appeared on arXiv on 28 September!)\n- Application to phylogenetic inference is also a new contribution that could be expanded; I wonder how it would scale.\n\nWeaknesses: - Critical missing details for phylogenetic inference. What are the states? What are the actions?\n- It should be noted that an alternative approach to the problem in the Bayesian posterior setting is to train a single GFlowNet with a *stochastic* reward, as done in [Deleu et al., UAI 2022] and [Deleu et al., NeurIPS 2023].\n- The experimental validation is a little thin (esp. given the next point). \n  - This is not a major weakness for me given that there may be no natural baselines, and it is made up for by the good idea and diversity of experiments.\n  - However, there are still comparisons to be made, such as using different training objectives for the client models (TB or CB).\n- The \"contrastive loss\" is claimed as original, but in fact it is not. I suggest that the authors revise the discussion on this in section 3.3 and in the claimed contributions. \n  - It is well known that the expected square difference between two independent samples from a distribution is the same as twice the variance. So the loss in section 3.3 is equivalently optimizing variance of $\\log P_F(\\tau)-\\log R(x)-\\log P_B(\\tau\\mid x)$.\n  - This variance loss is described in \"GFlowNets and VI\" [Malkin et al., ICLR 2023] (\"local baseline\"). It was independently discovered and tested in \"Robust scheduling\" [Zhang et al., ICLR 2023]. Such a gradient variance reduction method is originally proposed in \"VarGrad\" [Richter et al., NeurIPS 2020].\n  - This leads me to wonder whether differences between CL and TB (Figure 6) are only due to insufficiently high learning rate on logZ for TB, or differently tuned learning rates for the policies in both algorithms. \n    - Figure 7, which is hidden in the Appendix, shows that on other domains, CB performs similarly to TB/DB.\n    - It seems a little misleading not to point to Figure 7 in the main text. In my opinion, it would not make the paper weaker to show all four plots in the main text and to say that first steps are made towards understanding when CB is helpful and when it is not.\n  - Related to this, the proofs of all the results in section 3.3 have one-line reductions to existing results (the TB training theorem and the TB gradient analysis theorem). The current proofs are quite obfuscated.\n- The bound in Theorem 2 is nice but may not be very useful in practice. If one of the models is missing a mode, which is quite possible with reverse-KL objectives like those used here, $\\alpha_n$ will be very small, and the final bound is not useful. Therefore, I wonder if one can obtain similar bounds on other divergences that are less sensitive than the Jeffrey divergence to mode-dropping.\n\nI am happy to increase the score from 5 to 6 or even 8 if these are satisfactorily addressed.\n\nQuestions: I like the motivation paragraph on the first page, but what do you see as the main future applications: Privacy-preserving distributed training? Large-scale Bayesian inference where the full reward is expensive to compute on a single client? Modular combination of pretrained GFlowNets?\n\nIn particular, I am not sure to understand the private-rewards application. If each GFlowNet fits (close to) perfectly and can share its policies, then each client's reward density can be recovered as the ratio of its forward and backward policies.", "labeling_timestamp": "2026-01-11T16:56:42.921965", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper only mentions Bayesian phylogenetic inference as one of the experimental settings (Abstract and Introduction) but the provided content contains no details on dataset sizes, runtime, computational cost, or analysis of scalability to larger, realistic phylogenetic problems. Therefore the reviewer's statement that scalability is unclear is accurate according to the paper.", "evidence": "We empirically attest the performance of FC-GFlowNets in four settings, including grid-world, sequence, multiset generation, and Bayesian phylogenetic inference.", "section": "ABSTRACT"}
{"claim": "The theoretical results assume linear neural networks and Gaussian-distributed weights, which are strong assumptions limiting applicability to realistic neural networks.", "claim_type": "methodology", "paper_id": "h05eQniJsQ", "paper_title": "Understanding Certified Training with Interval Bound Propagation", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "zdpzuBcupz", "reviewer": "Reviewer_C4Wz", "review_text": "Summary: This paper studies the interval bound propagation-based (IBP-based) training method, one of the most popular approaches to obtaining neural networks with certifiable guarantees. Although existing work demonstrated the effectiveness of this approach, the theoretical understanding of IBP is limited. Under the assumption of linear neural networks and Gaussian weights, this paper proposed a new measure of bound tightness and derived a few theorems to show how the bound tightness changes when propagating among linear layers, and how the width and depths of a randomly initialized network impact bound tightness.\n\nSome empirical results on a few MNIST and CIFAR-10 models demonstrate that certified training indeed improves the tightness measure proposed in this paper. Some interesting empirical results were demonstrated with models varying depths and widths, demonstrating their correlations with bound tightness and accuracy.\n\nThe paper studies an important topic with some novel results, however, its current version has some weaknesses and unresolved questions, see below, so I feel the current version of the paper is below the acceptance threshold.\n\nStrengths: 1. The topic of the paper is relevant, and it is an open challenge. We still don’t understand certified training very well, and this paper is a great attempt to bring in some new understanding.\n\n2. Some novel theoretical insights are given, such as on the tightness of bound propagation and propagation invariance. Also, the growth of the bounds under initialization and its relation with model width may be a useful result to guide practical training.\n\n3. The bound propagation invariance condition may lead to new insights into the design of neural network architecture to make the bounds tighter.\n\n4. Some results are extended to a 2-layer ReLU network, although this part was not emphasized in the paper.\n\nWeaknesses: 1. The theoretical results have strong assumptions such as linear neural networks, and neural network weights under Gaussian distribution. This is generally not a big concern if the authors can demonstrate that these theoretical insights can lead to great practical improvements, \n\n2. but here the theoretical results developed do not lead to a better model that can outperform existing approaches, and some evaluations are quite weak (e.g., on a single MNIST model only). Since only a few models and networks are shown, it is unsure how general the results are.\n\n3. Although some empirical results are shown to support the theory for ReLU networks, it is hard to argue these observations are indeed the consequence of the theory. For example, “certified training increases tightness” and “larger networks lead to better accuracy” are very generic conclusions, and it is hard to convince the readers that they result from the theory developed in this work.\n\nQuestions: 1. Figure 6 shows that increasing model width is beneficial, however, it is on a simple MNIST network. Can you demonstrate this result on larger networks and datasets? In particular, if we use a state-of-the-art method and model, and enlarge the model size by 4x, how much gain can we obtain over the state-of-the-art? Is the gain consistent over multiple models (e.g., CNN, resnet), training methods (IBP, CROWN-IBP, SABR, MTL-IBP), and datasets (CIFAR10, CIFAR100, TinyImageNet)?\n\n2. Based on Theorem 3.4, can we reparameterize the network such that the bounds are always tight, and the training process just needs to search from a subspace of weights that lead to tight bounds, rather than using gradient descent to enforce tight bounds? For L2 norm certified defense, the state-of-the-art methods use this approach (such as orthogonal convolution layers and their variants).", "labeling_timestamp": "2026-01-11T16:56:33.412806", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly states that the theoretical analysis focuses on deep linear networks (DLNs), so the claim that the theory assumes linear networks is supported. However, the provided paper content does not state or quote any assumption that weights are Gaussian-distributed; no mention of Gaussian weight distributions appears in the shown sections, so that part of the claim is not supported by the provided text.", "evidence": "Setting: \"We focus our theoretical analysis on deep linear networks (DLNs), i.e., f(x) = Π_{i=1}^L W^{(i)} x, popular for theoretical discussion of neural networks (Saxe et al., 2014; Ji & Telgarsky, 2019; Wu et al., 2019).\"", "section": "3 UNDERSTANDING IBP TRAINING (Setting)"}
{"claim": "The developed theoretical results do not produce a model that outperforms existing approaches, indicating limited practical improvement from the analysis.", "claim_type": "baseline", "paper_id": "h05eQniJsQ", "paper_title": "Understanding Certified Training with Interval Bound Propagation", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "zdpzuBcupz", "reviewer": "Reviewer_C4Wz", "review_text": "Summary: This paper studies the interval bound propagation-based (IBP-based) training method, one of the most popular approaches to obtaining neural networks with certifiable guarantees. Although existing work demonstrated the effectiveness of this approach, the theoretical understanding of IBP is limited. Under the assumption of linear neural networks and Gaussian weights, this paper proposed a new measure of bound tightness and derived a few theorems to show how the bound tightness changes when propagating among linear layers, and how the width and depths of a randomly initialized network impact bound tightness.\n\nSome empirical results on a few MNIST and CIFAR-10 models demonstrate that certified training indeed improves the tightness measure proposed in this paper. Some interesting empirical results were demonstrated with models varying depths and widths, demonstrating their correlations with bound tightness and accuracy.\n\nThe paper studies an important topic with some novel results, however, its current version has some weaknesses and unresolved questions, see below, so I feel the current version of the paper is below the acceptance threshold.\n\nStrengths: 1. The topic of the paper is relevant, and it is an open challenge. We still don’t understand certified training very well, and this paper is a great attempt to bring in some new understanding.\n\n2. Some novel theoretical insights are given, such as on the tightness of bound propagation and propagation invariance. Also, the growth of the bounds under initialization and its relation with model width may be a useful result to guide practical training.\n\n3. The bound propagation invariance condition may lead to new insights into the design of neural network architecture to make the bounds tighter.\n\n4. Some results are extended to a 2-layer ReLU network, although this part was not emphasized in the paper.\n\nWeaknesses: 1. The theoretical results have strong assumptions such as linear neural networks, and neural network weights under Gaussian distribution. This is generally not a big concern if the authors can demonstrate that these theoretical insights can lead to great practical improvements, \n\n2. but here the theoretical results developed do not lead to a better model that can outperform existing approaches, and some evaluations are quite weak (e.g., on a single MNIST model only). Since only a few models and networks are shown, it is unsure how general the results are.\n\n3. Although some empirical results are shown to support the theory for ReLU networks, it is hard to argue these observations are indeed the consequence of the theory. For example, “certified training increases tightness” and “larger networks lead to better accuracy” are very generic conclusions, and it is hard to convince the readers that they result from the theory developed in this work.\n\nQuestions: 1. Figure 6 shows that increasing model width is beneficial, however, it is on a simple MNIST network. Can you demonstrate this result on larger networks and datasets? In particular, if we use a state-of-the-art method and model, and enlarge the model size by 4x, how much gain can we obtain over the state-of-the-art? Is the gain consistent over multiple models (e.g., CNN, resnet), training methods (IBP, CROWN-IBP, SABR, MTL-IBP), and datasets (CIFAR10, CIFAR100, TinyImageNet)?\n\n2. Based on Theorem 3.4, can we reparameterize the network such that the bounds are always tight, and the training process just needs to search from a subspace of weights that lead to tight bounds, rather than using gradient descent to enforce tight bounds? For L2 norm certified defense, the state-of-the-art methods use this approach (such as orthogonal convolution layers and their variants).", "labeling_timestamp": "2026-01-11T16:56:27.403662", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper reports that its empirical study yields state-of-the-art certified accuracy and that increasing width improves certified accuracy, contradicting the reviewer's claim that the results do not produce a model outperforming existing approaches.", "evidence": "Abstract: \"...conducting an extensive empirical study, (i) confirming this transferability and yielding state-of-the-art certified accuracy...\"; Introduction: \"...we confirm the predictiveness of our theoretical results for deep ReLU networks and observe that: (i) increasing network width but not depth improves state-of-the-art certified accuracy...\"", "section": "Abstract; 1 INTRODUCTION"}
{"claim": "The empirical evaluation is weak, relying on only a single MNIST model example for some claims.", "claim_type": "experimental", "paper_id": "h05eQniJsQ", "paper_title": "Understanding Certified Training with Interval Bound Propagation", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "zdpzuBcupz", "reviewer": "Reviewer_C4Wz", "review_text": "Summary: This paper studies the interval bound propagation-based (IBP-based) training method, one of the most popular approaches to obtaining neural networks with certifiable guarantees. Although existing work demonstrated the effectiveness of this approach, the theoretical understanding of IBP is limited. Under the assumption of linear neural networks and Gaussian weights, this paper proposed a new measure of bound tightness and derived a few theorems to show how the bound tightness changes when propagating among linear layers, and how the width and depths of a randomly initialized network impact bound tightness.\n\nSome empirical results on a few MNIST and CIFAR-10 models demonstrate that certified training indeed improves the tightness measure proposed in this paper. Some interesting empirical results were demonstrated with models varying depths and widths, demonstrating their correlations with bound tightness and accuracy.\n\nThe paper studies an important topic with some novel results, however, its current version has some weaknesses and unresolved questions, see below, so I feel the current version of the paper is below the acceptance threshold.\n\nStrengths: 1. The topic of the paper is relevant, and it is an open challenge. We still don’t understand certified training very well, and this paper is a great attempt to bring in some new understanding.\n\n2. Some novel theoretical insights are given, such as on the tightness of bound propagation and propagation invariance. Also, the growth of the bounds under initialization and its relation with model width may be a useful result to guide practical training.\n\n3. The bound propagation invariance condition may lead to new insights into the design of neural network architecture to make the bounds tighter.\n\n4. Some results are extended to a 2-layer ReLU network, although this part was not emphasized in the paper.\n\nWeaknesses: 1. The theoretical results have strong assumptions such as linear neural networks, and neural network weights under Gaussian distribution. This is generally not a big concern if the authors can demonstrate that these theoretical insights can lead to great practical improvements, \n\n2. but here the theoretical results developed do not lead to a better model that can outperform existing approaches, and some evaluations are quite weak (e.g., on a single MNIST model only). Since only a few models and networks are shown, it is unsure how general the results are.\n\n3. Although some empirical results are shown to support the theory for ReLU networks, it is hard to argue these observations are indeed the consequence of the theory. For example, “certified training increases tightness” and “larger networks lead to better accuracy” are very generic conclusions, and it is hard to convince the readers that they result from the theory developed in this work.\n\nQuestions: 1. Figure 6 shows that increasing model width is beneficial, however, it is on a simple MNIST network. Can you demonstrate this result on larger networks and datasets? In particular, if we use a state-of-the-art method and model, and enlarge the model size by 4x, how much gain can we obtain over the state-of-the-art? Is the gain consistent over multiple models (e.g., CNN, resnet), training methods (IBP, CROWN-IBP, SABR, MTL-IBP), and datasets (CIFAR10, CIFAR100, TinyImageNet)?\n\n2. Based on Theorem 3.4, can we reparameterize the network such that the bounds are always tight, and the training process just needs to search from a subspace of weights that lead to tight bounds, rather than using gradient descent to enforce tight bounds? For L2 norm certified defense, the state-of-the-art methods use this approach (such as orthogonal convolution layers and their variants).", "labeling_timestamp": "2026-01-11T16:56:38.174874", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper states it performs an \"extensive empirical study\" across ReLU networks and reports multiple empirical findings (e.g., effects of width, transfer to ReLU, state-of-the-art certified accuracy), which contradicts the claim that the evaluation relies on only a single MNIST model example.", "evidence": "“Finally, we show how these results on DLNs transfer to ReLU networks, before conducting an extensive empirical study, (i) confirming this transferability and yielding state-of-the-art certified accuracy, (ii) finding that while all IBP-based training methods lead to high tightness, this increase is dominated by the size of the propagated input regions rather than the robustness specification, and finally (iii) observing that non-IBP-based methods do not increase tightness.”", "section": "Abstract / Introduction"}
{"claim": "Only a few models and network configurations are evaluated, so the generality of the results across architectures and datasets is unclear.", "claim_type": "experimental", "paper_id": "h05eQniJsQ", "paper_title": "Understanding Certified Training with Interval Bound Propagation", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "zdpzuBcupz", "reviewer": "Reviewer_C4Wz", "review_text": "Summary: This paper studies the interval bound propagation-based (IBP-based) training method, one of the most popular approaches to obtaining neural networks with certifiable guarantees. Although existing work demonstrated the effectiveness of this approach, the theoretical understanding of IBP is limited. Under the assumption of linear neural networks and Gaussian weights, this paper proposed a new measure of bound tightness and derived a few theorems to show how the bound tightness changes when propagating among linear layers, and how the width and depths of a randomly initialized network impact bound tightness.\n\nSome empirical results on a few MNIST and CIFAR-10 models demonstrate that certified training indeed improves the tightness measure proposed in this paper. Some interesting empirical results were demonstrated with models varying depths and widths, demonstrating their correlations with bound tightness and accuracy.\n\nThe paper studies an important topic with some novel results, however, its current version has some weaknesses and unresolved questions, see below, so I feel the current version of the paper is below the acceptance threshold.\n\nStrengths: 1. The topic of the paper is relevant, and it is an open challenge. We still don’t understand certified training very well, and this paper is a great attempt to bring in some new understanding.\n\n2. Some novel theoretical insights are given, such as on the tightness of bound propagation and propagation invariance. Also, the growth of the bounds under initialization and its relation with model width may be a useful result to guide practical training.\n\n3. The bound propagation invariance condition may lead to new insights into the design of neural network architecture to make the bounds tighter.\n\n4. Some results are extended to a 2-layer ReLU network, although this part was not emphasized in the paper.\n\nWeaknesses: 1. The theoretical results have strong assumptions such as linear neural networks, and neural network weights under Gaussian distribution. This is generally not a big concern if the authors can demonstrate that these theoretical insights can lead to great practical improvements, \n\n2. but here the theoretical results developed do not lead to a better model that can outperform existing approaches, and some evaluations are quite weak (e.g., on a single MNIST model only). Since only a few models and networks are shown, it is unsure how general the results are.\n\n3. Although some empirical results are shown to support the theory for ReLU networks, it is hard to argue these observations are indeed the consequence of the theory. For example, “certified training increases tightness” and “larger networks lead to better accuracy” are very generic conclusions, and it is hard to convince the readers that they result from the theory developed in this work.\n\nQuestions: 1. Figure 6 shows that increasing model width is beneficial, however, it is on a simple MNIST network. Can you demonstrate this result on larger networks and datasets? In particular, if we use a state-of-the-art method and model, and enlarge the model size by 4x, how much gain can we obtain over the state-of-the-art? Is the gain consistent over multiple models (e.g., CNN, resnet), training methods (IBP, CROWN-IBP, SABR, MTL-IBP), and datasets (CIFAR10, CIFAR100, TinyImageNet)?\n\n2. Based on Theorem 3.4, can we reparameterize the network such that the bounds are always tight, and the training process just needs to search from a subspace of weights that lead to tight bounds, rather than using gradient descent to enforce tight bounds? For L2 norm certified defense, the state-of-the-art methods use this approach (such as orthogonal convolution layers and their variants).", "labeling_timestamp": "2026-01-11T16:56:53.295683", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states it conducted an \"extensive empirical study\" and reports experiments varying network width and depth and comparing IBP-based and non-IBP methods, which contradicts the claim that only a few models/configurations were evaluated.", "evidence": "1) \"conducting an extensive empirical study, (i) confirming this transferability and yielding state-of-the-art certified accuracy, (ii) finding that while all IBP-based training methods lead to high tightness... and finally (iii) observing that non-IBP-based methods do not increase tightness.\" 2) \"Conducting an extensive empirical study, we confirm the predictiveness of our theoretical results for deep ReLU networks and observe that: (i) increasing network width but not depth improves state-of-the-art certified accuracy, (ii) IBP training significantly increases tightness...\"", "section": "Abstract / Introduction"}
{"claim": "Empirical observations on ReLU networks are presented, but it is unclear that these observations are consequences of the linear-theory developed.", "claim_type": "methodology", "paper_id": "h05eQniJsQ", "paper_title": "Understanding Certified Training with Interval Bound Propagation", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "zdpzuBcupz", "reviewer": "Reviewer_C4Wz", "review_text": "Summary: This paper studies the interval bound propagation-based (IBP-based) training method, one of the most popular approaches to obtaining neural networks with certifiable guarantees. Although existing work demonstrated the effectiveness of this approach, the theoretical understanding of IBP is limited. Under the assumption of linear neural networks and Gaussian weights, this paper proposed a new measure of bound tightness and derived a few theorems to show how the bound tightness changes when propagating among linear layers, and how the width and depths of a randomly initialized network impact bound tightness.\n\nSome empirical results on a few MNIST and CIFAR-10 models demonstrate that certified training indeed improves the tightness measure proposed in this paper. Some interesting empirical results were demonstrated with models varying depths and widths, demonstrating their correlations with bound tightness and accuracy.\n\nThe paper studies an important topic with some novel results, however, its current version has some weaknesses and unresolved questions, see below, so I feel the current version of the paper is below the acceptance threshold.\n\nStrengths: 1. The topic of the paper is relevant, and it is an open challenge. We still don’t understand certified training very well, and this paper is a great attempt to bring in some new understanding.\n\n2. Some novel theoretical insights are given, such as on the tightness of bound propagation and propagation invariance. Also, the growth of the bounds under initialization and its relation with model width may be a useful result to guide practical training.\n\n3. The bound propagation invariance condition may lead to new insights into the design of neural network architecture to make the bounds tighter.\n\n4. Some results are extended to a 2-layer ReLU network, although this part was not emphasized in the paper.\n\nWeaknesses: 1. The theoretical results have strong assumptions such as linear neural networks, and neural network weights under Gaussian distribution. This is generally not a big concern if the authors can demonstrate that these theoretical insights can lead to great practical improvements, \n\n2. but here the theoretical results developed do not lead to a better model that can outperform existing approaches, and some evaluations are quite weak (e.g., on a single MNIST model only). Since only a few models and networks are shown, it is unsure how general the results are.\n\n3. Although some empirical results are shown to support the theory for ReLU networks, it is hard to argue these observations are indeed the consequence of the theory. For example, “certified training increases tightness” and “larger networks lead to better accuracy” are very generic conclusions, and it is hard to convince the readers that they result from the theory developed in this work.\n\nQuestions: 1. Figure 6 shows that increasing model width is beneficial, however, it is on a simple MNIST network. Can you demonstrate this result on larger networks and datasets? In particular, if we use a state-of-the-art method and model, and enlarge the model size by 4x, how much gain can we obtain over the state-of-the-art? Is the gain consistent over multiple models (e.g., CNN, resnet), training methods (IBP, CROWN-IBP, SABR, MTL-IBP), and datasets (CIFAR10, CIFAR100, TinyImageNet)?\n\n2. Based on Theorem 3.4, can we reparameterize the network such that the bounds are always tight, and the training process just needs to search from a subspace of weights that lead to tight bounds, rather than using gradient descent to enforce tight bounds? For L2 norm certified defense, the state-of-the-art methods use this approach (such as orthogonal convolution layers and their variants).", "labeling_timestamp": "2026-01-11T16:56:54.603404", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states that it transfers the DLN (linear) results to ReLU networks and conducts an extensive empirical study that confirms the predictiveness of the theoretical results for deep ReLU networks. Thus the claim that it is 'unclear' that the ReLU observations follow from the linear theory is contradicted by the paper's stated transfer and empirical confirmation.", "evidence": "1) \"Finally, we show how these results on DLNs transfer to ReLU networks, before conducting an extensive empirical study, (i) confirming this transferability and yielding state-of-the-art certified accuracy, (ii) finding that while all IBP-based training methods lead to high tightness...\" (Abstract)\n\n2) \"Conducting an extensive empirical study, we confirm the predictiveness of our theoretical results for deep ReLU networks and observe that: (i) increasing network width but not depth improves state-of-the-art certified accuracy, (ii) IBP training significantly increases tightness, almost to the point of propagation invariance...\" (Introduction)\n\n3) \"After proving key results on DLNs, we will show how they transfer to ReLU networks.\" (Section 3, Setting)", "section": "Abstract / Introduction / Section 3 (Setting)"}
{"claim": "The claim that certified training increases bound tightness is generic and is not convincingly attributed to the theoretical development in the paper.", "claim_type": "novelty", "paper_id": "h05eQniJsQ", "paper_title": "Understanding Certified Training with Interval Bound Propagation", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "zdpzuBcupz", "reviewer": "Reviewer_C4Wz", "review_text": "Summary: This paper studies the interval bound propagation-based (IBP-based) training method, one of the most popular approaches to obtaining neural networks with certifiable guarantees. Although existing work demonstrated the effectiveness of this approach, the theoretical understanding of IBP is limited. Under the assumption of linear neural networks and Gaussian weights, this paper proposed a new measure of bound tightness and derived a few theorems to show how the bound tightness changes when propagating among linear layers, and how the width and depths of a randomly initialized network impact bound tightness.\n\nSome empirical results on a few MNIST and CIFAR-10 models demonstrate that certified training indeed improves the tightness measure proposed in this paper. Some interesting empirical results were demonstrated with models varying depths and widths, demonstrating their correlations with bound tightness and accuracy.\n\nThe paper studies an important topic with some novel results, however, its current version has some weaknesses and unresolved questions, see below, so I feel the current version of the paper is below the acceptance threshold.\n\nStrengths: 1. The topic of the paper is relevant, and it is an open challenge. We still don’t understand certified training very well, and this paper is a great attempt to bring in some new understanding.\n\n2. Some novel theoretical insights are given, such as on the tightness of bound propagation and propagation invariance. Also, the growth of the bounds under initialization and its relation with model width may be a useful result to guide practical training.\n\n3. The bound propagation invariance condition may lead to new insights into the design of neural network architecture to make the bounds tighter.\n\n4. Some results are extended to a 2-layer ReLU network, although this part was not emphasized in the paper.\n\nWeaknesses: 1. The theoretical results have strong assumptions such as linear neural networks, and neural network weights under Gaussian distribution. This is generally not a big concern if the authors can demonstrate that these theoretical insights can lead to great practical improvements, \n\n2. but here the theoretical results developed do not lead to a better model that can outperform existing approaches, and some evaluations are quite weak (e.g., on a single MNIST model only). Since only a few models and networks are shown, it is unsure how general the results are.\n\n3. Although some empirical results are shown to support the theory for ReLU networks, it is hard to argue these observations are indeed the consequence of the theory. For example, “certified training increases tightness” and “larger networks lead to better accuracy” are very generic conclusions, and it is hard to convince the readers that they result from the theory developed in this work.\n\nQuestions: 1. Figure 6 shows that increasing model width is beneficial, however, it is on a simple MNIST network. Can you demonstrate this result on larger networks and datasets? In particular, if we use a state-of-the-art method and model, and enlarge the model size by 4x, how much gain can we obtain over the state-of-the-art? Is the gain consistent over multiple models (e.g., CNN, resnet), training methods (IBP, CROWN-IBP, SABR, MTL-IBP), and datasets (CIFAR10, CIFAR100, TinyImageNet)?\n\n2. Based on Theorem 3.4, can we reparameterize the network such that the bounds are always tight, and the training process just needs to search from a subspace of weights that lead to tight bounds, rather than using gradient descent to enforce tight bounds? For L2 norm certified defense, the state-of-the-art methods use this approach (such as orthogonal convolution layers and their variants).", "labeling_timestamp": "2026-01-11T16:57:03.745758", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly claims and proves theoretically that IBP training increases bound tightness for deep linear networks and then studies transfer to ReLU networks, and also provides empirical confirmation. Thus the assertion that the increase is presented only generically and not attributed to theoretical development is contradicted by the paper.", "evidence": "1) \"We first show theoretically that, for deep linear models (DLNs), tightness decreases with width and depth at initialization, but improves with IBP training.\" 2) \"we show that (i) at initialization, tightness decreases with width (polynomially) and depth (exponentially), (ii) tightness is increased by IBP training, and (iii) sufficient width becomes crucial for trained networks.\"", "section": "Abstract; Section 3 (Understanding IBP Training)"}
{"claim": "The claim that larger networks lead to better accuracy is generic and is not convincingly shown to follow from the proposed theory.", "claim_type": "novelty", "paper_id": "h05eQniJsQ", "paper_title": "Understanding Certified Training with Interval Bound Propagation", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "zdpzuBcupz", "reviewer": "Reviewer_C4Wz", "review_text": "Summary: This paper studies the interval bound propagation-based (IBP-based) training method, one of the most popular approaches to obtaining neural networks with certifiable guarantees. Although existing work demonstrated the effectiveness of this approach, the theoretical understanding of IBP is limited. Under the assumption of linear neural networks and Gaussian weights, this paper proposed a new measure of bound tightness and derived a few theorems to show how the bound tightness changes when propagating among linear layers, and how the width and depths of a randomly initialized network impact bound tightness.\n\nSome empirical results on a few MNIST and CIFAR-10 models demonstrate that certified training indeed improves the tightness measure proposed in this paper. Some interesting empirical results were demonstrated with models varying depths and widths, demonstrating their correlations with bound tightness and accuracy.\n\nThe paper studies an important topic with some novel results, however, its current version has some weaknesses and unresolved questions, see below, so I feel the current version of the paper is below the acceptance threshold.\n\nStrengths: 1. The topic of the paper is relevant, and it is an open challenge. We still don’t understand certified training very well, and this paper is a great attempt to bring in some new understanding.\n\n2. Some novel theoretical insights are given, such as on the tightness of bound propagation and propagation invariance. Also, the growth of the bounds under initialization and its relation with model width may be a useful result to guide practical training.\n\n3. The bound propagation invariance condition may lead to new insights into the design of neural network architecture to make the bounds tighter.\n\n4. Some results are extended to a 2-layer ReLU network, although this part was not emphasized in the paper.\n\nWeaknesses: 1. The theoretical results have strong assumptions such as linear neural networks, and neural network weights under Gaussian distribution. This is generally not a big concern if the authors can demonstrate that these theoretical insights can lead to great practical improvements, \n\n2. but here the theoretical results developed do not lead to a better model that can outperform existing approaches, and some evaluations are quite weak (e.g., on a single MNIST model only). Since only a few models and networks are shown, it is unsure how general the results are.\n\n3. Although some empirical results are shown to support the theory for ReLU networks, it is hard to argue these observations are indeed the consequence of the theory. For example, “certified training increases tightness” and “larger networks lead to better accuracy” are very generic conclusions, and it is hard to convince the readers that they result from the theory developed in this work.\n\nQuestions: 1. Figure 6 shows that increasing model width is beneficial, however, it is on a simple MNIST network. Can you demonstrate this result on larger networks and datasets? In particular, if we use a state-of-the-art method and model, and enlarge the model size by 4x, how much gain can we obtain over the state-of-the-art? Is the gain consistent over multiple models (e.g., CNN, resnet), training methods (IBP, CROWN-IBP, SABR, MTL-IBP), and datasets (CIFAR10, CIFAR100, TinyImageNet)?\n\n2. Based on Theorem 3.4, can we reparameterize the network such that the bounds are always tight, and the training process just needs to search from a subspace of weights that lead to tight bounds, rather than using gradient descent to enforce tight bounds? For L2 norm certified defense, the state-of-the-art methods use this approach (such as orthogonal convolution layers and their variants).", "labeling_timestamp": "2026-01-11T16:57:11.945105", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper provides both theoretical results linking network width to IBP bound tightness and empirical results showing that increasing width (but not depth) improves certified accuracy. Thus the reviewer's claim that the paper does not convincingly show larger networks lead to better accuracy is contradicted by the paper.", "evidence": "Abstract: \"We first show theoretically that, for deep linear models (DLNs), tightness decreases with width and depth at initialization, but improves with IBP training.\"  Introduction (This Work): \"We show that (i) at initialization, tightness decreases with width (polynomially) and depth (exponentially), (ii) tightness is increased by IBP training, and (iii) sufficient width becomes crucial for trained networks.\"  Introduction (Empirical findings): \"(i) increasing network width but not depth improves state-of-the-art certified accuracy,\"", "section": "Abstract / Introduction (This Work and Empirical findings)"}
{"claim": "Figure 6 demonstrates width benefits only on a simple MNIST network, and larger networks and datasets are not evaluated.", "claim_type": "experimental", "paper_id": "h05eQniJsQ", "paper_title": "Understanding Certified Training with Interval Bound Propagation", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "zdpzuBcupz", "reviewer": "Reviewer_C4Wz", "review_text": "Summary: This paper studies the interval bound propagation-based (IBP-based) training method, one of the most popular approaches to obtaining neural networks with certifiable guarantees. Although existing work demonstrated the effectiveness of this approach, the theoretical understanding of IBP is limited. Under the assumption of linear neural networks and Gaussian weights, this paper proposed a new measure of bound tightness and derived a few theorems to show how the bound tightness changes when propagating among linear layers, and how the width and depths of a randomly initialized network impact bound tightness.\n\nSome empirical results on a few MNIST and CIFAR-10 models demonstrate that certified training indeed improves the tightness measure proposed in this paper. Some interesting empirical results were demonstrated with models varying depths and widths, demonstrating their correlations with bound tightness and accuracy.\n\nThe paper studies an important topic with some novel results, however, its current version has some weaknesses and unresolved questions, see below, so I feel the current version of the paper is below the acceptance threshold.\n\nStrengths: 1. The topic of the paper is relevant, and it is an open challenge. We still don’t understand certified training very well, and this paper is a great attempt to bring in some new understanding.\n\n2. Some novel theoretical insights are given, such as on the tightness of bound propagation and propagation invariance. Also, the growth of the bounds under initialization and its relation with model width may be a useful result to guide practical training.\n\n3. The bound propagation invariance condition may lead to new insights into the design of neural network architecture to make the bounds tighter.\n\n4. Some results are extended to a 2-layer ReLU network, although this part was not emphasized in the paper.\n\nWeaknesses: 1. The theoretical results have strong assumptions such as linear neural networks, and neural network weights under Gaussian distribution. This is generally not a big concern if the authors can demonstrate that these theoretical insights can lead to great practical improvements, \n\n2. but here the theoretical results developed do not lead to a better model that can outperform existing approaches, and some evaluations are quite weak (e.g., on a single MNIST model only). Since only a few models and networks are shown, it is unsure how general the results are.\n\n3. Although some empirical results are shown to support the theory for ReLU networks, it is hard to argue these observations are indeed the consequence of the theory. For example, “certified training increases tightness” and “larger networks lead to better accuracy” are very generic conclusions, and it is hard to convince the readers that they result from the theory developed in this work.\n\nQuestions: 1. Figure 6 shows that increasing model width is beneficial, however, it is on a simple MNIST network. Can you demonstrate this result on larger networks and datasets? In particular, if we use a state-of-the-art method and model, and enlarge the model size by 4x, how much gain can we obtain over the state-of-the-art? Is the gain consistent over multiple models (e.g., CNN, resnet), training methods (IBP, CROWN-IBP, SABR, MTL-IBP), and datasets (CIFAR10, CIFAR100, TinyImageNet)?\n\n2. Based on Theorem 3.4, can we reparameterize the network such that the bounds are always tight, and the training process just needs to search from a subspace of weights that lead to tight bounds, rather than using gradient descent to enforce tight bounds? For L2 norm certified defense, the state-of-the-art methods use this approach (such as orthogonal convolution layers and their variants).", "labeling_timestamp": "2026-01-11T16:57:06.231738", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt states that an extensive empirical study finds that increasing width (but not depth) improves certified accuracy, but the excerpt does not include Figure 6, experimental details, or dataset names. Therefore we cannot verify the reviewer's specific claim about Figure 6 showing results only on a simple MNIST network and that larger networks/datasets were not evaluated.", "evidence": "Abstract: \"...before conducting an extensive empirical study, (i) confirming this transferability and yielding state-of-the-art certified accuracy...\"; Introduction: \"...we confirm the predictiveness of our theoretical results for deep ReLU networks and observe that: (i) increasing network width but not depth improves state-of-the-art certified accuracy...\"", "section": "Abstract / Introduction"}
{"claim": "The paper does not evaluate whether enlarging model size by four times yields measurable gains over state-of-the-art methods on modern models.", "claim_type": "baseline", "paper_id": "h05eQniJsQ", "paper_title": "Understanding Certified Training with Interval Bound Propagation", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "zdpzuBcupz", "reviewer": "Reviewer_C4Wz", "review_text": "Summary: This paper studies the interval bound propagation-based (IBP-based) training method, one of the most popular approaches to obtaining neural networks with certifiable guarantees. Although existing work demonstrated the effectiveness of this approach, the theoretical understanding of IBP is limited. Under the assumption of linear neural networks and Gaussian weights, this paper proposed a new measure of bound tightness and derived a few theorems to show how the bound tightness changes when propagating among linear layers, and how the width and depths of a randomly initialized network impact bound tightness.\n\nSome empirical results on a few MNIST and CIFAR-10 models demonstrate that certified training indeed improves the tightness measure proposed in this paper. Some interesting empirical results were demonstrated with models varying depths and widths, demonstrating their correlations with bound tightness and accuracy.\n\nThe paper studies an important topic with some novel results, however, its current version has some weaknesses and unresolved questions, see below, so I feel the current version of the paper is below the acceptance threshold.\n\nStrengths: 1. The topic of the paper is relevant, and it is an open challenge. We still don’t understand certified training very well, and this paper is a great attempt to bring in some new understanding.\n\n2. Some novel theoretical insights are given, such as on the tightness of bound propagation and propagation invariance. Also, the growth of the bounds under initialization and its relation with model width may be a useful result to guide practical training.\n\n3. The bound propagation invariance condition may lead to new insights into the design of neural network architecture to make the bounds tighter.\n\n4. Some results are extended to a 2-layer ReLU network, although this part was not emphasized in the paper.\n\nWeaknesses: 1. The theoretical results have strong assumptions such as linear neural networks, and neural network weights under Gaussian distribution. This is generally not a big concern if the authors can demonstrate that these theoretical insights can lead to great practical improvements, \n\n2. but here the theoretical results developed do not lead to a better model that can outperform existing approaches, and some evaluations are quite weak (e.g., on a single MNIST model only). Since only a few models and networks are shown, it is unsure how general the results are.\n\n3. Although some empirical results are shown to support the theory for ReLU networks, it is hard to argue these observations are indeed the consequence of the theory. For example, “certified training increases tightness” and “larger networks lead to better accuracy” are very generic conclusions, and it is hard to convince the readers that they result from the theory developed in this work.\n\nQuestions: 1. Figure 6 shows that increasing model width is beneficial, however, it is on a simple MNIST network. Can you demonstrate this result on larger networks and datasets? In particular, if we use a state-of-the-art method and model, and enlarge the model size by 4x, how much gain can we obtain over the state-of-the-art? Is the gain consistent over multiple models (e.g., CNN, resnet), training methods (IBP, CROWN-IBP, SABR, MTL-IBP), and datasets (CIFAR10, CIFAR100, TinyImageNet)?\n\n2. Based on Theorem 3.4, can we reparameterize the network such that the bounds are always tight, and the training process just needs to search from a subspace of weights that lead to tight bounds, rather than using gradient descent to enforce tight bounds? For L2 norm certified defense, the state-of-the-art methods use this approach (such as orthogonal convolution layers and their variants).", "labeling_timestamp": "2026-01-11T16:57:22.835394", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper reports empirical results showing that increasing network width (but not depth) improves state-of-the-art certified accuracy, so it does evaluate width scaling in general. However, the provided text does not report any experiment or explicit analysis specifically about enlarging model size by four times on modern models, so whether a 4× enlargement was evaluated or yields measurable gains is not addressed in the quoted content.", "evidence": "\"Conducting an extensive empirical study, we confirm the predictiveness of our theoretical results for deep ReLU networks and observe that: (i) increasing network width but not depth improves state-of-the-art certified accuracy, (ii) IBP training significantly increases tightness, almost to the point of propagation invariance, ...\"", "section": "Abstract / Empirical study summary"}
{"claim": "The paper does not test whether observed gains are consistent across multiple architectures, training methods, and datasets like CIFAR and TinyImageNet.", "claim_type": "experimental", "paper_id": "h05eQniJsQ", "paper_title": "Understanding Certified Training with Interval Bound Propagation", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "zdpzuBcupz", "reviewer": "Reviewer_C4Wz", "review_text": "Summary: This paper studies the interval bound propagation-based (IBP-based) training method, one of the most popular approaches to obtaining neural networks with certifiable guarantees. Although existing work demonstrated the effectiveness of this approach, the theoretical understanding of IBP is limited. Under the assumption of linear neural networks and Gaussian weights, this paper proposed a new measure of bound tightness and derived a few theorems to show how the bound tightness changes when propagating among linear layers, and how the width and depths of a randomly initialized network impact bound tightness.\n\nSome empirical results on a few MNIST and CIFAR-10 models demonstrate that certified training indeed improves the tightness measure proposed in this paper. Some interesting empirical results were demonstrated with models varying depths and widths, demonstrating their correlations with bound tightness and accuracy.\n\nThe paper studies an important topic with some novel results, however, its current version has some weaknesses and unresolved questions, see below, so I feel the current version of the paper is below the acceptance threshold.\n\nStrengths: 1. The topic of the paper is relevant, and it is an open challenge. We still don’t understand certified training very well, and this paper is a great attempt to bring in some new understanding.\n\n2. Some novel theoretical insights are given, such as on the tightness of bound propagation and propagation invariance. Also, the growth of the bounds under initialization and its relation with model width may be a useful result to guide practical training.\n\n3. The bound propagation invariance condition may lead to new insights into the design of neural network architecture to make the bounds tighter.\n\n4. Some results are extended to a 2-layer ReLU network, although this part was not emphasized in the paper.\n\nWeaknesses: 1. The theoretical results have strong assumptions such as linear neural networks, and neural network weights under Gaussian distribution. This is generally not a big concern if the authors can demonstrate that these theoretical insights can lead to great practical improvements, \n\n2. but here the theoretical results developed do not lead to a better model that can outperform existing approaches, and some evaluations are quite weak (e.g., on a single MNIST model only). Since only a few models and networks are shown, it is unsure how general the results are.\n\n3. Although some empirical results are shown to support the theory for ReLU networks, it is hard to argue these observations are indeed the consequence of the theory. For example, “certified training increases tightness” and “larger networks lead to better accuracy” are very generic conclusions, and it is hard to convince the readers that they result from the theory developed in this work.\n\nQuestions: 1. Figure 6 shows that increasing model width is beneficial, however, it is on a simple MNIST network. Can you demonstrate this result on larger networks and datasets? In particular, if we use a state-of-the-art method and model, and enlarge the model size by 4x, how much gain can we obtain over the state-of-the-art? Is the gain consistent over multiple models (e.g., CNN, resnet), training methods (IBP, CROWN-IBP, SABR, MTL-IBP), and datasets (CIFAR10, CIFAR100, TinyImageNet)?\n\n2. Based on Theorem 3.4, can we reparameterize the network such that the bounds are always tight, and the training process just needs to search from a subspace of weights that lead to tight bounds, rather than using gradient descent to enforce tight bounds? For L2 norm certified defense, the state-of-the-art methods use this approach (such as orthogonal convolution layers and their variants).", "labeling_timestamp": "2026-01-11T16:57:39.579275", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper does evaluate multiple architectures and training methods: it analyzes deep linear networks (DLNs) and shows transfer to ReLU networks, and it discusses and empirically compares various IBP-based and non-IBP-based training methods. However, the provided paper content does not mention experiments on specific datasets such as CIFAR or TinyImageNet, so the dataset aspect of the claim is not supported by the paper excerpt.", "evidence": "“Finally, we show how these results on DLNs transfer to ReLU networks, before conducting an extensive empirical study, (i) confirming this transferability and yielding state-of-the-art certified accuracy, (ii) finding that while all IBP-based training methods lead to high tightness, this increase is dominated by the size of the propagated input regions rather than the robustness specification, and finally (iii) observing that non-IBP-based methods do not increase tightness.”\n\n“Setting We focus our theoretical analysis on deep linear networks (DLNs), i.e., f(x)=ΠLi=1 W(i) x. After proving key results on DLNs, we will show how they transfer to ReLU networks.”", "section": "Abstract; Section 3 (Setting)"}
{"claim": "The authors do not explore reparameterizing networks to guarantee tight bounds and restrict training to a subspace of weights, as suggested by Theorem 3.4.", "claim_type": "methodology", "paper_id": "h05eQniJsQ", "paper_title": "Understanding Certified Training with Interval Bound Propagation", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "zdpzuBcupz", "reviewer": "Reviewer_C4Wz", "review_text": "Summary: This paper studies the interval bound propagation-based (IBP-based) training method, one of the most popular approaches to obtaining neural networks with certifiable guarantees. Although existing work demonstrated the effectiveness of this approach, the theoretical understanding of IBP is limited. Under the assumption of linear neural networks and Gaussian weights, this paper proposed a new measure of bound tightness and derived a few theorems to show how the bound tightness changes when propagating among linear layers, and how the width and depths of a randomly initialized network impact bound tightness.\n\nSome empirical results on a few MNIST and CIFAR-10 models demonstrate that certified training indeed improves the tightness measure proposed in this paper. Some interesting empirical results were demonstrated with models varying depths and widths, demonstrating their correlations with bound tightness and accuracy.\n\nThe paper studies an important topic with some novel results, however, its current version has some weaknesses and unresolved questions, see below, so I feel the current version of the paper is below the acceptance threshold.\n\nStrengths: 1. The topic of the paper is relevant, and it is an open challenge. We still don’t understand certified training very well, and this paper is a great attempt to bring in some new understanding.\n\n2. Some novel theoretical insights are given, such as on the tightness of bound propagation and propagation invariance. Also, the growth of the bounds under initialization and its relation with model width may be a useful result to guide practical training.\n\n3. The bound propagation invariance condition may lead to new insights into the design of neural network architecture to make the bounds tighter.\n\n4. Some results are extended to a 2-layer ReLU network, although this part was not emphasized in the paper.\n\nWeaknesses: 1. The theoretical results have strong assumptions such as linear neural networks, and neural network weights under Gaussian distribution. This is generally not a big concern if the authors can demonstrate that these theoretical insights can lead to great practical improvements, \n\n2. but here the theoretical results developed do not lead to a better model that can outperform existing approaches, and some evaluations are quite weak (e.g., on a single MNIST model only). Since only a few models and networks are shown, it is unsure how general the results are.\n\n3. Although some empirical results are shown to support the theory for ReLU networks, it is hard to argue these observations are indeed the consequence of the theory. For example, “certified training increases tightness” and “larger networks lead to better accuracy” are very generic conclusions, and it is hard to convince the readers that they result from the theory developed in this work.\n\nQuestions: 1. Figure 6 shows that increasing model width is beneficial, however, it is on a simple MNIST network. Can you demonstrate this result on larger networks and datasets? In particular, if we use a state-of-the-art method and model, and enlarge the model size by 4x, how much gain can we obtain over the state-of-the-art? Is the gain consistent over multiple models (e.g., CNN, resnet), training methods (IBP, CROWN-IBP, SABR, MTL-IBP), and datasets (CIFAR10, CIFAR100, TinyImageNet)?\n\n2. Based on Theorem 3.4, can we reparameterize the network such that the bounds are always tight, and the training process just needs to search from a subspace of weights that lead to tight bounds, rather than using gradient descent to enforce tight bounds? For L2 norm certified defense, the state-of-the-art methods use this approach (such as orthogonal convolution layers and their variants).", "labeling_timestamp": "2026-01-11T16:57:31.002955", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper derives necessary and sufficient conditions on weights for IBP bounds to be exact and studies their regularization effects and empirical tightness, but nowhere proposes or evaluates reparameterizing networks or restricting training to a weight subspace to guarantee tight bounds. The listed contributions and experiments focus on analysing conditions, a tightness metric, and IBP-based training methods, not on reparameterization strategies.", "evidence": "Abstract: \"we derive necessary and sufficient conditions on weight matrices for IBP bounds to become exact and demonstrate that these impose strong regularization, providing an explanation for the observed robustness-accuracy trade-off.\" \nIntroduction / This Work: \"To this end, we derive necessary and sufficient conditions on a network's weights under which IBP bounds become tight, a property we call propagation invariance, and prove that it implies an extreme regularization...\" \nSection 3 (Setting): \"After proving key results on DLNs, we will show how they transfer to ReLU networks.\"", "section": "Abstract; Introduction (This Work); Section 3 (Setting)"}
{"claim": "State-of-the-art L2 certified defenses use reparameterization techniques, such as orthogonal convolution layers, which the paper does not discuss or compare against.", "claim_type": "baseline", "paper_id": "h05eQniJsQ", "paper_title": "Understanding Certified Training with Interval Bound Propagation", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "zdpzuBcupz", "reviewer": "Reviewer_C4Wz", "review_text": "Summary: This paper studies the interval bound propagation-based (IBP-based) training method, one of the most popular approaches to obtaining neural networks with certifiable guarantees. Although existing work demonstrated the effectiveness of this approach, the theoretical understanding of IBP is limited. Under the assumption of linear neural networks and Gaussian weights, this paper proposed a new measure of bound tightness and derived a few theorems to show how the bound tightness changes when propagating among linear layers, and how the width and depths of a randomly initialized network impact bound tightness.\n\nSome empirical results on a few MNIST and CIFAR-10 models demonstrate that certified training indeed improves the tightness measure proposed in this paper. Some interesting empirical results were demonstrated with models varying depths and widths, demonstrating their correlations with bound tightness and accuracy.\n\nThe paper studies an important topic with some novel results, however, its current version has some weaknesses and unresolved questions, see below, so I feel the current version of the paper is below the acceptance threshold.\n\nStrengths: 1. The topic of the paper is relevant, and it is an open challenge. We still don’t understand certified training very well, and this paper is a great attempt to bring in some new understanding.\n\n2. Some novel theoretical insights are given, such as on the tightness of bound propagation and propagation invariance. Also, the growth of the bounds under initialization and its relation with model width may be a useful result to guide practical training.\n\n3. The bound propagation invariance condition may lead to new insights into the design of neural network architecture to make the bounds tighter.\n\n4. Some results are extended to a 2-layer ReLU network, although this part was not emphasized in the paper.\n\nWeaknesses: 1. The theoretical results have strong assumptions such as linear neural networks, and neural network weights under Gaussian distribution. This is generally not a big concern if the authors can demonstrate that these theoretical insights can lead to great practical improvements, \n\n2. but here the theoretical results developed do not lead to a better model that can outperform existing approaches, and some evaluations are quite weak (e.g., on a single MNIST model only). Since only a few models and networks are shown, it is unsure how general the results are.\n\n3. Although some empirical results are shown to support the theory for ReLU networks, it is hard to argue these observations are indeed the consequence of the theory. For example, “certified training increases tightness” and “larger networks lead to better accuracy” are very generic conclusions, and it is hard to convince the readers that they result from the theory developed in this work.\n\nQuestions: 1. Figure 6 shows that increasing model width is beneficial, however, it is on a simple MNIST network. Can you demonstrate this result on larger networks and datasets? In particular, if we use a state-of-the-art method and model, and enlarge the model size by 4x, how much gain can we obtain over the state-of-the-art? Is the gain consistent over multiple models (e.g., CNN, resnet), training methods (IBP, CROWN-IBP, SABR, MTL-IBP), and datasets (CIFAR10, CIFAR100, TinyImageNet)?\n\n2. Based on Theorem 3.4, can we reparameterize the network such that the bounds are always tight, and the training process just needs to search from a subspace of weights that lead to tight bounds, rather than using gradient descent to enforce tight bounds? For L2 norm certified defense, the state-of-the-art methods use this approach (such as orthogonal convolution layers and their variants).", "labeling_timestamp": "2026-01-11T16:58:26.178152", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper does not discuss or compare against reparameterization techniques (e.g., orthogonal convolution layers) and focuses on IBP-based methods and ℓ∞ perturbations, so the critique that the paper omits such comparisons is supported. However, the paper does not make or endorse the claim that state-of-the-art L2 certified defenses use reparameterization techniques, and it does not address L2 defenses at all, so that part is not supported by the paper.", "evidence": ["As we focus on ℓ∞ perturbations in this work, we henceforth drop the subscript p for notational clarity.", "As a result, all methods obtaining state-of-the-art performance leverage IBP bounds either directly (Shi et al., 2021), as regularizer (Palma et al., 2022), or to precisely but unsoundly approximate the worst-case loss (Müller et al., 2022b; Mao et al., 2023; Palma et al., 2023).", "To alleviate the resulting robustness-accuracy trade-off, all current state-of-the-art certified training methods combine IBP and adversarial training by using IBP bounds only for regularization (IBP-R (Palma et al., 2022)), by only propagating small, adversarially selected regions (SABR (Müller et al., 2022b)), using IBP bounds only for the first layers and PGD bounds for the remainder of the network (TAPS (Mao et al., 2023)), or combining losses over adversarial samples and IBP bounds (CC-IBP, MTL-IBP (Palma et al., 2023))."], "section": "2 BACKGROUND / Introduction"}
{"claim": "Results extended to a two-layer ReLU network are not emphasized or sufficiently highlighted in the paper's presentation.", "claim_type": "presentation", "paper_id": "h05eQniJsQ", "paper_title": "Understanding Certified Training with Interval Bound Propagation", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "zdpzuBcupz", "reviewer": "Reviewer_C4Wz", "review_text": "Summary: This paper studies the interval bound propagation-based (IBP-based) training method, one of the most popular approaches to obtaining neural networks with certifiable guarantees. Although existing work demonstrated the effectiveness of this approach, the theoretical understanding of IBP is limited. Under the assumption of linear neural networks and Gaussian weights, this paper proposed a new measure of bound tightness and derived a few theorems to show how the bound tightness changes when propagating among linear layers, and how the width and depths of a randomly initialized network impact bound tightness.\n\nSome empirical results on a few MNIST and CIFAR-10 models demonstrate that certified training indeed improves the tightness measure proposed in this paper. Some interesting empirical results were demonstrated with models varying depths and widths, demonstrating their correlations with bound tightness and accuracy.\n\nThe paper studies an important topic with some novel results, however, its current version has some weaknesses and unresolved questions, see below, so I feel the current version of the paper is below the acceptance threshold.\n\nStrengths: 1. The topic of the paper is relevant, and it is an open challenge. We still don’t understand certified training very well, and this paper is a great attempt to bring in some new understanding.\n\n2. Some novel theoretical insights are given, such as on the tightness of bound propagation and propagation invariance. Also, the growth of the bounds under initialization and its relation with model width may be a useful result to guide practical training.\n\n3. The bound propagation invariance condition may lead to new insights into the design of neural network architecture to make the bounds tighter.\n\n4. Some results are extended to a 2-layer ReLU network, although this part was not emphasized in the paper.\n\nWeaknesses: 1. The theoretical results have strong assumptions such as linear neural networks, and neural network weights under Gaussian distribution. This is generally not a big concern if the authors can demonstrate that these theoretical insights can lead to great practical improvements, \n\n2. but here the theoretical results developed do not lead to a better model that can outperform existing approaches, and some evaluations are quite weak (e.g., on a single MNIST model only). Since only a few models and networks are shown, it is unsure how general the results are.\n\n3. Although some empirical results are shown to support the theory for ReLU networks, it is hard to argue these observations are indeed the consequence of the theory. For example, “certified training increases tightness” and “larger networks lead to better accuracy” are very generic conclusions, and it is hard to convince the readers that they result from the theory developed in this work.\n\nQuestions: 1. Figure 6 shows that increasing model width is beneficial, however, it is on a simple MNIST network. Can you demonstrate this result on larger networks and datasets? In particular, if we use a state-of-the-art method and model, and enlarge the model size by 4x, how much gain can we obtain over the state-of-the-art? Is the gain consistent over multiple models (e.g., CNN, resnet), training methods (IBP, CROWN-IBP, SABR, MTL-IBP), and datasets (CIFAR10, CIFAR100, TinyImageNet)?\n\n2. Based on Theorem 3.4, can we reparameterize the network such that the bounds are always tight, and the training process just needs to search from a subspace of weights that lead to tight bounds, rather than using gradient descent to enforce tight bounds? For L2 norm certified defense, the state-of-the-art methods use this approach (such as orthogonal convolution layers and their variants).", "labeling_timestamp": "2026-01-11T16:57:42.770312", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states that results on deep linear networks are transferred to ReLU networks, includes a figure illustrating propagation for a two-layer network, and reports an empirical study confirming transferability—indicating the extension to ReLU is presented and highlighted.", "evidence": "Abstract: \"Finally, we show how these results on DLNs transfer to ReLU networks, before conducting an extensive empirical study, (i) confirming this transferability...\"\nSection 2 (Background): \"We illustrate this propagation process for a two-layer network in Figure 1.\"\nSection 3 (Setting): \"After proving key results on DLNs, we will show how they transfer to ReLU networks.\"", "section": "Abstract; Section 2 (Background); Section 3 (Setting)"}
{"claim": "The paper lacks explicit comparisons with modern certified training methods like CROWN-IBP, SABR, and MTL-IBP.", "claim_type": "baseline", "paper_id": "h05eQniJsQ", "paper_title": "Understanding Certified Training with Interval Bound Propagation", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "zdpzuBcupz", "reviewer": "Reviewer_C4Wz", "review_text": "Summary: This paper studies the interval bound propagation-based (IBP-based) training method, one of the most popular approaches to obtaining neural networks with certifiable guarantees. Although existing work demonstrated the effectiveness of this approach, the theoretical understanding of IBP is limited. Under the assumption of linear neural networks and Gaussian weights, this paper proposed a new measure of bound tightness and derived a few theorems to show how the bound tightness changes when propagating among linear layers, and how the width and depths of a randomly initialized network impact bound tightness.\n\nSome empirical results on a few MNIST and CIFAR-10 models demonstrate that certified training indeed improves the tightness measure proposed in this paper. Some interesting empirical results were demonstrated with models varying depths and widths, demonstrating their correlations with bound tightness and accuracy.\n\nThe paper studies an important topic with some novel results, however, its current version has some weaknesses and unresolved questions, see below, so I feel the current version of the paper is below the acceptance threshold.\n\nStrengths: 1. The topic of the paper is relevant, and it is an open challenge. We still don’t understand certified training very well, and this paper is a great attempt to bring in some new understanding.\n\n2. Some novel theoretical insights are given, such as on the tightness of bound propagation and propagation invariance. Also, the growth of the bounds under initialization and its relation with model width may be a useful result to guide practical training.\n\n3. The bound propagation invariance condition may lead to new insights into the design of neural network architecture to make the bounds tighter.\n\n4. Some results are extended to a 2-layer ReLU network, although this part was not emphasized in the paper.\n\nWeaknesses: 1. The theoretical results have strong assumptions such as linear neural networks, and neural network weights under Gaussian distribution. This is generally not a big concern if the authors can demonstrate that these theoretical insights can lead to great practical improvements, \n\n2. but here the theoretical results developed do not lead to a better model that can outperform existing approaches, and some evaluations are quite weak (e.g., on a single MNIST model only). Since only a few models and networks are shown, it is unsure how general the results are.\n\n3. Although some empirical results are shown to support the theory for ReLU networks, it is hard to argue these observations are indeed the consequence of the theory. For example, “certified training increases tightness” and “larger networks lead to better accuracy” are very generic conclusions, and it is hard to convince the readers that they result from the theory developed in this work.\n\nQuestions: 1. Figure 6 shows that increasing model width is beneficial, however, it is on a simple MNIST network. Can you demonstrate this result on larger networks and datasets? In particular, if we use a state-of-the-art method and model, and enlarge the model size by 4x, how much gain can we obtain over the state-of-the-art? Is the gain consistent over multiple models (e.g., CNN, resnet), training methods (IBP, CROWN-IBP, SABR, MTL-IBP), and datasets (CIFAR10, CIFAR100, TinyImageNet)?\n\n2. Based on Theorem 3.4, can we reparameterize the network such that the bounds are always tight, and the training process just needs to search from a subspace of weights that lead to tight bounds, rather than using gradient descent to enforce tight bounds? For L2 norm certified defense, the state-of-the-art methods use this approach (such as orthogonal convolution layers and their variants).", "labeling_timestamp": "2026-01-11T16:58:04.159697", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly references and discusses modern IBP-based certified training methods (including SABR and MTL-IBP) and reports an extensive empirical study yielding state-of-the-art certified accuracy, so it does not ‘lack explicit comparisons’ with such methods.", "evidence": "\"all current state-of-the-art certified training methods combine IBP and adversarial training by using IBP bounds only for regularization (IBP-R (Palma et al., 2022)), by only propagating small, adversarially selected regions (SABR (Müller et al., 2022b)), using IBP bounds only for the first layers and PGD bounds for the remainder of the network (TAPS (Mao et al., 2023)), or combining losses over adversarial samples and IBP bounds (CC-IBP, MTL-IBP (Palma et al., 2023)).\"", "section": "2 BACKGROUND"}
{"claim": "The paper's title overreaches by implying comprehensive coverage of training-free guidance beyond the specific subset of works actually addressed.", "claim_type": "presentation", "paper_id": "N8YbGX98vc", "paper_title": "TFG: Unified Training-Free Guidance for Diffusion Models", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "6i2pz4NB9X", "reviewer": "Reviewer_Xm6V", "review_text": "Summary: The paper introduces Training-Free Guidance (TFG), a novel framework designed to enhance the generation of samples with desired properties using diffusion models, without necessitating additional model training. TFG aims to resolve the shortcomings of existing training-free methods by offering a unified algorithmic framework that simplifies the comparison and application of such methods across a wide range of tasks. By theoretically and empirically analyzing a hyper-parameter design space within this framework, the authors develop an effective strategy for hyper-parameter selection applicable to various tasks. Their comprehensive benchmarks across multiple tasks and targets demonstrate TFG's superior performance, achieving an average improvement of 7.4% over existing methods.\n\nStrengths: 1. Given the burgeoning interest in the area of training-free guidance, the establishment of a unified benchmark as presented in this paper is a commendable contribution that holds the potential to significantly advance research in this field. The authors' efforts in this direction are highly appreciated and underscore the importance of standardized benchmarks for facilitating future developments.\n\n2. The experiments conducted in this study are extensive, reflecting a high degree of diligence and thoroughness. Such a comprehensive experimental approach is commendable and warrants recognition. Consequently, I believe this aspect of the paper merits a positive evaluation for its contribution to validating the proposed framework and its applicability across a diverse range of tasks.\n\nWeaknesses: There are several areas where improvements could significantly enhance its contributions. Addressing these points satisfactorily would make a strong case for elevating the paper's status to \"Accept\" or \"Strong Accept\".\n\n1. While the paper provides valuable insights into a specific line of training-free guidance, it is important to acknowledge the existence of a broader spectrum of works outside this category. This observation suggests that the title of the paper may slightly overreach, potentially implying a more comprehensive coverage than is actually presented. (Question 1)\n\n2. The paper commendably supports the motivations behind conditional diffusion in Appendix A, offering a solid foundation for its relevance. However, the rationale for focusing specifically on training-free approaches appears less extensively articulated. (Question 2)\n\n3. The authors claimed that they have theoretically grounded their unified framework. However, upon review, the theoretical underpinnings presented seem to require further elaboration to fully substantiate this claim. (Question 3, 4, 5)\n\n4. The authors claimed that \"the studies of training-free methods become the study within the hyperparameter space of our framework\". Nonetheless, it appears that the search space defined within this framework might not fully encompass the range of hyper-parameters considered in existing literature. This limitation could potentially restrict the framework's applicability or comparative analysis capabilities. (Question 6, 7)\n\nQuestions: 1. The field of training-free guidance encompasses a wide array of studies beyond those specifically addressed in this paper (e.g. [1]-[3]). Could you elaborate on how these works relate to the scope and contributions of your paper?\n\n2. Your paper operates under the assumption that the forward model is known, leveraging training-free guidance for solving inverse problems. Given that knowing the forward model allows for the generation of a large number of samples to train a conditional diffusion model at a relatively low computational cost (e.g., 10 A100 GPU hours) and training-based approaches are much better than training-free ones, could you discuss the significant motivations of adopting a training-free approach in this context?\n\n3. The theoretical foundation of your paper seems to rest significantly on Lemma 4.1, which revisits the variance of MMSE estimator of the signal corrupted by Gaussian noise (e.g., (2.8) in [4]). This formula is widely adopted in diffusion papers (e.g., [5]). Given its established nature and previous applications, could you elaborate on how this lemma specifically contributes to the novel aspects of your framework?\n\n4. Concerning Lemma 4.1, it appears there is no assurance that the generated image accurately follows the conditional distribution, nor is there a guarantee that the loss decreases at each iteration as suggested by equation (7). Could you provide further clarification or additional theoretical support to address these concerns?\n\n5. Several techniques introduced in the paper lack direct theoretical underpinning:\n- The concept of \"time-travel\" being an Ornstein-Uhlenbeck process is intriguing but lacks a detailed derivation. Could you expand on how the theory of the Ornstein-Uhlenbeck process quantifies the benefits of time-travel in your framework?\n- The selection of hyperparameters seems not to be grounded in theory. Could you discuss the rationale behind these choices and any potential theoretical support?\n\n6. The paper posits that the study of training-free methods can be encapsulated within the hyperparameter space of your framework. However, specific hyperparameter settings critical for the performance in existing works, such as the step sizes used in Face generation (at.sqrt()) and Style transfer ((correction * correction).mean().sqrt().item() * unconditional_guidance_scale / (norm_grad * norm_grad).mean().sqrt().item() * 0.2) in FreeDoM, are not explicitly covered. Could you address the omission of these settings and their impact on the comprehensiveness of your study?\n\n7. The hyperparameter search settings for the baselines in your comparative analysis are not disclosed, raising questions about the fairness and validity of the comparisons. Could you provide more details on these settings to ensure a transparent and equitable comparison?\n\n[1] Feng, Weixi, et al. \"Training-free structured diffusion guidance for compositional text-to-image synthesis.\" ICLR 2023.\n\n[2] Chen, Minghao, et al. \"Training-free layout control with cross-attention guidance.\" WACV 2024.\n\n[3] Mo, Sicheng, et al. \"Freecontrol: Training-free spatial control of any text-to-image diffusion model with any condition.\" CVPR 2024.\n\n[4] Efron, Bradley. \"Tweedie’s formula and selection bias.\" JASA 2011.\n\n[5] Kadkhodaie, Zahra, et al. \"Generalization in diffusion models arises from geometry-adaptive harmonic representation.\" ICLR 2024.", "labeling_timestamp": "2026-01-11T16:58:11.432535", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper's title and statements (e.g., “unified” and “encompassing existing methods”) give the impression of broad/comprehensive coverage, but the paper explicitly reviews and ties only a specific set of prior algorithms (it summarizes five existing approaches in Section 2.1). Thus the reviewer’s critique that the title overreaches is partially supported: the authors claim unification of \"existing methods\" while practically addressing a limited subset in their review and experiments.", "evidence": "Title: \"TFG: Unified Training-Free Guidance for Diffusion Models\" \nAbstract: \"introduces a novel algorithmic framework encompassing existing methods as special cases, unifying the study of training-free guidance into the analysis of an algorithm-agnostic design space.\" \nSection 2.1: \"We review and summarize five existing approaches below\"", "section": "Title; Abstract; Section 2.1"}
{"claim": "The manuscript insufficiently articulates the rationale for focusing specifically on training-free approaches compared to other approaches.", "claim_type": "presentation", "paper_id": "N8YbGX98vc", "paper_title": "TFG: Unified Training-Free Guidance for Diffusion Models", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "6i2pz4NB9X", "reviewer": "Reviewer_Xm6V", "review_text": "Summary: The paper introduces Training-Free Guidance (TFG), a novel framework designed to enhance the generation of samples with desired properties using diffusion models, without necessitating additional model training. TFG aims to resolve the shortcomings of existing training-free methods by offering a unified algorithmic framework that simplifies the comparison and application of such methods across a wide range of tasks. By theoretically and empirically analyzing a hyper-parameter design space within this framework, the authors develop an effective strategy for hyper-parameter selection applicable to various tasks. Their comprehensive benchmarks across multiple tasks and targets demonstrate TFG's superior performance, achieving an average improvement of 7.4% over existing methods.\n\nStrengths: 1. Given the burgeoning interest in the area of training-free guidance, the establishment of a unified benchmark as presented in this paper is a commendable contribution that holds the potential to significantly advance research in this field. The authors' efforts in this direction are highly appreciated and underscore the importance of standardized benchmarks for facilitating future developments.\n\n2. The experiments conducted in this study are extensive, reflecting a high degree of diligence and thoroughness. Such a comprehensive experimental approach is commendable and warrants recognition. Consequently, I believe this aspect of the paper merits a positive evaluation for its contribution to validating the proposed framework and its applicability across a diverse range of tasks.\n\nWeaknesses: There are several areas where improvements could significantly enhance its contributions. Addressing these points satisfactorily would make a strong case for elevating the paper's status to \"Accept\" or \"Strong Accept\".\n\n1. While the paper provides valuable insights into a specific line of training-free guidance, it is important to acknowledge the existence of a broader spectrum of works outside this category. This observation suggests that the title of the paper may slightly overreach, potentially implying a more comprehensive coverage than is actually presented. (Question 1)\n\n2. The paper commendably supports the motivations behind conditional diffusion in Appendix A, offering a solid foundation for its relevance. However, the rationale for focusing specifically on training-free approaches appears less extensively articulated. (Question 2)\n\n3. The authors claimed that they have theoretically grounded their unified framework. However, upon review, the theoretical underpinnings presented seem to require further elaboration to fully substantiate this claim. (Question 3, 4, 5)\n\n4. The authors claimed that \"the studies of training-free methods become the study within the hyperparameter space of our framework\". Nonetheless, it appears that the search space defined within this framework might not fully encompass the range of hyper-parameters considered in existing literature. This limitation could potentially restrict the framework's applicability or comparative analysis capabilities. (Question 6, 7)\n\nQuestions: 1. The field of training-free guidance encompasses a wide array of studies beyond those specifically addressed in this paper (e.g. [1]-[3]). Could you elaborate on how these works relate to the scope and contributions of your paper?\n\n2. Your paper operates under the assumption that the forward model is known, leveraging training-free guidance for solving inverse problems. Given that knowing the forward model allows for the generation of a large number of samples to train a conditional diffusion model at a relatively low computational cost (e.g., 10 A100 GPU hours) and training-based approaches are much better than training-free ones, could you discuss the significant motivations of adopting a training-free approach in this context?\n\n3. The theoretical foundation of your paper seems to rest significantly on Lemma 4.1, which revisits the variance of MMSE estimator of the signal corrupted by Gaussian noise (e.g., (2.8) in [4]). This formula is widely adopted in diffusion papers (e.g., [5]). Given its established nature and previous applications, could you elaborate on how this lemma specifically contributes to the novel aspects of your framework?\n\n4. Concerning Lemma 4.1, it appears there is no assurance that the generated image accurately follows the conditional distribution, nor is there a guarantee that the loss decreases at each iteration as suggested by equation (7). Could you provide further clarification or additional theoretical support to address these concerns?\n\n5. Several techniques introduced in the paper lack direct theoretical underpinning:\n- The concept of \"time-travel\" being an Ornstein-Uhlenbeck process is intriguing but lacks a detailed derivation. Could you expand on how the theory of the Ornstein-Uhlenbeck process quantifies the benefits of time-travel in your framework?\n- The selection of hyperparameters seems not to be grounded in theory. Could you discuss the rationale behind these choices and any potential theoretical support?\n\n6. The paper posits that the study of training-free methods can be encapsulated within the hyperparameter space of your framework. However, specific hyperparameter settings critical for the performance in existing works, such as the step sizes used in Face generation (at.sqrt()) and Style transfer ((correction * correction).mean().sqrt().item() * unconditional_guidance_scale / (norm_grad * norm_grad).mean().sqrt().item() * 0.2) in FreeDoM, are not explicitly covered. Could you address the omission of these settings and their impact on the comprehensiveness of your study?\n\n7. The hyperparameter search settings for the baselines in your comparative analysis are not disclosed, raising questions about the fairness and validity of the comparisons. Could you provide more details on these settings to ensure a transparent and equitable comparison?\n\n[1] Feng, Weixi, et al. \"Training-free structured diffusion guidance for compositional text-to-image synthesis.\" ICLR 2023.\n\n[2] Chen, Minghao, et al. \"Training-free layout control with cross-attention guidance.\" WACV 2024.\n\n[3] Mo, Sicheng, et al. \"Freecontrol: Training-free spatial control of any text-to-image diffusion model with any condition.\" CVPR 2024.\n\n[4] Efron, Bradley. \"Tweedie’s formula and selection bias.\" JASA 2011.\n\n[5] Kadkhodaie, Zahra, et al. \"Generalization in diffusion models arises from geometry-adaptive harmonic representation.\" ICLR 2024.", "labeling_timestamp": "2026-01-11T16:58:11.585624", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly explains why training-free guidance is important and contrasts it with training-based approaches, noting that training-based methods require specialized training for each conditioning signal and are resource-intensive, whereas training-free methods use off-the-shelf predictors without additional training.", "evidence": "\"Conditional generation methods like classifier-based guidance [66, 7] and classifier-free guidance [23] typically require training a specialized model for each conditioning signal (e.g., a noise-conditional classifier or a text-conditional denoiser). This resource-intensive and time-consuming process greatly limits their applicability. In contrast, training-free guidance aims to generate samples that align with certain targets specified through an off-the-shelf differentiable target predictor without involving any additional training.\"", "section": "Introduction"}
{"claim": "The theoretical underpinnings presented in the paper are not elaborated enough to fully substantiate the claimed theoretical grounding of the unified framework.", "claim_type": "subjective", "paper_id": "N8YbGX98vc", "paper_title": "TFG: Unified Training-Free Guidance for Diffusion Models", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "6i2pz4NB9X", "reviewer": "Reviewer_Xm6V", "review_text": "Summary: The paper introduces Training-Free Guidance (TFG), a novel framework designed to enhance the generation of samples with desired properties using diffusion models, without necessitating additional model training. TFG aims to resolve the shortcomings of existing training-free methods by offering a unified algorithmic framework that simplifies the comparison and application of such methods across a wide range of tasks. By theoretically and empirically analyzing a hyper-parameter design space within this framework, the authors develop an effective strategy for hyper-parameter selection applicable to various tasks. Their comprehensive benchmarks across multiple tasks and targets demonstrate TFG's superior performance, achieving an average improvement of 7.4% over existing methods.\n\nStrengths: 1. Given the burgeoning interest in the area of training-free guidance, the establishment of a unified benchmark as presented in this paper is a commendable contribution that holds the potential to significantly advance research in this field. The authors' efforts in this direction are highly appreciated and underscore the importance of standardized benchmarks for facilitating future developments.\n\n2. The experiments conducted in this study are extensive, reflecting a high degree of diligence and thoroughness. Such a comprehensive experimental approach is commendable and warrants recognition. Consequently, I believe this aspect of the paper merits a positive evaluation for its contribution to validating the proposed framework and its applicability across a diverse range of tasks.\n\nWeaknesses: There are several areas where improvements could significantly enhance its contributions. Addressing these points satisfactorily would make a strong case for elevating the paper's status to \"Accept\" or \"Strong Accept\".\n\n1. While the paper provides valuable insights into a specific line of training-free guidance, it is important to acknowledge the existence of a broader spectrum of works outside this category. This observation suggests that the title of the paper may slightly overreach, potentially implying a more comprehensive coverage than is actually presented. (Question 1)\n\n2. The paper commendably supports the motivations behind conditional diffusion in Appendix A, offering a solid foundation for its relevance. However, the rationale for focusing specifically on training-free approaches appears less extensively articulated. (Question 2)\n\n3. The authors claimed that they have theoretically grounded their unified framework. However, upon review, the theoretical underpinnings presented seem to require further elaboration to fully substantiate this claim. (Question 3, 4, 5)\n\n4. The authors claimed that \"the studies of training-free methods become the study within the hyperparameter space of our framework\". Nonetheless, it appears that the search space defined within this framework might not fully encompass the range of hyper-parameters considered in existing literature. This limitation could potentially restrict the framework's applicability or comparative analysis capabilities. (Question 6, 7)\n\nQuestions: 1. The field of training-free guidance encompasses a wide array of studies beyond those specifically addressed in this paper (e.g. [1]-[3]). Could you elaborate on how these works relate to the scope and contributions of your paper?\n\n2. Your paper operates under the assumption that the forward model is known, leveraging training-free guidance for solving inverse problems. Given that knowing the forward model allows for the generation of a large number of samples to train a conditional diffusion model at a relatively low computational cost (e.g., 10 A100 GPU hours) and training-based approaches are much better than training-free ones, could you discuss the significant motivations of adopting a training-free approach in this context?\n\n3. The theoretical foundation of your paper seems to rest significantly on Lemma 4.1, which revisits the variance of MMSE estimator of the signal corrupted by Gaussian noise (e.g., (2.8) in [4]). This formula is widely adopted in diffusion papers (e.g., [5]). Given its established nature and previous applications, could you elaborate on how this lemma specifically contributes to the novel aspects of your framework?\n\n4. Concerning Lemma 4.1, it appears there is no assurance that the generated image accurately follows the conditional distribution, nor is there a guarantee that the loss decreases at each iteration as suggested by equation (7). Could you provide further clarification or additional theoretical support to address these concerns?\n\n5. Several techniques introduced in the paper lack direct theoretical underpinning:\n- The concept of \"time-travel\" being an Ornstein-Uhlenbeck process is intriguing but lacks a detailed derivation. Could you expand on how the theory of the Ornstein-Uhlenbeck process quantifies the benefits of time-travel in your framework?\n- The selection of hyperparameters seems not to be grounded in theory. Could you discuss the rationale behind these choices and any potential theoretical support?\n\n6. The paper posits that the study of training-free methods can be encapsulated within the hyperparameter space of your framework. However, specific hyperparameter settings critical for the performance in existing works, such as the step sizes used in Face generation (at.sqrt()) and Style transfer ((correction * correction).mean().sqrt().item() * unconditional_guidance_scale / (norm_grad * norm_grad).mean().sqrt().item() * 0.2) in FreeDoM, are not explicitly covered. Could you address the omission of these settings and their impact on the comprehensiveness of your study?\n\n7. The hyperparameter search settings for the baselines in your comparative analysis are not disclosed, raising questions about the fairness and validity of the comparisons. Could you provide more details on these settings to ensure a transparent and equitable comparison?\n\n[1] Feng, Weixi, et al. \"Training-free structured diffusion guidance for compositional text-to-image synthesis.\" ICLR 2023.\n\n[2] Chen, Minghao, et al. \"Training-free layout control with cross-attention guidance.\" WACV 2024.\n\n[3] Mo, Sicheng, et al. \"Freecontrol: Training-free spatial control of any text-to-image diffusion model with any condition.\" CVPR 2024.\n\n[4] Efron, Bradley. \"Tweedie’s formula and selection bias.\" JASA 2011.\n\n[5] Kadkhodaie, Zahra, et al. \"Generalization in diffusion models arises from geometry-adaptive harmonic representation.\" ICLR 2024.", "labeling_timestamp": "2026-01-11T16:58:11.287085", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper repeatedly claims to provide theoretical analysis and motivation for the unified framework (Abstract; Introduction; Sec. 3.1), but the provided excerpt does not include the detailed theoretical derivations, proofs, or depth needed to judge whether the theoretical underpinnings are elaborated enough to fully substantiate the claimed grounding. Therefore, from the available content it is not possible to determine if the theoretical treatment is sufficient.", "evidence": "1) \"Via theoretical and empirical investigation, we propose an efficient and effective hyper-parameter searching strategy...\" (Abstract). 2) \"Within our framework, we analyze the underlying theoretical motivation of each hyper-parameter and conduct comprehensive experiments to identify their influence.\" (Introduction). 3) \"This sections introduces our unified framework for training-free guidance (TFG, Algorithm 1) and formally defines its design space in Definition 3.1. We demonstrate the advantage of TFG by drawing connections between TFG and other algorithm\" (Sec. 3.1).", "section": "Abstract; Introduction; 3.1 Unification and extension"}
{"claim": "The authors' claim that training-free studies reduce to a hyperparameter study within their framework is unsupported because the framework's search space may omit important hyperparameters used in prior works.", "claim_type": "methodology", "paper_id": "N8YbGX98vc", "paper_title": "TFG: Unified Training-Free Guidance for Diffusion Models", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "6i2pz4NB9X", "reviewer": "Reviewer_Xm6V", "review_text": "Summary: The paper introduces Training-Free Guidance (TFG), a novel framework designed to enhance the generation of samples with desired properties using diffusion models, without necessitating additional model training. TFG aims to resolve the shortcomings of existing training-free methods by offering a unified algorithmic framework that simplifies the comparison and application of such methods across a wide range of tasks. By theoretically and empirically analyzing a hyper-parameter design space within this framework, the authors develop an effective strategy for hyper-parameter selection applicable to various tasks. Their comprehensive benchmarks across multiple tasks and targets demonstrate TFG's superior performance, achieving an average improvement of 7.4% over existing methods.\n\nStrengths: 1. Given the burgeoning interest in the area of training-free guidance, the establishment of a unified benchmark as presented in this paper is a commendable contribution that holds the potential to significantly advance research in this field. The authors' efforts in this direction are highly appreciated and underscore the importance of standardized benchmarks for facilitating future developments.\n\n2. The experiments conducted in this study are extensive, reflecting a high degree of diligence and thoroughness. Such a comprehensive experimental approach is commendable and warrants recognition. Consequently, I believe this aspect of the paper merits a positive evaluation for its contribution to validating the proposed framework and its applicability across a diverse range of tasks.\n\nWeaknesses: There are several areas where improvements could significantly enhance its contributions. Addressing these points satisfactorily would make a strong case for elevating the paper's status to \"Accept\" or \"Strong Accept\".\n\n1. While the paper provides valuable insights into a specific line of training-free guidance, it is important to acknowledge the existence of a broader spectrum of works outside this category. This observation suggests that the title of the paper may slightly overreach, potentially implying a more comprehensive coverage than is actually presented. (Question 1)\n\n2. The paper commendably supports the motivations behind conditional diffusion in Appendix A, offering a solid foundation for its relevance. However, the rationale for focusing specifically on training-free approaches appears less extensively articulated. (Question 2)\n\n3. The authors claimed that they have theoretically grounded their unified framework. However, upon review, the theoretical underpinnings presented seem to require further elaboration to fully substantiate this claim. (Question 3, 4, 5)\n\n4. The authors claimed that \"the studies of training-free methods become the study within the hyperparameter space of our framework\". Nonetheless, it appears that the search space defined within this framework might not fully encompass the range of hyper-parameters considered in existing literature. This limitation could potentially restrict the framework's applicability or comparative analysis capabilities. (Question 6, 7)\n\nQuestions: 1. The field of training-free guidance encompasses a wide array of studies beyond those specifically addressed in this paper (e.g. [1]-[3]). Could you elaborate on how these works relate to the scope and contributions of your paper?\n\n2. Your paper operates under the assumption that the forward model is known, leveraging training-free guidance for solving inverse problems. Given that knowing the forward model allows for the generation of a large number of samples to train a conditional diffusion model at a relatively low computational cost (e.g., 10 A100 GPU hours) and training-based approaches are much better than training-free ones, could you discuss the significant motivations of adopting a training-free approach in this context?\n\n3. The theoretical foundation of your paper seems to rest significantly on Lemma 4.1, which revisits the variance of MMSE estimator of the signal corrupted by Gaussian noise (e.g., (2.8) in [4]). This formula is widely adopted in diffusion papers (e.g., [5]). Given its established nature and previous applications, could you elaborate on how this lemma specifically contributes to the novel aspects of your framework?\n\n4. Concerning Lemma 4.1, it appears there is no assurance that the generated image accurately follows the conditional distribution, nor is there a guarantee that the loss decreases at each iteration as suggested by equation (7). Could you provide further clarification or additional theoretical support to address these concerns?\n\n5. Several techniques introduced in the paper lack direct theoretical underpinning:\n- The concept of \"time-travel\" being an Ornstein-Uhlenbeck process is intriguing but lacks a detailed derivation. Could you expand on how the theory of the Ornstein-Uhlenbeck process quantifies the benefits of time-travel in your framework?\n- The selection of hyperparameters seems not to be grounded in theory. Could you discuss the rationale behind these choices and any potential theoretical support?\n\n6. The paper posits that the study of training-free methods can be encapsulated within the hyperparameter space of your framework. However, specific hyperparameter settings critical for the performance in existing works, such as the step sizes used in Face generation (at.sqrt()) and Style transfer ((correction * correction).mean().sqrt().item() * unconditional_guidance_scale / (norm_grad * norm_grad).mean().sqrt().item() * 0.2) in FreeDoM, are not explicitly covered. Could you address the omission of these settings and their impact on the comprehensiveness of your study?\n\n7. The hyperparameter search settings for the baselines in your comparative analysis are not disclosed, raising questions about the fairness and validity of the comparisons. Could you provide more details on these settings to ensure a transparent and equitable comparison?\n\n[1] Feng, Weixi, et al. \"Training-free structured diffusion guidance for compositional text-to-image synthesis.\" ICLR 2023.\n\n[2] Chen, Minghao, et al. \"Training-free layout control with cross-attention guidance.\" WACV 2024.\n\n[3] Mo, Sicheng, et al. \"Freecontrol: Training-free spatial control of any text-to-image diffusion model with any condition.\" CVPR 2024.\n\n[4] Efron, Bradley. \"Tweedie’s formula and selection bias.\" JASA 2011.\n\n[5] Kadkhodaie, Zahra, et al. \"Generalization in diffusion models arises from geometry-adaptive harmonic representation.\" ICLR 2024.", "labeling_timestamp": "2026-01-11T16:58:40.457600", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly claims that existing training-free methods are encompassed by the TFG design space and that the study reduces to analysis of hyper-parameter choices. Thus the reviewer's statement that this claim is unsupported (due to possible omitted hyperparameters) contradicts the paper's stated claim and intent to include prior methods as special cases.", "evidence": "“We show that existing approaches are special cases of the TFG as they correspond to particular hyper-parameter subspace in our unified space. In other words, TFG naturally simplifies and reduces the study of training-free guidance, as well as the comparisons between existing methods, into the analysis of hyper-parameter choices in our unified design space.”", "section": "1 Introduction"}
{"claim": "Lemma 4.1 appears to restate an established MMSE variance formula widely used in prior diffusion literature, leaving unclear its novel contribution to the framework.", "claim_type": "novelty", "paper_id": "N8YbGX98vc", "paper_title": "TFG: Unified Training-Free Guidance for Diffusion Models", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "6i2pz4NB9X", "reviewer": "Reviewer_Xm6V", "review_text": "Summary: The paper introduces Training-Free Guidance (TFG), a novel framework designed to enhance the generation of samples with desired properties using diffusion models, without necessitating additional model training. TFG aims to resolve the shortcomings of existing training-free methods by offering a unified algorithmic framework that simplifies the comparison and application of such methods across a wide range of tasks. By theoretically and empirically analyzing a hyper-parameter design space within this framework, the authors develop an effective strategy for hyper-parameter selection applicable to various tasks. Their comprehensive benchmarks across multiple tasks and targets demonstrate TFG's superior performance, achieving an average improvement of 7.4% over existing methods.\n\nStrengths: 1. Given the burgeoning interest in the area of training-free guidance, the establishment of a unified benchmark as presented in this paper is a commendable contribution that holds the potential to significantly advance research in this field. The authors' efforts in this direction are highly appreciated and underscore the importance of standardized benchmarks for facilitating future developments.\n\n2. The experiments conducted in this study are extensive, reflecting a high degree of diligence and thoroughness. Such a comprehensive experimental approach is commendable and warrants recognition. Consequently, I believe this aspect of the paper merits a positive evaluation for its contribution to validating the proposed framework and its applicability across a diverse range of tasks.\n\nWeaknesses: There are several areas where improvements could significantly enhance its contributions. Addressing these points satisfactorily would make a strong case for elevating the paper's status to \"Accept\" or \"Strong Accept\".\n\n1. While the paper provides valuable insights into a specific line of training-free guidance, it is important to acknowledge the existence of a broader spectrum of works outside this category. This observation suggests that the title of the paper may slightly overreach, potentially implying a more comprehensive coverage than is actually presented. (Question 1)\n\n2. The paper commendably supports the motivations behind conditional diffusion in Appendix A, offering a solid foundation for its relevance. However, the rationale for focusing specifically on training-free approaches appears less extensively articulated. (Question 2)\n\n3. The authors claimed that they have theoretically grounded their unified framework. However, upon review, the theoretical underpinnings presented seem to require further elaboration to fully substantiate this claim. (Question 3, 4, 5)\n\n4. The authors claimed that \"the studies of training-free methods become the study within the hyperparameter space of our framework\". Nonetheless, it appears that the search space defined within this framework might not fully encompass the range of hyper-parameters considered in existing literature. This limitation could potentially restrict the framework's applicability or comparative analysis capabilities. (Question 6, 7)\n\nQuestions: 1. The field of training-free guidance encompasses a wide array of studies beyond those specifically addressed in this paper (e.g. [1]-[3]). Could you elaborate on how these works relate to the scope and contributions of your paper?\n\n2. Your paper operates under the assumption that the forward model is known, leveraging training-free guidance for solving inverse problems. Given that knowing the forward model allows for the generation of a large number of samples to train a conditional diffusion model at a relatively low computational cost (e.g., 10 A100 GPU hours) and training-based approaches are much better than training-free ones, could you discuss the significant motivations of adopting a training-free approach in this context?\n\n3. The theoretical foundation of your paper seems to rest significantly on Lemma 4.1, which revisits the variance of MMSE estimator of the signal corrupted by Gaussian noise (e.g., (2.8) in [4]). This formula is widely adopted in diffusion papers (e.g., [5]). Given its established nature and previous applications, could you elaborate on how this lemma specifically contributes to the novel aspects of your framework?\n\n4. Concerning Lemma 4.1, it appears there is no assurance that the generated image accurately follows the conditional distribution, nor is there a guarantee that the loss decreases at each iteration as suggested by equation (7). Could you provide further clarification or additional theoretical support to address these concerns?\n\n5. Several techniques introduced in the paper lack direct theoretical underpinning:\n- The concept of \"time-travel\" being an Ornstein-Uhlenbeck process is intriguing but lacks a detailed derivation. Could you expand on how the theory of the Ornstein-Uhlenbeck process quantifies the benefits of time-travel in your framework?\n- The selection of hyperparameters seems not to be grounded in theory. Could you discuss the rationale behind these choices and any potential theoretical support?\n\n6. The paper posits that the study of training-free methods can be encapsulated within the hyperparameter space of your framework. However, specific hyperparameter settings critical for the performance in existing works, such as the step sizes used in Face generation (at.sqrt()) and Style transfer ((correction * correction).mean().sqrt().item() * unconditional_guidance_scale / (norm_grad * norm_grad).mean().sqrt().item() * 0.2) in FreeDoM, are not explicitly covered. Could you address the omission of these settings and their impact on the comprehensiveness of your study?\n\n7. The hyperparameter search settings for the baselines in your comparative analysis are not disclosed, raising questions about the fairness and validity of the comparisons. Could you provide more details on these settings to ensure a transparent and equitable comparison?\n\n[1] Feng, Weixi, et al. \"Training-free structured diffusion guidance for compositional text-to-image synthesis.\" ICLR 2023.\n\n[2] Chen, Minghao, et al. \"Training-free layout control with cross-attention guidance.\" WACV 2024.\n\n[3] Mo, Sicheng, et al. \"Freecontrol: Training-free spatial control of any text-to-image diffusion model with any condition.\" CVPR 2024.\n\n[4] Efron, Bradley. \"Tweedie’s formula and selection bias.\" JASA 2011.\n\n[5] Kadkhodaie, Zahra, et al. \"Generalization in diffusion models arises from geometry-adaptive harmonic representation.\" ICLR 2024.", "labeling_timestamp": "2026-01-11T16:58:42.556643", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not include Lemma 4.1 or its statement, so there is insufficient information to judge whether that lemma merely restates an established MMSE variance formula or provides a novel contribution.", "evidence": "\"Within our framework, we analyze the underlying theoretical motivation of each hyper-parameter and conduct comprehensive experiments to identify their influence.\" \n\n\"This sections introduces our unified framework for training-free guidance (TFG, Algorithm 1) and formally defines its design space in Definition 3.1.\"", "section": "Introduction; 3.1 Unification and extension"}
{"claim": "There is no provided assurance that generated images produced by the method follow the intended conditional distribution as asserted in the paper.", "claim_type": "experimental", "paper_id": "N8YbGX98vc", "paper_title": "TFG: Unified Training-Free Guidance for Diffusion Models", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "6i2pz4NB9X", "reviewer": "Reviewer_Xm6V", "review_text": "Summary: The paper introduces Training-Free Guidance (TFG), a novel framework designed to enhance the generation of samples with desired properties using diffusion models, without necessitating additional model training. TFG aims to resolve the shortcomings of existing training-free methods by offering a unified algorithmic framework that simplifies the comparison and application of such methods across a wide range of tasks. By theoretically and empirically analyzing a hyper-parameter design space within this framework, the authors develop an effective strategy for hyper-parameter selection applicable to various tasks. Their comprehensive benchmarks across multiple tasks and targets demonstrate TFG's superior performance, achieving an average improvement of 7.4% over existing methods.\n\nStrengths: 1. Given the burgeoning interest in the area of training-free guidance, the establishment of a unified benchmark as presented in this paper is a commendable contribution that holds the potential to significantly advance research in this field. The authors' efforts in this direction are highly appreciated and underscore the importance of standardized benchmarks for facilitating future developments.\n\n2. The experiments conducted in this study are extensive, reflecting a high degree of diligence and thoroughness. Such a comprehensive experimental approach is commendable and warrants recognition. Consequently, I believe this aspect of the paper merits a positive evaluation for its contribution to validating the proposed framework and its applicability across a diverse range of tasks.\n\nWeaknesses: There are several areas where improvements could significantly enhance its contributions. Addressing these points satisfactorily would make a strong case for elevating the paper's status to \"Accept\" or \"Strong Accept\".\n\n1. While the paper provides valuable insights into a specific line of training-free guidance, it is important to acknowledge the existence of a broader spectrum of works outside this category. This observation suggests that the title of the paper may slightly overreach, potentially implying a more comprehensive coverage than is actually presented. (Question 1)\n\n2. The paper commendably supports the motivations behind conditional diffusion in Appendix A, offering a solid foundation for its relevance. However, the rationale for focusing specifically on training-free approaches appears less extensively articulated. (Question 2)\n\n3. The authors claimed that they have theoretically grounded their unified framework. However, upon review, the theoretical underpinnings presented seem to require further elaboration to fully substantiate this claim. (Question 3, 4, 5)\n\n4. The authors claimed that \"the studies of training-free methods become the study within the hyperparameter space of our framework\". Nonetheless, it appears that the search space defined within this framework might not fully encompass the range of hyper-parameters considered in existing literature. This limitation could potentially restrict the framework's applicability or comparative analysis capabilities. (Question 6, 7)\n\nQuestions: 1. The field of training-free guidance encompasses a wide array of studies beyond those specifically addressed in this paper (e.g. [1]-[3]). Could you elaborate on how these works relate to the scope and contributions of your paper?\n\n2. Your paper operates under the assumption that the forward model is known, leveraging training-free guidance for solving inverse problems. Given that knowing the forward model allows for the generation of a large number of samples to train a conditional diffusion model at a relatively low computational cost (e.g., 10 A100 GPU hours) and training-based approaches are much better than training-free ones, could you discuss the significant motivations of adopting a training-free approach in this context?\n\n3. The theoretical foundation of your paper seems to rest significantly on Lemma 4.1, which revisits the variance of MMSE estimator of the signal corrupted by Gaussian noise (e.g., (2.8) in [4]). This formula is widely adopted in diffusion papers (e.g., [5]). Given its established nature and previous applications, could you elaborate on how this lemma specifically contributes to the novel aspects of your framework?\n\n4. Concerning Lemma 4.1, it appears there is no assurance that the generated image accurately follows the conditional distribution, nor is there a guarantee that the loss decreases at each iteration as suggested by equation (7). Could you provide further clarification or additional theoretical support to address these concerns?\n\n5. Several techniques introduced in the paper lack direct theoretical underpinning:\n- The concept of \"time-travel\" being an Ornstein-Uhlenbeck process is intriguing but lacks a detailed derivation. Could you expand on how the theory of the Ornstein-Uhlenbeck process quantifies the benefits of time-travel in your framework?\n- The selection of hyperparameters seems not to be grounded in theory. Could you discuss the rationale behind these choices and any potential theoretical support?\n\n6. The paper posits that the study of training-free methods can be encapsulated within the hyperparameter space of your framework. However, specific hyperparameter settings critical for the performance in existing works, such as the step sizes used in Face generation (at.sqrt()) and Style transfer ((correction * correction).mean().sqrt().item() * unconditional_guidance_scale / (norm_grad * norm_grad).mean().sqrt().item() * 0.2) in FreeDoM, are not explicitly covered. Could you address the omission of these settings and their impact on the comprehensiveness of your study?\n\n7. The hyperparameter search settings for the baselines in your comparative analysis are not disclosed, raising questions about the fairness and validity of the comparisons. Could you provide more details on these settings to ensure a transparent and equitable comparison?\n\n[1] Feng, Weixi, et al. \"Training-free structured diffusion guidance for compositional text-to-image synthesis.\" ICLR 2023.\n\n[2] Chen, Minghao, et al. \"Training-free layout control with cross-attention guidance.\" WACV 2024.\n\n[3] Mo, Sicheng, et al. \"Freecontrol: Training-free spatial control of any text-to-image diffusion model with any condition.\" CVPR 2024.\n\n[4] Efron, Bradley. \"Tweedie’s formula and selection bias.\" JASA 2011.\n\n[5] Kadkhodaie, Zahra, et al. \"Generalization in diffusion models arises from geometry-adaptive harmonic representation.\" ICLR 2024.", "labeling_timestamp": "2026-01-11T16:58:48.782919", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly states that training-based guidance can yield unbiased samples from the conditional distribution, but repeatedly emphasizes that training-free guidance is more difficult and does not provide a corresponding assurance or theoretical guarantee that its outputs follow the intended conditional distribution. The provided content contains theoretical guarantees for training-based methods but only empirical/theoretical analysis of hyperparameters for TFG, not a proof that TFG samples exactly from p0(x|c).", "evidence": ["if we denote the trained classifier as f ( x t ) (that implicitly depends on c and model parameters), we can replace ϵ θ ( x t , t ) in Equation (3) by ϵ θ ( x t , t ) - √ 1 -¯ α t ∇ x t log f ( x t ) upon sampling to obtain unbiased sample x 0 ∼ p 0 ( x 0 | c ) .", "This paper in contrast focuses on conditional generation in a training-free manner: given a diffusion model ϵ θ ( x , t ) and an off-the-shelf target predictor f ( x ) (we omit the subscript c below), we aim to generate samples from p 0 ( x | c ) without any additional training. Unlike training-based methods that can accurately estimate f ( x t , t ) , training-free guidance is significantly more difficult since it involves guiding a noisy data x t using f ( x ) defined over the clean data space.", "Existing methods, though effective in various individual applications, often lack theoretical grounding and rigorous testing on extensive benchmarks."], "section": ["2 Background", "2 Background", "Abstract"]}
{"claim": "Equation (7) is presented without a guarantee or proof that the loss decreases at each iteration as the equation suggests.", "claim_type": "methodology", "paper_id": "N8YbGX98vc", "paper_title": "TFG: Unified Training-Free Guidance for Diffusion Models", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "6i2pz4NB9X", "reviewer": "Reviewer_Xm6V", "review_text": "Summary: The paper introduces Training-Free Guidance (TFG), a novel framework designed to enhance the generation of samples with desired properties using diffusion models, without necessitating additional model training. TFG aims to resolve the shortcomings of existing training-free methods by offering a unified algorithmic framework that simplifies the comparison and application of such methods across a wide range of tasks. By theoretically and empirically analyzing a hyper-parameter design space within this framework, the authors develop an effective strategy for hyper-parameter selection applicable to various tasks. Their comprehensive benchmarks across multiple tasks and targets demonstrate TFG's superior performance, achieving an average improvement of 7.4% over existing methods.\n\nStrengths: 1. Given the burgeoning interest in the area of training-free guidance, the establishment of a unified benchmark as presented in this paper is a commendable contribution that holds the potential to significantly advance research in this field. The authors' efforts in this direction are highly appreciated and underscore the importance of standardized benchmarks for facilitating future developments.\n\n2. The experiments conducted in this study are extensive, reflecting a high degree of diligence and thoroughness. Such a comprehensive experimental approach is commendable and warrants recognition. Consequently, I believe this aspect of the paper merits a positive evaluation for its contribution to validating the proposed framework and its applicability across a diverse range of tasks.\n\nWeaknesses: There are several areas where improvements could significantly enhance its contributions. Addressing these points satisfactorily would make a strong case for elevating the paper's status to \"Accept\" or \"Strong Accept\".\n\n1. While the paper provides valuable insights into a specific line of training-free guidance, it is important to acknowledge the existence of a broader spectrum of works outside this category. This observation suggests that the title of the paper may slightly overreach, potentially implying a more comprehensive coverage than is actually presented. (Question 1)\n\n2. The paper commendably supports the motivations behind conditional diffusion in Appendix A, offering a solid foundation for its relevance. However, the rationale for focusing specifically on training-free approaches appears less extensively articulated. (Question 2)\n\n3. The authors claimed that they have theoretically grounded their unified framework. However, upon review, the theoretical underpinnings presented seem to require further elaboration to fully substantiate this claim. (Question 3, 4, 5)\n\n4. The authors claimed that \"the studies of training-free methods become the study within the hyperparameter space of our framework\". Nonetheless, it appears that the search space defined within this framework might not fully encompass the range of hyper-parameters considered in existing literature. This limitation could potentially restrict the framework's applicability or comparative analysis capabilities. (Question 6, 7)\n\nQuestions: 1. The field of training-free guidance encompasses a wide array of studies beyond those specifically addressed in this paper (e.g. [1]-[3]). Could you elaborate on how these works relate to the scope and contributions of your paper?\n\n2. Your paper operates under the assumption that the forward model is known, leveraging training-free guidance for solving inverse problems. Given that knowing the forward model allows for the generation of a large number of samples to train a conditional diffusion model at a relatively low computational cost (e.g., 10 A100 GPU hours) and training-based approaches are much better than training-free ones, could you discuss the significant motivations of adopting a training-free approach in this context?\n\n3. The theoretical foundation of your paper seems to rest significantly on Lemma 4.1, which revisits the variance of MMSE estimator of the signal corrupted by Gaussian noise (e.g., (2.8) in [4]). This formula is widely adopted in diffusion papers (e.g., [5]). Given its established nature and previous applications, could you elaborate on how this lemma specifically contributes to the novel aspects of your framework?\n\n4. Concerning Lemma 4.1, it appears there is no assurance that the generated image accurately follows the conditional distribution, nor is there a guarantee that the loss decreases at each iteration as suggested by equation (7). Could you provide further clarification or additional theoretical support to address these concerns?\n\n5. Several techniques introduced in the paper lack direct theoretical underpinning:\n- The concept of \"time-travel\" being an Ornstein-Uhlenbeck process is intriguing but lacks a detailed derivation. Could you expand on how the theory of the Ornstein-Uhlenbeck process quantifies the benefits of time-travel in your framework?\n- The selection of hyperparameters seems not to be grounded in theory. Could you discuss the rationale behind these choices and any potential theoretical support?\n\n6. The paper posits that the study of training-free methods can be encapsulated within the hyperparameter space of your framework. However, specific hyperparameter settings critical for the performance in existing works, such as the step sizes used in Face generation (at.sqrt()) and Style transfer ((correction * correction).mean().sqrt().item() * unconditional_guidance_scale / (norm_grad * norm_grad).mean().sqrt().item() * 0.2) in FreeDoM, are not explicitly covered. Could you address the omission of these settings and their impact on the comprehensiveness of your study?\n\n7. The hyperparameter search settings for the baselines in your comparative analysis are not disclosed, raising questions about the fairness and validity of the comparisons. Could you provide more details on these settings to ensure a transparent and equitable comparison?\n\n[1] Feng, Weixi, et al. \"Training-free structured diffusion guidance for compositional text-to-image synthesis.\" ICLR 2023.\n\n[2] Chen, Minghao, et al. \"Training-free layout control with cross-attention guidance.\" WACV 2024.\n\n[3] Mo, Sicheng, et al. \"Freecontrol: Training-free spatial control of any text-to-image diffusion model with any condition.\" CVPR 2024.\n\n[4] Efron, Bradley. \"Tweedie’s formula and selection bias.\" JASA 2011.\n\n[5] Kadkhodaie, Zahra, et al. \"Generalization in diffusion models arises from geometry-adaptive harmonic representation.\" ICLR 2024.", "labeling_timestamp": "2026-01-11T16:58:45.518210", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not contain Equation (7) nor any surrounding discussion or a proof/guarantee about loss decreasing per iteration. Therefore there is insufficient information to determine whether Eq. (7) is presented without a guarantee or proof.", "evidence": "References in the excerpt only mention earlier equations (e.g., \"replace ϵ_θ (x_t, t) in Equation (3)\" and \"DPS replaces ∇ log f(x_t, t) in Equation (5) by ...\"). No Equation (7) or a proof/guarantee is present in the provided text.", "section": "Sections 2 and 2.1 (Existing algorithms)"}
{"claim": "The concept of 'time-travel' being modeled as an Ornstein-Uhlenbeck process lacks a detailed derivation or justification within the paper.", "claim_type": "methodology", "paper_id": "N8YbGX98vc", "paper_title": "TFG: Unified Training-Free Guidance for Diffusion Models", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "6i2pz4NB9X", "reviewer": "Reviewer_Xm6V", "review_text": "Summary: The paper introduces Training-Free Guidance (TFG), a novel framework designed to enhance the generation of samples with desired properties using diffusion models, without necessitating additional model training. TFG aims to resolve the shortcomings of existing training-free methods by offering a unified algorithmic framework that simplifies the comparison and application of such methods across a wide range of tasks. By theoretically and empirically analyzing a hyper-parameter design space within this framework, the authors develop an effective strategy for hyper-parameter selection applicable to various tasks. Their comprehensive benchmarks across multiple tasks and targets demonstrate TFG's superior performance, achieving an average improvement of 7.4% over existing methods.\n\nStrengths: 1. Given the burgeoning interest in the area of training-free guidance, the establishment of a unified benchmark as presented in this paper is a commendable contribution that holds the potential to significantly advance research in this field. The authors' efforts in this direction are highly appreciated and underscore the importance of standardized benchmarks for facilitating future developments.\n\n2. The experiments conducted in this study are extensive, reflecting a high degree of diligence and thoroughness. Such a comprehensive experimental approach is commendable and warrants recognition. Consequently, I believe this aspect of the paper merits a positive evaluation for its contribution to validating the proposed framework and its applicability across a diverse range of tasks.\n\nWeaknesses: There are several areas where improvements could significantly enhance its contributions. Addressing these points satisfactorily would make a strong case for elevating the paper's status to \"Accept\" or \"Strong Accept\".\n\n1. While the paper provides valuable insights into a specific line of training-free guidance, it is important to acknowledge the existence of a broader spectrum of works outside this category. This observation suggests that the title of the paper may slightly overreach, potentially implying a more comprehensive coverage than is actually presented. (Question 1)\n\n2. The paper commendably supports the motivations behind conditional diffusion in Appendix A, offering a solid foundation for its relevance. However, the rationale for focusing specifically on training-free approaches appears less extensively articulated. (Question 2)\n\n3. The authors claimed that they have theoretically grounded their unified framework. However, upon review, the theoretical underpinnings presented seem to require further elaboration to fully substantiate this claim. (Question 3, 4, 5)\n\n4. The authors claimed that \"the studies of training-free methods become the study within the hyperparameter space of our framework\". Nonetheless, it appears that the search space defined within this framework might not fully encompass the range of hyper-parameters considered in existing literature. This limitation could potentially restrict the framework's applicability or comparative analysis capabilities. (Question 6, 7)\n\nQuestions: 1. The field of training-free guidance encompasses a wide array of studies beyond those specifically addressed in this paper (e.g. [1]-[3]). Could you elaborate on how these works relate to the scope and contributions of your paper?\n\n2. Your paper operates under the assumption that the forward model is known, leveraging training-free guidance for solving inverse problems. Given that knowing the forward model allows for the generation of a large number of samples to train a conditional diffusion model at a relatively low computational cost (e.g., 10 A100 GPU hours) and training-based approaches are much better than training-free ones, could you discuss the significant motivations of adopting a training-free approach in this context?\n\n3. The theoretical foundation of your paper seems to rest significantly on Lemma 4.1, which revisits the variance of MMSE estimator of the signal corrupted by Gaussian noise (e.g., (2.8) in [4]). This formula is widely adopted in diffusion papers (e.g., [5]). Given its established nature and previous applications, could you elaborate on how this lemma specifically contributes to the novel aspects of your framework?\n\n4. Concerning Lemma 4.1, it appears there is no assurance that the generated image accurately follows the conditional distribution, nor is there a guarantee that the loss decreases at each iteration as suggested by equation (7). Could you provide further clarification or additional theoretical support to address these concerns?\n\n5. Several techniques introduced in the paper lack direct theoretical underpinning:\n- The concept of \"time-travel\" being an Ornstein-Uhlenbeck process is intriguing but lacks a detailed derivation. Could you expand on how the theory of the Ornstein-Uhlenbeck process quantifies the benefits of time-travel in your framework?\n- The selection of hyperparameters seems not to be grounded in theory. Could you discuss the rationale behind these choices and any potential theoretical support?\n\n6. The paper posits that the study of training-free methods can be encapsulated within the hyperparameter space of your framework. However, specific hyperparameter settings critical for the performance in existing works, such as the step sizes used in Face generation (at.sqrt()) and Style transfer ((correction * correction).mean().sqrt().item() * unconditional_guidance_scale / (norm_grad * norm_grad).mean().sqrt().item() * 0.2) in FreeDoM, are not explicitly covered. Could you address the omission of these settings and their impact on the comprehensiveness of your study?\n\n7. The hyperparameter search settings for the baselines in your comparative analysis are not disclosed, raising questions about the fairness and validity of the comparisons. Could you provide more details on these settings to ensure a transparent and equitable comparison?\n\n[1] Feng, Weixi, et al. \"Training-free structured diffusion guidance for compositional text-to-image synthesis.\" ICLR 2023.\n\n[2] Chen, Minghao, et al. \"Training-free layout control with cross-attention guidance.\" WACV 2024.\n\n[3] Mo, Sicheng, et al. \"Freecontrol: Training-free spatial control of any text-to-image diffusion model with any condition.\" CVPR 2024.\n\n[4] Efron, Bradley. \"Tweedie’s formula and selection bias.\" JASA 2011.\n\n[5] Kadkhodaie, Zahra, et al. \"Generalization in diffusion models arises from geometry-adaptive harmonic representation.\" ICLR 2024.", "labeling_timestamp": "2026-01-11T16:58:59.312111", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper mentions a 'time-travel strategy' (a recurrent denoise/add-noise procedure) in Section 2.1 but the provided content does not present any formulation or derivation modeling this strategy as an Ornstein–Uhlenbeck process, nor any detailed justification of such a modeling choice. Therefore the claim about OU-modeling and its lack of derivation cannot be verified from the paper excerpt.", "evidence": "FreeDoM [78] generalizes DPS by introducing a 'recurrent strategy' (called 'time-travel strategy' [39, 10, 70]) that iteratively denoises x_{t-1} from x_t and adds noise to x_{t-1} to regenerate x_t back and forth. This strategy empirically enhances the strength of the guidance at the cost of additional computation.", "section": "Section 2.1 (Existing algorithms)"}
{"claim": "Key hyperparameter selection choices in the proposed method are not grounded in theory or given principled justification in the manuscript.", "claim_type": "methodology", "paper_id": "N8YbGX98vc", "paper_title": "TFG: Unified Training-Free Guidance for Diffusion Models", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "6i2pz4NB9X", "reviewer": "Reviewer_Xm6V", "review_text": "Summary: The paper introduces Training-Free Guidance (TFG), a novel framework designed to enhance the generation of samples with desired properties using diffusion models, without necessitating additional model training. TFG aims to resolve the shortcomings of existing training-free methods by offering a unified algorithmic framework that simplifies the comparison and application of such methods across a wide range of tasks. By theoretically and empirically analyzing a hyper-parameter design space within this framework, the authors develop an effective strategy for hyper-parameter selection applicable to various tasks. Their comprehensive benchmarks across multiple tasks and targets demonstrate TFG's superior performance, achieving an average improvement of 7.4% over existing methods.\n\nStrengths: 1. Given the burgeoning interest in the area of training-free guidance, the establishment of a unified benchmark as presented in this paper is a commendable contribution that holds the potential to significantly advance research in this field. The authors' efforts in this direction are highly appreciated and underscore the importance of standardized benchmarks for facilitating future developments.\n\n2. The experiments conducted in this study are extensive, reflecting a high degree of diligence and thoroughness. Such a comprehensive experimental approach is commendable and warrants recognition. Consequently, I believe this aspect of the paper merits a positive evaluation for its contribution to validating the proposed framework and its applicability across a diverse range of tasks.\n\nWeaknesses: There are several areas where improvements could significantly enhance its contributions. Addressing these points satisfactorily would make a strong case for elevating the paper's status to \"Accept\" or \"Strong Accept\".\n\n1. While the paper provides valuable insights into a specific line of training-free guidance, it is important to acknowledge the existence of a broader spectrum of works outside this category. This observation suggests that the title of the paper may slightly overreach, potentially implying a more comprehensive coverage than is actually presented. (Question 1)\n\n2. The paper commendably supports the motivations behind conditional diffusion in Appendix A, offering a solid foundation for its relevance. However, the rationale for focusing specifically on training-free approaches appears less extensively articulated. (Question 2)\n\n3. The authors claimed that they have theoretically grounded their unified framework. However, upon review, the theoretical underpinnings presented seem to require further elaboration to fully substantiate this claim. (Question 3, 4, 5)\n\n4. The authors claimed that \"the studies of training-free methods become the study within the hyperparameter space of our framework\". Nonetheless, it appears that the search space defined within this framework might not fully encompass the range of hyper-parameters considered in existing literature. This limitation could potentially restrict the framework's applicability or comparative analysis capabilities. (Question 6, 7)\n\nQuestions: 1. The field of training-free guidance encompasses a wide array of studies beyond those specifically addressed in this paper (e.g. [1]-[3]). Could you elaborate on how these works relate to the scope and contributions of your paper?\n\n2. Your paper operates under the assumption that the forward model is known, leveraging training-free guidance for solving inverse problems. Given that knowing the forward model allows for the generation of a large number of samples to train a conditional diffusion model at a relatively low computational cost (e.g., 10 A100 GPU hours) and training-based approaches are much better than training-free ones, could you discuss the significant motivations of adopting a training-free approach in this context?\n\n3. The theoretical foundation of your paper seems to rest significantly on Lemma 4.1, which revisits the variance of MMSE estimator of the signal corrupted by Gaussian noise (e.g., (2.8) in [4]). This formula is widely adopted in diffusion papers (e.g., [5]). Given its established nature and previous applications, could you elaborate on how this lemma specifically contributes to the novel aspects of your framework?\n\n4. Concerning Lemma 4.1, it appears there is no assurance that the generated image accurately follows the conditional distribution, nor is there a guarantee that the loss decreases at each iteration as suggested by equation (7). Could you provide further clarification or additional theoretical support to address these concerns?\n\n5. Several techniques introduced in the paper lack direct theoretical underpinning:\n- The concept of \"time-travel\" being an Ornstein-Uhlenbeck process is intriguing but lacks a detailed derivation. Could you expand on how the theory of the Ornstein-Uhlenbeck process quantifies the benefits of time-travel in your framework?\n- The selection of hyperparameters seems not to be grounded in theory. Could you discuss the rationale behind these choices and any potential theoretical support?\n\n6. The paper posits that the study of training-free methods can be encapsulated within the hyperparameter space of your framework. However, specific hyperparameter settings critical for the performance in existing works, such as the step sizes used in Face generation (at.sqrt()) and Style transfer ((correction * correction).mean().sqrt().item() * unconditional_guidance_scale / (norm_grad * norm_grad).mean().sqrt().item() * 0.2) in FreeDoM, are not explicitly covered. Could you address the omission of these settings and their impact on the comprehensiveness of your study?\n\n7. The hyperparameter search settings for the baselines in your comparative analysis are not disclosed, raising questions about the fairness and validity of the comparisons. Could you provide more details on these settings to ensure a transparent and equitable comparison?\n\n[1] Feng, Weixi, et al. \"Training-free structured diffusion guidance for compositional text-to-image synthesis.\" ICLR 2023.\n\n[2] Chen, Minghao, et al. \"Training-free layout control with cross-attention guidance.\" WACV 2024.\n\n[3] Mo, Sicheng, et al. \"Freecontrol: Training-free spatial control of any text-to-image diffusion model with any condition.\" CVPR 2024.\n\n[4] Efron, Bradley. \"Tweedie’s formula and selection bias.\" JASA 2011.\n\n[5] Kadkhodaie, Zahra, et al. \"Generalization in diffusion models arises from geometry-adaptive harmonic representation.\" ICLR 2024.", "labeling_timestamp": "2026-01-11T16:59:03.248538", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states that it provides theoretical analysis and a principled hyper-parameter searching strategy, and it claims to analyze the theoretical motivation of each hyper-parameter, so the reviewer's assertion that key hyperparameter choices lack theoretical grounding is contradicted by the manuscript.", "evidence": "1) \"Via theoretical and empirical investigation, we propose an efficient and effective hyper-parameter searching strategy that can be readily applied to any downstream task.\" 2) \"Within our framework, we analyze the underlying theoretical motivation of each hyper-parameter and conduct comprehensive experiments to identify their influence.\" 3) \"(2) theoretically and empirically analyze the space to propose an effective space-searching strategy for general problems,\"", "section": "Abstract; Introduction (Section 1)"}
{"claim": "The framework does not explicitly cover specific hyperparameter settings critical for existing works, such as the step size formula used in Face generation (at.sqrt()).", "claim_type": "methodology", "paper_id": "N8YbGX98vc", "paper_title": "TFG: Unified Training-Free Guidance for Diffusion Models", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "6i2pz4NB9X", "reviewer": "Reviewer_Xm6V", "review_text": "Summary: The paper introduces Training-Free Guidance (TFG), a novel framework designed to enhance the generation of samples with desired properties using diffusion models, without necessitating additional model training. TFG aims to resolve the shortcomings of existing training-free methods by offering a unified algorithmic framework that simplifies the comparison and application of such methods across a wide range of tasks. By theoretically and empirically analyzing a hyper-parameter design space within this framework, the authors develop an effective strategy for hyper-parameter selection applicable to various tasks. Their comprehensive benchmarks across multiple tasks and targets demonstrate TFG's superior performance, achieving an average improvement of 7.4% over existing methods.\n\nStrengths: 1. Given the burgeoning interest in the area of training-free guidance, the establishment of a unified benchmark as presented in this paper is a commendable contribution that holds the potential to significantly advance research in this field. The authors' efforts in this direction are highly appreciated and underscore the importance of standardized benchmarks for facilitating future developments.\n\n2. The experiments conducted in this study are extensive, reflecting a high degree of diligence and thoroughness. Such a comprehensive experimental approach is commendable and warrants recognition. Consequently, I believe this aspect of the paper merits a positive evaluation for its contribution to validating the proposed framework and its applicability across a diverse range of tasks.\n\nWeaknesses: There are several areas where improvements could significantly enhance its contributions. Addressing these points satisfactorily would make a strong case for elevating the paper's status to \"Accept\" or \"Strong Accept\".\n\n1. While the paper provides valuable insights into a specific line of training-free guidance, it is important to acknowledge the existence of a broader spectrum of works outside this category. This observation suggests that the title of the paper may slightly overreach, potentially implying a more comprehensive coverage than is actually presented. (Question 1)\n\n2. The paper commendably supports the motivations behind conditional diffusion in Appendix A, offering a solid foundation for its relevance. However, the rationale for focusing specifically on training-free approaches appears less extensively articulated. (Question 2)\n\n3. The authors claimed that they have theoretically grounded their unified framework. However, upon review, the theoretical underpinnings presented seem to require further elaboration to fully substantiate this claim. (Question 3, 4, 5)\n\n4. The authors claimed that \"the studies of training-free methods become the study within the hyperparameter space of our framework\". Nonetheless, it appears that the search space defined within this framework might not fully encompass the range of hyper-parameters considered in existing literature. This limitation could potentially restrict the framework's applicability or comparative analysis capabilities. (Question 6, 7)\n\nQuestions: 1. The field of training-free guidance encompasses a wide array of studies beyond those specifically addressed in this paper (e.g. [1]-[3]). Could you elaborate on how these works relate to the scope and contributions of your paper?\n\n2. Your paper operates under the assumption that the forward model is known, leveraging training-free guidance for solving inverse problems. Given that knowing the forward model allows for the generation of a large number of samples to train a conditional diffusion model at a relatively low computational cost (e.g., 10 A100 GPU hours) and training-based approaches are much better than training-free ones, could you discuss the significant motivations of adopting a training-free approach in this context?\n\n3. The theoretical foundation of your paper seems to rest significantly on Lemma 4.1, which revisits the variance of MMSE estimator of the signal corrupted by Gaussian noise (e.g., (2.8) in [4]). This formula is widely adopted in diffusion papers (e.g., [5]). Given its established nature and previous applications, could you elaborate on how this lemma specifically contributes to the novel aspects of your framework?\n\n4. Concerning Lemma 4.1, it appears there is no assurance that the generated image accurately follows the conditional distribution, nor is there a guarantee that the loss decreases at each iteration as suggested by equation (7). Could you provide further clarification or additional theoretical support to address these concerns?\n\n5. Several techniques introduced in the paper lack direct theoretical underpinning:\n- The concept of \"time-travel\" being an Ornstein-Uhlenbeck process is intriguing but lacks a detailed derivation. Could you expand on how the theory of the Ornstein-Uhlenbeck process quantifies the benefits of time-travel in your framework?\n- The selection of hyperparameters seems not to be grounded in theory. Could you discuss the rationale behind these choices and any potential theoretical support?\n\n6. The paper posits that the study of training-free methods can be encapsulated within the hyperparameter space of your framework. However, specific hyperparameter settings critical for the performance in existing works, such as the step sizes used in Face generation (at.sqrt()) and Style transfer ((correction * correction).mean().sqrt().item() * unconditional_guidance_scale / (norm_grad * norm_grad).mean().sqrt().item() * 0.2) in FreeDoM, are not explicitly covered. Could you address the omission of these settings and their impact on the comprehensiveness of your study?\n\n7. The hyperparameter search settings for the baselines in your comparative analysis are not disclosed, raising questions about the fairness and validity of the comparisons. Could you provide more details on these settings to ensure a transparent and equitable comparison?\n\n[1] Feng, Weixi, et al. \"Training-free structured diffusion guidance for compositional text-to-image synthesis.\" ICLR 2023.\n\n[2] Chen, Minghao, et al. \"Training-free layout control with cross-attention guidance.\" WACV 2024.\n\n[3] Mo, Sicheng, et al. \"Freecontrol: Training-free spatial control of any text-to-image diffusion model with any condition.\" CVPR 2024.\n\n[4] Efron, Bradley. \"Tweedie’s formula and selection bias.\" JASA 2011.\n\n[5] Kadkhodaie, Zahra, et al. \"Generalization in diffusion models arises from geometry-adaptive harmonic representation.\" ICLR 2024.", "labeling_timestamp": "2026-01-11T16:59:08.119038", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states that TFG unifies existing methods and that existing approaches correspond to particular hyper-parameter subspaces in the unified design space; it also proposes a hyper-parameter searching strategy. Thus it claims to explicitly cover hyperparameter settings of prior works rather than omitting them.", "evidence": "We show that existing approaches are special cases of the TFG as they correspond to particular hyper-parameter subspace in our unified space.", "section": "3.1 Unification and extension (also stated in Abstract/Introduction)"}
{"claim": "The framework omits the specific hyperparameter formulation used in FreeDoM style transfer, namely the complex correction and norm_grad based scaling expression.", "claim_type": "methodology", "paper_id": "N8YbGX98vc", "paper_title": "TFG: Unified Training-Free Guidance for Diffusion Models", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "6i2pz4NB9X", "reviewer": "Reviewer_Xm6V", "review_text": "Summary: The paper introduces Training-Free Guidance (TFG), a novel framework designed to enhance the generation of samples with desired properties using diffusion models, without necessitating additional model training. TFG aims to resolve the shortcomings of existing training-free methods by offering a unified algorithmic framework that simplifies the comparison and application of such methods across a wide range of tasks. By theoretically and empirically analyzing a hyper-parameter design space within this framework, the authors develop an effective strategy for hyper-parameter selection applicable to various tasks. Their comprehensive benchmarks across multiple tasks and targets demonstrate TFG's superior performance, achieving an average improvement of 7.4% over existing methods.\n\nStrengths: 1. Given the burgeoning interest in the area of training-free guidance, the establishment of a unified benchmark as presented in this paper is a commendable contribution that holds the potential to significantly advance research in this field. The authors' efforts in this direction are highly appreciated and underscore the importance of standardized benchmarks for facilitating future developments.\n\n2. The experiments conducted in this study are extensive, reflecting a high degree of diligence and thoroughness. Such a comprehensive experimental approach is commendable and warrants recognition. Consequently, I believe this aspect of the paper merits a positive evaluation for its contribution to validating the proposed framework and its applicability across a diverse range of tasks.\n\nWeaknesses: There are several areas where improvements could significantly enhance its contributions. Addressing these points satisfactorily would make a strong case for elevating the paper's status to \"Accept\" or \"Strong Accept\".\n\n1. While the paper provides valuable insights into a specific line of training-free guidance, it is important to acknowledge the existence of a broader spectrum of works outside this category. This observation suggests that the title of the paper may slightly overreach, potentially implying a more comprehensive coverage than is actually presented. (Question 1)\n\n2. The paper commendably supports the motivations behind conditional diffusion in Appendix A, offering a solid foundation for its relevance. However, the rationale for focusing specifically on training-free approaches appears less extensively articulated. (Question 2)\n\n3. The authors claimed that they have theoretically grounded their unified framework. However, upon review, the theoretical underpinnings presented seem to require further elaboration to fully substantiate this claim. (Question 3, 4, 5)\n\n4. The authors claimed that \"the studies of training-free methods become the study within the hyperparameter space of our framework\". Nonetheless, it appears that the search space defined within this framework might not fully encompass the range of hyper-parameters considered in existing literature. This limitation could potentially restrict the framework's applicability or comparative analysis capabilities. (Question 6, 7)\n\nQuestions: 1. The field of training-free guidance encompasses a wide array of studies beyond those specifically addressed in this paper (e.g. [1]-[3]). Could you elaborate on how these works relate to the scope and contributions of your paper?\n\n2. Your paper operates under the assumption that the forward model is known, leveraging training-free guidance for solving inverse problems. Given that knowing the forward model allows for the generation of a large number of samples to train a conditional diffusion model at a relatively low computational cost (e.g., 10 A100 GPU hours) and training-based approaches are much better than training-free ones, could you discuss the significant motivations of adopting a training-free approach in this context?\n\n3. The theoretical foundation of your paper seems to rest significantly on Lemma 4.1, which revisits the variance of MMSE estimator of the signal corrupted by Gaussian noise (e.g., (2.8) in [4]). This formula is widely adopted in diffusion papers (e.g., [5]). Given its established nature and previous applications, could you elaborate on how this lemma specifically contributes to the novel aspects of your framework?\n\n4. Concerning Lemma 4.1, it appears there is no assurance that the generated image accurately follows the conditional distribution, nor is there a guarantee that the loss decreases at each iteration as suggested by equation (7). Could you provide further clarification or additional theoretical support to address these concerns?\n\n5. Several techniques introduced in the paper lack direct theoretical underpinning:\n- The concept of \"time-travel\" being an Ornstein-Uhlenbeck process is intriguing but lacks a detailed derivation. Could you expand on how the theory of the Ornstein-Uhlenbeck process quantifies the benefits of time-travel in your framework?\n- The selection of hyperparameters seems not to be grounded in theory. Could you discuss the rationale behind these choices and any potential theoretical support?\n\n6. The paper posits that the study of training-free methods can be encapsulated within the hyperparameter space of your framework. However, specific hyperparameter settings critical for the performance in existing works, such as the step sizes used in Face generation (at.sqrt()) and Style transfer ((correction * correction).mean().sqrt().item() * unconditional_guidance_scale / (norm_grad * norm_grad).mean().sqrt().item() * 0.2) in FreeDoM, are not explicitly covered. Could you address the omission of these settings and their impact on the comprehensiveness of your study?\n\n7. The hyperparameter search settings for the baselines in your comparative analysis are not disclosed, raising questions about the fairness and validity of the comparisons. Could you provide more details on these settings to ensure a transparent and equitable comparison?\n\n[1] Feng, Weixi, et al. \"Training-free structured diffusion guidance for compositional text-to-image synthesis.\" ICLR 2023.\n\n[2] Chen, Minghao, et al. \"Training-free layout control with cross-attention guidance.\" WACV 2024.\n\n[3] Mo, Sicheng, et al. \"Freecontrol: Training-free spatial control of any text-to-image diffusion model with any condition.\" CVPR 2024.\n\n[4] Efron, Bradley. \"Tweedie’s formula and selection bias.\" JASA 2011.\n\n[5] Kadkhodaie, Zahra, et al. \"Generalization in diffusion models arises from geometry-adaptive harmonic representation.\" ICLR 2024.", "labeling_timestamp": "2026-01-11T16:59:22.680914", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states that existing methods (including FreeDoM) are special cases within the TFG hyper-parameter design space and provides pseudo-code of reviewed methods in Appendix B, so it does not omit FreeDoM's formulation from the unified framework.", "evidence": "1) \"We show that existing approaches are special cases of the TFG as they correspond to particular hyper-parameter subspace in our unified space.\" 2) \"We review and summarize five existing approaches below, and provide a schematic and a copy of pseudo-code in Appendix B for the sake of reference.\" 3) \"FreeDoM [78] generalizes DPS by introducing a 'recurrent strategy' ... FreeDoM also points out the importance of altering guidance strength at different time steps t , but a comprehensive study on which schedule is better is not provided.\"", "section": "Introduction; 2.1 Existing algorithms; 3.1 Unification and extension"}
{"claim": "The hyperparameter search settings used for baseline methods in experimental comparisons are not disclosed, raising concerns about comparison fairness.", "claim_type": "baseline", "paper_id": "N8YbGX98vc", "paper_title": "TFG: Unified Training-Free Guidance for Diffusion Models", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "6i2pz4NB9X", "reviewer": "Reviewer_Xm6V", "review_text": "Summary: The paper introduces Training-Free Guidance (TFG), a novel framework designed to enhance the generation of samples with desired properties using diffusion models, without necessitating additional model training. TFG aims to resolve the shortcomings of existing training-free methods by offering a unified algorithmic framework that simplifies the comparison and application of such methods across a wide range of tasks. By theoretically and empirically analyzing a hyper-parameter design space within this framework, the authors develop an effective strategy for hyper-parameter selection applicable to various tasks. Their comprehensive benchmarks across multiple tasks and targets demonstrate TFG's superior performance, achieving an average improvement of 7.4% over existing methods.\n\nStrengths: 1. Given the burgeoning interest in the area of training-free guidance, the establishment of a unified benchmark as presented in this paper is a commendable contribution that holds the potential to significantly advance research in this field. The authors' efforts in this direction are highly appreciated and underscore the importance of standardized benchmarks for facilitating future developments.\n\n2. The experiments conducted in this study are extensive, reflecting a high degree of diligence and thoroughness. Such a comprehensive experimental approach is commendable and warrants recognition. Consequently, I believe this aspect of the paper merits a positive evaluation for its contribution to validating the proposed framework and its applicability across a diverse range of tasks.\n\nWeaknesses: There are several areas where improvements could significantly enhance its contributions. Addressing these points satisfactorily would make a strong case for elevating the paper's status to \"Accept\" or \"Strong Accept\".\n\n1. While the paper provides valuable insights into a specific line of training-free guidance, it is important to acknowledge the existence of a broader spectrum of works outside this category. This observation suggests that the title of the paper may slightly overreach, potentially implying a more comprehensive coverage than is actually presented. (Question 1)\n\n2. The paper commendably supports the motivations behind conditional diffusion in Appendix A, offering a solid foundation for its relevance. However, the rationale for focusing specifically on training-free approaches appears less extensively articulated. (Question 2)\n\n3. The authors claimed that they have theoretically grounded their unified framework. However, upon review, the theoretical underpinnings presented seem to require further elaboration to fully substantiate this claim. (Question 3, 4, 5)\n\n4. The authors claimed that \"the studies of training-free methods become the study within the hyperparameter space of our framework\". Nonetheless, it appears that the search space defined within this framework might not fully encompass the range of hyper-parameters considered in existing literature. This limitation could potentially restrict the framework's applicability or comparative analysis capabilities. (Question 6, 7)\n\nQuestions: 1. The field of training-free guidance encompasses a wide array of studies beyond those specifically addressed in this paper (e.g. [1]-[3]). Could you elaborate on how these works relate to the scope and contributions of your paper?\n\n2. Your paper operates under the assumption that the forward model is known, leveraging training-free guidance for solving inverse problems. Given that knowing the forward model allows for the generation of a large number of samples to train a conditional diffusion model at a relatively low computational cost (e.g., 10 A100 GPU hours) and training-based approaches are much better than training-free ones, could you discuss the significant motivations of adopting a training-free approach in this context?\n\n3. The theoretical foundation of your paper seems to rest significantly on Lemma 4.1, which revisits the variance of MMSE estimator of the signal corrupted by Gaussian noise (e.g., (2.8) in [4]). This formula is widely adopted in diffusion papers (e.g., [5]). Given its established nature and previous applications, could you elaborate on how this lemma specifically contributes to the novel aspects of your framework?\n\n4. Concerning Lemma 4.1, it appears there is no assurance that the generated image accurately follows the conditional distribution, nor is there a guarantee that the loss decreases at each iteration as suggested by equation (7). Could you provide further clarification or additional theoretical support to address these concerns?\n\n5. Several techniques introduced in the paper lack direct theoretical underpinning:\n- The concept of \"time-travel\" being an Ornstein-Uhlenbeck process is intriguing but lacks a detailed derivation. Could you expand on how the theory of the Ornstein-Uhlenbeck process quantifies the benefits of time-travel in your framework?\n- The selection of hyperparameters seems not to be grounded in theory. Could you discuss the rationale behind these choices and any potential theoretical support?\n\n6. The paper posits that the study of training-free methods can be encapsulated within the hyperparameter space of your framework. However, specific hyperparameter settings critical for the performance in existing works, such as the step sizes used in Face generation (at.sqrt()) and Style transfer ((correction * correction).mean().sqrt().item() * unconditional_guidance_scale / (norm_grad * norm_grad).mean().sqrt().item() * 0.2) in FreeDoM, are not explicitly covered. Could you address the omission of these settings and their impact on the comprehensiveness of your study?\n\n7. The hyperparameter search settings for the baselines in your comparative analysis are not disclosed, raising questions about the fairness and validity of the comparisons. Could you provide more details on these settings to ensure a transparent and equitable comparison?\n\n[1] Feng, Weixi, et al. \"Training-free structured diffusion guidance for compositional text-to-image synthesis.\" ICLR 2023.\n\n[2] Chen, Minghao, et al. \"Training-free layout control with cross-attention guidance.\" WACV 2024.\n\n[3] Mo, Sicheng, et al. \"Freecontrol: Training-free spatial control of any text-to-image diffusion model with any condition.\" CVPR 2024.\n\n[4] Efron, Bradley. \"Tweedie’s formula and selection bias.\" JASA 2011.\n\n[5] Kadkhodaie, Zahra, et al. \"Generalization in diffusion models arises from geometry-adaptive harmonic representation.\" ICLR 2024.", "labeling_timestamp": "2026-01-11T16:59:22.630331", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper states that baseline hyper-parameters were chosen via a grid search for fair comparison and provides a code link, indicating the authors did disclose their hyper-parameter search procedure (and likely settings) rather than withholding them.", "evidence": "“for each of the ten labels, we use the pretrained diffusion model and classifiers from [7, 9] to generate 2048 samples, where the hyper-parameters are selected via a grid search for the fairness of comparison.”; “Code is available at https://github.com/YWolfeee/Training-Free-Guidance .”", "section": "Section 3 (Case study on CIFAR10) and Footnote in Abstract"}
{"claim": "When the forward model is known, generating many samples to train a conditional diffusion model is computationally cheap, making training-based approaches potentially superior to training-free approaches.", "claim_type": "experimental", "paper_id": "N8YbGX98vc", "paper_title": "TFG: Unified Training-Free Guidance for Diffusion Models", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "6i2pz4NB9X", "reviewer": "Reviewer_Xm6V", "review_text": "Summary: The paper introduces Training-Free Guidance (TFG), a novel framework designed to enhance the generation of samples with desired properties using diffusion models, without necessitating additional model training. TFG aims to resolve the shortcomings of existing training-free methods by offering a unified algorithmic framework that simplifies the comparison and application of such methods across a wide range of tasks. By theoretically and empirically analyzing a hyper-parameter design space within this framework, the authors develop an effective strategy for hyper-parameter selection applicable to various tasks. Their comprehensive benchmarks across multiple tasks and targets demonstrate TFG's superior performance, achieving an average improvement of 7.4% over existing methods.\n\nStrengths: 1. Given the burgeoning interest in the area of training-free guidance, the establishment of a unified benchmark as presented in this paper is a commendable contribution that holds the potential to significantly advance research in this field. The authors' efforts in this direction are highly appreciated and underscore the importance of standardized benchmarks for facilitating future developments.\n\n2. The experiments conducted in this study are extensive, reflecting a high degree of diligence and thoroughness. Such a comprehensive experimental approach is commendable and warrants recognition. Consequently, I believe this aspect of the paper merits a positive evaluation for its contribution to validating the proposed framework and its applicability across a diverse range of tasks.\n\nWeaknesses: There are several areas where improvements could significantly enhance its contributions. Addressing these points satisfactorily would make a strong case for elevating the paper's status to \"Accept\" or \"Strong Accept\".\n\n1. While the paper provides valuable insights into a specific line of training-free guidance, it is important to acknowledge the existence of a broader spectrum of works outside this category. This observation suggests that the title of the paper may slightly overreach, potentially implying a more comprehensive coverage than is actually presented. (Question 1)\n\n2. The paper commendably supports the motivations behind conditional diffusion in Appendix A, offering a solid foundation for its relevance. However, the rationale for focusing specifically on training-free approaches appears less extensively articulated. (Question 2)\n\n3. The authors claimed that they have theoretically grounded their unified framework. However, upon review, the theoretical underpinnings presented seem to require further elaboration to fully substantiate this claim. (Question 3, 4, 5)\n\n4. The authors claimed that \"the studies of training-free methods become the study within the hyperparameter space of our framework\". Nonetheless, it appears that the search space defined within this framework might not fully encompass the range of hyper-parameters considered in existing literature. This limitation could potentially restrict the framework's applicability or comparative analysis capabilities. (Question 6, 7)\n\nQuestions: 1. The field of training-free guidance encompasses a wide array of studies beyond those specifically addressed in this paper (e.g. [1]-[3]). Could you elaborate on how these works relate to the scope and contributions of your paper?\n\n2. Your paper operates under the assumption that the forward model is known, leveraging training-free guidance for solving inverse problems. Given that knowing the forward model allows for the generation of a large number of samples to train a conditional diffusion model at a relatively low computational cost (e.g., 10 A100 GPU hours) and training-based approaches are much better than training-free ones, could you discuss the significant motivations of adopting a training-free approach in this context?\n\n3. The theoretical foundation of your paper seems to rest significantly on Lemma 4.1, which revisits the variance of MMSE estimator of the signal corrupted by Gaussian noise (e.g., (2.8) in [4]). This formula is widely adopted in diffusion papers (e.g., [5]). Given its established nature and previous applications, could you elaborate on how this lemma specifically contributes to the novel aspects of your framework?\n\n4. Concerning Lemma 4.1, it appears there is no assurance that the generated image accurately follows the conditional distribution, nor is there a guarantee that the loss decreases at each iteration as suggested by equation (7). Could you provide further clarification or additional theoretical support to address these concerns?\n\n5. Several techniques introduced in the paper lack direct theoretical underpinning:\n- The concept of \"time-travel\" being an Ornstein-Uhlenbeck process is intriguing but lacks a detailed derivation. Could you expand on how the theory of the Ornstein-Uhlenbeck process quantifies the benefits of time-travel in your framework?\n- The selection of hyperparameters seems not to be grounded in theory. Could you discuss the rationale behind these choices and any potential theoretical support?\n\n6. The paper posits that the study of training-free methods can be encapsulated within the hyperparameter space of your framework. However, specific hyperparameter settings critical for the performance in existing works, such as the step sizes used in Face generation (at.sqrt()) and Style transfer ((correction * correction).mean().sqrt().item() * unconditional_guidance_scale / (norm_grad * norm_grad).mean().sqrt().item() * 0.2) in FreeDoM, are not explicitly covered. Could you address the omission of these settings and their impact on the comprehensiveness of your study?\n\n7. The hyperparameter search settings for the baselines in your comparative analysis are not disclosed, raising questions about the fairness and validity of the comparisons. Could you provide more details on these settings to ensure a transparent and equitable comparison?\n\n[1] Feng, Weixi, et al. \"Training-free structured diffusion guidance for compositional text-to-image synthesis.\" ICLR 2023.\n\n[2] Chen, Minghao, et al. \"Training-free layout control with cross-attention guidance.\" WACV 2024.\n\n[3] Mo, Sicheng, et al. \"Freecontrol: Training-free spatial control of any text-to-image diffusion model with any condition.\" CVPR 2024.\n\n[4] Efron, Bradley. \"Tweedie’s formula and selection bias.\" JASA 2011.\n\n[5] Kadkhodaie, Zahra, et al. \"Generalization in diffusion models arises from geometry-adaptive harmonic representation.\" ICLR 2024.", "labeling_timestamp": "2026-01-11T16:59:31.160933", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper states that training-based guidance requires training specialized models and describes this as resource-intensive and time-consuming, contradicting the reviewer's claim that generating many samples to train a conditional model is computationally cheap when the forward model is known.", "evidence": "\"Conditional generation methods like classifier-based guidance [66, 7] and classifier-free guidance [23] typically require training a specialized model for each conditioning signal (e.g., a noise-conditional classifier or a text-conditional denoiser). This resource-intensive and time-consuming process greatly limits their applicability.\"", "section": "1 Introduction"}
{"claim": "The paper does not explain how cited training-free works (e.g., [1]-[3]) relate to its scope or how they are encompassed by the proposed framework.", "claim_type": "methodology", "paper_id": "N8YbGX98vc", "paper_title": "TFG: Unified Training-Free Guidance for Diffusion Models", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "6i2pz4NB9X", "reviewer": "Reviewer_Xm6V", "review_text": "Summary: The paper introduces Training-Free Guidance (TFG), a novel framework designed to enhance the generation of samples with desired properties using diffusion models, without necessitating additional model training. TFG aims to resolve the shortcomings of existing training-free methods by offering a unified algorithmic framework that simplifies the comparison and application of such methods across a wide range of tasks. By theoretically and empirically analyzing a hyper-parameter design space within this framework, the authors develop an effective strategy for hyper-parameter selection applicable to various tasks. Their comprehensive benchmarks across multiple tasks and targets demonstrate TFG's superior performance, achieving an average improvement of 7.4% over existing methods.\n\nStrengths: 1. Given the burgeoning interest in the area of training-free guidance, the establishment of a unified benchmark as presented in this paper is a commendable contribution that holds the potential to significantly advance research in this field. The authors' efforts in this direction are highly appreciated and underscore the importance of standardized benchmarks for facilitating future developments.\n\n2. The experiments conducted in this study are extensive, reflecting a high degree of diligence and thoroughness. Such a comprehensive experimental approach is commendable and warrants recognition. Consequently, I believe this aspect of the paper merits a positive evaluation for its contribution to validating the proposed framework and its applicability across a diverse range of tasks.\n\nWeaknesses: There are several areas where improvements could significantly enhance its contributions. Addressing these points satisfactorily would make a strong case for elevating the paper's status to \"Accept\" or \"Strong Accept\".\n\n1. While the paper provides valuable insights into a specific line of training-free guidance, it is important to acknowledge the existence of a broader spectrum of works outside this category. This observation suggests that the title of the paper may slightly overreach, potentially implying a more comprehensive coverage than is actually presented. (Question 1)\n\n2. The paper commendably supports the motivations behind conditional diffusion in Appendix A, offering a solid foundation for its relevance. However, the rationale for focusing specifically on training-free approaches appears less extensively articulated. (Question 2)\n\n3. The authors claimed that they have theoretically grounded their unified framework. However, upon review, the theoretical underpinnings presented seem to require further elaboration to fully substantiate this claim. (Question 3, 4, 5)\n\n4. The authors claimed that \"the studies of training-free methods become the study within the hyperparameter space of our framework\". Nonetheless, it appears that the search space defined within this framework might not fully encompass the range of hyper-parameters considered in existing literature. This limitation could potentially restrict the framework's applicability or comparative analysis capabilities. (Question 6, 7)\n\nQuestions: 1. The field of training-free guidance encompasses a wide array of studies beyond those specifically addressed in this paper (e.g. [1]-[3]). Could you elaborate on how these works relate to the scope and contributions of your paper?\n\n2. Your paper operates under the assumption that the forward model is known, leveraging training-free guidance for solving inverse problems. Given that knowing the forward model allows for the generation of a large number of samples to train a conditional diffusion model at a relatively low computational cost (e.g., 10 A100 GPU hours) and training-based approaches are much better than training-free ones, could you discuss the significant motivations of adopting a training-free approach in this context?\n\n3. The theoretical foundation of your paper seems to rest significantly on Lemma 4.1, which revisits the variance of MMSE estimator of the signal corrupted by Gaussian noise (e.g., (2.8) in [4]). This formula is widely adopted in diffusion papers (e.g., [5]). Given its established nature and previous applications, could you elaborate on how this lemma specifically contributes to the novel aspects of your framework?\n\n4. Concerning Lemma 4.1, it appears there is no assurance that the generated image accurately follows the conditional distribution, nor is there a guarantee that the loss decreases at each iteration as suggested by equation (7). Could you provide further clarification or additional theoretical support to address these concerns?\n\n5. Several techniques introduced in the paper lack direct theoretical underpinning:\n- The concept of \"time-travel\" being an Ornstein-Uhlenbeck process is intriguing but lacks a detailed derivation. Could you expand on how the theory of the Ornstein-Uhlenbeck process quantifies the benefits of time-travel in your framework?\n- The selection of hyperparameters seems not to be grounded in theory. Could you discuss the rationale behind these choices and any potential theoretical support?\n\n6. The paper posits that the study of training-free methods can be encapsulated within the hyperparameter space of your framework. However, specific hyperparameter settings critical for the performance in existing works, such as the step sizes used in Face generation (at.sqrt()) and Style transfer ((correction * correction).mean().sqrt().item() * unconditional_guidance_scale / (norm_grad * norm_grad).mean().sqrt().item() * 0.2) in FreeDoM, are not explicitly covered. Could you address the omission of these settings and their impact on the comprehensiveness of your study?\n\n7. The hyperparameter search settings for the baselines in your comparative analysis are not disclosed, raising questions about the fairness and validity of the comparisons. Could you provide more details on these settings to ensure a transparent and equitable comparison?\n\n[1] Feng, Weixi, et al. \"Training-free structured diffusion guidance for compositional text-to-image synthesis.\" ICLR 2023.\n\n[2] Chen, Minghao, et al. \"Training-free layout control with cross-attention guidance.\" WACV 2024.\n\n[3] Mo, Sicheng, et al. \"Freecontrol: Training-free spatial control of any text-to-image diffusion model with any condition.\" CVPR 2024.\n\n[4] Efron, Bradley. \"Tweedie’s formula and selection bias.\" JASA 2011.\n\n[5] Kadkhodaie, Zahra, et al. \"Generalization in diffusion models arises from geometry-adaptive harmonic representation.\" ICLR 2024.", "labeling_timestamp": "2026-01-11T16:59:32.333769", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states that it unifies existing training-free methods and shows they are special cases of the proposed TFG framework, and it includes a review of existing algorithms and pointers to Algorithm 1 / Definition 3.1 and pseudo-code in Appendix B that establish these connections.", "evidence": "We show that existing approaches are special cases of the TFG as they correspond to particular hyper-parameter subspace in our unified space.", "section": "Introduction"}
{"claim": "Several techniques introduced in the paper are presented without direct theoretical underpinning or proofs linking them to the claimed performance gains.", "claim_type": "methodology", "paper_id": "N8YbGX98vc", "paper_title": "TFG: Unified Training-Free Guidance for Diffusion Models", "paper_venue": "neurips2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "6i2pz4NB9X", "reviewer": "Reviewer_Xm6V", "review_text": "Summary: The paper introduces Training-Free Guidance (TFG), a novel framework designed to enhance the generation of samples with desired properties using diffusion models, without necessitating additional model training. TFG aims to resolve the shortcomings of existing training-free methods by offering a unified algorithmic framework that simplifies the comparison and application of such methods across a wide range of tasks. By theoretically and empirically analyzing a hyper-parameter design space within this framework, the authors develop an effective strategy for hyper-parameter selection applicable to various tasks. Their comprehensive benchmarks across multiple tasks and targets demonstrate TFG's superior performance, achieving an average improvement of 7.4% over existing methods.\n\nStrengths: 1. Given the burgeoning interest in the area of training-free guidance, the establishment of a unified benchmark as presented in this paper is a commendable contribution that holds the potential to significantly advance research in this field. The authors' efforts in this direction are highly appreciated and underscore the importance of standardized benchmarks for facilitating future developments.\n\n2. The experiments conducted in this study are extensive, reflecting a high degree of diligence and thoroughness. Such a comprehensive experimental approach is commendable and warrants recognition. Consequently, I believe this aspect of the paper merits a positive evaluation for its contribution to validating the proposed framework and its applicability across a diverse range of tasks.\n\nWeaknesses: There are several areas where improvements could significantly enhance its contributions. Addressing these points satisfactorily would make a strong case for elevating the paper's status to \"Accept\" or \"Strong Accept\".\n\n1. While the paper provides valuable insights into a specific line of training-free guidance, it is important to acknowledge the existence of a broader spectrum of works outside this category. This observation suggests that the title of the paper may slightly overreach, potentially implying a more comprehensive coverage than is actually presented. (Question 1)\n\n2. The paper commendably supports the motivations behind conditional diffusion in Appendix A, offering a solid foundation for its relevance. However, the rationale for focusing specifically on training-free approaches appears less extensively articulated. (Question 2)\n\n3. The authors claimed that they have theoretically grounded their unified framework. However, upon review, the theoretical underpinnings presented seem to require further elaboration to fully substantiate this claim. (Question 3, 4, 5)\n\n4. The authors claimed that \"the studies of training-free methods become the study within the hyperparameter space of our framework\". Nonetheless, it appears that the search space defined within this framework might not fully encompass the range of hyper-parameters considered in existing literature. This limitation could potentially restrict the framework's applicability or comparative analysis capabilities. (Question 6, 7)\n\nQuestions: 1. The field of training-free guidance encompasses a wide array of studies beyond those specifically addressed in this paper (e.g. [1]-[3]). Could you elaborate on how these works relate to the scope and contributions of your paper?\n\n2. Your paper operates under the assumption that the forward model is known, leveraging training-free guidance for solving inverse problems. Given that knowing the forward model allows for the generation of a large number of samples to train a conditional diffusion model at a relatively low computational cost (e.g., 10 A100 GPU hours) and training-based approaches are much better than training-free ones, could you discuss the significant motivations of adopting a training-free approach in this context?\n\n3. The theoretical foundation of your paper seems to rest significantly on Lemma 4.1, which revisits the variance of MMSE estimator of the signal corrupted by Gaussian noise (e.g., (2.8) in [4]). This formula is widely adopted in diffusion papers (e.g., [5]). Given its established nature and previous applications, could you elaborate on how this lemma specifically contributes to the novel aspects of your framework?\n\n4. Concerning Lemma 4.1, it appears there is no assurance that the generated image accurately follows the conditional distribution, nor is there a guarantee that the loss decreases at each iteration as suggested by equation (7). Could you provide further clarification or additional theoretical support to address these concerns?\n\n5. Several techniques introduced in the paper lack direct theoretical underpinning:\n- The concept of \"time-travel\" being an Ornstein-Uhlenbeck process is intriguing but lacks a detailed derivation. Could you expand on how the theory of the Ornstein-Uhlenbeck process quantifies the benefits of time-travel in your framework?\n- The selection of hyperparameters seems not to be grounded in theory. Could you discuss the rationale behind these choices and any potential theoretical support?\n\n6. The paper posits that the study of training-free methods can be encapsulated within the hyperparameter space of your framework. However, specific hyperparameter settings critical for the performance in existing works, such as the step sizes used in Face generation (at.sqrt()) and Style transfer ((correction * correction).mean().sqrt().item() * unconditional_guidance_scale / (norm_grad * norm_grad).mean().sqrt().item() * 0.2) in FreeDoM, are not explicitly covered. Could you address the omission of these settings and their impact on the comprehensiveness of your study?\n\n7. The hyperparameter search settings for the baselines in your comparative analysis are not disclosed, raising questions about the fairness and validity of the comparisons. Could you provide more details on these settings to ensure a transparent and equitable comparison?\n\n[1] Feng, Weixi, et al. \"Training-free structured diffusion guidance for compositional text-to-image synthesis.\" ICLR 2023.\n\n[2] Chen, Minghao, et al. \"Training-free layout control with cross-attention guidance.\" WACV 2024.\n\n[3] Mo, Sicheng, et al. \"Freecontrol: Training-free spatial control of any text-to-image diffusion model with any condition.\" CVPR 2024.\n\n[4] Efron, Bradley. \"Tweedie’s formula and selection bias.\" JASA 2011.\n\n[5] Kadkhodaie, Zahra, et al. \"Generalization in diffusion models arises from geometry-adaptive harmonic representation.\" ICLR 2024.", "labeling_timestamp": "2026-01-11T16:59:37.119312", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states that it provides theoretical investigation and analyzes the theoretical motivation of its components (hyper-parameters), which contradicts the reviewer's claim that several techniques are presented without theoretical underpinning or proofs.", "evidence": "Abstract: \"Via theoretical and empirical investigation, we propose an efficient and effective hyper-parameter searching strategy...\"; Introduction: \"Within our framework, we analyze the underlying theoretical motivation of each hyper-parameter and conduct comprehensive experiments to identify their influence.\"", "section": "Abstract; 1 Introduction"}
{"claim": "The central assumption that ||w|| = ||\\hat{w}|| + o(||w||) is contentious and may not hold in practical fixed-bit quantization scenarios.", "claim_type": "methodology", "paper_id": "CXjz7p4qha", "paper_title": "Rotation Invariant Quantization for Model Compression", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "fhz0No7vIA", "reviewer": "Reviewer_qJtZ", "review_text": "Summary: The paper examines post-training quantization of neural networks featuring linear layers, taking into account cosine distortion. The authors first demonstrate the rate-distortion trade-off between the original and quantized weights to establish the step size, $\\Delta_{\\ell}$, for each layer. Notably, all $\\Delta_{\\ell}$ values are governed by a singular parameter, $k$. Additionally, the paper investigates the rate-deviation analysis, wherein the deviation assesses the disparity in output. The authors introduce surrogate models in which the quantized weight is uniformly distributed across a space subject to random rotation, characterized by the angle $\\theta_{\\ell}$. Subsequently, it is proven that the mutual information is minimized when using the product distribution.\n\nStrengths: 1. The paper is well-written and straightforward to understand.\n2. The concept of rotational invariance in neural network quantization is intriguing.\n\nWeaknesses: 1. The central assumption, $\\||w\\|| = \\||\\hat{w}\\|| + o(\\||w\\||)$, appears to be contentious. For instance, with fixed-bit quantization, the discrepancy between $\\||w\\||$ and $\\||\\hat{w}\\||$ is proportional to $\\||w\\||$, or in other words, $O(\\||w\\||)$.\n\n2. Lemma 1 could benefit from a more rigorous presentation, especially concerning the $o(\\cdot)$. Additionally, both Lemma 1 and Corollary 1 do not appear to offer significant novel contributions.\n\n3. The surrogate model feels somewhat contrived. Moreover, the method for deriving $\\tilde{w}_{\\ell}$ is not clear. For instance, are the angles $\\theta_{\\ell}$ specified? The notion of being \"uniformly distributed on a cone\" is ambiguous due to its unbounded norm. As a result, Theorem 1 requires a more comprehensive problem definition.\n\n4. There seem to be some technical inaccuracies in the proof of Theorem 1. It is also essential to ensure that each mutual information, \n$I(w_{\\ell}, \\tilde{w}_{\\ell})$, adheres to the distortion criteria.\n\n5. The link between distortion (related to weights) and deviation (pertaining to output) is unclear.\n\nQuestions: Please check Weakness.", "labeling_timestamp": "2026-01-11T17:00:02.649203", "model": "gpt-5-mini", "label": "Not Determined", "justification": "Error during labeling: Invalid \\uXXXX escape: line 3 column 120 (char 150)", "evidence": "", "section": ""}
{"claim": "Under fixed-bit quantization, the difference between ||w|| and ||\\hat{w}|| scales on the order of ||w||, contradicting the small o(||w||) assumption.", "claim_type": "quantitative", "paper_id": "CXjz7p4qha", "paper_title": "Rotation Invariant Quantization for Model Compression", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "fhz0No7vIA", "reviewer": "Reviewer_qJtZ", "review_text": "Summary: The paper examines post-training quantization of neural networks featuring linear layers, taking into account cosine distortion. The authors first demonstrate the rate-distortion trade-off between the original and quantized weights to establish the step size, $\\Delta_{\\ell}$, for each layer. Notably, all $\\Delta_{\\ell}$ values are governed by a singular parameter, $k$. Additionally, the paper investigates the rate-deviation analysis, wherein the deviation assesses the disparity in output. The authors introduce surrogate models in which the quantized weight is uniformly distributed across a space subject to random rotation, characterized by the angle $\\theta_{\\ell}$. Subsequently, it is proven that the mutual information is minimized when using the product distribution.\n\nStrengths: 1. The paper is well-written and straightforward to understand.\n2. The concept of rotational invariance in neural network quantization is intriguing.\n\nWeaknesses: 1. The central assumption, $\\||w\\|| = \\||\\hat{w}\\|| + o(\\||w\\||)$, appears to be contentious. For instance, with fixed-bit quantization, the discrepancy between $\\||w\\||$ and $\\||\\hat{w}\\||$ is proportional to $\\||w\\||$, or in other words, $O(\\||w\\||)$.\n\n2. Lemma 1 could benefit from a more rigorous presentation, especially concerning the $o(\\cdot)$. Additionally, both Lemma 1 and Corollary 1 do not appear to offer significant novel contributions.\n\n3. The surrogate model feels somewhat contrived. Moreover, the method for deriving $\\tilde{w}_{\\ell}$ is not clear. For instance, are the angles $\\theta_{\\ell}$ specified? The notion of being \"uniformly distributed on a cone\" is ambiguous due to its unbounded norm. As a result, Theorem 1 requires a more comprehensive problem definition.\n\n4. There seem to be some technical inaccuracies in the proof of Theorem 1. It is also essential to ensure that each mutual information, \n$I(w_{\\ell}, \\tilde{w}_{\\ell})$, adheres to the distortion criteria.\n\n5. The link between distortion (related to weights) and deviation (pertaining to output) is unclear.\n\nQuestions: Please check Weakness.", "labeling_timestamp": "2026-01-11T16:59:49.348772", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper mentions fixed-bit quantization in Related Work and describes RIQ choosing bin widths proportional to layer norms, but it does not state or analyze that ||w|| - ||\\hat{w}|| scales as O(||w||) under fixed-bit quantization nor does it discuss a small o(||w||) assumption or contradict it. There is insufficient evidence in the provided text to confirm the reviewer's claim.", "evidence": "1) \"In Wu et al. (2020); Banner et al. (2019); Idelbayev et al. (2021), the authors considered fixed-bit quantization methods, where all layers are quantized at the same integer bit rate.\" 2) \"The main theme of our approach is picking the quantization bin width to be proportional to the layers' norm.\"", "section": "2 RELATED WORK; 1 INTRODUCTION (contributions)"}
{"claim": "Lemma 1 lacks a rigorous presentation, particularly regarding the o(·) notation and its formal justification.", "claim_type": "methodology", "paper_id": "CXjz7p4qha", "paper_title": "Rotation Invariant Quantization for Model Compression", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "fhz0No7vIA", "reviewer": "Reviewer_qJtZ", "review_text": "Summary: The paper examines post-training quantization of neural networks featuring linear layers, taking into account cosine distortion. The authors first demonstrate the rate-distortion trade-off between the original and quantized weights to establish the step size, $\\Delta_{\\ell}$, for each layer. Notably, all $\\Delta_{\\ell}$ values are governed by a singular parameter, $k$. Additionally, the paper investigates the rate-deviation analysis, wherein the deviation assesses the disparity in output. The authors introduce surrogate models in which the quantized weight is uniformly distributed across a space subject to random rotation, characterized by the angle $\\theta_{\\ell}$. Subsequently, it is proven that the mutual information is minimized when using the product distribution.\n\nStrengths: 1. The paper is well-written and straightforward to understand.\n2. The concept of rotational invariance in neural network quantization is intriguing.\n\nWeaknesses: 1. The central assumption, $\\||w\\|| = \\||\\hat{w}\\|| + o(\\||w\\||)$, appears to be contentious. For instance, with fixed-bit quantization, the discrepancy between $\\||w\\||$ and $\\||\\hat{w}\\||$ is proportional to $\\||w\\||$, or in other words, $O(\\||w\\||)$.\n\n2. Lemma 1 could benefit from a more rigorous presentation, especially concerning the $o(\\cdot)$. Additionally, both Lemma 1 and Corollary 1 do not appear to offer significant novel contributions.\n\n3. The surrogate model feels somewhat contrived. Moreover, the method for deriving $\\tilde{w}_{\\ell}$ is not clear. For instance, are the angles $\\theta_{\\ell}$ specified? The notion of being \"uniformly distributed on a cone\" is ambiguous due to its unbounded norm. As a result, Theorem 1 requires a more comprehensive problem definition.\n\n4. There seem to be some technical inaccuracies in the proof of Theorem 1. It is also essential to ensure that each mutual information, \n$I(w_{\\ell}, \\tilde{w}_{\\ell})$, adheres to the distortion criteria.\n\n5. The link between distortion (related to weights) and deviation (pertaining to output) is unclear.\n\nQuestions: Please check Weakness.", "labeling_timestamp": "2026-01-11T17:00:06.211533", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not contain Lemma 1, any lemma statements, nor any use of o(·) notation or a discussion formalizing it. Therefore there is insufficient information in the supplied content to determine whether Lemma 1’s presentation (and the o(·) notation justification) is rigorous or not.", "evidence": "\"To analyze the optimality of the RIQ algorithm, we introduce a universal surrogate model that depicts quantization in terms of rotation of the model weights.\" \"In this section, we rigorously define the model compression optimization problem and the relevant known results on quantization and the rate-distortion theory.\"", "section": "Introduction / Section 3 Preliminaries (provided excerpt) — Lemma 1 not present"}
{"claim": "Lemma 1 and Corollary 1 do not appear to provide significant novel contributions beyond existing results.", "claim_type": "novelty", "paper_id": "CXjz7p4qha", "paper_title": "Rotation Invariant Quantization for Model Compression", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "fhz0No7vIA", "reviewer": "Reviewer_qJtZ", "review_text": "Summary: The paper examines post-training quantization of neural networks featuring linear layers, taking into account cosine distortion. The authors first demonstrate the rate-distortion trade-off between the original and quantized weights to establish the step size, $\\Delta_{\\ell}$, for each layer. Notably, all $\\Delta_{\\ell}$ values are governed by a singular parameter, $k$. Additionally, the paper investigates the rate-deviation analysis, wherein the deviation assesses the disparity in output. The authors introduce surrogate models in which the quantized weight is uniformly distributed across a space subject to random rotation, characterized by the angle $\\theta_{\\ell}$. Subsequently, it is proven that the mutual information is minimized when using the product distribution.\n\nStrengths: 1. The paper is well-written and straightforward to understand.\n2. The concept of rotational invariance in neural network quantization is intriguing.\n\nWeaknesses: 1. The central assumption, $\\||w\\|| = \\||\\hat{w}\\|| + o(\\||w\\||)$, appears to be contentious. For instance, with fixed-bit quantization, the discrepancy between $\\||w\\||$ and $\\||\\hat{w}\\||$ is proportional to $\\||w\\||$, or in other words, $O(\\||w\\||)$.\n\n2. Lemma 1 could benefit from a more rigorous presentation, especially concerning the $o(\\cdot)$. Additionally, both Lemma 1 and Corollary 1 do not appear to offer significant novel contributions.\n\n3. The surrogate model feels somewhat contrived. Moreover, the method for deriving $\\tilde{w}_{\\ell}$ is not clear. For instance, are the angles $\\theta_{\\ell}$ specified? The notion of being \"uniformly distributed on a cone\" is ambiguous due to its unbounded norm. As a result, Theorem 1 requires a more comprehensive problem definition.\n\n4. There seem to be some technical inaccuracies in the proof of Theorem 1. It is also essential to ensure that each mutual information, \n$I(w_{\\ell}, \\tilde{w}_{\\ell})$, adheres to the distortion criteria.\n\n5. The link between distortion (related to weights) and deviation (pertaining to output) is unclear.\n\nQuestions: Please check Weakness.", "labeling_timestamp": "2026-01-11T17:00:09.031616", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt claims theoretical results about optimality and introduces a surrogate model, but it does not include or display Lemma 1 or Corollary 1 themselves. Therefore it is impossible to verify whether those specific statements are novel or redundant relative to prior work from the given content.", "evidence": "To analyze the optimality of the RIQ algorithm, we introduce a universal surrogate model that depicts quantization in terms of rotation of the model weights. Its analysis reveals that the rate-distortion minimizing distribution for NN models is a spherical (rotation invariant) distribution constructed by the product of layers' spherical distribution.", "section": "1 INTRODUCTION"}
{"claim": "The surrogate model appears contrived and its assumptions lack clear motivation or connection to realistic quantization processes.", "claim_type": "subjective", "paper_id": "CXjz7p4qha", "paper_title": "Rotation Invariant Quantization for Model Compression", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "fhz0No7vIA", "reviewer": "Reviewer_qJtZ", "review_text": "Summary: The paper examines post-training quantization of neural networks featuring linear layers, taking into account cosine distortion. The authors first demonstrate the rate-distortion trade-off between the original and quantized weights to establish the step size, $\\Delta_{\\ell}$, for each layer. Notably, all $\\Delta_{\\ell}$ values are governed by a singular parameter, $k$. Additionally, the paper investigates the rate-deviation analysis, wherein the deviation assesses the disparity in output. The authors introduce surrogate models in which the quantized weight is uniformly distributed across a space subject to random rotation, characterized by the angle $\\theta_{\\ell}$. Subsequently, it is proven that the mutual information is minimized when using the product distribution.\n\nStrengths: 1. The paper is well-written and straightforward to understand.\n2. The concept of rotational invariance in neural network quantization is intriguing.\n\nWeaknesses: 1. The central assumption, $\\||w\\|| = \\||\\hat{w}\\|| + o(\\||w\\||)$, appears to be contentious. For instance, with fixed-bit quantization, the discrepancy between $\\||w\\||$ and $\\||\\hat{w}\\||$ is proportional to $\\||w\\||$, or in other words, $O(\\||w\\||)$.\n\n2. Lemma 1 could benefit from a more rigorous presentation, especially concerning the $o(\\cdot)$. Additionally, both Lemma 1 and Corollary 1 do not appear to offer significant novel contributions.\n\n3. The surrogate model feels somewhat contrived. Moreover, the method for deriving $\\tilde{w}_{\\ell}$ is not clear. For instance, are the angles $\\theta_{\\ell}$ specified? The notion of being \"uniformly distributed on a cone\" is ambiguous due to its unbounded norm. As a result, Theorem 1 requires a more comprehensive problem definition.\n\n4. There seem to be some technical inaccuracies in the proof of Theorem 1. It is also essential to ensure that each mutual information, \n$I(w_{\\ell}, \\tilde{w}_{\\ell})$, adheres to the distortion criteria.\n\n5. The link between distortion (related to weights) and deviation (pertaining to output) is unclear.\n\nQuestions: Please check Weakness.", "labeling_timestamp": "2026-01-11T17:00:27.350462", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly introduces and motivates the surrogate model via rotation-invariance arguments: it frames distortion with a rotation-invariant cosine measure, argues that rate-distortion optimality implies rotation-invariant solutions, and states the surrogate models quantization as rotations to analyze optimality. Thus the paper does provide clear motivation and connection for the surrogate model.", "evidence": "\"To analyze the optimality of the RIQ algorithm, we introduce a universal surrogate model that depicts quantization in terms of rotation of the model weights. Its analysis reveals that the rate-distortion minimizing distribution for NN models is a spherical (rotation invariant) distribution constructed by the product of layers' spherical distribution.\"; \"This distortion reflects the rotation angle that is required to align ˆf(x) with f(x). Noticeably, eq. (1) is a rotation-invariant measure... Thus, characterizing the connection between the deviation to the distortion in each layer is a key to optimizing the model quantization... Since these distortions are invariant to rotations, the rate-distortion theory tells that the optimal quantization must be rotation invariant as well, as we show in the sequel. This motivates a searching paradigm over rotation-invariant solutions, where the layers' rate are jointly adjusted.\"", "section": "Introduction; 3 Preliminaries / Problem Statement"}
{"claim": "The derivation procedure for the surrogate quantized weight \\tilde{w}_{\\ell} is not clearly specified in the manuscript.", "claim_type": "methodology", "paper_id": "CXjz7p4qha", "paper_title": "Rotation Invariant Quantization for Model Compression", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "fhz0No7vIA", "reviewer": "Reviewer_qJtZ", "review_text": "Summary: The paper examines post-training quantization of neural networks featuring linear layers, taking into account cosine distortion. The authors first demonstrate the rate-distortion trade-off between the original and quantized weights to establish the step size, $\\Delta_{\\ell}$, for each layer. Notably, all $\\Delta_{\\ell}$ values are governed by a singular parameter, $k$. Additionally, the paper investigates the rate-deviation analysis, wherein the deviation assesses the disparity in output. The authors introduce surrogate models in which the quantized weight is uniformly distributed across a space subject to random rotation, characterized by the angle $\\theta_{\\ell}$. Subsequently, it is proven that the mutual information is minimized when using the product distribution.\n\nStrengths: 1. The paper is well-written and straightforward to understand.\n2. The concept of rotational invariance in neural network quantization is intriguing.\n\nWeaknesses: 1. The central assumption, $\\||w\\|| = \\||\\hat{w}\\|| + o(\\||w\\||)$, appears to be contentious. For instance, with fixed-bit quantization, the discrepancy between $\\||w\\||$ and $\\||\\hat{w}\\||$ is proportional to $\\||w\\||$, or in other words, $O(\\||w\\||)$.\n\n2. Lemma 1 could benefit from a more rigorous presentation, especially concerning the $o(\\cdot)$. Additionally, both Lemma 1 and Corollary 1 do not appear to offer significant novel contributions.\n\n3. The surrogate model feels somewhat contrived. Moreover, the method for deriving $\\tilde{w}_{\\ell}$ is not clear. For instance, are the angles $\\theta_{\\ell}$ specified? The notion of being \"uniformly distributed on a cone\" is ambiguous due to its unbounded norm. As a result, Theorem 1 requires a more comprehensive problem definition.\n\n4. There seem to be some technical inaccuracies in the proof of Theorem 1. It is also essential to ensure that each mutual information, \n$I(w_{\\ell}, \\tilde{w}_{\\ell})$, adheres to the distortion criteria.\n\n5. The link between distortion (related to weights) and deviation (pertaining to output) is unclear.\n\nQuestions: Please check Weakness.", "labeling_timestamp": "2026-01-11T17:00:15.778991", "model": "gpt-5-mini", "label": "Supported", "justification": "The manuscript refers to a 'universal surrogate model' used to analyze quantization but does not present the derivation steps or an explicit procedure/formula for the surrogate quantized weight \\tilde{w}_{\\ell} in the provided sections (Introduction and Preliminaries). There is mention of the surrogate model conceptually, but no clear derivation or specification of \\tilde{w}_{\\ell} is given.", "evidence": "To analyze the optimality of the RIQ algorithm, we introduce a universal surrogate model that depicts quantization in terms of rotation of the model weights. Its analysis reveals that the rate-distortion minimizing distribution for NN models is a spherical (rotation invariant) distribution constructed by the product of layers' spherical distribution.", "section": "1 INTRODUCTION"}
{"claim": "The paper does not specify how the angles θ_{\\ell} of the random rotation are chosen or sampled.", "claim_type": "methodology", "paper_id": "CXjz7p4qha", "paper_title": "Rotation Invariant Quantization for Model Compression", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "fhz0No7vIA", "reviewer": "Reviewer_qJtZ", "review_text": "Summary: The paper examines post-training quantization of neural networks featuring linear layers, taking into account cosine distortion. The authors first demonstrate the rate-distortion trade-off between the original and quantized weights to establish the step size, $\\Delta_{\\ell}$, for each layer. Notably, all $\\Delta_{\\ell}$ values are governed by a singular parameter, $k$. Additionally, the paper investigates the rate-deviation analysis, wherein the deviation assesses the disparity in output. The authors introduce surrogate models in which the quantized weight is uniformly distributed across a space subject to random rotation, characterized by the angle $\\theta_{\\ell}$. Subsequently, it is proven that the mutual information is minimized when using the product distribution.\n\nStrengths: 1. The paper is well-written and straightforward to understand.\n2. The concept of rotational invariance in neural network quantization is intriguing.\n\nWeaknesses: 1. The central assumption, $\\||w\\|| = \\||\\hat{w}\\|| + o(\\||w\\||)$, appears to be contentious. For instance, with fixed-bit quantization, the discrepancy between $\\||w\\||$ and $\\||\\hat{w}\\||$ is proportional to $\\||w\\||$, or in other words, $O(\\||w\\||)$.\n\n2. Lemma 1 could benefit from a more rigorous presentation, especially concerning the $o(\\cdot)$. Additionally, both Lemma 1 and Corollary 1 do not appear to offer significant novel contributions.\n\n3. The surrogate model feels somewhat contrived. Moreover, the method for deriving $\\tilde{w}_{\\ell}$ is not clear. For instance, are the angles $\\theta_{\\ell}$ specified? The notion of being \"uniformly distributed on a cone\" is ambiguous due to its unbounded norm. As a result, Theorem 1 requires a more comprehensive problem definition.\n\n4. There seem to be some technical inaccuracies in the proof of Theorem 1. It is also essential to ensure that each mutual information, \n$I(w_{\\ell}, \\tilde{w}_{\\ell})$, adheres to the distortion criteria.\n\n5. The link between distortion (related to weights) and deviation (pertaining to output) is unclear.\n\nQuestions: Please check Weakness.", "labeling_timestamp": "2026-01-11T17:00:32.272542", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper discusses rotation-invariance and models quantization in terms of rotations (Introduction and Sec. 3.1) but nowhere in the provided content does it define or describe how angles θ_{ℓ} (theta_ℓ) are chosen or sampled. There is discussion of rotations but no specification of sampling or parametrization of angles.", "evidence": "\"Noticeably, eq. (1) is a rotation-invariant measure, which means that its value does not change when an arbitrary rotation is applied to its arguments.\"", "section": "3.1 PROBLEM STATEMENT"}
{"claim": "The phrase 'uniformly distributed on a cone' is ambiguous because it does not address the cone's unbounded norm or distribution support.", "claim_type": "methodology", "paper_id": "CXjz7p4qha", "paper_title": "Rotation Invariant Quantization for Model Compression", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "fhz0No7vIA", "reviewer": "Reviewer_qJtZ", "review_text": "Summary: The paper examines post-training quantization of neural networks featuring linear layers, taking into account cosine distortion. The authors first demonstrate the rate-distortion trade-off between the original and quantized weights to establish the step size, $\\Delta_{\\ell}$, for each layer. Notably, all $\\Delta_{\\ell}$ values are governed by a singular parameter, $k$. Additionally, the paper investigates the rate-deviation analysis, wherein the deviation assesses the disparity in output. The authors introduce surrogate models in which the quantized weight is uniformly distributed across a space subject to random rotation, characterized by the angle $\\theta_{\\ell}$. Subsequently, it is proven that the mutual information is minimized when using the product distribution.\n\nStrengths: 1. The paper is well-written and straightforward to understand.\n2. The concept of rotational invariance in neural network quantization is intriguing.\n\nWeaknesses: 1. The central assumption, $\\||w\\|| = \\||\\hat{w}\\|| + o(\\||w\\||)$, appears to be contentious. For instance, with fixed-bit quantization, the discrepancy between $\\||w\\||$ and $\\||\\hat{w}\\||$ is proportional to $\\||w\\||$, or in other words, $O(\\||w\\||)$.\n\n2. Lemma 1 could benefit from a more rigorous presentation, especially concerning the $o(\\cdot)$. Additionally, both Lemma 1 and Corollary 1 do not appear to offer significant novel contributions.\n\n3. The surrogate model feels somewhat contrived. Moreover, the method for deriving $\\tilde{w}_{\\ell}$ is not clear. For instance, are the angles $\\theta_{\\ell}$ specified? The notion of being \"uniformly distributed on a cone\" is ambiguous due to its unbounded norm. As a result, Theorem 1 requires a more comprehensive problem definition.\n\n4. There seem to be some technical inaccuracies in the proof of Theorem 1. It is also essential to ensure that each mutual information, \n$I(w_{\\ell}, \\tilde{w}_{\\ell})$, adheres to the distortion criteria.\n\n5. The link between distortion (related to weights) and deviation (pertaining to output) is unclear.\n\nQuestions: Please check Weakness.", "labeling_timestamp": "2026-01-11T17:00:39.140358", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not contain the phrase 'uniformly distributed on a cone' and instead discusses spherical (rotation-invariant) distributions. Because the specific phrasing referenced by the reviewer does not appear in the supplied text, there is insufficient information to determine whether that phrasing in the paper is ambiguous with respect to the cone's unbounded norm or distribution support.", "evidence": "1) \"Its analysis reveals that the rate-distortion minimizing distribution for NN models is a spherical (rotation invariant) distribution constructed by the product of layers' spherical distribution.\" 2) \"Since both norm and cosine distance are invariant to rotations, this yields the optimal solution in terms of rate distortion.\"", "section": "1 INTRODUCTION"}
{"claim": "Theorem 1 lacks a comprehensive problem definition, making its assumptions and applicability unclear.", "claim_type": "methodology", "paper_id": "CXjz7p4qha", "paper_title": "Rotation Invariant Quantization for Model Compression", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "fhz0No7vIA", "reviewer": "Reviewer_qJtZ", "review_text": "Summary: The paper examines post-training quantization of neural networks featuring linear layers, taking into account cosine distortion. The authors first demonstrate the rate-distortion trade-off between the original and quantized weights to establish the step size, $\\Delta_{\\ell}$, for each layer. Notably, all $\\Delta_{\\ell}$ values are governed by a singular parameter, $k$. Additionally, the paper investigates the rate-deviation analysis, wherein the deviation assesses the disparity in output. The authors introduce surrogate models in which the quantized weight is uniformly distributed across a space subject to random rotation, characterized by the angle $\\theta_{\\ell}$. Subsequently, it is proven that the mutual information is minimized when using the product distribution.\n\nStrengths: 1. The paper is well-written and straightforward to understand.\n2. The concept of rotational invariance in neural network quantization is intriguing.\n\nWeaknesses: 1. The central assumption, $\\||w\\|| = \\||\\hat{w}\\|| + o(\\||w\\||)$, appears to be contentious. For instance, with fixed-bit quantization, the discrepancy between $\\||w\\||$ and $\\||\\hat{w}\\||$ is proportional to $\\||w\\||$, or in other words, $O(\\||w\\||)$.\n\n2. Lemma 1 could benefit from a more rigorous presentation, especially concerning the $o(\\cdot)$. Additionally, both Lemma 1 and Corollary 1 do not appear to offer significant novel contributions.\n\n3. The surrogate model feels somewhat contrived. Moreover, the method for deriving $\\tilde{w}_{\\ell}$ is not clear. For instance, are the angles $\\theta_{\\ell}$ specified? The notion of being \"uniformly distributed on a cone\" is ambiguous due to its unbounded norm. As a result, Theorem 1 requires a more comprehensive problem definition.\n\n4. There seem to be some technical inaccuracies in the proof of Theorem 1. It is also essential to ensure that each mutual information, \n$I(w_{\\ell}, \\tilde{w}_{\\ell})$, adheres to the distortion criteria.\n\n5. The link between distortion (related to weights) and deviation (pertaining to output) is unclear.\n\nQuestions: Please check Weakness.", "labeling_timestamp": "2026-01-11T17:00:35.651879", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not contain Theorem 1 or its statement, so there is insufficient information to judge whether Theorem 1 lacks a comprehensive problem definition or whether its assumptions and applicability are clear.", "evidence": "Abstract: \"Then, we prove that our rotation-invariant approach is optimal in terms of compression.\"; 3.1 PROBLEM STATEMENT: \"Formally, given a trained model f , and a sample x , we wish to find a quantized model ˆ f whose weights ˆ w [1: L ] solves the following optimization problem.\" (the actual theorem statement and Theorem 1 are not present in the provided content)", "section": "Abstract; 3.1 PROBLEM STATEMENT"}
{"claim": "There are technical inaccuracies in the proof of Theorem 1 that need correction and clarification.", "claim_type": "methodology", "paper_id": "CXjz7p4qha", "paper_title": "Rotation Invariant Quantization for Model Compression", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "fhz0No7vIA", "reviewer": "Reviewer_qJtZ", "review_text": "Summary: The paper examines post-training quantization of neural networks featuring linear layers, taking into account cosine distortion. The authors first demonstrate the rate-distortion trade-off between the original and quantized weights to establish the step size, $\\Delta_{\\ell}$, for each layer. Notably, all $\\Delta_{\\ell}$ values are governed by a singular parameter, $k$. Additionally, the paper investigates the rate-deviation analysis, wherein the deviation assesses the disparity in output. The authors introduce surrogate models in which the quantized weight is uniformly distributed across a space subject to random rotation, characterized by the angle $\\theta_{\\ell}$. Subsequently, it is proven that the mutual information is minimized when using the product distribution.\n\nStrengths: 1. The paper is well-written and straightforward to understand.\n2. The concept of rotational invariance in neural network quantization is intriguing.\n\nWeaknesses: 1. The central assumption, $\\||w\\|| = \\||\\hat{w}\\|| + o(\\||w\\||)$, appears to be contentious. For instance, with fixed-bit quantization, the discrepancy between $\\||w\\||$ and $\\||\\hat{w}\\||$ is proportional to $\\||w\\||$, or in other words, $O(\\||w\\||)$.\n\n2. Lemma 1 could benefit from a more rigorous presentation, especially concerning the $o(\\cdot)$. Additionally, both Lemma 1 and Corollary 1 do not appear to offer significant novel contributions.\n\n3. The surrogate model feels somewhat contrived. Moreover, the method for deriving $\\tilde{w}_{\\ell}$ is not clear. For instance, are the angles $\\theta_{\\ell}$ specified? The notion of being \"uniformly distributed on a cone\" is ambiguous due to its unbounded norm. As a result, Theorem 1 requires a more comprehensive problem definition.\n\n4. There seem to be some technical inaccuracies in the proof of Theorem 1. It is also essential to ensure that each mutual information, \n$I(w_{\\ell}, \\tilde{w}_{\\ell})$, adheres to the distortion criteria.\n\n5. The link between distortion (related to weights) and deviation (pertaining to output) is unclear.\n\nQuestions: Please check Weakness.", "labeling_timestamp": "2026-01-11T17:00:50.544004", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The supplied paper excerpt does not include Theorem 1 or its proof, so whether there are technical inaccuracies in that proof cannot be verified from the provided content. The paper claims a proof exists, but the proof/text of Theorem 1 is not present in the excerpt.", "evidence": "\"Then, we prove that our rotation-invariant approach is optimal in terms of compression.\"", "section": "ABSTRACT"}
{"claim": "The manuscript does not demonstrate that each mutual information term I(w_{\\ell}, \\tilde{w}_{\\ell}) satisfies the claimed distortion constraints.", "claim_type": "methodology", "paper_id": "CXjz7p4qha", "paper_title": "Rotation Invariant Quantization for Model Compression", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "fhz0No7vIA", "reviewer": "Reviewer_qJtZ", "review_text": "Summary: The paper examines post-training quantization of neural networks featuring linear layers, taking into account cosine distortion. The authors first demonstrate the rate-distortion trade-off between the original and quantized weights to establish the step size, $\\Delta_{\\ell}$, for each layer. Notably, all $\\Delta_{\\ell}$ values are governed by a singular parameter, $k$. Additionally, the paper investigates the rate-deviation analysis, wherein the deviation assesses the disparity in output. The authors introduce surrogate models in which the quantized weight is uniformly distributed across a space subject to random rotation, characterized by the angle $\\theta_{\\ell}$. Subsequently, it is proven that the mutual information is minimized when using the product distribution.\n\nStrengths: 1. The paper is well-written and straightforward to understand.\n2. The concept of rotational invariance in neural network quantization is intriguing.\n\nWeaknesses: 1. The central assumption, $\\||w\\|| = \\||\\hat{w}\\|| + o(\\||w\\||)$, appears to be contentious. For instance, with fixed-bit quantization, the discrepancy between $\\||w\\||$ and $\\||\\hat{w}\\||$ is proportional to $\\||w\\||$, or in other words, $O(\\||w\\||)$.\n\n2. Lemma 1 could benefit from a more rigorous presentation, especially concerning the $o(\\cdot)$. Additionally, both Lemma 1 and Corollary 1 do not appear to offer significant novel contributions.\n\n3. The surrogate model feels somewhat contrived. Moreover, the method for deriving $\\tilde{w}_{\\ell}$ is not clear. For instance, are the angles $\\theta_{\\ell}$ specified? The notion of being \"uniformly distributed on a cone\" is ambiguous due to its unbounded norm. As a result, Theorem 1 requires a more comprehensive problem definition.\n\n4. There seem to be some technical inaccuracies in the proof of Theorem 1. It is also essential to ensure that each mutual information, \n$I(w_{\\ell}, \\tilde{w}_{\\ell})$, adheres to the distortion criteria.\n\n5. The link between distortion (related to weights) and deviation (pertaining to output) is unclear.\n\nQuestions: Please check Weakness.", "labeling_timestamp": "2026-01-11T17:00:57.508746", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt defines layerwise distortion (cosine distance) and discusses rate-distortion and mixed-precision quantization (Sections 3 and 3.1), but it does not mention or analyze mutual information terms I(w_ℓ, \tilde{w}_ℓ) or show that per-layer mutual information satisfies any claimed distortion constraints in the available text. Therefore there is insufficient information to confirm the reviewer's claim.", "evidence": "“Hereafter, the term deviation is used to depict d_{f,x}(w[1:L], \\hat w[1:L]), which is the cosine distance between the outputs, whereas the term distortion refers to the cosine distance d(w_\\ell, \\hat w_\\ell) between the weights w_\\ell and their quantized representation \\hat w_\\ell.”\n\n“Thus, characterizing the connection between the deviation to the distortion in each layer is a key to optimizing the model quantization.”", "section": "3.1 PROBLEM STATEMENT"}
{"claim": "The relationship between weight distortion measures and output deviation is not clearly established or justified in the paper.", "claim_type": "methodology", "paper_id": "CXjz7p4qha", "paper_title": "Rotation Invariant Quantization for Model Compression", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "fhz0No7vIA", "reviewer": "Reviewer_qJtZ", "review_text": "Summary: The paper examines post-training quantization of neural networks featuring linear layers, taking into account cosine distortion. The authors first demonstrate the rate-distortion trade-off between the original and quantized weights to establish the step size, $\\Delta_{\\ell}$, for each layer. Notably, all $\\Delta_{\\ell}$ values are governed by a singular parameter, $k$. Additionally, the paper investigates the rate-deviation analysis, wherein the deviation assesses the disparity in output. The authors introduce surrogate models in which the quantized weight is uniformly distributed across a space subject to random rotation, characterized by the angle $\\theta_{\\ell}$. Subsequently, it is proven that the mutual information is minimized when using the product distribution.\n\nStrengths: 1. The paper is well-written and straightforward to understand.\n2. The concept of rotational invariance in neural network quantization is intriguing.\n\nWeaknesses: 1. The central assumption, $\\||w\\|| = \\||\\hat{w}\\|| + o(\\||w\\||)$, appears to be contentious. For instance, with fixed-bit quantization, the discrepancy between $\\||w\\||$ and $\\||\\hat{w}\\||$ is proportional to $\\||w\\||$, or in other words, $O(\\||w\\||)$.\n\n2. Lemma 1 could benefit from a more rigorous presentation, especially concerning the $o(\\cdot)$. Additionally, both Lemma 1 and Corollary 1 do not appear to offer significant novel contributions.\n\n3. The surrogate model feels somewhat contrived. Moreover, the method for deriving $\\tilde{w}_{\\ell}$ is not clear. For instance, are the angles $\\theta_{\\ell}$ specified? The notion of being \"uniformly distributed on a cone\" is ambiguous due to its unbounded norm. As a result, Theorem 1 requires a more comprehensive problem definition.\n\n4. There seem to be some technical inaccuracies in the proof of Theorem 1. It is also essential to ensure that each mutual information, \n$I(w_{\\ell}, \\tilde{w}_{\\ell})$, adheres to the distortion criteria.\n\n5. The link between distortion (related to weights) and deviation (pertaining to output) is unclear.\n\nQuestions: Please check Weakness.", "labeling_timestamp": "2026-01-11T17:00:57.228006", "model": "gpt-5-mini", "label": "Supported", "justification": "In Section 3.1 the paper itself admits that deriving an exact link between per-layer weight distortions and the final output deviation is intricate and provides only an intuitive explanation rather than a clear, formal establishment in the quoted content.", "evidence": "“The resulting deviation in eq. (1) relates to the distortions gathered through the layers. In particular, each quantized layer produces a rotation distortion in its output, and this distortion keeps propagating and accumulating through the layers until reaching the model's output. Thus, characterizing the connection between the deviation to the distortion in each layer is a key to optimizing the model quantization. Though finding the exact link is intricate in general, intuitively, as the layers' quantization rate jointly decrease, the deviation increases monotonically with the layers' distortion.”", "section": "3.1 PROBLEM STATEMENT"}
{"claim": "Both the critic and the generator are implemented as LLMs, which may amplify and propagate inherent LLM errors and biases throughout Prospector's decision-making pipeline.", "claim_type": "methodology", "paper_id": "YKK1jXEWja", "paper_title": "Prospector: Improving LLM Agents with Self-Asking and Trajectory Ranking", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "2SqlrxhfRb", "reviewer": "Reviewer_EsKc", "review_text": "Summary: The paper introduces Prospector, an innovative LLM agent designed for decision-making tasks. Unlike previous methods such as ReAct and Reflexion, which rely on few-shot in-context learning or use feedback from the environment, Prospector integrates two novel components: Self-Asking and Trajectory Ranking. Self-Asking allows the LLM to pose and answer its own questions during few-shot demonstrations, aiming to collect more pertinent information for decision-making. Trajectory Ranking, on the other hand, involves generating multiple action trajectories and selecting the most rewarding one using reward prediction models. The authors show that Prospector significantly outperforms existing methods on benchmark tasks like ALFWorld and WebShop.\n\nStrengths: 1. The paper addresses a gap in current LLM-based decision-making methods by integrating feedback from the environment and incorporating stochasticity in trajectory generation.\n2. The proposed method shows empirical success, outperforming existing state-of-the-art methods on standard benchmarks.\n3. Prospector offers an approach that avoids costly fine-tuning, making it more generalizable and efficient.\n\nWeaknesses: 1. Both the critic and the generator are LLMs. This could amplify any existing issues inherent to LLMs.\n2. Limited discussion on the limitations of the reward prediction models used for Trajectory Ranking.\n3. The paper could benefit from a more comprehensive analysis comparing the computational overhead introduced by the Self-Asking and Trajectory Ranking components.\n\nQuestions: 1. How does the computational complexity of Prospector compare to that of existing methods like ReAct and Reflexion?\n2. Could you elaborate on the reward prediction models used in Trajectory Ranking? What are the limitations of the reward prediction models you used, and how do they impact the overall performance of Prospector?\n3. Are there specific types of questions or domains where the Self-Asking mechanism is more or less effective?", "labeling_timestamp": "2026-01-11T17:01:04.927854", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly states that both the actor (generator) and the critic are implemented as LLMs (supported). However, the paper does not claim or analyze that this design \"may amplify and propagate inherent LLM errors and biases throughout Prospector's decision-making pipeline\"; that consequence is not discussed, so the second part of the reviewer claim is not supported by the paper.", "evidence": "1) \"Prospector is a reflective LLM agent that consists of two complementary LLMs such as LLM Actor and LLM Critic for solving complex interactive decisionmaking tasks.\" 2) \"Few-shot LLM Critics. Motivated by recent methods of using LLMs as an evaluator..., we attempt to use LLMs as reward estimators for interactive decision-making tasks.\"", "section": "Introduction (Figure 1 caption) and Section 3.2 (Trajectory Ranking / Few-shot LLM Critics)"}
{"claim": "The paper provides limited discussion of the limitations and failure modes of the reward prediction models used for Trajectory Ranking.", "claim_type": "subjective", "paper_id": "YKK1jXEWja", "paper_title": "Prospector: Improving LLM Agents with Self-Asking and Trajectory Ranking", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "2SqlrxhfRb", "reviewer": "Reviewer_EsKc", "review_text": "Summary: The paper introduces Prospector, an innovative LLM agent designed for decision-making tasks. Unlike previous methods such as ReAct and Reflexion, which rely on few-shot in-context learning or use feedback from the environment, Prospector integrates two novel components: Self-Asking and Trajectory Ranking. Self-Asking allows the LLM to pose and answer its own questions during few-shot demonstrations, aiming to collect more pertinent information for decision-making. Trajectory Ranking, on the other hand, involves generating multiple action trajectories and selecting the most rewarding one using reward prediction models. The authors show that Prospector significantly outperforms existing methods on benchmark tasks like ALFWorld and WebShop.\n\nStrengths: 1. The paper addresses a gap in current LLM-based decision-making methods by integrating feedback from the environment and incorporating stochasticity in trajectory generation.\n2. The proposed method shows empirical success, outperforming existing state-of-the-art methods on standard benchmarks.\n3. Prospector offers an approach that avoids costly fine-tuning, making it more generalizable and efficient.\n\nWeaknesses: 1. Both the critic and the generator are LLMs. This could amplify any existing issues inherent to LLMs.\n2. Limited discussion on the limitations of the reward prediction models used for Trajectory Ranking.\n3. The paper could benefit from a more comprehensive analysis comparing the computational overhead introduced by the Self-Asking and Trajectory Ranking components.\n\nQuestions: 1. How does the computational complexity of Prospector compare to that of existing methods like ReAct and Reflexion?\n2. Could you elaborate on the reward prediction models used in Trajectory Ranking? What are the limitations of the reward prediction models you used, and how do they impact the overall performance of Prospector?\n3. Are there specific types of questions or domains where the Self-Asking mechanism is more or less effective?", "labeling_timestamp": "2026-01-11T17:01:01.878370", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper only briefly acknowledges limits of the reward predictors (noting GPT-3 struggles in few-shot reward prediction and that fine-tuning can help) but does not present an extended analysis of failure modes or limitations of those models beyond these short remarks and pointer to tables.", "evidence": "\"In some complex environments such as WebShop (Yao et al., 2022a), one of the most powerful LLMs such as GPT-3 have difficulty in reward prediction in a few-shot manner (see Table 6).\" \n\n\"We investigate the impact of the reward model used in trajectory ranking, and demonstrate that Prospector robustly generates strategic actions in both cases when using LLM-based reward model is additionally fine-tuned or not. (see Table 4 and 7).\"", "section": "3.2 Trajectory Ranking; Contributions / Experiments sections"}
{"claim": "The paper lacks a comprehensive analysis comparing the additional computational overhead introduced by Self-Asking and Trajectory Ranking components.", "claim_type": "quantitative", "paper_id": "YKK1jXEWja", "paper_title": "Prospector: Improving LLM Agents with Self-Asking and Trajectory Ranking", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "2SqlrxhfRb", "reviewer": "Reviewer_EsKc", "review_text": "Summary: The paper introduces Prospector, an innovative LLM agent designed for decision-making tasks. Unlike previous methods such as ReAct and Reflexion, which rely on few-shot in-context learning or use feedback from the environment, Prospector integrates two novel components: Self-Asking and Trajectory Ranking. Self-Asking allows the LLM to pose and answer its own questions during few-shot demonstrations, aiming to collect more pertinent information for decision-making. Trajectory Ranking, on the other hand, involves generating multiple action trajectories and selecting the most rewarding one using reward prediction models. The authors show that Prospector significantly outperforms existing methods on benchmark tasks like ALFWorld and WebShop.\n\nStrengths: 1. The paper addresses a gap in current LLM-based decision-making methods by integrating feedback from the environment and incorporating stochasticity in trajectory generation.\n2. The proposed method shows empirical success, outperforming existing state-of-the-art methods on standard benchmarks.\n3. Prospector offers an approach that avoids costly fine-tuning, making it more generalizable and efficient.\n\nWeaknesses: 1. Both the critic and the generator are LLMs. This could amplify any existing issues inherent to LLMs.\n2. Limited discussion on the limitations of the reward prediction models used for Trajectory Ranking.\n3. The paper could benefit from a more comprehensive analysis comparing the computational overhead introduced by the Self-Asking and Trajectory Ranking components.\n\nQuestions: 1. How does the computational complexity of Prospector compare to that of existing methods like ReAct and Reflexion?\n2. Could you elaborate on the reward prediction models used in Trajectory Ranking? What are the limitations of the reward prediction models you used, and how do they impact the overall performance of Prospector?\n3. Are there specific types of questions or domains where the Self-Asking mechanism is more or less effective?", "labeling_timestamp": "2026-01-11T17:01:15.177058", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes Self-Asking and Trajectory Ranking and reports task success rates and reward-model investigations, but it does not present measurements or a quantitative comparison of computational overhead (e.g., runtime, number of model calls, latency, or cost) introduced by those components. The method and experiments focus on effectiveness rather than a comprehensive cost/compute analysis.", "evidence": "1) \"Since generating trajectory from an LLM is relatively much cheaper than training an LLM, we consider generating trajectories and selecting the best trajectory among them instead of training the LLM.\" (Section 3.2)\n2) \"We demonstrate that Prospector significantly outperforms ReAct (Yao et al., 2022b) and Reflexion (Shinn et al., 2023) on the representative language-based interactive decisionmaking benchmarks including ALFWorld (Shridhar et al., 2020b) and WebShop (Yao et al., 2022a) (see Table 2 and 5).\" (Contributions / Introduction)", "section": "3.2 TRAJECTORY RANKING; Introduction / Contributions"}
{"claim": "The paper does not report or compare Prospector's computational complexity or runtime to existing methods such as ReAct and Reflexion.", "claim_type": "baseline", "paper_id": "YKK1jXEWja", "paper_title": "Prospector: Improving LLM Agents with Self-Asking and Trajectory Ranking", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "2SqlrxhfRb", "reviewer": "Reviewer_EsKc", "review_text": "Summary: The paper introduces Prospector, an innovative LLM agent designed for decision-making tasks. Unlike previous methods such as ReAct and Reflexion, which rely on few-shot in-context learning or use feedback from the environment, Prospector integrates two novel components: Self-Asking and Trajectory Ranking. Self-Asking allows the LLM to pose and answer its own questions during few-shot demonstrations, aiming to collect more pertinent information for decision-making. Trajectory Ranking, on the other hand, involves generating multiple action trajectories and selecting the most rewarding one using reward prediction models. The authors show that Prospector significantly outperforms existing methods on benchmark tasks like ALFWorld and WebShop.\n\nStrengths: 1. The paper addresses a gap in current LLM-based decision-making methods by integrating feedback from the environment and incorporating stochasticity in trajectory generation.\n2. The proposed method shows empirical success, outperforming existing state-of-the-art methods on standard benchmarks.\n3. Prospector offers an approach that avoids costly fine-tuning, making it more generalizable and efficient.\n\nWeaknesses: 1. Both the critic and the generator are LLMs. This could amplify any existing issues inherent to LLMs.\n2. Limited discussion on the limitations of the reward prediction models used for Trajectory Ranking.\n3. The paper could benefit from a more comprehensive analysis comparing the computational overhead introduced by the Self-Asking and Trajectory Ranking components.\n\nQuestions: 1. How does the computational complexity of Prospector compare to that of existing methods like ReAct and Reflexion?\n2. Could you elaborate on the reward prediction models used in Trajectory Ranking? What are the limitations of the reward prediction models you used, and how do they impact the overall performance of Prospector?\n3. Are there specific types of questions or domains where the Self-Asking mechanism is more or less effective?", "labeling_timestamp": "2026-01-11T17:01:46.031641", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper compares Prospector to ReAct and Reflexion in terms of success rate on benchmarks and discusses method design, but contains no reported measurements or analyses of computational complexity or runtime (no tables or text reporting runtimes or complexity comparisons).", "evidence": ["Abstract: \"On the representative decision-making benchmark environments such as ALFWorld and WebShop, we empirically demonstrate that Prospector can considerably increase the success rate of given tasks, while outperforming recent advancements such as ReAct and Reflexion.\"", "Section 4.1.1 (SUCCESS RATE): \"In Table 2, we compare the success rate of Prospector with the recent LLM agents such as ReAct Yao et al. (2022b) and Reflexion (Shinn et al., 2023) on the ALFWorld environment.\"", "Section 3.2 (TRAJECTORY RANKING): \"Since generating trajectory from an LLM is relatively much cheaper than training an LLM, we consider generating trajectories and selecting the best trajectory among them instead of training the LLM.\""], "section": "Abstract; 3.2 Trajectory Ranking; 4.1.1 Success Rate"}
{"claim": "The manuscript does not clearly specify which reward prediction models are used for Trajectory Ranking or detail their architectures and training procedures.", "claim_type": "methodology", "paper_id": "YKK1jXEWja", "paper_title": "Prospector: Improving LLM Agents with Self-Asking and Trajectory Ranking", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "2SqlrxhfRb", "reviewer": "Reviewer_EsKc", "review_text": "Summary: The paper introduces Prospector, an innovative LLM agent designed for decision-making tasks. Unlike previous methods such as ReAct and Reflexion, which rely on few-shot in-context learning or use feedback from the environment, Prospector integrates two novel components: Self-Asking and Trajectory Ranking. Self-Asking allows the LLM to pose and answer its own questions during few-shot demonstrations, aiming to collect more pertinent information for decision-making. Trajectory Ranking, on the other hand, involves generating multiple action trajectories and selecting the most rewarding one using reward prediction models. The authors show that Prospector significantly outperforms existing methods on benchmark tasks like ALFWorld and WebShop.\n\nStrengths: 1. The paper addresses a gap in current LLM-based decision-making methods by integrating feedback from the environment and incorporating stochasticity in trajectory generation.\n2. The proposed method shows empirical success, outperforming existing state-of-the-art methods on standard benchmarks.\n3. Prospector offers an approach that avoids costly fine-tuning, making it more generalizable and efficient.\n\nWeaknesses: 1. Both the critic and the generator are LLMs. This could amplify any existing issues inherent to LLMs.\n2. Limited discussion on the limitations of the reward prediction models used for Trajectory Ranking.\n3. The paper could benefit from a more comprehensive analysis comparing the computational overhead introduced by the Self-Asking and Trajectory Ranking components.\n\nQuestions: 1. How does the computational complexity of Prospector compare to that of existing methods like ReAct and Reflexion?\n2. Could you elaborate on the reward prediction models used in Trajectory Ranking? What are the limitations of the reward prediction models you used, and how do they impact the overall performance of Prospector?\n3. Are there specific types of questions or domains where the Self-Asking mechanism is more or less effective?", "labeling_timestamp": "2026-01-11T17:01:49.156988", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper describes two kinds of reward predictors (few-shot LLM critics and fine-tuned LLM critics) and provides a critic prompt template, but the main text does not list specific model names/architectures or training procedures — it only refers to details in tables (experiments/appendix). Thus the claim is partially correct: the paper gives high-level descriptions but lacks clear, explicit architecture/training details in the main text provided.", "evidence": "“Therefore, we investigate two methods to estimate the trajectory reward from a given dataset: (1) Few-shot LLM critics, and (2) Fine-tuned LLM critics.”\n\n“Few-shot LLM Critics. Motivated by recent methods of using LLMs as an evaluator (Li et al., 2023; Ye et al., 2023), we attempt to use LLMs as reward estimators for interactive decision-making tasks. To evaluate the trajectories without additional training of the reward model, we use few-shot in-context learning with reward-labeled trajectories.”\n\n“Fine-tuned LLM Critics. In some complex environments such as WebShop (Yao et al., 2022a), one of the most powerful LLMs such as GPT-3 have difficulty in reward prediction in a few-shot manner (see Table 6). In this case, open-sourced LLMs fine-tuned on trajectory data can help to increase the performance of Prospector agents. The details can be found in Table 5 and Table 7 in the Experiment section.”", "section": "3.2 TRAJECTORY RANKING"}
{"claim": "The paper does not analyze how limitations or inaccuracies of the reward prediction models affect Prospector's trajectory selection and overall performance.", "claim_type": "experimental", "paper_id": "YKK1jXEWja", "paper_title": "Prospector: Improving LLM Agents with Self-Asking and Trajectory Ranking", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "2SqlrxhfRb", "reviewer": "Reviewer_EsKc", "review_text": "Summary: The paper introduces Prospector, an innovative LLM agent designed for decision-making tasks. Unlike previous methods such as ReAct and Reflexion, which rely on few-shot in-context learning or use feedback from the environment, Prospector integrates two novel components: Self-Asking and Trajectory Ranking. Self-Asking allows the LLM to pose and answer its own questions during few-shot demonstrations, aiming to collect more pertinent information for decision-making. Trajectory Ranking, on the other hand, involves generating multiple action trajectories and selecting the most rewarding one using reward prediction models. The authors show that Prospector significantly outperforms existing methods on benchmark tasks like ALFWorld and WebShop.\n\nStrengths: 1. The paper addresses a gap in current LLM-based decision-making methods by integrating feedback from the environment and incorporating stochasticity in trajectory generation.\n2. The proposed method shows empirical success, outperforming existing state-of-the-art methods on standard benchmarks.\n3. Prospector offers an approach that avoids costly fine-tuning, making it more generalizable and efficient.\n\nWeaknesses: 1. Both the critic and the generator are LLMs. This could amplify any existing issues inherent to LLMs.\n2. Limited discussion on the limitations of the reward prediction models used for Trajectory Ranking.\n3. The paper could benefit from a more comprehensive analysis comparing the computational overhead introduced by the Self-Asking and Trajectory Ranking components.\n\nQuestions: 1. How does the computational complexity of Prospector compare to that of existing methods like ReAct and Reflexion?\n2. Could you elaborate on the reward prediction models used in Trajectory Ranking? What are the limitations of the reward prediction models you used, and how do they impact the overall performance of Prospector?\n3. Are there specific types of questions or domains where the Self-Asking mechanism is more or less effective?", "labeling_timestamp": "2026-01-11T17:01:28.600581", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states that it investigates the impact of the reward model used in trajectory ranking and reports comparisons between few-shot (LLM critic) and fine-tuned LLM critics, including noting GPT-3's difficulty in few-shot reward prediction and referring to specific result tables (Tables 4, 6, 7). Thus it does analyze how limitations/inaccuracies of reward prediction models affect Prospector's performance.", "evidence": "1) \"We investigate the impact of the reward model used in trajectory ranking, and demonstrate that Prospector robustly generates strategic actions in both cases when using LLM-based reward model is additionally fine-tuned or not. (see Table 4 and 7).\" 2) \"Few-shot LLM Critics. ... To evaluate the trajectories without additional training of the reward model, we use few-shot in-context learning with reward-labeled trajectories.\" 3) \"Fine-tuned LLM Critics. In some complex environments such as WebShop (Yao et al., 2022a), one of the most powerful LLMs such as GPT-3 have difficulty in reward prediction in a few-shot manner (see Table 6). In this case, open-sourced LLMs fine-tuned on trajectory data can help to increase the performance of Prospector agents.\"", "section": "Abstract; 3.2 Trajectory Ranking"}
{"claim": "There is no analysis of whether the Self-Asking mechanism performs differently across question types, task domains, or input distributions.", "claim_type": "experimental", "paper_id": "YKK1jXEWja", "paper_title": "Prospector: Improving LLM Agents with Self-Asking and Trajectory Ranking", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "2SqlrxhfRb", "reviewer": "Reviewer_EsKc", "review_text": "Summary: The paper introduces Prospector, an innovative LLM agent designed for decision-making tasks. Unlike previous methods such as ReAct and Reflexion, which rely on few-shot in-context learning or use feedback from the environment, Prospector integrates two novel components: Self-Asking and Trajectory Ranking. Self-Asking allows the LLM to pose and answer its own questions during few-shot demonstrations, aiming to collect more pertinent information for decision-making. Trajectory Ranking, on the other hand, involves generating multiple action trajectories and selecting the most rewarding one using reward prediction models. The authors show that Prospector significantly outperforms existing methods on benchmark tasks like ALFWorld and WebShop.\n\nStrengths: 1. The paper addresses a gap in current LLM-based decision-making methods by integrating feedback from the environment and incorporating stochasticity in trajectory generation.\n2. The proposed method shows empirical success, outperforming existing state-of-the-art methods on standard benchmarks.\n3. Prospector offers an approach that avoids costly fine-tuning, making it more generalizable and efficient.\n\nWeaknesses: 1. Both the critic and the generator are LLMs. This could amplify any existing issues inherent to LLMs.\n2. Limited discussion on the limitations of the reward prediction models used for Trajectory Ranking.\n3. The paper could benefit from a more comprehensive analysis comparing the computational overhead introduced by the Self-Asking and Trajectory Ranking components.\n\nQuestions: 1. How does the computational complexity of Prospector compare to that of existing methods like ReAct and Reflexion?\n2. Could you elaborate on the reward prediction models used in Trajectory Ranking? What are the limitations of the reward prediction models you used, and how do they impact the overall performance of Prospector?\n3. Are there specific types of questions or domains where the Self-Asking mechanism is more or less effective?", "labeling_timestamp": "2026-01-11T17:01:51.987485", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes and introduces Self-Asking (Section 3.1) and reports overall performance comparisons on benchmarks (Section 4), but contains no analysis or breakdown showing how Self-Asking performs across different question types, task domains, or input distributions. It does report other analyses (e.g., impact of reward model) but not the claimed per-type/domain/input analysis.", "evidence": "1) \"We present Self-Asking which elicits the LLM agent to generate a question and answer itself, by prompting few-shot demonstrations (i.e. in-context learning).\" (Section 3.1)\n\n2) \"In Table 2, we compare the success rate of Prospector with the recent LLM agents such as ReAct ... on the ALFWorld environment.\" (Section 4.1.1)\n\n3) \"We investigate the impact of the reward model used in trajectory ranking, and demonstrate that Prospector robustly generates strategic actions ... (see Table 4 and 7).\" (Introduction / Contributions)", "section": "3.1 SELF-ASKING; 4 EXPERIMENTS"}
{"claim": "The paper lacks a rigorous empirical or theoretical demonstration that SHRED-generated samples lie on the data manifold.", "claim_type": "experimental", "paper_id": "ZnmofqLWMQ", "paper_title": "Zero-shot Image Restoration via Diffusion Inversion", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ijAQPsrAef", "reviewer": "Reviewer_bZYN", "review_text": "Summary: The paper presents SHRED (zero-SHot image REstoration via Diffusion inversion), a new image restoration method using pre-trained diffusion models. SHRED uniquely maintains the integrity of the data manifold by not altering the reverse sampling process during restoration. It optimizes the initial diffusion noise to reconstruct high-quality images efficiently, avoiding the need for model retraining or fine-tuning. SHRED demonstrates superior performance on various image restoration tasks, achieving state-of-the-art results in zero-shot benchmarks.\n\nStrengths: **Strengths of the Paper:**\n\n1. **Originality:**\n   - The introduction of SHRED represents a new direction in leveraging pre-trained diffusion models for image restoration tasks. The approach of not altering the reverse sampling process is a departure from previous methods, addressing limitations from prior results.\n\n2. **Quality:**\n   - The quality of the research is evident in the comprehensive experimental validation across various tasks such as inpainting, super-resolution, and blind deconvolution. The use of well-established metrics like FID and LPIPS lends credibility to the reported results.\n   - The state-of-the-art performance of SHRED, as demonstrated through quantitative and qualitative evaluations, underscores the method's effectiveness.\n\n3. **Clarity:**\n   - Clarity is one of the paper's strengths, with a well-organized presentation of the content. The clarity in writing ensures that the concepts are accessible and the results are understandable.\n   - The background provided on DDPM and DDIM is thorough, facilitating a clear understanding of the advancements SHRED brings to the field.\n\n4. **Significance:**\n   - The paper is significant in its potential applicability to a broad spectrum of image restoration tasks, demonstrating the adaptability of SHRED to different challenges without the need for retraining.\n\nThe paper's contributions are presented with clarity and are supported by solid empirical evidence, making it a valuable addition to the literature on diffusion models and image restoration.\n\nWeaknesses: **Weaknesses of the Paper:**\n\n1. **Guarantee of Data Manifold Integrity:**\n   - The paper positions SHRED as a method that maintains the integrity of the data manifold during image restoration, which is a central claim for its novelty. However, the paper lacks a rigorous demonstration or proof that the samples generated by SHRED indeed lie on the data manifold. This is a significant gap, as the main criticism of prior methods is their potential deviation from the manifold. To strengthen this claim, the authors could provide empirical evidence or a theoretical guarantee, possibly through visualizations of the manifold or quantitative measures that can assess this aspect.\n\n2. **Comparison with MCG from Chung et al. (2022b):**\n   - The paper does not provide a detailed comparison with the MCG method proposed by Chung et al. (2022b), which also aims to correct samples to ensure they are on the data manifold. A deeper theoretical and empirical analysis comparing SHRED with MCG would be beneficial. This could include side-by-side comparisons on the same tasks, using the same metrics, and a discussion on the theoretical underpinnings of both methods. Such a comparison would be valuable for readers to understand the relative merits and trade-offs of these approaches.\n\n3. **Computational Efficiency:**\n   - In Table 3, SHRED is slower than DDRM and DDPM, which could limit its practicality for real-world applications where computational resources or time are constrained. The authors could explore ways to improve the efficiency of SHRED, perhaps by optimizing the algorithm or by proposing a more computationally efficient variant that maintains most of the method's benefits.\n\n4. **Novelty and Originality in Mathematical Derivation:**\n   - The mathematical derivation of SHRED's methodology does not appear to be a novel contribution, which may lead to questions about the paper's originality. The authors could strengthen this aspect by clearly delineating the novel components of their mathematical approach, contrasting it with existing methods, and discussing how these novel aspects contribute to the method's performance.\n\nQuestions: To address the above weaknesses, the authors could consider the following actions:\n\n- Provide empirical evidence or theoretical justification for the claim that SHRED generates samples along the data manifold.\n- Conduct a thorough comparison with MCG, including both theoretical and empirical analyses.\n- Investigate and propose methods to improve the computational efficiency of SHRED.\n- Clarify the novelty in the mathematical derivation of SHRED, differentiating it from existing approaches.\n\nBy addressing these points, the authors could significantly strengthen the paper and its contributions to the field.", "labeling_timestamp": "2026-01-11T17:01:52.709442", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper repeatedly claims that SHRED 'ensures' or 'provides more guarantees' that the generation path lies on the data manifold (Introduction and contributions), but the provided content does not include a formal theoretical proof, theorem, or a dedicated empirical test demonstrating manifold membership. The evidence shows assertions of a guarantee but no rigorous demonstration is presented in the quoted sections.", "evidence": ["\"we devise a new zero-shot image restoration method that we call in short SHRED ... which, by construction, ensures that the sample generation path always lies on the data manifold.\"", "\"This design choice does not alter the intermediate diffusion latents and thus provides more guarantees that the generated images lies in the in-distribution manifold.\"", "\"We experimentally validate our method on several image restoration tasks. Our method SHRED achieves state of the art results on multiple zero-shot IR benchmarks especially in terms of image quality quantified using FID.\""], "section": "Introduction / Contributions"}
{"claim": "The paper provides no quantitative measures or visualizations to assess whether SHRED preserves the data manifold during restoration.", "claim_type": "quantitative", "paper_id": "ZnmofqLWMQ", "paper_title": "Zero-shot Image Restoration via Diffusion Inversion", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ijAQPsrAef", "reviewer": "Reviewer_bZYN", "review_text": "Summary: The paper presents SHRED (zero-SHot image REstoration via Diffusion inversion), a new image restoration method using pre-trained diffusion models. SHRED uniquely maintains the integrity of the data manifold by not altering the reverse sampling process during restoration. It optimizes the initial diffusion noise to reconstruct high-quality images efficiently, avoiding the need for model retraining or fine-tuning. SHRED demonstrates superior performance on various image restoration tasks, achieving state-of-the-art results in zero-shot benchmarks.\n\nStrengths: **Strengths of the Paper:**\n\n1. **Originality:**\n   - The introduction of SHRED represents a new direction in leveraging pre-trained diffusion models for image restoration tasks. The approach of not altering the reverse sampling process is a departure from previous methods, addressing limitations from prior results.\n\n2. **Quality:**\n   - The quality of the research is evident in the comprehensive experimental validation across various tasks such as inpainting, super-resolution, and blind deconvolution. The use of well-established metrics like FID and LPIPS lends credibility to the reported results.\n   - The state-of-the-art performance of SHRED, as demonstrated through quantitative and qualitative evaluations, underscores the method's effectiveness.\n\n3. **Clarity:**\n   - Clarity is one of the paper's strengths, with a well-organized presentation of the content. The clarity in writing ensures that the concepts are accessible and the results are understandable.\n   - The background provided on DDPM and DDIM is thorough, facilitating a clear understanding of the advancements SHRED brings to the field.\n\n4. **Significance:**\n   - The paper is significant in its potential applicability to a broad spectrum of image restoration tasks, demonstrating the adaptability of SHRED to different challenges without the need for retraining.\n\nThe paper's contributions are presented with clarity and are supported by solid empirical evidence, making it a valuable addition to the literature on diffusion models and image restoration.\n\nWeaknesses: **Weaknesses of the Paper:**\n\n1. **Guarantee of Data Manifold Integrity:**\n   - The paper positions SHRED as a method that maintains the integrity of the data manifold during image restoration, which is a central claim for its novelty. However, the paper lacks a rigorous demonstration or proof that the samples generated by SHRED indeed lie on the data manifold. This is a significant gap, as the main criticism of prior methods is their potential deviation from the manifold. To strengthen this claim, the authors could provide empirical evidence or a theoretical guarantee, possibly through visualizations of the manifold or quantitative measures that can assess this aspect.\n\n2. **Comparison with MCG from Chung et al. (2022b):**\n   - The paper does not provide a detailed comparison with the MCG method proposed by Chung et al. (2022b), which also aims to correct samples to ensure they are on the data manifold. A deeper theoretical and empirical analysis comparing SHRED with MCG would be beneficial. This could include side-by-side comparisons on the same tasks, using the same metrics, and a discussion on the theoretical underpinnings of both methods. Such a comparison would be valuable for readers to understand the relative merits and trade-offs of these approaches.\n\n3. **Computational Efficiency:**\n   - In Table 3, SHRED is slower than DDRM and DDPM, which could limit its practicality for real-world applications where computational resources or time are constrained. The authors could explore ways to improve the efficiency of SHRED, perhaps by optimizing the algorithm or by proposing a more computationally efficient variant that maintains most of the method's benefits.\n\n4. **Novelty and Originality in Mathematical Derivation:**\n   - The mathematical derivation of SHRED's methodology does not appear to be a novel contribution, which may lead to questions about the paper's originality. The authors could strengthen this aspect by clearly delineating the novel components of their mathematical approach, contrasting it with existing methods, and discussing how these novel aspects contribute to the method's performance.\n\nQuestions: To address the above weaknesses, the authors could consider the following actions:\n\n- Provide empirical evidence or theoretical justification for the claim that SHRED generates samples along the data manifold.\n- Conduct a thorough comparison with MCG, including both theoretical and empirical analyses.\n- Investigate and propose methods to improve the computational efficiency of SHRED.\n- Clarify the novelty in the mathematical derivation of SHRED, differentiating it from existing approaches.\n\nBy addressing these points, the authors could significantly strengthen the paper and its contributions to the field.", "labeling_timestamp": "2026-01-11T17:01:58.590327", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper includes both visualizations and quantitative metrics relevant to assessing restoration behavior: Figure 1 visualizes intermediate latents and reconstructions (showing xt and x0|t), Figure 2 shows inversion reconstructions and reports PSNR mean and std, and the Abstract cites FID-based image-quality results. Thus the claim that the paper provides no quantitative measures or visualizations is false.", "evidence": "Figure 1: Image inpainting with SHRED. ... The top and bottom row images show x_t and x_0|t for different values of t respectively. The reconstruction of x_0|t allows us to skip time steps and make the generation process and the gradient back-propagation much more efficient.\n\nFigure 2: Inversion results using our optimization scheme on samples from both CelebA and ImageNet validation datasets. The PSNR mean and standard deviation are computed over 10 runs.\n\nAbstract: \"SHRED achieves state of the art results on multiple zero-shot IR benchmarks especially in terms of image quality quantified using FID.\"", "section": "Introduction (Figure 1), Section 3 / Figures (Figure 2), Abstract"}
{"claim": "The paper does not include a detailed empirical comparison with the MCG method from Chung et al. (2022b) on the same tasks and metrics.", "claim_type": "baseline", "paper_id": "ZnmofqLWMQ", "paper_title": "Zero-shot Image Restoration via Diffusion Inversion", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ijAQPsrAef", "reviewer": "Reviewer_bZYN", "review_text": "Summary: The paper presents SHRED (zero-SHot image REstoration via Diffusion inversion), a new image restoration method using pre-trained diffusion models. SHRED uniquely maintains the integrity of the data manifold by not altering the reverse sampling process during restoration. It optimizes the initial diffusion noise to reconstruct high-quality images efficiently, avoiding the need for model retraining or fine-tuning. SHRED demonstrates superior performance on various image restoration tasks, achieving state-of-the-art results in zero-shot benchmarks.\n\nStrengths: **Strengths of the Paper:**\n\n1. **Originality:**\n   - The introduction of SHRED represents a new direction in leveraging pre-trained diffusion models for image restoration tasks. The approach of not altering the reverse sampling process is a departure from previous methods, addressing limitations from prior results.\n\n2. **Quality:**\n   - The quality of the research is evident in the comprehensive experimental validation across various tasks such as inpainting, super-resolution, and blind deconvolution. The use of well-established metrics like FID and LPIPS lends credibility to the reported results.\n   - The state-of-the-art performance of SHRED, as demonstrated through quantitative and qualitative evaluations, underscores the method's effectiveness.\n\n3. **Clarity:**\n   - Clarity is one of the paper's strengths, with a well-organized presentation of the content. The clarity in writing ensures that the concepts are accessible and the results are understandable.\n   - The background provided on DDPM and DDIM is thorough, facilitating a clear understanding of the advancements SHRED brings to the field.\n\n4. **Significance:**\n   - The paper is significant in its potential applicability to a broad spectrum of image restoration tasks, demonstrating the adaptability of SHRED to different challenges without the need for retraining.\n\nThe paper's contributions are presented with clarity and are supported by solid empirical evidence, making it a valuable addition to the literature on diffusion models and image restoration.\n\nWeaknesses: **Weaknesses of the Paper:**\n\n1. **Guarantee of Data Manifold Integrity:**\n   - The paper positions SHRED as a method that maintains the integrity of the data manifold during image restoration, which is a central claim for its novelty. However, the paper lacks a rigorous demonstration or proof that the samples generated by SHRED indeed lie on the data manifold. This is a significant gap, as the main criticism of prior methods is their potential deviation from the manifold. To strengthen this claim, the authors could provide empirical evidence or a theoretical guarantee, possibly through visualizations of the manifold or quantitative measures that can assess this aspect.\n\n2. **Comparison with MCG from Chung et al. (2022b):**\n   - The paper does not provide a detailed comparison with the MCG method proposed by Chung et al. (2022b), which also aims to correct samples to ensure they are on the data manifold. A deeper theoretical and empirical analysis comparing SHRED with MCG would be beneficial. This could include side-by-side comparisons on the same tasks, using the same metrics, and a discussion on the theoretical underpinnings of both methods. Such a comparison would be valuable for readers to understand the relative merits and trade-offs of these approaches.\n\n3. **Computational Efficiency:**\n   - In Table 3, SHRED is slower than DDRM and DDPM, which could limit its practicality for real-world applications where computational resources or time are constrained. The authors could explore ways to improve the efficiency of SHRED, perhaps by optimizing the algorithm or by proposing a more computationally efficient variant that maintains most of the method's benefits.\n\n4. **Novelty and Originality in Mathematical Derivation:**\n   - The mathematical derivation of SHRED's methodology does not appear to be a novel contribution, which may lead to questions about the paper's originality. The authors could strengthen this aspect by clearly delineating the novel components of their mathematical approach, contrasting it with existing methods, and discussing how these novel aspects contribute to the method's performance.\n\nQuestions: To address the above weaknesses, the authors could consider the following actions:\n\n- Provide empirical evidence or theoretical justification for the claim that SHRED generates samples along the data manifold.\n- Conduct a thorough comparison with MCG, including both theoretical and empirical analyses.\n- Investigate and propose methods to improve the computational efficiency of SHRED.\n- Clarify the novelty in the mathematical derivation of SHRED, differentiating it from existing approaches.\n\nBy addressing these points, the authors could significantly strengthen the paper and its contributions to the field.", "labeling_timestamp": "2026-01-11T17:02:22.429106", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper cites Chung et al. (2022b) and discusses their approach (Related Work), and it states that the method is experimentally validated and achieves state-of-the-art results (Abstract/Intro). However, the provided content does not include the Experiments section or any tables/plots that would show a detailed empirical comparison (same tasks and metrics) against the MCG method, so there is insufficient information to determine whether such a comparison is present.", "evidence": "1) \"Chung et al. (2022b) argues that relying on an iterative procedure consisting of reverse diffusion steps and a projectionbased consistency step runs the risk of stepping outside of the data manifold, a risk they mitigate using an additional correction term.\" \n\n2) \"We experimentally validate our method on several image restoration tasks. Our method SHRED achieves state of the art results on multiple zero-shot IR benchmarks especially in terms of image quality quantified using FID.\"", "section": "Related Work; Abstract / Introduction"}
{"claim": "The paper lacks a theoretical analysis contrasting SHRED's approach with MCG's theoretical underpinnings.", "claim_type": "methodology", "paper_id": "ZnmofqLWMQ", "paper_title": "Zero-shot Image Restoration via Diffusion Inversion", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ijAQPsrAef", "reviewer": "Reviewer_bZYN", "review_text": "Summary: The paper presents SHRED (zero-SHot image REstoration via Diffusion inversion), a new image restoration method using pre-trained diffusion models. SHRED uniquely maintains the integrity of the data manifold by not altering the reverse sampling process during restoration. It optimizes the initial diffusion noise to reconstruct high-quality images efficiently, avoiding the need for model retraining or fine-tuning. SHRED demonstrates superior performance on various image restoration tasks, achieving state-of-the-art results in zero-shot benchmarks.\n\nStrengths: **Strengths of the Paper:**\n\n1. **Originality:**\n   - The introduction of SHRED represents a new direction in leveraging pre-trained diffusion models for image restoration tasks. The approach of not altering the reverse sampling process is a departure from previous methods, addressing limitations from prior results.\n\n2. **Quality:**\n   - The quality of the research is evident in the comprehensive experimental validation across various tasks such as inpainting, super-resolution, and blind deconvolution. The use of well-established metrics like FID and LPIPS lends credibility to the reported results.\n   - The state-of-the-art performance of SHRED, as demonstrated through quantitative and qualitative evaluations, underscores the method's effectiveness.\n\n3. **Clarity:**\n   - Clarity is one of the paper's strengths, with a well-organized presentation of the content. The clarity in writing ensures that the concepts are accessible and the results are understandable.\n   - The background provided on DDPM and DDIM is thorough, facilitating a clear understanding of the advancements SHRED brings to the field.\n\n4. **Significance:**\n   - The paper is significant in its potential applicability to a broad spectrum of image restoration tasks, demonstrating the adaptability of SHRED to different challenges without the need for retraining.\n\nThe paper's contributions are presented with clarity and are supported by solid empirical evidence, making it a valuable addition to the literature on diffusion models and image restoration.\n\nWeaknesses: **Weaknesses of the Paper:**\n\n1. **Guarantee of Data Manifold Integrity:**\n   - The paper positions SHRED as a method that maintains the integrity of the data manifold during image restoration, which is a central claim for its novelty. However, the paper lacks a rigorous demonstration or proof that the samples generated by SHRED indeed lie on the data manifold. This is a significant gap, as the main criticism of prior methods is their potential deviation from the manifold. To strengthen this claim, the authors could provide empirical evidence or a theoretical guarantee, possibly through visualizations of the manifold or quantitative measures that can assess this aspect.\n\n2. **Comparison with MCG from Chung et al. (2022b):**\n   - The paper does not provide a detailed comparison with the MCG method proposed by Chung et al. (2022b), which also aims to correct samples to ensure they are on the data manifold. A deeper theoretical and empirical analysis comparing SHRED with MCG would be beneficial. This could include side-by-side comparisons on the same tasks, using the same metrics, and a discussion on the theoretical underpinnings of both methods. Such a comparison would be valuable for readers to understand the relative merits and trade-offs of these approaches.\n\n3. **Computational Efficiency:**\n   - In Table 3, SHRED is slower than DDRM and DDPM, which could limit its practicality for real-world applications where computational resources or time are constrained. The authors could explore ways to improve the efficiency of SHRED, perhaps by optimizing the algorithm or by proposing a more computationally efficient variant that maintains most of the method's benefits.\n\n4. **Novelty and Originality in Mathematical Derivation:**\n   - The mathematical derivation of SHRED's methodology does not appear to be a novel contribution, which may lead to questions about the paper's originality. The authors could strengthen this aspect by clearly delineating the novel components of their mathematical approach, contrasting it with existing methods, and discussing how these novel aspects contribute to the method's performance.\n\nQuestions: To address the above weaknesses, the authors could consider the following actions:\n\n- Provide empirical evidence or theoretical justification for the claim that SHRED generates samples along the data manifold.\n- Conduct a thorough comparison with MCG, including both theoretical and empirical analyses.\n- Investigate and propose methods to improve the computational efficiency of SHRED.\n- Clarify the novelty in the mathematical derivation of SHRED, differentiating it from existing approaches.\n\nBy addressing these points, the authors could significantly strengthen the paper and its contributions to the field.", "labeling_timestamp": "2026-01-11T17:02:20.259005", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper's Introduction, Related Work, and Contributions discuss other prior methods (e.g., RePaint, DDRM, DDNM, GAN inversion) and present SHRED's formulation and contributions, but there is no mention of MCG nor any theoretical comparison or contrast between SHRED and MCG in the provided content.", "evidence": "Zero-shot methods Recently, methods that can be repurposed to different inverse problems have been called zero-shot. Song & Ermon (2019) proposes a method based on guiding the reverse diffusion process with the unmasked region to solve inpainting in a zero-shot manner. Song et al. (2021) proposes the use of gradient guidance to solve inverse problems in the context of medical imaging. Choi et al. (2021) applies low-frequency guidance from a reference image to solve super-resolution. RePaint (Lugmayr et al., 2022) solves the inpainting problem by guiding the diffusion process with the unmasked region. To condition the generation process, the reverse diffusion iterations are altered by sampling the unmasked regions using the given image information. DDRM (Kawar et al., 2022) proposes an inverse problem solver based on posterior sampling by introducing a variational inference objective for learning the posterior distribution of the inverse problem.", "section": "Related Work"}
{"claim": "Table 3 shows SHRED is slower than DDRM and DDPM, indicating worse computational efficiency.", "claim_type": "experimental", "paper_id": "ZnmofqLWMQ", "paper_title": "Zero-shot Image Restoration via Diffusion Inversion", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ijAQPsrAef", "reviewer": "Reviewer_bZYN", "review_text": "Summary: The paper presents SHRED (zero-SHot image REstoration via Diffusion inversion), a new image restoration method using pre-trained diffusion models. SHRED uniquely maintains the integrity of the data manifold by not altering the reverse sampling process during restoration. It optimizes the initial diffusion noise to reconstruct high-quality images efficiently, avoiding the need for model retraining or fine-tuning. SHRED demonstrates superior performance on various image restoration tasks, achieving state-of-the-art results in zero-shot benchmarks.\n\nStrengths: **Strengths of the Paper:**\n\n1. **Originality:**\n   - The introduction of SHRED represents a new direction in leveraging pre-trained diffusion models for image restoration tasks. The approach of not altering the reverse sampling process is a departure from previous methods, addressing limitations from prior results.\n\n2. **Quality:**\n   - The quality of the research is evident in the comprehensive experimental validation across various tasks such as inpainting, super-resolution, and blind deconvolution. The use of well-established metrics like FID and LPIPS lends credibility to the reported results.\n   - The state-of-the-art performance of SHRED, as demonstrated through quantitative and qualitative evaluations, underscores the method's effectiveness.\n\n3. **Clarity:**\n   - Clarity is one of the paper's strengths, with a well-organized presentation of the content. The clarity in writing ensures that the concepts are accessible and the results are understandable.\n   - The background provided on DDPM and DDIM is thorough, facilitating a clear understanding of the advancements SHRED brings to the field.\n\n4. **Significance:**\n   - The paper is significant in its potential applicability to a broad spectrum of image restoration tasks, demonstrating the adaptability of SHRED to different challenges without the need for retraining.\n\nThe paper's contributions are presented with clarity and are supported by solid empirical evidence, making it a valuable addition to the literature on diffusion models and image restoration.\n\nWeaknesses: **Weaknesses of the Paper:**\n\n1. **Guarantee of Data Manifold Integrity:**\n   - The paper positions SHRED as a method that maintains the integrity of the data manifold during image restoration, which is a central claim for its novelty. However, the paper lacks a rigorous demonstration or proof that the samples generated by SHRED indeed lie on the data manifold. This is a significant gap, as the main criticism of prior methods is their potential deviation from the manifold. To strengthen this claim, the authors could provide empirical evidence or a theoretical guarantee, possibly through visualizations of the manifold or quantitative measures that can assess this aspect.\n\n2. **Comparison with MCG from Chung et al. (2022b):**\n   - The paper does not provide a detailed comparison with the MCG method proposed by Chung et al. (2022b), which also aims to correct samples to ensure they are on the data manifold. A deeper theoretical and empirical analysis comparing SHRED with MCG would be beneficial. This could include side-by-side comparisons on the same tasks, using the same metrics, and a discussion on the theoretical underpinnings of both methods. Such a comparison would be valuable for readers to understand the relative merits and trade-offs of these approaches.\n\n3. **Computational Efficiency:**\n   - In Table 3, SHRED is slower than DDRM and DDPM, which could limit its practicality for real-world applications where computational resources or time are constrained. The authors could explore ways to improve the efficiency of SHRED, perhaps by optimizing the algorithm or by proposing a more computationally efficient variant that maintains most of the method's benefits.\n\n4. **Novelty and Originality in Mathematical Derivation:**\n   - The mathematical derivation of SHRED's methodology does not appear to be a novel contribution, which may lead to questions about the paper's originality. The authors could strengthen this aspect by clearly delineating the novel components of their mathematical approach, contrasting it with existing methods, and discussing how these novel aspects contribute to the method's performance.\n\nQuestions: To address the above weaknesses, the authors could consider the following actions:\n\n- Provide empirical evidence or theoretical justification for the claim that SHRED generates samples along the data manifold.\n- Conduct a thorough comparison with MCG, including both theoretical and empirical analyses.\n- Investigate and propose methods to improve the computational efficiency of SHRED.\n- Clarify the novelty in the mathematical derivation of SHRED, differentiating it from existing approaches.\n\nBy addressing these points, the authors could significantly strengthen the paper and its contributions to the field.", "labeling_timestamp": "2026-01-11T17:02:14.915150", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt discusses SHRED's computational cost and that the method uses optimizations to mitigate it, but no Table 3 or any explicit runtime comparison between SHRED, DDRM, and DDPM is included in the supplied content. Therefore the claim about Table 3 showing SHRED is slower cannot be verified from the paper text provided.", "evidence": "Since SHRED is an optimization method that works by iteratively optimizing the initial noise, multiple forward passes of the diffusion process are required which can be computationally expensive. A single forward pass of a standard diffusion model (with 1000 steps) takes in the order of minutes on a typical modern GPU.", "section": "Introduction"}
{"claim": "SHRED's slower runtime could limit its practicality for real-world applications with constrained computational resources or time.", "claim_type": "subjective", "paper_id": "ZnmofqLWMQ", "paper_title": "Zero-shot Image Restoration via Diffusion Inversion", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ijAQPsrAef", "reviewer": "Reviewer_bZYN", "review_text": "Summary: The paper presents SHRED (zero-SHot image REstoration via Diffusion inversion), a new image restoration method using pre-trained diffusion models. SHRED uniquely maintains the integrity of the data manifold by not altering the reverse sampling process during restoration. It optimizes the initial diffusion noise to reconstruct high-quality images efficiently, avoiding the need for model retraining or fine-tuning. SHRED demonstrates superior performance on various image restoration tasks, achieving state-of-the-art results in zero-shot benchmarks.\n\nStrengths: **Strengths of the Paper:**\n\n1. **Originality:**\n   - The introduction of SHRED represents a new direction in leveraging pre-trained diffusion models for image restoration tasks. The approach of not altering the reverse sampling process is a departure from previous methods, addressing limitations from prior results.\n\n2. **Quality:**\n   - The quality of the research is evident in the comprehensive experimental validation across various tasks such as inpainting, super-resolution, and blind deconvolution. The use of well-established metrics like FID and LPIPS lends credibility to the reported results.\n   - The state-of-the-art performance of SHRED, as demonstrated through quantitative and qualitative evaluations, underscores the method's effectiveness.\n\n3. **Clarity:**\n   - Clarity is one of the paper's strengths, with a well-organized presentation of the content. The clarity in writing ensures that the concepts are accessible and the results are understandable.\n   - The background provided on DDPM and DDIM is thorough, facilitating a clear understanding of the advancements SHRED brings to the field.\n\n4. **Significance:**\n   - The paper is significant in its potential applicability to a broad spectrum of image restoration tasks, demonstrating the adaptability of SHRED to different challenges without the need for retraining.\n\nThe paper's contributions are presented with clarity and are supported by solid empirical evidence, making it a valuable addition to the literature on diffusion models and image restoration.\n\nWeaknesses: **Weaknesses of the Paper:**\n\n1. **Guarantee of Data Manifold Integrity:**\n   - The paper positions SHRED as a method that maintains the integrity of the data manifold during image restoration, which is a central claim for its novelty. However, the paper lacks a rigorous demonstration or proof that the samples generated by SHRED indeed lie on the data manifold. This is a significant gap, as the main criticism of prior methods is their potential deviation from the manifold. To strengthen this claim, the authors could provide empirical evidence or a theoretical guarantee, possibly through visualizations of the manifold or quantitative measures that can assess this aspect.\n\n2. **Comparison with MCG from Chung et al. (2022b):**\n   - The paper does not provide a detailed comparison with the MCG method proposed by Chung et al. (2022b), which also aims to correct samples to ensure they are on the data manifold. A deeper theoretical and empirical analysis comparing SHRED with MCG would be beneficial. This could include side-by-side comparisons on the same tasks, using the same metrics, and a discussion on the theoretical underpinnings of both methods. Such a comparison would be valuable for readers to understand the relative merits and trade-offs of these approaches.\n\n3. **Computational Efficiency:**\n   - In Table 3, SHRED is slower than DDRM and DDPM, which could limit its practicality for real-world applications where computational resources or time are constrained. The authors could explore ways to improve the efficiency of SHRED, perhaps by optimizing the algorithm or by proposing a more computationally efficient variant that maintains most of the method's benefits.\n\n4. **Novelty and Originality in Mathematical Derivation:**\n   - The mathematical derivation of SHRED's methodology does not appear to be a novel contribution, which may lead to questions about the paper's originality. The authors could strengthen this aspect by clearly delineating the novel components of their mathematical approach, contrasting it with existing methods, and discussing how these novel aspects contribute to the method's performance.\n\nQuestions: To address the above weaknesses, the authors could consider the following actions:\n\n- Provide empirical evidence or theoretical justification for the claim that SHRED generates samples along the data manifold.\n- Conduct a thorough comparison with MCG, including both theoretical and empirical analyses.\n- Investigate and propose methods to improve the computational efficiency of SHRED.\n- Clarify the novelty in the mathematical derivation of SHRED, differentiating it from existing approaches.\n\nBy addressing these points, the authors could significantly strengthen the paper and its contributions to the field.", "labeling_timestamp": "2026-01-11T17:02:24.275904", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly states that SHRED's optimization requires multiple forward diffusion passes and that a direct implementation would be \"computationally demanding and ultimately impractical,\" supporting the reviewer's concern. However, the paper also proposes mitigation (DDIM skipping and a δt trade-off) and claims an efficient/practical inversion, so the concern is acknowledged but the authors argue it can be alleviated.", "evidence": "Since SHRED is an optimization method that works by iteratively optimizing the initial noise, multiple forward passes of the diffusion process are required which can be computationally expensive. A single forward pass of a standard diffusion model (with 1000 steps) takes in the order of minutes on a typical modern GPU. Thus, a direct implementation of the optimization procedure in the case of the Diffusion model as a black box would be computationally demanding and ultimately impractical.", "section": "Introduction"}
{"claim": "The paper does not propose optimizations or a more computationally efficient SHRED variant to address its higher runtime.", "claim_type": "methodology", "paper_id": "ZnmofqLWMQ", "paper_title": "Zero-shot Image Restoration via Diffusion Inversion", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ijAQPsrAef", "reviewer": "Reviewer_bZYN", "review_text": "Summary: The paper presents SHRED (zero-SHot image REstoration via Diffusion inversion), a new image restoration method using pre-trained diffusion models. SHRED uniquely maintains the integrity of the data manifold by not altering the reverse sampling process during restoration. It optimizes the initial diffusion noise to reconstruct high-quality images efficiently, avoiding the need for model retraining or fine-tuning. SHRED demonstrates superior performance on various image restoration tasks, achieving state-of-the-art results in zero-shot benchmarks.\n\nStrengths: **Strengths of the Paper:**\n\n1. **Originality:**\n   - The introduction of SHRED represents a new direction in leveraging pre-trained diffusion models for image restoration tasks. The approach of not altering the reverse sampling process is a departure from previous methods, addressing limitations from prior results.\n\n2. **Quality:**\n   - The quality of the research is evident in the comprehensive experimental validation across various tasks such as inpainting, super-resolution, and blind deconvolution. The use of well-established metrics like FID and LPIPS lends credibility to the reported results.\n   - The state-of-the-art performance of SHRED, as demonstrated through quantitative and qualitative evaluations, underscores the method's effectiveness.\n\n3. **Clarity:**\n   - Clarity is one of the paper's strengths, with a well-organized presentation of the content. The clarity in writing ensures that the concepts are accessible and the results are understandable.\n   - The background provided on DDPM and DDIM is thorough, facilitating a clear understanding of the advancements SHRED brings to the field.\n\n4. **Significance:**\n   - The paper is significant in its potential applicability to a broad spectrum of image restoration tasks, demonstrating the adaptability of SHRED to different challenges without the need for retraining.\n\nThe paper's contributions are presented with clarity and are supported by solid empirical evidence, making it a valuable addition to the literature on diffusion models and image restoration.\n\nWeaknesses: **Weaknesses of the Paper:**\n\n1. **Guarantee of Data Manifold Integrity:**\n   - The paper positions SHRED as a method that maintains the integrity of the data manifold during image restoration, which is a central claim for its novelty. However, the paper lacks a rigorous demonstration or proof that the samples generated by SHRED indeed lie on the data manifold. This is a significant gap, as the main criticism of prior methods is their potential deviation from the manifold. To strengthen this claim, the authors could provide empirical evidence or a theoretical guarantee, possibly through visualizations of the manifold or quantitative measures that can assess this aspect.\n\n2. **Comparison with MCG from Chung et al. (2022b):**\n   - The paper does not provide a detailed comparison with the MCG method proposed by Chung et al. (2022b), which also aims to correct samples to ensure they are on the data manifold. A deeper theoretical and empirical analysis comparing SHRED with MCG would be beneficial. This could include side-by-side comparisons on the same tasks, using the same metrics, and a discussion on the theoretical underpinnings of both methods. Such a comparison would be valuable for readers to understand the relative merits and trade-offs of these approaches.\n\n3. **Computational Efficiency:**\n   - In Table 3, SHRED is slower than DDRM and DDPM, which could limit its practicality for real-world applications where computational resources or time are constrained. The authors could explore ways to improve the efficiency of SHRED, perhaps by optimizing the algorithm or by proposing a more computationally efficient variant that maintains most of the method's benefits.\n\n4. **Novelty and Originality in Mathematical Derivation:**\n   - The mathematical derivation of SHRED's methodology does not appear to be a novel contribution, which may lead to questions about the paper's originality. The authors could strengthen this aspect by clearly delineating the novel components of their mathematical approach, contrasting it with existing methods, and discussing how these novel aspects contribute to the method's performance.\n\nQuestions: To address the above weaknesses, the authors could consider the following actions:\n\n- Provide empirical evidence or theoretical justification for the claim that SHRED generates samples along the data manifold.\n- Conduct a thorough comparison with MCG, including both theoretical and empirical analyses.\n- Investigate and propose methods to improve the computational efficiency of SHRED.\n- Clarify the novelty in the mathematical derivation of SHRED, differentiating it from existing approaches.\n\nBy addressing these points, the authors could significantly strengthen the paper and its contributions to the field.", "labeling_timestamp": "2026-01-11T17:02:37.564836", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly proposes an efficiency improvement for SHRED by leveraging DDIM's ability to skip forward diffusion time steps (introducing a δt hyperparameter) to reduce computation and make gradient backpropagation more efficient.", "evidence": "Abstract: \"To mitigate the substantial computational cost associated with inverting a fully unrolled diffusion model, we leverage the inherent capability of these models to skip ahead in the forward diffusion process using arbitrary large time steps.\" Introduction / Contributions: \"Thanks to the non-Markovian marginal distributions used in DDIM, our method introduces a hyperparameter δt that can be used to prioritize either image quality or computational cost.\" Figure 1 caption: \"The reconstruction of x0|t allows us to skip time steps and make the generation process and the gradient back-propagation much more efficient.\" Contributions point 2: \"We leverage the capabilty of DDIM to skip ahead in the forward diffusion process by arbitrary time steps and propose an efficient and practical diffusion model inversion in the context of image restoration.\"", "section": "Abstract; Introduction (Contributions); Figure 1 caption"}
{"claim": "The mathematical derivation of SHRED does not appear to present novel theoretical contributions distinct from existing methods.", "claim_type": "novelty", "paper_id": "ZnmofqLWMQ", "paper_title": "Zero-shot Image Restoration via Diffusion Inversion", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ijAQPsrAef", "reviewer": "Reviewer_bZYN", "review_text": "Summary: The paper presents SHRED (zero-SHot image REstoration via Diffusion inversion), a new image restoration method using pre-trained diffusion models. SHRED uniquely maintains the integrity of the data manifold by not altering the reverse sampling process during restoration. It optimizes the initial diffusion noise to reconstruct high-quality images efficiently, avoiding the need for model retraining or fine-tuning. SHRED demonstrates superior performance on various image restoration tasks, achieving state-of-the-art results in zero-shot benchmarks.\n\nStrengths: **Strengths of the Paper:**\n\n1. **Originality:**\n   - The introduction of SHRED represents a new direction in leveraging pre-trained diffusion models for image restoration tasks. The approach of not altering the reverse sampling process is a departure from previous methods, addressing limitations from prior results.\n\n2. **Quality:**\n   - The quality of the research is evident in the comprehensive experimental validation across various tasks such as inpainting, super-resolution, and blind deconvolution. The use of well-established metrics like FID and LPIPS lends credibility to the reported results.\n   - The state-of-the-art performance of SHRED, as demonstrated through quantitative and qualitative evaluations, underscores the method's effectiveness.\n\n3. **Clarity:**\n   - Clarity is one of the paper's strengths, with a well-organized presentation of the content. The clarity in writing ensures that the concepts are accessible and the results are understandable.\n   - The background provided on DDPM and DDIM is thorough, facilitating a clear understanding of the advancements SHRED brings to the field.\n\n4. **Significance:**\n   - The paper is significant in its potential applicability to a broad spectrum of image restoration tasks, demonstrating the adaptability of SHRED to different challenges without the need for retraining.\n\nThe paper's contributions are presented with clarity and are supported by solid empirical evidence, making it a valuable addition to the literature on diffusion models and image restoration.\n\nWeaknesses: **Weaknesses of the Paper:**\n\n1. **Guarantee of Data Manifold Integrity:**\n   - The paper positions SHRED as a method that maintains the integrity of the data manifold during image restoration, which is a central claim for its novelty. However, the paper lacks a rigorous demonstration or proof that the samples generated by SHRED indeed lie on the data manifold. This is a significant gap, as the main criticism of prior methods is their potential deviation from the manifold. To strengthen this claim, the authors could provide empirical evidence or a theoretical guarantee, possibly through visualizations of the manifold or quantitative measures that can assess this aspect.\n\n2. **Comparison with MCG from Chung et al. (2022b):**\n   - The paper does not provide a detailed comparison with the MCG method proposed by Chung et al. (2022b), which also aims to correct samples to ensure they are on the data manifold. A deeper theoretical and empirical analysis comparing SHRED with MCG would be beneficial. This could include side-by-side comparisons on the same tasks, using the same metrics, and a discussion on the theoretical underpinnings of both methods. Such a comparison would be valuable for readers to understand the relative merits and trade-offs of these approaches.\n\n3. **Computational Efficiency:**\n   - In Table 3, SHRED is slower than DDRM and DDPM, which could limit its practicality for real-world applications where computational resources or time are constrained. The authors could explore ways to improve the efficiency of SHRED, perhaps by optimizing the algorithm or by proposing a more computationally efficient variant that maintains most of the method's benefits.\n\n4. **Novelty and Originality in Mathematical Derivation:**\n   - The mathematical derivation of SHRED's methodology does not appear to be a novel contribution, which may lead to questions about the paper's originality. The authors could strengthen this aspect by clearly delineating the novel components of their mathematical approach, contrasting it with existing methods, and discussing how these novel aspects contribute to the method's performance.\n\nQuestions: To address the above weaknesses, the authors could consider the following actions:\n\n- Provide empirical evidence or theoretical justification for the claim that SHRED generates samples along the data manifold.\n- Conduct a thorough comparison with MCG, including both theoretical and empirical analyses.\n- Investigate and propose methods to improve the computational efficiency of SHRED.\n- Clarify the novelty in the mathematical derivation of SHRED, differentiating it from existing approaches.\n\nBy addressing these points, the authors could significantly strengthen the paper and its contributions to the field.", "labeling_timestamp": "2026-01-11T17:02:53.543512", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper's mathematical content largely reuses existing DDIM/DDPM formulations and frames the IR problem as an optimization over the DDIM initial noise; it does not present new theoretical results or formal derivations beyond applying known DDIM properties and an optimization scheme. The main claims of novelty are methodological (optimizing only the initial noise, and leveraging DDIM skipping) rather than new theoretical contributions. See Introduction (claims 1–2), Background (DDIM description), and the algorithm update in Section 4 showing a straightforward gradient descent on the initial noise.", "evidence": "\"SHRED use a pre-trained Denoising Diffusion Implicit Models (DDIM) (Song et al., 2020) and exploits the existing deterministic correspondence between noise and images in DDIMs by casting the inverse restoration problem as a latent estimation problem, i.e., where the latent variable is the input noise to the diffusion model.\" (Introduction)", "section": "Introduction / 3.1 DENOISING DIFFUSION PROBABILISTIC MODELS / 4 IMAGE RESTORATION VIA SHRED"}
{"claim": "The authors do not clearly delineate which components of their mathematical approach are novel compared to prior work.", "claim_type": "novelty", "paper_id": "ZnmofqLWMQ", "paper_title": "Zero-shot Image Restoration via Diffusion Inversion", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ijAQPsrAef", "reviewer": "Reviewer_bZYN", "review_text": "Summary: The paper presents SHRED (zero-SHot image REstoration via Diffusion inversion), a new image restoration method using pre-trained diffusion models. SHRED uniquely maintains the integrity of the data manifold by not altering the reverse sampling process during restoration. It optimizes the initial diffusion noise to reconstruct high-quality images efficiently, avoiding the need for model retraining or fine-tuning. SHRED demonstrates superior performance on various image restoration tasks, achieving state-of-the-art results in zero-shot benchmarks.\n\nStrengths: **Strengths of the Paper:**\n\n1. **Originality:**\n   - The introduction of SHRED represents a new direction in leveraging pre-trained diffusion models for image restoration tasks. The approach of not altering the reverse sampling process is a departure from previous methods, addressing limitations from prior results.\n\n2. **Quality:**\n   - The quality of the research is evident in the comprehensive experimental validation across various tasks such as inpainting, super-resolution, and blind deconvolution. The use of well-established metrics like FID and LPIPS lends credibility to the reported results.\n   - The state-of-the-art performance of SHRED, as demonstrated through quantitative and qualitative evaluations, underscores the method's effectiveness.\n\n3. **Clarity:**\n   - Clarity is one of the paper's strengths, with a well-organized presentation of the content. The clarity in writing ensures that the concepts are accessible and the results are understandable.\n   - The background provided on DDPM and DDIM is thorough, facilitating a clear understanding of the advancements SHRED brings to the field.\n\n4. **Significance:**\n   - The paper is significant in its potential applicability to a broad spectrum of image restoration tasks, demonstrating the adaptability of SHRED to different challenges without the need for retraining.\n\nThe paper's contributions are presented with clarity and are supported by solid empirical evidence, making it a valuable addition to the literature on diffusion models and image restoration.\n\nWeaknesses: **Weaknesses of the Paper:**\n\n1. **Guarantee of Data Manifold Integrity:**\n   - The paper positions SHRED as a method that maintains the integrity of the data manifold during image restoration, which is a central claim for its novelty. However, the paper lacks a rigorous demonstration or proof that the samples generated by SHRED indeed lie on the data manifold. This is a significant gap, as the main criticism of prior methods is their potential deviation from the manifold. To strengthen this claim, the authors could provide empirical evidence or a theoretical guarantee, possibly through visualizations of the manifold or quantitative measures that can assess this aspect.\n\n2. **Comparison with MCG from Chung et al. (2022b):**\n   - The paper does not provide a detailed comparison with the MCG method proposed by Chung et al. (2022b), which also aims to correct samples to ensure they are on the data manifold. A deeper theoretical and empirical analysis comparing SHRED with MCG would be beneficial. This could include side-by-side comparisons on the same tasks, using the same metrics, and a discussion on the theoretical underpinnings of both methods. Such a comparison would be valuable for readers to understand the relative merits and trade-offs of these approaches.\n\n3. **Computational Efficiency:**\n   - In Table 3, SHRED is slower than DDRM and DDPM, which could limit its practicality for real-world applications where computational resources or time are constrained. The authors could explore ways to improve the efficiency of SHRED, perhaps by optimizing the algorithm or by proposing a more computationally efficient variant that maintains most of the method's benefits.\n\n4. **Novelty and Originality in Mathematical Derivation:**\n   - The mathematical derivation of SHRED's methodology does not appear to be a novel contribution, which may lead to questions about the paper's originality. The authors could strengthen this aspect by clearly delineating the novel components of their mathematical approach, contrasting it with existing methods, and discussing how these novel aspects contribute to the method's performance.\n\nQuestions: To address the above weaknesses, the authors could consider the following actions:\n\n- Provide empirical evidence or theoretical justification for the claim that SHRED generates samples along the data manifold.\n- Conduct a thorough comparison with MCG, including both theoretical and empirical analyses.\n- Investigate and propose methods to improve the computational efficiency of SHRED.\n- Clarify the novelty in the mathematical derivation of SHRED, differentiating it from existing approaches.\n\nBy addressing these points, the authors could significantly strengthen the paper and its contributions to the field.", "labeling_timestamp": "2026-01-11T17:02:52.363897", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states its novel contributions and contrasts them with prior work (see Introduction and Related Work). The authors list specific novel choices (optimizing only the initial noise, leveraging DDIM skipping for efficient inversion, and not fine-tuning the diffusion model) and explain how these differ from existing diffusion- and GAN-based IR methods.", "evidence": "1) \"Our main contributions can be summarized as: 1. We are the first to cast the IR task as a latent optimization problem in the context of diffusion models where only the initial noise is optimized. This design choice does not alter the intermediate diffusion latents and thus provides more guarantees that the generated images lies in the in-distribution manifold. 2. We leverage the capabilty of DDIM to skip ahead in the forward diffusion process by arbitrary time steps and propose an efficient and practical diffusion model inversion in the context of image restoration. We note that in our method, DDIM is neither fine-tuned nor retrained.\"  (Section 1 INTRODUCTION)\n\n2) \"Those approaches start from a random noise vector as the diffusion input and recursively alter the diffusion sampling process ... Chung et al. (2022b) shows that this procedure may cause the generated image to step outside the data manifold. In this work, we devise a new zero-shot image restoration method ... To this end, we opt for a simple yet effective strategy: we do not alter the reverse sampling, i.e., all the intermediate latents, once an initial noise is sampled. More specifically, SHRED use a pre-trained Denoising Diffusion Implicit Models (DDIM) ... by casting the inverse restoration problem as a latent estimation problem, i.e., where the latent variable is the input noise to the diffusion model.\" (Section 1 INTRODUCTION)\n\n3) \"We note that SHRED is separate enough to the GAN inversion literature. ... In our method, the diffusion model is neither fine-tuned nor trained.\" (Section 2 RELATED WORK)", "section": "1 INTRODUCTION; 2 RELATED WORK"}
{"claim": "The manuscript fails to discuss how any claimed novel mathematical aspects concretely contribute to SHRED's empirical performance.", "claim_type": "methodology", "paper_id": "ZnmofqLWMQ", "paper_title": "Zero-shot Image Restoration via Diffusion Inversion", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "ijAQPsrAef", "reviewer": "Reviewer_bZYN", "review_text": "Summary: The paper presents SHRED (zero-SHot image REstoration via Diffusion inversion), a new image restoration method using pre-trained diffusion models. SHRED uniquely maintains the integrity of the data manifold by not altering the reverse sampling process during restoration. It optimizes the initial diffusion noise to reconstruct high-quality images efficiently, avoiding the need for model retraining or fine-tuning. SHRED demonstrates superior performance on various image restoration tasks, achieving state-of-the-art results in zero-shot benchmarks.\n\nStrengths: **Strengths of the Paper:**\n\n1. **Originality:**\n   - The introduction of SHRED represents a new direction in leveraging pre-trained diffusion models for image restoration tasks. The approach of not altering the reverse sampling process is a departure from previous methods, addressing limitations from prior results.\n\n2. **Quality:**\n   - The quality of the research is evident in the comprehensive experimental validation across various tasks such as inpainting, super-resolution, and blind deconvolution. The use of well-established metrics like FID and LPIPS lends credibility to the reported results.\n   - The state-of-the-art performance of SHRED, as demonstrated through quantitative and qualitative evaluations, underscores the method's effectiveness.\n\n3. **Clarity:**\n   - Clarity is one of the paper's strengths, with a well-organized presentation of the content. The clarity in writing ensures that the concepts are accessible and the results are understandable.\n   - The background provided on DDPM and DDIM is thorough, facilitating a clear understanding of the advancements SHRED brings to the field.\n\n4. **Significance:**\n   - The paper is significant in its potential applicability to a broad spectrum of image restoration tasks, demonstrating the adaptability of SHRED to different challenges without the need for retraining.\n\nThe paper's contributions are presented with clarity and are supported by solid empirical evidence, making it a valuable addition to the literature on diffusion models and image restoration.\n\nWeaknesses: **Weaknesses of the Paper:**\n\n1. **Guarantee of Data Manifold Integrity:**\n   - The paper positions SHRED as a method that maintains the integrity of the data manifold during image restoration, which is a central claim for its novelty. However, the paper lacks a rigorous demonstration or proof that the samples generated by SHRED indeed lie on the data manifold. This is a significant gap, as the main criticism of prior methods is their potential deviation from the manifold. To strengthen this claim, the authors could provide empirical evidence or a theoretical guarantee, possibly through visualizations of the manifold or quantitative measures that can assess this aspect.\n\n2. **Comparison with MCG from Chung et al. (2022b):**\n   - The paper does not provide a detailed comparison with the MCG method proposed by Chung et al. (2022b), which also aims to correct samples to ensure they are on the data manifold. A deeper theoretical and empirical analysis comparing SHRED with MCG would be beneficial. This could include side-by-side comparisons on the same tasks, using the same metrics, and a discussion on the theoretical underpinnings of both methods. Such a comparison would be valuable for readers to understand the relative merits and trade-offs of these approaches.\n\n3. **Computational Efficiency:**\n   - In Table 3, SHRED is slower than DDRM and DDPM, which could limit its practicality for real-world applications where computational resources or time are constrained. The authors could explore ways to improve the efficiency of SHRED, perhaps by optimizing the algorithm or by proposing a more computationally efficient variant that maintains most of the method's benefits.\n\n4. **Novelty and Originality in Mathematical Derivation:**\n   - The mathematical derivation of SHRED's methodology does not appear to be a novel contribution, which may lead to questions about the paper's originality. The authors could strengthen this aspect by clearly delineating the novel components of their mathematical approach, contrasting it with existing methods, and discussing how these novel aspects contribute to the method's performance.\n\nQuestions: To address the above weaknesses, the authors could consider the following actions:\n\n- Provide empirical evidence or theoretical justification for the claim that SHRED generates samples along the data manifold.\n- Conduct a thorough comparison with MCG, including both theoretical and empirical analyses.\n- Investigate and propose methods to improve the computational efficiency of SHRED.\n- Clarify the novelty in the mathematical derivation of SHRED, differentiating it from existing approaches.\n\nBy addressing these points, the authors could significantly strengthen the paper and its contributions to the field.", "labeling_timestamp": "2026-01-11T17:02:47.807691", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper states the novel mathematical choices (optimizing only the initial noise, not altering intermediate latents; leveraging DDIM skip-ahead via δt) and asserts qualitative benefits (staying on the data manifold; trading quality vs cost), but the provided manuscript excerpt contains no concrete analysis, ablation, or empirical attribution showing how these mathematical aspects quantitatively produce the reported empirical gains—only claims and high-level statements are given.", "evidence": "\"we opt for a simple yet effective strategy: we do not alter the reverse sampling, i.e., all the intermediate latents, once an initial noise is sampled.\"; \"this design choice does not alter the intermediate diffusion latents and thus provides more guarantees that the generated images lies in the in-distribution manifold.\"; \"our method introduces a hyperparameter δt that can be used to prioritize either image quality or computational cost.\"", "section": "Introduction / Contributions"}
{"claim": "The experiment constructs a dataset using a larger generative model and then shows that this dataset helps train a smaller model, creating a circular validation loop.", "claim_type": "experimental", "paper_id": "IRXyPm9IPW", "paper_title": "Multimodal Large Language Models Make Text-to-Image Generative Models Align Better", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "juBZrLsCxq", "reviewer": "Reviewer_zspv", "review_text": "Comment: Dear authors, \n\nThank you for offering a comprehensive rebuttal in a polite manner despite my review is the only one showing negative polarity. I understand you have concerns about the review; sharing the same concerns, I have re-read the paper, which is why it takes time for me to respond. \n\nFirst of all, by the question raised \"Unfortunately, we did not fully understand your specific suggestion regarding 'training a bigger model to show the true value of the method.'\" I was referring to the experiment in section 4.2, which I believe is to use the constructed dataset to show that generative models can benefit from the dataset to align better (per line 157). In this case, it seems to me that the authors are forming a loop of construct a dataset with a bigger generative model and then show that it can help train a better smaller model. This is where I ask the question about the \"true value\" of the method yet to be validated. \n\nI understand the above requirement might be difficult to implement but I believe it will be essential for the study. For example, a simple logic loophole is that the bigger model will of course be a better aligned one and help the smaller model to perform better, it's like the conventional knowledge distillation research that we can get better small model with the help of a big model, this does not mean the same techniques will help improve the bigger model. \n\nOn the other hand, the authors might want to respond that \"getting a better big model\" is not the goal of this research, this research is about investigating how GPT-4V level models can offer better-aligned data to train smaller model in the beginning. In this case, I agree the paper is better justified, but in this case, it seems to me the paper will need some re-writing work, e.g., line 19 sends out a much stronger message than this. \n\nAgain, I thank the reviewers for offering a professional and polite rebuttal, and it takes me a while to respond because the current situation makes me feel like I need to re-read the paper, but it seems some of the concerns are still there. There is still a chance that I misread some parts, and I'm here for the authors to enlighten on these parts.", "labeling_timestamp": "2026-01-11T17:02:58.576384", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper does construct a dataset from images produced by text-to-image generative models and then uses that dataset (VisionPrefer) to train a reward model and to fine-tune generative models (§3 and §4). However, the reviewer claim that the dataset was constructed using a single \"larger generative model\" and that this creates a \"circular validation loop\" is not supported: annotations are produced by a multimodal LLM (GPT-4V), not by a larger generative image model, and the paper describes using a pool of different generative models to produce images rather than relying on one larger model (§3 Step-2 and Step-3).", "evidence": ["Step-2: Image Generation. We generate images using different text-to-image generative models (see details in Appendix C.2) by sampling textual prompts constructed in Step-1 as input. For each prompt, we generate four images by randomly selecting different generative models from the model pools with different classifier-free guidance scale values, to ensure high diversity.", "Step-3: Preference Generation. We employ state-of-the-art multimodal large language model, GPT-4 V, to provide three types of feedback: (1) Scalar scores ... (2) Preference ranking ... and (3) Textual explanations ...", "In this section, we first train a corresponding reward model named VP-Score and evaluate it on existing human-preference datasets (§ 4.1). Next, we enhance existing text-to-image generative models by adopting two reinforcement learning algorithms (§ 4.2) to validate the efficacy of VisionPrefer and VP-Score."], "section": "§3 VisionPrefer (Step-2 and Step-3) and §4 Experiments"}
{"claim": "The observed improvement of the smaller model may simply result from knowledge distillation effects from the stronger large model, not from a novel dataset construction method.", "claim_type": "experimental", "paper_id": "IRXyPm9IPW", "paper_title": "Multimodal Large Language Models Make Text-to-Image Generative Models Align Better", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "juBZrLsCxq", "reviewer": "Reviewer_zspv", "review_text": "Comment: Dear authors, \n\nThank you for offering a comprehensive rebuttal in a polite manner despite my review is the only one showing negative polarity. I understand you have concerns about the review; sharing the same concerns, I have re-read the paper, which is why it takes time for me to respond. \n\nFirst of all, by the question raised \"Unfortunately, we did not fully understand your specific suggestion regarding 'training a bigger model to show the true value of the method.'\" I was referring to the experiment in section 4.2, which I believe is to use the constructed dataset to show that generative models can benefit from the dataset to align better (per line 157). In this case, it seems to me that the authors are forming a loop of construct a dataset with a bigger generative model and then show that it can help train a better smaller model. This is where I ask the question about the \"true value\" of the method yet to be validated. \n\nI understand the above requirement might be difficult to implement but I believe it will be essential for the study. For example, a simple logic loophole is that the bigger model will of course be a better aligned one and help the smaller model to perform better, it's like the conventional knowledge distillation research that we can get better small model with the help of a big model, this does not mean the same techniques will help improve the bigger model. \n\nOn the other hand, the authors might want to respond that \"getting a better big model\" is not the goal of this research, this research is about investigating how GPT-4V level models can offer better-aligned data to train smaller model in the beginning. In this case, I agree the paper is better justified, but in this case, it seems to me the paper will need some re-writing work, e.g., line 19 sends out a much stronger message than this. \n\nAgain, I thank the reviewers for offering a professional and polite rebuttal, and it takes me a while to respond because the current situation makes me feel like I need to re-read the paper, but it seems some of the concerns are still there. There is still a chance that I misread some parts, and I'm here for the authors to enlighten on these parts.", "labeling_timestamp": "2026-01-11T17:03:05.060419", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper states that it uses a multimodal LLM (GPT-4V) to generate the VisionPrefer dataset and then trains a reward model (VP-Score) on that data and uses it to fine-tune smaller generative models. However, the paper does not present experiments or analyses that isolate or rule out the possibility that improvements come from distilling GPT-4V’s preferences (i.e., a knowledge-distillation effect) rather than from the proposed dataset-construction methodology itself.", "evidence": "Step-3: Preference Generation. We employ state-of-the-art multimodal large language model, GPT-4 V, to provide three types of feedback: (1) Scalar scores that indicate the fine-grained quality regarding multiple aspects, (2) Preference ranking according to the scalar scores, and (3) Textual explanations ... Finally, we obtain 1.2M preference choices.\n\nAbstract: To address these challenges, we first leverage multimodal large language models to create VisionPrefer, a fine-grained preference dataset that captures multiple preference aspects ... Then we train a corresponding reward model, VP-Score, over VisionPrefer to guide the tuning of text-to-image generative models.", "section": "3 VisionPrefer; Abstract"}
{"claim": "The paper does not include experiments training a larger model with the constructed dataset to test whether the method improves models at that scale.", "claim_type": "experimental", "paper_id": "IRXyPm9IPW", "paper_title": "Multimodal Large Language Models Make Text-to-Image Generative Models Align Better", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "juBZrLsCxq", "reviewer": "Reviewer_zspv", "review_text": "Comment: Dear authors, \n\nThank you for offering a comprehensive rebuttal in a polite manner despite my review is the only one showing negative polarity. I understand you have concerns about the review; sharing the same concerns, I have re-read the paper, which is why it takes time for me to respond. \n\nFirst of all, by the question raised \"Unfortunately, we did not fully understand your specific suggestion regarding 'training a bigger model to show the true value of the method.'\" I was referring to the experiment in section 4.2, which I believe is to use the constructed dataset to show that generative models can benefit from the dataset to align better (per line 157). In this case, it seems to me that the authors are forming a loop of construct a dataset with a bigger generative model and then show that it can help train a better smaller model. This is where I ask the question about the \"true value\" of the method yet to be validated. \n\nI understand the above requirement might be difficult to implement but I believe it will be essential for the study. For example, a simple logic loophole is that the bigger model will of course be a better aligned one and help the smaller model to perform better, it's like the conventional knowledge distillation research that we can get better small model with the help of a big model, this does not mean the same techniques will help improve the bigger model. \n\nOn the other hand, the authors might want to respond that \"getting a better big model\" is not the goal of this research, this research is about investigating how GPT-4V level models can offer better-aligned data to train smaller model in the beginning. In this case, I agree the paper is better justified, but in this case, it seems to me the paper will need some re-writing work, e.g., line 19 sends out a much stronger message than this. \n\nAgain, I thank the reviewers for offering a professional and polite rebuttal, and it takes me a while to respond because the current situation makes me feel like I need to re-read the paper, but it seems some of the concerns are still there. There is still a chance that I misread some parts, and I'm here for the authors to enlighten on these parts.", "labeling_timestamp": "2026-01-11T17:03:11.196941", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes training a reward model (VP-Score) and fine-tuning existing text-to-image generative models using PPO and DPO, but does not report experiments that train a larger model from scratch or explicitly evaluate the method at larger model scale.", "evidence": "Abstract: \"...we adopt two reinforcement learning methods, Proximal Policy Optimization (PPO) and Direct Policy Optimization (DPO), to supervised fine-tune generative models...\"; Experiments (§4): \"Next, we enhance existing text-to-image generative models by adopting two reinforcement learning algorithms (§ 4.2) to validate the efficacy of VisionPrefer and VP-Score.\"", "section": "Abstract; 4 Experiments (section 4 / §4.2)"}
{"claim": "The manuscript conflates dataset quality benefits with advantages from model capacity, failing to separate dataset-induced improvements from model size effects.", "claim_type": "experimental", "paper_id": "IRXyPm9IPW", "paper_title": "Multimodal Large Language Models Make Text-to-Image Generative Models Align Better", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "juBZrLsCxq", "reviewer": "Reviewer_zspv", "review_text": "Comment: Dear authors, \n\nThank you for offering a comprehensive rebuttal in a polite manner despite my review is the only one showing negative polarity. I understand you have concerns about the review; sharing the same concerns, I have re-read the paper, which is why it takes time for me to respond. \n\nFirst of all, by the question raised \"Unfortunately, we did not fully understand your specific suggestion regarding 'training a bigger model to show the true value of the method.'\" I was referring to the experiment in section 4.2, which I believe is to use the constructed dataset to show that generative models can benefit from the dataset to align better (per line 157). In this case, it seems to me that the authors are forming a loop of construct a dataset with a bigger generative model and then show that it can help train a better smaller model. This is where I ask the question about the \"true value\" of the method yet to be validated. \n\nI understand the above requirement might be difficult to implement but I believe it will be essential for the study. For example, a simple logic loophole is that the bigger model will of course be a better aligned one and help the smaller model to perform better, it's like the conventional knowledge distillation research that we can get better small model with the help of a big model, this does not mean the same techniques will help improve the bigger model. \n\nOn the other hand, the authors might want to respond that \"getting a better big model\" is not the goal of this research, this research is about investigating how GPT-4V level models can offer better-aligned data to train smaller model in the beginning. In this case, I agree the paper is better justified, but in this case, it seems to me the paper will need some re-writing work, e.g., line 19 sends out a much stronger message than this. \n\nAgain, I thank the reviewers for offering a professional and polite rebuttal, and it takes me a while to respond because the current situation makes me feel like I need to re-read the paper, but it seems some of the concerns are still there. There is still a chance that I misread some parts, and I'm here for the authors to enlighten on these parts.", "labeling_timestamp": "2026-01-11T17:03:21.562327", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes creating a large AI-labeled preference dataset and training/fine-tuning models with a fixed reward-model architecture, but it contains no analysis or ablation that isolates improvements due to the dataset from those due to model capacity (model size). The methods and experiments use fixed model architectures and diverse model pools for image generation without explicitly separating dataset-induced gains from model-size effects.", "evidence": "“We generate images using different text-to-image generative models (see details in Appendix C.2) by sampling textual prompts constructed in Step-1 as input. For each prompt, we generate four images by randomly selecting different generative models from the model pools with different classifier-free guidance scale values, to ensure high diversity.” (Section 3: VisionPrefer)\n\n“VP-Score adopts the same model structure as ImageReward [34], which is a open-source human-preference reward model and utilizes BLIP [14] as the backbone.” (Section 4.1: Reward Modeling)\n\n“We employ two reinforcement learning methods, Proximal Policy Optimization (PPO) and Direct Policy Optimization (DPO), to enhance generative models to better align with human preferences.” (Abstract / Section 4)", "section": "3 VisionPrefer; 4.1 Reward Modeling"}
{"claim": "Line 19 of the paper overstates the paper's goals and should be rewritten to reflect the study's focus on using large models to generate aligned data for smaller models.", "claim_type": "presentation", "paper_id": "IRXyPm9IPW", "paper_title": "Multimodal Large Language Models Make Text-to-Image Generative Models Align Better", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "juBZrLsCxq", "reviewer": "Reviewer_zspv", "review_text": "Comment: Dear authors, \n\nThank you for offering a comprehensive rebuttal in a polite manner despite my review is the only one showing negative polarity. I understand you have concerns about the review; sharing the same concerns, I have re-read the paper, which is why it takes time for me to respond. \n\nFirst of all, by the question raised \"Unfortunately, we did not fully understand your specific suggestion regarding 'training a bigger model to show the true value of the method.'\" I was referring to the experiment in section 4.2, which I believe is to use the constructed dataset to show that generative models can benefit from the dataset to align better (per line 157). In this case, it seems to me that the authors are forming a loop of construct a dataset with a bigger generative model and then show that it can help train a better smaller model. This is where I ask the question about the \"true value\" of the method yet to be validated. \n\nI understand the above requirement might be difficult to implement but I believe it will be essential for the study. For example, a simple logic loophole is that the bigger model will of course be a better aligned one and help the smaller model to perform better, it's like the conventional knowledge distillation research that we can get better small model with the help of a big model, this does not mean the same techniques will help improve the bigger model. \n\nOn the other hand, the authors might want to respond that \"getting a better big model\" is not the goal of this research, this research is about investigating how GPT-4V level models can offer better-aligned data to train smaller model in the beginning. In this case, I agree the paper is better justified, but in this case, it seems to me the paper will need some re-writing work, e.g., line 19 sends out a much stronger message than this. \n\nAgain, I thank the reviewers for offering a professional and polite rebuttal, and it takes me a while to respond because the current situation makes me feel like I need to re-read the paper, but it seems some of the concerns are still there. There is still a chance that I misread some parts, and I'm here for the authors to enlighten on these parts.", "labeling_timestamp": "2026-01-11T17:03:21.618244", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The reviewer refers to a specific line number (line 19) which is not present in the provided paper text, so we cannot verify what exact wording appears on that line or whether it ‘‘overstates’’ the goals. The paper, however, explicitly states that its focus is on using multimodal large language models to generate preference data and use that to align text-to-image models, so it is unclear whether a rewrite is needed without seeing the exact line.", "evidence": "Abstract: \"To address these challenges, we first leverage multimodal large language models to create VisionPrefer, a fine-grained preference dataset...\"; Introduction: \"Can Multimodal Large Language Models act as a Human-Aligned Preference Annotator for Text-to-Image Generation?\"; Section 3: \"We employ state-of-the-art multimodal large language model, GPT-4 V, to provide three types of feedback...\"", "section": "Abstract; 1 Introduction; 3 VisionPrefer"}
{"claim": "The paper fails to justify the 'true value' of the method adequately, leaving unclear whether the proposed approach provides benefits beyond replicating the larger model's alignment.", "claim_type": "experimental", "paper_id": "IRXyPm9IPW", "paper_title": "Multimodal Large Language Models Make Text-to-Image Generative Models Align Better", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "juBZrLsCxq", "reviewer": "Reviewer_zspv", "review_text": "Comment: Dear authors, \n\nThank you for offering a comprehensive rebuttal in a polite manner despite my review is the only one showing negative polarity. I understand you have concerns about the review; sharing the same concerns, I have re-read the paper, which is why it takes time for me to respond. \n\nFirst of all, by the question raised \"Unfortunately, we did not fully understand your specific suggestion regarding 'training a bigger model to show the true value of the method.'\" I was referring to the experiment in section 4.2, which I believe is to use the constructed dataset to show that generative models can benefit from the dataset to align better (per line 157). In this case, it seems to me that the authors are forming a loop of construct a dataset with a bigger generative model and then show that it can help train a better smaller model. This is where I ask the question about the \"true value\" of the method yet to be validated. \n\nI understand the above requirement might be difficult to implement but I believe it will be essential for the study. For example, a simple logic loophole is that the bigger model will of course be a better aligned one and help the smaller model to perform better, it's like the conventional knowledge distillation research that we can get better small model with the help of a big model, this does not mean the same techniques will help improve the bigger model. \n\nOn the other hand, the authors might want to respond that \"getting a better big model\" is not the goal of this research, this research is about investigating how GPT-4V level models can offer better-aligned data to train smaller model in the beginning. In this case, I agree the paper is better justified, but in this case, it seems to me the paper will need some re-writing work, e.g., line 19 sends out a much stronger message than this. \n\nAgain, I thank the reviewers for offering a professional and polite rebuttal, and it takes me a while to respond because the current situation makes me feel like I need to re-read the paper, but it seems some of the concerns are still there. There is still a chance that I misread some parts, and I'm here for the authors to enlighten on these parts.", "labeling_timestamp": "2026-01-11T17:03:40.866818", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper presents multiple validations showing the method's value beyond merely replicating the MLLM annotator: they train VP-Score on VisionPrefer and report that its preference prediction accuracy is comparable to human annotators, that VP-Score exhibits competitive correlation with human preferences versus existing reward models, and that tuning with VisionPrefer improves alignment and generalizes better than prior human-preference metrics. These statements indicate the authors did provide empirical justification rather than leaving the benefit unclear.", "evidence": "“The preference prediction accuracy of VP-Score is validated to be comparable to that of human annotators.”; “Experimental results demonstrate that VP-Score exhibits a competitive correlation with human preferences compared to existing human preference reward models.”; “extensive experimental results demonstrate that VisionPrefer significantly improves text-image alignment in compositional image generation across diverse aspects ... and generalizes better than previous human-preference metrics across various image distributions.”", "section": "Abstract; 1 Introduction; 4 Experiments (4.1 Reward Modeling)"}
{"claim": "Several parts of the paper are unclear enough that the reviewer could misread them, indicating the manuscript needs clarification in key sections.", "claim_type": "presentation", "paper_id": "IRXyPm9IPW", "paper_title": "Multimodal Large Language Models Make Text-to-Image Generative Models Align Better", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "juBZrLsCxq", "reviewer": "Reviewer_zspv", "review_text": "Comment: Dear authors, \n\nThank you for offering a comprehensive rebuttal in a polite manner despite my review is the only one showing negative polarity. I understand you have concerns about the review; sharing the same concerns, I have re-read the paper, which is why it takes time for me to respond. \n\nFirst of all, by the question raised \"Unfortunately, we did not fully understand your specific suggestion regarding 'training a bigger model to show the true value of the method.'\" I was referring to the experiment in section 4.2, which I believe is to use the constructed dataset to show that generative models can benefit from the dataset to align better (per line 157). In this case, it seems to me that the authors are forming a loop of construct a dataset with a bigger generative model and then show that it can help train a better smaller model. This is where I ask the question about the \"true value\" of the method yet to be validated. \n\nI understand the above requirement might be difficult to implement but I believe it will be essential for the study. For example, a simple logic loophole is that the bigger model will of course be a better aligned one and help the smaller model to perform better, it's like the conventional knowledge distillation research that we can get better small model with the help of a big model, this does not mean the same techniques will help improve the bigger model. \n\nOn the other hand, the authors might want to respond that \"getting a better big model\" is not the goal of this research, this research is about investigating how GPT-4V level models can offer better-aligned data to train smaller model in the beginning. In this case, I agree the paper is better justified, but in this case, it seems to me the paper will need some re-writing work, e.g., line 19 sends out a much stronger message than this. \n\nAgain, I thank the reviewers for offering a professional and polite rebuttal, and it takes me a while to respond because the current situation makes me feel like I need to re-read the paper, but it seems some of the concerns are still there. There is still a chance that I misread some parts, and I'm here for the authors to enlighten on these parts.", "labeling_timestamp": "2026-01-11T17:03:29.538885", "model": "gpt-5-mini", "label": "Supported", "justification": "The manuscript contains multiple truncated/garbled passages and formatting inconsistencies that make parts unclear and prone to misreading (e.g., broken sentences in Section 3, a truncated sentence in Section 4.1, and a duplicated/misaligned table header in Table 1). These issues indicate the paper needs clarification in key sections.", "evidence": "1) Section 3 (Step-1): \"a significant portion of the prompts in the DiffusionDB is biased towards certain\n\n> > > > > = Figure 2: VisionPrefer construction pipeline.\" 2) Table 1 header and columns: \"Feedback Format   | Feedback Format   | Feedback Format   |\\n| Dataset ... | Ranking          | Text          | Scalar          |\" 3) Section 4.1 truncated sentence: \"Specifically, we employ the average scores of each sample in VisionP\"", "section": "3 VisionPrefer; Table 1; 4.1 Reward Modeling"}
{"claim": "The authors did not fully explain how training a larger model would demonstrate the method's effectiveness, leaving the experimental suggestion unaddressed.", "claim_type": "methodology", "paper_id": "IRXyPm9IPW", "paper_title": "Multimodal Large Language Models Make Text-to-Image Generative Models Align Better", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "juBZrLsCxq", "reviewer": "Reviewer_zspv", "review_text": "Comment: Dear authors, \n\nThank you for offering a comprehensive rebuttal in a polite manner despite my review is the only one showing negative polarity. I understand you have concerns about the review; sharing the same concerns, I have re-read the paper, which is why it takes time for me to respond. \n\nFirst of all, by the question raised \"Unfortunately, we did not fully understand your specific suggestion regarding 'training a bigger model to show the true value of the method.'\" I was referring to the experiment in section 4.2, which I believe is to use the constructed dataset to show that generative models can benefit from the dataset to align better (per line 157). In this case, it seems to me that the authors are forming a loop of construct a dataset with a bigger generative model and then show that it can help train a better smaller model. This is where I ask the question about the \"true value\" of the method yet to be validated. \n\nI understand the above requirement might be difficult to implement but I believe it will be essential for the study. For example, a simple logic loophole is that the bigger model will of course be a better aligned one and help the smaller model to perform better, it's like the conventional knowledge distillation research that we can get better small model with the help of a big model, this does not mean the same techniques will help improve the bigger model. \n\nOn the other hand, the authors might want to respond that \"getting a better big model\" is not the goal of this research, this research is about investigating how GPT-4V level models can offer better-aligned data to train smaller model in the beginning. In this case, I agree the paper is better justified, but in this case, it seems to me the paper will need some re-writing work, e.g., line 19 sends out a much stronger message than this. \n\nAgain, I thank the reviewers for offering a professional and polite rebuttal, and it takes me a while to respond because the current situation makes me feel like I need to re-read the paper, but it seems some of the concerns are still there. There is still a chance that I misread some parts, and I'm here for the authors to enlighten on these parts.", "labeling_timestamp": "2026-01-11T17:03:41.831377", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does not discuss training a larger model as an experimental suggestion nor explain how that would demonstrate the method's effectiveness. The experiments described focus on training a reward model (VP-Score) and fine-tuning generative models using PPO and DPO, so the reviewer's claim about an unaddressed suggestion to train a larger model cannot be verified from the paper.", "evidence": "\"To validate the effectiveness of VisionPrefer and VP-Score, we adopt two reinforcement learning methods, Proximal Policy Optimization (PPO) and Direct Policy Optimization (DPO), to supervised fine-tune generative models, and extensive experimental results demonstrate that VisionPrefer significantly improves text-image alignment in compositional image generation across diverse aspects, e.g., aesthetic, and generalizes better than previous human-preference metrics across various image distributions.\"", "section": "Abstract"}
{"claim": "Section 4.2's claim that generative models can better align using the constructed dataset is not convincingly supported by the presented experiments.", "claim_type": "experimental", "paper_id": "IRXyPm9IPW", "paper_title": "Multimodal Large Language Models Make Text-to-Image Generative Models Align Better", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "juBZrLsCxq", "reviewer": "Reviewer_zspv", "review_text": "Comment: Dear authors, \n\nThank you for offering a comprehensive rebuttal in a polite manner despite my review is the only one showing negative polarity. I understand you have concerns about the review; sharing the same concerns, I have re-read the paper, which is why it takes time for me to respond. \n\nFirst of all, by the question raised \"Unfortunately, we did not fully understand your specific suggestion regarding 'training a bigger model to show the true value of the method.'\" I was referring to the experiment in section 4.2, which I believe is to use the constructed dataset to show that generative models can benefit from the dataset to align better (per line 157). In this case, it seems to me that the authors are forming a loop of construct a dataset with a bigger generative model and then show that it can help train a better smaller model. This is where I ask the question about the \"true value\" of the method yet to be validated. \n\nI understand the above requirement might be difficult to implement but I believe it will be essential for the study. For example, a simple logic loophole is that the bigger model will of course be a better aligned one and help the smaller model to perform better, it's like the conventional knowledge distillation research that we can get better small model with the help of a big model, this does not mean the same techniques will help improve the bigger model. \n\nOn the other hand, the authors might want to respond that \"getting a better big model\" is not the goal of this research, this research is about investigating how GPT-4V level models can offer better-aligned data to train smaller model in the beginning. In this case, I agree the paper is better justified, but in this case, it seems to me the paper will need some re-writing work, e.g., line 19 sends out a much stronger message than this. \n\nAgain, I thank the reviewers for offering a professional and polite rebuttal, and it takes me a while to respond because the current situation makes me feel like I need to re-read the paper, but it seems some of the concerns are still there. There is still a chance that I misread some parts, and I'm here for the authors to enlighten on these parts.", "labeling_timestamp": "2026-01-11T17:03:53.790394", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper asserts that VisionPrefer and VP-Score improve alignment and states that §4.2 uses PPO and DPO to validate this, but the provided excerpt does not include the actual §4.2 experimental results, metrics, or analyses needed to assess whether those experiments convincingly support the claim.", "evidence": "To validate the effectiveness of VisionPrefer and VP-Score, we adopt two reinforcement learning methods, Proximal Policy Optimization (PPO) and Direct Policy Optimization (DPO), to supervised fine-tune generative models, and extensive experimental results demonstrate that VisionPrefer significantly improves text-image alignment in compositional image generation across diverse aspects, e.g., aesthetic, and generalizes better than previous human-preference metrics across various image distributions.", "section": "Abstract and Section 4 (Experiments)"}
{"claim": "The a-posteriori and a-priori comparison made in the response is unclear to me.", "claim_type": "presentation", "paper_id": "BmG3NgH5xu", "paper_title": "FERERO: A Flexible Framework for Preference-Guided Multi-Objective Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "A5lBLaSAao", "reviewer": "Reviewer_PTWJ", "review_text": "Comment: Thanks for the response. The a-posteriori and a-priori comparison made in the response is unclear to me. Can you further clarify the difference of the approach from the one in [c,g]?  In these works, first, the user specifies preferences (a general polyhedral $C$ in [c] and $C=\\mathbb{R}^d_{+}$ in [g]. The objective function is unknown. Then, sequential evaluation of designs is performed, taking into account $C$. Only the objective values at queried designs are observed (with noise). When the algorithms stop, they output an estimated Pareto set $P$ of designs that is ($\\epsilon,\\delta$)-PAC correct. It seems to me that the main difference from the prior work is the knowledge and use of gradient information. \n\nCan this algorithm be used to find all Pareto optimal solutions or a desired subset of them? Do you have any guarantees on coverage of all Pareto optimal solutions or the diversity of returned solutions, etc?", "labeling_timestamp": "2026-01-11T17:03:48.798726", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not present any a-priori or a-posteriori comparison nor the authors' response that the reviewer refers to. Therefore there is insufficient information in the paper content to evaluate the reviewer’s claim about that comparison's clarity.", "evidence": "Table 1: Comparison to existing methods.\n...\nCan we develop a principled framework to capture flexible preferences and admit provably convergent deterministic and stochastic algorithms?", "section": "Introduction / Table 1"}
{"claim": "The paper does not clearly explain how its approach differs from the prior methods described in [c,g].", "claim_type": "presentation", "paper_id": "BmG3NgH5xu", "paper_title": "FERERO: A Flexible Framework for Preference-Guided Multi-Objective Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "A5lBLaSAao", "reviewer": "Reviewer_PTWJ", "review_text": "Comment: Thanks for the response. The a-posteriori and a-priori comparison made in the response is unclear to me. Can you further clarify the difference of the approach from the one in [c,g]?  In these works, first, the user specifies preferences (a general polyhedral $C$ in [c] and $C=\\mathbb{R}^d_{+}$ in [g]. The objective function is unknown. Then, sequential evaluation of designs is performed, taking into account $C$. Only the objective values at queried designs are observed (with noise). When the algorithms stop, they output an estimated Pareto set $P$ of designs that is ($\\epsilon,\\delta$)-PAC correct. It seems to me that the main difference from the prior work is the knowledge and use of gradient information. \n\nCan this algorithm be used to find all Pareto optimal solutions or a desired subset of them? Do you have any guarantees on coverage of all Pareto optimal solutions or the diversity of returned solutions, etc?", "labeling_timestamp": "2026-01-11T17:03:56.868645", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly compares FERERO to prior methods and states concrete differences (flexible preference modeling, an adaptive single-loop subprogram, and convergence guarantees). It provides a comparison table and specific claims about eliminating multiple subprograms and being the first single-loop primal algorithm, so the reviewer claim that the paper does not clearly explain differences is contradicted by the paper text.", "evidence": "1) \"A comparison of our methods to existing methods is summarized in Table 1.\" 2) \"Specifically, our contributions are listed as follows: ... C2) Under the FERERO framework, we develop a meta primal algorithm with a unified subprogram adaptive to both objectives and constraints to meet flexible preferences, eliminating the need for multiple subprograms under different active constraints. C3) Under the FERERO framework, we develop a practical single-loop algorithm with nonasymptotic convergence guarantees. To our best knowledge, this is the first single-loop primal algorithm in constrained vector optimization with convergence guarantees.\" 3) \"Because of this, it neither requires solving different subprograms at different stages nor requires different treatment of the active set of inequalities as in existing works [33, 41, 44].\"", "section": "Introduction; Table 1; Section 2.2"}
{"claim": "The main difference from the prior works [c,g] appears to be the knowledge and use of gradient information.", "claim_type": "methodology", "paper_id": "BmG3NgH5xu", "paper_title": "FERERO: A Flexible Framework for Preference-Guided Multi-Objective Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "A5lBLaSAao", "reviewer": "Reviewer_PTWJ", "review_text": "Comment: Thanks for the response. The a-posteriori and a-priori comparison made in the response is unclear to me. Can you further clarify the difference of the approach from the one in [c,g]?  In these works, first, the user specifies preferences (a general polyhedral $C$ in [c] and $C=\\mathbb{R}^d_{+}$ in [g]. The objective function is unknown. Then, sequential evaluation of designs is performed, taking into account $C$. Only the objective values at queried designs are observed (with noise). When the algorithms stop, they output an estimated Pareto set $P$ of designs that is ($\\epsilon,\\delta$)-PAC correct. It seems to me that the main difference from the prior work is the knowledge and use of gradient information. \n\nCan this algorithm be used to find all Pareto optimal solutions or a desired subset of them? Do you have any guarantees on coverage of all Pareto optimal solutions or the diversity of returned solutions, etc?", "labeling_timestamp": "2026-01-11T17:04:04.139858", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper identifies multiple primary differences from prior work (flexible preference modeling, an adaptive unified subprogram, and the first single-loop primal algorithm with convergence guarantees) rather than framing the main difference as merely knowledge/use of gradient information. While the method does use gradients to form update directions, the authors emphasize framework flexibility and algorithmic/convergence contributions as the main novelties.", "evidence": ["C1) We cast the PMOL problem as a constrained vector optimization problem, and develop the FERERO framework to capture flexible preferences.\nC2) Under the FERERO framework, we develop a meta primal algorithm with a unified subprogram adaptive to both objectives and constraints to meet flexible preferences, eliminating the need for multiple subprograms under different active constraints.\nC3) Under the FERERO framework, we develop a practical single-loop algorithm with nonasymptotic convergence guarantees. To our best knowledge, this is the first single-loop primal algorithm in constrained vector optimization with convergence guarantees.", "By the Lagrangian of (2.1), the optimal update direction can be expressed in a simple form as a weighted combination of the gradients, i.e. d*(θ) = -∇F(θ) A^⊤_ag λ*, with A_ag := [A; B_g; B_h], and ..."], "section": ["Introduction (Contributions)", "2.2 Find the preference-guided direction"]}
{"claim": "The paper does not clarify whether the proposed algorithm can find all Pareto optimal solutions or only a desired subset.", "claim_type": "methodology", "paper_id": "BmG3NgH5xu", "paper_title": "FERERO: A Flexible Framework for Preference-Guided Multi-Objective Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "A5lBLaSAao", "reviewer": "Reviewer_PTWJ", "review_text": "Comment: Thanks for the response. The a-posteriori and a-priori comparison made in the response is unclear to me. Can you further clarify the difference of the approach from the one in [c,g]?  In these works, first, the user specifies preferences (a general polyhedral $C$ in [c] and $C=\\mathbb{R}^d_{+}$ in [g]. The objective function is unknown. Then, sequential evaluation of designs is performed, taking into account $C$. Only the objective values at queried designs are observed (with noise). When the algorithms stop, they output an estimated Pareto set $P$ of designs that is ($\\epsilon,\\delta$)-PAC correct. It seems to me that the main difference from the prior work is the knowledge and use of gradient information. \n\nCan this algorithm be used to find all Pareto optimal solutions or a desired subset of them? Do you have any guarantees on coverage of all Pareto optimal solutions or the diversity of returned solutions, etc?", "labeling_timestamp": "2026-01-11T17:04:12.045722", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper consistently frames the goal as finding preference-guided solutions and provides convergence to KKT/stationary solutions for the constrained (preference) problem, but it does not state that the algorithm will enumerate or be able to find every Pareto-optimal point (the full Pareto front). The paper defines the CA-optimal set P_A and discusses finding preference-guided optimal models and convergence to KKT solutions, but it does not clarify whether the method can recover all Pareto-optimal solutions vs. only those consistent with a chosen preference specification.", "evidence": "\"Finding specific preference-guided Pareto solutions that represent different tradeoffs among multiple objectives is critical yet challenging in multi-objective problems.\" (Abstract)\n\n\"The C_A-optimal set, denoted as P_A, contains all the C_A-optimal models. When A = I_M, P_A is the Pareto optimal set P.\" (Section 2.1)\n\n\"Our goal is to design an algorithm that converges to a KKT solution based on (2.1).\" (end of Section 2.2)", "section": "Abstract; Section 2.1; Section 2.2"}
{"claim": "The paper provides no guarantees or evaluation for coverage of all Pareto optimal solutions or the diversity of returned solutions.", "claim_type": "experimental", "paper_id": "BmG3NgH5xu", "paper_title": "FERERO: A Flexible Framework for Preference-Guided Multi-Objective Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "A5lBLaSAao", "reviewer": "Reviewer_PTWJ", "review_text": "Comment: Thanks for the response. The a-posteriori and a-priori comparison made in the response is unclear to me. Can you further clarify the difference of the approach from the one in [c,g]?  In these works, first, the user specifies preferences (a general polyhedral $C$ in [c] and $C=\\mathbb{R}^d_{+}$ in [g]. The objective function is unknown. Then, sequential evaluation of designs is performed, taking into account $C$. Only the objective values at queried designs are observed (with noise). When the algorithms stop, they output an estimated Pareto set $P$ of designs that is ($\\epsilon,\\delta$)-PAC correct. It seems to me that the main difference from the prior work is the knowledge and use of gradient information. \n\nCan this algorithm be used to find all Pareto optimal solutions or a desired subset of them? Do you have any guarantees on coverage of all Pareto optimal solutions or the diversity of returned solutions, etc?", "labeling_timestamp": "2026-01-11T17:04:13.615595", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper provides convergence guarantees to KKT/preference-guided solutions (single-solution convergence) and reports experiments demonstrating finding preference-guided models, but it does not state any theoretical guarantees about covering the entire Pareto front or explicitly evaluate diversity/coverage of returned solutions in the provided content.", "evidence": "\"C3) Under the FERERO framework, we develop a practical single-loop algorithm with nonasymptotic convergence guarantees. To our best knowledge, this is the first single-loop primal algorithm in constrained vector optimization with convergence guarantees.\" and \"C4) We apply the proposed algorithms to various synthetic and real-world image and speech datasets to demonstrate its ability to find flexible preference-guided optimal models.\"", "section": "Contributions / Abstract"}
{"claim": "Algorithm 1 requires solving an optimization problem with n variables in each iteration (line 10), increasing per-iteration computational complexity.", "claim_type": "methodology", "paper_id": "TTrzgEZt9s", "paper_title": "Distributionally Robust Optimization with Bias and Variance Reduction", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "x0lC1jsG0I", "reviewer": "Reviewer_Ed47", "review_text": "Summary: The paper provides a practical algorithm to solve DRO problems. The algorithm enjoys a linear convergence rate and it does not require tuning multiple hyperparameters. The author further demonstrated the proposed algorithm can achieve 2-3x speedup compared to other methods vias read-data sets.\n\nStrengths: 1. The paper is well-written and the contribution is significant. The algorithm is easy to understand and easy to implement. The authors also provide intuition behind the algorithm via texts and figures.\n\nWeaknesses: 1. The paper needs to solve an optimization problem with n variables in each iteration (line 10 in Algorithm 1). It is very computationally expensive if n is large. I wonder whether the algorithm can reduce to another one-step stochastic gradient descent in this step. For example, the two-time scale method in distributionally robust RL [1].\n\n[1] Yang, Wenhao, et al. \"Avoiding model estimation in robust Markov decision processes with a generative model.\" arXiv preprint arXiv:2302.01248 (2023).\n\n2. The paper only provides theoretical results for the case $\\mu>0$. I am not sure if it is necessary or if it is only for theoretical convenience. It is known that the DRO objective $\\mathcal{R}_\\sigma$ could induce smoothness. Some discussions are needed and numerical examples with $\\mu=0$ will also help.\n\nQuestions: The papers (including the appendix) are long. I am not sure whether it is suitable for a conference review.\n\nI am curious that whether there are some unbiased estimators could be used, e.g., Wang et al (2023).\n\nWang, Shengbo, et al. \"A finite sample complexity bound for distributionally robust Q-learning.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2023.", "labeling_timestamp": "2026-01-11T17:04:26.772519", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper states that computing the gradient requires solving an inner maximization over the n-dimensional weight vector q, and that this maximization reduces to an isotonic regression solved by Pool Adjacent Violators in O(n) time. Algorithm 1 (which uses q_l as the running estimate) therefore requires solving this n-variable optimization when computing q.", "evidence": "“To solve for the maximizer, we prove by standard convex duality arguments (Prop. 3, Appx. B) that when D = D_f is an f-divergence, the maximum over q can be expressed as a minimization problem that reduces to isotonic regression problem involving f^∗ … Isotonic regression can be solved exactly by the Pool Adjacent Violators algorithm (Best et al., 2000), which runs in O(n) time when the losses are sorted; see Appx. C.”\n\n“Define R_σ := R_P(σ). … the full-batch gradient w ↦→ ∇R_σ(ℓ(w)) ∈ R^d can be computed by solving the inner maximization to retrieve l ↦→ ∇R_σ(l) ∈ R^n …”", "section": "2 MINIMIZING SPECTRAL RISK WITH BIAS AND VARIANCE REDUCTION"}
{"claim": "Solving an n-variable optimization in each iteration is computationally expensive when n is large, raising scalability concerns.", "claim_type": "methodology", "paper_id": "TTrzgEZt9s", "paper_title": "Distributionally Robust Optimization with Bias and Variance Reduction", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "x0lC1jsG0I", "reviewer": "Reviewer_Ed47", "review_text": "Summary: The paper provides a practical algorithm to solve DRO problems. The algorithm enjoys a linear convergence rate and it does not require tuning multiple hyperparameters. The author further demonstrated the proposed algorithm can achieve 2-3x speedup compared to other methods vias read-data sets.\n\nStrengths: 1. The paper is well-written and the contribution is significant. The algorithm is easy to understand and easy to implement. The authors also provide intuition behind the algorithm via texts and figures.\n\nWeaknesses: 1. The paper needs to solve an optimization problem with n variables in each iteration (line 10 in Algorithm 1). It is very computationally expensive if n is large. I wonder whether the algorithm can reduce to another one-step stochastic gradient descent in this step. For example, the two-time scale method in distributionally robust RL [1].\n\n[1] Yang, Wenhao, et al. \"Avoiding model estimation in robust Markov decision processes with a generative model.\" arXiv preprint arXiv:2302.01248 (2023).\n\n2. The paper only provides theoretical results for the case $\\mu>0$. I am not sure if it is necessary or if it is only for theoretical convenience. It is known that the DRO objective $\\mathcal{R}_\\sigma$ could induce smoothness. Some discussions are needed and numerical examples with $\\mu=0$ will also help.\n\nQuestions: The papers (including the appendix) are long. I am not sure whether it is suitable for a conference review.\n\nI am curious that whether there are some unbiased estimators could be used, e.g., Wang et al (2023).\n\nWang, Shengbo, et al. \"A finite sample complexity bound for distributionally robust Q-learning.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2023.", "labeling_timestamp": "2026-01-11T17:04:19.824014", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly states that computing the gradient requires n oracle calls and calls this 'prohibitive', and notes the inner isotonic-regression step runs in O(n) time, which supports the reviewer's scalability concern for large n.", "evidence": "“While the gradient is computable, however, accessing ℓ(w) and ∇ℓ(w) requires n calls to the function/gradient oracles {ℓ_i, ∇ℓ_i}_{i=1}^n, which can be prohibitive.”\n\n“Isotonic regression can be solved exactly by the Pool Adjacent Violators algorithm (Best et al., 2000), which runs in O(n) time when the losses are sorted; see Appx. C.”", "section": "Section 2: Minimizing Spectral Risk with Bias and Variance Reduction (Bias Reduction via Loss Estimation; Spectral Risk Measures and their Gradients)"}
{"claim": "The paper does not investigate whether the inner optimization step can be replaced by a one-step stochastic gradient descent or two-time scale method.", "claim_type": "methodology", "paper_id": "TTrzgEZt9s", "paper_title": "Distributionally Robust Optimization with Bias and Variance Reduction", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "x0lC1jsG0I", "reviewer": "Reviewer_Ed47", "review_text": "Summary: The paper provides a practical algorithm to solve DRO problems. The algorithm enjoys a linear convergence rate and it does not require tuning multiple hyperparameters. The author further demonstrated the proposed algorithm can achieve 2-3x speedup compared to other methods vias read-data sets.\n\nStrengths: 1. The paper is well-written and the contribution is significant. The algorithm is easy to understand and easy to implement. The authors also provide intuition behind the algorithm via texts and figures.\n\nWeaknesses: 1. The paper needs to solve an optimization problem with n variables in each iteration (line 10 in Algorithm 1). It is very computationally expensive if n is large. I wonder whether the algorithm can reduce to another one-step stochastic gradient descent in this step. For example, the two-time scale method in distributionally robust RL [1].\n\n[1] Yang, Wenhao, et al. \"Avoiding model estimation in robust Markov decision processes with a generative model.\" arXiv preprint arXiv:2302.01248 (2023).\n\n2. The paper only provides theoretical results for the case $\\mu>0$. I am not sure if it is necessary or if it is only for theoretical convenience. It is known that the DRO objective $\\mathcal{R}_\\sigma$ could induce smoothness. Some discussions are needed and numerical examples with $\\mu=0$ will also help.\n\nQuestions: The papers (including the appendix) are long. I am not sure whether it is suitable for a conference review.\n\nI am curious that whether there are some unbiased estimators could be used, e.g., Wang et al (2023).\n\nWang, Shengbo, et al. \"A finite sample complexity bound for distributionally robust Q-learning.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2023.", "labeling_timestamp": "2026-01-11T17:04:44.201537", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes solving the inner maximization exactly via an isotonic-regression formulation (Pool Adjacent Violators) and implements bias reduction by maintaining a full loss table l and using q_l as a running estimate. It does not discuss or evaluate replacing the inner optimization with a single-step stochastic gradient update or a two-time-scale scheme.", "evidence": "1) \"To solve for the maximizer, we prove by standard convex duality arguments (Prop. 3, Appx. B) that when D = D_f is an f-divergence, the maximum over q can be expressed as a minimization problem that reduces to isotonic regression problem involving f^*, the convex conjugate of f. Isotonic regression can be solved exactly by the Pool Adjacent Violators algorithm (Best et al., 2000), which runs in O(n) time when the losses are sorted; see Appx. C.\"  2) \"The key ingredient of bias reduction in Prospect is maintaining a table l ∈ R^n of losses such that l ≈ ℓ(w) for the current iterate w ∈ R^d, and using q_l as a running estimate of q_{ℓ(w)}.\"", "section": "2 MINIMIZING SPECTRAL RISK WITH BIAS AND VARIANCE REDUCTION (subsections: Spectral Risk Measures and their Gradients; Bias Reduction via Loss Estimation)"}
{"claim": "Theoretical results are provided only for the case mu>0, with no theoretical analysis presented for the mu=0 case.", "claim_type": "methodology", "paper_id": "TTrzgEZt9s", "paper_title": "Distributionally Robust Optimization with Bias and Variance Reduction", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "x0lC1jsG0I", "reviewer": "Reviewer_Ed47", "review_text": "Summary: The paper provides a practical algorithm to solve DRO problems. The algorithm enjoys a linear convergence rate and it does not require tuning multiple hyperparameters. The author further demonstrated the proposed algorithm can achieve 2-3x speedup compared to other methods vias read-data sets.\n\nStrengths: 1. The paper is well-written and the contribution is significant. The algorithm is easy to understand and easy to implement. The authors also provide intuition behind the algorithm via texts and figures.\n\nWeaknesses: 1. The paper needs to solve an optimization problem with n variables in each iteration (line 10 in Algorithm 1). It is very computationally expensive if n is large. I wonder whether the algorithm can reduce to another one-step stochastic gradient descent in this step. For example, the two-time scale method in distributionally robust RL [1].\n\n[1] Yang, Wenhao, et al. \"Avoiding model estimation in robust Markov decision processes with a generative model.\" arXiv preprint arXiv:2302.01248 (2023).\n\n2. The paper only provides theoretical results for the case $\\mu>0$. I am not sure if it is necessary or if it is only for theoretical convenience. It is known that the DRO objective $\\mathcal{R}_\\sigma$ could induce smoothness. Some discussions are needed and numerical examples with $\\mu=0$ will also help.\n\nQuestions: The papers (including the appendix) are long. I am not sure whether it is suitable for a conference review.\n\nI am curious that whether there are some unbiased estimators could be used, e.g., Wang et al (2023).\n\nWang, Shengbo, et al. \"A finite sample complexity bound for distributionally robust Q-learning.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2023.", "labeling_timestamp": "2026-01-11T17:04:39.211315", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper's theoretical guarantees are stated for positive shift cost (ν>0); the analysis (differentiability and linear convergence) explicitly assumes ν>0 and strong convexity of the divergence, and no analysis for the ν=0 case is provided in the quoted text.", "evidence": "\"we prove that it enjoys linear convergence for smooth regularized losses.\" (Abstract); \"Theoretically, Prospect converges linearly for any positive shift cost on regularized convex losses.\" (Contributions); \"When ν > 0 and the map q ↦→ D ( q ∥ 1_n/n ) is strongly convex over P(σ), we have ... R_σ is in fact differentiable with gradient given by ...\" (Sec. 2).", "section": "Abstract; Contributions; Sec. 2"}
{"claim": "The paper lacks discussion on whether the mu>0 assumption is necessary or simply used for theoretical convenience.", "claim_type": "methodology", "paper_id": "TTrzgEZt9s", "paper_title": "Distributionally Robust Optimization with Bias and Variance Reduction", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "x0lC1jsG0I", "reviewer": "Reviewer_Ed47", "review_text": "Summary: The paper provides a practical algorithm to solve DRO problems. The algorithm enjoys a linear convergence rate and it does not require tuning multiple hyperparameters. The author further demonstrated the proposed algorithm can achieve 2-3x speedup compared to other methods vias read-data sets.\n\nStrengths: 1. The paper is well-written and the contribution is significant. The algorithm is easy to understand and easy to implement. The authors also provide intuition behind the algorithm via texts and figures.\n\nWeaknesses: 1. The paper needs to solve an optimization problem with n variables in each iteration (line 10 in Algorithm 1). It is very computationally expensive if n is large. I wonder whether the algorithm can reduce to another one-step stochastic gradient descent in this step. For example, the two-time scale method in distributionally robust RL [1].\n\n[1] Yang, Wenhao, et al. \"Avoiding model estimation in robust Markov decision processes with a generative model.\" arXiv preprint arXiv:2302.01248 (2023).\n\n2. The paper only provides theoretical results for the case $\\mu>0$. I am not sure if it is necessary or if it is only for theoretical convenience. It is known that the DRO objective $\\mathcal{R}_\\sigma$ could induce smoothness. Some discussions are needed and numerical examples with $\\mu=0$ will also help.\n\nQuestions: The papers (including the appendix) are long. I am not sure whether it is suitable for a conference review.\n\nI am curious that whether there are some unbiased estimators could be used, e.g., Wang et al (2023).\n\nWang, Shengbo, et al. \"A finite sample complexity bound for distributionally robust Q-learning.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2023.", "labeling_timestamp": "2026-01-11T17:04:50.532169", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper repeatedly states linear convergence results under a strong-convexity/regularization assumption (µ>0) but does not discuss whether this assumption is necessary versus being a theoretical convenience. The Introduction/Contributions and Section 2 assert results for ‘regularized’ or ‘strongly convex’ losses without analyzing necessity of the assumption.", "evidence": "\"Theoretically, Prospect converges linearly for any positive shift cost on regularized convex losses.\" \n\n\"Our goal is to achieve unconditional linear convergence for smooth, strongly convex (regularized) losses with a single hyperparameter.\"", "section": "Abstract / Introduction (Contributions) and Sec. 2"}
{"claim": "No numerical experiments or empirical examples are provided for the mu=0 case to illustrate practical behavior without the mu>0 assumption.", "claim_type": "experimental", "paper_id": "TTrzgEZt9s", "paper_title": "Distributionally Robust Optimization with Bias and Variance Reduction", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "x0lC1jsG0I", "reviewer": "Reviewer_Ed47", "review_text": "Summary: The paper provides a practical algorithm to solve DRO problems. The algorithm enjoys a linear convergence rate and it does not require tuning multiple hyperparameters. The author further demonstrated the proposed algorithm can achieve 2-3x speedup compared to other methods vias read-data sets.\n\nStrengths: 1. The paper is well-written and the contribution is significant. The algorithm is easy to understand and easy to implement. The authors also provide intuition behind the algorithm via texts and figures.\n\nWeaknesses: 1. The paper needs to solve an optimization problem with n variables in each iteration (line 10 in Algorithm 1). It is very computationally expensive if n is large. I wonder whether the algorithm can reduce to another one-step stochastic gradient descent in this step. For example, the two-time scale method in distributionally robust RL [1].\n\n[1] Yang, Wenhao, et al. \"Avoiding model estimation in robust Markov decision processes with a generative model.\" arXiv preprint arXiv:2302.01248 (2023).\n\n2. The paper only provides theoretical results for the case $\\mu>0$. I am not sure if it is necessary or if it is only for theoretical convenience. It is known that the DRO objective $\\mathcal{R}_\\sigma$ could induce smoothness. Some discussions are needed and numerical examples with $\\mu=0$ will also help.\n\nQuestions: The papers (including the appendix) are long. I am not sure whether it is suitable for a conference review.\n\nI am curious that whether there are some unbiased estimators could be used, e.g., Wang et al (2023).\n\nWang, Shengbo, et al. \"A finite sample complexity bound for distributionally robust Q-learning.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2023.", "labeling_timestamp": "2026-01-11T17:04:54.132506", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper's theory and derivations explicitly assume a positive shift cost (ν>0), and the excerpt reports empirical results for Prospect but does not state whether any experiments were run with μ=0 (ν=0). The provided content therefore does not contain information confirming or denying that numerical experiments for the μ=0 case were included.", "evidence": "1) \"Theoretically, Prospect converges linearly for any positive shift cost on regularized convex losses.\" (Contributions).\n2) \"When ν > 0 and the map q ↦→ D ( q ∥ 1 n /n ) is strongly convex over P ( σ ), we have that ... R_σ is in fact differentiable with gradient given by ...\" (Sec. 2).\n3) \"Empirically, we show that Prospect can converge 2-3x faster than baselines such as SGD and stochastic saddle-point methods on distribution shift and fairness benchmarks spanning tabular, vision, and language domains.\" (Abstract).", "section": "Abstract; Contributions; Section 2 (Spectral Risk Measures and their Gradients)"}
{"claim": "The manuscript, including its appendix, is very long, raising concerns about its suitability and conciseness for a conference submission.", "claim_type": "presentation", "paper_id": "TTrzgEZt9s", "paper_title": "Distributionally Robust Optimization with Bias and Variance Reduction", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "x0lC1jsG0I", "reviewer": "Reviewer_Ed47", "review_text": "Summary: The paper provides a practical algorithm to solve DRO problems. The algorithm enjoys a linear convergence rate and it does not require tuning multiple hyperparameters. The author further demonstrated the proposed algorithm can achieve 2-3x speedup compared to other methods vias read-data sets.\n\nStrengths: 1. The paper is well-written and the contribution is significant. The algorithm is easy to understand and easy to implement. The authors also provide intuition behind the algorithm via texts and figures.\n\nWeaknesses: 1. The paper needs to solve an optimization problem with n variables in each iteration (line 10 in Algorithm 1). It is very computationally expensive if n is large. I wonder whether the algorithm can reduce to another one-step stochastic gradient descent in this step. For example, the two-time scale method in distributionally robust RL [1].\n\n[1] Yang, Wenhao, et al. \"Avoiding model estimation in robust Markov decision processes with a generative model.\" arXiv preprint arXiv:2302.01248 (2023).\n\n2. The paper only provides theoretical results for the case $\\mu>0$. I am not sure if it is necessary or if it is only for theoretical convenience. It is known that the DRO objective $\\mathcal{R}_\\sigma$ could induce smoothness. Some discussions are needed and numerical examples with $\\mu=0$ will also help.\n\nQuestions: The papers (including the appendix) are long. I am not sure whether it is suitable for a conference review.\n\nI am curious that whether there are some unbiased estimators could be used, e.g., Wang et al (2023).\n\nWang, Shengbo, et al. \"A finite sample complexity bound for distributionally robust Q-learning.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2023.", "labeling_timestamp": "2026-01-11T17:05:20.959420", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper references appendices (Appx. B, Appx. C) and detailed proofs, but does not state or provide information in the provided content about the overall manuscript length or whether the manuscript plus appendix is 'very long' or unsuitable for a conference submission. Therefore the claim cannot be confirmed from the paper text shown.", "evidence": "\"See Appx. B for further details on SRMs including the extremile (Daouia et al., 2019) in Equation (22) and ESRM (Cotter & Tasche, 2006) in Equation (23).\"; \"Isotonic regression can be solved exactly by the Pool Adjacent Violators algorithm (Best et al., 2000), which runs in O ( n ) time when the losses are sorted; see Appx. C.\"", "section": "Section 2 (MINIMIZING SPECTRAL RISK WITH BIAS AND VARIANCE REDUCTION)"}
{"claim": "The paper does not consider or evaluate unbiased estimators such as those proposed by Wang et al. (2023), omitting potentially relevant methodological alternatives.", "claim_type": "methodology", "paper_id": "TTrzgEZt9s", "paper_title": "Distributionally Robust Optimization with Bias and Variance Reduction", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "x0lC1jsG0I", "reviewer": "Reviewer_Ed47", "review_text": "Summary: The paper provides a practical algorithm to solve DRO problems. The algorithm enjoys a linear convergence rate and it does not require tuning multiple hyperparameters. The author further demonstrated the proposed algorithm can achieve 2-3x speedup compared to other methods vias read-data sets.\n\nStrengths: 1. The paper is well-written and the contribution is significant. The algorithm is easy to understand and easy to implement. The authors also provide intuition behind the algorithm via texts and figures.\n\nWeaknesses: 1. The paper needs to solve an optimization problem with n variables in each iteration (line 10 in Algorithm 1). It is very computationally expensive if n is large. I wonder whether the algorithm can reduce to another one-step stochastic gradient descent in this step. For example, the two-time scale method in distributionally robust RL [1].\n\n[1] Yang, Wenhao, et al. \"Avoiding model estimation in robust Markov decision processes with a generative model.\" arXiv preprint arXiv:2302.01248 (2023).\n\n2. The paper only provides theoretical results for the case $\\mu>0$. I am not sure if it is necessary or if it is only for theoretical convenience. It is known that the DRO objective $\\mathcal{R}_\\sigma$ could induce smoothness. Some discussions are needed and numerical examples with $\\mu=0$ will also help.\n\nQuestions: The papers (including the appendix) are long. I am not sure whether it is suitable for a conference review.\n\nI am curious that whether there are some unbiased estimators could be used, e.g., Wang et al (2023).\n\nWang, Shengbo, et al. \"A finite sample complexity bound for distributionally robust Q-learning.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2023.", "labeling_timestamp": "2026-01-11T17:05:14.219650", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper cites Wang et al. (2023) in a broad literature survey (Related Work) but does not discuss or evaluate the unbiased estimators from that work. The empirical comparisons and listed baseline methods (SGD, stochastic regularized dual averaging, LSVRG, stochastic saddle-point SAGA) do not include Wang et al.'s estimators, indicating omission of that methodological alternative.", "evidence": "Related Work: \"Examples of DRO formulations range throughout diverse contexts ... (Liu et al., 2022b; Kallus et al., 2022; Liu et al., 2022c; Xu et al., 2023; Wang et al., 2023; Lotidis et al., 2023; Kallus et al., 2022; Ren & Majumdar, 2022; Clement & Kroer, 2021).\" In comparisons: \"In comparisons, we include stochastic algorithms that either are single-hyperparameter 'out-of-thebox' methods such as stochastic gradient descent and stochastic regularized dual averaging (Xiao, 2009), or multi-hyperparameter methods that converge linearly on strongly convex SRM-based objectives, such as LSVRG (Mehta et al., 2023) and stochastic saddle-point SAGA (Palaniappan & Bach, 2016).\"", "section": "Related Work / Comparisons"}
{"claim": "The paper does not clearly explain the advantages of single-level optimization in MG loss over IOT's bi-level optimization.", "claim_type": "presentation", "paper_id": "P50qJuu4IY", "paper_title": "Self-Supervised Learning with the Matching Gap", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "W9ybmgKZdT", "reviewer": "Reviewer_tJTN", "review_text": "Summary: This paper proposed an alternative loss, Matching Gap (MG), to contrastive loss for self-supervised representation learning. Unlike contrastive loss enforcing the sample-wise invariance to data perturbations, the MG loss is a set-based loss, driven by minimizing the difference between the ground-truth transport loss and the optimal transport loss computed in the representation space using the Sinkhorn algorithm. The authors detailedly discussed the differences and connections of MG loss to contrastive loss and prior optimal transport, showing the unique properties of the proposed method. Finally, experiments on ImageNet-1k dataset suggested a comparable performance of MG to prior arts.\n\nStrengths: 1. The paper is overall well-motivated. The reliance on data augmentation is one of the most prominent nuisances of contrastive learning. It is good to see more exploration toward bypassing this issue.\n\n2. The theoretical analysis presented MG loss in a straightforward way and is overall easy to grasp. It also discussed the links between MG loss and contrastive loss/invert optimal transport loss, showing its unique properties as a set-based loss with single-level optimization.\n\n3. MG loss exhibited superior performance to contrastive loss in weak augmentation and low training epochs regime.\n\nWeaknesses: 1. The advantages of the single-level optimization in MG loss over the bi-level optimization in IOT loss are not provided clearly. Figure 2 shows that MG loss slightly underperforms but is competitive with IOT loss. I wonder if it improves the training speed/convergence or reduces the memory consumption?\n\n2. Unfair comparisons. The implementation of the experiments largely followed the setting of Dino, which used two global crops and ten local crops by default. However, some of the baseline methods, e.g., MoCov3 and I-JEPA, used only two global crops, making it unfair to directly compare the performance with MG loss on the default setting.\n\n3. Even under the potentially unfair comparison, the performance of MG loss is only comparable and sometimes even inferior to the contrastive loss.\n\n4. Some notations are used without first introduced, e.g., $c(\\cdot,\\cdot)$ in Introduction and $t(\\cdot,\\cdot)$ in Sec. 3.\n\nQuestions: See the weaknesses.\n\nOverall, I think the proposed loss is interesting, and I like the presentation of this paper. However, the evaluation part still has significant room for improvement.", "labeling_timestamp": "2026-01-11T17:05:46.989142", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper makes brief claims that the matching gap (MG) is 'conceptually and computationally simpler' and notes it 'can be easily differentiated using Danskin's theorem', but it does not provide a clear, detailed explanation or comparison of the specific advantages of single-level MG optimization over IOT's bi-level optimization in the provided content.", "evidence": "\"we implement the matching gap using the Sinkhorn algorithm and show that it can be easily differentiated using Danskin's theorem.\"; \"we scale up both Shi et al.'s approach and ours, the matching gap loss, to ImageNet-1k and show that they work similarly, with the matching gap being both conceptually and computationally simpler.\"", "section": "Abstract; Introduction (Our Contributions)"}
{"claim": "Figure 2 shows MG loss slightly underperforms but remains competitive with IOT loss.", "claim_type": "experimental", "paper_id": "P50qJuu4IY", "paper_title": "Self-Supervised Learning with the Matching Gap", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "W9ybmgKZdT", "reviewer": "Reviewer_tJTN", "review_text": "Summary: This paper proposed an alternative loss, Matching Gap (MG), to contrastive loss for self-supervised representation learning. Unlike contrastive loss enforcing the sample-wise invariance to data perturbations, the MG loss is a set-based loss, driven by minimizing the difference between the ground-truth transport loss and the optimal transport loss computed in the representation space using the Sinkhorn algorithm. The authors detailedly discussed the differences and connections of MG loss to contrastive loss and prior optimal transport, showing the unique properties of the proposed method. Finally, experiments on ImageNet-1k dataset suggested a comparable performance of MG to prior arts.\n\nStrengths: 1. The paper is overall well-motivated. The reliance on data augmentation is one of the most prominent nuisances of contrastive learning. It is good to see more exploration toward bypassing this issue.\n\n2. The theoretical analysis presented MG loss in a straightforward way and is overall easy to grasp. It also discussed the links between MG loss and contrastive loss/invert optimal transport loss, showing its unique properties as a set-based loss with single-level optimization.\n\n3. MG loss exhibited superior performance to contrastive loss in weak augmentation and low training epochs regime.\n\nWeaknesses: 1. The advantages of the single-level optimization in MG loss over the bi-level optimization in IOT loss are not provided clearly. Figure 2 shows that MG loss slightly underperforms but is competitive with IOT loss. I wonder if it improves the training speed/convergence or reduces the memory consumption?\n\n2. Unfair comparisons. The implementation of the experiments largely followed the setting of Dino, which used two global crops and ten local crops by default. However, some of the baseline methods, e.g., MoCov3 and I-JEPA, used only two global crops, making it unfair to directly compare the performance with MG loss on the default setting.\n\n3. Even under the potentially unfair comparison, the performance of MG loss is only comparable and sometimes even inferior to the contrastive loss.\n\n4. Some notations are used without first introduced, e.g., $c(\\cdot,\\cdot)$ in Introduction and $t(\\cdot,\\cdot)$ in Sec. 3.\n\nQuestions: See the weaknesses.\n\nOverall, I think the proposed loss is interesting, and I like the presentation of this paper. However, the evaluation part still has significant room for improvement.", "labeling_timestamp": "2026-01-11T17:05:38.156689", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper states that the Matching Gap (MG) is competitive with prior work and reports higher ImageNet numbers than I-JEPA; it does not support a claim that MG 'slightly underperforms' IOT. No Figure 2 or evidence showing MG underperforming IOT is present in the provided content.", "evidence": "Abstract: \"Using only cropping and flipping, we achieve 74.2% top-1 accuracy with a ViT-B/16 on ImageNet-1k, to be compared to 72.9% for I-JEPA (Assran et al., 2023).\"; Our Contributions: \"Our approach achieves 76.7% top1 accuracy on ImageNet-1k with a ViT-B, with augmentations, and 74.2% without, improving on Assran et al. (2023).\"; Our Contributions: \"we scale up both Shi et al.'s approach and ours, the matching gap loss, to ImageNet-1k and show that they work similarly, with the matching gap being both conceptually and computationally simpler.\"", "section": "Abstract / Our Contributions (Introduction)"}
{"claim": "The authors did not evaluate whether MG improves training speed, convergence, or reduces memory consumption compared to IOT.", "claim_type": "experimental", "paper_id": "P50qJuu4IY", "paper_title": "Self-Supervised Learning with the Matching Gap", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "W9ybmgKZdT", "reviewer": "Reviewer_tJTN", "review_text": "Summary: This paper proposed an alternative loss, Matching Gap (MG), to contrastive loss for self-supervised representation learning. Unlike contrastive loss enforcing the sample-wise invariance to data perturbations, the MG loss is a set-based loss, driven by minimizing the difference between the ground-truth transport loss and the optimal transport loss computed in the representation space using the Sinkhorn algorithm. The authors detailedly discussed the differences and connections of MG loss to contrastive loss and prior optimal transport, showing the unique properties of the proposed method. Finally, experiments on ImageNet-1k dataset suggested a comparable performance of MG to prior arts.\n\nStrengths: 1. The paper is overall well-motivated. The reliance on data augmentation is one of the most prominent nuisances of contrastive learning. It is good to see more exploration toward bypassing this issue.\n\n2. The theoretical analysis presented MG loss in a straightforward way and is overall easy to grasp. It also discussed the links between MG loss and contrastive loss/invert optimal transport loss, showing its unique properties as a set-based loss with single-level optimization.\n\n3. MG loss exhibited superior performance to contrastive loss in weak augmentation and low training epochs regime.\n\nWeaknesses: 1. The advantages of the single-level optimization in MG loss over the bi-level optimization in IOT loss are not provided clearly. Figure 2 shows that MG loss slightly underperforms but is competitive with IOT loss. I wonder if it improves the training speed/convergence or reduces the memory consumption?\n\n2. Unfair comparisons. The implementation of the experiments largely followed the setting of Dino, which used two global crops and ten local crops by default. However, some of the baseline methods, e.g., MoCov3 and I-JEPA, used only two global crops, making it unfair to directly compare the performance with MG loss on the default setting.\n\n3. Even under the potentially unfair comparison, the performance of MG loss is only comparable and sometimes even inferior to the contrastive loss.\n\n4. Some notations are used without first introduced, e.g., $c(\\cdot,\\cdot)$ in Introduction and $t(\\cdot,\\cdot)$ in Sec. 3.\n\nQuestions: See the weaknesses.\n\nOverall, I think the proposed loss is interesting, and I like the presentation of this paper. However, the evaluation part still has significant room for improvement.", "labeling_timestamp": "2026-01-11T17:05:35.049547", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper reports accuracy comparisons and states the matching gap is conceptually and computationally simpler, and describes algorithmic complexity of Sinkhorn vs assignment, but it does not present any empirical evaluation of training speed, convergence behavior, or memory usage comparing MG to IOT/I-JEPA.", "evidence": ["Abstract: \"We implement the matching gap using the Sinkhorn algorithm and show that it can be easily differentiated using Danskin's theorem. In practice, we show that we can learn competitive features, even without extensive data augmentations: Using only cropping and flipping, we achieve 74.2% top-1 accuracy with a ViT-B/16 on ImageNet-1k, to be compared to 72.9% for I-JEPA (Assran et al., 2023).\"", "Our Contributions: \"We observe that learning with a matching loss is competitive with other approaches, and works well even with no data augmentation beyond cropping. We scale up both Shi et al.'s approach and ours, the matching gap loss, to ImageNet-1k and show that they work similarly, with the matching gap being both conceptually and computationally simpler. Our approach achieves 76.7% top1 accuracy on ImageNet-1k with a ViT-B, with augmentations, and 74.2% without, improving on Assran et al. (2023).\"", "Section 2.2: \"When ε = 0, one recovers the optimal assignment problem... with worst-case complexity O(n^3). When ε > 0, the problem can be solved with the Sinkhorn fixed point iterations... with quadratically scaling w.r.t. sample size n... and better GPU performance.\""], "section": "Abstract; Our Contributions; Section 2.2"}
{"claim": "The experiments use Dino's default augmentation setting of two global crops plus ten local crops.", "claim_type": "methodology", "paper_id": "P50qJuu4IY", "paper_title": "Self-Supervised Learning with the Matching Gap", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "W9ybmgKZdT", "reviewer": "Reviewer_tJTN", "review_text": "Summary: This paper proposed an alternative loss, Matching Gap (MG), to contrastive loss for self-supervised representation learning. Unlike contrastive loss enforcing the sample-wise invariance to data perturbations, the MG loss is a set-based loss, driven by minimizing the difference between the ground-truth transport loss and the optimal transport loss computed in the representation space using the Sinkhorn algorithm. The authors detailedly discussed the differences and connections of MG loss to contrastive loss and prior optimal transport, showing the unique properties of the proposed method. Finally, experiments on ImageNet-1k dataset suggested a comparable performance of MG to prior arts.\n\nStrengths: 1. The paper is overall well-motivated. The reliance on data augmentation is one of the most prominent nuisances of contrastive learning. It is good to see more exploration toward bypassing this issue.\n\n2. The theoretical analysis presented MG loss in a straightforward way and is overall easy to grasp. It also discussed the links between MG loss and contrastive loss/invert optimal transport loss, showing its unique properties as a set-based loss with single-level optimization.\n\n3. MG loss exhibited superior performance to contrastive loss in weak augmentation and low training epochs regime.\n\nWeaknesses: 1. The advantages of the single-level optimization in MG loss over the bi-level optimization in IOT loss are not provided clearly. Figure 2 shows that MG loss slightly underperforms but is competitive with IOT loss. I wonder if it improves the training speed/convergence or reduces the memory consumption?\n\n2. Unfair comparisons. The implementation of the experiments largely followed the setting of Dino, which used two global crops and ten local crops by default. However, some of the baseline methods, e.g., MoCov3 and I-JEPA, used only two global crops, making it unfair to directly compare the performance with MG loss on the default setting.\n\n3. Even under the potentially unfair comparison, the performance of MG loss is only comparable and sometimes even inferior to the contrastive loss.\n\n4. Some notations are used without first introduced, e.g., $c(\\cdot,\\cdot)$ in Introduction and $t(\\cdot,\\cdot)$ in Sec. 3.\n\nQuestions: See the weaknesses.\n\nOverall, I think the proposed loss is interesting, and I like the presentation of this paper. However, the evaluation part still has significant room for improvement.", "labeling_timestamp": "2026-01-11T17:05:35.128391", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does not state that it used DINO's default augmentation schedule (two global + ten local crops). It only describes using two perturbations/augmentations per image in the theoretical setup and elsewhere highlights experiments with minimal augmentations (\"cropping and flipping\") or unspecified \"augmentations\", but provides no explicit claim that the DINO multi-crop protocol was used.", "evidence": "\"For each image z_i from a set of n images, we apply two different augmentations/perturbations A1 and A2 to obtain two images A1(z_i) and A2(z_i).\"; \"We observe that learning with a matching loss is competitive with other approaches, and works well even with no data augmentation beyond cropping.\"", "section": "Section 2.1 (Self-Supervised Learning...) and Abstract / Contributions"}
{"claim": "Some baseline methods, such as MoCov3 and I-JEPA, used only two global crops in their original implementations.", "claim_type": "baseline", "paper_id": "P50qJuu4IY", "paper_title": "Self-Supervised Learning with the Matching Gap", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "W9ybmgKZdT", "reviewer": "Reviewer_tJTN", "review_text": "Summary: This paper proposed an alternative loss, Matching Gap (MG), to contrastive loss for self-supervised representation learning. Unlike contrastive loss enforcing the sample-wise invariance to data perturbations, the MG loss is a set-based loss, driven by minimizing the difference between the ground-truth transport loss and the optimal transport loss computed in the representation space using the Sinkhorn algorithm. The authors detailedly discussed the differences and connections of MG loss to contrastive loss and prior optimal transport, showing the unique properties of the proposed method. Finally, experiments on ImageNet-1k dataset suggested a comparable performance of MG to prior arts.\n\nStrengths: 1. The paper is overall well-motivated. The reliance on data augmentation is one of the most prominent nuisances of contrastive learning. It is good to see more exploration toward bypassing this issue.\n\n2. The theoretical analysis presented MG loss in a straightforward way and is overall easy to grasp. It also discussed the links between MG loss and contrastive loss/invert optimal transport loss, showing its unique properties as a set-based loss with single-level optimization.\n\n3. MG loss exhibited superior performance to contrastive loss in weak augmentation and low training epochs regime.\n\nWeaknesses: 1. The advantages of the single-level optimization in MG loss over the bi-level optimization in IOT loss are not provided clearly. Figure 2 shows that MG loss slightly underperforms but is competitive with IOT loss. I wonder if it improves the training speed/convergence or reduces the memory consumption?\n\n2. Unfair comparisons. The implementation of the experiments largely followed the setting of Dino, which used two global crops and ten local crops by default. However, some of the baseline methods, e.g., MoCov3 and I-JEPA, used only two global crops, making it unfair to directly compare the performance with MG loss on the default setting.\n\n3. Even under the potentially unfair comparison, the performance of MG loss is only comparable and sometimes even inferior to the contrastive loss.\n\n4. Some notations are used without first introduced, e.g., $c(\\cdot,\\cdot)$ in Introduction and $t(\\cdot,\\cdot)$ in Sec. 3.\n\nQuestions: See the weaknesses.\n\nOverall, I think the proposed loss is interesting, and I like the presentation of this paper. However, the evaluation part still has significant room for improvement.", "labeling_timestamp": "2026-01-11T17:05:54.129178", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper text mentions I-JEPA in comparison and describes the common practice of using two augmentations/views per image in contrastive setups, but it does not state that MoCov3 or I-JEPA used only two global crops in their original implementations. MoCov3 is not mentioned, and no explicit claim about the number or type of crops used by I-JEPA appears in the quoted sections.", "evidence": "Abstract: \"...to be compared to 72.9% for I-JEPA (Assran et al., 2023).\"; Section 2.1: \"For each image z_i from a set of n images, we apply two different augmentations/perturbations A1 and A2 to obtain two images A1(z_i) and A2(z_i).\"", "section": "Abstract; Section 2.1 (Self-Supervised Learning, With a Contrastive Lens)"}
{"claim": "Directly comparing MG with baselines that used fewer crops is unfair because the crop settings differ.", "claim_type": "baseline", "paper_id": "P50qJuu4IY", "paper_title": "Self-Supervised Learning with the Matching Gap", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "W9ybmgKZdT", "reviewer": "Reviewer_tJTN", "review_text": "Summary: This paper proposed an alternative loss, Matching Gap (MG), to contrastive loss for self-supervised representation learning. Unlike contrastive loss enforcing the sample-wise invariance to data perturbations, the MG loss is a set-based loss, driven by minimizing the difference between the ground-truth transport loss and the optimal transport loss computed in the representation space using the Sinkhorn algorithm. The authors detailedly discussed the differences and connections of MG loss to contrastive loss and prior optimal transport, showing the unique properties of the proposed method. Finally, experiments on ImageNet-1k dataset suggested a comparable performance of MG to prior arts.\n\nStrengths: 1. The paper is overall well-motivated. The reliance on data augmentation is one of the most prominent nuisances of contrastive learning. It is good to see more exploration toward bypassing this issue.\n\n2. The theoretical analysis presented MG loss in a straightforward way and is overall easy to grasp. It also discussed the links between MG loss and contrastive loss/invert optimal transport loss, showing its unique properties as a set-based loss with single-level optimization.\n\n3. MG loss exhibited superior performance to contrastive loss in weak augmentation and low training epochs regime.\n\nWeaknesses: 1. The advantages of the single-level optimization in MG loss over the bi-level optimization in IOT loss are not provided clearly. Figure 2 shows that MG loss slightly underperforms but is competitive with IOT loss. I wonder if it improves the training speed/convergence or reduces the memory consumption?\n\n2. Unfair comparisons. The implementation of the experiments largely followed the setting of Dino, which used two global crops and ten local crops by default. However, some of the baseline methods, e.g., MoCov3 and I-JEPA, used only two global crops, making it unfair to directly compare the performance with MG loss on the default setting.\n\n3. Even under the potentially unfair comparison, the performance of MG loss is only comparable and sometimes even inferior to the contrastive loss.\n\n4. Some notations are used without first introduced, e.g., $c(\\cdot,\\cdot)$ in Introduction and $t(\\cdot,\\cdot)$ in Sec. 3.\n\nQuestions: See the weaknesses.\n\nOverall, I think the proposed loss is interesting, and I like the presentation of this paper. However, the evaluation part still has significant room for improvement.", "labeling_timestamp": "2026-01-11T17:06:07.472684", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper claims MG performs well with minimal augmentations and directly compares its accuracies to baselines (e.g., I-JEPA), but it does not state the number or types of crops used by those baselines or discuss differences in crop settings. Therefore the paper does not provide enough information to confirm the reviewer's claim that baselines used fewer crops making the comparison unfair.", "evidence": "\"In practice, we show that we can learn competitive features, even without extensive data augmentations: Using only cropping and flipping, we achieve 74.2% top-1 accuracy with a ViT-B/16 on ImageNet-1k, to be compared to 72.9% for I-JEPA (Assran et al., 2023).\"", "section": "Abstract / Contributions"}
{"claim": "Despite potentially unfair settings, MG loss performance is only comparable and sometimes inferior to the contrastive loss.", "claim_type": "experimental", "paper_id": "P50qJuu4IY", "paper_title": "Self-Supervised Learning with the Matching Gap", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "W9ybmgKZdT", "reviewer": "Reviewer_tJTN", "review_text": "Summary: This paper proposed an alternative loss, Matching Gap (MG), to contrastive loss for self-supervised representation learning. Unlike contrastive loss enforcing the sample-wise invariance to data perturbations, the MG loss is a set-based loss, driven by minimizing the difference between the ground-truth transport loss and the optimal transport loss computed in the representation space using the Sinkhorn algorithm. The authors detailedly discussed the differences and connections of MG loss to contrastive loss and prior optimal transport, showing the unique properties of the proposed method. Finally, experiments on ImageNet-1k dataset suggested a comparable performance of MG to prior arts.\n\nStrengths: 1. The paper is overall well-motivated. The reliance on data augmentation is one of the most prominent nuisances of contrastive learning. It is good to see more exploration toward bypassing this issue.\n\n2. The theoretical analysis presented MG loss in a straightforward way and is overall easy to grasp. It also discussed the links between MG loss and contrastive loss/invert optimal transport loss, showing its unique properties as a set-based loss with single-level optimization.\n\n3. MG loss exhibited superior performance to contrastive loss in weak augmentation and low training epochs regime.\n\nWeaknesses: 1. The advantages of the single-level optimization in MG loss over the bi-level optimization in IOT loss are not provided clearly. Figure 2 shows that MG loss slightly underperforms but is competitive with IOT loss. I wonder if it improves the training speed/convergence or reduces the memory consumption?\n\n2. Unfair comparisons. The implementation of the experiments largely followed the setting of Dino, which used two global crops and ten local crops by default. However, some of the baseline methods, e.g., MoCov3 and I-JEPA, used only two global crops, making it unfair to directly compare the performance with MG loss on the default setting.\n\n3. Even under the potentially unfair comparison, the performance of MG loss is only comparable and sometimes even inferior to the contrastive loss.\n\n4. Some notations are used without first introduced, e.g., $c(\\cdot,\\cdot)$ in Introduction and $t(\\cdot,\\cdot)$ in Sec. 3.\n\nQuestions: See the weaknesses.\n\nOverall, I think the proposed loss is interesting, and I like the presentation of this paper. However, the evaluation part still has significant room for improvement.", "labeling_timestamp": "2026-01-11T17:06:09.479066", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper claims the matching gap (MG) loss is competitive and reports ImageNet top-1 numbers (e.g. 76.7% and 74.2%) and an improvement over I-JEPA, but the provided content does not present a direct, explicit comparison of MG against standard contrastive (InfoNCE) losses to support the reviewer's statement that MG is only comparable or sometimes inferior. Therefore there is insufficient information in the paper excerpt to confirm the reviewer’s claim.", "evidence": ["\"In practice, we show that we can learn competitive features, even without extensive data augmentations: Using only cropping and flipping, we achieve 74.2% top-1 accuracy with a ViT-B/16 on ImageNet-1k, to be compared to 72.9% for I-JEPA (Assran et al., 2023).\" (Abstract)", "\"Our approach achieves 76.7% top1 accuracy on ImageNet-1k with a ViT-B, with augmentations, and 74.2% without, improving on Assran et al. (2023). This shows that our approach is a competitive alternative to the joint embedding predictive architectures paradigm (JEPA...).\" (Introduction - Our Contributions)"], "section": "Abstract; Introduction - Our Contributions"}
{"claim": "The paper lacks controlled experiments that equalize crop augmentation settings between MG and baseline methods.", "claim_type": "experimental", "paper_id": "P50qJuu4IY", "paper_title": "Self-Supervised Learning with the Matching Gap", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "W9ybmgKZdT", "reviewer": "Reviewer_tJTN", "review_text": "Summary: This paper proposed an alternative loss, Matching Gap (MG), to contrastive loss for self-supervised representation learning. Unlike contrastive loss enforcing the sample-wise invariance to data perturbations, the MG loss is a set-based loss, driven by minimizing the difference between the ground-truth transport loss and the optimal transport loss computed in the representation space using the Sinkhorn algorithm. The authors detailedly discussed the differences and connections of MG loss to contrastive loss and prior optimal transport, showing the unique properties of the proposed method. Finally, experiments on ImageNet-1k dataset suggested a comparable performance of MG to prior arts.\n\nStrengths: 1. The paper is overall well-motivated. The reliance on data augmentation is one of the most prominent nuisances of contrastive learning. It is good to see more exploration toward bypassing this issue.\n\n2. The theoretical analysis presented MG loss in a straightforward way and is overall easy to grasp. It also discussed the links between MG loss and contrastive loss/invert optimal transport loss, showing its unique properties as a set-based loss with single-level optimization.\n\n3. MG loss exhibited superior performance to contrastive loss in weak augmentation and low training epochs regime.\n\nWeaknesses: 1. The advantages of the single-level optimization in MG loss over the bi-level optimization in IOT loss are not provided clearly. Figure 2 shows that MG loss slightly underperforms but is competitive with IOT loss. I wonder if it improves the training speed/convergence or reduces the memory consumption?\n\n2. Unfair comparisons. The implementation of the experiments largely followed the setting of Dino, which used two global crops and ten local crops by default. However, some of the baseline methods, e.g., MoCov3 and I-JEPA, used only two global crops, making it unfair to directly compare the performance with MG loss on the default setting.\n\n3. Even under the potentially unfair comparison, the performance of MG loss is only comparable and sometimes even inferior to the contrastive loss.\n\n4. Some notations are used without first introduced, e.g., $c(\\cdot,\\cdot)$ in Introduction and $t(\\cdot,\\cdot)$ in Sec. 3.\n\nQuestions: See the weaknesses.\n\nOverall, I think the proposed loss is interesting, and I like the presentation of this paper. However, the evaluation part still has significant room for improvement.", "labeling_timestamp": "2026-01-11T17:06:27.520927", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper reports results with reduced augmentations (e.g., 'only cropping and flipping') and compares to baselines, but the provided text does not describe experiments that control or equalize crop-augmentation settings between the matching-gap method and baseline methods.", "evidence": "\"we show that we can learn competitive features, even without extensive data augmentations: Using only cropping and flipping, we achieve 74.2% top-1 accuracy with a ViT-B/16 on ImageNet-1k, to be compared to 72.9% for I-JEPA (Assran et al., 2023).\"", "section": "Abstract / Our Contributions"}
{"claim": "Notations like c(·,·) in the Introduction and t(·,·) in Section 3 are used without first being defined.", "claim_type": "methodology", "paper_id": "P50qJuu4IY", "paper_title": "Self-Supervised Learning with the Matching Gap", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "W9ybmgKZdT", "reviewer": "Reviewer_tJTN", "review_text": "Summary: This paper proposed an alternative loss, Matching Gap (MG), to contrastive loss for self-supervised representation learning. Unlike contrastive loss enforcing the sample-wise invariance to data perturbations, the MG loss is a set-based loss, driven by minimizing the difference between the ground-truth transport loss and the optimal transport loss computed in the representation space using the Sinkhorn algorithm. The authors detailedly discussed the differences and connections of MG loss to contrastive loss and prior optimal transport, showing the unique properties of the proposed method. Finally, experiments on ImageNet-1k dataset suggested a comparable performance of MG to prior arts.\n\nStrengths: 1. The paper is overall well-motivated. The reliance on data augmentation is one of the most prominent nuisances of contrastive learning. It is good to see more exploration toward bypassing this issue.\n\n2. The theoretical analysis presented MG loss in a straightforward way and is overall easy to grasp. It also discussed the links between MG loss and contrastive loss/invert optimal transport loss, showing its unique properties as a set-based loss with single-level optimization.\n\n3. MG loss exhibited superior performance to contrastive loss in weak augmentation and low training epochs regime.\n\nWeaknesses: 1. The advantages of the single-level optimization in MG loss over the bi-level optimization in IOT loss are not provided clearly. Figure 2 shows that MG loss slightly underperforms but is competitive with IOT loss. I wonder if it improves the training speed/convergence or reduces the memory consumption?\n\n2. Unfair comparisons. The implementation of the experiments largely followed the setting of Dino, which used two global crops and ten local crops by default. However, some of the baseline methods, e.g., MoCov3 and I-JEPA, used only two global crops, making it unfair to directly compare the performance with MG loss on the default setting.\n\n3. Even under the potentially unfair comparison, the performance of MG loss is only comparable and sometimes even inferior to the contrastive loss.\n\n4. Some notations are used without first introduced, e.g., $c(\\cdot,\\cdot)$ in Introduction and $t(\\cdot,\\cdot)$ in Sec. 3.\n\nQuestions: See the weaknesses.\n\nOverall, I think the proposed loss is interesting, and I like the presentation of this paper. However, the evaluation part still has significant room for improvement.", "labeling_timestamp": "2026-01-11T17:06:31.492926", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly defines the cost function c(·,·) (in the Contributions/Introduction and again in Section 2.2). The provided Section 3 excerpt does not use or introduce a t(·,·) notation, so the reviewer's claim that both c and t are used without definition is incorrect given the paper text available.", "evidence": [{"quote": "To promote invariance, we expect that a mean cost between the n coupled representations, 1 n ∑ n i =1 c ( x i , y i ) , using a cost function c : R d × R d → R , remains small.", "section": "Introduction (Contributions)"}, {"quote": "Let ( x 1 , . . . , x n ) and ( y 1 , . . . , y n ) be two families of vectors in R d and c : R d × R d → R a cost function between vectors.", "section": "Section 2.2"}, {"quote": "Computing optimal matchings, as presented in § 2.2, is inherently an unsupervised task: the goal is to recover a good pairing of points (3) from two arbitrary point clouds ( x 1 , . . . , x n ) and ( y 1 , . . . , y n ) , given in no specific order. Instead, in the SSL setting, data comes in the form of pairs ( x 1 , y 1 ) , . . . , ( x n , y n ) , that bind two different views of the same input. This is a supervised task, since the ground-truth pairing is known as the rescaled identity, J n := 1 n I n ∈ B n .", "section": "Section 3 (excerpt)"}], "section": ""}
{"claim": "The evaluation reports only classification accuracy and omits uncertainty measures such as calibration and likelihood-based metrics across datasets.", "claim_type": "experimental", "paper_id": "wJaCsnT9UE", "paper_title": "Sharpness-diversity tradeoff: improving flat ensembles with SharpBalance", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "haBGktuqCH", "reviewer": "Reviewer_8MB6", "review_text": "Summary: Ensemble methods and sharpness-aware optimization techniques are well-known strategies for improving generalization. This work identifies a trade-off between sharpness and diversity, observing that reducing sharpness can diminish diversity and harm ensemble performance. Through theoretical and empirical analysis of this sharpness-diversity trade-off, the authors present SharpBalance, an algorithm for training ensembles with sharpness-aware solutions without sacrificing diversity. Evaluation results on CIFAR-10/100, TinyImageNet, and their corrupted variants confirm the effectiveness of SharpBalance.\n\nStrengths: - Ensemble methods and sharpness-aware optimization techniques are both prominent approaches for improving generalization. The aim of this work, which combines these two approaches, is well-motivated.\n- While the theoretical analysis uses the variance metric to indicate diversity, the experimental results show consistent trends across different diversity metrics. It suggests that the proposed analysis is widely applicable to the general concept of diversity.\n- Extensive empirical results effectively validate the theoretical analysis. The summary plots of the results are generally highly readable.\n\nWeaknesses: - The evaluation results are centered exclusively on classification accuracy; since ensembling usually highlights both predictive accuracy and uncertainty, relying solely on accuracy to assess overall performance is insufficient. \n- Specifically, for the corrupted CIFAR benchmark, uncertainty metrics like negative log-likelihood or expected calibration error are more important than test accuracy, but these aspects are not currently considered.\n- It seems that all experiments were conducted exclusively with residual networks. It is essential to verify if the proposed analysis and algorithm are applicable to other architecture families as well.\n\nQuestions: - It appears that the current emphasis is on logit-ensemble (lines 82-83). Does the same rationale apply when ensembling categorical probabilities (i.e., probability-ensemble)?\n- In the proposed SharpBalance algorithm, it seems that the training data and objective for the i-th member are defined using other members (such as members i+1, i+2, as illustrated in the figure). Does this imply that in practice, each member is trained sequentially?", "labeling_timestamp": "2026-01-11T17:06:44.285918", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The reviewer is correct that the paper does not mention uncertainty metrics such as calibration (e.g., ECE) or likelihood-based evaluation (e.g., NLL) in the provided content. However, the claim that the evaluation reports \"only classification accuracy\" is incorrect: the paper explicitly reports and evaluates other metrics (sharpness, diversity measures including variance/DER/KL, and an ensemble improvement rate) beyond raw accuracy.", "evidence": "“In our experiments, diversity is measured using variance defined above, as well as two other widely used metrics in ensemble learning, namely Disagreement Error Ratio (DER) ... and KL divergence ...”\n\n“We characterize the effectiveness of ensembling by the metric called ensemble improvement rate (EIR) [Theisen et al., 2023], which is defined as the ensembling improvement over the average performance of single models.”\n\n“Empirically, we measure the sharpness of the NN via the adaptive worst-case sharpness [Kwon et al., 2021, Andriushchenko et al., 2023].”\n\n“Here ℓ(·) is a loss function, which, for instance, can be the cross entropy loss or ℓ2 loss.”", "section": "Section 2 (Background) and Abstract"}
{"claim": "Relying solely on accuracy is insufficient to assess ensemble performance because ensembles also provide uncertainty estimates and calibration that are not evaluated.", "claim_type": "methodology", "paper_id": "wJaCsnT9UE", "paper_title": "Sharpness-diversity tradeoff: improving flat ensembles with SharpBalance", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "haBGktuqCH", "reviewer": "Reviewer_8MB6", "review_text": "Summary: Ensemble methods and sharpness-aware optimization techniques are well-known strategies for improving generalization. This work identifies a trade-off between sharpness and diversity, observing that reducing sharpness can diminish diversity and harm ensemble performance. Through theoretical and empirical analysis of this sharpness-diversity trade-off, the authors present SharpBalance, an algorithm for training ensembles with sharpness-aware solutions without sacrificing diversity. Evaluation results on CIFAR-10/100, TinyImageNet, and their corrupted variants confirm the effectiveness of SharpBalance.\n\nStrengths: - Ensemble methods and sharpness-aware optimization techniques are both prominent approaches for improving generalization. The aim of this work, which combines these two approaches, is well-motivated.\n- While the theoretical analysis uses the variance metric to indicate diversity, the experimental results show consistent trends across different diversity metrics. It suggests that the proposed analysis is widely applicable to the general concept of diversity.\n- Extensive empirical results effectively validate the theoretical analysis. The summary plots of the results are generally highly readable.\n\nWeaknesses: - The evaluation results are centered exclusively on classification accuracy; since ensembling usually highlights both predictive accuracy and uncertainty, relying solely on accuracy to assess overall performance is insufficient. \n- Specifically, for the corrupted CIFAR benchmark, uncertainty metrics like negative log-likelihood or expected calibration error are more important than test accuracy, but these aspects are not currently considered.\n- It seems that all experiments were conducted exclusively with residual networks. It is essential to verify if the proposed analysis and algorithm are applicable to other architecture families as well.\n\nQuestions: - It appears that the current emphasis is on logit-ensemble (lines 82-83). Does the same rationale apply when ensembling categorical probabilities (i.e., probability-ensemble)?\n- In the proposed SharpBalance algorithm, it seems that the training data and objective for the i-th member are defined using other members (such as members i+1, i+2, as illustrated in the figure). Does this imply that in practice, each member is trained sequentially?", "labeling_timestamp": "2026-01-11T17:06:30.409439", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper evaluates ensemble effectiveness using accuracy-based metrics (ensemble test error / Ensemble Improvement Rate) and focuses on sharpness and diversity measures, but it does not report uncertainty or calibration metrics (e.g., NLL, ECE) in the provided content. Thus the reviewer's point that accuracy alone is insufficient and uncertainty/calibration were not evaluated is partly correct: the paper goes beyond raw accuracy by studying sharpness/diversity, but it does not appear to evaluate calibration or other uncertainty estimates.", "evidence": "1) \"We characterize the effectiveness of ensembling by the metric called ensemble improvement rate (EIR) ... Let E_ens denote the test error of an ensemble; the EIR is then defined as follows:\" 2) \"In our experiments, diversity is measured using variance defined above, as well as two other widely used metrics in ensemble learning, namely Disagreement Error Ratio (DER) ... and KL divergence ...\"", "section": "2 Background (Ensembling; Diversity metrics)"}
{"claim": "For the corrupted CIFAR benchmark, important uncertainty metrics like negative log-likelihood and expected calibration error are not reported.", "claim_type": "experimental", "paper_id": "wJaCsnT9UE", "paper_title": "Sharpness-diversity tradeoff: improving flat ensembles with SharpBalance", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "haBGktuqCH", "reviewer": "Reviewer_8MB6", "review_text": "Summary: Ensemble methods and sharpness-aware optimization techniques are well-known strategies for improving generalization. This work identifies a trade-off between sharpness and diversity, observing that reducing sharpness can diminish diversity and harm ensemble performance. Through theoretical and empirical analysis of this sharpness-diversity trade-off, the authors present SharpBalance, an algorithm for training ensembles with sharpness-aware solutions without sacrificing diversity. Evaluation results on CIFAR-10/100, TinyImageNet, and their corrupted variants confirm the effectiveness of SharpBalance.\n\nStrengths: - Ensemble methods and sharpness-aware optimization techniques are both prominent approaches for improving generalization. The aim of this work, which combines these two approaches, is well-motivated.\n- While the theoretical analysis uses the variance metric to indicate diversity, the experimental results show consistent trends across different diversity metrics. It suggests that the proposed analysis is widely applicable to the general concept of diversity.\n- Extensive empirical results effectively validate the theoretical analysis. The summary plots of the results are generally highly readable.\n\nWeaknesses: - The evaluation results are centered exclusively on classification accuracy; since ensembling usually highlights both predictive accuracy and uncertainty, relying solely on accuracy to assess overall performance is insufficient. \n- Specifically, for the corrupted CIFAR benchmark, uncertainty metrics like negative log-likelihood or expected calibration error are more important than test accuracy, but these aspects are not currently considered.\n- It seems that all experiments were conducted exclusively with residual networks. It is essential to verify if the proposed analysis and algorithm are applicable to other architecture families as well.\n\nQuestions: - It appears that the current emphasis is on logit-ensemble (lines 82-83). Does the same rationale apply when ensembling categorical probabilities (i.e., probability-ensemble)?\n- In the proposed SharpBalance algorithm, it seems that the training data and objective for the i-th member are defined using other members (such as members i+1, i+2, as illustrated in the figure). Does this imply that in practice, each member is trained sequentially?", "labeling_timestamp": "2026-01-11T17:06:35.022405", "model": "gpt-5-mini", "label": "Supported", "justification": "In the provided paper content, the reported experimental metrics include diversity measures (variance, DER, KL divergence), sharpness, and ensemble improvement; there is no mention of negative log-likelihood (NLL) or expected calibration error (ECE) for the corrupted CIFAR evaluations. Therefore the claim that those uncertainty metrics are not reported is supported by the paper content supplied.", "evidence": "1) \"In our experiments, diversity is measured using variance defined above, as well as two other widely used metrics in ensemble learning, namely Disagreement Error Ratio (DER) ... and KL divergence ...\" (Section 2).\n2) \"Empirically, we confirm this improvement and demonstrate that SharpBalance enhances overall ensemble performance, outperforming baseline methods in CIFAR-10, CIFAR-100, TinyImageNet, and their corrupted versions to assess OOD performance.\" (Introduction/Contributions) — no NLL or ECE are listed among reported metrics.", "section": "Section 2 (Background) and Introduction"}
{"claim": "All experiments appear to be conducted only with residual network architectures, so applicability to other architecture families is not demonstrated.", "claim_type": "experimental", "paper_id": "wJaCsnT9UE", "paper_title": "Sharpness-diversity tradeoff: improving flat ensembles with SharpBalance", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "haBGktuqCH", "reviewer": "Reviewer_8MB6", "review_text": "Summary: Ensemble methods and sharpness-aware optimization techniques are well-known strategies for improving generalization. This work identifies a trade-off between sharpness and diversity, observing that reducing sharpness can diminish diversity and harm ensemble performance. Through theoretical and empirical analysis of this sharpness-diversity trade-off, the authors present SharpBalance, an algorithm for training ensembles with sharpness-aware solutions without sacrificing diversity. Evaluation results on CIFAR-10/100, TinyImageNet, and their corrupted variants confirm the effectiveness of SharpBalance.\n\nStrengths: - Ensemble methods and sharpness-aware optimization techniques are both prominent approaches for improving generalization. The aim of this work, which combines these two approaches, is well-motivated.\n- While the theoretical analysis uses the variance metric to indicate diversity, the experimental results show consistent trends across different diversity metrics. It suggests that the proposed analysis is widely applicable to the general concept of diversity.\n- Extensive empirical results effectively validate the theoretical analysis. The summary plots of the results are generally highly readable.\n\nWeaknesses: - The evaluation results are centered exclusively on classification accuracy; since ensembling usually highlights both predictive accuracy and uncertainty, relying solely on accuracy to assess overall performance is insufficient. \n- Specifically, for the corrupted CIFAR benchmark, uncertainty metrics like negative log-likelihood or expected calibration error are more important than test accuracy, but these aspects are not currently considered.\n- It seems that all experiments were conducted exclusively with residual networks. It is essential to verify if the proposed analysis and algorithm are applicable to other architecture families as well.\n\nQuestions: - It appears that the current emphasis is on logit-ensemble (lines 82-83). Does the same rationale apply when ensembling categorical probabilities (i.e., probability-ensemble)?\n- In the proposed SharpBalance algorithm, it seems that the training data and objective for the i-th member are defined using other members (such as members i+1, i+2, as illustrated in the figure). Does this imply that in practice, each member is trained sequentially?", "labeling_timestamp": "2026-01-11T17:06:54.153548", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper's experiments are described using ResNet18 (a residual network) and no other architecture families are mentioned in the provided content, so applicability to non-residual architectures is not demonstrated.", "evidence": "\"Each marker represents a three-member ResNet18 ensemble trained on CIFAR-10.\"\"we verify that SharpBalance improves the sharpness-diversity tradeoff in training the ResNet18 ensemble on CIFAR10.\"", "section": "Figure 1 caption and Section 1 (Introduction)"}
{"claim": "The paper emphasizes logit-ensemble; it is unclear whether the same rationale and results apply when ensembling categorical probability outputs (probability-ensemble).", "claim_type": "methodology", "paper_id": "wJaCsnT9UE", "paper_title": "Sharpness-diversity tradeoff: improving flat ensembles with SharpBalance", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "haBGktuqCH", "reviewer": "Reviewer_8MB6", "review_text": "Summary: Ensemble methods and sharpness-aware optimization techniques are well-known strategies for improving generalization. This work identifies a trade-off between sharpness and diversity, observing that reducing sharpness can diminish diversity and harm ensemble performance. Through theoretical and empirical analysis of this sharpness-diversity trade-off, the authors present SharpBalance, an algorithm for training ensembles with sharpness-aware solutions without sacrificing diversity. Evaluation results on CIFAR-10/100, TinyImageNet, and their corrupted variants confirm the effectiveness of SharpBalance.\n\nStrengths: - Ensemble methods and sharpness-aware optimization techniques are both prominent approaches for improving generalization. The aim of this work, which combines these two approaches, is well-motivated.\n- While the theoretical analysis uses the variance metric to indicate diversity, the experimental results show consistent trends across different diversity metrics. It suggests that the proposed analysis is widely applicable to the general concept of diversity.\n- Extensive empirical results effectively validate the theoretical analysis. The summary plots of the results are generally highly readable.\n\nWeaknesses: - The evaluation results are centered exclusively on classification accuracy; since ensembling usually highlights both predictive accuracy and uncertainty, relying solely on accuracy to assess overall performance is insufficient. \n- Specifically, for the corrupted CIFAR benchmark, uncertainty metrics like negative log-likelihood or expected calibration error are more important than test accuracy, but these aspects are not currently considered.\n- It seems that all experiments were conducted exclusively with residual networks. It is essential to verify if the proposed analysis and algorithm are applicable to other architecture families as well.\n\nQuestions: - It appears that the current emphasis is on logit-ensemble (lines 82-83). Does the same rationale apply when ensembling categorical probabilities (i.e., probability-ensemble)?\n- In the proposed SharpBalance algorithm, it seems that the training data and objective for the i-th member are defined using other members (such as members i+1, i+2, as illustrated in the figure). Does this imply that in practice, each member is trained sequentially?", "labeling_timestamp": "2026-01-11T17:06:56.917541", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly states that for classification they average predicted logits to form the ensemble (Section 2) and does not discuss ensembling probabilities or analyze whether its theoretical/empirical results extend to probability-ensembles, so it is indeed unclear from the paper whether the same rationale/results apply to probability averaging.", "evidence": "\"For classification tasks, the ensemble's output is derived by averaging the predicted logits of these individual networks.\"", "section": "Section 2 (Background)"}
{"claim": "The SharpBalance algorithm description implies each member's training depends on others' outputs; it is unclear whether members are trained sequentially or jointly in practice.", "claim_type": "methodology", "paper_id": "wJaCsnT9UE", "paper_title": "Sharpness-diversity tradeoff: improving flat ensembles with SharpBalance", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "haBGktuqCH", "reviewer": "Reviewer_8MB6", "review_text": "Summary: Ensemble methods and sharpness-aware optimization techniques are well-known strategies for improving generalization. This work identifies a trade-off between sharpness and diversity, observing that reducing sharpness can diminish diversity and harm ensemble performance. Through theoretical and empirical analysis of this sharpness-diversity trade-off, the authors present SharpBalance, an algorithm for training ensembles with sharpness-aware solutions without sacrificing diversity. Evaluation results on CIFAR-10/100, TinyImageNet, and their corrupted variants confirm the effectiveness of SharpBalance.\n\nStrengths: - Ensemble methods and sharpness-aware optimization techniques are both prominent approaches for improving generalization. The aim of this work, which combines these two approaches, is well-motivated.\n- While the theoretical analysis uses the variance metric to indicate diversity, the experimental results show consistent trends across different diversity metrics. It suggests that the proposed analysis is widely applicable to the general concept of diversity.\n- Extensive empirical results effectively validate the theoretical analysis. The summary plots of the results are generally highly readable.\n\nWeaknesses: - The evaluation results are centered exclusively on classification accuracy; since ensembling usually highlights both predictive accuracy and uncertainty, relying solely on accuracy to assess overall performance is insufficient. \n- Specifically, for the corrupted CIFAR benchmark, uncertainty metrics like negative log-likelihood or expected calibration error are more important than test accuracy, but these aspects are not currently considered.\n- It seems that all experiments were conducted exclusively with residual networks. It is essential to verify if the proposed analysis and algorithm are applicable to other architecture families as well.\n\nQuestions: - It appears that the current emphasis is on logit-ensemble (lines 82-83). Does the same rationale apply when ensembling categorical probabilities (i.e., probability-ensemble)?\n- In the proposed SharpBalance algorithm, it seems that the training data and objective for the i-th member are defined using other members (such as members i+1, i+2, as illustrated in the figure). Does this imply that in practice, each member is trained sequentially?", "labeling_timestamp": "2026-01-11T17:07:07.170293", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper does not state that members' training depends on others' outputs; instead it says each member optimizes sharpness on different data subsets (contradicting the claimed dependence). However, the paper also does not specify whether ensemble members are trained sequentially or jointly, so the reviewer's uncertainty about training order is accurate.", "evidence": "“This method is designed based on our theoretical results, which suggest that training different ensemble members using a loss function that aims to reduce sharpness on different subsets of the training data can improve the trade-off between sharpness and diversity.”\n\n“Aligned with theoretical insights, our SharpBalance method lets each ensemble member minimize the sharpness objective exclusively on a subset of training data, termed the sharpness-aware set. The sharpness-aware set of each ensemble member is diversified by an adaptive strategy based on data-dependent sharpness measures.”", "section": "1 Introduction"}
{"claim": "The free energy must be written in terms of importance weights targeting the generative joint density.", "claim_type": "methodology", "paper_id": "dxwIaCVkWU", "paper_title": "Divide-and-Conquer Predictive Coding: a structured Bayesian inference algorithm", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "x5hzHkvqAb", "reviewer": "Reviewer_DHBv", "review_text": "Comment: The authors have filled in the missing results, and distinguished their contribution from other papers, so I have raised my score.\n\n> It is indeed mathematically necessary to write the free energy in terms of importance weights targeting the generative joint density. By definition, a variational free energy is the expected value of the negative logarithm of an importance weight. Equivalently, the VFE is the cross-entropy of the generative joint distribution, taken with respect to the proposal distribution (recognition model), minus the entropy of the recognition model. The product of normalized complete conditionals (with the normalizers estimated by importance sampling) would only estimate the entropy of the recognition model, the second term.\n\nI was not proposing to drop either term from the free energy, but rather to rewrite it without any reference to \"weights\" or \"strictly properly weighting.\"  That is, precisely to rewrite it as \"the cross-entropy of the generative joint distribution, taken with respect to the proposal distribution (recognition model), minus the entropy of the recognition model,\" where the recognition model is defined to be the product over the normalized \"complete conditionals\" (with the normalizers computed with Langevin dynamics).  This would (in this reviewer's opinion) substantially simplify the presentation.  But perhaps there is some obstacle to writing it this way that I am overlooking.", "labeling_timestamp": "2026-01-11T17:07:13.008474", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper formulates the variational free energy in a particle/importance-weight framework and explicitly describes importance weights targeting an unnormalized target density (the generative joint) in Section 2, and then uses importance weights for complete conditionals and to assemble the joint via a divide-and-conquer SMC recursion in Section 3. However, the paper does not state as an absolute necessity that the free energy must be written only in terms of importance weights targeting the generative joint density; it additionally develops importance-weighting for complete conditionals and a recursive SMC construction to target the joint.", "evidence": "1) \"The ratio of densities in Equation 2 is an example of a weight used to approximate a distribution known only up to its normalizing constant. The proposal distribution q(z) admits tractable sampling, while the unnormalized target density γ_θ(z) admits tractable, pointwise density evaluation.\"  2) \"Particle Algorithms ... Any proposal meeting these conditions ... defines a free energy functional, analogous to Equation 2 in upper-bounding the model surprisal:\"  3) \"Since the proposal from which we can sample by predictive coding is not the optimal coordinate update, we importance weight for the true complete conditional distribution that is optimal ... resampling with respect to these weights corrects for discretization error, yields particles distributed according to the true complete conditional, and estimates the complete conditional's normalizer ... The recursive step of 'Divide and Conquer' Sequential Monte Carlo ... exploits the estimates ˆZ_θ ... to weigh the samples for the complete target density.\"", "section": "Section 2 (Background) and Section 3 (Divide-and-Conquer Predictive Coding)"}
{"claim": "By definition, a variational free energy equals the expected negative logarithm of an importance weight.", "claim_type": "methodology", "paper_id": "dxwIaCVkWU", "paper_title": "Divide-and-Conquer Predictive Coding: a structured Bayesian inference algorithm", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "x5hzHkvqAb", "reviewer": "Reviewer_DHBv", "review_text": "Comment: The authors have filled in the missing results, and distinguished their contribution from other papers, so I have raised my score.\n\n> It is indeed mathematically necessary to write the free energy in terms of importance weights targeting the generative joint density. By definition, a variational free energy is the expected value of the negative logarithm of an importance weight. Equivalently, the VFE is the cross-entropy of the generative joint distribution, taken with respect to the proposal distribution (recognition model), minus the entropy of the recognition model. The product of normalized complete conditionals (with the normalizers estimated by importance sampling) would only estimate the entropy of the recognition model, the second term.\n\nI was not proposing to drop either term from the free energy, but rather to rewrite it without any reference to \"weights\" or \"strictly properly weighting.\"  That is, precisely to rewrite it as \"the cross-entropy of the generative joint distribution, taken with respect to the proposal distribution (recognition model), minus the entropy of the recognition model,\" where the recognition model is defined to be the product over the normalized \"complete conditionals\" (with the normalizers computed with Langevin dynamics).  This would (in this reviewer's opinion) substantially simplify the presentation.  But perhaps there is some obstacle to writing it this way that I am overlooking.", "labeling_timestamp": "2026-01-11T17:07:20.878627", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper defines the variational free energy in the empirical Bayes context (Section 2) and explicitly notes that the ratio of unnormalized target to proposal densities (an importance weight) appears in Equation 2; the free energy is presented in terms of this ratio, i.e. the expectation under q of log q minus log γ, which is equivalent to the expected negative log importance weight.", "evidence": "“Neal and Hinton [1998] reduced empirical Bayes to minimization of the variational free energy :”\n\n“The ratio of densities in Equation 2 is an example of a weight used to approximate a distribution known only up to its normalizing constant. The proposal distribution q ( z ) admits tractable sampling, while the unnormalized target density γ θ ( z ) admits tractable, pointwise density evaluation.”", "section": "2 Background"}
{"claim": "The variational free energy equals the cross-entropy of the generative joint under the proposal distribution minus the recognition model's entropy.", "claim_type": "methodology", "paper_id": "dxwIaCVkWU", "paper_title": "Divide-and-Conquer Predictive Coding: a structured Bayesian inference algorithm", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "x5hzHkvqAb", "reviewer": "Reviewer_DHBv", "review_text": "Comment: The authors have filled in the missing results, and distinguished their contribution from other papers, so I have raised my score.\n\n> It is indeed mathematically necessary to write the free energy in terms of importance weights targeting the generative joint density. By definition, a variational free energy is the expected value of the negative logarithm of an importance weight. Equivalently, the VFE is the cross-entropy of the generative joint distribution, taken with respect to the proposal distribution (recognition model), minus the entropy of the recognition model. The product of normalized complete conditionals (with the normalizers estimated by importance sampling) would only estimate the entropy of the recognition model, the second term.\n\nI was not proposing to drop either term from the free energy, but rather to rewrite it without any reference to \"weights\" or \"strictly properly weighting.\"  That is, precisely to rewrite it as \"the cross-entropy of the generative joint distribution, taken with respect to the proposal distribution (recognition model), minus the entropy of the recognition model,\" where the recognition model is defined to be the product over the normalized \"complete conditionals\" (with the normalizers computed with Langevin dynamics).  This would (in this reviewer's opinion) substantially simplify the presentation.  But perhaps there is some obstacle to writing it this way that I am overlooking.", "labeling_timestamp": "2026-01-11T17:07:48.846598", "model": "gpt-5-mini", "label": "Supported", "justification": "In Section 2 the paper defines the variational free energy as the objective minimized in empirical Bayes and frames it as a free-energy functional comparing the proposal q and the (unnormalized) joint γ_θ. This corresponds to F(q) = E_q[log q(z) - log p_θ(x,z)], which is equivalent to the cross-entropy term (−E_q[log p_θ(x,z)]) minus the recognition-model entropy (−E_q[log q(z)]).", "evidence": "Neal and Hinton [1998] reduced empirical Bayes to minimization of the variational free energy :\n\nAny proposal meeting these conditions (see Proposition 1 in Appendix B and Naesseth et al. [2015], Stites et al. [2021]) defines a free energy functional, analogous to Equation 2 in upper-bounding the model surprisal:", "section": "2 Background"}
{"claim": "Using the product of normalized complete conditionals with normalizers estimated by importance sampling only estimates the recognition model's entropy, not the full variational free energy.", "claim_type": "methodology", "paper_id": "dxwIaCVkWU", "paper_title": "Divide-and-Conquer Predictive Coding: a structured Bayesian inference algorithm", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "x5hzHkvqAb", "reviewer": "Reviewer_DHBv", "review_text": "Comment: The authors have filled in the missing results, and distinguished their contribution from other papers, so I have raised my score.\n\n> It is indeed mathematically necessary to write the free energy in terms of importance weights targeting the generative joint density. By definition, a variational free energy is the expected value of the negative logarithm of an importance weight. Equivalently, the VFE is the cross-entropy of the generative joint distribution, taken with respect to the proposal distribution (recognition model), minus the entropy of the recognition model. The product of normalized complete conditionals (with the normalizers estimated by importance sampling) would only estimate the entropy of the recognition model, the second term.\n\nI was not proposing to drop either term from the free energy, but rather to rewrite it without any reference to \"weights\" or \"strictly properly weighting.\"  That is, precisely to rewrite it as \"the cross-entropy of the generative joint distribution, taken with respect to the proposal distribution (recognition model), minus the entropy of the recognition model,\" where the recognition model is defined to be the product over the normalized \"complete conditionals\" (with the normalizers computed with Langevin dynamics).  This would (in this reviewer's opinion) substantially simplify the presentation.  But perhaps there is some obstacle to writing it this way that I am overlooking.", "labeling_timestamp": "2026-01-11T17:07:51.372627", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper describes using importance weights and estimated normalizers for complete conditionals to correct samples and to weigh particles for the joint target (Section 3), and frames the method as minimizing the full variational free energy (Section 2). It does not state that using the product of normalized complete conditionals with importance-sampled normalizers yields only an estimate of the recognition-model entropy, nor does it provide explicit derivations or statements limiting the estimates to the entropy term. Therefore the claim is not verifiable from the provided text.", "evidence": "From Section 3: \"The optimal proposal q* for each random variable would equal, if it had closed form, the complete conditional density for that variable, containing all information from other random variables\"; \"Since the proposal from which we can sample by predictive coding is not the optimal coordinate update, we importance weight for the true complete conditional distribution that is optimal\"; \"resampling with respect to these weights corrects for discretization error, yields particles distributed according to the true complete conditional, and estimates the complete conditional's normalizer\"; \"The recursive step of 'Divide and Conquer' Sequential Monte Carlo ... exploits the estimates ˆZ_θ^{t-1}(z\\z)^{t} to weigh the samples for the complete target density.\" From Section 2: \"Neal and Hinton [1998] reduced empirical Bayes to minimization of the variational free energy:\" and \"Any proposal meeting these conditions ... defines a free energy functional, analogous to Equation 2 in upper-bounding the model surprisal.\"", "section": "Section 3 (Divide-and-Conquer Predictive Coding) and Section 2 (Background)"}
{"claim": "The paper's use of terms like 'weights' and 'strictly properly weighting' unnecessarily complicates the presentation and could be simplified.", "claim_type": "presentation", "paper_id": "dxwIaCVkWU", "paper_title": "Divide-and-Conquer Predictive Coding: a structured Bayesian inference algorithm", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "x5hzHkvqAb", "reviewer": "Reviewer_DHBv", "review_text": "Comment: The authors have filled in the missing results, and distinguished their contribution from other papers, so I have raised my score.\n\n> It is indeed mathematically necessary to write the free energy in terms of importance weights targeting the generative joint density. By definition, a variational free energy is the expected value of the negative logarithm of an importance weight. Equivalently, the VFE is the cross-entropy of the generative joint distribution, taken with respect to the proposal distribution (recognition model), minus the entropy of the recognition model. The product of normalized complete conditionals (with the normalizers estimated by importance sampling) would only estimate the entropy of the recognition model, the second term.\n\nI was not proposing to drop either term from the free energy, but rather to rewrite it without any reference to \"weights\" or \"strictly properly weighting.\"  That is, precisely to rewrite it as \"the cross-entropy of the generative joint distribution, taken with respect to the proposal distribution (recognition model), minus the entropy of the recognition model,\" where the recognition model is defined to be the product over the normalized \"complete conditionals\" (with the normalizers computed with Langevin dynamics).  This would (in this reviewer's opinion) substantially simplify the presentation.  But perhaps there is some obstacle to writing it this way that I am overlooking.", "labeling_timestamp": "2026-01-11T17:07:44.303385", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper clearly uses terminology around 'weights' and importance-weighted particles (Sections 2 and 3), so the reviewer is correct that such terms appear. However, the phrase 'strictly properly weighting' does not appear in the provided text, and the claim that this terminology 'unnecessarily complicates the presentation and could be simplified' is a subjective judgment not supported or argued for within the paper itself.", "evidence": "1) \"The ratio of densities in Equation 2 is an example of a weight used to approximate a distribution known only up to its normalizing constant.\" 2) \"particle algorithms approach empirical Bayes problems by setting the proposal to a collection of weighted particles ( w_k , z_k )\" 3) \"we importance weight for the true complete conditional distribution that is optimal\" 4) \"resampling with respect to these weights corrects for discretization error, yields particles distributed according to the true complete conditional, and estimates the complete conditional's normalizer\"", "section": "Background (Section 2) and Divide-and-Conquer Predictive Coding (Section 3)"}
{"claim": "The authors did not present a rewritten free energy as cross-entropy minus entropy with the recognition model defined by normalized complete conditionals computed via Langevin dynamics.", "claim_type": "methodology", "paper_id": "dxwIaCVkWU", "paper_title": "Divide-and-Conquer Predictive Coding: a structured Bayesian inference algorithm", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "x5hzHkvqAb", "reviewer": "Reviewer_DHBv", "review_text": "Comment: The authors have filled in the missing results, and distinguished their contribution from other papers, so I have raised my score.\n\n> It is indeed mathematically necessary to write the free energy in terms of importance weights targeting the generative joint density. By definition, a variational free energy is the expected value of the negative logarithm of an importance weight. Equivalently, the VFE is the cross-entropy of the generative joint distribution, taken with respect to the proposal distribution (recognition model), minus the entropy of the recognition model. The product of normalized complete conditionals (with the normalizers estimated by importance sampling) would only estimate the entropy of the recognition model, the second term.\n\nI was not proposing to drop either term from the free energy, but rather to rewrite it without any reference to \"weights\" or \"strictly properly weighting.\"  That is, precisely to rewrite it as \"the cross-entropy of the generative joint distribution, taken with respect to the proposal distribution (recognition model), minus the entropy of the recognition model,\" where the recognition model is defined to be the product over the normalized \"complete conditionals\" (with the normalizers computed with Langevin dynamics).  This would (in this reviewer's opinion) substantially simplify the presentation.  But perhaps there is some obstacle to writing it this way that I am overlooking.", "labeling_timestamp": "2026-01-11T17:08:12.568310", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly defines the variational free energy (Equation 2), states that the optimal proposal for each variable is the complete conditional (Equation 5), and describes using unadjusted Langevin proposals parameterized by prediction errors to target those complete conditionals—i.e. the paper does present a free-energy-based formulation together with recognition/proposal updates that target normalized complete conditionals via Langevin dynamics.", "evidence": "“Neal and Hinton [1998] reduced empirical Bayes to minimization of the variational free energy :” (Section 2)\n\n“The optimal proposal q * for each random variable would equal, if it had closed form, the complete conditional density for that variable, containing all information from other random variables” (Section 3, Equation 5)\n\n“We therefore define ε_z in DCPC as the complete conditional's score function” (Section 3)\n\n“unadjusted Langevin proposals parameterized in terms of biologically plausible prediction errors.” (Abstract)", "section": "Abstract; Section 2 (Background); Section 3 (Divide-and-Conquer Predictive Coding)"}
{"claim": "It is unclear whether any obstacle prevents rewriting the free energy without reference to importance weights, indicating an unaddressed technical justification.", "claim_type": "methodology", "paper_id": "dxwIaCVkWU", "paper_title": "Divide-and-Conquer Predictive Coding: a structured Bayesian inference algorithm", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "x5hzHkvqAb", "reviewer": "Reviewer_DHBv", "review_text": "Comment: The authors have filled in the missing results, and distinguished their contribution from other papers, so I have raised my score.\n\n> It is indeed mathematically necessary to write the free energy in terms of importance weights targeting the generative joint density. By definition, a variational free energy is the expected value of the negative logarithm of an importance weight. Equivalently, the VFE is the cross-entropy of the generative joint distribution, taken with respect to the proposal distribution (recognition model), minus the entropy of the recognition model. The product of normalized complete conditionals (with the normalizers estimated by importance sampling) would only estimate the entropy of the recognition model, the second term.\n\nI was not proposing to drop either term from the free energy, but rather to rewrite it without any reference to \"weights\" or \"strictly properly weighting.\"  That is, precisely to rewrite it as \"the cross-entropy of the generative joint distribution, taken with respect to the proposal distribution (recognition model), minus the entropy of the recognition model,\" where the recognition model is defined to be the product over the normalized \"complete conditionals\" (with the normalizers computed with Langevin dynamics).  This would (in this reviewer's opinion) substantially simplify the presentation.  But perhaps there is some obstacle to writing it this way that I am overlooking.", "labeling_timestamp": "2026-01-11T17:08:02.125271", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly uses importance weights and resampling to correct proposals (Section 3) and does not provide a derivation or discussion showing how the variational free energy could be rewritten to avoid reference to importance weights. Thus it is correct that it remains unclear and technically unaddressed whether such a rewriting is possible.", "evidence": "\"Since the proposal from which we can sample by predictive coding is not the optimal coordinate update, we importance weight for the true complete conditional distribution that is optimal\"; \"The ratio of densities in Equation 2 is an example of a weight used to approximate a distribution known only up to its normalizing constant.\"", "section": "Section 3 (Divide-and-Conquer Predictive Coding) and Section 2 (Background)"}
{"claim": "The claimed unification may not be a strong contribution because GRAN can also unify one-shot and autoregressive models by setting the block size equal to the number of nodes.", "claim_type": "methodology", "paper_id": "lmYGRGyL4i", "paper_title": "Uncovering the Spectrum of Graph Generative Models: From One-Shot to Sequential", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "2VjET9ABua", "reviewer": "Reviewer_gvwJ", "review_text": "Summary: This paper presents a diffusion-based graph generative model that unifies both one-shot and autoregressive generative models. The node removal process is conducted with a denoising diffusion model and the insertion reverses the process by predicting the number of nodes that have been removed. Setting the number of nodes from 1 to n enables the unification of one-shot and autoregressive generation.\n\nStrengths: 1. This paper proposes the novel unification between one-shot and autoregressive graph generative models using diffusion models.\n2. The introduction of flexible-sized block-wise generation for graph generation stands out as a noteworthy contribution.\n\nWeaknesses: 1. Can the unification of one-shot and autoregressive graph generative models be a strong contribution? For instance, GRAN (Liao et al., 2019) can also be the unification between one-shot and autoregressive graph generative model by setting the block size as the number of nodes. What is the key difference of the work from GRAN except for the usage of diffusion models?\n2. I wonder if it is proper to say the performance as the new state-of-the-art results as mentioned in the abstract. The FCD for QM9 and NSPDK for ZINC do not seem to be state-of-the-art results. Also, as the authors adapted DiGress, the performance comparison with DiGress can be meaningful.\n3. Lack of detailed analysis on the sample quality-time trade-off. A more detailed analysis of the correlation between the sample quality and time (or memory consumption) is needed by comparing the one-shot and autoregressive versions of the IFH model.\n\nQuestions: 1. Which level of sequentiality did the authors use (I cannot find details in appx D.)? Does the degree of sequentiality imply the block size (or the number of steps)?\n2. The generic graph generation results in appx B do not look good enough. Is there any particular reason that the model works okay for molecular graphs but not for non-attributed generic graphs?", "labeling_timestamp": "2026-01-11T17:08:45.282885", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly acknowledges Liao et al. (2019, GRAN) as a precursor that investigated different fixed block sizes, which supports the reviewer's point that block-based methods can interpolate between block-sequential and one-shot behaviors. However, the paper also claims a more general diffusion-based framework (IFH) that (a) frames node removal as an absorbing diffusion process, (b) introduces learned insertion/filler/halt components, and (c) shows that one-shot models are a 1-step instance of IFH and that any degree of sequentiality can be obtained by choosing different removal processes. Thus the reviewer’s critique is only partially accurate: GRAN is related, but the paper argues its contribution is broader than simply changing block size.", "evidence": "1) \"the work by Liao et al. (2019) is a precursor of ours, as the authors investigated the use of different fixed block sizes in a domain-agnostic setup with grid graphs.\" 2) \"It can be shown that one-shot models are 1-step IFH models, and that any degree of sequentiality can be achieved by choosing different removal processes, down to 1 node at a time.\" 3) \"We use the denoising diffusion models' theory to develop a node removal process, which destroys a given graph through many steps. An insertion model reverses this process by predicting how many nodes have been removed from the intermediate subgraphs.\"", "section": "2.1 Related Works; 1 Introduction; Abstract"}
{"claim": "The paper does not explain the key difference between the proposed method and GRAN beyond the use of diffusion models.", "claim_type": "baseline", "paper_id": "lmYGRGyL4i", "paper_title": "Uncovering the Spectrum of Graph Generative Models: From One-Shot to Sequential", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "2VjET9ABua", "reviewer": "Reviewer_gvwJ", "review_text": "Summary: This paper presents a diffusion-based graph generative model that unifies both one-shot and autoregressive generative models. The node removal process is conducted with a denoising diffusion model and the insertion reverses the process by predicting the number of nodes that have been removed. Setting the number of nodes from 1 to n enables the unification of one-shot and autoregressive generation.\n\nStrengths: 1. This paper proposes the novel unification between one-shot and autoregressive graph generative models using diffusion models.\n2. The introduction of flexible-sized block-wise generation for graph generation stands out as a noteworthy contribution.\n\nWeaknesses: 1. Can the unification of one-shot and autoregressive graph generative models be a strong contribution? For instance, GRAN (Liao et al., 2019) can also be the unification between one-shot and autoregressive graph generative model by setting the block size as the number of nodes. What is the key difference of the work from GRAN except for the usage of diffusion models?\n2. I wonder if it is proper to say the performance as the new state-of-the-art results as mentioned in the abstract. The FCD for QM9 and NSPDK for ZINC do not seem to be state-of-the-art results. Also, as the authors adapted DiGress, the performance comparison with DiGress can be meaningful.\n3. Lack of detailed analysis on the sample quality-time trade-off. A more detailed analysis of the correlation between the sample quality and time (or memory consumption) is needed by comparing the one-shot and autoregressive versions of the IFH model.\n\nQuestions: 1. Which level of sequentiality did the authors use (I cannot find details in appx D.)? Does the degree of sequentiality imply the block size (or the number of steps)?\n2. The generic graph generation results in appx B do not look good enough. Is there any particular reason that the model works okay for molecular graphs but not for non-attributed generic graphs?", "labeling_timestamp": "2026-01-11T17:08:40.200091", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly describes differences beyond simply using diffusion: it introduces an Insert-Fill-Halt framework with an insertion model that predicts block sizes, a filler that can reuse one-shot models, and a halt model, and contrasts this with GRAN’s fixed block-size approach (Sections: Abstract, Introduction, 2.1, 3).", "evidence": "Abstract: \"An insertion model reverses this process by predicting how many nodes have been removed from the intermediate subgraphs. Then, generation happens by iteratively adding new blocks of nodes, with size sampled from the insertion model, and content generated using any one-shot model. By adjusting the knob on node removal, the framework allows for any degree of sequentiality, from one-shot to fully sequential, and any node ordering, e.g., random and BFS.\" Introduction: \"The Insertion Model chooses how many new nodes to generate, the Filler Model how to fill the new nodes' labels, features, and connections, and the Halt Model chooses if generation needs to terminate... one-shot models are 1-step IFH models, and that any degree of sequentiality can be achieved by choosing different removal processes, down to 1 node at a time.\" 2.1 Sequential models: \"the work by Liao et al. (2019) is a precursor of ours, as the authors investigated the use of different fixed block sizes in a domain-agnostic setup with grid graphs.\" 3 REMOVING NODES AS A GRAPH NOISE PROCESS: \"Differently from (Kong et al., 2023), we do not limit the process to the choice of one node per step, but we study the diffusion process both from the node ordering and nodes number perspectives.\"", "section": "Abstract; Introduction; 2.1 Sequential models; 3 REMOVING NODES AS A GRAPH NOISE PROCESS"}
{"claim": "The paper's claim of new state-of-the-art performance is questionable because FCD for QM9 and NSPDK for ZINC do not appear to be state-of-the-art.", "claim_type": "baseline", "paper_id": "lmYGRGyL4i", "paper_title": "Uncovering the Spectrum of Graph Generative Models: From One-Shot to Sequential", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "2VjET9ABua", "reviewer": "Reviewer_gvwJ", "review_text": "Summary: This paper presents a diffusion-based graph generative model that unifies both one-shot and autoregressive generative models. The node removal process is conducted with a denoising diffusion model and the insertion reverses the process by predicting the number of nodes that have been removed. Setting the number of nodes from 1 to n enables the unification of one-shot and autoregressive generation.\n\nStrengths: 1. This paper proposes the novel unification between one-shot and autoregressive graph generative models using diffusion models.\n2. The introduction of flexible-sized block-wise generation for graph generation stands out as a noteworthy contribution.\n\nWeaknesses: 1. Can the unification of one-shot and autoregressive graph generative models be a strong contribution? For instance, GRAN (Liao et al., 2019) can also be the unification between one-shot and autoregressive graph generative model by setting the block size as the number of nodes. What is the key difference of the work from GRAN except for the usage of diffusion models?\n2. I wonder if it is proper to say the performance as the new state-of-the-art results as mentioned in the abstract. The FCD for QM9 and NSPDK for ZINC do not seem to be state-of-the-art results. Also, as the authors adapted DiGress, the performance comparison with DiGress can be meaningful.\n3. Lack of detailed analysis on the sample quality-time trade-off. A more detailed analysis of the correlation between the sample quality and time (or memory consumption) is needed by comparing the one-shot and autoregressive versions of the IFH model.\n\nQuestions: 1. Which level of sequentiality did the authors use (I cannot find details in appx D.)? Does the degree of sequentiality imply the block size (or the number of steps)?\n2. The generic graph generation results in appx B do not look good enough. Is there any particular reason that the model works okay for molecular graphs but not for non-attributed generic graphs?", "labeling_timestamp": "2026-01-11T17:08:31.052934", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does claim new state-of-the-art results (Abstract, Introduction), but the provided excerpt contains no experimental tables, metric values, or specific results for FCD on QM9 or NSPDK on ZINC. Therefore we cannot verify the reviewer's assertion about those specific metrics from the supplied paper content.", "evidence": "Abstract: \"As a case study, we adapt DiGress, a diffusion-based one-shot model, to the whole spectrum of sequentiality, reaching new state of the art results, and motivating a renewed interest in developing autoregressive graph generative models.\" Introduction: \"We show empirically that adapting Digress (Vignac et al., 2022), a state-of-the-art graph generative model, to 1-node sequential, leads to surpassing all autoregressive baselines, and is competitive with other state-of-the-art one-shot baselines such as CDGS (Huang et al., 2023).\"", "section": "Abstract; 1 INTRODUCTION"}
{"claim": "The paper lacks a performance comparison with DiGress, which is meaningful because the authors adapted components from DiGress.", "claim_type": "baseline", "paper_id": "lmYGRGyL4i", "paper_title": "Uncovering the Spectrum of Graph Generative Models: From One-Shot to Sequential", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "2VjET9ABua", "reviewer": "Reviewer_gvwJ", "review_text": "Summary: This paper presents a diffusion-based graph generative model that unifies both one-shot and autoregressive generative models. The node removal process is conducted with a denoising diffusion model and the insertion reverses the process by predicting the number of nodes that have been removed. Setting the number of nodes from 1 to n enables the unification of one-shot and autoregressive generation.\n\nStrengths: 1. This paper proposes the novel unification between one-shot and autoregressive graph generative models using diffusion models.\n2. The introduction of flexible-sized block-wise generation for graph generation stands out as a noteworthy contribution.\n\nWeaknesses: 1. Can the unification of one-shot and autoregressive graph generative models be a strong contribution? For instance, GRAN (Liao et al., 2019) can also be the unification between one-shot and autoregressive graph generative model by setting the block size as the number of nodes. What is the key difference of the work from GRAN except for the usage of diffusion models?\n2. I wonder if it is proper to say the performance as the new state-of-the-art results as mentioned in the abstract. The FCD for QM9 and NSPDK for ZINC do not seem to be state-of-the-art results. Also, as the authors adapted DiGress, the performance comparison with DiGress can be meaningful.\n3. Lack of detailed analysis on the sample quality-time trade-off. A more detailed analysis of the correlation between the sample quality and time (or memory consumption) is needed by comparing the one-shot and autoregressive versions of the IFH model.\n\nQuestions: 1. Which level of sequentiality did the authors use (I cannot find details in appx D.)? Does the degree of sequentiality imply the block size (or the number of steps)?\n2. The generic graph generation results in appx B do not look good enough. Is there any particular reason that the model works okay for molecular graphs but not for non-attributed generic graphs?", "labeling_timestamp": "2026-01-11T17:08:42.583074", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper states that the authors adapted DiGress as a case study and reports comparisons against autoregressive baselines and other one-shot baselines (e.g., CDGS), but the provided content does not show any direct performance comparison to the original DiGress model. Thus the claim that the paper lacks a performance comparison with DiGress is supported by the paper text shown.", "evidence": "Abstract: \"As a case study, we adapt DiGress, a diffusion-based one-shot model, to the whole spectrum of sequentiality, reaching new state of the art results, and motivating a renewed interest in developing autoregressive graph generative models.\" \nIntroduction: \"We show empirically that adapting Digress (Vignac et al., 2022), a state-of-the-art graph generative model, to 1-node sequential, leads to surpassing all autoregressive baselines, and is competitive with other state-of-the-art one-shot baselines such as CDGS (Huang et al., 2023).\"", "section": "Abstract; 1 INTRODUCTION"}
{"claim": "The paper lacks a detailed analysis of the trade-off between sample quality and time or memory consumption.", "claim_type": "experimental", "paper_id": "lmYGRGyL4i", "paper_title": "Uncovering the Spectrum of Graph Generative Models: From One-Shot to Sequential", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "2VjET9ABua", "reviewer": "Reviewer_gvwJ", "review_text": "Summary: This paper presents a diffusion-based graph generative model that unifies both one-shot and autoregressive generative models. The node removal process is conducted with a denoising diffusion model and the insertion reverses the process by predicting the number of nodes that have been removed. Setting the number of nodes from 1 to n enables the unification of one-shot and autoregressive generation.\n\nStrengths: 1. This paper proposes the novel unification between one-shot and autoregressive graph generative models using diffusion models.\n2. The introduction of flexible-sized block-wise generation for graph generation stands out as a noteworthy contribution.\n\nWeaknesses: 1. Can the unification of one-shot and autoregressive graph generative models be a strong contribution? For instance, GRAN (Liao et al., 2019) can also be the unification between one-shot and autoregressive graph generative model by setting the block size as the number of nodes. What is the key difference of the work from GRAN except for the usage of diffusion models?\n2. I wonder if it is proper to say the performance as the new state-of-the-art results as mentioned in the abstract. The FCD for QM9 and NSPDK for ZINC do not seem to be state-of-the-art results. Also, as the authors adapted DiGress, the performance comparison with DiGress can be meaningful.\n3. Lack of detailed analysis on the sample quality-time trade-off. A more detailed analysis of the correlation between the sample quality and time (or memory consumption) is needed by comparing the one-shot and autoregressive versions of the IFH model.\n\nQuestions: 1. Which level of sequentiality did the authors use (I cannot find details in appx D.)? Does the degree of sequentiality imply the block size (or the number of steps)?\n2. The generic graph generation results in appx B do not look good enough. Is there any particular reason that the model works okay for molecular graphs but not for non-attributed generic graphs?", "labeling_timestamp": "2026-01-11T17:08:35.411032", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states that it conducts an analysis of the sample quality–time trade-off, so the reviewer claim that the paper lacks such an analysis contradicts the paper's stated content.", "evidence": "“Based on this, we conduct the first analysis of the sample quality-time trade-off across a range of molecular and generic graphs datasets.”", "section": "ABSTRACT"}
{"claim": "The authors do not compare the one-shot and autoregressive versions of IFH in terms of sample quality versus time and memory trade-offs.", "claim_type": "baseline", "paper_id": "lmYGRGyL4i", "paper_title": "Uncovering the Spectrum of Graph Generative Models: From One-Shot to Sequential", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "2VjET9ABua", "reviewer": "Reviewer_gvwJ", "review_text": "Summary: This paper presents a diffusion-based graph generative model that unifies both one-shot and autoregressive generative models. The node removal process is conducted with a denoising diffusion model and the insertion reverses the process by predicting the number of nodes that have been removed. Setting the number of nodes from 1 to n enables the unification of one-shot and autoregressive generation.\n\nStrengths: 1. This paper proposes the novel unification between one-shot and autoregressive graph generative models using diffusion models.\n2. The introduction of flexible-sized block-wise generation for graph generation stands out as a noteworthy contribution.\n\nWeaknesses: 1. Can the unification of one-shot and autoregressive graph generative models be a strong contribution? For instance, GRAN (Liao et al., 2019) can also be the unification between one-shot and autoregressive graph generative model by setting the block size as the number of nodes. What is the key difference of the work from GRAN except for the usage of diffusion models?\n2. I wonder if it is proper to say the performance as the new state-of-the-art results as mentioned in the abstract. The FCD for QM9 and NSPDK for ZINC do not seem to be state-of-the-art results. Also, as the authors adapted DiGress, the performance comparison with DiGress can be meaningful.\n3. Lack of detailed analysis on the sample quality-time trade-off. A more detailed analysis of the correlation between the sample quality and time (or memory consumption) is needed by comparing the one-shot and autoregressive versions of the IFH model.\n\nQuestions: 1. Which level of sequentiality did the authors use (I cannot find details in appx D.)? Does the degree of sequentiality imply the block size (or the number of steps)?\n2. The generic graph generation results in appx B do not look good enough. Is there any particular reason that the model works okay for molecular graphs but not for non-attributed generic graphs?", "labeling_timestamp": "2026-01-11T17:09:04.710188", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly states it analyzes the sample quality vs time trade-off (so the reviewer’s claim is false on the time aspect), but the provided paper content contains no discussion or comparison of memory trade-offs (so the reviewer’s claim is true for memory).", "evidence": "\"we conduct the first analysis of the sample quality-time trade-off across a range of molecular and generic graphs datasets.\" (Abstract) ; \"we conduct the first analysis of the sample quality-time trade-off\" (Introduction/Abstract).", "section": "Abstract; Introduction (statements about analyzing sample quality-time trade-off). Note: the provided excerpts do not contain any discussion of memory trade-offs."}
{"claim": "The manuscript does not specify which level of sequentiality was used; the reviewer could not find details in Appendix D.", "claim_type": "methodology", "paper_id": "lmYGRGyL4i", "paper_title": "Uncovering the Spectrum of Graph Generative Models: From One-Shot to Sequential", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "2VjET9ABua", "reviewer": "Reviewer_gvwJ", "review_text": "Summary: This paper presents a diffusion-based graph generative model that unifies both one-shot and autoregressive generative models. The node removal process is conducted with a denoising diffusion model and the insertion reverses the process by predicting the number of nodes that have been removed. Setting the number of nodes from 1 to n enables the unification of one-shot and autoregressive generation.\n\nStrengths: 1. This paper proposes the novel unification between one-shot and autoregressive graph generative models using diffusion models.\n2. The introduction of flexible-sized block-wise generation for graph generation stands out as a noteworthy contribution.\n\nWeaknesses: 1. Can the unification of one-shot and autoregressive graph generative models be a strong contribution? For instance, GRAN (Liao et al., 2019) can also be the unification between one-shot and autoregressive graph generative model by setting the block size as the number of nodes. What is the key difference of the work from GRAN except for the usage of diffusion models?\n2. I wonder if it is proper to say the performance as the new state-of-the-art results as mentioned in the abstract. The FCD for QM9 and NSPDK for ZINC do not seem to be state-of-the-art results. Also, as the authors adapted DiGress, the performance comparison with DiGress can be meaningful.\n3. Lack of detailed analysis on the sample quality-time trade-off. A more detailed analysis of the correlation between the sample quality and time (or memory consumption) is needed by comparing the one-shot and autoregressive versions of the IFH model.\n\nQuestions: 1. Which level of sequentiality did the authors use (I cannot find details in appx D.)? Does the degree of sequentiality imply the block size (or the number of steps)?\n2. The generic graph generation results in appx B do not look good enough. Is there any particular reason that the model works okay for molecular graphs but not for non-attributed generic graphs?", "labeling_timestamp": "2026-01-11T17:09:17.666086", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly refers to levels of sequentiality and gives a concrete example (adapting DiGress to a 1-node sequential setting). It also states that one-shot models correspond to 1-step IFH models and that the framework spans any degree of sequentiality, so the manuscript does specify levels of sequentiality (contradicting the reviewer’s claim).", "evidence": "Abstract: \"By adjusting the knob on node removal, the framework allows for any degree of sequentiality, from one-shot to fully sequential, and any node ordering, e.g., random and BFS.\"; Introduction: \"We show empirically that adapting Digress (Vignac et al., 2022), a state-of-the-art graph generative model, to 1-node sequential, leads to surpassing all autoregressive baselines...\"; Introduction: \"It can be shown that one-shot models are 1-step IFH models, and that any degree of sequentiality can be achieved by choosing different removal processes, down to 1 node at a time.\"", "section": "Abstract; 1 INTRODUCTION"}
{"claim": "It is unclear whether the degree of sequentiality corresponds to block size or to the number of generation steps.", "claim_type": "experimental", "paper_id": "lmYGRGyL4i", "paper_title": "Uncovering the Spectrum of Graph Generative Models: From One-Shot to Sequential", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "2VjET9ABua", "reviewer": "Reviewer_gvwJ", "review_text": "Summary: This paper presents a diffusion-based graph generative model that unifies both one-shot and autoregressive generative models. The node removal process is conducted with a denoising diffusion model and the insertion reverses the process by predicting the number of nodes that have been removed. Setting the number of nodes from 1 to n enables the unification of one-shot and autoregressive generation.\n\nStrengths: 1. This paper proposes the novel unification between one-shot and autoregressive graph generative models using diffusion models.\n2. The introduction of flexible-sized block-wise generation for graph generation stands out as a noteworthy contribution.\n\nWeaknesses: 1. Can the unification of one-shot and autoregressive graph generative models be a strong contribution? For instance, GRAN (Liao et al., 2019) can also be the unification between one-shot and autoregressive graph generative model by setting the block size as the number of nodes. What is the key difference of the work from GRAN except for the usage of diffusion models?\n2. I wonder if it is proper to say the performance as the new state-of-the-art results as mentioned in the abstract. The FCD for QM9 and NSPDK for ZINC do not seem to be state-of-the-art results. Also, as the authors adapted DiGress, the performance comparison with DiGress can be meaningful.\n3. Lack of detailed analysis on the sample quality-time trade-off. A more detailed analysis of the correlation between the sample quality and time (or memory consumption) is needed by comparing the one-shot and autoregressive versions of the IFH model.\n\nQuestions: 1. Which level of sequentiality did the authors use (I cannot find details in appx D.)? Does the degree of sequentiality imply the block size (or the number of steps)?\n2. The generic graph generation results in appx B do not look good enough. Is there any particular reason that the model works okay for molecular graphs but not for non-attributed generic graphs?", "labeling_timestamp": "2026-01-11T17:09:14.478309", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states that sequentiality is controlled by the node removal process (i.e., how many nodes are removed per step / block size) and that this yields the full spectrum from 1-step (one-shot) to 1-node-at-a-time (fully sequential), so the relationship is made clear.", "evidence": ["By adjusting the knob on node removal, the framework allows for any degree of sequentiality, from one-shot to fully sequential, and any node ordering, e.g., random and BFS.", "It can be shown that one-shot models are 1-step IFH models, and that any degree of sequentiality can be achieved by choosing different removal processes, down to 1 node at a time.", "Differently from (Kong et al., 2023), we do not limit the process to the choice of one node per step, but we study the diffusion process both from the node ordering and nodes number perspectives."], "section": ["Abstract", "1 INTRODUCTION", "3 REMOVING NODES AS A GRAPH NOISE PROCESS"]}
{"claim": "The generic graph generation results reported in Appendix B are not satisfactory compared to the molecular graph results.", "claim_type": "experimental", "paper_id": "lmYGRGyL4i", "paper_title": "Uncovering the Spectrum of Graph Generative Models: From One-Shot to Sequential", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "2VjET9ABua", "reviewer": "Reviewer_gvwJ", "review_text": "Summary: This paper presents a diffusion-based graph generative model that unifies both one-shot and autoregressive generative models. The node removal process is conducted with a denoising diffusion model and the insertion reverses the process by predicting the number of nodes that have been removed. Setting the number of nodes from 1 to n enables the unification of one-shot and autoregressive generation.\n\nStrengths: 1. This paper proposes the novel unification between one-shot and autoregressive graph generative models using diffusion models.\n2. The introduction of flexible-sized block-wise generation for graph generation stands out as a noteworthy contribution.\n\nWeaknesses: 1. Can the unification of one-shot and autoregressive graph generative models be a strong contribution? For instance, GRAN (Liao et al., 2019) can also be the unification between one-shot and autoregressive graph generative model by setting the block size as the number of nodes. What is the key difference of the work from GRAN except for the usage of diffusion models?\n2. I wonder if it is proper to say the performance as the new state-of-the-art results as mentioned in the abstract. The FCD for QM9 and NSPDK for ZINC do not seem to be state-of-the-art results. Also, as the authors adapted DiGress, the performance comparison with DiGress can be meaningful.\n3. Lack of detailed analysis on the sample quality-time trade-off. A more detailed analysis of the correlation between the sample quality and time (or memory consumption) is needed by comparing the one-shot and autoregressive versions of the IFH model.\n\nQuestions: 1. Which level of sequentiality did the authors use (I cannot find details in appx D.)? Does the degree of sequentiality imply the block size (or the number of steps)?\n2. The generic graph generation results in appx B do not look good enough. Is there any particular reason that the model works okay for molecular graphs but not for non-attributed generic graphs?", "labeling_timestamp": "2026-01-11T17:09:12.342597", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not include Appendix B or the detailed experimental results needed to compare generic graph generation results with molecular graph results. The paper mentions experiments across molecular and generic datasets (Abstract) and claims strong empirical performance (Introduction), but no Appendix B content or specific comparative numbers are present in the supplied text to confirm the reviewer's claim.", "evidence": "Abstract: \"Based on this, we conduct the first analysis of the sample quality-time trade-off across a range of molecular and generic graphs datasets.\" Introduction: \"We show empirically that adapting Digress (Vignac et al., 2022), a state-of-the-art graph generative model, to 1-node sequential, leads to surpassing all autoregressive baselines, and is competitive with other state-of-the-art one-shot baselines such as CDGS (Huang et al., 2023).\"", "section": "ABSTRACT; 1 INTRODUCTION"}
{"claim": "The paper does not provide an explanation for why the model performs well on molecular graphs but poorly on non-attributed generic graphs.", "claim_type": "experimental", "paper_id": "lmYGRGyL4i", "paper_title": "Uncovering the Spectrum of Graph Generative Models: From One-Shot to Sequential", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "2VjET9ABua", "reviewer": "Reviewer_gvwJ", "review_text": "Summary: This paper presents a diffusion-based graph generative model that unifies both one-shot and autoregressive generative models. The node removal process is conducted with a denoising diffusion model and the insertion reverses the process by predicting the number of nodes that have been removed. Setting the number of nodes from 1 to n enables the unification of one-shot and autoregressive generation.\n\nStrengths: 1. This paper proposes the novel unification between one-shot and autoregressive graph generative models using diffusion models.\n2. The introduction of flexible-sized block-wise generation for graph generation stands out as a noteworthy contribution.\n\nWeaknesses: 1. Can the unification of one-shot and autoregressive graph generative models be a strong contribution? For instance, GRAN (Liao et al., 2019) can also be the unification between one-shot and autoregressive graph generative model by setting the block size as the number of nodes. What is the key difference of the work from GRAN except for the usage of diffusion models?\n2. I wonder if it is proper to say the performance as the new state-of-the-art results as mentioned in the abstract. The FCD for QM9 and NSPDK for ZINC do not seem to be state-of-the-art results. Also, as the authors adapted DiGress, the performance comparison with DiGress can be meaningful.\n3. Lack of detailed analysis on the sample quality-time trade-off. A more detailed analysis of the correlation between the sample quality and time (or memory consumption) is needed by comparing the one-shot and autoregressive versions of the IFH model.\n\nQuestions: 1. Which level of sequentiality did the authors use (I cannot find details in appx D.)? Does the degree of sequentiality imply the block size (or the number of steps)?\n2. The generic graph generation results in appx B do not look good enough. Is there any particular reason that the model works okay for molecular graphs but not for non-attributed generic graphs?", "labeling_timestamp": "2026-01-11T17:09:25.230680", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper mentions empirical analysis across molecular and generic graph datasets (Abstract, Introduction) and reports empirical results, but nowhere in the provided content does it offer a theoretical or empirical explanation for why the model would perform well on molecular (attributed) graphs yet poorly on non-attributed generic graphs. There is no dedicated discussion or rationale addressing differences in performance between these dataset types.", "evidence": "1) \"we conduct the first analysis of the sample quality-time trade-off across a range of molecular and generic graphs datasets.\"  (Abstract)\n2) \"Looking into the literature, this is not always the case, as one-shot models have entered the stateof-the-art on challenging datasets like ZINC250k (Irwin et al., 2012), Ego (Sen et al., 2008), and many more.\"  (Introduction)\n3) \"We show empirically that adapting Digress (Vignac et al., 2022), a state-of-the-art graph generative model, to 1-node sequential, leads to surpassing all autoregressive baselines, and is competitive with other state-of-the-art one-shot baselines such as CDGS (Huang et al., 2023).\"  (Introduction)", "section": "Abstract; Introduction"}
{"claim": "The method is described in a way that is complex and may be difficult for readers to follow and to implement reproducibly.", "claim_type": "presentation", "paper_id": "KkYZmepjHn", "paper_title": "Fast Sampling via Discrete Non-Markov Diffusion Models with Predetermined Transition Time", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "rZg9TvKtrg", "reviewer": "Reviewer_Uw1Q", "review_text": "Summary: The paper introduces a discrete non-Markov diffusion model (DNDM) aimed at accelerating the sampling process in discrete diffusion models. The proposed method reduces the number of neural network function evaluations to speed up the sampling process while maintaining sample quality. The paper explores the transition from finite to infinite step sampling, providing new insights into bridging the gap between discrete and continuous-time processes. Experiments on natural language generation and machine translation tasks illustrate the competitive performance of the method in terms of speed and quality compared to existing methods.\n\nStrengths: * The introduction of a discrete non-Markov diffusion model (DNDM) provides a new method for accelerating the sampling process in discrete diffusion models in a training-free manner. It reduces the number of neural network function evaluations, enhancing the efficiency of the sampling process to speedup by 3x for 50 steps.\n* The authors conducted extensive experiments on natural language generation and machine translation tasks, demonstrating the effectiveness of the proposed method, for both multinomial and absorbing diffusions.\n\nWeaknesses: * The method involves a complex process that might be challenging to easily follow and implement. More details or visualizations on how the transition time distribution is determined and whether it can be adapted for different types of discrete diffusion models will be helpful for a better understanding of the motivation and methodology.\n* The comparison with other acceleration methods is not very convincing, especially at a practical smaller number of sampling steps. Instead of RDM baseline, how does the proposed method compare with other existing acceleration techniques for discrete diffusion models in terms of both efficiency and quality? \n* While the method is tested on natural language generation and machine translation tasks, its applicability to other modalities such as image or video generation is not unknown, which might limited the scope of the proposed method.\n\nQuestions: Suggest to address concerns in the weakness section.", "labeling_timestamp": "2026-01-11T17:09:39.511107", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper supplies theoretical proofs and points readers to implementation details in appendices, and explicitly provides code, which contradicts the claim that the method is described in a way that makes reproducible implementation difficult.", "evidence": "Abstract: \"Codes are available at https://github.com/ uclaml/DNDM .\"; Section 2: \"...we leave detailed discussions of these losses to Appendix B.\"; Section 3: \"We can prove the ELBO of the DNDM is equivalent to the ELBO of the original process (5) up to some constant, which further supports the neural network p_θ(·|x_t) trained by the Markov process in (1), remains applicable to DNDM. (See Appendix B.3 for details).\"", "section": "Abstract; Section 2 (Training the Neural Network); Section 3 (Discrete Non-Markov Diffusion Models)"}
{"claim": "The paper lacks detailed explanation or visualizations showing how the transition time distribution is determined.", "claim_type": "methodology", "paper_id": "KkYZmepjHn", "paper_title": "Fast Sampling via Discrete Non-Markov Diffusion Models with Predetermined Transition Time", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "rZg9TvKtrg", "reviewer": "Reviewer_Uw1Q", "review_text": "Summary: The paper introduces a discrete non-Markov diffusion model (DNDM) aimed at accelerating the sampling process in discrete diffusion models. The proposed method reduces the number of neural network function evaluations to speed up the sampling process while maintaining sample quality. The paper explores the transition from finite to infinite step sampling, providing new insights into bridging the gap between discrete and continuous-time processes. Experiments on natural language generation and machine translation tasks illustrate the competitive performance of the method in terms of speed and quality compared to existing methods.\n\nStrengths: * The introduction of a discrete non-Markov diffusion model (DNDM) provides a new method for accelerating the sampling process in discrete diffusion models in a training-free manner. It reduces the number of neural network function evaluations, enhancing the efficiency of the sampling process to speedup by 3x for 50 steps.\n* The authors conducted extensive experiments on natural language generation and machine translation tasks, demonstrating the effectiveness of the proposed method, for both multinomial and absorbing diffusions.\n\nWeaknesses: * The method involves a complex process that might be challenging to easily follow and implement. More details or visualizations on how the transition time distribution is determined and whether it can be adapted for different types of discrete diffusion models will be helpful for a better understanding of the motivation and methodology.\n* The comparison with other acceleration methods is not very convincing, especially at a practical smaller number of sampling steps. Instead of RDM baseline, how does the proposed method compare with other existing acceleration techniques for discrete diffusion models in terms of both efficiency and quality? \n* While the method is tested on natural language generation and machine translation tasks, its applicability to other modalities such as image or video generation is not unknown, which might limited the scope of the proposed method.\n\nQuestions: Suggest to address concerns in the weakness section.", "labeling_timestamp": "2026-01-11T17:09:45.198408", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper defines transition time τ and notes b_t ~ Bernoulli(β_t) (Section 2 and 3.1) and discusses qualitative properties of τ (Remark 3.4), but in the provided content there is no detailed derivation, analytic expression, or visualization of the transition-time distribution itself.", "evidence": ["In this section, we introduce a non-Markov process such that the joint distribution of (x_0, x_t) remains the same as the one defined with Markov process in Section 2.", "Definition 3.2. Transition time τ is the time that the token x_t transition from x_0 to noise, i.e., τ := min_t { t | b_t = 0 }.", "where b_t is independently drawn from the Bernoulli distribution Bernoulli(β_t) and w is drawn from the noise distribution q_noise.", "Remark 3.4. (7) and (8) suggest that even though there are T distinct time steps, not every time in the range 1:T is crucial for capturing the process. Therefore, our primary focus should be on the most significant time step, i.e., the transition time τ , enabling faster reverse sampling."], "section": "Section 3.1 (Forward and Reverse Process), Definition 3.2, Remark 3.4"}
{"claim": "The paper does not evaluate whether the transition time distribution can be adapted for different types of discrete diffusion models.", "claim_type": "experimental", "paper_id": "KkYZmepjHn", "paper_title": "Fast Sampling via Discrete Non-Markov Diffusion Models with Predetermined Transition Time", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "rZg9TvKtrg", "reviewer": "Reviewer_Uw1Q", "review_text": "Summary: The paper introduces a discrete non-Markov diffusion model (DNDM) aimed at accelerating the sampling process in discrete diffusion models. The proposed method reduces the number of neural network function evaluations to speed up the sampling process while maintaining sample quality. The paper explores the transition from finite to infinite step sampling, providing new insights into bridging the gap between discrete and continuous-time processes. Experiments on natural language generation and machine translation tasks illustrate the competitive performance of the method in terms of speed and quality compared to existing methods.\n\nStrengths: * The introduction of a discrete non-Markov diffusion model (DNDM) provides a new method for accelerating the sampling process in discrete diffusion models in a training-free manner. It reduces the number of neural network function evaluations, enhancing the efficiency of the sampling process to speedup by 3x for 50 steps.\n* The authors conducted extensive experiments on natural language generation and machine translation tasks, demonstrating the effectiveness of the proposed method, for both multinomial and absorbing diffusions.\n\nWeaknesses: * The method involves a complex process that might be challenging to easily follow and implement. More details or visualizations on how the transition time distribution is determined and whether it can be adapted for different types of discrete diffusion models will be helpful for a better understanding of the motivation and methodology.\n* The comparison with other acceleration methods is not very convincing, especially at a practical smaller number of sampling steps. Instead of RDM baseline, how does the proposed method compare with other existing acceleration techniques for discrete diffusion models in terms of both efficiency and quality? \n* While the method is tested on natural language generation and machine translation tasks, its applicability to other modalities such as image or video generation is not unknown, which might limited the scope of the proposed method.\n\nQuestions: Suggest to address concerns in the weakness section.", "labeling_timestamp": "2026-01-11T17:09:53.016061", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper introduces a predetermined transition time set and analyzes its use for accelerated sampling (Abstract, Introduction, Section 3.1) and claims applicability to multinomial and absorbing diffusions, but the provided content contains no evaluation or experiment that investigates adapting the transition time distribution across different discrete diffusion model types.", "evidence": "Abstract: \"we propose discrete non-Markov diffusion models (DNDM), which naturally induce the predetermined transition time set.\" Introduction: \"Based on the predetermined transition time set T in DNDM, we design an accelerated sampling algorithm that reduces the required number of neural network function evaluations.\" Section 3.1: \"Definition 3.2. Transition time τ is the time that the token x_t transition from x_0 to noise...\" and Remark 3.4: \"...our primary focus should be on the most significant time step, i.e., the transition time τ, enabling faster reverse sampling.\"", "section": "Abstract; Introduction; Section 3.1"}
{"claim": "Comparisons to other acceleration methods are unconvincing for small practical numbers of sampling steps.", "claim_type": "baseline", "paper_id": "KkYZmepjHn", "paper_title": "Fast Sampling via Discrete Non-Markov Diffusion Models with Predetermined Transition Time", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "rZg9TvKtrg", "reviewer": "Reviewer_Uw1Q", "review_text": "Summary: The paper introduces a discrete non-Markov diffusion model (DNDM) aimed at accelerating the sampling process in discrete diffusion models. The proposed method reduces the number of neural network function evaluations to speed up the sampling process while maintaining sample quality. The paper explores the transition from finite to infinite step sampling, providing new insights into bridging the gap between discrete and continuous-time processes. Experiments on natural language generation and machine translation tasks illustrate the competitive performance of the method in terms of speed and quality compared to existing methods.\n\nStrengths: * The introduction of a discrete non-Markov diffusion model (DNDM) provides a new method for accelerating the sampling process in discrete diffusion models in a training-free manner. It reduces the number of neural network function evaluations, enhancing the efficiency of the sampling process to speedup by 3x for 50 steps.\n* The authors conducted extensive experiments on natural language generation and machine translation tasks, demonstrating the effectiveness of the proposed method, for both multinomial and absorbing diffusions.\n\nWeaknesses: * The method involves a complex process that might be challenging to easily follow and implement. More details or visualizations on how the transition time distribution is determined and whether it can be adapted for different types of discrete diffusion models will be helpful for a better understanding of the motivation and methodology.\n* The comparison with other acceleration methods is not very convincing, especially at a practical smaller number of sampling steps. Instead of RDM baseline, how does the proposed method compare with other existing acceleration techniques for discrete diffusion models in terms of both efficiency and quality? \n* While the method is tested on natural language generation and machine translation tasks, its applicability to other modalities such as image or video generation is not unknown, which might limited the scope of the proposed method.\n\nQuestions: Suggest to address concerns in the weakness section.", "labeling_timestamp": "2026-01-11T17:09:41.862245", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly reports quantitative comparisons showing acceleration at small sampling step counts (e.g., T=50) and states experimental results demonstrating superior speed and quality vs existing methods, so the reviewer claim that comparisons are unconvincing for small practical numbers of steps contradicts the paper's reported claims.", "evidence": "\"Notably, our algorithm is about 3 × faster than baselines for T = 50 and about 30 × faster for T = 1000 while preserving the sample quality.\"; \"Extensive experiments on natural language generation and machine translation tasks demonstrate the superior performance of our method in terms of both generation speed and sample quality compared to existing methods for discrete diffusion models.\" (Abstract and Contributions)", "section": "Abstract / Contributions (Introduction)"}
{"claim": "The paper compares only to an RDM baseline and does not compare against other existing acceleration techniques for discrete diffusion models.", "claim_type": "baseline", "paper_id": "KkYZmepjHn", "paper_title": "Fast Sampling via Discrete Non-Markov Diffusion Models with Predetermined Transition Time", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "rZg9TvKtrg", "reviewer": "Reviewer_Uw1Q", "review_text": "Summary: The paper introduces a discrete non-Markov diffusion model (DNDM) aimed at accelerating the sampling process in discrete diffusion models. The proposed method reduces the number of neural network function evaluations to speed up the sampling process while maintaining sample quality. The paper explores the transition from finite to infinite step sampling, providing new insights into bridging the gap between discrete and continuous-time processes. Experiments on natural language generation and machine translation tasks illustrate the competitive performance of the method in terms of speed and quality compared to existing methods.\n\nStrengths: * The introduction of a discrete non-Markov diffusion model (DNDM) provides a new method for accelerating the sampling process in discrete diffusion models in a training-free manner. It reduces the number of neural network function evaluations, enhancing the efficiency of the sampling process to speedup by 3x for 50 steps.\n* The authors conducted extensive experiments on natural language generation and machine translation tasks, demonstrating the effectiveness of the proposed method, for both multinomial and absorbing diffusions.\n\nWeaknesses: * The method involves a complex process that might be challenging to easily follow and implement. More details or visualizations on how the transition time distribution is determined and whether it can be adapted for different types of discrete diffusion models will be helpful for a better understanding of the motivation and methodology.\n* The comparison with other acceleration methods is not very convincing, especially at a practical smaller number of sampling steps. Instead of RDM baseline, how does the proposed method compare with other existing acceleration techniques for discrete diffusion models in terms of both efficiency and quality? \n* While the method is tested on natural language generation and machine translation tasks, its applicability to other modalities such as image or video generation is not unknown, which might limited the scope of the proposed method.\n\nQuestions: Suggest to address concerns in the weakness section.", "labeling_timestamp": "2026-01-11T17:09:56.314332", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper mentions RDM (Zheng et al., 2023) and claims empirical comparisons to \"existing methods\", but the provided content does not list the actual baselines used in experiments or show experimental comparison tables. Therefore it is impossible to determine from the supplied text whether the comparisons are limited to only RDM or include other acceleration techniques.", "evidence": "1) \"Very recently, Zheng et al. (2023) introduced a reparameterized diffusion model (RDM) that can improve sampling speed and sample quality in text generation tasks.\" (Introduction)\n\n2) \"Extensive experiments on natural language generation and machine translation tasks demonstrate the superior performance of our method in terms of both generation speed and sample quality compared to existing methods for discrete diffusion models.\" (Abstract)\n\n3) \"Notably, our algorithm is about 3 × faster than baselines for T = 50 and about 30 × faster for T = 1000 while preserving the sample quality.\" (Contributions)", "section": "Abstract; Introduction (Section 1); Contributions"}
{"claim": "The paper does not report efficiency and quality comparisons versus other acceleration techniques beyond the RDM baseline.", "claim_type": "baseline", "paper_id": "KkYZmepjHn", "paper_title": "Fast Sampling via Discrete Non-Markov Diffusion Models with Predetermined Transition Time", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "rZg9TvKtrg", "reviewer": "Reviewer_Uw1Q", "review_text": "Summary: The paper introduces a discrete non-Markov diffusion model (DNDM) aimed at accelerating the sampling process in discrete diffusion models. The proposed method reduces the number of neural network function evaluations to speed up the sampling process while maintaining sample quality. The paper explores the transition from finite to infinite step sampling, providing new insights into bridging the gap between discrete and continuous-time processes. Experiments on natural language generation and machine translation tasks illustrate the competitive performance of the method in terms of speed and quality compared to existing methods.\n\nStrengths: * The introduction of a discrete non-Markov diffusion model (DNDM) provides a new method for accelerating the sampling process in discrete diffusion models in a training-free manner. It reduces the number of neural network function evaluations, enhancing the efficiency of the sampling process to speedup by 3x for 50 steps.\n* The authors conducted extensive experiments on natural language generation and machine translation tasks, demonstrating the effectiveness of the proposed method, for both multinomial and absorbing diffusions.\n\nWeaknesses: * The method involves a complex process that might be challenging to easily follow and implement. More details or visualizations on how the transition time distribution is determined and whether it can be adapted for different types of discrete diffusion models will be helpful for a better understanding of the motivation and methodology.\n* The comparison with other acceleration methods is not very convincing, especially at a practical smaller number of sampling steps. Instead of RDM baseline, how does the proposed method compare with other existing acceleration techniques for discrete diffusion models in terms of both efficiency and quality? \n* While the method is tested on natural language generation and machine translation tasks, its applicability to other modalities such as image or video generation is not unknown, which might limited the scope of the proposed method.\n\nQuestions: Suggest to address concerns in the weakness section.", "labeling_timestamp": "2026-01-11T17:10:22.849742", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt mentions RDM and claims superiority over \"existing methods for discrete diffusion models\" and reports speedups vs unspecified \"baselines\", but it does not list which other acceleration techniques were compared or provide experimental comparison details in the quoted sections. Based on the supplied content, there is insufficient information to verify whether the paper reports efficiency and quality comparisons beyond the RDM baseline.", "evidence": ["Very recently, Zheng et al. (2023) introduced a reparameterized diffusion model (RDM) that can improve sampling speed and sample quality in text generation tasks. However, their proposed algorithm is a training-based approach.", "Extensive experiments on natural language generation and machine translation tasks demonstrate the superior performance of our method in terms of both generation speed and sample quality compared to existing methods for discrete diffusion models.", "Moreover, |T| is provably less than T and approaches O (1) as T goes to infinity. We provide both theoretical analysis and empirical experiments showing that the improvement in the number of function evaluations (NFE) is significant. Notably, our algorithm is about 3 × faster than baselines for T = 50 and about 30 × faster for T = 1000 while preserving the sample quality."], "section": "Abstract / Introduction / Contributions"}
{"claim": "Applicability of the proposed method to other modalities such as image or video generation is not evaluated, limiting the demonstrated scope.", "claim_type": "experimental", "paper_id": "KkYZmepjHn", "paper_title": "Fast Sampling via Discrete Non-Markov Diffusion Models with Predetermined Transition Time", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "rZg9TvKtrg", "reviewer": "Reviewer_Uw1Q", "review_text": "Summary: The paper introduces a discrete non-Markov diffusion model (DNDM) aimed at accelerating the sampling process in discrete diffusion models. The proposed method reduces the number of neural network function evaluations to speed up the sampling process while maintaining sample quality. The paper explores the transition from finite to infinite step sampling, providing new insights into bridging the gap between discrete and continuous-time processes. Experiments on natural language generation and machine translation tasks illustrate the competitive performance of the method in terms of speed and quality compared to existing methods.\n\nStrengths: * The introduction of a discrete non-Markov diffusion model (DNDM) provides a new method for accelerating the sampling process in discrete diffusion models in a training-free manner. It reduces the number of neural network function evaluations, enhancing the efficiency of the sampling process to speedup by 3x for 50 steps.\n* The authors conducted extensive experiments on natural language generation and machine translation tasks, demonstrating the effectiveness of the proposed method, for both multinomial and absorbing diffusions.\n\nWeaknesses: * The method involves a complex process that might be challenging to easily follow and implement. More details or visualizations on how the transition time distribution is determined and whether it can be adapted for different types of discrete diffusion models will be helpful for a better understanding of the motivation and methodology.\n* The comparison with other acceleration methods is not very convincing, especially at a practical smaller number of sampling steps. Instead of RDM baseline, how does the proposed method compare with other existing acceleration techniques for discrete diffusion models in terms of both efficiency and quality? \n* While the method is tested on natural language generation and machine translation tasks, its applicability to other modalities such as image or video generation is not unknown, which might limited the scope of the proposed method.\n\nQuestions: Suggest to address concerns in the weakness section.", "labeling_timestamp": "2026-01-11T17:10:10.078097", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper's experiments are described as being on natural language generation and machine translation only; it does not report evaluations on image or video generation, so the claim that applicability to those modalities is not evaluated is accurate.", "evidence": "\"Extensive experiments on natural language generation and machine translation tasks demonstrate the superior performance of our method in terms of both generation speed and sample quality compared to existing methods for discrete diffusion models.\"", "section": "Abstract"}
{"claim": "The paper's organization is very hard to follow, which significantly hinders the reader's ability to understand the technical arguments and results.", "claim_type": "presentation", "paper_id": "lWXedJyLuL", "paper_title": "A Unified Causal View of Instruction Tuning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "f6n4G0QZIH", "reviewer": "Reviewer_nrtp", "review_text": "Summary: This paper tries to learn task-required causal factors and only use those to make predictions for a given task. The motivation is claimed to be that the causal factors of different tasks are spuriously correlated through the task variable $T$. The authors theoretically prove the causal factor can be identified without mixing information from others, then they propose a Structural Instruction Tuning (SIT) method to learn the task-required causal representations that can mimic the causal factors for each task.\n\nStrengths: 1. Exploring causal LLMs is very important for both research and applications.\n\n2. This is a very novel work in this direction.\n\nWeaknesses: 1. The organization of this paper is very hard to follow.\n\n2. The technical quality is poor. There are serious technical flaws in this paper. See the questions below.\n\nQuestions: Q1. SCMs should be constructed from structual functions. Intuitive causal graphs are very informal and not relible. Equation 1 is not enough, because the structual functions should appear before the graphs. I think it would be better to illustrate Fig 1 with a specific example (i.e., what $L_i, X, Y$ and $T$ stands for in a specific task, and how the non-causal factors correlate with $Y_t$).\n\nQ2. The authors use different $L_i$ for different causal factors, and this is good. Nevertheless, I think $Y_t$ in Fig.1 should also be notated as different nodes for different $t$. \n\nQ3. The authors claim that \"Causally speaking, there\nexist backdoor paths between the target labels and non-causal latent factors through the task\nvariable T.\". But when we are doing a task, the task variable is given. And in Fig.1, we have $L_i \\perp L_j|T$. So, there must be a problem arising from either the claim itself or the causal graph. Considering question Q1, I think it is more likely the latter. I would appreciate it if the authors could provide a specific example to illustrate how the non-causal factors can correlate with $Y_t$ (e.g., the simplest text classification). This is one of the most important questions for me, and I'm looking forward to the authors' response.\n\nQ4. Following the above question, even if $T$ can act as a confounder and spurious correlations exist in the data, how does the traditional training process make the model learn the spurious correlations? This problem is also untouched, which makes the motivation informal and weak.\n\nQ5. It seems that the only confounder in this paper is the task $T$, I can hardly come up with any applications where different features are caused by the task but don't cause the target label.\n\nQ6. Section 3.2 is really hard to follow. How to alleviate the influence of the spurious correlations? By backdoor adjustment or introducing intervened data?\n\nQ7. The definition of identifiability is confusing. What's the identifiability of a SCM? When we say the identifiability, it usually refers to the causal quantity on two specific variables rather than a SCM. Also, the target of the UIC loss in Eq.6 is vague. What causal quantity do you want to measure?\n\nQ8. I guess the authors want to say that, if $L_i$ is identifiable, then it is the causal factor of $Y_t$. So, the UIC loss is used to promise the identifiability. There are two issues need to be addressed. One is why identifiable factors are causal ones, and another is why UIC can select identifiable factors. I'm not clear whether the second problem has been addressed by Section 3.2, since I think it's not well orangized. Nevertheless, I think the first problem is untouched.\n\nQ9. I'm not sure whether the baselines are advanced enough. The paper only compares with two simple baselines.", "labeling_timestamp": "2026-01-11T17:10:10.328664", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper presents a clear, standard organization (Abstract, Introduction with explicit motivation, Related Work, Problem Formulation and Theory, Figures with pointers to sections, and an explicit contributions list), which indicates it is organized and guides the reader through technical arguments rather than hindering understanding.", "evidence": "1) \"ABSTRACT\\n\\nInstruction tuning on a mixture of tasks has improved zero-shot capabilities in natural language processing (NLP) . ... The utility of our approach is verified by improvements of zero-shot ability on a range of unseen datasets and tasks.\"\\n\\n2) \"Figure 1: Left : The causal graph induced by the meta Structural Causal Model (meta-SCM) for integrating different NLP tasks. ... See Section 3.1 for a detailed description. Right : The model overview of Structural Instruction Tuning (SIT), aiming at learning the representations for task-required causal factors. Task information based on prompts guides the causal factor selection. Detailed description can be found in Section 4.\"\\n\\n3) \"In summary, the main contributions of this paper are as follows: (i) Theoretically, we provide uniform identifiability conditions based on the topology of SCM. ... (iii) Experimentally, we verify the effectiveness of SIT on both in-domain and out-of-domain (OOD) datasets, e.g., 60.51% improvement on Gigaword in terms of Rouge-L. We also show better cross-task adaptability of SIT on unseen tasks, e.g., 31.30% improvement on RTE in terms of accuracy.\"", "section": "Abstract; Introduction (end, contributions); Figure 1 caption"}
{"claim": "The paper's technical quality is poor and contains serious technical flaws that undermine the validity of the proposed approach and conclusions.", "claim_type": "subjective", "paper_id": "lWXedJyLuL", "paper_title": "A Unified Causal View of Instruction Tuning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "f6n4G0QZIH", "reviewer": "Reviewer_nrtp", "review_text": "Summary: This paper tries to learn task-required causal factors and only use those to make predictions for a given task. The motivation is claimed to be that the causal factors of different tasks are spuriously correlated through the task variable $T$. The authors theoretically prove the causal factor can be identified without mixing information from others, then they propose a Structural Instruction Tuning (SIT) method to learn the task-required causal representations that can mimic the causal factors for each task.\n\nStrengths: 1. Exploring causal LLMs is very important for both research and applications.\n\n2. This is a very novel work in this direction.\n\nWeaknesses: 1. The organization of this paper is very hard to follow.\n\n2. The technical quality is poor. There are serious technical flaws in this paper. See the questions below.\n\nQuestions: Q1. SCMs should be constructed from structual functions. Intuitive causal graphs are very informal and not relible. Equation 1 is not enough, because the structual functions should appear before the graphs. I think it would be better to illustrate Fig 1 with a specific example (i.e., what $L_i, X, Y$ and $T$ stands for in a specific task, and how the non-causal factors correlate with $Y_t$).\n\nQ2. The authors use different $L_i$ for different causal factors, and this is good. Nevertheless, I think $Y_t$ in Fig.1 should also be notated as different nodes for different $t$. \n\nQ3. The authors claim that \"Causally speaking, there\nexist backdoor paths between the target labels and non-causal latent factors through the task\nvariable T.\". But when we are doing a task, the task variable is given. And in Fig.1, we have $L_i \\perp L_j|T$. So, there must be a problem arising from either the claim itself or the causal graph. Considering question Q1, I think it is more likely the latter. I would appreciate it if the authors could provide a specific example to illustrate how the non-causal factors can correlate with $Y_t$ (e.g., the simplest text classification). This is one of the most important questions for me, and I'm looking forward to the authors' response.\n\nQ4. Following the above question, even if $T$ can act as a confounder and spurious correlations exist in the data, how does the traditional training process make the model learn the spurious correlations? This problem is also untouched, which makes the motivation informal and weak.\n\nQ5. It seems that the only confounder in this paper is the task $T$, I can hardly come up with any applications where different features are caused by the task but don't cause the target label.\n\nQ6. Section 3.2 is really hard to follow. How to alleviate the influence of the spurious correlations? By backdoor adjustment or introducing intervened data?\n\nQ7. The definition of identifiability is confusing. What's the identifiability of a SCM? When we say the identifiability, it usually refers to the causal quantity on two specific variables rather than a SCM. Also, the target of the UIC loss in Eq.6 is vague. What causal quantity do you want to measure?\n\nQ8. I guess the authors want to say that, if $L_i$ is identifiable, then it is the causal factor of $Y_t$. So, the UIC loss is used to promise the identifiability. There are two issues need to be addressed. One is why identifiable factors are causal ones, and another is why UIC can select identifiable factors. I'm not clear whether the second problem has been addressed by Section 3.2, since I think it's not well orangized. Nevertheless, I think the first problem is untouched.\n\nQ9. I'm not sure whether the baselines are advanced enough. The paper only compares with two simple baselines.", "labeling_timestamp": "2026-01-11T17:10:35.105044", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper presents formal theoretical results (identifiability conditions and proofs) and empirical evidence of substantial improvements, indicating technical soundness rather than 'serious technical flaws' that would undermine validity. The Abstract and main contributions explicitly claim provable identifiability and experimental gains.", "evidence": "Abstract: \"Theoretically, we prove the causal factor can be identified without mixing information from others.\"; Summary of contributions: \"(i) Theoretically, we provide uniform identifiability conditions based on the topology of SCM... (iii) Experimentally, we verify the effectiveness of SIT on both in-domain and out-of-domain (OOD) datasets, e.g., 60.51% improvement on Gigaword in terms of Rouge-L. We also show better cross-task adaptability of SIT on unseen tasks, e.g., 31.30% improvement on RTE in terms of accuracy.\"", "section": "Abstract; Summary of contributions (end of Introduction / Section 1)"}
{"claim": "Equation 1 is insufficient because the paper fails to specify the underlying structural functions before presenting the causal graph.", "claim_type": "methodology", "paper_id": "lWXedJyLuL", "paper_title": "A Unified Causal View of Instruction Tuning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "f6n4G0QZIH", "reviewer": "Reviewer_nrtp", "review_text": "Summary: This paper tries to learn task-required causal factors and only use those to make predictions for a given task. The motivation is claimed to be that the causal factors of different tasks are spuriously correlated through the task variable $T$. The authors theoretically prove the causal factor can be identified without mixing information from others, then they propose a Structural Instruction Tuning (SIT) method to learn the task-required causal representations that can mimic the causal factors for each task.\n\nStrengths: 1. Exploring causal LLMs is very important for both research and applications.\n\n2. This is a very novel work in this direction.\n\nWeaknesses: 1. The organization of this paper is very hard to follow.\n\n2. The technical quality is poor. There are serious technical flaws in this paper. See the questions below.\n\nQuestions: Q1. SCMs should be constructed from structual functions. Intuitive causal graphs are very informal and not relible. Equation 1 is not enough, because the structual functions should appear before the graphs. I think it would be better to illustrate Fig 1 with a specific example (i.e., what $L_i, X, Y$ and $T$ stands for in a specific task, and how the non-causal factors correlate with $Y_t$).\n\nQ2. The authors use different $L_i$ for different causal factors, and this is good. Nevertheless, I think $Y_t$ in Fig.1 should also be notated as different nodes for different $t$. \n\nQ3. The authors claim that \"Causally speaking, there\nexist backdoor paths between the target labels and non-causal latent factors through the task\nvariable T.\". But when we are doing a task, the task variable is given. And in Fig.1, we have $L_i \\perp L_j|T$. So, there must be a problem arising from either the claim itself or the causal graph. Considering question Q1, I think it is more likely the latter. I would appreciate it if the authors could provide a specific example to illustrate how the non-causal factors can correlate with $Y_t$ (e.g., the simplest text classification). This is one of the most important questions for me, and I'm looking forward to the authors' response.\n\nQ4. Following the above question, even if $T$ can act as a confounder and spurious correlations exist in the data, how does the traditional training process make the model learn the spurious correlations? This problem is also untouched, which makes the motivation informal and weak.\n\nQ5. It seems that the only confounder in this paper is the task $T$, I can hardly come up with any applications where different features are caused by the task but don't cause the target label.\n\nQ6. Section 3.2 is really hard to follow. How to alleviate the influence of the spurious correlations? By backdoor adjustment or introducing intervened data?\n\nQ7. The definition of identifiability is confusing. What's the identifiability of a SCM? When we say the identifiability, it usually refers to the causal quantity on two specific variables rather than a SCM. Also, the target of the UIC loss in Eq.6 is vague. What causal quantity do you want to measure?\n\nQ8. I guess the authors want to say that, if $L_i$ is identifiable, then it is the causal factor of $Y_t$. So, the UIC loss is used to promise the identifiability. There are two issues need to be addressed. One is why identifiable factors are causal ones, and another is why UIC can select identifiable factors. I'm not clear whether the second problem has been addressed by Section 3.2, since I think it's not well orangized. Nevertheless, I think the first problem is untouched.\n\nQ9. I'm not sure whether the baselines are advanced enough. The paper only compares with two simple baselines.", "labeling_timestamp": "2026-01-11T17:11:05.888488", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper presents the causal graph (Figure 1) in the Introduction before it formalizes the structural (generating) functions; Equation 1 and the discussion of structural functions only appear later in Section 3.1, so the paper does not specify the underlying structural functions prior to showing the causal graph.", "evidence": "1) \"The causal graph induced by the meta-SCM is depicted on the left in Figure 1.\" 2) \"Structural causal models characterize causal relationships between variables through a set of structural functions, which we denote as generating processes in the remainder of the paper. And these structural functions induce a corresponding causal graph. Based on the above causal relations, we formalize the generating process for each variable. ... Now we formalise the generating process for a given task as follow:\" 3) \"In Equation 1, Pa ( · ) denotes the set of parent nodes in the causal graph...\"", "section": "Introduction (Figure 1) and Section 3.1"}
{"claim": "The work relies on intuitive causal graphs without formally defining structural functions, making the causal claims informal and unreliable.", "claim_type": "methodology", "paper_id": "lWXedJyLuL", "paper_title": "A Unified Causal View of Instruction Tuning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "f6n4G0QZIH", "reviewer": "Reviewer_nrtp", "review_text": "Summary: This paper tries to learn task-required causal factors and only use those to make predictions for a given task. The motivation is claimed to be that the causal factors of different tasks are spuriously correlated through the task variable $T$. The authors theoretically prove the causal factor can be identified without mixing information from others, then they propose a Structural Instruction Tuning (SIT) method to learn the task-required causal representations that can mimic the causal factors for each task.\n\nStrengths: 1. Exploring causal LLMs is very important for both research and applications.\n\n2. This is a very novel work in this direction.\n\nWeaknesses: 1. The organization of this paper is very hard to follow.\n\n2. The technical quality is poor. There are serious technical flaws in this paper. See the questions below.\n\nQuestions: Q1. SCMs should be constructed from structual functions. Intuitive causal graphs are very informal and not relible. Equation 1 is not enough, because the structual functions should appear before the graphs. I think it would be better to illustrate Fig 1 with a specific example (i.e., what $L_i, X, Y$ and $T$ stands for in a specific task, and how the non-causal factors correlate with $Y_t$).\n\nQ2. The authors use different $L_i$ for different causal factors, and this is good. Nevertheless, I think $Y_t$ in Fig.1 should also be notated as different nodes for different $t$. \n\nQ3. The authors claim that \"Causally speaking, there\nexist backdoor paths between the target labels and non-causal latent factors through the task\nvariable T.\". But when we are doing a task, the task variable is given. And in Fig.1, we have $L_i \\perp L_j|T$. So, there must be a problem arising from either the claim itself or the causal graph. Considering question Q1, I think it is more likely the latter. I would appreciate it if the authors could provide a specific example to illustrate how the non-causal factors can correlate with $Y_t$ (e.g., the simplest text classification). This is one of the most important questions for me, and I'm looking forward to the authors' response.\n\nQ4. Following the above question, even if $T$ can act as a confounder and spurious correlations exist in the data, how does the traditional training process make the model learn the spurious correlations? This problem is also untouched, which makes the motivation informal and weak.\n\nQ5. It seems that the only confounder in this paper is the task $T$, I can hardly come up with any applications where different features are caused by the task but don't cause the target label.\n\nQ6. Section 3.2 is really hard to follow. How to alleviate the influence of the spurious correlations? By backdoor adjustment or introducing intervened data?\n\nQ7. The definition of identifiability is confusing. What's the identifiability of a SCM? When we say the identifiability, it usually refers to the causal quantity on two specific variables rather than a SCM. Also, the target of the UIC loss in Eq.6 is vague. What causal quantity do you want to measure?\n\nQ8. I guess the authors want to say that, if $L_i$ is identifiable, then it is the causal factor of $Y_t$. So, the UIC loss is used to promise the identifiability. There are two issues need to be addressed. One is why identifiable factors are causal ones, and another is why UIC can select identifiable factors. I'm not clear whether the second problem has been addressed by Section 3.2, since I think it's not well orangized. Nevertheless, I think the first problem is untouched.\n\nQ9. I'm not sure whether the baselines are advanced enough. The paper only compares with two simple baselines.", "labeling_timestamp": "2026-01-11T17:11:01.668727", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states and formalizes structural functions (as generating processes), introduces an Equation 1 for the data-generating process, specifies it is an additive noise model (ANM), and assumes priors for latent factors (exponential family). Thus the work does more than rely on intuitive graphs and does formally define structural functions.", "evidence": "“Structural causal models characterize causal relationships between variables through a set of structural functions, which we denote as generating processes in the remainder of the paper. And these structural functions induce a corresponding causal graph. Based on the above causal relations, we formalize the generating process for each variable.”\n\n“Now we formalise the generating process for a given task as follow: ... In Equation 1, Pa(·) denotes the set of parent nodes in the causal graph, indicating that only taskrequired latent factors will be selected. The symbol := is often used in SCMs to emphasize that the formula represents a data generating process rather than a simple mathematical equation. Equation 1 is categorized as an additive noise model (ANM), which is a widely used type of SCM.”\n\n“For latent factors L_i, considering that exponential family distribution has universal approximation capability for a given distribution, we assume the prior of latent factors p_{L_i}(L_i = l_i | D = d) is given by :”", "section": "3.1 META STRUCTURAL CAUSAL MODEL"}
{"claim": "Figure 1 lacks a concrete example mapping L_i, X, Y, and T to a specific task to clarify what each variable represents in practice.", "claim_type": "presentation", "paper_id": "lWXedJyLuL", "paper_title": "A Unified Causal View of Instruction Tuning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "f6n4G0QZIH", "reviewer": "Reviewer_nrtp", "review_text": "Summary: This paper tries to learn task-required causal factors and only use those to make predictions for a given task. The motivation is claimed to be that the causal factors of different tasks are spuriously correlated through the task variable $T$. The authors theoretically prove the causal factor can be identified without mixing information from others, then they propose a Structural Instruction Tuning (SIT) method to learn the task-required causal representations that can mimic the causal factors for each task.\n\nStrengths: 1. Exploring causal LLMs is very important for both research and applications.\n\n2. This is a very novel work in this direction.\n\nWeaknesses: 1. The organization of this paper is very hard to follow.\n\n2. The technical quality is poor. There are serious technical flaws in this paper. See the questions below.\n\nQuestions: Q1. SCMs should be constructed from structual functions. Intuitive causal graphs are very informal and not relible. Equation 1 is not enough, because the structual functions should appear before the graphs. I think it would be better to illustrate Fig 1 with a specific example (i.e., what $L_i, X, Y$ and $T$ stands for in a specific task, and how the non-causal factors correlate with $Y_t$).\n\nQ2. The authors use different $L_i$ for different causal factors, and this is good. Nevertheless, I think $Y_t$ in Fig.1 should also be notated as different nodes for different $t$. \n\nQ3. The authors claim that \"Causally speaking, there\nexist backdoor paths between the target labels and non-causal latent factors through the task\nvariable T.\". But when we are doing a task, the task variable is given. And in Fig.1, we have $L_i \\perp L_j|T$. So, there must be a problem arising from either the claim itself or the causal graph. Considering question Q1, I think it is more likely the latter. I would appreciate it if the authors could provide a specific example to illustrate how the non-causal factors can correlate with $Y_t$ (e.g., the simplest text classification). This is one of the most important questions for me, and I'm looking forward to the authors' response.\n\nQ4. Following the above question, even if $T$ can act as a confounder and spurious correlations exist in the data, how does the traditional training process make the model learn the spurious correlations? This problem is also untouched, which makes the motivation informal and weak.\n\nQ5. It seems that the only confounder in this paper is the task $T$, I can hardly come up with any applications where different features are caused by the task but don't cause the target label.\n\nQ6. Section 3.2 is really hard to follow. How to alleviate the influence of the spurious correlations? By backdoor adjustment or introducing intervened data?\n\nQ7. The definition of identifiability is confusing. What's the identifiability of a SCM? When we say the identifiability, it usually refers to the causal quantity on two specific variables rather than a SCM. Also, the target of the UIC loss in Eq.6 is vague. What causal quantity do you want to measure?\n\nQ8. I guess the authors want to say that, if $L_i$ is identifiable, then it is the causal factor of $Y_t$. So, the UIC loss is used to promise the identifiability. There are two issues need to be addressed. One is why identifiable factors are causal ones, and another is why UIC can select identifiable factors. I'm not clear whether the second problem has been addressed by Section 3.2, since I think it's not well orangized. Nevertheless, I think the first problem is untouched.\n\nQ9. I'm not sure whether the baselines are advanced enough. The paper only compares with two simple baselines.", "labeling_timestamp": "2026-01-11T17:10:50.583401", "model": "gpt-5-mini", "label": "Supported", "justification": "Figure 1 (its caption) presents a generic causal graph and explicitly refers the reader to Section 3.1 for details; the paper provides concrete mappings (e.g., X_t/Y_t and examples of L_i) in Section 3.1 rather than in the figure itself, so the claim that Figure 1 lacks a concrete example mapping is accurate.", "evidence": "Figure 1 caption: \"Left : The causal graph induced by the meta Structural Causal Model (meta-SCM) for integrating different NLP tasks. White nodes denote observed variables and grey nodes denote unobserved variables. Dashed lines denote edges that may be absent, while solid lines denote invariant processes. See Section 3.1 for a detailed description.\"; Section 3.1: \"X_t, Y_t represent the source context and target label respectively for NLP tasks, where subscript t denotes a specific task, such as documents and summaries, or sentences and sentiment polarity.\"; Section 1 (examples): \"For example, the connotative semantics in a document are the causal factors for sentiment analysis, while the core denotative semantics are the causal factors for topic classification.\"", "section": "Figure 1 caption; Section 3.1; Section 1"}
{"claim": "In Figure 1, representing Y_t as a single node is misleading; Y should be notated as distinct nodes for different tasks t.", "claim_type": "presentation", "paper_id": "lWXedJyLuL", "paper_title": "A Unified Causal View of Instruction Tuning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "f6n4G0QZIH", "reviewer": "Reviewer_nrtp", "review_text": "Summary: This paper tries to learn task-required causal factors and only use those to make predictions for a given task. The motivation is claimed to be that the causal factors of different tasks are spuriously correlated through the task variable $T$. The authors theoretically prove the causal factor can be identified without mixing information from others, then they propose a Structural Instruction Tuning (SIT) method to learn the task-required causal representations that can mimic the causal factors for each task.\n\nStrengths: 1. Exploring causal LLMs is very important for both research and applications.\n\n2. This is a very novel work in this direction.\n\nWeaknesses: 1. The organization of this paper is very hard to follow.\n\n2. The technical quality is poor. There are serious technical flaws in this paper. See the questions below.\n\nQuestions: Q1. SCMs should be constructed from structual functions. Intuitive causal graphs are very informal and not relible. Equation 1 is not enough, because the structual functions should appear before the graphs. I think it would be better to illustrate Fig 1 with a specific example (i.e., what $L_i, X, Y$ and $T$ stands for in a specific task, and how the non-causal factors correlate with $Y_t$).\n\nQ2. The authors use different $L_i$ for different causal factors, and this is good. Nevertheless, I think $Y_t$ in Fig.1 should also be notated as different nodes for different $t$. \n\nQ3. The authors claim that \"Causally speaking, there\nexist backdoor paths between the target labels and non-causal latent factors through the task\nvariable T.\". But when we are doing a task, the task variable is given. And in Fig.1, we have $L_i \\perp L_j|T$. So, there must be a problem arising from either the claim itself or the causal graph. Considering question Q1, I think it is more likely the latter. I would appreciate it if the authors could provide a specific example to illustrate how the non-causal factors can correlate with $Y_t$ (e.g., the simplest text classification). This is one of the most important questions for me, and I'm looking forward to the authors' response.\n\nQ4. Following the above question, even if $T$ can act as a confounder and spurious correlations exist in the data, how does the traditional training process make the model learn the spurious correlations? This problem is also untouched, which makes the motivation informal and weak.\n\nQ5. It seems that the only confounder in this paper is the task $T$, I can hardly come up with any applications where different features are caused by the task but don't cause the target label.\n\nQ6. Section 3.2 is really hard to follow. How to alleviate the influence of the spurious correlations? By backdoor adjustment or introducing intervened data?\n\nQ7. The definition of identifiability is confusing. What's the identifiability of a SCM? When we say the identifiability, it usually refers to the causal quantity on two specific variables rather than a SCM. Also, the target of the UIC loss in Eq.6 is vague. What causal quantity do you want to measure?\n\nQ8. I guess the authors want to say that, if $L_i$ is identifiable, then it is the causal factor of $Y_t$. So, the UIC loss is used to promise the identifiability. There are two issues need to be addressed. One is why identifiable factors are causal ones, and another is why UIC can select identifiable factors. I'm not clear whether the second problem has been addressed by Section 3.2, since I think it's not well orangized. Nevertheless, I think the first problem is untouched.\n\nQ9. I'm not sure whether the baselines are advanced enough. The paper only compares with two simple baselines.", "labeling_timestamp": "2026-01-11T17:10:53.134604", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly models X_t and Y_t with a subscript t to denote per-task variables and describes connections to Y_t per task (e.g., dashed lines for task-specific latent factor links). Thus it treats Y as task-specific (Y_t), contradicting the claim that Figure 1 represents Y as a single, non-task-specific node.", "evidence": "\"X_t, Y_t represent the source context and target label respectively for NLP tasks, where subscript t denotes a specific task, such as documents and summaries, or sentences and sentiment polarity.\" \n\n\"L → X_t , Y_t indicates that the source context X_t and target label Y_t are generated from the latent factors L . Differently, not all latent factors are used to generate target label Y_t for a certain task. Consequently, dashed lines are employed to signify the uncertain connection from these latent factors to the target labels Y_t .\"", "section": "3.1 META STRUCTURAL CAUSAL MODEL"}
{"claim": "If the task variable T is observed during task execution, it cannot plausibly act as a backdoor confounder producing spurious correlations as claimed.", "claim_type": "methodology", "paper_id": "lWXedJyLuL", "paper_title": "A Unified Causal View of Instruction Tuning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "f6n4G0QZIH", "reviewer": "Reviewer_nrtp", "review_text": "Summary: This paper tries to learn task-required causal factors and only use those to make predictions for a given task. The motivation is claimed to be that the causal factors of different tasks are spuriously correlated through the task variable $T$. The authors theoretically prove the causal factor can be identified without mixing information from others, then they propose a Structural Instruction Tuning (SIT) method to learn the task-required causal representations that can mimic the causal factors for each task.\n\nStrengths: 1. Exploring causal LLMs is very important for both research and applications.\n\n2. This is a very novel work in this direction.\n\nWeaknesses: 1. The organization of this paper is very hard to follow.\n\n2. The technical quality is poor. There are serious technical flaws in this paper. See the questions below.\n\nQuestions: Q1. SCMs should be constructed from structual functions. Intuitive causal graphs are very informal and not relible. Equation 1 is not enough, because the structual functions should appear before the graphs. I think it would be better to illustrate Fig 1 with a specific example (i.e., what $L_i, X, Y$ and $T$ stands for in a specific task, and how the non-causal factors correlate with $Y_t$).\n\nQ2. The authors use different $L_i$ for different causal factors, and this is good. Nevertheless, I think $Y_t$ in Fig.1 should also be notated as different nodes for different $t$. \n\nQ3. The authors claim that \"Causally speaking, there\nexist backdoor paths between the target labels and non-causal latent factors through the task\nvariable T.\". But when we are doing a task, the task variable is given. And in Fig.1, we have $L_i \\perp L_j|T$. So, there must be a problem arising from either the claim itself or the causal graph. Considering question Q1, I think it is more likely the latter. I would appreciate it if the authors could provide a specific example to illustrate how the non-causal factors can correlate with $Y_t$ (e.g., the simplest text classification). This is one of the most important questions for me, and I'm looking forward to the authors' response.\n\nQ4. Following the above question, even if $T$ can act as a confounder and spurious correlations exist in the data, how does the traditional training process make the model learn the spurious correlations? This problem is also untouched, which makes the motivation informal and weak.\n\nQ5. It seems that the only confounder in this paper is the task $T$, I can hardly come up with any applications where different features are caused by the task but don't cause the target label.\n\nQ6. Section 3.2 is really hard to follow. How to alleviate the influence of the spurious correlations? By backdoor adjustment or introducing intervened data?\n\nQ7. The definition of identifiability is confusing. What's the identifiability of a SCM? When we say the identifiability, it usually refers to the causal quantity on two specific variables rather than a SCM. Also, the target of the UIC loss in Eq.6 is vague. What causal quantity do you want to measure?\n\nQ8. I guess the authors want to say that, if $L_i$ is identifiable, then it is the causal factor of $Y_t$. So, the UIC loss is used to promise the identifiability. There are two issues need to be addressed. One is why identifiable factors are causal ones, and another is why UIC can select identifiable factors. I'm not clear whether the second problem has been addressed by Section 3.2, since I think it's not well orangized. Nevertheless, I think the first problem is untouched.\n\nQ9. I'm not sure whether the baselines are advanced enough. The paper only compares with two simple baselines.", "labeling_timestamp": "2026-01-11T17:11:25.770781", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper attributes spurious correlations to inherent dataset properties (D) influencing latent factors (L), and treats T as an observed task variable used to guide causal-factor selection. It does not claim that the observed task variable T serves as a backdoor confounder producing spurious correlations.", "evidence": "1) \"As later analyzed, inherent dataset properties cause traditional methods to learn spurious correlations.\"  2) \"T = { t 1 , t 2 , t 3 , · · · , t m } represents m different NLP tasks.\"  3) \"D → L indicates the abstract language properties are influenced by the inherent dataset properties of different datasets across tasks.\"  4) \"Task information based on prompts guides the causal factor selection.\"", "section": "1 INTRODUCTION; 3.1 META STRUCTURAL CAUSAL MODEL; Figure 1 description"}
{"claim": "The conditional independence L_i ⟂ L_j | T in Figure 1 appears to contradict the paper's claim that non-causal latent factors correlate with Y_t via T.", "claim_type": "methodology", "paper_id": "lWXedJyLuL", "paper_title": "A Unified Causal View of Instruction Tuning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "f6n4G0QZIH", "reviewer": "Reviewer_nrtp", "review_text": "Summary: This paper tries to learn task-required causal factors and only use those to make predictions for a given task. The motivation is claimed to be that the causal factors of different tasks are spuriously correlated through the task variable $T$. The authors theoretically prove the causal factor can be identified without mixing information from others, then they propose a Structural Instruction Tuning (SIT) method to learn the task-required causal representations that can mimic the causal factors for each task.\n\nStrengths: 1. Exploring causal LLMs is very important for both research and applications.\n\n2. This is a very novel work in this direction.\n\nWeaknesses: 1. The organization of this paper is very hard to follow.\n\n2. The technical quality is poor. There are serious technical flaws in this paper. See the questions below.\n\nQuestions: Q1. SCMs should be constructed from structual functions. Intuitive causal graphs are very informal and not relible. Equation 1 is not enough, because the structual functions should appear before the graphs. I think it would be better to illustrate Fig 1 with a specific example (i.e., what $L_i, X, Y$ and $T$ stands for in a specific task, and how the non-causal factors correlate with $Y_t$).\n\nQ2. The authors use different $L_i$ for different causal factors, and this is good. Nevertheless, I think $Y_t$ in Fig.1 should also be notated as different nodes for different $t$. \n\nQ3. The authors claim that \"Causally speaking, there\nexist backdoor paths between the target labels and non-causal latent factors through the task\nvariable T.\". But when we are doing a task, the task variable is given. And in Fig.1, we have $L_i \\perp L_j|T$. So, there must be a problem arising from either the claim itself or the causal graph. Considering question Q1, I think it is more likely the latter. I would appreciate it if the authors could provide a specific example to illustrate how the non-causal factors can correlate with $Y_t$ (e.g., the simplest text classification). This is one of the most important questions for me, and I'm looking forward to the authors' response.\n\nQ4. Following the above question, even if $T$ can act as a confounder and spurious correlations exist in the data, how does the traditional training process make the model learn the spurious correlations? This problem is also untouched, which makes the motivation informal and weak.\n\nQ5. It seems that the only confounder in this paper is the task $T$, I can hardly come up with any applications where different features are caused by the task but don't cause the target label.\n\nQ6. Section 3.2 is really hard to follow. How to alleviate the influence of the spurious correlations? By backdoor adjustment or introducing intervened data?\n\nQ7. The definition of identifiability is confusing. What's the identifiability of a SCM? When we say the identifiability, it usually refers to the causal quantity on two specific variables rather than a SCM. Also, the target of the UIC loss in Eq.6 is vague. What causal quantity do you want to measure?\n\nQ8. I guess the authors want to say that, if $L_i$ is identifiable, then it is the causal factor of $Y_t$. So, the UIC loss is used to promise the identifiability. There are two issues need to be addressed. One is why identifiable factors are causal ones, and another is why UIC can select identifiable factors. I'm not clear whether the second problem has been addressed by Section 3.2, since I think it's not well orangized. Nevertheless, I think the first problem is untouched.\n\nQ9. I'm not sure whether the baselines are advanced enough. The paper only compares with two simple baselines.", "labeling_timestamp": "2026-01-11T17:11:28.563551", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper text does not state or show an explicit conditional independence L_i ⟂ L_j | T; the description instead emphasizes that latent factors L are influenced by dataset properties D and that only some latent factors causally generate Y_t depending on the task. The excerpt does not provide the Figure 1 graphical detail or any explicit conditional-independence claim conditioned on T, so there is insufficient evidence in the paper content shown to confirm the reviewer’s asserted contradiction.", "evidence": "“L = { L1, L2, L3, ..., Ln } represents the abstract properties of language which are shared across different tasks. They are unobservable latent factors.”\n\n“D → L indicates the abstract language properties are influenced by the inherent dataset properties of different datasets across tasks. For instance, news summarization datasets may focus on denotative semantics, while sentiment analysis datasets prioritize connotative elements.”\n\n“Differently, not all latent factors are used to generate target label Yt for a certain task. Consequently, dashed lines are employed to signify the uncertain connection from these latent factors to the target labels Yt.”\n\n“For latent factors Li, considering that exponential family distribution has universal approximation capability for a given distribution, we assume the prior of latent factors pLi(Li = li | D = d) is given by :”", "section": "3.1 META STRUCTURAL CAUSAL MODEL"}
{"claim": "The authors do not provide a specific concrete example (for instance, a simple text classification) showing how non-causal factors become correlated with Y_t.", "claim_type": "methodology", "paper_id": "lWXedJyLuL", "paper_title": "A Unified Causal View of Instruction Tuning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "f6n4G0QZIH", "reviewer": "Reviewer_nrtp", "review_text": "Summary: This paper tries to learn task-required causal factors and only use those to make predictions for a given task. The motivation is claimed to be that the causal factors of different tasks are spuriously correlated through the task variable $T$. The authors theoretically prove the causal factor can be identified without mixing information from others, then they propose a Structural Instruction Tuning (SIT) method to learn the task-required causal representations that can mimic the causal factors for each task.\n\nStrengths: 1. Exploring causal LLMs is very important for both research and applications.\n\n2. This is a very novel work in this direction.\n\nWeaknesses: 1. The organization of this paper is very hard to follow.\n\n2. The technical quality is poor. There are serious technical flaws in this paper. See the questions below.\n\nQuestions: Q1. SCMs should be constructed from structual functions. Intuitive causal graphs are very informal and not relible. Equation 1 is not enough, because the structual functions should appear before the graphs. I think it would be better to illustrate Fig 1 with a specific example (i.e., what $L_i, X, Y$ and $T$ stands for in a specific task, and how the non-causal factors correlate with $Y_t$).\n\nQ2. The authors use different $L_i$ for different causal factors, and this is good. Nevertheless, I think $Y_t$ in Fig.1 should also be notated as different nodes for different $t$. \n\nQ3. The authors claim that \"Causally speaking, there\nexist backdoor paths between the target labels and non-causal latent factors through the task\nvariable T.\". But when we are doing a task, the task variable is given. And in Fig.1, we have $L_i \\perp L_j|T$. So, there must be a problem arising from either the claim itself or the causal graph. Considering question Q1, I think it is more likely the latter. I would appreciate it if the authors could provide a specific example to illustrate how the non-causal factors can correlate with $Y_t$ (e.g., the simplest text classification). This is one of the most important questions for me, and I'm looking forward to the authors' response.\n\nQ4. Following the above question, even if $T$ can act as a confounder and spurious correlations exist in the data, how does the traditional training process make the model learn the spurious correlations? This problem is also untouched, which makes the motivation informal and weak.\n\nQ5. It seems that the only confounder in this paper is the task $T$, I can hardly come up with any applications where different features are caused by the task but don't cause the target label.\n\nQ6. Section 3.2 is really hard to follow. How to alleviate the influence of the spurious correlations? By backdoor adjustment or introducing intervened data?\n\nQ7. The definition of identifiability is confusing. What's the identifiability of a SCM? When we say the identifiability, it usually refers to the causal quantity on two specific variables rather than a SCM. Also, the target of the UIC loss in Eq.6 is vague. What causal quantity do you want to measure?\n\nQ8. I guess the authors want to say that, if $L_i$ is identifiable, then it is the causal factor of $Y_t$. So, the UIC loss is used to promise the identifiability. There are two issues need to be addressed. One is why identifiable factors are causal ones, and another is why UIC can select identifiable factors. I'm not clear whether the second problem has been addressed by Section 3.2, since I think it's not well orangized. Nevertheless, I think the first problem is untouched.\n\nQ9. I'm not sure whether the baselines are advanced enough. The paper only compares with two simple baselines.", "labeling_timestamp": "2026-01-11T17:11:21.373197", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper discusses spurious correlations and gives high-level, conceptual examples (e.g., connotative vs. denotative semantics) but does not present a specific concrete example or a simple text-classification demonstration that shows how non-causal factors become correlated with Y_t.", "evidence": "1) \"As later analyzed, inherent dataset properties cause traditional methods to learn spurious correlations.\" 2) \"For example, the connotative semantics in a document are the causal factors for sentiment analysis, while the core denotative semantics are the causal factors for topic classification.\" 3) \"L → X_t, Y_t indicates that the source context X_t and target label Y_t are generated from the latent factors L. Considering that source context X_t carries all the information of L, X_t is pointed by all of L. Differently, not all latent factors are used to generate target label Y_t for a certain task.\"", "section": "1 INTRODUCTION; 3.1 META STRUCTURAL CAUSAL MODEL"}
{"claim": "The paper fails to explain how standard training procedures cause models to learn spurious correlations arising from the task confounder T.", "claim_type": "methodology", "paper_id": "lWXedJyLuL", "paper_title": "A Unified Causal View of Instruction Tuning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "f6n4G0QZIH", "reviewer": "Reviewer_nrtp", "review_text": "Summary: This paper tries to learn task-required causal factors and only use those to make predictions for a given task. The motivation is claimed to be that the causal factors of different tasks are spuriously correlated through the task variable $T$. The authors theoretically prove the causal factor can be identified without mixing information from others, then they propose a Structural Instruction Tuning (SIT) method to learn the task-required causal representations that can mimic the causal factors for each task.\n\nStrengths: 1. Exploring causal LLMs is very important for both research and applications.\n\n2. This is a very novel work in this direction.\n\nWeaknesses: 1. The organization of this paper is very hard to follow.\n\n2. The technical quality is poor. There are serious technical flaws in this paper. See the questions below.\n\nQuestions: Q1. SCMs should be constructed from structual functions. Intuitive causal graphs are very informal and not relible. Equation 1 is not enough, because the structual functions should appear before the graphs. I think it would be better to illustrate Fig 1 with a specific example (i.e., what $L_i, X, Y$ and $T$ stands for in a specific task, and how the non-causal factors correlate with $Y_t$).\n\nQ2. The authors use different $L_i$ for different causal factors, and this is good. Nevertheless, I think $Y_t$ in Fig.1 should also be notated as different nodes for different $t$. \n\nQ3. The authors claim that \"Causally speaking, there\nexist backdoor paths between the target labels and non-causal latent factors through the task\nvariable T.\". But when we are doing a task, the task variable is given. And in Fig.1, we have $L_i \\perp L_j|T$. So, there must be a problem arising from either the claim itself or the causal graph. Considering question Q1, I think it is more likely the latter. I would appreciate it if the authors could provide a specific example to illustrate how the non-causal factors can correlate with $Y_t$ (e.g., the simplest text classification). This is one of the most important questions for me, and I'm looking forward to the authors' response.\n\nQ4. Following the above question, even if $T$ can act as a confounder and spurious correlations exist in the data, how does the traditional training process make the model learn the spurious correlations? This problem is also untouched, which makes the motivation informal and weak.\n\nQ5. It seems that the only confounder in this paper is the task $T$, I can hardly come up with any applications where different features are caused by the task but don't cause the target label.\n\nQ6. Section 3.2 is really hard to follow. How to alleviate the influence of the spurious correlations? By backdoor adjustment or introducing intervened data?\n\nQ7. The definition of identifiability is confusing. What's the identifiability of a SCM? When we say the identifiability, it usually refers to the causal quantity on two specific variables rather than a SCM. Also, the target of the UIC loss in Eq.6 is vague. What causal quantity do you want to measure?\n\nQ8. I guess the authors want to say that, if $L_i$ is identifiable, then it is the causal factor of $Y_t$. So, the UIC loss is used to promise the identifiability. There are two issues need to be addressed. One is why identifiable factors are causal ones, and another is why UIC can select identifiable factors. I'm not clear whether the second problem has been addressed by Section 3.2, since I think it's not well orangized. Nevertheless, I think the first problem is untouched.\n\nQ9. I'm not sure whether the baselines are advanced enough. The paper only compares with two simple baselines.", "labeling_timestamp": "2026-01-11T17:11:39.287817", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper states that traditional/instruction-tuning methods exploit surface correlations and that inherent dataset properties (D) induce spurious correlations via latent factors L, but it does not provide a detailed account of *how* standard training procedures (e.g., optimization dynamics, loss objectives) cause models to learn those spurious correlations from the task variable T. Thus the claim that the paper 'fails to explain how standard training procedures cause' this is partially true: the paper attributes spurious correlations to dataset/task properties but lacks a mechanistic explanation of standard training procedures producing them.", "evidence": "\"existing methods often learn features that exhibit correlations between instruction-formatted samples and target labels, rather than causal relationships.\"", "section": "Abstract"}
{"claim": "Assuming the only confounder is the task variable T appears unrealistic, and the paper does not justify the applicability of this assumption to real tasks.", "claim_type": "methodology", "paper_id": "lWXedJyLuL", "paper_title": "A Unified Causal View of Instruction Tuning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "f6n4G0QZIH", "reviewer": "Reviewer_nrtp", "review_text": "Summary: This paper tries to learn task-required causal factors and only use those to make predictions for a given task. The motivation is claimed to be that the causal factors of different tasks are spuriously correlated through the task variable $T$. The authors theoretically prove the causal factor can be identified without mixing information from others, then they propose a Structural Instruction Tuning (SIT) method to learn the task-required causal representations that can mimic the causal factors for each task.\n\nStrengths: 1. Exploring causal LLMs is very important for both research and applications.\n\n2. This is a very novel work in this direction.\n\nWeaknesses: 1. The organization of this paper is very hard to follow.\n\n2. The technical quality is poor. There are serious technical flaws in this paper. See the questions below.\n\nQuestions: Q1. SCMs should be constructed from structual functions. Intuitive causal graphs are very informal and not relible. Equation 1 is not enough, because the structual functions should appear before the graphs. I think it would be better to illustrate Fig 1 with a specific example (i.e., what $L_i, X, Y$ and $T$ stands for in a specific task, and how the non-causal factors correlate with $Y_t$).\n\nQ2. The authors use different $L_i$ for different causal factors, and this is good. Nevertheless, I think $Y_t$ in Fig.1 should also be notated as different nodes for different $t$. \n\nQ3. The authors claim that \"Causally speaking, there\nexist backdoor paths between the target labels and non-causal latent factors through the task\nvariable T.\". But when we are doing a task, the task variable is given. And in Fig.1, we have $L_i \\perp L_j|T$. So, there must be a problem arising from either the claim itself or the causal graph. Considering question Q1, I think it is more likely the latter. I would appreciate it if the authors could provide a specific example to illustrate how the non-causal factors can correlate with $Y_t$ (e.g., the simplest text classification). This is one of the most important questions for me, and I'm looking forward to the authors' response.\n\nQ4. Following the above question, even if $T$ can act as a confounder and spurious correlations exist in the data, how does the traditional training process make the model learn the spurious correlations? This problem is also untouched, which makes the motivation informal and weak.\n\nQ5. It seems that the only confounder in this paper is the task $T$, I can hardly come up with any applications where different features are caused by the task but don't cause the target label.\n\nQ6. Section 3.2 is really hard to follow. How to alleviate the influence of the spurious correlations? By backdoor adjustment or introducing intervened data?\n\nQ7. The definition of identifiability is confusing. What's the identifiability of a SCM? When we say the identifiability, it usually refers to the causal quantity on two specific variables rather than a SCM. Also, the target of the UIC loss in Eq.6 is vague. What causal quantity do you want to measure?\n\nQ8. I guess the authors want to say that, if $L_i$ is identifiable, then it is the causal factor of $Y_t$. So, the UIC loss is used to promise the identifiability. There are two issues need to be addressed. One is why identifiable factors are causal ones, and another is why UIC can select identifiable factors. I'm not clear whether the second problem has been addressed by Section 3.2, since I think it's not well orangized. Nevertheless, I think the first problem is untouched.\n\nQ9. I'm not sure whether the baselines are advanced enough. The paper only compares with two simple baselines.", "labeling_timestamp": "2026-01-11T17:11:45.328029", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper does not assume the only confounder is the task variable T: it explicitly models inherent dataset properties D (with D → L) affecting latent factors L. It also provides justification of applicability via the Uniform Identifiability Condition (UIC) claiming applicability to a wide range of SCM topologies.", "evidence": "1) \"D represents inherent dataset properties.\" \n\n2) \"D → L indicates the abstract language properties are influenced by the inherent dataset properties of different datasets across tasks.\" \n\n3) \"On the theoretical side, we present the Uniform Identifiability Condition (UIC), a sufficient and necessary condition to ensure identifiability of latent factors in the meta-SCM. ... Importantly, this theoretical result is applicable to a wide range of SCMs with certain topological structures...\"", "section": "3.1 META STRUCTURAL CAUSAL MODEL; Introduction / Abstract"}
{"claim": "Section 3.2 is unclear about whether the method alleviates spurious correlations via backdoor adjustment, intervened data, or another mechanism.", "claim_type": "presentation", "paper_id": "lWXedJyLuL", "paper_title": "A Unified Causal View of Instruction Tuning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "f6n4G0QZIH", "reviewer": "Reviewer_nrtp", "review_text": "Summary: This paper tries to learn task-required causal factors and only use those to make predictions for a given task. The motivation is claimed to be that the causal factors of different tasks are spuriously correlated through the task variable $T$. The authors theoretically prove the causal factor can be identified without mixing information from others, then they propose a Structural Instruction Tuning (SIT) method to learn the task-required causal representations that can mimic the causal factors for each task.\n\nStrengths: 1. Exploring causal LLMs is very important for both research and applications.\n\n2. This is a very novel work in this direction.\n\nWeaknesses: 1. The organization of this paper is very hard to follow.\n\n2. The technical quality is poor. There are serious technical flaws in this paper. See the questions below.\n\nQuestions: Q1. SCMs should be constructed from structual functions. Intuitive causal graphs are very informal and not relible. Equation 1 is not enough, because the structual functions should appear before the graphs. I think it would be better to illustrate Fig 1 with a specific example (i.e., what $L_i, X, Y$ and $T$ stands for in a specific task, and how the non-causal factors correlate with $Y_t$).\n\nQ2. The authors use different $L_i$ for different causal factors, and this is good. Nevertheless, I think $Y_t$ in Fig.1 should also be notated as different nodes for different $t$. \n\nQ3. The authors claim that \"Causally speaking, there\nexist backdoor paths between the target labels and non-causal latent factors through the task\nvariable T.\". But when we are doing a task, the task variable is given. And in Fig.1, we have $L_i \\perp L_j|T$. So, there must be a problem arising from either the claim itself or the causal graph. Considering question Q1, I think it is more likely the latter. I would appreciate it if the authors could provide a specific example to illustrate how the non-causal factors can correlate with $Y_t$ (e.g., the simplest text classification). This is one of the most important questions for me, and I'm looking forward to the authors' response.\n\nQ4. Following the above question, even if $T$ can act as a confounder and spurious correlations exist in the data, how does the traditional training process make the model learn the spurious correlations? This problem is also untouched, which makes the motivation informal and weak.\n\nQ5. It seems that the only confounder in this paper is the task $T$, I can hardly come up with any applications where different features are caused by the task but don't cause the target label.\n\nQ6. Section 3.2 is really hard to follow. How to alleviate the influence of the spurious correlations? By backdoor adjustment or introducing intervened data?\n\nQ7. The definition of identifiability is confusing. What's the identifiability of a SCM? When we say the identifiability, it usually refers to the causal quantity on two specific variables rather than a SCM. Also, the target of the UIC loss in Eq.6 is vague. What causal quantity do you want to measure?\n\nQ8. I guess the authors want to say that, if $L_i$ is identifiable, then it is the causal factor of $Y_t$. So, the UIC loss is used to promise the identifiability. There are two issues need to be addressed. One is why identifiable factors are causal ones, and another is why UIC can select identifiable factors. I'm not clear whether the second problem has been addressed by Section 3.2, since I think it's not well orangized. Nevertheless, I think the first problem is untouched.\n\nQ9. I'm not sure whether the baselines are advanced enough. The paper only compares with two simple baselines.", "labeling_timestamp": "2026-01-11T17:11:53.700638", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt includes Section 3 and 3.1 but does not include Section 3.2, so there is insufficient information to evaluate whether Section 3.2 is unclear about which mechanism (backdoor adjustment, intervened data, or another) is used to alleviate spurious correlations.", "evidence": "\"In this section, we formulate various NLP tasks within a single structural causal model, named meta structural causal model (meta-SCM). Furthermore, we propose a uniform identifiability condition (UIC) based on the topological structure of SCM, which guarantees that latent factors in the SCM can be identified without mixing information from others by fitting the observed data.\" (Section 3 intro). \"See Section 3.1 for a detailed description.\" (Figure 1 caption).", "section": "Section 3 (Section 3.1 present; Section 3.2 not included in provided content)"}
{"claim": "The paper's definition of identifiability is confusing and does not clearly state what object or quantity is meant to be identifiable.", "claim_type": "presentation", "paper_id": "lWXedJyLuL", "paper_title": "A Unified Causal View of Instruction Tuning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "f6n4G0QZIH", "reviewer": "Reviewer_nrtp", "review_text": "Summary: This paper tries to learn task-required causal factors and only use those to make predictions for a given task. The motivation is claimed to be that the causal factors of different tasks are spuriously correlated through the task variable $T$. The authors theoretically prove the causal factor can be identified without mixing information from others, then they propose a Structural Instruction Tuning (SIT) method to learn the task-required causal representations that can mimic the causal factors for each task.\n\nStrengths: 1. Exploring causal LLMs is very important for both research and applications.\n\n2. This is a very novel work in this direction.\n\nWeaknesses: 1. The organization of this paper is very hard to follow.\n\n2. The technical quality is poor. There are serious technical flaws in this paper. See the questions below.\n\nQuestions: Q1. SCMs should be constructed from structual functions. Intuitive causal graphs are very informal and not relible. Equation 1 is not enough, because the structual functions should appear before the graphs. I think it would be better to illustrate Fig 1 with a specific example (i.e., what $L_i, X, Y$ and $T$ stands for in a specific task, and how the non-causal factors correlate with $Y_t$).\n\nQ2. The authors use different $L_i$ for different causal factors, and this is good. Nevertheless, I think $Y_t$ in Fig.1 should also be notated as different nodes for different $t$. \n\nQ3. The authors claim that \"Causally speaking, there\nexist backdoor paths between the target labels and non-causal latent factors through the task\nvariable T.\". But when we are doing a task, the task variable is given. And in Fig.1, we have $L_i \\perp L_j|T$. So, there must be a problem arising from either the claim itself or the causal graph. Considering question Q1, I think it is more likely the latter. I would appreciate it if the authors could provide a specific example to illustrate how the non-causal factors can correlate with $Y_t$ (e.g., the simplest text classification). This is one of the most important questions for me, and I'm looking forward to the authors' response.\n\nQ4. Following the above question, even if $T$ can act as a confounder and spurious correlations exist in the data, how does the traditional training process make the model learn the spurious correlations? This problem is also untouched, which makes the motivation informal and weak.\n\nQ5. It seems that the only confounder in this paper is the task $T$, I can hardly come up with any applications where different features are caused by the task but don't cause the target label.\n\nQ6. Section 3.2 is really hard to follow. How to alleviate the influence of the spurious correlations? By backdoor adjustment or introducing intervened data?\n\nQ7. The definition of identifiability is confusing. What's the identifiability of a SCM? When we say the identifiability, it usually refers to the causal quantity on two specific variables rather than a SCM. Also, the target of the UIC loss in Eq.6 is vague. What causal quantity do you want to measure?\n\nQ8. I guess the authors want to say that, if $L_i$ is identifiable, then it is the causal factor of $Y_t$. So, the UIC loss is used to promise the identifiability. There are two issues need to be addressed. One is why identifiable factors are causal ones, and another is why UIC can select identifiable factors. I'm not clear whether the second problem has been addressed by Section 3.2, since I think it's not well orangized. Nevertheless, I think the first problem is untouched.\n\nQ9. I'm not sure whether the baselines are advanced enough. The paper only compares with two simple baselines.", "labeling_timestamp": "2026-01-11T17:11:55.459986", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly defines what is meant by identifiability — it refers to identifying latent factors/representations and provides a Uniform Identifiability Condition (UIC) guaranteeing latent factors can be separated without mixing. Thus the claim that the paper does not clearly state the object of identifiability is contradicted by the text.", "evidence": "1) \"On the theoretical side, we present the Uniform Identifiability Condition (UIC), a sufficient and necessary condition to ensure identifiability of latent factors in the meta-SCM. The UIC guarantees these factors can be separated without mixing information from other factors by fitting the observed data.\" 2) \"The concept of identifiability encompasses whether a representation learned from observed data can match the true underlying latent factors which are responsible for the data generating process, under acceptable transformation operations such as permutation or scaling (Lehmann & Casella, 2006).\" 3) \"Theoretically, we prove the causal factor can be identified without mixing information from others.\"", "section": "3 PROBLEM FORMULATION AND THEORY; 2 RELATED WORK; ABSTRACT"}
{"claim": "Identifiability is typically defined for specific causal quantities between variables rather than for an entire SCM, but the paper treats identifiability at the SCM level without justification.", "claim_type": "methodology", "paper_id": "lWXedJyLuL", "paper_title": "A Unified Causal View of Instruction Tuning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "f6n4G0QZIH", "reviewer": "Reviewer_nrtp", "review_text": "Summary: This paper tries to learn task-required causal factors and only use those to make predictions for a given task. The motivation is claimed to be that the causal factors of different tasks are spuriously correlated through the task variable $T$. The authors theoretically prove the causal factor can be identified without mixing information from others, then they propose a Structural Instruction Tuning (SIT) method to learn the task-required causal representations that can mimic the causal factors for each task.\n\nStrengths: 1. Exploring causal LLMs is very important for both research and applications.\n\n2. This is a very novel work in this direction.\n\nWeaknesses: 1. The organization of this paper is very hard to follow.\n\n2. The technical quality is poor. There are serious technical flaws in this paper. See the questions below.\n\nQuestions: Q1. SCMs should be constructed from structual functions. Intuitive causal graphs are very informal and not relible. Equation 1 is not enough, because the structual functions should appear before the graphs. I think it would be better to illustrate Fig 1 with a specific example (i.e., what $L_i, X, Y$ and $T$ stands for in a specific task, and how the non-causal factors correlate with $Y_t$).\n\nQ2. The authors use different $L_i$ for different causal factors, and this is good. Nevertheless, I think $Y_t$ in Fig.1 should also be notated as different nodes for different $t$. \n\nQ3. The authors claim that \"Causally speaking, there\nexist backdoor paths between the target labels and non-causal latent factors through the task\nvariable T.\". But when we are doing a task, the task variable is given. And in Fig.1, we have $L_i \\perp L_j|T$. So, there must be a problem arising from either the claim itself or the causal graph. Considering question Q1, I think it is more likely the latter. I would appreciate it if the authors could provide a specific example to illustrate how the non-causal factors can correlate with $Y_t$ (e.g., the simplest text classification). This is one of the most important questions for me, and I'm looking forward to the authors' response.\n\nQ4. Following the above question, even if $T$ can act as a confounder and spurious correlations exist in the data, how does the traditional training process make the model learn the spurious correlations? This problem is also untouched, which makes the motivation informal and weak.\n\nQ5. It seems that the only confounder in this paper is the task $T$, I can hardly come up with any applications where different features are caused by the task but don't cause the target label.\n\nQ6. Section 3.2 is really hard to follow. How to alleviate the influence of the spurious correlations? By backdoor adjustment or introducing intervened data?\n\nQ7. The definition of identifiability is confusing. What's the identifiability of a SCM? When we say the identifiability, it usually refers to the causal quantity on two specific variables rather than a SCM. Also, the target of the UIC loss in Eq.6 is vague. What causal quantity do you want to measure?\n\nQ8. I guess the authors want to say that, if $L_i$ is identifiable, then it is the causal factor of $Y_t$. So, the UIC loss is used to promise the identifiability. There are two issues need to be addressed. One is why identifiable factors are causal ones, and another is why UIC can select identifiable factors. I'm not clear whether the second problem has been addressed by Section 3.2, since I think it's not well orangized. Nevertheless, I think the first problem is untouched.\n\nQ9. I'm not sure whether the baselines are advanced enough. The paper only compares with two simple baselines.", "labeling_timestamp": "2026-01-11T17:12:14.669160", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly provides a theoretical justification: it introduces a Uniform Identifiability Condition (UIC) and claims it is a sufficient and necessary condition guaranteeing identifiability of latent factors in the meta-SCM, with proofs and discussion of topology-based guarantees, so it does not treat identifiability at the SCM level without justification.", "evidence": "On the theoretical side, we present the Uniform Identifiability Condition (UIC), a sufficient and necessary condition to ensure identifiability of latent factors in the meta-SCM. The UIC guarantees these factors can be separated without mixing information from other factors by fitting the observed data.", "section": "Introduction / Section 3: Problem Formulation and Theory"}
{"claim": "The causal quantity that the proposed UIC loss in Equation 6 is intended to measure is vague and left unspecified.", "claim_type": "methodology", "paper_id": "lWXedJyLuL", "paper_title": "A Unified Causal View of Instruction Tuning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "f6n4G0QZIH", "reviewer": "Reviewer_nrtp", "review_text": "Summary: This paper tries to learn task-required causal factors and only use those to make predictions for a given task. The motivation is claimed to be that the causal factors of different tasks are spuriously correlated through the task variable $T$. The authors theoretically prove the causal factor can be identified without mixing information from others, then they propose a Structural Instruction Tuning (SIT) method to learn the task-required causal representations that can mimic the causal factors for each task.\n\nStrengths: 1. Exploring causal LLMs is very important for both research and applications.\n\n2. This is a very novel work in this direction.\n\nWeaknesses: 1. The organization of this paper is very hard to follow.\n\n2. The technical quality is poor. There are serious technical flaws in this paper. See the questions below.\n\nQuestions: Q1. SCMs should be constructed from structual functions. Intuitive causal graphs are very informal and not relible. Equation 1 is not enough, because the structual functions should appear before the graphs. I think it would be better to illustrate Fig 1 with a specific example (i.e., what $L_i, X, Y$ and $T$ stands for in a specific task, and how the non-causal factors correlate with $Y_t$).\n\nQ2. The authors use different $L_i$ for different causal factors, and this is good. Nevertheless, I think $Y_t$ in Fig.1 should also be notated as different nodes for different $t$. \n\nQ3. The authors claim that \"Causally speaking, there\nexist backdoor paths between the target labels and non-causal latent factors through the task\nvariable T.\". But when we are doing a task, the task variable is given. And in Fig.1, we have $L_i \\perp L_j|T$. So, there must be a problem arising from either the claim itself or the causal graph. Considering question Q1, I think it is more likely the latter. I would appreciate it if the authors could provide a specific example to illustrate how the non-causal factors can correlate with $Y_t$ (e.g., the simplest text classification). This is one of the most important questions for me, and I'm looking forward to the authors' response.\n\nQ4. Following the above question, even if $T$ can act as a confounder and spurious correlations exist in the data, how does the traditional training process make the model learn the spurious correlations? This problem is also untouched, which makes the motivation informal and weak.\n\nQ5. It seems that the only confounder in this paper is the task $T$, I can hardly come up with any applications where different features are caused by the task but don't cause the target label.\n\nQ6. Section 3.2 is really hard to follow. How to alleviate the influence of the spurious correlations? By backdoor adjustment or introducing intervened data?\n\nQ7. The definition of identifiability is confusing. What's the identifiability of a SCM? When we say the identifiability, it usually refers to the causal quantity on two specific variables rather than a SCM. Also, the target of the UIC loss in Eq.6 is vague. What causal quantity do you want to measure?\n\nQ8. I guess the authors want to say that, if $L_i$ is identifiable, then it is the causal factor of $Y_t$. So, the UIC loss is used to promise the identifiability. There are two issues need to be addressed. One is why identifiable factors are causal ones, and another is why UIC can select identifiable factors. I'm not clear whether the second problem has been addressed by Section 3.2, since I think it's not well orangized. Nevertheless, I think the first problem is untouched.\n\nQ9. I'm not sure whether the baselines are advanced enough. The paper only compares with two simple baselines.", "labeling_timestamp": "2026-01-11T17:12:18.431876", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt introduces the Uniform Identifiability Condition (UIC) as a theoretical identifiability condition (Introduction / Sec. 3) but does not show or describe a 'UIC loss' or any Equation 6 in the supplied content. Therefore there is insufficient information in the given text to confirm the reviewer’s claim about vagueness or specification of a UIC loss.", "evidence": "On the theoretical side, we present the Uniform Identifiability Condition (UIC), a sufficient and necessary condition to ensure identifiability of latent factors in the meta-SCM.", "section": "Introduction / 3 PROBLEM FORMULATION AND THEORY"}
{"claim": "The paper does not justify why identifiable latent factors would necessarily correspond to the true causal factors of Y_t.", "claim_type": "methodology", "paper_id": "lWXedJyLuL", "paper_title": "A Unified Causal View of Instruction Tuning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "f6n4G0QZIH", "reviewer": "Reviewer_nrtp", "review_text": "Summary: This paper tries to learn task-required causal factors and only use those to make predictions for a given task. The motivation is claimed to be that the causal factors of different tasks are spuriously correlated through the task variable $T$. The authors theoretically prove the causal factor can be identified without mixing information from others, then they propose a Structural Instruction Tuning (SIT) method to learn the task-required causal representations that can mimic the causal factors for each task.\n\nStrengths: 1. Exploring causal LLMs is very important for both research and applications.\n\n2. This is a very novel work in this direction.\n\nWeaknesses: 1. The organization of this paper is very hard to follow.\n\n2. The technical quality is poor. There are serious technical flaws in this paper. See the questions below.\n\nQuestions: Q1. SCMs should be constructed from structual functions. Intuitive causal graphs are very informal and not relible. Equation 1 is not enough, because the structual functions should appear before the graphs. I think it would be better to illustrate Fig 1 with a specific example (i.e., what $L_i, X, Y$ and $T$ stands for in a specific task, and how the non-causal factors correlate with $Y_t$).\n\nQ2. The authors use different $L_i$ for different causal factors, and this is good. Nevertheless, I think $Y_t$ in Fig.1 should also be notated as different nodes for different $t$. \n\nQ3. The authors claim that \"Causally speaking, there\nexist backdoor paths between the target labels and non-causal latent factors through the task\nvariable T.\". But when we are doing a task, the task variable is given. And in Fig.1, we have $L_i \\perp L_j|T$. So, there must be a problem arising from either the claim itself or the causal graph. Considering question Q1, I think it is more likely the latter. I would appreciate it if the authors could provide a specific example to illustrate how the non-causal factors can correlate with $Y_t$ (e.g., the simplest text classification). This is one of the most important questions for me, and I'm looking forward to the authors' response.\n\nQ4. Following the above question, even if $T$ can act as a confounder and spurious correlations exist in the data, how does the traditional training process make the model learn the spurious correlations? This problem is also untouched, which makes the motivation informal and weak.\n\nQ5. It seems that the only confounder in this paper is the task $T$, I can hardly come up with any applications where different features are caused by the task but don't cause the target label.\n\nQ6. Section 3.2 is really hard to follow. How to alleviate the influence of the spurious correlations? By backdoor adjustment or introducing intervened data?\n\nQ7. The definition of identifiability is confusing. What's the identifiability of a SCM? When we say the identifiability, it usually refers to the causal quantity on two specific variables rather than a SCM. Also, the target of the UIC loss in Eq.6 is vague. What causal quantity do you want to measure?\n\nQ8. I guess the authors want to say that, if $L_i$ is identifiable, then it is the causal factor of $Y_t$. So, the UIC loss is used to promise the identifiability. There are two issues need to be addressed. One is why identifiable factors are causal ones, and another is why UIC can select identifiable factors. I'm not clear whether the second problem has been addressed by Section 3.2, since I think it's not well orangized. Nevertheless, I think the first problem is untouched.\n\nQ9. I'm not sure whether the baselines are advanced enough. The paper only compares with two simple baselines.", "labeling_timestamp": "2026-01-11T17:12:22.244322", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states a theoretical identifiability result (the Uniform Identifiability Condition) and claims to prove that causal factors can be identified without mixing information from other latent factors, thereby justifying the correspondence between identifiable latent factors and task-required causal factors for Y_t.", "evidence": "1) \"Theoretically, we prove the causal factor can be identified without mixing information from others.\" (Abstract)\n2) \"In this section, ... we propose a uniform identifiability condition (UIC) based on the topological structure of SCM, which guarantees that latent factors in the SCM can be identified without mixing information from others by fitting the observed data.\" (Section 3)", "section": "Abstract; Section 3 (Problem Formulation and Theory)"}
{"claim": "The paper does not convincingly demonstrate or theoretically justify that the UIC loss can reliably select identifiable factors from the latent representation.", "claim_type": "methodology", "paper_id": "lWXedJyLuL", "paper_title": "A Unified Causal View of Instruction Tuning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "f6n4G0QZIH", "reviewer": "Reviewer_nrtp", "review_text": "Summary: This paper tries to learn task-required causal factors and only use those to make predictions for a given task. The motivation is claimed to be that the causal factors of different tasks are spuriously correlated through the task variable $T$. The authors theoretically prove the causal factor can be identified without mixing information from others, then they propose a Structural Instruction Tuning (SIT) method to learn the task-required causal representations that can mimic the causal factors for each task.\n\nStrengths: 1. Exploring causal LLMs is very important for both research and applications.\n\n2. This is a very novel work in this direction.\n\nWeaknesses: 1. The organization of this paper is very hard to follow.\n\n2. The technical quality is poor. There are serious technical flaws in this paper. See the questions below.\n\nQuestions: Q1. SCMs should be constructed from structual functions. Intuitive causal graphs are very informal and not relible. Equation 1 is not enough, because the structual functions should appear before the graphs. I think it would be better to illustrate Fig 1 with a specific example (i.e., what $L_i, X, Y$ and $T$ stands for in a specific task, and how the non-causal factors correlate with $Y_t$).\n\nQ2. The authors use different $L_i$ for different causal factors, and this is good. Nevertheless, I think $Y_t$ in Fig.1 should also be notated as different nodes for different $t$. \n\nQ3. The authors claim that \"Causally speaking, there\nexist backdoor paths between the target labels and non-causal latent factors through the task\nvariable T.\". But when we are doing a task, the task variable is given. And in Fig.1, we have $L_i \\perp L_j|T$. So, there must be a problem arising from either the claim itself or the causal graph. Considering question Q1, I think it is more likely the latter. I would appreciate it if the authors could provide a specific example to illustrate how the non-causal factors can correlate with $Y_t$ (e.g., the simplest text classification). This is one of the most important questions for me, and I'm looking forward to the authors' response.\n\nQ4. Following the above question, even if $T$ can act as a confounder and spurious correlations exist in the data, how does the traditional training process make the model learn the spurious correlations? This problem is also untouched, which makes the motivation informal and weak.\n\nQ5. It seems that the only confounder in this paper is the task $T$, I can hardly come up with any applications where different features are caused by the task but don't cause the target label.\n\nQ6. Section 3.2 is really hard to follow. How to alleviate the influence of the spurious correlations? By backdoor adjustment or introducing intervened data?\n\nQ7. The definition of identifiability is confusing. What's the identifiability of a SCM? When we say the identifiability, it usually refers to the causal quantity on two specific variables rather than a SCM. Also, the target of the UIC loss in Eq.6 is vague. What causal quantity do you want to measure?\n\nQ8. I guess the authors want to say that, if $L_i$ is identifiable, then it is the causal factor of $Y_t$. So, the UIC loss is used to promise the identifiability. There are two issues need to be addressed. One is why identifiable factors are causal ones, and another is why UIC can select identifiable factors. I'm not clear whether the second problem has been addressed by Section 3.2, since I think it's not well orangized. Nevertheless, I think the first problem is untouched.\n\nQ9. I'm not sure whether the baselines are advanced enough. The paper only compares with two simple baselines.", "labeling_timestamp": "2026-01-11T17:13:00.208071", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly claims a theoretical identifiability result and introduces the Uniform Identifiability Condition (UIC) as a sufficient and necessary condition guaranteeing latent factors can be identified without mixing; thus it does provide a theoretical justification (contradicting the reviewer's assertion that it does not).", "evidence": "Abstract: \"Theoretically, we prove the causal factor can be identified without mixing information from others.\"; Section 3: \"we propose a uniform identifiability condition (UIC) based on the topological structure of SCM, which guarantees that latent factors in the SCM can be identified without mixing information from others by fitting the observed data.\"", "section": "Abstract; Section 3 (PROBLEM FORMULATION AND THEORY)"}
{"claim": "The experimental evaluation compares only against two simple baselines, which may not be sufficiently advanced to validate the proposed method's performance.", "claim_type": "baseline", "paper_id": "lWXedJyLuL", "paper_title": "A Unified Causal View of Instruction Tuning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "f6n4G0QZIH", "reviewer": "Reviewer_nrtp", "review_text": "Summary: This paper tries to learn task-required causal factors and only use those to make predictions for a given task. The motivation is claimed to be that the causal factors of different tasks are spuriously correlated through the task variable $T$. The authors theoretically prove the causal factor can be identified without mixing information from others, then they propose a Structural Instruction Tuning (SIT) method to learn the task-required causal representations that can mimic the causal factors for each task.\n\nStrengths: 1. Exploring causal LLMs is very important for both research and applications.\n\n2. This is a very novel work in this direction.\n\nWeaknesses: 1. The organization of this paper is very hard to follow.\n\n2. The technical quality is poor. There are serious technical flaws in this paper. See the questions below.\n\nQuestions: Q1. SCMs should be constructed from structual functions. Intuitive causal graphs are very informal and not relible. Equation 1 is not enough, because the structual functions should appear before the graphs. I think it would be better to illustrate Fig 1 with a specific example (i.e., what $L_i, X, Y$ and $T$ stands for in a specific task, and how the non-causal factors correlate with $Y_t$).\n\nQ2. The authors use different $L_i$ for different causal factors, and this is good. Nevertheless, I think $Y_t$ in Fig.1 should also be notated as different nodes for different $t$. \n\nQ3. The authors claim that \"Causally speaking, there\nexist backdoor paths between the target labels and non-causal latent factors through the task\nvariable T.\". But when we are doing a task, the task variable is given. And in Fig.1, we have $L_i \\perp L_j|T$. So, there must be a problem arising from either the claim itself or the causal graph. Considering question Q1, I think it is more likely the latter. I would appreciate it if the authors could provide a specific example to illustrate how the non-causal factors can correlate with $Y_t$ (e.g., the simplest text classification). This is one of the most important questions for me, and I'm looking forward to the authors' response.\n\nQ4. Following the above question, even if $T$ can act as a confounder and spurious correlations exist in the data, how does the traditional training process make the model learn the spurious correlations? This problem is also untouched, which makes the motivation informal and weak.\n\nQ5. It seems that the only confounder in this paper is the task $T$, I can hardly come up with any applications where different features are caused by the task but don't cause the target label.\n\nQ6. Section 3.2 is really hard to follow. How to alleviate the influence of the spurious correlations? By backdoor adjustment or introducing intervened data?\n\nQ7. The definition of identifiability is confusing. What's the identifiability of a SCM? When we say the identifiability, it usually refers to the causal quantity on two specific variables rather than a SCM. Also, the target of the UIC loss in Eq.6 is vague. What causal quantity do you want to measure?\n\nQ8. I guess the authors want to say that, if $L_i$ is identifiable, then it is the causal factor of $Y_t$. So, the UIC loss is used to promise the identifiability. There are two issues need to be addressed. One is why identifiable factors are causal ones, and another is why UIC can select identifiable factors. I'm not clear whether the second problem has been addressed by Section 3.2, since I think it's not well orangized. Nevertheless, I think the first problem is untouched.\n\nQ9. I'm not sure whether the baselines are advanced enough. The paper only compares with two simple baselines.", "labeling_timestamp": "2026-01-11T17:12:27.510007", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper text reports improvements on datasets but does not include the experimental section or any description of which baselines were used or how many were compared. Therefore we cannot tell from the available content whether the evaluation compared only two simple baselines.", "evidence": "\"The utility of our approach is verified by improvements of zero-shot ability on a range of unseen datasets and tasks.\" and \"Experimentally, we verify the effectiveness of SIT on both in-domain and out-of-domain (OOD) datasets, e.g., 60.51% improvement on Gigaword in terms of Rouge-L. We also show better cross-task adaptability of SIT on unseen tasks, e.g., 31.30% improvement on RTE in terms of accuracy.\"", "section": "ABSTRACT"}
{"claim": "The paper overlooks several crucial references relevant to active learning, including works that explore the theoretical importance of near decision boundaries.", "claim_type": "subjective", "paper_id": "yZBpnKpBCw", "paper_title": "Time- and Label-efficient Active Learning by Diversity and Uncertainty of Probabilities", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "AhzqNtmeeP", "reviewer": "Reviewer_Y5kQ", "review_text": "Summary: The paper introduces FALCUN, a method in active learning, which promises efficiencies in labeling and processing. It brings a dynamic acquisition strategy to the table, focusing initially on uncertain instances near decision boundaries and gradually shifting to emphasize batch diversity. While the proposed approach looks new in some aspects, the lack of acknowledgment of previous works that have explored similar territories casts a shadow over its originality.\n\nKey foundational concepts seem to echo pre-existing methodologies in active learning literature, which were not appropriately credited, making FALCUN appear more as an incremental advancement rather than a groundbreaking innovation. Additionally, some experimental results, particularly those involving LeNet and BADGE's performance, seem somewhat inconclusive and could benefit from further rigor and validation to strengthen their credibility.\n\nFurthermore, the paper’s experimental breadth appears limited. A more expansive inclusion of diverse and challenging benchmarks, such as CIFAR-100 and TinyImageNet, would enhance the robustness of FALCUN’s evaluation, providing a more comprehensive understanding of its effectiveness and applicability across various domains. A reconsideration of these aspects could elevate the paper’s contributions and clarity, fostering a more nuanced appreciation of FALCUN's position in the active learning landscape.\n\nStrengths: - The proposed algorithm tries to improve efficiencies in labeling and processing through a dynamic acquisition strategy, emphasizing a transition from focusing on uncertain instances to prioritizing batch diversity, showcasing a potential way in active learning strategies.\n\nWeaknesses: - The paper seems to overlook several crucial references relevant to the discussed topic. Fundamental concepts such as the theoretical importance of the near decision boundary have been comprehensively explored and articulated in previous works, notably in references [1] and [2]. These pivotal papers, along with others, offer profound insights that would augment the paper’s foundational grounding. Moreover, the proposition of a linear-time scalable algorithm, a core element presented in the paper, has previously been introduced and elaborated upon in reference [3]. In addition to that, an influential work cited as [4] such as PowerBALD/or PowerEntropy adopting a sampling similar to Eq (6) but with better originality has also proposed a scaling approach that intriguingly maintains the algorithm’s linear-time complexity.\n\n- The active learning results presented in the paper for LeNet on datasets like EMNIST or RepeatedMNIST seem somewhat unconvincing. Typically, in prevailing literature and experiments, around 300-500 images are required (not > 1000 images) to achieve accuracy comparable to supervised learning methods on these datasets. The reported results in the paper appear to deviate from these established benchmarks. It may be beneficial to revisit and scrutinize the experimental setup, methodology, and the specific implementation of LeNet in the active learning context to ensure that the presented results are robust, reliable, and in alignment with existing standards and expectations in the field. This could enhance the credibility and persuasiveness of the results and the overall contributions of the paper.\n\n- The performance of BADGE as presented in the paper raises some questions. It seems that with appropriate hyperparameter settings, BADGE should be capable of delivering much improved results. This discrepancy suggests that there might be room for optimizing the configuration of BADGE in the experiments, ensuring that it operates under the most suitable conditions for a fair and rigorous comparison. To uphold the integrity and reliability of the comparative analysis, it might be beneficial to revisit and fine-tune the hyperparameters used with BADGE, ensuring that its performance is accurately represented and evaluated against its full potential.\n\n- The experimental section of the paper seems somewhat limited and could be enhanced to bolster the claims made. Incorporating a broader array of realistic scenarios, such as tests involving CIFAR-100, TinyImageNet, or ImageNet, would offer a more comprehensive insight into the method’s applicability and effectiveness. Including such varied and complex datasets in the evaluation would not only strengthen the validity of the results but also improve the generalizability of the conclusions drawn. This expansion in the experimental design would be instrumental in substantiating the method's robustness and adaptability across diverse challenges and use-cases.\n\n[1] Efficient Active Learning with Abstention, NeurIPS 2022 - https://openreview.net/forum?id=4u-oGqB4Lf6\n\n[2] Active Learning with Neural Networks: Insights from Nonparametric Statistics, NeurIPS 2022 - https://openreview.net/forum?id=LRMmgkcoCnW\n\n[3] Active Learning in Bayesian Neural Networks with Balanced Entropy Learning Principle, ICLR 2023 - https://openreview.net/forum?id=ZTMuZ68B1g\n\n[4] Stochastic Batch Acquisition: A Simple Baseline for Deep Active Learning, TMLR 2023 - https://openreview.net/forum?id=vcHwQyNBjW\n\nQuestions: - What criteria determine the **most informative region**? Is a point considered most informative solely based on its proximity to the decision boundary?\n\n- **Could the authors please provide a more explicit explanation regarding the source of diversity in the proposed formula?** The margin considers two class boundaries. It might not be sufficiently generalizable. It would be beneficial to have a more explicit elucidation on how diversity is incorporated and manifests itself within the algorithmic design. Understanding the origins and implementation of diversity within the formula can enhance the comprehension of its functionality and overall impact on the method's performance. An in-depth clarification would contribute to a more robust and insightful evaluation of the proposed approach’s effectiveness and novelty.\n\n- It seems crucial to **re-evaluate the experiment setups in the study to enhance the reliability and comprehensiveness of the findings**. Including a broader selection of benchmarks in the evaluation process would be instrumental in demonstrating the robustness and versatility of the proposed method across varied scenarios. **A more diversified array of benchmarks** will not only contribute to a deeper, better understanding of the method's performance but also bolster the study's overall credibility and impact. Therefore, revisiting and expanding the experiment setups with additional benchmarks is a highly recommended step to enrich the empirical validation of the study.\n\n- Section 4.4 discusses the influence of $\\gamma$ parameter, indicating that **different benchmarks may require distinct $\\gamma$ parameter values for optimal performance**. This aspect raises a practical concern: how can users effectively determine or choose an appropriate $\\gamma$ value a priori for various benchmarks? The ability to discern and select suitable parameters is crucial for the method's practical applicability and usability in real-world scenarios. Clarification or guidance on this matter would significantly enhance the method’s practical value.", "labeling_timestamp": "2026-01-11T17:12:47.899610", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly discusses the role of decision boundaries and uses margin uncertainty, citing prior theoretical/empirical works (Roth & Small 2006; Bahri et al. 2022; Jiang & Gupta 2021). Thus it does not clearly 'overlook' works exploring the importance of near-decision-boundary instances.", "evidence": "For uncertainty, we use the margin uncertainty, i.e., the difference between the probabilities of its two most probable classes: ... Margin is a common choice for uncertainty (Roth & Small, 2006; Bahri et al., 2022; Jiang & Gupta, 2021) and naturally captures the class boundaries between each class pairs.", "section": "2.2 ACQUISITION (Uncertainty Component)"}
{"claim": "Key foundational concepts in the paper echo pre-existing active learning methodologies that are not appropriately credited.", "claim_type": "novelty", "paper_id": "yZBpnKpBCw", "paper_title": "Time- and Label-efficient Active Learning by Diversity and Uncertainty of Probabilities", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "AhzqNtmeeP", "reviewer": "Reviewer_Y5kQ", "review_text": "Summary: The paper introduces FALCUN, a method in active learning, which promises efficiencies in labeling and processing. It brings a dynamic acquisition strategy to the table, focusing initially on uncertain instances near decision boundaries and gradually shifting to emphasize batch diversity. While the proposed approach looks new in some aspects, the lack of acknowledgment of previous works that have explored similar territories casts a shadow over its originality.\n\nKey foundational concepts seem to echo pre-existing methodologies in active learning literature, which were not appropriately credited, making FALCUN appear more as an incremental advancement rather than a groundbreaking innovation. Additionally, some experimental results, particularly those involving LeNet and BADGE's performance, seem somewhat inconclusive and could benefit from further rigor and validation to strengthen their credibility.\n\nFurthermore, the paper’s experimental breadth appears limited. A more expansive inclusion of diverse and challenging benchmarks, such as CIFAR-100 and TinyImageNet, would enhance the robustness of FALCUN’s evaluation, providing a more comprehensive understanding of its effectiveness and applicability across various domains. A reconsideration of these aspects could elevate the paper’s contributions and clarity, fostering a more nuanced appreciation of FALCUN's position in the active learning landscape.\n\nStrengths: - The proposed algorithm tries to improve efficiencies in labeling and processing through a dynamic acquisition strategy, emphasizing a transition from focusing on uncertain instances to prioritizing batch diversity, showcasing a potential way in active learning strategies.\n\nWeaknesses: - The paper seems to overlook several crucial references relevant to the discussed topic. Fundamental concepts such as the theoretical importance of the near decision boundary have been comprehensively explored and articulated in previous works, notably in references [1] and [2]. These pivotal papers, along with others, offer profound insights that would augment the paper’s foundational grounding. Moreover, the proposition of a linear-time scalable algorithm, a core element presented in the paper, has previously been introduced and elaborated upon in reference [3]. In addition to that, an influential work cited as [4] such as PowerBALD/or PowerEntropy adopting a sampling similar to Eq (6) but with better originality has also proposed a scaling approach that intriguingly maintains the algorithm’s linear-time complexity.\n\n- The active learning results presented in the paper for LeNet on datasets like EMNIST or RepeatedMNIST seem somewhat unconvincing. Typically, in prevailing literature and experiments, around 300-500 images are required (not > 1000 images) to achieve accuracy comparable to supervised learning methods on these datasets. The reported results in the paper appear to deviate from these established benchmarks. It may be beneficial to revisit and scrutinize the experimental setup, methodology, and the specific implementation of LeNet in the active learning context to ensure that the presented results are robust, reliable, and in alignment with existing standards and expectations in the field. This could enhance the credibility and persuasiveness of the results and the overall contributions of the paper.\n\n- The performance of BADGE as presented in the paper raises some questions. It seems that with appropriate hyperparameter settings, BADGE should be capable of delivering much improved results. This discrepancy suggests that there might be room for optimizing the configuration of BADGE in the experiments, ensuring that it operates under the most suitable conditions for a fair and rigorous comparison. To uphold the integrity and reliability of the comparative analysis, it might be beneficial to revisit and fine-tune the hyperparameters used with BADGE, ensuring that its performance is accurately represented and evaluated against its full potential.\n\n- The experimental section of the paper seems somewhat limited and could be enhanced to bolster the claims made. Incorporating a broader array of realistic scenarios, such as tests involving CIFAR-100, TinyImageNet, or ImageNet, would offer a more comprehensive insight into the method’s applicability and effectiveness. Including such varied and complex datasets in the evaluation would not only strengthen the validity of the results but also improve the generalizability of the conclusions drawn. This expansion in the experimental design would be instrumental in substantiating the method's robustness and adaptability across diverse challenges and use-cases.\n\n[1] Efficient Active Learning with Abstention, NeurIPS 2022 - https://openreview.net/forum?id=4u-oGqB4Lf6\n\n[2] Active Learning with Neural Networks: Insights from Nonparametric Statistics, NeurIPS 2022 - https://openreview.net/forum?id=LRMmgkcoCnW\n\n[3] Active Learning in Bayesian Neural Networks with Balanced Entropy Learning Principle, ICLR 2023 - https://openreview.net/forum?id=ZTMuZ68B1g\n\n[4] Stochastic Batch Acquisition: A Simple Baseline for Deep Active Learning, TMLR 2023 - https://openreview.net/forum?id=vcHwQyNBjW\n\nQuestions: - What criteria determine the **most informative region**? Is a point considered most informative solely based on its proximity to the decision boundary?\n\n- **Could the authors please provide a more explicit explanation regarding the source of diversity in the proposed formula?** The margin considers two class boundaries. It might not be sufficiently generalizable. It would be beneficial to have a more explicit elucidation on how diversity is incorporated and manifests itself within the algorithmic design. Understanding the origins and implementation of diversity within the formula can enhance the comprehension of its functionality and overall impact on the method's performance. An in-depth clarification would contribute to a more robust and insightful evaluation of the proposed approach’s effectiveness and novelty.\n\n- It seems crucial to **re-evaluate the experiment setups in the study to enhance the reliability and comprehensiveness of the findings**. Including a broader selection of benchmarks in the evaluation process would be instrumental in demonstrating the robustness and versatility of the proposed method across varied scenarios. **A more diversified array of benchmarks** will not only contribute to a deeper, better understanding of the method's performance but also bolster the study's overall credibility and impact. Therefore, revisiting and expanding the experiment setups with additional benchmarks is a highly recommended step to enrich the empirical validation of the study.\n\n- Section 4.4 discusses the influence of $\\gamma$ parameter, indicating that **different benchmarks may require distinct $\\gamma$ parameter values for optimal performance**. This aspect raises a practical concern: how can users effectively determine or choose an appropriate $\\gamma$ value a priori for various benchmarks? The ability to discern and select suitable parameters is crucial for the method's practical applicability and usability in real-world scenarios. Clarification or guidance on this matter would significantly enhance the method’s practical value.", "labeling_timestamp": "2026-01-11T17:12:58.442454", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly cites and discusses multiple prior active learning methods and concepts (e.g., latent-space diversity methods, margin uncertainty, hybrid approaches) and compares against state-of-the-art methods, indicating that foundational concepts are credited rather than omitted.", "evidence": "“To measure diversity, established AL methods often exploit the latent representations of a deep learning model (Ash et al., 2020; Sener & Savarese, 2018; Prabhu et al., 2021; Zhdanov, 2019).”; “Margin is a common choice for uncertainty (Roth & Small, 2006; Bahri et al., 2022; Jiang & Gupta, 2021).”; “Compared to state-of-the-art methods like BADGE, CLUE, and AlfaMix, FALCUN consistently excels in quality and speed...”", "section": "Introduction; 2.2 Acquisition; Abstract; Related Work"}
{"claim": "The claimed linear-time scalable algorithm replicates ideas previously introduced in prior work [3] and is not acknowledged as such.", "claim_type": "novelty", "paper_id": "yZBpnKpBCw", "paper_title": "Time- and Label-efficient Active Learning by Diversity and Uncertainty of Probabilities", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "AhzqNtmeeP", "reviewer": "Reviewer_Y5kQ", "review_text": "Summary: The paper introduces FALCUN, a method in active learning, which promises efficiencies in labeling and processing. It brings a dynamic acquisition strategy to the table, focusing initially on uncertain instances near decision boundaries and gradually shifting to emphasize batch diversity. While the proposed approach looks new in some aspects, the lack of acknowledgment of previous works that have explored similar territories casts a shadow over its originality.\n\nKey foundational concepts seem to echo pre-existing methodologies in active learning literature, which were not appropriately credited, making FALCUN appear more as an incremental advancement rather than a groundbreaking innovation. Additionally, some experimental results, particularly those involving LeNet and BADGE's performance, seem somewhat inconclusive and could benefit from further rigor and validation to strengthen their credibility.\n\nFurthermore, the paper’s experimental breadth appears limited. A more expansive inclusion of diverse and challenging benchmarks, such as CIFAR-100 and TinyImageNet, would enhance the robustness of FALCUN’s evaluation, providing a more comprehensive understanding of its effectiveness and applicability across various domains. A reconsideration of these aspects could elevate the paper’s contributions and clarity, fostering a more nuanced appreciation of FALCUN's position in the active learning landscape.\n\nStrengths: - The proposed algorithm tries to improve efficiencies in labeling and processing through a dynamic acquisition strategy, emphasizing a transition from focusing on uncertain instances to prioritizing batch diversity, showcasing a potential way in active learning strategies.\n\nWeaknesses: - The paper seems to overlook several crucial references relevant to the discussed topic. Fundamental concepts such as the theoretical importance of the near decision boundary have been comprehensively explored and articulated in previous works, notably in references [1] and [2]. These pivotal papers, along with others, offer profound insights that would augment the paper’s foundational grounding. Moreover, the proposition of a linear-time scalable algorithm, a core element presented in the paper, has previously been introduced and elaborated upon in reference [3]. In addition to that, an influential work cited as [4] such as PowerBALD/or PowerEntropy adopting a sampling similar to Eq (6) but with better originality has also proposed a scaling approach that intriguingly maintains the algorithm’s linear-time complexity.\n\n- The active learning results presented in the paper for LeNet on datasets like EMNIST or RepeatedMNIST seem somewhat unconvincing. Typically, in prevailing literature and experiments, around 300-500 images are required (not > 1000 images) to achieve accuracy comparable to supervised learning methods on these datasets. The reported results in the paper appear to deviate from these established benchmarks. It may be beneficial to revisit and scrutinize the experimental setup, methodology, and the specific implementation of LeNet in the active learning context to ensure that the presented results are robust, reliable, and in alignment with existing standards and expectations in the field. This could enhance the credibility and persuasiveness of the results and the overall contributions of the paper.\n\n- The performance of BADGE as presented in the paper raises some questions. It seems that with appropriate hyperparameter settings, BADGE should be capable of delivering much improved results. This discrepancy suggests that there might be room for optimizing the configuration of BADGE in the experiments, ensuring that it operates under the most suitable conditions for a fair and rigorous comparison. To uphold the integrity and reliability of the comparative analysis, it might be beneficial to revisit and fine-tune the hyperparameters used with BADGE, ensuring that its performance is accurately represented and evaluated against its full potential.\n\n- The experimental section of the paper seems somewhat limited and could be enhanced to bolster the claims made. Incorporating a broader array of realistic scenarios, such as tests involving CIFAR-100, TinyImageNet, or ImageNet, would offer a more comprehensive insight into the method’s applicability and effectiveness. Including such varied and complex datasets in the evaluation would not only strengthen the validity of the results but also improve the generalizability of the conclusions drawn. This expansion in the experimental design would be instrumental in substantiating the method's robustness and adaptability across diverse challenges and use-cases.\n\n[1] Efficient Active Learning with Abstention, NeurIPS 2022 - https://openreview.net/forum?id=4u-oGqB4Lf6\n\n[2] Active Learning with Neural Networks: Insights from Nonparametric Statistics, NeurIPS 2022 - https://openreview.net/forum?id=LRMmgkcoCnW\n\n[3] Active Learning in Bayesian Neural Networks with Balanced Entropy Learning Principle, ICLR 2023 - https://openreview.net/forum?id=ZTMuZ68B1g\n\n[4] Stochastic Batch Acquisition: A Simple Baseline for Deep Active Learning, TMLR 2023 - https://openreview.net/forum?id=vcHwQyNBjW\n\nQuestions: - What criteria determine the **most informative region**? Is a point considered most informative solely based on its proximity to the decision boundary?\n\n- **Could the authors please provide a more explicit explanation regarding the source of diversity in the proposed formula?** The margin considers two class boundaries. It might not be sufficiently generalizable. It would be beneficial to have a more explicit elucidation on how diversity is incorporated and manifests itself within the algorithmic design. Understanding the origins and implementation of diversity within the formula can enhance the comprehension of its functionality and overall impact on the method's performance. An in-depth clarification would contribute to a more robust and insightful evaluation of the proposed approach’s effectiveness and novelty.\n\n- It seems crucial to **re-evaluate the experiment setups in the study to enhance the reliability and comprehensiveness of the findings**. Including a broader selection of benchmarks in the evaluation process would be instrumental in demonstrating the robustness and versatility of the proposed method across varied scenarios. **A more diversified array of benchmarks** will not only contribute to a deeper, better understanding of the method's performance but also bolster the study's overall credibility and impact. Therefore, revisiting and expanding the experiment setups with additional benchmarks is a highly recommended step to enrich the empirical validation of the study.\n\n- Section 4.4 discusses the influence of $\\gamma$ parameter, indicating that **different benchmarks may require distinct $\\gamma$ parameter values for optimal performance**. This aspect raises a practical concern: how can users effectively determine or choose an appropriate $\\gamma$ value a priori for various benchmarks? The ability to discern and select suitable parameters is crucial for the method's practical applicability and usability in real-world scenarios. Clarification or guidance on this matter would significantly enhance the method’s practical value.", "labeling_timestamp": "2026-01-11T17:12:53.191095", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper asserts novelty (e.g., using output probabilities for fast acquisition and a self-adjusting uncertainty/diversity balance) and cites several prior works, but the provided content does not mention or identify reference '[3]' nor describe a previously introduced linear-time scalable algorithm that FALCUN would be replicating. Therefore there is insufficient information in the paper excerpt to verify the reviewer’s claim that FALCUN replicates ideas from prior work [3] and fails to acknowledge them.", "evidence": "“We propose FALCUN, a novel deep batch active learning method that is labeland time-efficient. … We overcome this structural problem by exclusively operating on the low-dimensional probability space, yielding much faster acquisition times.”", "section": "ABSTRACT"}
{"claim": "A prior approach like PowerBALD or PowerEntropy uses sampling similar to the paper's Eq (6) and proposes more original scaling maintaining linear-time complexity.", "claim_type": "baseline", "paper_id": "yZBpnKpBCw", "paper_title": "Time- and Label-efficient Active Learning by Diversity and Uncertainty of Probabilities", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "AhzqNtmeeP", "reviewer": "Reviewer_Y5kQ", "review_text": "Summary: The paper introduces FALCUN, a method in active learning, which promises efficiencies in labeling and processing. It brings a dynamic acquisition strategy to the table, focusing initially on uncertain instances near decision boundaries and gradually shifting to emphasize batch diversity. While the proposed approach looks new in some aspects, the lack of acknowledgment of previous works that have explored similar territories casts a shadow over its originality.\n\nKey foundational concepts seem to echo pre-existing methodologies in active learning literature, which were not appropriately credited, making FALCUN appear more as an incremental advancement rather than a groundbreaking innovation. Additionally, some experimental results, particularly those involving LeNet and BADGE's performance, seem somewhat inconclusive and could benefit from further rigor and validation to strengthen their credibility.\n\nFurthermore, the paper’s experimental breadth appears limited. A more expansive inclusion of diverse and challenging benchmarks, such as CIFAR-100 and TinyImageNet, would enhance the robustness of FALCUN’s evaluation, providing a more comprehensive understanding of its effectiveness and applicability across various domains. A reconsideration of these aspects could elevate the paper’s contributions and clarity, fostering a more nuanced appreciation of FALCUN's position in the active learning landscape.\n\nStrengths: - The proposed algorithm tries to improve efficiencies in labeling and processing through a dynamic acquisition strategy, emphasizing a transition from focusing on uncertain instances to prioritizing batch diversity, showcasing a potential way in active learning strategies.\n\nWeaknesses: - The paper seems to overlook several crucial references relevant to the discussed topic. Fundamental concepts such as the theoretical importance of the near decision boundary have been comprehensively explored and articulated in previous works, notably in references [1] and [2]. These pivotal papers, along with others, offer profound insights that would augment the paper’s foundational grounding. Moreover, the proposition of a linear-time scalable algorithm, a core element presented in the paper, has previously been introduced and elaborated upon in reference [3]. In addition to that, an influential work cited as [4] such as PowerBALD/or PowerEntropy adopting a sampling similar to Eq (6) but with better originality has also proposed a scaling approach that intriguingly maintains the algorithm’s linear-time complexity.\n\n- The active learning results presented in the paper for LeNet on datasets like EMNIST or RepeatedMNIST seem somewhat unconvincing. Typically, in prevailing literature and experiments, around 300-500 images are required (not > 1000 images) to achieve accuracy comparable to supervised learning methods on these datasets. The reported results in the paper appear to deviate from these established benchmarks. It may be beneficial to revisit and scrutinize the experimental setup, methodology, and the specific implementation of LeNet in the active learning context to ensure that the presented results are robust, reliable, and in alignment with existing standards and expectations in the field. This could enhance the credibility and persuasiveness of the results and the overall contributions of the paper.\n\n- The performance of BADGE as presented in the paper raises some questions. It seems that with appropriate hyperparameter settings, BADGE should be capable of delivering much improved results. This discrepancy suggests that there might be room for optimizing the configuration of BADGE in the experiments, ensuring that it operates under the most suitable conditions for a fair and rigorous comparison. To uphold the integrity and reliability of the comparative analysis, it might be beneficial to revisit and fine-tune the hyperparameters used with BADGE, ensuring that its performance is accurately represented and evaluated against its full potential.\n\n- The experimental section of the paper seems somewhat limited and could be enhanced to bolster the claims made. Incorporating a broader array of realistic scenarios, such as tests involving CIFAR-100, TinyImageNet, or ImageNet, would offer a more comprehensive insight into the method’s applicability and effectiveness. Including such varied and complex datasets in the evaluation would not only strengthen the validity of the results but also improve the generalizability of the conclusions drawn. This expansion in the experimental design would be instrumental in substantiating the method's robustness and adaptability across diverse challenges and use-cases.\n\n[1] Efficient Active Learning with Abstention, NeurIPS 2022 - https://openreview.net/forum?id=4u-oGqB4Lf6\n\n[2] Active Learning with Neural Networks: Insights from Nonparametric Statistics, NeurIPS 2022 - https://openreview.net/forum?id=LRMmgkcoCnW\n\n[3] Active Learning in Bayesian Neural Networks with Balanced Entropy Learning Principle, ICLR 2023 - https://openreview.net/forum?id=ZTMuZ68B1g\n\n[4] Stochastic Batch Acquisition: A Simple Baseline for Deep Active Learning, TMLR 2023 - https://openreview.net/forum?id=vcHwQyNBjW\n\nQuestions: - What criteria determine the **most informative region**? Is a point considered most informative solely based on its proximity to the decision boundary?\n\n- **Could the authors please provide a more explicit explanation regarding the source of diversity in the proposed formula?** The margin considers two class boundaries. It might not be sufficiently generalizable. It would be beneficial to have a more explicit elucidation on how diversity is incorporated and manifests itself within the algorithmic design. Understanding the origins and implementation of diversity within the formula can enhance the comprehension of its functionality and overall impact on the method's performance. An in-depth clarification would contribute to a more robust and insightful evaluation of the proposed approach’s effectiveness and novelty.\n\n- It seems crucial to **re-evaluate the experiment setups in the study to enhance the reliability and comprehensiveness of the findings**. Including a broader selection of benchmarks in the evaluation process would be instrumental in demonstrating the robustness and versatility of the proposed method across varied scenarios. **A more diversified array of benchmarks** will not only contribute to a deeper, better understanding of the method's performance but also bolster the study's overall credibility and impact. Therefore, revisiting and expanding the experiment setups with additional benchmarks is a highly recommended step to enrich the empirical validation of the study.\n\n- Section 4.4 discusses the influence of $\\gamma$ parameter, indicating that **different benchmarks may require distinct $\\gamma$ parameter values for optimal performance**. This aspect raises a practical concern: how can users effectively determine or choose an appropriate $\\gamma$ value a priori for various benchmarks? The ability to discern and select suitable parameters is crucial for the method's practical applicability and usability in real-world scenarios. Clarification or guidance on this matter would significantly enhance the method’s practical value.", "labeling_timestamp": "2026-01-11T17:13:14.159833", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does describe sampling from a distribution proportional to a relevance score (Eq. (6)) with a scaling parameter γ (see Algorithm 1 and Section 2.2), but it does not mention or compare to prior approaches named PowerBALD or PowerEntropy, nor does it state that those prior works use a similar sampling or propose a more original scaling that maintains linear-time complexity. Therefore the claim about those prior approaches cannot be verified from the paper.", "evidence": "Algorithm 1: \"8: Sample from distribution with probability proportional to relevance using Equation (6)\"\n\nSection 2.2: \"Given the relevance scores, we choose x as a next query sample x_q with probability ... where γ is a parameter that controls the influence of the relevance scores. Note that γ = 0 corresponds to a uniform selection and larger values for γ result in a stronger focus on the calculated relevance scores getting more and more deterministic (rich values get richer). Thus, γ controls the trade-off between exploration (more randomness) and exploitation (more focus on larger values in r(x)).\"", "section": "Algorithm 1; 2.2 Acquisition"}
{"claim": "The active learning results for LeNet on EMNIST or RepeatedMNIST seem unconvincing compared to literature that achieves comparable accuracy with 300–500 labeled images.", "claim_type": "baseline", "paper_id": "yZBpnKpBCw", "paper_title": "Time- and Label-efficient Active Learning by Diversity and Uncertainty of Probabilities", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "AhzqNtmeeP", "reviewer": "Reviewer_Y5kQ", "review_text": "Summary: The paper introduces FALCUN, a method in active learning, which promises efficiencies in labeling and processing. It brings a dynamic acquisition strategy to the table, focusing initially on uncertain instances near decision boundaries and gradually shifting to emphasize batch diversity. While the proposed approach looks new in some aspects, the lack of acknowledgment of previous works that have explored similar territories casts a shadow over its originality.\n\nKey foundational concepts seem to echo pre-existing methodologies in active learning literature, which were not appropriately credited, making FALCUN appear more as an incremental advancement rather than a groundbreaking innovation. Additionally, some experimental results, particularly those involving LeNet and BADGE's performance, seem somewhat inconclusive and could benefit from further rigor and validation to strengthen their credibility.\n\nFurthermore, the paper’s experimental breadth appears limited. A more expansive inclusion of diverse and challenging benchmarks, such as CIFAR-100 and TinyImageNet, would enhance the robustness of FALCUN’s evaluation, providing a more comprehensive understanding of its effectiveness and applicability across various domains. A reconsideration of these aspects could elevate the paper’s contributions and clarity, fostering a more nuanced appreciation of FALCUN's position in the active learning landscape.\n\nStrengths: - The proposed algorithm tries to improve efficiencies in labeling and processing through a dynamic acquisition strategy, emphasizing a transition from focusing on uncertain instances to prioritizing batch diversity, showcasing a potential way in active learning strategies.\n\nWeaknesses: - The paper seems to overlook several crucial references relevant to the discussed topic. Fundamental concepts such as the theoretical importance of the near decision boundary have been comprehensively explored and articulated in previous works, notably in references [1] and [2]. These pivotal papers, along with others, offer profound insights that would augment the paper’s foundational grounding. Moreover, the proposition of a linear-time scalable algorithm, a core element presented in the paper, has previously been introduced and elaborated upon in reference [3]. In addition to that, an influential work cited as [4] such as PowerBALD/or PowerEntropy adopting a sampling similar to Eq (6) but with better originality has also proposed a scaling approach that intriguingly maintains the algorithm’s linear-time complexity.\n\n- The active learning results presented in the paper for LeNet on datasets like EMNIST or RepeatedMNIST seem somewhat unconvincing. Typically, in prevailing literature and experiments, around 300-500 images are required (not > 1000 images) to achieve accuracy comparable to supervised learning methods on these datasets. The reported results in the paper appear to deviate from these established benchmarks. It may be beneficial to revisit and scrutinize the experimental setup, methodology, and the specific implementation of LeNet in the active learning context to ensure that the presented results are robust, reliable, and in alignment with existing standards and expectations in the field. This could enhance the credibility and persuasiveness of the results and the overall contributions of the paper.\n\n- The performance of BADGE as presented in the paper raises some questions. It seems that with appropriate hyperparameter settings, BADGE should be capable of delivering much improved results. This discrepancy suggests that there might be room for optimizing the configuration of BADGE in the experiments, ensuring that it operates under the most suitable conditions for a fair and rigorous comparison. To uphold the integrity and reliability of the comparative analysis, it might be beneficial to revisit and fine-tune the hyperparameters used with BADGE, ensuring that its performance is accurately represented and evaluated against its full potential.\n\n- The experimental section of the paper seems somewhat limited and could be enhanced to bolster the claims made. Incorporating a broader array of realistic scenarios, such as tests involving CIFAR-100, TinyImageNet, or ImageNet, would offer a more comprehensive insight into the method’s applicability and effectiveness. Including such varied and complex datasets in the evaluation would not only strengthen the validity of the results but also improve the generalizability of the conclusions drawn. This expansion in the experimental design would be instrumental in substantiating the method's robustness and adaptability across diverse challenges and use-cases.\n\n[1] Efficient Active Learning with Abstention, NeurIPS 2022 - https://openreview.net/forum?id=4u-oGqB4Lf6\n\n[2] Active Learning with Neural Networks: Insights from Nonparametric Statistics, NeurIPS 2022 - https://openreview.net/forum?id=LRMmgkcoCnW\n\n[3] Active Learning in Bayesian Neural Networks with Balanced Entropy Learning Principle, ICLR 2023 - https://openreview.net/forum?id=ZTMuZ68B1g\n\n[4] Stochastic Batch Acquisition: A Simple Baseline for Deep Active Learning, TMLR 2023 - https://openreview.net/forum?id=vcHwQyNBjW\n\nQuestions: - What criteria determine the **most informative region**? Is a point considered most informative solely based on its proximity to the decision boundary?\n\n- **Could the authors please provide a more explicit explanation regarding the source of diversity in the proposed formula?** The margin considers two class boundaries. It might not be sufficiently generalizable. It would be beneficial to have a more explicit elucidation on how diversity is incorporated and manifests itself within the algorithmic design. Understanding the origins and implementation of diversity within the formula can enhance the comprehension of its functionality and overall impact on the method's performance. An in-depth clarification would contribute to a more robust and insightful evaluation of the proposed approach’s effectiveness and novelty.\n\n- It seems crucial to **re-evaluate the experiment setups in the study to enhance the reliability and comprehensiveness of the findings**. Including a broader selection of benchmarks in the evaluation process would be instrumental in demonstrating the robustness and versatility of the proposed method across varied scenarios. **A more diversified array of benchmarks** will not only contribute to a deeper, better understanding of the method's performance but also bolster the study's overall credibility and impact. Therefore, revisiting and expanding the experiment setups with additional benchmarks is a highly recommended step to enrich the empirical validation of the study.\n\n- Section 4.4 discusses the influence of $\\gamma$ parameter, indicating that **different benchmarks may require distinct $\\gamma$ parameter values for optimal performance**. This aspect raises a practical concern: how can users effectively determine or choose an appropriate $\\gamma$ value a priori for various benchmarks? The ability to discern and select suitable parameters is crucial for the method's practical applicability and usability in real-world scenarios. Clarification or guidance on this matter would significantly enhance the method’s practical value.", "labeling_timestamp": "2026-01-11T17:13:21.745115", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper content does not include any experimental results or discussion specifically for LeNet on EMNIST or RepeatedMNIST, nor any numeric comparison showing accuracies with 300–500 labeled images. The abstract and introduction mention extensive experiments and MNIST subsets, but the specific datasets and LeNet results referenced by the reviewer are not present in the supplied text, so the claim cannot be verified from the paper content given.", "evidence": "“In extensive experiments, we show FALCUNs suitability for diverse use cases, including image and tabular data.”\n\n“Figure 1: Each simplex illustrates the probability space of a three-class subset of MNIST.”", "section": "ABSTRACT; 1 INTRODUCTION (Figure 1 caption)"}
{"claim": "The paper reports needing more than 1000 labeled images to match supervised accuracy, which deviates from established benchmarks requiring around 300–500 images.", "claim_type": "experimental", "paper_id": "yZBpnKpBCw", "paper_title": "Time- and Label-efficient Active Learning by Diversity and Uncertainty of Probabilities", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "AhzqNtmeeP", "reviewer": "Reviewer_Y5kQ", "review_text": "Summary: The paper introduces FALCUN, a method in active learning, which promises efficiencies in labeling and processing. It brings a dynamic acquisition strategy to the table, focusing initially on uncertain instances near decision boundaries and gradually shifting to emphasize batch diversity. While the proposed approach looks new in some aspects, the lack of acknowledgment of previous works that have explored similar territories casts a shadow over its originality.\n\nKey foundational concepts seem to echo pre-existing methodologies in active learning literature, which were not appropriately credited, making FALCUN appear more as an incremental advancement rather than a groundbreaking innovation. Additionally, some experimental results, particularly those involving LeNet and BADGE's performance, seem somewhat inconclusive and could benefit from further rigor and validation to strengthen their credibility.\n\nFurthermore, the paper’s experimental breadth appears limited. A more expansive inclusion of diverse and challenging benchmarks, such as CIFAR-100 and TinyImageNet, would enhance the robustness of FALCUN’s evaluation, providing a more comprehensive understanding of its effectiveness and applicability across various domains. A reconsideration of these aspects could elevate the paper’s contributions and clarity, fostering a more nuanced appreciation of FALCUN's position in the active learning landscape.\n\nStrengths: - The proposed algorithm tries to improve efficiencies in labeling and processing through a dynamic acquisition strategy, emphasizing a transition from focusing on uncertain instances to prioritizing batch diversity, showcasing a potential way in active learning strategies.\n\nWeaknesses: - The paper seems to overlook several crucial references relevant to the discussed topic. Fundamental concepts such as the theoretical importance of the near decision boundary have been comprehensively explored and articulated in previous works, notably in references [1] and [2]. These pivotal papers, along with others, offer profound insights that would augment the paper’s foundational grounding. Moreover, the proposition of a linear-time scalable algorithm, a core element presented in the paper, has previously been introduced and elaborated upon in reference [3]. In addition to that, an influential work cited as [4] such as PowerBALD/or PowerEntropy adopting a sampling similar to Eq (6) but with better originality has also proposed a scaling approach that intriguingly maintains the algorithm’s linear-time complexity.\n\n- The active learning results presented in the paper for LeNet on datasets like EMNIST or RepeatedMNIST seem somewhat unconvincing. Typically, in prevailing literature and experiments, around 300-500 images are required (not > 1000 images) to achieve accuracy comparable to supervised learning methods on these datasets. The reported results in the paper appear to deviate from these established benchmarks. It may be beneficial to revisit and scrutinize the experimental setup, methodology, and the specific implementation of LeNet in the active learning context to ensure that the presented results are robust, reliable, and in alignment with existing standards and expectations in the field. This could enhance the credibility and persuasiveness of the results and the overall contributions of the paper.\n\n- The performance of BADGE as presented in the paper raises some questions. It seems that with appropriate hyperparameter settings, BADGE should be capable of delivering much improved results. This discrepancy suggests that there might be room for optimizing the configuration of BADGE in the experiments, ensuring that it operates under the most suitable conditions for a fair and rigorous comparison. To uphold the integrity and reliability of the comparative analysis, it might be beneficial to revisit and fine-tune the hyperparameters used with BADGE, ensuring that its performance is accurately represented and evaluated against its full potential.\n\n- The experimental section of the paper seems somewhat limited and could be enhanced to bolster the claims made. Incorporating a broader array of realistic scenarios, such as tests involving CIFAR-100, TinyImageNet, or ImageNet, would offer a more comprehensive insight into the method’s applicability and effectiveness. Including such varied and complex datasets in the evaluation would not only strengthen the validity of the results but also improve the generalizability of the conclusions drawn. This expansion in the experimental design would be instrumental in substantiating the method's robustness and adaptability across diverse challenges and use-cases.\n\n[1] Efficient Active Learning with Abstention, NeurIPS 2022 - https://openreview.net/forum?id=4u-oGqB4Lf6\n\n[2] Active Learning with Neural Networks: Insights from Nonparametric Statistics, NeurIPS 2022 - https://openreview.net/forum?id=LRMmgkcoCnW\n\n[3] Active Learning in Bayesian Neural Networks with Balanced Entropy Learning Principle, ICLR 2023 - https://openreview.net/forum?id=ZTMuZ68B1g\n\n[4] Stochastic Batch Acquisition: A Simple Baseline for Deep Active Learning, TMLR 2023 - https://openreview.net/forum?id=vcHwQyNBjW\n\nQuestions: - What criteria determine the **most informative region**? Is a point considered most informative solely based on its proximity to the decision boundary?\n\n- **Could the authors please provide a more explicit explanation regarding the source of diversity in the proposed formula?** The margin considers two class boundaries. It might not be sufficiently generalizable. It would be beneficial to have a more explicit elucidation on how diversity is incorporated and manifests itself within the algorithmic design. Understanding the origins and implementation of diversity within the formula can enhance the comprehension of its functionality and overall impact on the method's performance. An in-depth clarification would contribute to a more robust and insightful evaluation of the proposed approach’s effectiveness and novelty.\n\n- It seems crucial to **re-evaluate the experiment setups in the study to enhance the reliability and comprehensiveness of the findings**. Including a broader selection of benchmarks in the evaluation process would be instrumental in demonstrating the robustness and versatility of the proposed method across varied scenarios. **A more diversified array of benchmarks** will not only contribute to a deeper, better understanding of the method's performance but also bolster the study's overall credibility and impact. Therefore, revisiting and expanding the experiment setups with additional benchmarks is a highly recommended step to enrich the empirical validation of the study.\n\n- Section 4.4 discusses the influence of $\\gamma$ parameter, indicating that **different benchmarks may require distinct $\\gamma$ parameter values for optimal performance**. This aspect raises a practical concern: how can users effectively determine or choose an appropriate $\\gamma$ value a priori for various benchmarks? The ability to discern and select suitable parameters is crucial for the method's practical applicability and usability in real-world scenarios. Clarification or guidance on this matter would significantly enhance the method’s practical value.", "labeling_timestamp": "2026-01-11T17:13:30.353209", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not report any experimental result stating that more than 1000 labeled images are needed to match supervised accuracy, nor does it provide the comparative benchmark numbers (300–500) cited by the reviewer. The paper only makes general claims about label efficiency (that FALCUN is label-efficient) but gives no numerical labeled-image thresholds in the supplied sections.", "evidence": "Label Efficiency: Across varying datasets, active learning settings and model architectures FALCUN is always one of the most label-efficient methods.", "section": "INTRODUCTION"}
{"claim": "The LeNet experimental setup and implementation details are insufficiently described and should be revisited to validate the reported results.", "claim_type": "methodology", "paper_id": "yZBpnKpBCw", "paper_title": "Time- and Label-efficient Active Learning by Diversity and Uncertainty of Probabilities", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "AhzqNtmeeP", "reviewer": "Reviewer_Y5kQ", "review_text": "Summary: The paper introduces FALCUN, a method in active learning, which promises efficiencies in labeling and processing. It brings a dynamic acquisition strategy to the table, focusing initially on uncertain instances near decision boundaries and gradually shifting to emphasize batch diversity. While the proposed approach looks new in some aspects, the lack of acknowledgment of previous works that have explored similar territories casts a shadow over its originality.\n\nKey foundational concepts seem to echo pre-existing methodologies in active learning literature, which were not appropriately credited, making FALCUN appear more as an incremental advancement rather than a groundbreaking innovation. Additionally, some experimental results, particularly those involving LeNet and BADGE's performance, seem somewhat inconclusive and could benefit from further rigor and validation to strengthen their credibility.\n\nFurthermore, the paper’s experimental breadth appears limited. A more expansive inclusion of diverse and challenging benchmarks, such as CIFAR-100 and TinyImageNet, would enhance the robustness of FALCUN’s evaluation, providing a more comprehensive understanding of its effectiveness and applicability across various domains. A reconsideration of these aspects could elevate the paper’s contributions and clarity, fostering a more nuanced appreciation of FALCUN's position in the active learning landscape.\n\nStrengths: - The proposed algorithm tries to improve efficiencies in labeling and processing through a dynamic acquisition strategy, emphasizing a transition from focusing on uncertain instances to prioritizing batch diversity, showcasing a potential way in active learning strategies.\n\nWeaknesses: - The paper seems to overlook several crucial references relevant to the discussed topic. Fundamental concepts such as the theoretical importance of the near decision boundary have been comprehensively explored and articulated in previous works, notably in references [1] and [2]. These pivotal papers, along with others, offer profound insights that would augment the paper’s foundational grounding. Moreover, the proposition of a linear-time scalable algorithm, a core element presented in the paper, has previously been introduced and elaborated upon in reference [3]. In addition to that, an influential work cited as [4] such as PowerBALD/or PowerEntropy adopting a sampling similar to Eq (6) but with better originality has also proposed a scaling approach that intriguingly maintains the algorithm’s linear-time complexity.\n\n- The active learning results presented in the paper for LeNet on datasets like EMNIST or RepeatedMNIST seem somewhat unconvincing. Typically, in prevailing literature and experiments, around 300-500 images are required (not > 1000 images) to achieve accuracy comparable to supervised learning methods on these datasets. The reported results in the paper appear to deviate from these established benchmarks. It may be beneficial to revisit and scrutinize the experimental setup, methodology, and the specific implementation of LeNet in the active learning context to ensure that the presented results are robust, reliable, and in alignment with existing standards and expectations in the field. This could enhance the credibility and persuasiveness of the results and the overall contributions of the paper.\n\n- The performance of BADGE as presented in the paper raises some questions. It seems that with appropriate hyperparameter settings, BADGE should be capable of delivering much improved results. This discrepancy suggests that there might be room for optimizing the configuration of BADGE in the experiments, ensuring that it operates under the most suitable conditions for a fair and rigorous comparison. To uphold the integrity and reliability of the comparative analysis, it might be beneficial to revisit and fine-tune the hyperparameters used with BADGE, ensuring that its performance is accurately represented and evaluated against its full potential.\n\n- The experimental section of the paper seems somewhat limited and could be enhanced to bolster the claims made. Incorporating a broader array of realistic scenarios, such as tests involving CIFAR-100, TinyImageNet, or ImageNet, would offer a more comprehensive insight into the method’s applicability and effectiveness. Including such varied and complex datasets in the evaluation would not only strengthen the validity of the results but also improve the generalizability of the conclusions drawn. This expansion in the experimental design would be instrumental in substantiating the method's robustness and adaptability across diverse challenges and use-cases.\n\n[1] Efficient Active Learning with Abstention, NeurIPS 2022 - https://openreview.net/forum?id=4u-oGqB4Lf6\n\n[2] Active Learning with Neural Networks: Insights from Nonparametric Statistics, NeurIPS 2022 - https://openreview.net/forum?id=LRMmgkcoCnW\n\n[3] Active Learning in Bayesian Neural Networks with Balanced Entropy Learning Principle, ICLR 2023 - https://openreview.net/forum?id=ZTMuZ68B1g\n\n[4] Stochastic Batch Acquisition: A Simple Baseline for Deep Active Learning, TMLR 2023 - https://openreview.net/forum?id=vcHwQyNBjW\n\nQuestions: - What criteria determine the **most informative region**? Is a point considered most informative solely based on its proximity to the decision boundary?\n\n- **Could the authors please provide a more explicit explanation regarding the source of diversity in the proposed formula?** The margin considers two class boundaries. It might not be sufficiently generalizable. It would be beneficial to have a more explicit elucidation on how diversity is incorporated and manifests itself within the algorithmic design. Understanding the origins and implementation of diversity within the formula can enhance the comprehension of its functionality and overall impact on the method's performance. An in-depth clarification would contribute to a more robust and insightful evaluation of the proposed approach’s effectiveness and novelty.\n\n- It seems crucial to **re-evaluate the experiment setups in the study to enhance the reliability and comprehensiveness of the findings**. Including a broader selection of benchmarks in the evaluation process would be instrumental in demonstrating the robustness and versatility of the proposed method across varied scenarios. **A more diversified array of benchmarks** will not only contribute to a deeper, better understanding of the method's performance but also bolster the study's overall credibility and impact. Therefore, revisiting and expanding the experiment setups with additional benchmarks is a highly recommended step to enrich the empirical validation of the study.\n\n- Section 4.4 discusses the influence of $\\gamma$ parameter, indicating that **different benchmarks may require distinct $\\gamma$ parameter values for optimal performance**. This aspect raises a practical concern: how can users effectively determine or choose an appropriate $\\gamma$ value a priori for various benchmarks? The ability to discern and select suitable parameters is crucial for the method's practical applicability and usability in real-world scenarios. Clarification or guidance on this matter would significantly enhance the method’s practical value.", "labeling_timestamp": "2026-01-11T17:13:55.934129", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt contains methodology, algorithm pseudocode, and high-level experimental claims (e.g., MNIST example) but does not include any detailed experimental section or specific implementation/setup details for LeNet. Because the excerpt neither presents nor omits LeNet details explicitly, we cannot verify the reviewer's claim from the available content.", "evidence": "We show FALCUNs suitability for diverse use cases, including image and tabular data.", "section": "ABSTRACT"}
{"claim": "BADGE performance appears suboptimal in the paper, suggesting hyperparameters were not adequately tuned for a fair comparison.", "claim_type": "subjective", "paper_id": "yZBpnKpBCw", "paper_title": "Time- and Label-efficient Active Learning by Diversity and Uncertainty of Probabilities", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "AhzqNtmeeP", "reviewer": "Reviewer_Y5kQ", "review_text": "Summary: The paper introduces FALCUN, a method in active learning, which promises efficiencies in labeling and processing. It brings a dynamic acquisition strategy to the table, focusing initially on uncertain instances near decision boundaries and gradually shifting to emphasize batch diversity. While the proposed approach looks new in some aspects, the lack of acknowledgment of previous works that have explored similar territories casts a shadow over its originality.\n\nKey foundational concepts seem to echo pre-existing methodologies in active learning literature, which were not appropriately credited, making FALCUN appear more as an incremental advancement rather than a groundbreaking innovation. Additionally, some experimental results, particularly those involving LeNet and BADGE's performance, seem somewhat inconclusive and could benefit from further rigor and validation to strengthen their credibility.\n\nFurthermore, the paper’s experimental breadth appears limited. A more expansive inclusion of diverse and challenging benchmarks, such as CIFAR-100 and TinyImageNet, would enhance the robustness of FALCUN’s evaluation, providing a more comprehensive understanding of its effectiveness and applicability across various domains. A reconsideration of these aspects could elevate the paper’s contributions and clarity, fostering a more nuanced appreciation of FALCUN's position in the active learning landscape.\n\nStrengths: - The proposed algorithm tries to improve efficiencies in labeling and processing through a dynamic acquisition strategy, emphasizing a transition from focusing on uncertain instances to prioritizing batch diversity, showcasing a potential way in active learning strategies.\n\nWeaknesses: - The paper seems to overlook several crucial references relevant to the discussed topic. Fundamental concepts such as the theoretical importance of the near decision boundary have been comprehensively explored and articulated in previous works, notably in references [1] and [2]. These pivotal papers, along with others, offer profound insights that would augment the paper’s foundational grounding. Moreover, the proposition of a linear-time scalable algorithm, a core element presented in the paper, has previously been introduced and elaborated upon in reference [3]. In addition to that, an influential work cited as [4] such as PowerBALD/or PowerEntropy adopting a sampling similar to Eq (6) but with better originality has also proposed a scaling approach that intriguingly maintains the algorithm’s linear-time complexity.\n\n- The active learning results presented in the paper for LeNet on datasets like EMNIST or RepeatedMNIST seem somewhat unconvincing. Typically, in prevailing literature and experiments, around 300-500 images are required (not > 1000 images) to achieve accuracy comparable to supervised learning methods on these datasets. The reported results in the paper appear to deviate from these established benchmarks. It may be beneficial to revisit and scrutinize the experimental setup, methodology, and the specific implementation of LeNet in the active learning context to ensure that the presented results are robust, reliable, and in alignment with existing standards and expectations in the field. This could enhance the credibility and persuasiveness of the results and the overall contributions of the paper.\n\n- The performance of BADGE as presented in the paper raises some questions. It seems that with appropriate hyperparameter settings, BADGE should be capable of delivering much improved results. This discrepancy suggests that there might be room for optimizing the configuration of BADGE in the experiments, ensuring that it operates under the most suitable conditions for a fair and rigorous comparison. To uphold the integrity and reliability of the comparative analysis, it might be beneficial to revisit and fine-tune the hyperparameters used with BADGE, ensuring that its performance is accurately represented and evaluated against its full potential.\n\n- The experimental section of the paper seems somewhat limited and could be enhanced to bolster the claims made. Incorporating a broader array of realistic scenarios, such as tests involving CIFAR-100, TinyImageNet, or ImageNet, would offer a more comprehensive insight into the method’s applicability and effectiveness. Including such varied and complex datasets in the evaluation would not only strengthen the validity of the results but also improve the generalizability of the conclusions drawn. This expansion in the experimental design would be instrumental in substantiating the method's robustness and adaptability across diverse challenges and use-cases.\n\n[1] Efficient Active Learning with Abstention, NeurIPS 2022 - https://openreview.net/forum?id=4u-oGqB4Lf6\n\n[2] Active Learning with Neural Networks: Insights from Nonparametric Statistics, NeurIPS 2022 - https://openreview.net/forum?id=LRMmgkcoCnW\n\n[3] Active Learning in Bayesian Neural Networks with Balanced Entropy Learning Principle, ICLR 2023 - https://openreview.net/forum?id=ZTMuZ68B1g\n\n[4] Stochastic Batch Acquisition: A Simple Baseline for Deep Active Learning, TMLR 2023 - https://openreview.net/forum?id=vcHwQyNBjW\n\nQuestions: - What criteria determine the **most informative region**? Is a point considered most informative solely based on its proximity to the decision boundary?\n\n- **Could the authors please provide a more explicit explanation regarding the source of diversity in the proposed formula?** The margin considers two class boundaries. It might not be sufficiently generalizable. It would be beneficial to have a more explicit elucidation on how diversity is incorporated and manifests itself within the algorithmic design. Understanding the origins and implementation of diversity within the formula can enhance the comprehension of its functionality and overall impact on the method's performance. An in-depth clarification would contribute to a more robust and insightful evaluation of the proposed approach’s effectiveness and novelty.\n\n- It seems crucial to **re-evaluate the experiment setups in the study to enhance the reliability and comprehensiveness of the findings**. Including a broader selection of benchmarks in the evaluation process would be instrumental in demonstrating the robustness and versatility of the proposed method across varied scenarios. **A more diversified array of benchmarks** will not only contribute to a deeper, better understanding of the method's performance but also bolster the study's overall credibility and impact. Therefore, revisiting and expanding the experiment setups with additional benchmarks is a highly recommended step to enrich the empirical validation of the study.\n\n- Section 4.4 discusses the influence of $\\gamma$ parameter, indicating that **different benchmarks may require distinct $\\gamma$ parameter values for optimal performance**. This aspect raises a practical concern: how can users effectively determine or choose an appropriate $\\gamma$ value a priori for various benchmarks? The ability to discern and select suitable parameters is crucial for the method's practical applicability and usability in real-world scenarios. Clarification or guidance on this matter would significantly enhance the method’s practical value.", "labeling_timestamp": "2026-01-11T17:13:59.155622", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt states that FALCUN outperforms methods such as BADGE (Abstract, Introduction) but contains no experimental details or discussion about BADGE's hyperparameter settings or tuning. There is no evidence in the given content that BADGE was suboptimally tuned or that hyperparameter choices for competing methods were inadequate, so the reviewer’s suggestion cannot be confirmed from the paper text provided.", "evidence": "Compared to state-of-the-art methods like BADGE, CLUE, and AlfaMix, FALCUN consistently excels in quality and speed: while FALCUN is among the fastest methods, it has the highest average label efficiency.", "section": "Abstract"}
{"claim": "The paper does not provide explicit explanation of how diversity is sourced or incorporated in the proposed formula, limiting interpretability and generalizability.", "claim_type": "methodology", "paper_id": "yZBpnKpBCw", "paper_title": "Time- and Label-efficient Active Learning by Diversity and Uncertainty of Probabilities", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "AhzqNtmeeP", "reviewer": "Reviewer_Y5kQ", "review_text": "Summary: The paper introduces FALCUN, a method in active learning, which promises efficiencies in labeling and processing. It brings a dynamic acquisition strategy to the table, focusing initially on uncertain instances near decision boundaries and gradually shifting to emphasize batch diversity. While the proposed approach looks new in some aspects, the lack of acknowledgment of previous works that have explored similar territories casts a shadow over its originality.\n\nKey foundational concepts seem to echo pre-existing methodologies in active learning literature, which were not appropriately credited, making FALCUN appear more as an incremental advancement rather than a groundbreaking innovation. Additionally, some experimental results, particularly those involving LeNet and BADGE's performance, seem somewhat inconclusive and could benefit from further rigor and validation to strengthen their credibility.\n\nFurthermore, the paper’s experimental breadth appears limited. A more expansive inclusion of diverse and challenging benchmarks, such as CIFAR-100 and TinyImageNet, would enhance the robustness of FALCUN’s evaluation, providing a more comprehensive understanding of its effectiveness and applicability across various domains. A reconsideration of these aspects could elevate the paper’s contributions and clarity, fostering a more nuanced appreciation of FALCUN's position in the active learning landscape.\n\nStrengths: - The proposed algorithm tries to improve efficiencies in labeling and processing through a dynamic acquisition strategy, emphasizing a transition from focusing on uncertain instances to prioritizing batch diversity, showcasing a potential way in active learning strategies.\n\nWeaknesses: - The paper seems to overlook several crucial references relevant to the discussed topic. Fundamental concepts such as the theoretical importance of the near decision boundary have been comprehensively explored and articulated in previous works, notably in references [1] and [2]. These pivotal papers, along with others, offer profound insights that would augment the paper’s foundational grounding. Moreover, the proposition of a linear-time scalable algorithm, a core element presented in the paper, has previously been introduced and elaborated upon in reference [3]. In addition to that, an influential work cited as [4] such as PowerBALD/or PowerEntropy adopting a sampling similar to Eq (6) but with better originality has also proposed a scaling approach that intriguingly maintains the algorithm’s linear-time complexity.\n\n- The active learning results presented in the paper for LeNet on datasets like EMNIST or RepeatedMNIST seem somewhat unconvincing. Typically, in prevailing literature and experiments, around 300-500 images are required (not > 1000 images) to achieve accuracy comparable to supervised learning methods on these datasets. The reported results in the paper appear to deviate from these established benchmarks. It may be beneficial to revisit and scrutinize the experimental setup, methodology, and the specific implementation of LeNet in the active learning context to ensure that the presented results are robust, reliable, and in alignment with existing standards and expectations in the field. This could enhance the credibility and persuasiveness of the results and the overall contributions of the paper.\n\n- The performance of BADGE as presented in the paper raises some questions. It seems that with appropriate hyperparameter settings, BADGE should be capable of delivering much improved results. This discrepancy suggests that there might be room for optimizing the configuration of BADGE in the experiments, ensuring that it operates under the most suitable conditions for a fair and rigorous comparison. To uphold the integrity and reliability of the comparative analysis, it might be beneficial to revisit and fine-tune the hyperparameters used with BADGE, ensuring that its performance is accurately represented and evaluated against its full potential.\n\n- The experimental section of the paper seems somewhat limited and could be enhanced to bolster the claims made. Incorporating a broader array of realistic scenarios, such as tests involving CIFAR-100, TinyImageNet, or ImageNet, would offer a more comprehensive insight into the method’s applicability and effectiveness. Including such varied and complex datasets in the evaluation would not only strengthen the validity of the results but also improve the generalizability of the conclusions drawn. This expansion in the experimental design would be instrumental in substantiating the method's robustness and adaptability across diverse challenges and use-cases.\n\n[1] Efficient Active Learning with Abstention, NeurIPS 2022 - https://openreview.net/forum?id=4u-oGqB4Lf6\n\n[2] Active Learning with Neural Networks: Insights from Nonparametric Statistics, NeurIPS 2022 - https://openreview.net/forum?id=LRMmgkcoCnW\n\n[3] Active Learning in Bayesian Neural Networks with Balanced Entropy Learning Principle, ICLR 2023 - https://openreview.net/forum?id=ZTMuZ68B1g\n\n[4] Stochastic Batch Acquisition: A Simple Baseline for Deep Active Learning, TMLR 2023 - https://openreview.net/forum?id=vcHwQyNBjW\n\nQuestions: - What criteria determine the **most informative region**? Is a point considered most informative solely based on its proximity to the decision boundary?\n\n- **Could the authors please provide a more explicit explanation regarding the source of diversity in the proposed formula?** The margin considers two class boundaries. It might not be sufficiently generalizable. It would be beneficial to have a more explicit elucidation on how diversity is incorporated and manifests itself within the algorithmic design. Understanding the origins and implementation of diversity within the formula can enhance the comprehension of its functionality and overall impact on the method's performance. An in-depth clarification would contribute to a more robust and insightful evaluation of the proposed approach’s effectiveness and novelty.\n\n- It seems crucial to **re-evaluate the experiment setups in the study to enhance the reliability and comprehensiveness of the findings**. Including a broader selection of benchmarks in the evaluation process would be instrumental in demonstrating the robustness and versatility of the proposed method across varied scenarios. **A more diversified array of benchmarks** will not only contribute to a deeper, better understanding of the method's performance but also bolster the study's overall credibility and impact. Therefore, revisiting and expanding the experiment setups with additional benchmarks is a highly recommended step to enrich the empirical validation of the study.\n\n- Section 4.4 discusses the influence of $\\gamma$ parameter, indicating that **different benchmarks may require distinct $\\gamma$ parameter values for optimal performance**. This aspect raises a practical concern: how can users effectively determine or choose an appropriate $\\gamma$ value a priori for various benchmarks? The ability to discern and select suitable parameters is crucial for the method's practical applicability and usability in real-world scenarios. Clarification or guidance on this matter would significantly enhance the method’s practical value.", "labeling_timestamp": "2026-01-11T17:13:35.352596", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly describes how diversity is derived and incorporated: it initializes the diversity score from the margin uncertainty, updates the diversity score after each selected sample, normalizes it, and combines it with the uncertainty score to form the final relevance r(x)=u(x)+d(x). These explanations appear in Section 2.2 (Acquisition).", "evidence": "“we initialize the diversity score with the already calculated margin uncertainty and update it with every chosen sample x_q” | “Finally, we normalize the values to [0,1] to align it with the uncertainty scores” | “we combine the uncertainty and the diversity component by defining r(x) as the sum of the uncertainty u(x) and the normalized adaptive diversity score d(x)”", "section": "2.2 ACQUISITION"}
{"claim": "The margin-based measure considers two class boundaries and may not generalize to multiclass settings without further justification.", "claim_type": "methodology", "paper_id": "yZBpnKpBCw", "paper_title": "Time- and Label-efficient Active Learning by Diversity and Uncertainty of Probabilities", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "AhzqNtmeeP", "reviewer": "Reviewer_Y5kQ", "review_text": "Summary: The paper introduces FALCUN, a method in active learning, which promises efficiencies in labeling and processing. It brings a dynamic acquisition strategy to the table, focusing initially on uncertain instances near decision boundaries and gradually shifting to emphasize batch diversity. While the proposed approach looks new in some aspects, the lack of acknowledgment of previous works that have explored similar territories casts a shadow over its originality.\n\nKey foundational concepts seem to echo pre-existing methodologies in active learning literature, which were not appropriately credited, making FALCUN appear more as an incremental advancement rather than a groundbreaking innovation. Additionally, some experimental results, particularly those involving LeNet and BADGE's performance, seem somewhat inconclusive and could benefit from further rigor and validation to strengthen their credibility.\n\nFurthermore, the paper’s experimental breadth appears limited. A more expansive inclusion of diverse and challenging benchmarks, such as CIFAR-100 and TinyImageNet, would enhance the robustness of FALCUN’s evaluation, providing a more comprehensive understanding of its effectiveness and applicability across various domains. A reconsideration of these aspects could elevate the paper’s contributions and clarity, fostering a more nuanced appreciation of FALCUN's position in the active learning landscape.\n\nStrengths: - The proposed algorithm tries to improve efficiencies in labeling and processing through a dynamic acquisition strategy, emphasizing a transition from focusing on uncertain instances to prioritizing batch diversity, showcasing a potential way in active learning strategies.\n\nWeaknesses: - The paper seems to overlook several crucial references relevant to the discussed topic. Fundamental concepts such as the theoretical importance of the near decision boundary have been comprehensively explored and articulated in previous works, notably in references [1] and [2]. These pivotal papers, along with others, offer profound insights that would augment the paper’s foundational grounding. Moreover, the proposition of a linear-time scalable algorithm, a core element presented in the paper, has previously been introduced and elaborated upon in reference [3]. In addition to that, an influential work cited as [4] such as PowerBALD/or PowerEntropy adopting a sampling similar to Eq (6) but with better originality has also proposed a scaling approach that intriguingly maintains the algorithm’s linear-time complexity.\n\n- The active learning results presented in the paper for LeNet on datasets like EMNIST or RepeatedMNIST seem somewhat unconvincing. Typically, in prevailing literature and experiments, around 300-500 images are required (not > 1000 images) to achieve accuracy comparable to supervised learning methods on these datasets. The reported results in the paper appear to deviate from these established benchmarks. It may be beneficial to revisit and scrutinize the experimental setup, methodology, and the specific implementation of LeNet in the active learning context to ensure that the presented results are robust, reliable, and in alignment with existing standards and expectations in the field. This could enhance the credibility and persuasiveness of the results and the overall contributions of the paper.\n\n- The performance of BADGE as presented in the paper raises some questions. It seems that with appropriate hyperparameter settings, BADGE should be capable of delivering much improved results. This discrepancy suggests that there might be room for optimizing the configuration of BADGE in the experiments, ensuring that it operates under the most suitable conditions for a fair and rigorous comparison. To uphold the integrity and reliability of the comparative analysis, it might be beneficial to revisit and fine-tune the hyperparameters used with BADGE, ensuring that its performance is accurately represented and evaluated against its full potential.\n\n- The experimental section of the paper seems somewhat limited and could be enhanced to bolster the claims made. Incorporating a broader array of realistic scenarios, such as tests involving CIFAR-100, TinyImageNet, or ImageNet, would offer a more comprehensive insight into the method’s applicability and effectiveness. Including such varied and complex datasets in the evaluation would not only strengthen the validity of the results but also improve the generalizability of the conclusions drawn. This expansion in the experimental design would be instrumental in substantiating the method's robustness and adaptability across diverse challenges and use-cases.\n\n[1] Efficient Active Learning with Abstention, NeurIPS 2022 - https://openreview.net/forum?id=4u-oGqB4Lf6\n\n[2] Active Learning with Neural Networks: Insights from Nonparametric Statistics, NeurIPS 2022 - https://openreview.net/forum?id=LRMmgkcoCnW\n\n[3] Active Learning in Bayesian Neural Networks with Balanced Entropy Learning Principle, ICLR 2023 - https://openreview.net/forum?id=ZTMuZ68B1g\n\n[4] Stochastic Batch Acquisition: A Simple Baseline for Deep Active Learning, TMLR 2023 - https://openreview.net/forum?id=vcHwQyNBjW\n\nQuestions: - What criteria determine the **most informative region**? Is a point considered most informative solely based on its proximity to the decision boundary?\n\n- **Could the authors please provide a more explicit explanation regarding the source of diversity in the proposed formula?** The margin considers two class boundaries. It might not be sufficiently generalizable. It would be beneficial to have a more explicit elucidation on how diversity is incorporated and manifests itself within the algorithmic design. Understanding the origins and implementation of diversity within the formula can enhance the comprehension of its functionality and overall impact on the method's performance. An in-depth clarification would contribute to a more robust and insightful evaluation of the proposed approach’s effectiveness and novelty.\n\n- It seems crucial to **re-evaluate the experiment setups in the study to enhance the reliability and comprehensiveness of the findings**. Including a broader selection of benchmarks in the evaluation process would be instrumental in demonstrating the robustness and versatility of the proposed method across varied scenarios. **A more diversified array of benchmarks** will not only contribute to a deeper, better understanding of the method's performance but also bolster the study's overall credibility and impact. Therefore, revisiting and expanding the experiment setups with additional benchmarks is a highly recommended step to enrich the empirical validation of the study.\n\n- Section 4.4 discusses the influence of $\\gamma$ parameter, indicating that **different benchmarks may require distinct $\\gamma$ parameter values for optimal performance**. This aspect raises a practical concern: how can users effectively determine or choose an appropriate $\\gamma$ value a priori for various benchmarks? The ability to discern and select suitable parameters is crucial for the method's practical applicability and usability in real-world scenarios. Clarification or guidance on this matter would significantly enhance the method’s practical value.", "labeling_timestamp": "2026-01-11T17:13:52.629204", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly defines and justifies margin uncertainty in the multiclass setting: it defines margin as the difference between the top two class probabilities for C classes, states that margin 'naturally captures the class boundaries between each class pairs', and refers to Appendix A.2 for a formal relation to one-hot class encodings. Thus the paper provides justification for using margin in multiclass settings, contradicting the reviewer's claim.", "evidence": "“Our task is multi-class classification on an input space X and a set of labels Y = {1, . . . , C} for C classes.”\n\n“For uncertainty, we use the margin uncertainty, i.e., the difference between the probabilities of its two most probable classes: … where 0 ≤ u(x) ≤ 1. Margin is a common choice for uncertainty (Roth & Small, 2006; Bahri et al., 2022; Jiang & Gupta, 2021) and naturally captures the class boundaries between each class pairs. In contrast to other simple uncertainty scores like entropy or least confidence, the margin uncertainty has an extremal function that contains a diverse set of samples: its optima lie on the class boundaries in the probability space.”\n\n“In Appendix A.2, we show the relation of margin uncertainty and the distances of objects to the one-hot encodings of their two most probable classes in the probability space.”", "section": "2.1 NOTATION; 2.2 ACQUISITION (Uncertainty Component); Appendix A.2"}
{"claim": "Experimental evaluation is limited and omits more challenging benchmarks such as CIFAR-100, TinyImageNet, and ImageNet.", "claim_type": "experimental", "paper_id": "yZBpnKpBCw", "paper_title": "Time- and Label-efficient Active Learning by Diversity and Uncertainty of Probabilities", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "AhzqNtmeeP", "reviewer": "Reviewer_Y5kQ", "review_text": "Summary: The paper introduces FALCUN, a method in active learning, which promises efficiencies in labeling and processing. It brings a dynamic acquisition strategy to the table, focusing initially on uncertain instances near decision boundaries and gradually shifting to emphasize batch diversity. While the proposed approach looks new in some aspects, the lack of acknowledgment of previous works that have explored similar territories casts a shadow over its originality.\n\nKey foundational concepts seem to echo pre-existing methodologies in active learning literature, which were not appropriately credited, making FALCUN appear more as an incremental advancement rather than a groundbreaking innovation. Additionally, some experimental results, particularly those involving LeNet and BADGE's performance, seem somewhat inconclusive and could benefit from further rigor and validation to strengthen their credibility.\n\nFurthermore, the paper’s experimental breadth appears limited. A more expansive inclusion of diverse and challenging benchmarks, such as CIFAR-100 and TinyImageNet, would enhance the robustness of FALCUN’s evaluation, providing a more comprehensive understanding of its effectiveness and applicability across various domains. A reconsideration of these aspects could elevate the paper’s contributions and clarity, fostering a more nuanced appreciation of FALCUN's position in the active learning landscape.\n\nStrengths: - The proposed algorithm tries to improve efficiencies in labeling and processing through a dynamic acquisition strategy, emphasizing a transition from focusing on uncertain instances to prioritizing batch diversity, showcasing a potential way in active learning strategies.\n\nWeaknesses: - The paper seems to overlook several crucial references relevant to the discussed topic. Fundamental concepts such as the theoretical importance of the near decision boundary have been comprehensively explored and articulated in previous works, notably in references [1] and [2]. These pivotal papers, along with others, offer profound insights that would augment the paper’s foundational grounding. Moreover, the proposition of a linear-time scalable algorithm, a core element presented in the paper, has previously been introduced and elaborated upon in reference [3]. In addition to that, an influential work cited as [4] such as PowerBALD/or PowerEntropy adopting a sampling similar to Eq (6) but with better originality has also proposed a scaling approach that intriguingly maintains the algorithm’s linear-time complexity.\n\n- The active learning results presented in the paper for LeNet on datasets like EMNIST or RepeatedMNIST seem somewhat unconvincing. Typically, in prevailing literature and experiments, around 300-500 images are required (not > 1000 images) to achieve accuracy comparable to supervised learning methods on these datasets. The reported results in the paper appear to deviate from these established benchmarks. It may be beneficial to revisit and scrutinize the experimental setup, methodology, and the specific implementation of LeNet in the active learning context to ensure that the presented results are robust, reliable, and in alignment with existing standards and expectations in the field. This could enhance the credibility and persuasiveness of the results and the overall contributions of the paper.\n\n- The performance of BADGE as presented in the paper raises some questions. It seems that with appropriate hyperparameter settings, BADGE should be capable of delivering much improved results. This discrepancy suggests that there might be room for optimizing the configuration of BADGE in the experiments, ensuring that it operates under the most suitable conditions for a fair and rigorous comparison. To uphold the integrity and reliability of the comparative analysis, it might be beneficial to revisit and fine-tune the hyperparameters used with BADGE, ensuring that its performance is accurately represented and evaluated against its full potential.\n\n- The experimental section of the paper seems somewhat limited and could be enhanced to bolster the claims made. Incorporating a broader array of realistic scenarios, such as tests involving CIFAR-100, TinyImageNet, or ImageNet, would offer a more comprehensive insight into the method’s applicability and effectiveness. Including such varied and complex datasets in the evaluation would not only strengthen the validity of the results but also improve the generalizability of the conclusions drawn. This expansion in the experimental design would be instrumental in substantiating the method's robustness and adaptability across diverse challenges and use-cases.\n\n[1] Efficient Active Learning with Abstention, NeurIPS 2022 - https://openreview.net/forum?id=4u-oGqB4Lf6\n\n[2] Active Learning with Neural Networks: Insights from Nonparametric Statistics, NeurIPS 2022 - https://openreview.net/forum?id=LRMmgkcoCnW\n\n[3] Active Learning in Bayesian Neural Networks with Balanced Entropy Learning Principle, ICLR 2023 - https://openreview.net/forum?id=ZTMuZ68B1g\n\n[4] Stochastic Batch Acquisition: A Simple Baseline for Deep Active Learning, TMLR 2023 - https://openreview.net/forum?id=vcHwQyNBjW\n\nQuestions: - What criteria determine the **most informative region**? Is a point considered most informative solely based on its proximity to the decision boundary?\n\n- **Could the authors please provide a more explicit explanation regarding the source of diversity in the proposed formula?** The margin considers two class boundaries. It might not be sufficiently generalizable. It would be beneficial to have a more explicit elucidation on how diversity is incorporated and manifests itself within the algorithmic design. Understanding the origins and implementation of diversity within the formula can enhance the comprehension of its functionality and overall impact on the method's performance. An in-depth clarification would contribute to a more robust and insightful evaluation of the proposed approach’s effectiveness and novelty.\n\n- It seems crucial to **re-evaluate the experiment setups in the study to enhance the reliability and comprehensiveness of the findings**. Including a broader selection of benchmarks in the evaluation process would be instrumental in demonstrating the robustness and versatility of the proposed method across varied scenarios. **A more diversified array of benchmarks** will not only contribute to a deeper, better understanding of the method's performance but also bolster the study's overall credibility and impact. Therefore, revisiting and expanding the experiment setups with additional benchmarks is a highly recommended step to enrich the empirical validation of the study.\n\n- Section 4.4 discusses the influence of $\\gamma$ parameter, indicating that **different benchmarks may require distinct $\\gamma$ parameter values for optimal performance**. This aspect raises a practical concern: how can users effectively determine or choose an appropriate $\\gamma$ value a priori for various benchmarks? The ability to discern and select suitable parameters is crucial for the method's practical applicability and usability in real-world scenarios. Clarification or guidance on this matter would significantly enhance the method’s practical value.", "labeling_timestamp": "2026-01-11T17:14:13.421278", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpts describe “extensive experiments” and show results on MNIST, but do not list the full set of benchmarks used. Because the experimental section and full dataset list (e.g., Section 4) are not included here, the claim that the evaluation omits CIFAR-100, TinyImageNet, and ImageNet cannot be verified from the given content.", "evidence": "“In extensive experiments, we show FALCUNs suitability for diverse use cases, including image and tabular data.” / “Figure 1 shows the runtimes and selected samples of different AL methods for a subset of the MNIST dataset with three classes: our approach selects diverse and informative samples while it is among the fastest.”", "section": "Abstract; Introduction (Figure 1)"}
{"claim": "Section 4.4 indicates different benchmarks require distinct gamma values but the paper provides no guidance for choosing gamma a priori.", "claim_type": "methodology", "paper_id": "yZBpnKpBCw", "paper_title": "Time- and Label-efficient Active Learning by Diversity and Uncertainty of Probabilities", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "AhzqNtmeeP", "reviewer": "Reviewer_Y5kQ", "review_text": "Summary: The paper introduces FALCUN, a method in active learning, which promises efficiencies in labeling and processing. It brings a dynamic acquisition strategy to the table, focusing initially on uncertain instances near decision boundaries and gradually shifting to emphasize batch diversity. While the proposed approach looks new in some aspects, the lack of acknowledgment of previous works that have explored similar territories casts a shadow over its originality.\n\nKey foundational concepts seem to echo pre-existing methodologies in active learning literature, which were not appropriately credited, making FALCUN appear more as an incremental advancement rather than a groundbreaking innovation. Additionally, some experimental results, particularly those involving LeNet and BADGE's performance, seem somewhat inconclusive and could benefit from further rigor and validation to strengthen their credibility.\n\nFurthermore, the paper’s experimental breadth appears limited. A more expansive inclusion of diverse and challenging benchmarks, such as CIFAR-100 and TinyImageNet, would enhance the robustness of FALCUN’s evaluation, providing a more comprehensive understanding of its effectiveness and applicability across various domains. A reconsideration of these aspects could elevate the paper’s contributions and clarity, fostering a more nuanced appreciation of FALCUN's position in the active learning landscape.\n\nStrengths: - The proposed algorithm tries to improve efficiencies in labeling and processing through a dynamic acquisition strategy, emphasizing a transition from focusing on uncertain instances to prioritizing batch diversity, showcasing a potential way in active learning strategies.\n\nWeaknesses: - The paper seems to overlook several crucial references relevant to the discussed topic. Fundamental concepts such as the theoretical importance of the near decision boundary have been comprehensively explored and articulated in previous works, notably in references [1] and [2]. These pivotal papers, along with others, offer profound insights that would augment the paper’s foundational grounding. Moreover, the proposition of a linear-time scalable algorithm, a core element presented in the paper, has previously been introduced and elaborated upon in reference [3]. In addition to that, an influential work cited as [4] such as PowerBALD/or PowerEntropy adopting a sampling similar to Eq (6) but with better originality has also proposed a scaling approach that intriguingly maintains the algorithm’s linear-time complexity.\n\n- The active learning results presented in the paper for LeNet on datasets like EMNIST or RepeatedMNIST seem somewhat unconvincing. Typically, in prevailing literature and experiments, around 300-500 images are required (not > 1000 images) to achieve accuracy comparable to supervised learning methods on these datasets. The reported results in the paper appear to deviate from these established benchmarks. It may be beneficial to revisit and scrutinize the experimental setup, methodology, and the specific implementation of LeNet in the active learning context to ensure that the presented results are robust, reliable, and in alignment with existing standards and expectations in the field. This could enhance the credibility and persuasiveness of the results and the overall contributions of the paper.\n\n- The performance of BADGE as presented in the paper raises some questions. It seems that with appropriate hyperparameter settings, BADGE should be capable of delivering much improved results. This discrepancy suggests that there might be room for optimizing the configuration of BADGE in the experiments, ensuring that it operates under the most suitable conditions for a fair and rigorous comparison. To uphold the integrity and reliability of the comparative analysis, it might be beneficial to revisit and fine-tune the hyperparameters used with BADGE, ensuring that its performance is accurately represented and evaluated against its full potential.\n\n- The experimental section of the paper seems somewhat limited and could be enhanced to bolster the claims made. Incorporating a broader array of realistic scenarios, such as tests involving CIFAR-100, TinyImageNet, or ImageNet, would offer a more comprehensive insight into the method’s applicability and effectiveness. Including such varied and complex datasets in the evaluation would not only strengthen the validity of the results but also improve the generalizability of the conclusions drawn. This expansion in the experimental design would be instrumental in substantiating the method's robustness and adaptability across diverse challenges and use-cases.\n\n[1] Efficient Active Learning with Abstention, NeurIPS 2022 - https://openreview.net/forum?id=4u-oGqB4Lf6\n\n[2] Active Learning with Neural Networks: Insights from Nonparametric Statistics, NeurIPS 2022 - https://openreview.net/forum?id=LRMmgkcoCnW\n\n[3] Active Learning in Bayesian Neural Networks with Balanced Entropy Learning Principle, ICLR 2023 - https://openreview.net/forum?id=ZTMuZ68B1g\n\n[4] Stochastic Batch Acquisition: A Simple Baseline for Deep Active Learning, TMLR 2023 - https://openreview.net/forum?id=vcHwQyNBjW\n\nQuestions: - What criteria determine the **most informative region**? Is a point considered most informative solely based on its proximity to the decision boundary?\n\n- **Could the authors please provide a more explicit explanation regarding the source of diversity in the proposed formula?** The margin considers two class boundaries. It might not be sufficiently generalizable. It would be beneficial to have a more explicit elucidation on how diversity is incorporated and manifests itself within the algorithmic design. Understanding the origins and implementation of diversity within the formula can enhance the comprehension of its functionality and overall impact on the method's performance. An in-depth clarification would contribute to a more robust and insightful evaluation of the proposed approach’s effectiveness and novelty.\n\n- It seems crucial to **re-evaluate the experiment setups in the study to enhance the reliability and comprehensiveness of the findings**. Including a broader selection of benchmarks in the evaluation process would be instrumental in demonstrating the robustness and versatility of the proposed method across varied scenarios. **A more diversified array of benchmarks** will not only contribute to a deeper, better understanding of the method's performance but also bolster the study's overall credibility and impact. Therefore, revisiting and expanding the experiment setups with additional benchmarks is a highly recommended step to enrich the empirical validation of the study.\n\n- Section 4.4 discusses the influence of $\\gamma$ parameter, indicating that **different benchmarks may require distinct $\\gamma$ parameter values for optimal performance**. This aspect raises a practical concern: how can users effectively determine or choose an appropriate $\\gamma$ value a priori for various benchmarks? The ability to discern and select suitable parameters is crucial for the method's practical applicability and usability in real-world scenarios. Clarification or guidance on this matter would significantly enhance the method’s practical value.", "labeling_timestamp": "2026-01-11T17:14:10.996298", "model": "gpt-5-mini", "label": "Not Determined", "justification": "Section 2.2 describes the role of γ and states that its effect is analyzed in Section 4.4, but the provided paper content does not include Section 4.4 or any experimental guidance for choosing γ a priori. Therefore, there is insufficient information to confirm the reviewer’s specific claim about Section 4.4 indicating dataset-specific γ values and the absence of guidance.", "evidence": "“Note that γ = 0 corresponds to a uniform selection and larger values for γ result in a stronger focus on the calculated relevance scores getting more and more deterministic (rich values get richer). Thus, γ controls the trade-off between exploration (more randomness) and exploitation (more focus on larger values in r(x)). We analyze the effect of γ in Section 4.4.”", "section": "2.2 Acquisition"}
{"claim": "The presentation lacks explicit acknowledgment of prior works [1], [2], [3], and [4], which undermines the paper's originality claim.", "claim_type": "novelty", "paper_id": "yZBpnKpBCw", "paper_title": "Time- and Label-efficient Active Learning by Diversity and Uncertainty of Probabilities", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "AhzqNtmeeP", "reviewer": "Reviewer_Y5kQ", "review_text": "Summary: The paper introduces FALCUN, a method in active learning, which promises efficiencies in labeling and processing. It brings a dynamic acquisition strategy to the table, focusing initially on uncertain instances near decision boundaries and gradually shifting to emphasize batch diversity. While the proposed approach looks new in some aspects, the lack of acknowledgment of previous works that have explored similar territories casts a shadow over its originality.\n\nKey foundational concepts seem to echo pre-existing methodologies in active learning literature, which were not appropriately credited, making FALCUN appear more as an incremental advancement rather than a groundbreaking innovation. Additionally, some experimental results, particularly those involving LeNet and BADGE's performance, seem somewhat inconclusive and could benefit from further rigor and validation to strengthen their credibility.\n\nFurthermore, the paper’s experimental breadth appears limited. A more expansive inclusion of diverse and challenging benchmarks, such as CIFAR-100 and TinyImageNet, would enhance the robustness of FALCUN’s evaluation, providing a more comprehensive understanding of its effectiveness and applicability across various domains. A reconsideration of these aspects could elevate the paper’s contributions and clarity, fostering a more nuanced appreciation of FALCUN's position in the active learning landscape.\n\nStrengths: - The proposed algorithm tries to improve efficiencies in labeling and processing through a dynamic acquisition strategy, emphasizing a transition from focusing on uncertain instances to prioritizing batch diversity, showcasing a potential way in active learning strategies.\n\nWeaknesses: - The paper seems to overlook several crucial references relevant to the discussed topic. Fundamental concepts such as the theoretical importance of the near decision boundary have been comprehensively explored and articulated in previous works, notably in references [1] and [2]. These pivotal papers, along with others, offer profound insights that would augment the paper’s foundational grounding. Moreover, the proposition of a linear-time scalable algorithm, a core element presented in the paper, has previously been introduced and elaborated upon in reference [3]. In addition to that, an influential work cited as [4] such as PowerBALD/or PowerEntropy adopting a sampling similar to Eq (6) but with better originality has also proposed a scaling approach that intriguingly maintains the algorithm’s linear-time complexity.\n\n- The active learning results presented in the paper for LeNet on datasets like EMNIST or RepeatedMNIST seem somewhat unconvincing. Typically, in prevailing literature and experiments, around 300-500 images are required (not > 1000 images) to achieve accuracy comparable to supervised learning methods on these datasets. The reported results in the paper appear to deviate from these established benchmarks. It may be beneficial to revisit and scrutinize the experimental setup, methodology, and the specific implementation of LeNet in the active learning context to ensure that the presented results are robust, reliable, and in alignment with existing standards and expectations in the field. This could enhance the credibility and persuasiveness of the results and the overall contributions of the paper.\n\n- The performance of BADGE as presented in the paper raises some questions. It seems that with appropriate hyperparameter settings, BADGE should be capable of delivering much improved results. This discrepancy suggests that there might be room for optimizing the configuration of BADGE in the experiments, ensuring that it operates under the most suitable conditions for a fair and rigorous comparison. To uphold the integrity and reliability of the comparative analysis, it might be beneficial to revisit and fine-tune the hyperparameters used with BADGE, ensuring that its performance is accurately represented and evaluated against its full potential.\n\n- The experimental section of the paper seems somewhat limited and could be enhanced to bolster the claims made. Incorporating a broader array of realistic scenarios, such as tests involving CIFAR-100, TinyImageNet, or ImageNet, would offer a more comprehensive insight into the method’s applicability and effectiveness. Including such varied and complex datasets in the evaluation would not only strengthen the validity of the results but also improve the generalizability of the conclusions drawn. This expansion in the experimental design would be instrumental in substantiating the method's robustness and adaptability across diverse challenges and use-cases.\n\n[1] Efficient Active Learning with Abstention, NeurIPS 2022 - https://openreview.net/forum?id=4u-oGqB4Lf6\n\n[2] Active Learning with Neural Networks: Insights from Nonparametric Statistics, NeurIPS 2022 - https://openreview.net/forum?id=LRMmgkcoCnW\n\n[3] Active Learning in Bayesian Neural Networks with Balanced Entropy Learning Principle, ICLR 2023 - https://openreview.net/forum?id=ZTMuZ68B1g\n\n[4] Stochastic Batch Acquisition: A Simple Baseline for Deep Active Learning, TMLR 2023 - https://openreview.net/forum?id=vcHwQyNBjW\n\nQuestions: - What criteria determine the **most informative region**? Is a point considered most informative solely based on its proximity to the decision boundary?\n\n- **Could the authors please provide a more explicit explanation regarding the source of diversity in the proposed formula?** The margin considers two class boundaries. It might not be sufficiently generalizable. It would be beneficial to have a more explicit elucidation on how diversity is incorporated and manifests itself within the algorithmic design. Understanding the origins and implementation of diversity within the formula can enhance the comprehension of its functionality and overall impact on the method's performance. An in-depth clarification would contribute to a more robust and insightful evaluation of the proposed approach’s effectiveness and novelty.\n\n- It seems crucial to **re-evaluate the experiment setups in the study to enhance the reliability and comprehensiveness of the findings**. Including a broader selection of benchmarks in the evaluation process would be instrumental in demonstrating the robustness and versatility of the proposed method across varied scenarios. **A more diversified array of benchmarks** will not only contribute to a deeper, better understanding of the method's performance but also bolster the study's overall credibility and impact. Therefore, revisiting and expanding the experiment setups with additional benchmarks is a highly recommended step to enrich the empirical validation of the study.\n\n- Section 4.4 discusses the influence of $\\gamma$ parameter, indicating that **different benchmarks may require distinct $\\gamma$ parameter values for optimal performance**. This aspect raises a practical concern: how can users effectively determine or choose an appropriate $\\gamma$ value a priori for various benchmarks? The ability to discern and select suitable parameters is crucial for the method's practical applicability and usability in real-world scenarios. Clarification or guidance on this matter would significantly enhance the method’s practical value.", "labeling_timestamp": "2026-01-11T17:14:19.616630", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly cites and discusses multiple prior works on diversity and uncertainty in active learning (e.g., Ash et al., Sener & Savarese, Prabhu et al., Zhdanov, Kirsch et al.), so the claim that the presentation lacks explicit acknowledgment of prior works is false.", "evidence": "To measure diversity, established AL methods often exploit the latent representations of a deep learning model (Ash et al., 2020; Sener & Savarese, 2018; Prabhu et al., 2021; Zhdanov, 2019).", "section": "1 INTRODUCTION"}
{"claim": "Experimental results, particularly for LeNet and BADGE, are inconclusive and require further rigor and validation to strengthen the paper's credibility.", "claim_type": "subjective", "paper_id": "yZBpnKpBCw", "paper_title": "Time- and Label-efficient Active Learning by Diversity and Uncertainty of Probabilities", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "AhzqNtmeeP", "reviewer": "Reviewer_Y5kQ", "review_text": "Summary: The paper introduces FALCUN, a method in active learning, which promises efficiencies in labeling and processing. It brings a dynamic acquisition strategy to the table, focusing initially on uncertain instances near decision boundaries and gradually shifting to emphasize batch diversity. While the proposed approach looks new in some aspects, the lack of acknowledgment of previous works that have explored similar territories casts a shadow over its originality.\n\nKey foundational concepts seem to echo pre-existing methodologies in active learning literature, which were not appropriately credited, making FALCUN appear more as an incremental advancement rather than a groundbreaking innovation. Additionally, some experimental results, particularly those involving LeNet and BADGE's performance, seem somewhat inconclusive and could benefit from further rigor and validation to strengthen their credibility.\n\nFurthermore, the paper’s experimental breadth appears limited. A more expansive inclusion of diverse and challenging benchmarks, such as CIFAR-100 and TinyImageNet, would enhance the robustness of FALCUN’s evaluation, providing a more comprehensive understanding of its effectiveness and applicability across various domains. A reconsideration of these aspects could elevate the paper’s contributions and clarity, fostering a more nuanced appreciation of FALCUN's position in the active learning landscape.\n\nStrengths: - The proposed algorithm tries to improve efficiencies in labeling and processing through a dynamic acquisition strategy, emphasizing a transition from focusing on uncertain instances to prioritizing batch diversity, showcasing a potential way in active learning strategies.\n\nWeaknesses: - The paper seems to overlook several crucial references relevant to the discussed topic. Fundamental concepts such as the theoretical importance of the near decision boundary have been comprehensively explored and articulated in previous works, notably in references [1] and [2]. These pivotal papers, along with others, offer profound insights that would augment the paper’s foundational grounding. Moreover, the proposition of a linear-time scalable algorithm, a core element presented in the paper, has previously been introduced and elaborated upon in reference [3]. In addition to that, an influential work cited as [4] such as PowerBALD/or PowerEntropy adopting a sampling similar to Eq (6) but with better originality has also proposed a scaling approach that intriguingly maintains the algorithm’s linear-time complexity.\n\n- The active learning results presented in the paper for LeNet on datasets like EMNIST or RepeatedMNIST seem somewhat unconvincing. Typically, in prevailing literature and experiments, around 300-500 images are required (not > 1000 images) to achieve accuracy comparable to supervised learning methods on these datasets. The reported results in the paper appear to deviate from these established benchmarks. It may be beneficial to revisit and scrutinize the experimental setup, methodology, and the specific implementation of LeNet in the active learning context to ensure that the presented results are robust, reliable, and in alignment with existing standards and expectations in the field. This could enhance the credibility and persuasiveness of the results and the overall contributions of the paper.\n\n- The performance of BADGE as presented in the paper raises some questions. It seems that with appropriate hyperparameter settings, BADGE should be capable of delivering much improved results. This discrepancy suggests that there might be room for optimizing the configuration of BADGE in the experiments, ensuring that it operates under the most suitable conditions for a fair and rigorous comparison. To uphold the integrity and reliability of the comparative analysis, it might be beneficial to revisit and fine-tune the hyperparameters used with BADGE, ensuring that its performance is accurately represented and evaluated against its full potential.\n\n- The experimental section of the paper seems somewhat limited and could be enhanced to bolster the claims made. Incorporating a broader array of realistic scenarios, such as tests involving CIFAR-100, TinyImageNet, or ImageNet, would offer a more comprehensive insight into the method’s applicability and effectiveness. Including such varied and complex datasets in the evaluation would not only strengthen the validity of the results but also improve the generalizability of the conclusions drawn. This expansion in the experimental design would be instrumental in substantiating the method's robustness and adaptability across diverse challenges and use-cases.\n\n[1] Efficient Active Learning with Abstention, NeurIPS 2022 - https://openreview.net/forum?id=4u-oGqB4Lf6\n\n[2] Active Learning with Neural Networks: Insights from Nonparametric Statistics, NeurIPS 2022 - https://openreview.net/forum?id=LRMmgkcoCnW\n\n[3] Active Learning in Bayesian Neural Networks with Balanced Entropy Learning Principle, ICLR 2023 - https://openreview.net/forum?id=ZTMuZ68B1g\n\n[4] Stochastic Batch Acquisition: A Simple Baseline for Deep Active Learning, TMLR 2023 - https://openreview.net/forum?id=vcHwQyNBjW\n\nQuestions: - What criteria determine the **most informative region**? Is a point considered most informative solely based on its proximity to the decision boundary?\n\n- **Could the authors please provide a more explicit explanation regarding the source of diversity in the proposed formula?** The margin considers two class boundaries. It might not be sufficiently generalizable. It would be beneficial to have a more explicit elucidation on how diversity is incorporated and manifests itself within the algorithmic design. Understanding the origins and implementation of diversity within the formula can enhance the comprehension of its functionality and overall impact on the method's performance. An in-depth clarification would contribute to a more robust and insightful evaluation of the proposed approach’s effectiveness and novelty.\n\n- It seems crucial to **re-evaluate the experiment setups in the study to enhance the reliability and comprehensiveness of the findings**. Including a broader selection of benchmarks in the evaluation process would be instrumental in demonstrating the robustness and versatility of the proposed method across varied scenarios. **A more diversified array of benchmarks** will not only contribute to a deeper, better understanding of the method's performance but also bolster the study's overall credibility and impact. Therefore, revisiting and expanding the experiment setups with additional benchmarks is a highly recommended step to enrich the empirical validation of the study.\n\n- Section 4.4 discusses the influence of $\\gamma$ parameter, indicating that **different benchmarks may require distinct $\\gamma$ parameter values for optimal performance**. This aspect raises a practical concern: how can users effectively determine or choose an appropriate $\\gamma$ value a priori for various benchmarks? The ability to discern and select suitable parameters is crucial for the method's practical applicability and usability in real-world scenarios. Clarification or guidance on this matter would significantly enhance the method’s practical value.", "labeling_timestamp": "2026-01-11T17:14:26.121739", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not include the experimental results or the evaluation details needed to assess claims about LeNet or BADGE; while the abstract and introduction state that extensive experiments and comparisons (including BADGE) were performed, the actual experimental sections, numbers, and LeNet-specific results are not present in the text given, so the reviewer's claim cannot be verified from the available content.", "evidence": "1) \"In extensive experiments, we show FALCUNs suitability for diverse use cases, including image and tabular data. Compared to state-of-the-art methods like BADGE, CLUE, and AlfaMix, FALCUN consistently excels in quality and speed: while FALCUN is among the fastest methods, it has the highest average label efficiency.\" (Abstract)\n\n2) \"Figure 1 shows the runtimes and selected samples of different AL methods for a subset of the MNIST dataset with three classes: our approach selects diverse and informative samples while it is among the fastest. We discuss this figure in more detail in Section 3.\" (Introduction)\n\n3) \"We analyze the effect of γ in Section 4.4.\" (Section 2.2)", "section": "Abstract; Introduction; Methodology (Section 2.2)"}
{"claim": "The paper lacks results on real-world X-ray/CT datasets such as DGKr and w7Ci, which are necessary to demonstrate real-world applicability.", "claim_type": "experimental", "paper_id": "fMWrTAe5Iy", "paper_title": "R$^2$-Gaussian: Rectifying Radiative Gaussian Splatting for Tomographic Reconstruction", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Vdom4sRbOm", "reviewer": "Reviewer_w7Ci", "review_text": "Comment: I thank the authors for their thorough response, as well as my fellow reviewers for their insightful comments. I appreciate the author's effort to address my (overall minor) concerns and questions, and I lean towards maintaining my current score (_accept_).\n\nI do hope that, were the paper accepted, the authors would account for the reviewers' remarks, as summarized by the authors in their global response, e.g.:\n- **Including results on real-world data** c.f. _DGKr_ and _w7Ci_ (me). These new experiments gathered by the authors demonstrate the real-world applicability of their work (c.f. Tab. 1 and Fig. 1 of rebuttal PDF).\n- **Better discussing/referencing existing XR-GS works** c.f. _P8A3_ and _w7Ci_ (me). The results and discussion provided in the authors' response would benefit the readers, by better contextualizing their work.\n\nI would also suggest:\n- **Clarifying the isotropic simplification at the core of some claims/contributions**. E.g., the authors' claims that \"[X-Gaussian] _uses alpha-blending, which contradicts the unordered nature of X-ray imaging_\" [response] and that \"_we can individually integrate each 3D Gaussian to rasterize an X-ray projection_\" [L162 + Equation 5] are only correct in the context of the isotropic simplification of X-ray imaging. I.e., if actual physics effects, such as Compton scattering, were to be considered, then the ordering of the Gaussians would matter (see preprint [i] for contributions to XR-GS orthogonal to R$^2$-Gaussian's, as well as [ii, iii] w.r.t. why ordering matters in GS-based scattering simulation). I do agree with the authors that most CT reconstruction models adopt the isotropic simplification; and, therefore, that their rasterization simplification is legitimate. However, readers should be more explicitly made aware of the basis of the authors' claims (isotropic approximation [L100, L140, L162]).\n- **Clarifying the benefits of proposed GS voxelizer** compared to existing solutions, e.g., PyTorch3D PC-to-voxel solution, which is also CUDA-based and differentiable but may require a few tweaks to work on Gaussians.\n- **Including discussed references**, e.g. [c] (TV).\n\n-----\n#### Reference:\n\n[i] Gao, Zhongpai, et al. \"DDGS-CT: Direction-Disentangled Gaussian Splatting for Realistic Volume Rendering.\" arXiv preprint arXiv:2406.02518 (2024).\n\n[ii] Zhou, Yang, Songyin Wu, and Ling-Qi Yan. \"Unified Gaussian Primitives for Scene Representation and Rendering.\" arXiv preprint arXiv:2406.09733 (2024).\n\n[iii] Condor, Jorge, et al. \"Volumetric Primitives for Modeling and Rendering Scattering and Emissive Media.\" arXiv preprint arXiv:2405.15425 (2024).", "labeling_timestamp": "2026-01-11T17:14:22.853498", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper explicitly states it evaluates on \"real-world datasets,\" but the provided content does not list dataset names (such as DGKr or w7Ci). From the given excerpts we cannot confirm whether those specific datasets were included or omitted.", "evidence": "\"Experiments on synthetic and real-world datasets demonstrate that our method outperforms state-of-the-art approaches in accuracy and efficiency.\"", "section": "Abstract"}
{"claim": "The manuscript insufficiently discusses and references existing XR-GS works, reducing contextualization of the proposed approach for readers.", "claim_type": "subjective", "paper_id": "fMWrTAe5Iy", "paper_title": "R$^2$-Gaussian: Rectifying Radiative Gaussian Splatting for Tomographic Reconstruction", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Vdom4sRbOm", "reviewer": "Reviewer_w7Ci", "review_text": "Comment: I thank the authors for their thorough response, as well as my fellow reviewers for their insightful comments. I appreciate the author's effort to address my (overall minor) concerns and questions, and I lean towards maintaining my current score (_accept_).\n\nI do hope that, were the paper accepted, the authors would account for the reviewers' remarks, as summarized by the authors in their global response, e.g.:\n- **Including results on real-world data** c.f. _DGKr_ and _w7Ci_ (me). These new experiments gathered by the authors demonstrate the real-world applicability of their work (c.f. Tab. 1 and Fig. 1 of rebuttal PDF).\n- **Better discussing/referencing existing XR-GS works** c.f. _P8A3_ and _w7Ci_ (me). The results and discussion provided in the authors' response would benefit the readers, by better contextualizing their work.\n\nI would also suggest:\n- **Clarifying the isotropic simplification at the core of some claims/contributions**. E.g., the authors' claims that \"[X-Gaussian] _uses alpha-blending, which contradicts the unordered nature of X-ray imaging_\" [response] and that \"_we can individually integrate each 3D Gaussian to rasterize an X-ray projection_\" [L162 + Equation 5] are only correct in the context of the isotropic simplification of X-ray imaging. I.e., if actual physics effects, such as Compton scattering, were to be considered, then the ordering of the Gaussians would matter (see preprint [i] for contributions to XR-GS orthogonal to R$^2$-Gaussian's, as well as [ii, iii] w.r.t. why ordering matters in GS-based scattering simulation). I do agree with the authors that most CT reconstruction models adopt the isotropic simplification; and, therefore, that their rasterization simplification is legitimate. However, readers should be more explicitly made aware of the basis of the authors' claims (isotropic approximation [L100, L140, L162]).\n- **Clarifying the benefits of proposed GS voxelizer** compared to existing solutions, e.g., PyTorch3D PC-to-voxel solution, which is also CUDA-based and differentiable but may require a few tweaks to work on Gaussians.\n- **Including discussed references**, e.g. [c] (TV).\n\n-----\n#### Reference:\n\n[i] Gao, Zhongpai, et al. \"DDGS-CT: Direction-Disentangled Gaussian Splatting for Realistic Volume Rendering.\" arXiv preprint arXiv:2406.02518 (2024).\n\n[ii] Zhou, Yang, Songyin Wu, and Ling-Qi Yan. \"Unified Gaussian Primitives for Scene Representation and Rendering.\" arXiv preprint arXiv:2406.09733 (2024).\n\n[iii] Condor, Jorge, et al. \"Volumetric Primitives for Modeling and Rendering Scattering and Emissive Media.\" arXiv preprint arXiv:2405.15425 (2024).", "labeling_timestamp": "2026-01-11T17:14:44.856544", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly cites and discusses prior 3D Gaussian → X‑ray works (e.g. X‑Gaussian [7], Gao et al. [14], Li et al. [29]) in Related Work and contrasts their limitations with the proposed method, so the claim that the manuscript insufficiently discusses/references existing XR‑GS works is not supported by the paper.", "evidence": "\"Some concurrent works have extended 3DGS to X-ray imaging. X-Gaussian [7] modify 3DGS to synthesize novel-view X-ray projections. Gao et al. [14] improve X-Gaussian by considering complex noise-inducing physical effects. While they produce plausible 2D X-ray projections, they cannot directly extract 3D density volumes from trained Gaussians. Instead, they first augment projections with 3DGS, and then use traditional algorithms such as FDK for CT reconstruction, which is neither efficient nor effective. Li et al. [29] represent the density field with customized Gaussian kernels, but they replace the efficient rasterization with existing CT simulators. In comparison, our work can both rasterize X-ray projections and voxelize density volumes from Gaussians.\"", "section": "2 Related work"}
{"claim": "The authors do not explicitly state that their alpha-blending and per-Gaussian integration claims rely on the isotropic approximation of X-ray physics.", "claim_type": "methodology", "paper_id": "fMWrTAe5Iy", "paper_title": "R$^2$-Gaussian: Rectifying Radiative Gaussian Splatting for Tomographic Reconstruction", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Vdom4sRbOm", "reviewer": "Reviewer_w7Ci", "review_text": "Comment: I thank the authors for their thorough response, as well as my fellow reviewers for their insightful comments. I appreciate the author's effort to address my (overall minor) concerns and questions, and I lean towards maintaining my current score (_accept_).\n\nI do hope that, were the paper accepted, the authors would account for the reviewers' remarks, as summarized by the authors in their global response, e.g.:\n- **Including results on real-world data** c.f. _DGKr_ and _w7Ci_ (me). These new experiments gathered by the authors demonstrate the real-world applicability of their work (c.f. Tab. 1 and Fig. 1 of rebuttal PDF).\n- **Better discussing/referencing existing XR-GS works** c.f. _P8A3_ and _w7Ci_ (me). The results and discussion provided in the authors' response would benefit the readers, by better contextualizing their work.\n\nI would also suggest:\n- **Clarifying the isotropic simplification at the core of some claims/contributions**. E.g., the authors' claims that \"[X-Gaussian] _uses alpha-blending, which contradicts the unordered nature of X-ray imaging_\" [response] and that \"_we can individually integrate each 3D Gaussian to rasterize an X-ray projection_\" [L162 + Equation 5] are only correct in the context of the isotropic simplification of X-ray imaging. I.e., if actual physics effects, such as Compton scattering, were to be considered, then the ordering of the Gaussians would matter (see preprint [i] for contributions to XR-GS orthogonal to R$^2$-Gaussian's, as well as [ii, iii] w.r.t. why ordering matters in GS-based scattering simulation). I do agree with the authors that most CT reconstruction models adopt the isotropic simplification; and, therefore, that their rasterization simplification is legitimate. However, readers should be more explicitly made aware of the basis of the authors' claims (isotropic approximation [L100, L140, L162]).\n- **Clarifying the benefits of proposed GS voxelizer** compared to existing solutions, e.g., PyTorch3D PC-to-voxel solution, which is also CUDA-based and differentiable but may require a few tweaks to work on Gaussians.\n- **Including discussed references**, e.g. [c] (TV).\n\n-----\n#### Reference:\n\n[i] Gao, Zhongpai, et al. \"DDGS-CT: Direction-Disentangled Gaussian Splatting for Realistic Volume Rendering.\" arXiv preprint arXiv:2406.02518 (2024).\n\n[ii] Zhou, Yang, Songyin Wu, and Ling-Qi Yan. \"Unified Gaussian Primitives for Scene Representation and Rendering.\" arXiv preprint arXiv:2406.09733 (2024).\n\n[iii] Condor, Jorge, et al. \"Volumetric Primitives for Modeling and Rendering Scattering and Emissive Media.\" arXiv preprint arXiv:2405.15425 (2024).", "labeling_timestamp": "2026-01-11T17:14:46.093714", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly assumes an isotropic density model for X-ray imaging (Sec. 3.1) and separately describes using alpha-blending and per-Gaussian projection in the 3DGS rasterizer (Sec. 3.2), but it does not explicitly state that the alpha-blending or per-Gaussian integration claims depend on that isotropic approximation; the reliance is only implicit.", "evidence": "Sec. 3.1: \"σ ( x ) is the isotropic density (or attenuation coefficient in physics) at position x ∈ R 3 . Note that real-world projections contain minor anisotropic physical effects such as Compton scattering. Following previous works [13, 2, 55, 67], we do not explicitly model them but treat them as noise during the reconstruction.\";\nSec. 3.2: \"The projected 2D Gaussian retains the same opacity and color as its 3D counterpart but omits the third row and column of position and covariance. An RGB image is then rendered by compositing these 2D Gaussians using alpha-blending [45]: I rgb = C ( G 2 ) .\"", "section": "Sec. 3.1 (X-ray imaging) and Sec. 3.2 (3D Gaussian splatting)"}
{"claim": "Claims that alpha-blending contradicts the unordered nature of X-ray imaging are only valid under the isotropic simplification and are not generally true.", "claim_type": "methodology", "paper_id": "fMWrTAe5Iy", "paper_title": "R$^2$-Gaussian: Rectifying Radiative Gaussian Splatting for Tomographic Reconstruction", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Vdom4sRbOm", "reviewer": "Reviewer_w7Ci", "review_text": "Comment: I thank the authors for their thorough response, as well as my fellow reviewers for their insightful comments. I appreciate the author's effort to address my (overall minor) concerns and questions, and I lean towards maintaining my current score (_accept_).\n\nI do hope that, were the paper accepted, the authors would account for the reviewers' remarks, as summarized by the authors in their global response, e.g.:\n- **Including results on real-world data** c.f. _DGKr_ and _w7Ci_ (me). These new experiments gathered by the authors demonstrate the real-world applicability of their work (c.f. Tab. 1 and Fig. 1 of rebuttal PDF).\n- **Better discussing/referencing existing XR-GS works** c.f. _P8A3_ and _w7Ci_ (me). The results and discussion provided in the authors' response would benefit the readers, by better contextualizing their work.\n\nI would also suggest:\n- **Clarifying the isotropic simplification at the core of some claims/contributions**. E.g., the authors' claims that \"[X-Gaussian] _uses alpha-blending, which contradicts the unordered nature of X-ray imaging_\" [response] and that \"_we can individually integrate each 3D Gaussian to rasterize an X-ray projection_\" [L162 + Equation 5] are only correct in the context of the isotropic simplification of X-ray imaging. I.e., if actual physics effects, such as Compton scattering, were to be considered, then the ordering of the Gaussians would matter (see preprint [i] for contributions to XR-GS orthogonal to R$^2$-Gaussian's, as well as [ii, iii] w.r.t. why ordering matters in GS-based scattering simulation). I do agree with the authors that most CT reconstruction models adopt the isotropic simplification; and, therefore, that their rasterization simplification is legitimate. However, readers should be more explicitly made aware of the basis of the authors' claims (isotropic approximation [L100, L140, L162]).\n- **Clarifying the benefits of proposed GS voxelizer** compared to existing solutions, e.g., PyTorch3D PC-to-voxel solution, which is also CUDA-based and differentiable but may require a few tweaks to work on Gaussians.\n- **Including discussed references**, e.g. [c] (TV).\n\n-----\n#### Reference:\n\n[i] Gao, Zhongpai, et al. \"DDGS-CT: Direction-Disentangled Gaussian Splatting for Realistic Volume Rendering.\" arXiv preprint arXiv:2406.02518 (2024).\n\n[ii] Zhou, Yang, Songyin Wu, and Ling-Qi Yan. \"Unified Gaussian Primitives for Scene Representation and Rendering.\" arXiv preprint arXiv:2406.09733 (2024).\n\n[iii] Condor, Jorge, et al. \"Volumetric Primitives for Modeling and Rendering Scattering and Emissive Media.\" arXiv preprint arXiv:2405.15425 (2024).", "labeling_timestamp": "2026-01-11T17:14:50.690904", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper describes that 3D Gaussian splatting uses alpha-blending for RGB rendering (Sec. 3.2) and treats X-ray density as an isotropic σ(x) while not explicitly modeling anisotropic effects (Sec. 3.1). However, the paper does not state or analyze the specific reviewer assertion that \"alpha-blending contradicts the unordered nature of X-ray imaging\" is only valid under an isotropic simplification. The provided text discusses an integration bias and a rectified rasterizer for X-ray imaging but does not address whether the alpha-blending contradiction claim depends on isotropy versus anisotropy. Therefore there is insufficient information in the paper to confirm or refute the reviewer’s nuanced claim.", "evidence": ["An RGB image is then rendered by compositing these 2D Gaussians using alpha-blending [45]: I rgb = C ( G 2 ) .", "Here, σ ( x ) is the isotropic density (or attenuation coefficient in physics) at position x ∈ R 3 .", "Note that real-world projections contain minor anisotropic physical effects such as Compton scattering. Following previous works [13, 2, 55, 67], we do not explicitly model them but treat them as noise during the reconstruction.", "we will show in Sec. 4.2.1 that the standard 3DGS overlooks a covariance-related scaling factor when splatting a 3D Gaussian kernel onto the 2D image plane. This formulation leads to inconsistent volumetric properties queried from different views."], "section": "Sec. 3.2 and Sec. 3.1 (and mention of Sec. 4.2.1)"}
{"claim": "When physics effects like Compton scattering are considered, the ordering of Gaussians becomes important, which undermines the paper's unordered rasterization assumptions.", "claim_type": "methodology", "paper_id": "fMWrTAe5Iy", "paper_title": "R$^2$-Gaussian: Rectifying Radiative Gaussian Splatting for Tomographic Reconstruction", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Vdom4sRbOm", "reviewer": "Reviewer_w7Ci", "review_text": "Comment: I thank the authors for their thorough response, as well as my fellow reviewers for their insightful comments. I appreciate the author's effort to address my (overall minor) concerns and questions, and I lean towards maintaining my current score (_accept_).\n\nI do hope that, were the paper accepted, the authors would account for the reviewers' remarks, as summarized by the authors in their global response, e.g.:\n- **Including results on real-world data** c.f. _DGKr_ and _w7Ci_ (me). These new experiments gathered by the authors demonstrate the real-world applicability of their work (c.f. Tab. 1 and Fig. 1 of rebuttal PDF).\n- **Better discussing/referencing existing XR-GS works** c.f. _P8A3_ and _w7Ci_ (me). The results and discussion provided in the authors' response would benefit the readers, by better contextualizing their work.\n\nI would also suggest:\n- **Clarifying the isotropic simplification at the core of some claims/contributions**. E.g., the authors' claims that \"[X-Gaussian] _uses alpha-blending, which contradicts the unordered nature of X-ray imaging_\" [response] and that \"_we can individually integrate each 3D Gaussian to rasterize an X-ray projection_\" [L162 + Equation 5] are only correct in the context of the isotropic simplification of X-ray imaging. I.e., if actual physics effects, such as Compton scattering, were to be considered, then the ordering of the Gaussians would matter (see preprint [i] for contributions to XR-GS orthogonal to R$^2$-Gaussian's, as well as [ii, iii] w.r.t. why ordering matters in GS-based scattering simulation). I do agree with the authors that most CT reconstruction models adopt the isotropic simplification; and, therefore, that their rasterization simplification is legitimate. However, readers should be more explicitly made aware of the basis of the authors' claims (isotropic approximation [L100, L140, L162]).\n- **Clarifying the benefits of proposed GS voxelizer** compared to existing solutions, e.g., PyTorch3D PC-to-voxel solution, which is also CUDA-based and differentiable but may require a few tweaks to work on Gaussians.\n- **Including discussed references**, e.g. [c] (TV).\n\n-----\n#### Reference:\n\n[i] Gao, Zhongpai, et al. \"DDGS-CT: Direction-Disentangled Gaussian Splatting for Realistic Volume Rendering.\" arXiv preprint arXiv:2406.02518 (2024).\n\n[ii] Zhou, Yang, Songyin Wu, and Ling-Qi Yan. \"Unified Gaussian Primitives for Scene Representation and Rendering.\" arXiv preprint arXiv:2406.09733 (2024).\n\n[iii] Condor, Jorge, et al. \"Volumetric Primitives for Modeling and Rendering Scattering and Emissive Media.\" arXiv preprint arXiv:2405.15425 (2024).", "labeling_timestamp": "2026-01-11T17:14:54.019495", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states that it does not model physical effects like Compton scattering and instead treats them as noise (so such effects are not considered in their rasterization analysis), therefore the claim that these effects make ordering important and undermine the paper's unordered rasterization assumptions is not supported by the paper.", "evidence": "Note that real-world projections contain minor anisotropic physical effects such as Compton scattering. Following previous works [13, 2, 55, 67], we do not explicitly model them but treat them as noise during the reconstruction.", "section": "3.1 X-ray imaging"}
{"claim": "The paper does not clearly compare the proposed GS voxelizer to existing CUDA-based differentiable solutions such as PyTorch3D's point-cloud-to-voxel implementation.", "claim_type": "baseline", "paper_id": "fMWrTAe5Iy", "paper_title": "R$^2$-Gaussian: Rectifying Radiative Gaussian Splatting for Tomographic Reconstruction", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Vdom4sRbOm", "reviewer": "Reviewer_w7Ci", "review_text": "Comment: I thank the authors for their thorough response, as well as my fellow reviewers for their insightful comments. I appreciate the author's effort to address my (overall minor) concerns and questions, and I lean towards maintaining my current score (_accept_).\n\nI do hope that, were the paper accepted, the authors would account for the reviewers' remarks, as summarized by the authors in their global response, e.g.:\n- **Including results on real-world data** c.f. _DGKr_ and _w7Ci_ (me). These new experiments gathered by the authors demonstrate the real-world applicability of their work (c.f. Tab. 1 and Fig. 1 of rebuttal PDF).\n- **Better discussing/referencing existing XR-GS works** c.f. _P8A3_ and _w7Ci_ (me). The results and discussion provided in the authors' response would benefit the readers, by better contextualizing their work.\n\nI would also suggest:\n- **Clarifying the isotropic simplification at the core of some claims/contributions**. E.g., the authors' claims that \"[X-Gaussian] _uses alpha-blending, which contradicts the unordered nature of X-ray imaging_\" [response] and that \"_we can individually integrate each 3D Gaussian to rasterize an X-ray projection_\" [L162 + Equation 5] are only correct in the context of the isotropic simplification of X-ray imaging. I.e., if actual physics effects, such as Compton scattering, were to be considered, then the ordering of the Gaussians would matter (see preprint [i] for contributions to XR-GS orthogonal to R$^2$-Gaussian's, as well as [ii, iii] w.r.t. why ordering matters in GS-based scattering simulation). I do agree with the authors that most CT reconstruction models adopt the isotropic simplification; and, therefore, that their rasterization simplification is legitimate. However, readers should be more explicitly made aware of the basis of the authors' claims (isotropic approximation [L100, L140, L162]).\n- **Clarifying the benefits of proposed GS voxelizer** compared to existing solutions, e.g., PyTorch3D PC-to-voxel solution, which is also CUDA-based and differentiable but may require a few tweaks to work on Gaussians.\n- **Including discussed references**, e.g. [c] (TV).\n\n-----\n#### Reference:\n\n[i] Gao, Zhongpai, et al. \"DDGS-CT: Direction-Disentangled Gaussian Splatting for Realistic Volume Rendering.\" arXiv preprint arXiv:2406.02518 (2024).\n\n[ii] Zhou, Yang, Songyin Wu, and Ling-Qi Yan. \"Unified Gaussian Primitives for Scene Representation and Rendering.\" arXiv preprint arXiv:2406.09733 (2024).\n\n[iii] Condor, Jorge, et al. \"Volumetric Primitives for Modeling and Rendering Scattering and Emissive Media.\" arXiv preprint arXiv:2405.15425 (2024).", "labeling_timestamp": "2026-01-11T17:14:56.810855", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly states it develops a CUDA-based differentiable voxelizer (Abstract, Introduction, Related Work) but contains no mention or comparison to existing CUDA-based differentiable voxelization implementations (e.g., PyTorch3D point-cloud-to-voxel). Thus the reviewer's claim that the paper does not clearly compare its GS voxelizer to such existing solutions is accurate.", "evidence": "Abstract: \"...and developing a CUDA-based differentiable voxelizer.\"; Introduction / Contributions: \"Thirdly, we develop a CUDA-based differentiable voxelizer, which not only extracts 3D volumes from Gaussians but also enables voxel-based regularization during training.\"; Related Work (3DGS paragraph): \"In comparison, our work can both rasterize X-ray projections and voxelize density volumes from Gaussians.\" ", "section": "Abstract; Introduction (Contributions); Related work (3DGS paragraph)"}
{"claim": "It is unclear whether PyTorch3D's PC-to-voxel requires only minor tweaks to support Gaussians, so the claimed benefits of the proposed GS voxelizer are not justified.", "claim_type": "methodology", "paper_id": "fMWrTAe5Iy", "paper_title": "R$^2$-Gaussian: Rectifying Radiative Gaussian Splatting for Tomographic Reconstruction", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Vdom4sRbOm", "reviewer": "Reviewer_w7Ci", "review_text": "Comment: I thank the authors for their thorough response, as well as my fellow reviewers for their insightful comments. I appreciate the author's effort to address my (overall minor) concerns and questions, and I lean towards maintaining my current score (_accept_).\n\nI do hope that, were the paper accepted, the authors would account for the reviewers' remarks, as summarized by the authors in their global response, e.g.:\n- **Including results on real-world data** c.f. _DGKr_ and _w7Ci_ (me). These new experiments gathered by the authors demonstrate the real-world applicability of their work (c.f. Tab. 1 and Fig. 1 of rebuttal PDF).\n- **Better discussing/referencing existing XR-GS works** c.f. _P8A3_ and _w7Ci_ (me). The results and discussion provided in the authors' response would benefit the readers, by better contextualizing their work.\n\nI would also suggest:\n- **Clarifying the isotropic simplification at the core of some claims/contributions**. E.g., the authors' claims that \"[X-Gaussian] _uses alpha-blending, which contradicts the unordered nature of X-ray imaging_\" [response] and that \"_we can individually integrate each 3D Gaussian to rasterize an X-ray projection_\" [L162 + Equation 5] are only correct in the context of the isotropic simplification of X-ray imaging. I.e., if actual physics effects, such as Compton scattering, were to be considered, then the ordering of the Gaussians would matter (see preprint [i] for contributions to XR-GS orthogonal to R$^2$-Gaussian's, as well as [ii, iii] w.r.t. why ordering matters in GS-based scattering simulation). I do agree with the authors that most CT reconstruction models adopt the isotropic simplification; and, therefore, that their rasterization simplification is legitimate. However, readers should be more explicitly made aware of the basis of the authors' claims (isotropic approximation [L100, L140, L162]).\n- **Clarifying the benefits of proposed GS voxelizer** compared to existing solutions, e.g., PyTorch3D PC-to-voxel solution, which is also CUDA-based and differentiable but may require a few tweaks to work on Gaussians.\n- **Including discussed references**, e.g. [c] (TV).\n\n-----\n#### Reference:\n\n[i] Gao, Zhongpai, et al. \"DDGS-CT: Direction-Disentangled Gaussian Splatting for Realistic Volume Rendering.\" arXiv preprint arXiv:2406.02518 (2024).\n\n[ii] Zhou, Yang, Songyin Wu, and Ling-Qi Yan. \"Unified Gaussian Primitives for Scene Representation and Rendering.\" arXiv preprint arXiv:2406.09733 (2024).\n\n[iii] Condor, Jorge, et al. \"Volumetric Primitives for Modeling and Rendering Scattering and Emissive Media.\" arXiv preprint arXiv:2405.15425 (2024).", "labeling_timestamp": "2026-01-11T17:15:12.366077", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper clearly proposes a CUDA-based differentiable voxelizer and emphasizes that prior 3DGS works did not extract 3D density volumes directly (they used FDK or other simulators). However, the paper does not mention PyTorch3D or any existing PC-to-voxel implementation or compare against adapting such tools. Therefore it does not provide evidence to confirm or refute whether PyTorch3D's PC-to-voxel would require only minor tweaks to support Gaussians, so the reviewer's claim cannot be determined from the paper.", "evidence": "\"Thirdly, we develop a CUDA-based differentiable voxelizer, which not only extracts 3D volumes from Gaussians but also enables voxel-based regularization during training.\"", "section": "Abstract / Introduction"}
{"claim": "The authors should include the additional real-world experiments reported in their rebuttal (rebuttal Tab.1 and Fig.1) within the main manuscript.", "claim_type": "presentation", "paper_id": "fMWrTAe5Iy", "paper_title": "R$^2$-Gaussian: Rectifying Radiative Gaussian Splatting for Tomographic Reconstruction", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Vdom4sRbOm", "reviewer": "Reviewer_w7Ci", "review_text": "Comment: I thank the authors for their thorough response, as well as my fellow reviewers for their insightful comments. I appreciate the author's effort to address my (overall minor) concerns and questions, and I lean towards maintaining my current score (_accept_).\n\nI do hope that, were the paper accepted, the authors would account for the reviewers' remarks, as summarized by the authors in their global response, e.g.:\n- **Including results on real-world data** c.f. _DGKr_ and _w7Ci_ (me). These new experiments gathered by the authors demonstrate the real-world applicability of their work (c.f. Tab. 1 and Fig. 1 of rebuttal PDF).\n- **Better discussing/referencing existing XR-GS works** c.f. _P8A3_ and _w7Ci_ (me). The results and discussion provided in the authors' response would benefit the readers, by better contextualizing their work.\n\nI would also suggest:\n- **Clarifying the isotropic simplification at the core of some claims/contributions**. E.g., the authors' claims that \"[X-Gaussian] _uses alpha-blending, which contradicts the unordered nature of X-ray imaging_\" [response] and that \"_we can individually integrate each 3D Gaussian to rasterize an X-ray projection_\" [L162 + Equation 5] are only correct in the context of the isotropic simplification of X-ray imaging. I.e., if actual physics effects, such as Compton scattering, were to be considered, then the ordering of the Gaussians would matter (see preprint [i] for contributions to XR-GS orthogonal to R$^2$-Gaussian's, as well as [ii, iii] w.r.t. why ordering matters in GS-based scattering simulation). I do agree with the authors that most CT reconstruction models adopt the isotropic simplification; and, therefore, that their rasterization simplification is legitimate. However, readers should be more explicitly made aware of the basis of the authors' claims (isotropic approximation [L100, L140, L162]).\n- **Clarifying the benefits of proposed GS voxelizer** compared to existing solutions, e.g., PyTorch3D PC-to-voxel solution, which is also CUDA-based and differentiable but may require a few tweaks to work on Gaussians.\n- **Including discussed references**, e.g. [c] (TV).\n\n-----\n#### Reference:\n\n[i] Gao, Zhongpai, et al. \"DDGS-CT: Direction-Disentangled Gaussian Splatting for Realistic Volume Rendering.\" arXiv preprint arXiv:2406.02518 (2024).\n\n[ii] Zhou, Yang, Songyin Wu, and Ling-Qi Yan. \"Unified Gaussian Primitives for Scene Representation and Rendering.\" arXiv preprint arXiv:2406.09733 (2024).\n\n[iii] Condor, Jorge, et al. \"Volumetric Primitives for Modeling and Rendering Scattering and Emissive Media.\" arXiv preprint arXiv:2405.15425 (2024).", "labeling_timestamp": "2026-01-11T17:15:10.385170", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper reports experiments on synthetic and real-world datasets (Abstract, Introduction) but does not mention any additional real-world experiments that were presented only in a rebuttal, nor does it state that such rebuttal results should be moved into the main manuscript. Therefore there is insufficient information in the paper to confirm the reviewer's recommendation.", "evidence": "\"Experiments on synthetic and real-world datasets demonstrate that our method outperforms state-of-the-art approaches in accuracy and efficiency.\"", "section": "Abstract"}
{"claim": "The manuscript omits discussed references such as the cited TV reference [c], which should be included for completeness and proper attribution.", "claim_type": "subjective", "paper_id": "fMWrTAe5Iy", "paper_title": "R$^2$-Gaussian: Rectifying Radiative Gaussian Splatting for Tomographic Reconstruction", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Vdom4sRbOm", "reviewer": "Reviewer_w7Ci", "review_text": "Comment: I thank the authors for their thorough response, as well as my fellow reviewers for their insightful comments. I appreciate the author's effort to address my (overall minor) concerns and questions, and I lean towards maintaining my current score (_accept_).\n\nI do hope that, were the paper accepted, the authors would account for the reviewers' remarks, as summarized by the authors in their global response, e.g.:\n- **Including results on real-world data** c.f. _DGKr_ and _w7Ci_ (me). These new experiments gathered by the authors demonstrate the real-world applicability of their work (c.f. Tab. 1 and Fig. 1 of rebuttal PDF).\n- **Better discussing/referencing existing XR-GS works** c.f. _P8A3_ and _w7Ci_ (me). The results and discussion provided in the authors' response would benefit the readers, by better contextualizing their work.\n\nI would also suggest:\n- **Clarifying the isotropic simplification at the core of some claims/contributions**. E.g., the authors' claims that \"[X-Gaussian] _uses alpha-blending, which contradicts the unordered nature of X-ray imaging_\" [response] and that \"_we can individually integrate each 3D Gaussian to rasterize an X-ray projection_\" [L162 + Equation 5] are only correct in the context of the isotropic simplification of X-ray imaging. I.e., if actual physics effects, such as Compton scattering, were to be considered, then the ordering of the Gaussians would matter (see preprint [i] for contributions to XR-GS orthogonal to R$^2$-Gaussian's, as well as [ii, iii] w.r.t. why ordering matters in GS-based scattering simulation). I do agree with the authors that most CT reconstruction models adopt the isotropic simplification; and, therefore, that their rasterization simplification is legitimate. However, readers should be more explicitly made aware of the basis of the authors' claims (isotropic approximation [L100, L140, L162]).\n- **Clarifying the benefits of proposed GS voxelizer** compared to existing solutions, e.g., PyTorch3D PC-to-voxel solution, which is also CUDA-based and differentiable but may require a few tweaks to work on Gaussians.\n- **Including discussed references**, e.g. [c] (TV).\n\n-----\n#### Reference:\n\n[i] Gao, Zhongpai, et al. \"DDGS-CT: Direction-Disentangled Gaussian Splatting for Realistic Volume Rendering.\" arXiv preprint arXiv:2406.02518 (2024).\n\n[ii] Zhou, Yang, Songyin Wu, and Ling-Qi Yan. \"Unified Gaussian Primitives for Scene Representation and Rendering.\" arXiv preprint arXiv:2406.09733 (2024).\n\n[iii] Condor, Jorge, et al. \"Volumetric Primitives for Modeling and Rendering Scattering and Emissive Media.\" arXiv preprint arXiv:2405.15425 (2024).", "labeling_timestamp": "2026-01-11T17:15:12.573316", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper text does not mention a missing 'TV reference [c]' or discuss omitting that specific citation. The manuscript includes many citations in the Introduction and Related Work (e.g., [7,14], [13,2,55], [23]), but there is no evidence in the excerpt that a referenced item labeled '[c]' was discussed and omitted. Therefore there is insufficient information to confirm the reviewer's claim.", "evidence": "Some concurrent works have extended 3DGS to X-ray imaging. X-Gaussian [7] modify 3DGS to synthesize novel-view X-ray projections. Gao et al. [14] improve X-Gaussian by considering complex noise-inducing physical effects.", "section": "2 Related work"}
{"claim": "The paper fails to cite or discuss recent preprints (e.g., DDGS-CT and Unified Gaussian Primitives) that address ordering and scattering simulation relevant to XR-GS.", "claim_type": "other", "paper_id": "fMWrTAe5Iy", "paper_title": "R$^2$-Gaussian: Rectifying Radiative Gaussian Splatting for Tomographic Reconstruction", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Vdom4sRbOm", "reviewer": "Reviewer_w7Ci", "review_text": "Comment: I thank the authors for their thorough response, as well as my fellow reviewers for their insightful comments. I appreciate the author's effort to address my (overall minor) concerns and questions, and I lean towards maintaining my current score (_accept_).\n\nI do hope that, were the paper accepted, the authors would account for the reviewers' remarks, as summarized by the authors in their global response, e.g.:\n- **Including results on real-world data** c.f. _DGKr_ and _w7Ci_ (me). These new experiments gathered by the authors demonstrate the real-world applicability of their work (c.f. Tab. 1 and Fig. 1 of rebuttal PDF).\n- **Better discussing/referencing existing XR-GS works** c.f. _P8A3_ and _w7Ci_ (me). The results and discussion provided in the authors' response would benefit the readers, by better contextualizing their work.\n\nI would also suggest:\n- **Clarifying the isotropic simplification at the core of some claims/contributions**. E.g., the authors' claims that \"[X-Gaussian] _uses alpha-blending, which contradicts the unordered nature of X-ray imaging_\" [response] and that \"_we can individually integrate each 3D Gaussian to rasterize an X-ray projection_\" [L162 + Equation 5] are only correct in the context of the isotropic simplification of X-ray imaging. I.e., if actual physics effects, such as Compton scattering, were to be considered, then the ordering of the Gaussians would matter (see preprint [i] for contributions to XR-GS orthogonal to R$^2$-Gaussian's, as well as [ii, iii] w.r.t. why ordering matters in GS-based scattering simulation). I do agree with the authors that most CT reconstruction models adopt the isotropic simplification; and, therefore, that their rasterization simplification is legitimate. However, readers should be more explicitly made aware of the basis of the authors' claims (isotropic approximation [L100, L140, L162]).\n- **Clarifying the benefits of proposed GS voxelizer** compared to existing solutions, e.g., PyTorch3D PC-to-voxel solution, which is also CUDA-based and differentiable but may require a few tweaks to work on Gaussians.\n- **Including discussed references**, e.g. [c] (TV).\n\n-----\n#### Reference:\n\n[i] Gao, Zhongpai, et al. \"DDGS-CT: Direction-Disentangled Gaussian Splatting for Realistic Volume Rendering.\" arXiv preprint arXiv:2406.02518 (2024).\n\n[ii] Zhou, Yang, Songyin Wu, and Ling-Qi Yan. \"Unified Gaussian Primitives for Scene Representation and Rendering.\" arXiv preprint arXiv:2406.09733 (2024).\n\n[iii] Condor, Jorge, et al. \"Volumetric Primitives for Modeling and Rendering Scattering and Emissive Media.\" arXiv preprint arXiv:2405.15425 (2024).", "labeling_timestamp": "2026-01-11T17:15:35.989899", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper's Related Work and method sections list concurrent 3DGS-to-X-ray works ([7,14], [29]) but do not mention or cite the named preprints (DDGS-CT, Unified Gaussian Primitives). Moreover, the paper explicitly states it does not model scattering effects (‘treat them as noise’), indicating it does not engage with works that address ordering/scattering simulation.", "evidence": "1) \"Some concurrent works [7, 14] empirically modify 3DGS for X-ray view synthesis. Gao et al. [14] improve X-Gaussian by considering complex noise-inducing physical effects. While they produce plausible 2D X-ray projections, they cannot directly extract 3D density volumes from trained Gaussians.\" (Related work)\n\n2) \"Note that real-world projections contain minor anisotropic physical effects such as Compton scattering. Following previous works [13, 2, 55, 67], we do not explicitly model them but treat them as noise during the reconstruction.\" (Sec. 3.1 X-ray imaging)", "section": "Related work (Sec. 2) and X-ray imaging (Sec. 3.1)"}
{"claim": "The proposed model is not sufficiently novel because combining local and global perspectives in graph models is already a common idea.", "claim_type": "novelty", "paper_id": "auguNUCto5", "paper_title": "Boosting Temporal Graph Learning From Global and Local Perspectives", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "PXhDGnz45s", "reviewer": "Reviewer_L1Kz", "review_text": "Summary: The paper discusses the limitations of existing methods in temporal graph learning, which either focus on global or local perspectives but not both. To overcome this, the Global and Local Embedding Network (GLEN) is proposed. GLEN dynamically generates node embeddings by considering both global and local information. These embeddings are then fused using a cross-perspective module to capture high-order semantic relations. GLEN has been evaluated on multiple datasets and outperforms baselines in tasks like link prediction and dynamic node classification.\n\nStrengths: S1. Temporal graph is an important problem to address in practical world, yet majority of the research deals with static graphs only.\n\nS2. The analysis of existing RNN-based work and random walk/message passing models provides useful insights.\n\nS3. The writing/organization of the paper is generally clear, although some parts need more clarification. (See W2)\n\nWeaknesses: W1. The proposed model, while technically valid and sound, is not sufficiently novel or exciting. Combining local and global perspectives are common ideas in graphs. Even on temporal graph, point process based modeling aims to capture the graph-wide evolution pattern from  a global perspective, such as (Lu et al., 2019) and the below paper [a]. A detailed discussion on temporal point processes for temporal graph is warranted, potentially with additional experimental comparison.\n\n[a] Trend: Temporal event and node dynamics for graph representation learning. WWW 2022.\n\nW2. Certain parts in the motivation of the paper are not clearly explained. For example, the following sentences:\n\"Pairwise interactions observed in different graphs or even the same temporal graph typically have different temporal properties.\"\n\"Since the endogenous and exogenous factors driving the generative process ...\" \nI'm not exactly sure how they directly connect to or motivate the proposed method.\n\nW3. In Table 2, Random tends to perform the best compared to historical/inductive strategies. It is surprising and more discussion is needed. (Also, I'm not confident of the results in Table 2, as it has some discrepancy with the results in Table 3 -- e.g. for UCI, the results in Table 2 and Table 3 are different.\n\nQuestions: Please see Weaknesses.", "labeling_timestamp": "2026-01-11T17:15:28.013335", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states that prior temporal graph methods focus on either a global or a local perspective and claims novelty in jointly modeling and fusing both perspectives; thus the reviewer's claim that combining local and global perspectives is already common contradicts the paper's stated contribution.", "evidence": "1) \"These methods produce node embeddings from a global or local perspective and ignore the complementarity between them, thus facing limitations... To address the challenges, we propose the Global and Local Embedding Network (GLEN) for effective and efficient temporal graph representation learning. Specifically, GLEN dynamically generates embeddings for graph nodes by considering both global and local perspectives.\" 2) \"- New Finding. We innovatively present that modeling from both global and local perspectives is indispensable for temporal graph representation learning. To the best of our knowledge, we are the first in the subfield of temporal graph learning to propose a method that simultaneously models the graph structure from an entire global perspective and a local subgraph perspective, and fuses all node embeddings across views.\"", "section": "Abstract and Introduction (Contributions)"}
{"claim": "The paper lacks a detailed discussion of temporal point process approaches and their relevance to the proposed temporal graph method.", "claim_type": "subjective", "paper_id": "auguNUCto5", "paper_title": "Boosting Temporal Graph Learning From Global and Local Perspectives", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "PXhDGnz45s", "reviewer": "Reviewer_L1Kz", "review_text": "Summary: The paper discusses the limitations of existing methods in temporal graph learning, which either focus on global or local perspectives but not both. To overcome this, the Global and Local Embedding Network (GLEN) is proposed. GLEN dynamically generates node embeddings by considering both global and local information. These embeddings are then fused using a cross-perspective module to capture high-order semantic relations. GLEN has been evaluated on multiple datasets and outperforms baselines in tasks like link prediction and dynamic node classification.\n\nStrengths: S1. Temporal graph is an important problem to address in practical world, yet majority of the research deals with static graphs only.\n\nS2. The analysis of existing RNN-based work and random walk/message passing models provides useful insights.\n\nS3. The writing/organization of the paper is generally clear, although some parts need more clarification. (See W2)\n\nWeaknesses: W1. The proposed model, while technically valid and sound, is not sufficiently novel or exciting. Combining local and global perspectives are common ideas in graphs. Even on temporal graph, point process based modeling aims to capture the graph-wide evolution pattern from  a global perspective, such as (Lu et al., 2019) and the below paper [a]. A detailed discussion on temporal point processes for temporal graph is warranted, potentially with additional experimental comparison.\n\n[a] Trend: Temporal event and node dynamics for graph representation learning. WWW 2022.\n\nW2. Certain parts in the motivation of the paper are not clearly explained. For example, the following sentences:\n\"Pairwise interactions observed in different graphs or even the same temporal graph typically have different temporal properties.\"\n\"Since the endogenous and exogenous factors driving the generative process ...\" \nI'm not exactly sure how they directly connect to or motivate the proposed method.\n\nW3. In Table 2, Random tends to perform the best compared to historical/inductive strategies. It is surprising and more discussion is needed. (Also, I'm not confident of the results in Table 2, as it has some discrepancy with the results in Table 3 -- e.g. for UCI, the results in Table 2 and Table 3 are different.\n\nQuestions: Please see Weaknesses.", "labeling_timestamp": "2026-01-11T17:15:40.453527", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper's related work and preliminaries describe event streams, GNN-RNN, MP-TGN and WA-TGN approaches and time-encoding/memory mechanisms but contain no discussion of temporal point process (TPP) methods or their relevance to GLEN. Thus the claim that the paper lacks a detailed discussion of TPP approaches is supported.", "evidence": "1) \"Temporal graph methods. GNN-RNN-based temporal graph methods such as EvolveGCN (Pareja et al., 2020), CTGCN (Liu et al., 2020), and GCRN (Seo et al., 2018) ... Message-passing temporal graph networks (MP-TGNs) such as JODIE (Kumar et al., 2019), TGAT (Xu et al., 2020), TGN (Rossi et al., 2020), APAN (Wang et al., 2021b), and TPGNN (Wang et al., 2022) ... Walk-aggregating temporal graph networks (WA-TGNs) such as evolve2vec (Bastas et al., 2019a), STWalk (Pandhre et al., 2018), and EVONRL (Heidari & Papagelis, 2020)...\" 2) \"Interaction events occur temporally between nodes, which is represented as an event stream E = { e_uv(t) } ordered by time.\"", "section": "Related Works; Preliminaries"}
{"claim": "The paper does not provide experimental comparisons with temporal point process-based temporal graph models such as Lu et al. (2019) and WWW 2022.", "claim_type": "baseline", "paper_id": "auguNUCto5", "paper_title": "Boosting Temporal Graph Learning From Global and Local Perspectives", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "PXhDGnz45s", "reviewer": "Reviewer_L1Kz", "review_text": "Summary: The paper discusses the limitations of existing methods in temporal graph learning, which either focus on global or local perspectives but not both. To overcome this, the Global and Local Embedding Network (GLEN) is proposed. GLEN dynamically generates node embeddings by considering both global and local information. These embeddings are then fused using a cross-perspective module to capture high-order semantic relations. GLEN has been evaluated on multiple datasets and outperforms baselines in tasks like link prediction and dynamic node classification.\n\nStrengths: S1. Temporal graph is an important problem to address in practical world, yet majority of the research deals with static graphs only.\n\nS2. The analysis of existing RNN-based work and random walk/message passing models provides useful insights.\n\nS3. The writing/organization of the paper is generally clear, although some parts need more clarification. (See W2)\n\nWeaknesses: W1. The proposed model, while technically valid and sound, is not sufficiently novel or exciting. Combining local and global perspectives are common ideas in graphs. Even on temporal graph, point process based modeling aims to capture the graph-wide evolution pattern from  a global perspective, such as (Lu et al., 2019) and the below paper [a]. A detailed discussion on temporal point processes for temporal graph is warranted, potentially with additional experimental comparison.\n\n[a] Trend: Temporal event and node dynamics for graph representation learning. WWW 2022.\n\nW2. Certain parts in the motivation of the paper are not clearly explained. For example, the following sentences:\n\"Pairwise interactions observed in different graphs or even the same temporal graph typically have different temporal properties.\"\n\"Since the endogenous and exogenous factors driving the generative process ...\" \nI'm not exactly sure how they directly connect to or motivate the proposed method.\n\nW3. In Table 2, Random tends to perform the best compared to historical/inductive strategies. It is surprising and more discussion is needed. (Also, I'm not confident of the results in Table 2, as it has some discrepancy with the results in Table 3 -- e.g. for UCI, the results in Table 2 and Table 3 are different.\n\nQuestions: Please see Weaknesses.", "labeling_timestamp": "2026-01-11T17:15:55.460370", "model": "gpt-5-mini", "label": "Supported", "justification": "The provided paper content lists evaluated tasks and broadly claims outperforming baselines (Abstract) and surveys many prior methods (Related Works), but contains no experimental section or baseline list showing comparisons with temporal point process–based temporal graph models (e.g., Lu et al., 2019 or the WWW 2022 work). Lu et al. (2019) is only cited in the Introduction, not used as an experimental baseline.", "evidence": "Abstract: \"We evaluate GLEN on multiple real-world datasets and apply several negative sampling strategies. Sufficient experimental results demonstrate that GLEN outperforms other baselines in both link prediction and dynamic node classification tasks.\"; Related Works: \"Message-passing temporal graph networks (MP-TGNs) such as JODIE (Kumar et al., 2019), TGAT (Xu et al., 2020), TGN (Rossi et al., 2020), APAN (Wang et al., 2021b), and TPGNN (Wang et al., 2022) aggregate local information through the message passing machnism.\"; Introduction: \"... could result in less accurate inferences (Lu et al., 2019).\"", "section": "ABSTRACT; RELATED WORKS; INTRODUCTION"}
{"claim": "The motivation sentence about pairwise interactions having different temporal properties is not clearly explained or connected to the proposed method.", "claim_type": "presentation", "paper_id": "auguNUCto5", "paper_title": "Boosting Temporal Graph Learning From Global and Local Perspectives", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "PXhDGnz45s", "reviewer": "Reviewer_L1Kz", "review_text": "Summary: The paper discusses the limitations of existing methods in temporal graph learning, which either focus on global or local perspectives but not both. To overcome this, the Global and Local Embedding Network (GLEN) is proposed. GLEN dynamically generates node embeddings by considering both global and local information. These embeddings are then fused using a cross-perspective module to capture high-order semantic relations. GLEN has been evaluated on multiple datasets and outperforms baselines in tasks like link prediction and dynamic node classification.\n\nStrengths: S1. Temporal graph is an important problem to address in practical world, yet majority of the research deals with static graphs only.\n\nS2. The analysis of existing RNN-based work and random walk/message passing models provides useful insights.\n\nS3. The writing/organization of the paper is generally clear, although some parts need more clarification. (See W2)\n\nWeaknesses: W1. The proposed model, while technically valid and sound, is not sufficiently novel or exciting. Combining local and global perspectives are common ideas in graphs. Even on temporal graph, point process based modeling aims to capture the graph-wide evolution pattern from  a global perspective, such as (Lu et al., 2019) and the below paper [a]. A detailed discussion on temporal point processes for temporal graph is warranted, potentially with additional experimental comparison.\n\n[a] Trend: Temporal event and node dynamics for graph representation learning. WWW 2022.\n\nW2. Certain parts in the motivation of the paper are not clearly explained. For example, the following sentences:\n\"Pairwise interactions observed in different graphs or even the same temporal graph typically have different temporal properties.\"\n\"Since the endogenous and exogenous factors driving the generative process ...\" \nI'm not exactly sure how they directly connect to or motivate the proposed method.\n\nW3. In Table 2, Random tends to perform the best compared to historical/inductive strategies. It is surprising and more discussion is needed. (Also, I'm not confident of the results in Table 2, as it has some discrepancy with the results in Table 3 -- e.g. for UCI, the results in Table 2 and Table 3 are different.\n\nQuestions: Please see Weaknesses.", "labeling_timestamp": "2026-01-11T17:15:46.777970", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states that pairwise/temporal interactions exhibit different temporal granularities and explains why modeling both global and local temporal properties is necessary; it then connects this motivation to the proposed method by implementing a global TCN-based module and a local time-interval-weighted aggregator plus a cross-perspective fusion module.", "evidence": "1) \"The diversity of graph topologies across different domains leads to the complexity of temporal properties. Due to the regularity and abruptness of events, the pattern of events can also vary across time. Therefore, modeling at different time granularities have to be taken into account. RNNs learn the evolution patterns between adjacent graph snapshots at a coarse level. In contrast, MP-TGNs and WA-TGNs encode timestamps simultaneously while aggregating neighborhood contextual information... these two types of approaches model the temporal relevance of event occurrence in different forms and with different granularities...\" (Introduction).\n\n2) \"As shown in Figure 2, the framework of GLEN includes three major components: a GCN-TCN-based global embedding module, a local embedding module based on time interval weighting, and a cross-perspective fusion module. The global and local embedding modules respectively generate node embeddings from the global or local perspective.\" (Section 4.1).\n\n3) \"From the global perspective, we innovatively employ TCN instead of conventionally adopted RNNs for more stable and efficient training. From the local perspective, a new weighted sum algorithm based on time interval is devised to effectively aggregate neighborhood information. To better combine globally and locally acquired node embeddings, we introduce a cross-perspective fusion module based on a devised attention mechanism.\" (Introduction / Contributions).", "section": "Introduction; Section 4.1 (Overall Framework); Contributions"}
{"claim": "The sentence regarding endogenous and exogenous factors driving the generative process is not clearly connected to or motivating the proposed methodology.", "claim_type": "presentation", "paper_id": "auguNUCto5", "paper_title": "Boosting Temporal Graph Learning From Global and Local Perspectives", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "PXhDGnz45s", "reviewer": "Reviewer_L1Kz", "review_text": "Summary: The paper discusses the limitations of existing methods in temporal graph learning, which either focus on global or local perspectives but not both. To overcome this, the Global and Local Embedding Network (GLEN) is proposed. GLEN dynamically generates node embeddings by considering both global and local information. These embeddings are then fused using a cross-perspective module to capture high-order semantic relations. GLEN has been evaluated on multiple datasets and outperforms baselines in tasks like link prediction and dynamic node classification.\n\nStrengths: S1. Temporal graph is an important problem to address in practical world, yet majority of the research deals with static graphs only.\n\nS2. The analysis of existing RNN-based work and random walk/message passing models provides useful insights.\n\nS3. The writing/organization of the paper is generally clear, although some parts need more clarification. (See W2)\n\nWeaknesses: W1. The proposed model, while technically valid and sound, is not sufficiently novel or exciting. Combining local and global perspectives are common ideas in graphs. Even on temporal graph, point process based modeling aims to capture the graph-wide evolution pattern from  a global perspective, such as (Lu et al., 2019) and the below paper [a]. A detailed discussion on temporal point processes for temporal graph is warranted, potentially with additional experimental comparison.\n\n[a] Trend: Temporal event and node dynamics for graph representation learning. WWW 2022.\n\nW2. Certain parts in the motivation of the paper are not clearly explained. For example, the following sentences:\n\"Pairwise interactions observed in different graphs or even the same temporal graph typically have different temporal properties.\"\n\"Since the endogenous and exogenous factors driving the generative process ...\" \nI'm not exactly sure how they directly connect to or motivate the proposed method.\n\nW3. In Table 2, Random tends to perform the best compared to historical/inductive strategies. It is surprising and more discussion is needed. (Also, I'm not confident of the results in Table 2, as it has some discrepancy with the results in Table 3 -- e.g. for UCI, the results in Table 2 and Table 3 are different.\n\nQuestions: Please see Weaknesses.", "labeling_timestamp": "2026-01-11T17:15:56.242171", "model": "gpt-5-mini", "label": "Contradicted", "justification": "In the Introduction the paper immediately follows the sentence about endogenous/exogenous factors with concrete examples and contrasts between global (GNN-RNN) and local (TGN) methods, using that variation to motivate combining global and local perspectives in GLEN. Thus the sentence is presented as part of the motivation for the proposed methodology.", "evidence": "\"Since the endogenous and exogenous factors driving the generative process of networks are frequently complex and variable, temporal graphs across diverse domains tend to exhibit a variety of properties (Zheng et al., 2021). For instance, social networks and international trade networks may have extremely different characteristics (e.g., varying sparsities and edge recurrence patterns) (Poursafaei et al., 2022). GNNRNNmethods with the global perspective are more likely to consider the overall nature of a temporal graph since GNNs maintain the complete graph structure at different time steps. In contrast, finegrained patterns in motifs (Paranjape et al., 2017; Liu et al., 2021) such as the triadic closure process (Zhou et al., 2018; Liu et al., 2022) are better reflected in the encoding of local subgraphs by TGNs.\"", "section": "1 INTRODUCTION"}
{"claim": "In Table 2, the Random strategy outperforms historical and inductive strategies, which is surprising and lacks sufficient discussion.", "claim_type": "experimental", "paper_id": "auguNUCto5", "paper_title": "Boosting Temporal Graph Learning From Global and Local Perspectives", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "PXhDGnz45s", "reviewer": "Reviewer_L1Kz", "review_text": "Summary: The paper discusses the limitations of existing methods in temporal graph learning, which either focus on global or local perspectives but not both. To overcome this, the Global and Local Embedding Network (GLEN) is proposed. GLEN dynamically generates node embeddings by considering both global and local information. These embeddings are then fused using a cross-perspective module to capture high-order semantic relations. GLEN has been evaluated on multiple datasets and outperforms baselines in tasks like link prediction and dynamic node classification.\n\nStrengths: S1. Temporal graph is an important problem to address in practical world, yet majority of the research deals with static graphs only.\n\nS2. The analysis of existing RNN-based work and random walk/message passing models provides useful insights.\n\nS3. The writing/organization of the paper is generally clear, although some parts need more clarification. (See W2)\n\nWeaknesses: W1. The proposed model, while technically valid and sound, is not sufficiently novel or exciting. Combining local and global perspectives are common ideas in graphs. Even on temporal graph, point process based modeling aims to capture the graph-wide evolution pattern from  a global perspective, such as (Lu et al., 2019) and the below paper [a]. A detailed discussion on temporal point processes for temporal graph is warranted, potentially with additional experimental comparison.\n\n[a] Trend: Temporal event and node dynamics for graph representation learning. WWW 2022.\n\nW2. Certain parts in the motivation of the paper are not clearly explained. For example, the following sentences:\n\"Pairwise interactions observed in different graphs or even the same temporal graph typically have different temporal properties.\"\n\"Since the endogenous and exogenous factors driving the generative process ...\" \nI'm not exactly sure how they directly connect to or motivate the proposed method.\n\nW3. In Table 2, Random tends to perform the best compared to historical/inductive strategies. It is surprising and more discussion is needed. (Also, I'm not confident of the results in Table 2, as it has some discrepancy with the results in Table 3 -- e.g. for UCI, the results in Table 2 and Table 3 are different.\n\nQuestions: Please see Weaknesses.", "labeling_timestamp": "2026-01-11T17:16:01.301139", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not include Table 2 or the experimental results/tables that would show the Random, historical, and inductive strategies' performances, so the claim cannot be verified from the text supplied.", "evidence": "We evaluate GLEN on multiple real-world datasets and apply several negative sampling strategies.", "section": "ABSTRACT"}
{"claim": "There is a discrepancy between the UCI results reported in Table 2 and Table 3, indicating inconsistent or unreliable experimental reporting.", "claim_type": "experimental", "paper_id": "auguNUCto5", "paper_title": "Boosting Temporal Graph Learning From Global and Local Perspectives", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "PXhDGnz45s", "reviewer": "Reviewer_L1Kz", "review_text": "Summary: The paper discusses the limitations of existing methods in temporal graph learning, which either focus on global or local perspectives but not both. To overcome this, the Global and Local Embedding Network (GLEN) is proposed. GLEN dynamically generates node embeddings by considering both global and local information. These embeddings are then fused using a cross-perspective module to capture high-order semantic relations. GLEN has been evaluated on multiple datasets and outperforms baselines in tasks like link prediction and dynamic node classification.\n\nStrengths: S1. Temporal graph is an important problem to address in practical world, yet majority of the research deals with static graphs only.\n\nS2. The analysis of existing RNN-based work and random walk/message passing models provides useful insights.\n\nS3. The writing/organization of the paper is generally clear, although some parts need more clarification. (See W2)\n\nWeaknesses: W1. The proposed model, while technically valid and sound, is not sufficiently novel or exciting. Combining local and global perspectives are common ideas in graphs. Even on temporal graph, point process based modeling aims to capture the graph-wide evolution pattern from  a global perspective, such as (Lu et al., 2019) and the below paper [a]. A detailed discussion on temporal point processes for temporal graph is warranted, potentially with additional experimental comparison.\n\n[a] Trend: Temporal event and node dynamics for graph representation learning. WWW 2022.\n\nW2. Certain parts in the motivation of the paper are not clearly explained. For example, the following sentences:\n\"Pairwise interactions observed in different graphs or even the same temporal graph typically have different temporal properties.\"\n\"Since the endogenous and exogenous factors driving the generative process ...\" \nI'm not exactly sure how they directly connect to or motivate the proposed method.\n\nW3. In Table 2, Random tends to perform the best compared to historical/inductive strategies. It is surprising and more discussion is needed. (Also, I'm not confident of the results in Table 2, as it has some discrepancy with the results in Table 3 -- e.g. for UCI, the results in Table 2 and Table 3 are different.\n\nQuestions: Please see Weaknesses.", "labeling_timestamp": "2026-01-11T17:16:00.476663", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt describes experimental evaluation in the abstract and claims extensive results, but the excerpt does not include Tables 2 or 3 (nor an Experiments/Results section with those tables), so the alleged discrepancy cannot be verified from the given content.", "evidence": "We evaluate GLEN on multiple real-world datasets and apply several negative sampling strategies. Sufficient experimental results demonstrate that GLEN outperforms other baselines in both link prediction and dynamic node classification tasks.", "section": "Abstract"}
{"claim": "The paper should include additional experimental comparisons with temporal point process methods to validate claimed advantages of GLEN over global modeling approaches.", "claim_type": "baseline", "paper_id": "auguNUCto5", "paper_title": "Boosting Temporal Graph Learning From Global and Local Perspectives", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "PXhDGnz45s", "reviewer": "Reviewer_L1Kz", "review_text": "Summary: The paper discusses the limitations of existing methods in temporal graph learning, which either focus on global or local perspectives but not both. To overcome this, the Global and Local Embedding Network (GLEN) is proposed. GLEN dynamically generates node embeddings by considering both global and local information. These embeddings are then fused using a cross-perspective module to capture high-order semantic relations. GLEN has been evaluated on multiple datasets and outperforms baselines in tasks like link prediction and dynamic node classification.\n\nStrengths: S1. Temporal graph is an important problem to address in practical world, yet majority of the research deals with static graphs only.\n\nS2. The analysis of existing RNN-based work and random walk/message passing models provides useful insights.\n\nS3. The writing/organization of the paper is generally clear, although some parts need more clarification. (See W2)\n\nWeaknesses: W1. The proposed model, while technically valid and sound, is not sufficiently novel or exciting. Combining local and global perspectives are common ideas in graphs. Even on temporal graph, point process based modeling aims to capture the graph-wide evolution pattern from  a global perspective, such as (Lu et al., 2019) and the below paper [a]. A detailed discussion on temporal point processes for temporal graph is warranted, potentially with additional experimental comparison.\n\n[a] Trend: Temporal event and node dynamics for graph representation learning. WWW 2022.\n\nW2. Certain parts in the motivation of the paper are not clearly explained. For example, the following sentences:\n\"Pairwise interactions observed in different graphs or even the same temporal graph typically have different temporal properties.\"\n\"Since the endogenous and exogenous factors driving the generative process ...\" \nI'm not exactly sure how they directly connect to or motivate the proposed method.\n\nW3. In Table 2, Random tends to perform the best compared to historical/inductive strategies. It is surprising and more discussion is needed. (Also, I'm not confident of the results in Table 2, as it has some discrepancy with the results in Table 3 -- e.g. for UCI, the results in Table 2 and Table 3 are different.\n\nQuestions: Please see Weaknesses.", "labeling_timestamp": "2026-01-11T17:16:17.261838", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper's Related Works and method descriptions enumerate GNN-RNN, message-passing TGNs, and walk-aggregating TGNs as the compared/considered families but do not mention temporal point process (TPP) methods nor report experiments comparing against TPP-based approaches. Therefore the reviewer's recommendation that additional experimental comparisons with temporal point process methods are needed is consistent with what the paper omits.", "evidence": "\"GNN-RNN-based temporal graph methods such as EvolveGCN (Pareja et al., 2020), CTGCN (Liu et al., 2020), and GCRN (Seo et al., 2018) ... Message-passing temporal graph networks (MP-TGNs) such as JODIE (Kumar et al., 2019), TGAT (Xu et al., 2020), TGN (Rossi et al., 2020), APAN (Wang et al., 2021b), and TPGNN (Wang et al., 2022) ... Walk-aggregating temporal graph networks (WA-TGNs) such as evolve2vec (Bastas et al., 2019a), STWalk (Pandhre et al., 2018), and EVONRL (Heidari & Papagelis, 2020) ...\"", "section": "2 RELATED WORKS"}
{"claim": "The decision to frame the learning task as a POMDP lacks clear justification in the manuscript.", "claim_type": "methodology", "paper_id": "WcSofkUVge", "paper_title": "Utility-based Adaptive Teaching Strategies using Bayesian Theory of Mind", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "RYHtZ3asnL", "reviewer": "Reviewer_3FPZ", "review_text": "Summary: The paper presents a method for adaptive teaching based on utility theory. The authors propose a model in which the teacher's actions are determined by the expected utility of each teaching action. This utility-based framework aims to adapt teaching strategies according to students' needs and progress. The paper also details the model's implementation in a educational setting and provides empirical evidence of its effectiveness.\n\nStrengths: 1. Integrating utility theory into adaptive teaching is a novel approach.\n2. Beyond estimating a student's skill level, the paper also aims to estimate the learner's internal goal, offering finer-grained teaching.\n3. The paper outlines the model's mathematical foundation, ensuring replicability for other researchers.\n4. The results have potential applications in personalized learning, AI-driven educational platforms, and adaptive curriculum design.\n\nWeaknesses: 1. The paper's formulation is ambiguous.\n\n    1. The decision to frame the learning task as a POMDP lacks clear justification. While this relates to the proposed method, more concrete examples are needed for justification.\n\n    2. The task formulation for the ToM-teacher isn't clearly presented in the manuscript. From my perspective, the teaching task resembles a POMDP, akin to references [1, 2]. The teacher doesn't fully know the learner's internal state but tries to optimize the student's performance by providing demonstrations. Unlike earlier works, this paper also seeks to estimate the learner's transition function parameters (goals and observation function). Clarifying the task's formulation is crucial for understanding the technical value of the proposed approach, which remains unclear.\n\n2. The paper's objective is unclear. The authors claim, \"The goal of this work is to study whether learner-specific teachers who model the learner’s internal state are more efficient than learner-agnostic ones.\" Given the vague problem definition, it's unclear if this captures the paper's essence. Is the primary takeaway that Bayesian ToM-teachers excel at understanding human internal state changes?\n\n3. The evaluation appears lacking. The paper notes that the teacher has a general idea of the student's policy, and during testing, the student's actions are driven by a basic decision tree. This might mean there's minimal uncertainty in the learner's behavior. It would be beneficial to incorporate human-subject experiments to see real-world impact.\n\n4. Based on points from point 1 and equation 3, the teaching policy is essentially a greedy policy. Over time, this might not be the best strategy and could lead to suboptimal results. Without a well-defined teaching task, such a policy is not well justified.\n\n[1] Srivastava et al., Assistive Teaching of Motor Control Tasks to Humans, 2022.\n\n[2] Yu et al., COACH: Cooperative Robot Teaching, 2022.\n\n\n**update after rebuttal:**\n\nI agree with Reviewer C7a4's comment regarding the absence of related works in the submission. To appropriately position this work in the context of existing literature, some major changes are needed in the manuscript. Therefore, I am maintaining my current rating.\n\nQuestions: see weaknesses", "labeling_timestamp": "2026-01-11T17:16:24.937662", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The manuscript explicitly frames the environment as a Goal-Conditioned POMDP and provides the rationale: learners have limited information / sensory capacity and thus must maintain beliefs over states and use observations to infer state—motivating the POMDP formulation (see Sections 3.1 and 3.2).", "evidence": "We introduce the learners' environment as a Goal-Conditioned Partially Observable Markov Decision Process (GC-POMDP), which is a combination of a Goal-Conditioned Markov Decision Process (GC-MDP) and, similarly to Rabinowitz et al. (2018), a Partially Observable Markov Decision Process (POMDP). In GC-POMDPs, agents aim at achieving different goals with limited information on the current state of the environment.\n\nIn POMDPs, since the state is not directly observed, the learner must rely on the recent history of observations, to infer a distribution over states and maintain a belief on the environment state (Kaelbling et al., 1998; Ghavamzadeh et al., 2015).", "section": "3.1 LEARNING ENVIRONMENT; 3.2 LEARNER"}
{"claim": "The paper provides insufficient concrete examples to justify framing the teaching problem as a POMDP.", "claim_type": "subjective", "paper_id": "WcSofkUVge", "paper_title": "Utility-based Adaptive Teaching Strategies using Bayesian Theory of Mind", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "RYHtZ3asnL", "reviewer": "Reviewer_3FPZ", "review_text": "Summary: The paper presents a method for adaptive teaching based on utility theory. The authors propose a model in which the teacher's actions are determined by the expected utility of each teaching action. This utility-based framework aims to adapt teaching strategies according to students' needs and progress. The paper also details the model's implementation in a educational setting and provides empirical evidence of its effectiveness.\n\nStrengths: 1. Integrating utility theory into adaptive teaching is a novel approach.\n2. Beyond estimating a student's skill level, the paper also aims to estimate the learner's internal goal, offering finer-grained teaching.\n3. The paper outlines the model's mathematical foundation, ensuring replicability for other researchers.\n4. The results have potential applications in personalized learning, AI-driven educational platforms, and adaptive curriculum design.\n\nWeaknesses: 1. The paper's formulation is ambiguous.\n\n    1. The decision to frame the learning task as a POMDP lacks clear justification. While this relates to the proposed method, more concrete examples are needed for justification.\n\n    2. The task formulation for the ToM-teacher isn't clearly presented in the manuscript. From my perspective, the teaching task resembles a POMDP, akin to references [1, 2]. The teacher doesn't fully know the learner's internal state but tries to optimize the student's performance by providing demonstrations. Unlike earlier works, this paper also seeks to estimate the learner's transition function parameters (goals and observation function). Clarifying the task's formulation is crucial for understanding the technical value of the proposed approach, which remains unclear.\n\n2. The paper's objective is unclear. The authors claim, \"The goal of this work is to study whether learner-specific teachers who model the learner’s internal state are more efficient than learner-agnostic ones.\" Given the vague problem definition, it's unclear if this captures the paper's essence. Is the primary takeaway that Bayesian ToM-teachers excel at understanding human internal state changes?\n\n3. The evaluation appears lacking. The paper notes that the teacher has a general idea of the student's policy, and during testing, the student's actions are driven by a basic decision tree. This might mean there's minimal uncertainty in the learner's behavior. It would be beneficial to incorporate human-subject experiments to see real-world impact.\n\n4. Based on points from point 1 and equation 3, the teaching policy is essentially a greedy policy. Over time, this might not be the best strategy and could lead to suboptimal results. Without a well-defined teaching task, such a policy is not well justified.\n\n[1] Srivastava et al., Assistive Teaching of Motor Control Tasks to Humans, 2022.\n\n[2] Yu et al., COACH: Cooperative Robot Teaching, 2022.\n\n\n**update after rebuttal:**\n\nI agree with Reviewer C7a4's comment regarding the absence of related works in the submission. To appropriately position this work in the context of existing literature, some major changes are needed in the manuscript. Therefore, I am maintaining my current rating.\n\nQuestions: see weaknesses", "labeling_timestamp": "2026-01-11T17:16:45.812602", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly frames the problem as a Goal-Conditioned POMDP, defines the formal GC-POMDP model, and gives concrete instantiations (MiniGrid gridworld and a toy environment) plus detailed modeling of partial observability via observation functions/receptive fields and Bayesian belief updates. These provide concrete examples and justification for the POMDP formulation.", "evidence": "We introduce the learners' environment as a Goal-Conditioned Partially Observable Markov Decision Process (GC-POMDP), which is a combination of a Goal-Conditioned Markov Decision Process (GC-MDP) and, similarly to Rabinowitz et al. (2018), a Partially Observable Markov Decision Process (POMDP).\n\nIn practice, our GC-POMDPs are different instances of similar gridworld environments constructed from the MiniGrid library (Chevalier-Boisvert et al., 2023). Another example with a toy environment is described in Appendix A.\n\nMoreover, within MiniGrid environments, the observation functions v_i are defined by a square area of size v_i × v_i cells, known as the receptive field of learner L_i. This receptive field defines the localised region in front of the learner, mimicking visual sensory capacities and a larger receptive field size helps the learner reach its goal faster.", "section": "3.1 LEARNING ENVIRONMENT; 3.2 LEARNER; INTRODUCTION"}
{"claim": "The manuscript does not clearly present the task formulation for the Theory-of-Mind (ToM) teacher.", "claim_type": "presentation", "paper_id": "WcSofkUVge", "paper_title": "Utility-based Adaptive Teaching Strategies using Bayesian Theory of Mind", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "RYHtZ3asnL", "reviewer": "Reviewer_3FPZ", "review_text": "Summary: The paper presents a method for adaptive teaching based on utility theory. The authors propose a model in which the teacher's actions are determined by the expected utility of each teaching action. This utility-based framework aims to adapt teaching strategies according to students' needs and progress. The paper also details the model's implementation in a educational setting and provides empirical evidence of its effectiveness.\n\nStrengths: 1. Integrating utility theory into adaptive teaching is a novel approach.\n2. Beyond estimating a student's skill level, the paper also aims to estimate the learner's internal goal, offering finer-grained teaching.\n3. The paper outlines the model's mathematical foundation, ensuring replicability for other researchers.\n4. The results have potential applications in personalized learning, AI-driven educational platforms, and adaptive curriculum design.\n\nWeaknesses: 1. The paper's formulation is ambiguous.\n\n    1. The decision to frame the learning task as a POMDP lacks clear justification. While this relates to the proposed method, more concrete examples are needed for justification.\n\n    2. The task formulation for the ToM-teacher isn't clearly presented in the manuscript. From my perspective, the teaching task resembles a POMDP, akin to references [1, 2]. The teacher doesn't fully know the learner's internal state but tries to optimize the student's performance by providing demonstrations. Unlike earlier works, this paper also seeks to estimate the learner's transition function parameters (goals and observation function). Clarifying the task's formulation is crucial for understanding the technical value of the proposed approach, which remains unclear.\n\n2. The paper's objective is unclear. The authors claim, \"The goal of this work is to study whether learner-specific teachers who model the learner’s internal state are more efficient than learner-agnostic ones.\" Given the vague problem definition, it's unclear if this captures the paper's essence. Is the primary takeaway that Bayesian ToM-teachers excel at understanding human internal state changes?\n\n3. The evaluation appears lacking. The paper notes that the teacher has a general idea of the student's policy, and during testing, the student's actions are driven by a basic decision tree. This might mean there's minimal uncertainty in the learner's behavior. It would be beneficial to incorporate human-subject experiments to see real-world impact.\n\n4. Based on points from point 1 and equation 3, the teaching policy is essentially a greedy policy. Over time, this might not be the best strategy and could lead to suboptimal results. Without a well-defined teaching task, such a policy is not well justified.\n\n[1] Srivastava et al., Assistive Teaching of Motor Control Tasks to Humans, 2022.\n\n[2] Yu et al., COACH: Cooperative Robot Teaching, 2022.\n\n\n**update after rebuttal:**\n\nI agree with Reviewer C7a4's comment regarding the absence of related works in the submission. To appropriately position this work in the context of existing literature, some major changes are needed in the manuscript. Therefore, I am maintaining my current rating.\n\nQuestions: see weaknesses", "labeling_timestamp": "2026-01-11T17:16:35.937040", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly defines the ToM-teacher task: it specifies that teachers infer the learner's internal state via Bayesian inference from observations and use that belief to estimate demonstration utilities, selecting demonstrations that maximise learner reward while minimising teaching cost. The Methods section also formalises the environment (GC-POMDP) and learner beliefs, supporting a clear task formulation.", "evidence": "To achieve this, as depicted in Figure 1, we define ToM-teachers able to\n\n1. update a belief about the internal state (i.e. goal, intention, belief, sensory capacity) of an unknown learner through Bayesian inference based on observations of its behaviour in a simple environment, see Figure 1(A), and\n2. leverage this belief to estimate the utility of different demonstrations in a more complex environment, similarly to human planning as described in Ho et al. (2022), in order to select the most effective one for the specific observed learner, see Figure 1(B).\n\n\"Our ToM-equipped teachers construct models of learners' internal states from observations and leverage them to select demonstrations that maximise the learners' rewards while minimising teaching costs.\"", "section": "1 INTRODUCTION (also ABSTRACT; Methods: 3.1-3.2)"}
{"claim": "The paper fails to clearly differentiate its approach from prior POMDP-like teaching works, leaving technical novelty unclear.", "claim_type": "novelty", "paper_id": "WcSofkUVge", "paper_title": "Utility-based Adaptive Teaching Strategies using Bayesian Theory of Mind", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "RYHtZ3asnL", "reviewer": "Reviewer_3FPZ", "review_text": "Summary: The paper presents a method for adaptive teaching based on utility theory. The authors propose a model in which the teacher's actions are determined by the expected utility of each teaching action. This utility-based framework aims to adapt teaching strategies according to students' needs and progress. The paper also details the model's implementation in a educational setting and provides empirical evidence of its effectiveness.\n\nStrengths: 1. Integrating utility theory into adaptive teaching is a novel approach.\n2. Beyond estimating a student's skill level, the paper also aims to estimate the learner's internal goal, offering finer-grained teaching.\n3. The paper outlines the model's mathematical foundation, ensuring replicability for other researchers.\n4. The results have potential applications in personalized learning, AI-driven educational platforms, and adaptive curriculum design.\n\nWeaknesses: 1. The paper's formulation is ambiguous.\n\n    1. The decision to frame the learning task as a POMDP lacks clear justification. While this relates to the proposed method, more concrete examples are needed for justification.\n\n    2. The task formulation for the ToM-teacher isn't clearly presented in the manuscript. From my perspective, the teaching task resembles a POMDP, akin to references [1, 2]. The teacher doesn't fully know the learner's internal state but tries to optimize the student's performance by providing demonstrations. Unlike earlier works, this paper also seeks to estimate the learner's transition function parameters (goals and observation function). Clarifying the task's formulation is crucial for understanding the technical value of the proposed approach, which remains unclear.\n\n2. The paper's objective is unclear. The authors claim, \"The goal of this work is to study whether learner-specific teachers who model the learner’s internal state are more efficient than learner-agnostic ones.\" Given the vague problem definition, it's unclear if this captures the paper's essence. Is the primary takeaway that Bayesian ToM-teachers excel at understanding human internal state changes?\n\n3. The evaluation appears lacking. The paper notes that the teacher has a general idea of the student's policy, and during testing, the student's actions are driven by a basic decision tree. This might mean there's minimal uncertainty in the learner's behavior. It would be beneficial to incorporate human-subject experiments to see real-world impact.\n\n4. Based on points from point 1 and equation 3, the teaching policy is essentially a greedy policy. Over time, this might not be the best strategy and could lead to suboptimal results. Without a well-defined teaching task, such a policy is not well justified.\n\n[1] Srivastava et al., Assistive Teaching of Motor Control Tasks to Humans, 2022.\n\n[2] Yu et al., COACH: Cooperative Robot Teaching, 2022.\n\n\n**update after rebuttal:**\n\nI agree with Reviewer C7a4's comment regarding the absence of related works in the submission. To appropriately position this work in the context of existing literature, some major changes are needed in the manuscript. Therefore, I am maintaining my current rating.\n\nQuestions: see weaknesses", "labeling_timestamp": "2026-01-11T17:16:34.485230", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly contrasts its method with prior machine teaching and ToM work and states specific technical differences (Bayesian ToM of learner internal state, handling learners with unknown goals and sensory capacities, and using ToM to select demonstrations), so the claim that it fails to differentiate and leaves novelty unclear is contradicted by the text.", "evidence": "\"In our framework the teacher must select the most helpful demonstration from a given set for various types of learner. Yet, unlike these prior studies, our teacher assists various learners with different goals and sensory capacities, and thus different optimal demonstrations. Previous studies have demonstrated the benefits of adaptivity in sequential machine teaching (Chen et al., 2018) and motor control (Srivastava et al., 2022) for learning. Unlike this prior research, we introduce a model of ToM explicitly modeling the learner's mental state as a pivotal component of our teacher's adaptivity.\"", "section": "Related Work"}
{"claim": "The paper's primary objective is vaguely defined, making the core contribution and takeaway unclear.", "claim_type": "presentation", "paper_id": "WcSofkUVge", "paper_title": "Utility-based Adaptive Teaching Strategies using Bayesian Theory of Mind", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "RYHtZ3asnL", "reviewer": "Reviewer_3FPZ", "review_text": "Summary: The paper presents a method for adaptive teaching based on utility theory. The authors propose a model in which the teacher's actions are determined by the expected utility of each teaching action. This utility-based framework aims to adapt teaching strategies according to students' needs and progress. The paper also details the model's implementation in a educational setting and provides empirical evidence of its effectiveness.\n\nStrengths: 1. Integrating utility theory into adaptive teaching is a novel approach.\n2. Beyond estimating a student's skill level, the paper also aims to estimate the learner's internal goal, offering finer-grained teaching.\n3. The paper outlines the model's mathematical foundation, ensuring replicability for other researchers.\n4. The results have potential applications in personalized learning, AI-driven educational platforms, and adaptive curriculum design.\n\nWeaknesses: 1. The paper's formulation is ambiguous.\n\n    1. The decision to frame the learning task as a POMDP lacks clear justification. While this relates to the proposed method, more concrete examples are needed for justification.\n\n    2. The task formulation for the ToM-teacher isn't clearly presented in the manuscript. From my perspective, the teaching task resembles a POMDP, akin to references [1, 2]. The teacher doesn't fully know the learner's internal state but tries to optimize the student's performance by providing demonstrations. Unlike earlier works, this paper also seeks to estimate the learner's transition function parameters (goals and observation function). Clarifying the task's formulation is crucial for understanding the technical value of the proposed approach, which remains unclear.\n\n2. The paper's objective is unclear. The authors claim, \"The goal of this work is to study whether learner-specific teachers who model the learner’s internal state are more efficient than learner-agnostic ones.\" Given the vague problem definition, it's unclear if this captures the paper's essence. Is the primary takeaway that Bayesian ToM-teachers excel at understanding human internal state changes?\n\n3. The evaluation appears lacking. The paper notes that the teacher has a general idea of the student's policy, and during testing, the student's actions are driven by a basic decision tree. This might mean there's minimal uncertainty in the learner's behavior. It would be beneficial to incorporate human-subject experiments to see real-world impact.\n\n4. Based on points from point 1 and equation 3, the teaching policy is essentially a greedy policy. Over time, this might not be the best strategy and could lead to suboptimal results. Without a well-defined teaching task, such a policy is not well justified.\n\n[1] Srivastava et al., Assistive Teaching of Motor Control Tasks to Humans, 2022.\n\n[2] Yu et al., COACH: Cooperative Robot Teaching, 2022.\n\n\n**update after rebuttal:**\n\nI agree with Reviewer C7a4's comment regarding the absence of related works in the submission. To appropriately position this work in the context of existing literature, some major changes are needed in the manuscript. Therefore, I am maintaining my current rating.\n\nQuestions: see weaknesses", "labeling_timestamp": "2026-01-11T17:16:46.220352", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states its primary objective and core contributions: to develop Bayesian ToM-equipped teacher agents, evaluate whether learner-specific teachers outperform learner-agnostic ones, and study limitations due to inaccurate priors or limited observations. It also lists concrete capabilities of the proposed teachers (belief update and utility-based demonstration selection), making the objective and takeaways clear.", "evidence": "Abstract: \"Inspired by cognitive science, we build on Bayesian ToM mechanisms to design teacher agents that, like humans, tailor their teaching strategies to the learners.\" Introduction: \"The goal of this work is to study whether learner-specific teachers who model the learner's internal state are more efficient than learner-agnostic ones and more importantly to explore the limitations of ToM models with inaccurate priors or limited observation of the learner, in a context where providing guidance incurs a cost proportional to its informativeness.\" Introduction (capabilities): \"To achieve this, as depicted in Figure 1, we define ToM-teachers able to 1. update a belief about the internal state ... through Bayesian inference ... and 2. leverage this belief to estimate the utility of different demonstrations ... in order to select the most effective one for the specific observed learner.\"", "section": "Abstract and Introduction"}
{"claim": "It is unclear whether the main claim is that Bayesian ToM-teachers better capture human internal state changes.", "claim_type": "experimental", "paper_id": "WcSofkUVge", "paper_title": "Utility-based Adaptive Teaching Strategies using Bayesian Theory of Mind", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "RYHtZ3asnL", "reviewer": "Reviewer_3FPZ", "review_text": "Summary: The paper presents a method for adaptive teaching based on utility theory. The authors propose a model in which the teacher's actions are determined by the expected utility of each teaching action. This utility-based framework aims to adapt teaching strategies according to students' needs and progress. The paper also details the model's implementation in a educational setting and provides empirical evidence of its effectiveness.\n\nStrengths: 1. Integrating utility theory into adaptive teaching is a novel approach.\n2. Beyond estimating a student's skill level, the paper also aims to estimate the learner's internal goal, offering finer-grained teaching.\n3. The paper outlines the model's mathematical foundation, ensuring replicability for other researchers.\n4. The results have potential applications in personalized learning, AI-driven educational platforms, and adaptive curriculum design.\n\nWeaknesses: 1. The paper's formulation is ambiguous.\n\n    1. The decision to frame the learning task as a POMDP lacks clear justification. While this relates to the proposed method, more concrete examples are needed for justification.\n\n    2. The task formulation for the ToM-teacher isn't clearly presented in the manuscript. From my perspective, the teaching task resembles a POMDP, akin to references [1, 2]. The teacher doesn't fully know the learner's internal state but tries to optimize the student's performance by providing demonstrations. Unlike earlier works, this paper also seeks to estimate the learner's transition function parameters (goals and observation function). Clarifying the task's formulation is crucial for understanding the technical value of the proposed approach, which remains unclear.\n\n2. The paper's objective is unclear. The authors claim, \"The goal of this work is to study whether learner-specific teachers who model the learner’s internal state are more efficient than learner-agnostic ones.\" Given the vague problem definition, it's unclear if this captures the paper's essence. Is the primary takeaway that Bayesian ToM-teachers excel at understanding human internal state changes?\n\n3. The evaluation appears lacking. The paper notes that the teacher has a general idea of the student's policy, and during testing, the student's actions are driven by a basic decision tree. This might mean there's minimal uncertainty in the learner's behavior. It would be beneficial to incorporate human-subject experiments to see real-world impact.\n\n4. Based on points from point 1 and equation 3, the teaching policy is essentially a greedy policy. Over time, this might not be the best strategy and could lead to suboptimal results. Without a well-defined teaching task, such a policy is not well justified.\n\n[1] Srivastava et al., Assistive Teaching of Motor Control Tasks to Humans, 2022.\n\n[2] Yu et al., COACH: Cooperative Robot Teaching, 2022.\n\n\n**update after rebuttal:**\n\nI agree with Reviewer C7a4's comment regarding the absence of related works in the submission. To appropriately position this work in the context of existing literature, some major changes are needed in the manuscript. Therefore, I am maintaining my current rating.\n\nQuestions: see weaknesses", "labeling_timestamp": "2026-01-11T17:16:56.299215", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper clearly states its main claim: Bayesian ToM-equipped teachers model learners' internal states to select demonstrations that improve teaching efficiency (learners taught this way are more efficient than learner-agnostic ones). It does not frame the main claim as being about 'better capture[ing] human internal state changes' or leave that unclear.", "evidence": "Abstract: \"Our ToM-equipped teachers construct models of learners' internal states from observations and leverage them to select demonstrations that maximise the learners' rewards while minimising teaching costs. Our experiments in simulated environments demonstrate that learners taught this way are more efficient than those taught in a learner-agnostic way.\"", "section": "Abstract; 1 INTRODUCTION"}
{"claim": "During testing, the student's actions are driven by a basic decision tree, implying minimal uncertainty in learner behavior.", "claim_type": "methodology", "paper_id": "WcSofkUVge", "paper_title": "Utility-based Adaptive Teaching Strategies using Bayesian Theory of Mind", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "RYHtZ3asnL", "reviewer": "Reviewer_3FPZ", "review_text": "Summary: The paper presents a method for adaptive teaching based on utility theory. The authors propose a model in which the teacher's actions are determined by the expected utility of each teaching action. This utility-based framework aims to adapt teaching strategies according to students' needs and progress. The paper also details the model's implementation in a educational setting and provides empirical evidence of its effectiveness.\n\nStrengths: 1. Integrating utility theory into adaptive teaching is a novel approach.\n2. Beyond estimating a student's skill level, the paper also aims to estimate the learner's internal goal, offering finer-grained teaching.\n3. The paper outlines the model's mathematical foundation, ensuring replicability for other researchers.\n4. The results have potential applications in personalized learning, AI-driven educational platforms, and adaptive curriculum design.\n\nWeaknesses: 1. The paper's formulation is ambiguous.\n\n    1. The decision to frame the learning task as a POMDP lacks clear justification. While this relates to the proposed method, more concrete examples are needed for justification.\n\n    2. The task formulation for the ToM-teacher isn't clearly presented in the manuscript. From my perspective, the teaching task resembles a POMDP, akin to references [1, 2]. The teacher doesn't fully know the learner's internal state but tries to optimize the student's performance by providing demonstrations. Unlike earlier works, this paper also seeks to estimate the learner's transition function parameters (goals and observation function). Clarifying the task's formulation is crucial for understanding the technical value of the proposed approach, which remains unclear.\n\n2. The paper's objective is unclear. The authors claim, \"The goal of this work is to study whether learner-specific teachers who model the learner’s internal state are more efficient than learner-agnostic ones.\" Given the vague problem definition, it's unclear if this captures the paper's essence. Is the primary takeaway that Bayesian ToM-teachers excel at understanding human internal state changes?\n\n3. The evaluation appears lacking. The paper notes that the teacher has a general idea of the student's policy, and during testing, the student's actions are driven by a basic decision tree. This might mean there's minimal uncertainty in the learner's behavior. It would be beneficial to incorporate human-subject experiments to see real-world impact.\n\n4. Based on points from point 1 and equation 3, the teaching policy is essentially a greedy policy. Over time, this might not be the best strategy and could lead to suboptimal results. Without a well-defined teaching task, such a policy is not well justified.\n\n[1] Srivastava et al., Assistive Teaching of Motor Control Tasks to Humans, 2022.\n\n[2] Yu et al., COACH: Cooperative Robot Teaching, 2022.\n\n\n**update after rebuttal:**\n\nI agree with Reviewer C7a4's comment regarding the absence of related works in the submission. To appropriately position this work in the context of existing literature, some major changes are needed in the manuscript. Therefore, I am maintaining my current rating.\n\nQuestions: see weaknesses", "labeling_timestamp": "2026-01-11T17:17:19.303623", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper describes learners as goal-conditioned POMDP agents that maintain and update a Bayesian belief and act according to a policy conditioned on goal, observation function, and belief—no mention of a simple decision tree driving actions; this contradicts the claim.", "evidence": "In POMDPs, since the state is not directly observed, the learner must rely on the recent history of observations, to infer a distribution over states and maintain a belief on the environment state ... the belief of learner L_i about the state s ... is updated using Bayesian update. ... we additionally assume that all learners share a policy on the environment M_j conditioned by a goal, an observation function and a belief: ... To simulate a trajectory τ_i of learner L_i on M_j, one only needs to know the tuple (π_j, g_i, v_i, b_i,j^0).", "section": "3.2 LEARNER"}
{"claim": "The empirical evaluation lacks human-subject experiments to validate real-world impact.", "claim_type": "experimental", "paper_id": "WcSofkUVge", "paper_title": "Utility-based Adaptive Teaching Strategies using Bayesian Theory of Mind", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "RYHtZ3asnL", "reviewer": "Reviewer_3FPZ", "review_text": "Summary: The paper presents a method for adaptive teaching based on utility theory. The authors propose a model in which the teacher's actions are determined by the expected utility of each teaching action. This utility-based framework aims to adapt teaching strategies according to students' needs and progress. The paper also details the model's implementation in a educational setting and provides empirical evidence of its effectiveness.\n\nStrengths: 1. Integrating utility theory into adaptive teaching is a novel approach.\n2. Beyond estimating a student's skill level, the paper also aims to estimate the learner's internal goal, offering finer-grained teaching.\n3. The paper outlines the model's mathematical foundation, ensuring replicability for other researchers.\n4. The results have potential applications in personalized learning, AI-driven educational platforms, and adaptive curriculum design.\n\nWeaknesses: 1. The paper's formulation is ambiguous.\n\n    1. The decision to frame the learning task as a POMDP lacks clear justification. While this relates to the proposed method, more concrete examples are needed for justification.\n\n    2. The task formulation for the ToM-teacher isn't clearly presented in the manuscript. From my perspective, the teaching task resembles a POMDP, akin to references [1, 2]. The teacher doesn't fully know the learner's internal state but tries to optimize the student's performance by providing demonstrations. Unlike earlier works, this paper also seeks to estimate the learner's transition function parameters (goals and observation function). Clarifying the task's formulation is crucial for understanding the technical value of the proposed approach, which remains unclear.\n\n2. The paper's objective is unclear. The authors claim, \"The goal of this work is to study whether learner-specific teachers who model the learner’s internal state are more efficient than learner-agnostic ones.\" Given the vague problem definition, it's unclear if this captures the paper's essence. Is the primary takeaway that Bayesian ToM-teachers excel at understanding human internal state changes?\n\n3. The evaluation appears lacking. The paper notes that the teacher has a general idea of the student's policy, and during testing, the student's actions are driven by a basic decision tree. This might mean there's minimal uncertainty in the learner's behavior. It would be beneficial to incorporate human-subject experiments to see real-world impact.\n\n4. Based on points from point 1 and equation 3, the teaching policy is essentially a greedy policy. Over time, this might not be the best strategy and could lead to suboptimal results. Without a well-defined teaching task, such a policy is not well justified.\n\n[1] Srivastava et al., Assistive Teaching of Motor Control Tasks to Humans, 2022.\n\n[2] Yu et al., COACH: Cooperative Robot Teaching, 2022.\n\n\n**update after rebuttal:**\n\nI agree with Reviewer C7a4's comment regarding the absence of related works in the submission. To appropriately position this work in the context of existing literature, some major changes are needed in the manuscript. Therefore, I am maintaining my current rating.\n\nQuestions: see weaknesses", "labeling_timestamp": "2026-01-11T17:16:57.654582", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly reports experiments in simulated environments (toy and MiniGrid gridworld) and describes simulated learners; it contains no mention of human-subject or user studies to validate real-world impact.", "evidence": "\"Our experiments in simulated environments demonstrate that learners taught this way are more efficient than those taught in a learner-agnostic way.\"", "section": "ABSTRACT"}
{"claim": "Equation 3 and related discussion suggest the teaching policy is essentially greedy.", "claim_type": "methodology", "paper_id": "WcSofkUVge", "paper_title": "Utility-based Adaptive Teaching Strategies using Bayesian Theory of Mind", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "RYHtZ3asnL", "reviewer": "Reviewer_3FPZ", "review_text": "Summary: The paper presents a method for adaptive teaching based on utility theory. The authors propose a model in which the teacher's actions are determined by the expected utility of each teaching action. This utility-based framework aims to adapt teaching strategies according to students' needs and progress. The paper also details the model's implementation in a educational setting and provides empirical evidence of its effectiveness.\n\nStrengths: 1. Integrating utility theory into adaptive teaching is a novel approach.\n2. Beyond estimating a student's skill level, the paper also aims to estimate the learner's internal goal, offering finer-grained teaching.\n3. The paper outlines the model's mathematical foundation, ensuring replicability for other researchers.\n4. The results have potential applications in personalized learning, AI-driven educational platforms, and adaptive curriculum design.\n\nWeaknesses: 1. The paper's formulation is ambiguous.\n\n    1. The decision to frame the learning task as a POMDP lacks clear justification. While this relates to the proposed method, more concrete examples are needed for justification.\n\n    2. The task formulation for the ToM-teacher isn't clearly presented in the manuscript. From my perspective, the teaching task resembles a POMDP, akin to references [1, 2]. The teacher doesn't fully know the learner's internal state but tries to optimize the student's performance by providing demonstrations. Unlike earlier works, this paper also seeks to estimate the learner's transition function parameters (goals and observation function). Clarifying the task's formulation is crucial for understanding the technical value of the proposed approach, which remains unclear.\n\n2. The paper's objective is unclear. The authors claim, \"The goal of this work is to study whether learner-specific teachers who model the learner’s internal state are more efficient than learner-agnostic ones.\" Given the vague problem definition, it's unclear if this captures the paper's essence. Is the primary takeaway that Bayesian ToM-teachers excel at understanding human internal state changes?\n\n3. The evaluation appears lacking. The paper notes that the teacher has a general idea of the student's policy, and during testing, the student's actions are driven by a basic decision tree. This might mean there's minimal uncertainty in the learner's behavior. It would be beneficial to incorporate human-subject experiments to see real-world impact.\n\n4. Based on points from point 1 and equation 3, the teaching policy is essentially a greedy policy. Over time, this might not be the best strategy and could lead to suboptimal results. Without a well-defined teaching task, such a policy is not well justified.\n\n[1] Srivastava et al., Assistive Teaching of Motor Control Tasks to Humans, 2022.\n\n[2] Yu et al., COACH: Cooperative Robot Teaching, 2022.\n\n\n**update after rebuttal:**\n\nI agree with Reviewer C7a4's comment regarding the absence of related works in the submission. To appropriately position this work in the context of existing literature, some major changes are needed in the manuscript. Therefore, I am maintaining my current rating.\n\nQuestions: see weaknesses", "labeling_timestamp": "2026-01-11T17:17:03.188069", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided excerpt does not include Equation 3 nor the teacher policy description needed to assess whether the teaching policy is essentially greedy. The methods section contains placeholders for formulas (\"<!-- formula-not-decoded -->\") and the Teacher subsection is truncated, so there is insufficient information in the supplied text to confirm or contradict the claim.", "evidence": "\"<!-- formula-not-decoded -->\" (appears twice in section 3.2). \n\"## 3.3 TEACHER\n\nWe introduce an agent called teacher whose aim is to optimally help the learner maximise its reward on a GC-POMDP M demo = ( S demo , A , T demo , G , R demo ) by providing\"", "section": "3.2 LEARNER and 3.3 TEACHER"}
{"claim": "A greedy teaching policy may be suboptimal over time and is not justified without a well-defined teaching task.", "claim_type": "methodology", "paper_id": "WcSofkUVge", "paper_title": "Utility-based Adaptive Teaching Strategies using Bayesian Theory of Mind", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "RYHtZ3asnL", "reviewer": "Reviewer_3FPZ", "review_text": "Summary: The paper presents a method for adaptive teaching based on utility theory. The authors propose a model in which the teacher's actions are determined by the expected utility of each teaching action. This utility-based framework aims to adapt teaching strategies according to students' needs and progress. The paper also details the model's implementation in a educational setting and provides empirical evidence of its effectiveness.\n\nStrengths: 1. Integrating utility theory into adaptive teaching is a novel approach.\n2. Beyond estimating a student's skill level, the paper also aims to estimate the learner's internal goal, offering finer-grained teaching.\n3. The paper outlines the model's mathematical foundation, ensuring replicability for other researchers.\n4. The results have potential applications in personalized learning, AI-driven educational platforms, and adaptive curriculum design.\n\nWeaknesses: 1. The paper's formulation is ambiguous.\n\n    1. The decision to frame the learning task as a POMDP lacks clear justification. While this relates to the proposed method, more concrete examples are needed for justification.\n\n    2. The task formulation for the ToM-teacher isn't clearly presented in the manuscript. From my perspective, the teaching task resembles a POMDP, akin to references [1, 2]. The teacher doesn't fully know the learner's internal state but tries to optimize the student's performance by providing demonstrations. Unlike earlier works, this paper also seeks to estimate the learner's transition function parameters (goals and observation function). Clarifying the task's formulation is crucial for understanding the technical value of the proposed approach, which remains unclear.\n\n2. The paper's objective is unclear. The authors claim, \"The goal of this work is to study whether learner-specific teachers who model the learner’s internal state are more efficient than learner-agnostic ones.\" Given the vague problem definition, it's unclear if this captures the paper's essence. Is the primary takeaway that Bayesian ToM-teachers excel at understanding human internal state changes?\n\n3. The evaluation appears lacking. The paper notes that the teacher has a general idea of the student's policy, and during testing, the student's actions are driven by a basic decision tree. This might mean there's minimal uncertainty in the learner's behavior. It would be beneficial to incorporate human-subject experiments to see real-world impact.\n\n4. Based on points from point 1 and equation 3, the teaching policy is essentially a greedy policy. Over time, this might not be the best strategy and could lead to suboptimal results. Without a well-defined teaching task, such a policy is not well justified.\n\n[1] Srivastava et al., Assistive Teaching of Motor Control Tasks to Humans, 2022.\n\n[2] Yu et al., COACH: Cooperative Robot Teaching, 2022.\n\n\n**update after rebuttal:**\n\nI agree with Reviewer C7a4's comment regarding the absence of related works in the submission. To appropriately position this work in the context of existing literature, some major changes are needed in the manuscript. Therefore, I am maintaining my current rating.\n\nQuestions: see weaknesses", "labeling_timestamp": "2026-01-11T17:17:16.531235", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper defines a utility-based teaching task (reward minus cost) and studies adaptive (ToM-equipped) versus learner-agnostic strategies, but it does not discuss or analyze a 'greedy teaching policy' or explicitly claim that greedy policies are suboptimal over time or 'not justified' without a well-defined task. Thus the specific claim about greedy policies is not addressed.", "evidence": "\"The demonstration utility is optimal if it contains the necessary and sufficient amount of information for the learner to reach its goal.\" \"...we further find, on the other hand, that some learner-agnostic teaching strategies outperform ToM-teachers when inaccurate prior of the learner's policy and/or limited observation of its behaviour are available.\" ", "section": "Introduction"}
{"claim": "The manuscript omits important related works and lacks proper positioning within existing literature.", "claim_type": "subjective", "paper_id": "WcSofkUVge", "paper_title": "Utility-based Adaptive Teaching Strategies using Bayesian Theory of Mind", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "RYHtZ3asnL", "reviewer": "Reviewer_3FPZ", "review_text": "Summary: The paper presents a method for adaptive teaching based on utility theory. The authors propose a model in which the teacher's actions are determined by the expected utility of each teaching action. This utility-based framework aims to adapt teaching strategies according to students' needs and progress. The paper also details the model's implementation in a educational setting and provides empirical evidence of its effectiveness.\n\nStrengths: 1. Integrating utility theory into adaptive teaching is a novel approach.\n2. Beyond estimating a student's skill level, the paper also aims to estimate the learner's internal goal, offering finer-grained teaching.\n3. The paper outlines the model's mathematical foundation, ensuring replicability for other researchers.\n4. The results have potential applications in personalized learning, AI-driven educational platforms, and adaptive curriculum design.\n\nWeaknesses: 1. The paper's formulation is ambiguous.\n\n    1. The decision to frame the learning task as a POMDP lacks clear justification. While this relates to the proposed method, more concrete examples are needed for justification.\n\n    2. The task formulation for the ToM-teacher isn't clearly presented in the manuscript. From my perspective, the teaching task resembles a POMDP, akin to references [1, 2]. The teacher doesn't fully know the learner's internal state but tries to optimize the student's performance by providing demonstrations. Unlike earlier works, this paper also seeks to estimate the learner's transition function parameters (goals and observation function). Clarifying the task's formulation is crucial for understanding the technical value of the proposed approach, which remains unclear.\n\n2. The paper's objective is unclear. The authors claim, \"The goal of this work is to study whether learner-specific teachers who model the learner’s internal state are more efficient than learner-agnostic ones.\" Given the vague problem definition, it's unclear if this captures the paper's essence. Is the primary takeaway that Bayesian ToM-teachers excel at understanding human internal state changes?\n\n3. The evaluation appears lacking. The paper notes that the teacher has a general idea of the student's policy, and during testing, the student's actions are driven by a basic decision tree. This might mean there's minimal uncertainty in the learner's behavior. It would be beneficial to incorporate human-subject experiments to see real-world impact.\n\n4. Based on points from point 1 and equation 3, the teaching policy is essentially a greedy policy. Over time, this might not be the best strategy and could lead to suboptimal results. Without a well-defined teaching task, such a policy is not well justified.\n\n[1] Srivastava et al., Assistive Teaching of Motor Control Tasks to Humans, 2022.\n\n[2] Yu et al., COACH: Cooperative Robot Teaching, 2022.\n\n\n**update after rebuttal:**\n\nI agree with Reviewer C7a4's comment regarding the absence of related works in the submission. To appropriately position this work in the context of existing literature, some major changes are needed in the manuscript. Therefore, I am maintaining my current rating.\n\nQuestions: see weaknesses", "labeling_timestamp": "2026-01-11T17:17:16.779386", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper contains a dedicated Related Work section (Section 2) that cites prior cognitive science, Theory-of-Mind, machine teaching, and Bayesian inference literature and explicitly positions the contribution relative to these works, claiming novelty in leveraging ToM for teacher demonstration selection.", "evidence": "“In addition to cognitive science researches on human pedagogy (Shafto et al., 2014; Gweon, 2021; Ho et al., 2021), this work is related to the following interconnected research areas: Theory of Mind (ToM): ... Machine teaching: ... Unlike these prior studies, our teacher assists various learners with different goals and sensory capacities, and thus different optimal demonstrations. Previous studies have demonstrated the benefits of adaptivity in sequential machine teaching (Chen et al., 2018) and motor control (Srivastava et al., 2022) for learning. Unlike this prior research, we introduce a model of ToM explicitly modeling the learner's mental state as a pivotal component of our teacher's adaptivity.”", "section": "2 RELATED WORK"}
{"claim": "The paper does not include comparisons with prior methods such as Srivastava et al. and Yu et al.", "claim_type": "baseline", "paper_id": "WcSofkUVge", "paper_title": "Utility-based Adaptive Teaching Strategies using Bayesian Theory of Mind", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "RYHtZ3asnL", "reviewer": "Reviewer_3FPZ", "review_text": "Summary: The paper presents a method for adaptive teaching based on utility theory. The authors propose a model in which the teacher's actions are determined by the expected utility of each teaching action. This utility-based framework aims to adapt teaching strategies according to students' needs and progress. The paper also details the model's implementation in a educational setting and provides empirical evidence of its effectiveness.\n\nStrengths: 1. Integrating utility theory into adaptive teaching is a novel approach.\n2. Beyond estimating a student's skill level, the paper also aims to estimate the learner's internal goal, offering finer-grained teaching.\n3. The paper outlines the model's mathematical foundation, ensuring replicability for other researchers.\n4. The results have potential applications in personalized learning, AI-driven educational platforms, and adaptive curriculum design.\n\nWeaknesses: 1. The paper's formulation is ambiguous.\n\n    1. The decision to frame the learning task as a POMDP lacks clear justification. While this relates to the proposed method, more concrete examples are needed for justification.\n\n    2. The task formulation for the ToM-teacher isn't clearly presented in the manuscript. From my perspective, the teaching task resembles a POMDP, akin to references [1, 2]. The teacher doesn't fully know the learner's internal state but tries to optimize the student's performance by providing demonstrations. Unlike earlier works, this paper also seeks to estimate the learner's transition function parameters (goals and observation function). Clarifying the task's formulation is crucial for understanding the technical value of the proposed approach, which remains unclear.\n\n2. The paper's objective is unclear. The authors claim, \"The goal of this work is to study whether learner-specific teachers who model the learner’s internal state are more efficient than learner-agnostic ones.\" Given the vague problem definition, it's unclear if this captures the paper's essence. Is the primary takeaway that Bayesian ToM-teachers excel at understanding human internal state changes?\n\n3. The evaluation appears lacking. The paper notes that the teacher has a general idea of the student's policy, and during testing, the student's actions are driven by a basic decision tree. This might mean there's minimal uncertainty in the learner's behavior. It would be beneficial to incorporate human-subject experiments to see real-world impact.\n\n4. Based on points from point 1 and equation 3, the teaching policy is essentially a greedy policy. Over time, this might not be the best strategy and could lead to suboptimal results. Without a well-defined teaching task, such a policy is not well justified.\n\n[1] Srivastava et al., Assistive Teaching of Motor Control Tasks to Humans, 2022.\n\n[2] Yu et al., COACH: Cooperative Robot Teaching, 2022.\n\n\n**update after rebuttal:**\n\nI agree with Reviewer C7a4's comment regarding the absence of related works in the submission. To appropriately position this work in the context of existing literature, some major changes are needed in the manuscript. Therefore, I am maintaining my current rating.\n\nQuestions: see weaknesses", "labeling_timestamp": "2026-01-11T17:17:28.305586", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper cites Srivastava et al. in Related Work but does not present any empirical or experimental comparison to that prior method; Yu et al. is not mentioned anywhere in the provided content, so no comparison is included.", "evidence": "\"Previous studies have demonstrated the benefits of adaptivity in sequential machine teaching (Chen et al., 2018) and motor control (Srivastava et al., 2022) for learning. Unlike this prior research, we introduce a model of ToM explicitly modeling the learner's mental state as a pivotal component of our teacher's adaptivity.\"", "section": "2 RELATED WORK"}
{"claim": "The authors assume the teacher has a general idea of the student's policy, but the realism of this assumption is not justified.", "claim_type": "subjective", "paper_id": "WcSofkUVge", "paper_title": "Utility-based Adaptive Teaching Strategies using Bayesian Theory of Mind", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "RYHtZ3asnL", "reviewer": "Reviewer_3FPZ", "review_text": "Summary: The paper presents a method for adaptive teaching based on utility theory. The authors propose a model in which the teacher's actions are determined by the expected utility of each teaching action. This utility-based framework aims to adapt teaching strategies according to students' needs and progress. The paper also details the model's implementation in a educational setting and provides empirical evidence of its effectiveness.\n\nStrengths: 1. Integrating utility theory into adaptive teaching is a novel approach.\n2. Beyond estimating a student's skill level, the paper also aims to estimate the learner's internal goal, offering finer-grained teaching.\n3. The paper outlines the model's mathematical foundation, ensuring replicability for other researchers.\n4. The results have potential applications in personalized learning, AI-driven educational platforms, and adaptive curriculum design.\n\nWeaknesses: 1. The paper's formulation is ambiguous.\n\n    1. The decision to frame the learning task as a POMDP lacks clear justification. While this relates to the proposed method, more concrete examples are needed for justification.\n\n    2. The task formulation for the ToM-teacher isn't clearly presented in the manuscript. From my perspective, the teaching task resembles a POMDP, akin to references [1, 2]. The teacher doesn't fully know the learner's internal state but tries to optimize the student's performance by providing demonstrations. Unlike earlier works, this paper also seeks to estimate the learner's transition function parameters (goals and observation function). Clarifying the task's formulation is crucial for understanding the technical value of the proposed approach, which remains unclear.\n\n2. The paper's objective is unclear. The authors claim, \"The goal of this work is to study whether learner-specific teachers who model the learner’s internal state are more efficient than learner-agnostic ones.\" Given the vague problem definition, it's unclear if this captures the paper's essence. Is the primary takeaway that Bayesian ToM-teachers excel at understanding human internal state changes?\n\n3. The evaluation appears lacking. The paper notes that the teacher has a general idea of the student's policy, and during testing, the student's actions are driven by a basic decision tree. This might mean there's minimal uncertainty in the learner's behavior. It would be beneficial to incorporate human-subject experiments to see real-world impact.\n\n4. Based on points from point 1 and equation 3, the teaching policy is essentially a greedy policy. Over time, this might not be the best strategy and could lead to suboptimal results. Without a well-defined teaching task, such a policy is not well justified.\n\n[1] Srivastava et al., Assistive Teaching of Motor Control Tasks to Humans, 2022.\n\n[2] Yu et al., COACH: Cooperative Robot Teaching, 2022.\n\n\n**update after rebuttal:**\n\nI agree with Reviewer C7a4's comment regarding the absence of related works in the submission. To appropriately position this work in the context of existing literature, some major changes are needed in the manuscript. Therefore, I am maintaining my current rating.\n\nQuestions: see weaknesses", "labeling_timestamp": "2026-01-11T17:17:36.922447", "model": "gpt-5-mini", "label": "Supported", "justification": "Section 3.2 explicitly assumes all learners share a single policy (π), which implies the teacher has a model of the learner's policy; the paper does not provide an argument or empirical justification for the realism of this assumption.", "evidence": "“In practice, the learners use a single policy denoted π for all the considered GC-POMDPs.”; “To simulate a trajectory τ_i of learner L_i on M_j, one only needs to know the tuple (π_j, g_i, v_i, b_i,j^0).”", "section": "3.2 LEARNER"}
{"claim": "The paper claims to estimate learner transition and observation parameters but does not clearly explain how the learner's internal goal is estimated.", "claim_type": "methodology", "paper_id": "WcSofkUVge", "paper_title": "Utility-based Adaptive Teaching Strategies using Bayesian Theory of Mind", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "RYHtZ3asnL", "reviewer": "Reviewer_3FPZ", "review_text": "Summary: The paper presents a method for adaptive teaching based on utility theory. The authors propose a model in which the teacher's actions are determined by the expected utility of each teaching action. This utility-based framework aims to adapt teaching strategies according to students' needs and progress. The paper also details the model's implementation in a educational setting and provides empirical evidence of its effectiveness.\n\nStrengths: 1. Integrating utility theory into adaptive teaching is a novel approach.\n2. Beyond estimating a student's skill level, the paper also aims to estimate the learner's internal goal, offering finer-grained teaching.\n3. The paper outlines the model's mathematical foundation, ensuring replicability for other researchers.\n4. The results have potential applications in personalized learning, AI-driven educational platforms, and adaptive curriculum design.\n\nWeaknesses: 1. The paper's formulation is ambiguous.\n\n    1. The decision to frame the learning task as a POMDP lacks clear justification. While this relates to the proposed method, more concrete examples are needed for justification.\n\n    2. The task formulation for the ToM-teacher isn't clearly presented in the manuscript. From my perspective, the teaching task resembles a POMDP, akin to references [1, 2]. The teacher doesn't fully know the learner's internal state but tries to optimize the student's performance by providing demonstrations. Unlike earlier works, this paper also seeks to estimate the learner's transition function parameters (goals and observation function). Clarifying the task's formulation is crucial for understanding the technical value of the proposed approach, which remains unclear.\n\n2. The paper's objective is unclear. The authors claim, \"The goal of this work is to study whether learner-specific teachers who model the learner’s internal state are more efficient than learner-agnostic ones.\" Given the vague problem definition, it's unclear if this captures the paper's essence. Is the primary takeaway that Bayesian ToM-teachers excel at understanding human internal state changes?\n\n3. The evaluation appears lacking. The paper notes that the teacher has a general idea of the student's policy, and during testing, the student's actions are driven by a basic decision tree. This might mean there's minimal uncertainty in the learner's behavior. It would be beneficial to incorporate human-subject experiments to see real-world impact.\n\n4. Based on points from point 1 and equation 3, the teaching policy is essentially a greedy policy. Over time, this might not be the best strategy and could lead to suboptimal results. Without a well-defined teaching task, such a policy is not well justified.\n\n[1] Srivastava et al., Assistive Teaching of Motor Control Tasks to Humans, 2022.\n\n[2] Yu et al., COACH: Cooperative Robot Teaching, 2022.\n\n\n**update after rebuttal:**\n\nI agree with Reviewer C7a4's comment regarding the absence of related works in the submission. To appropriately position this work in the context of existing literature, some major changes are needed in the manuscript. Therefore, I am maintaining my current rating.\n\nQuestions: see weaknesses", "labeling_timestamp": "2026-01-11T17:18:07.430099", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly states that the teacher infers the learner's internal state (including goal and sensory capacity) via Bayesian inference from past behaviour (Introduction and ToM description). However, the provided Methods section does not present a clear, explicit procedure or decoded equations showing how the learner's goal posterior is computed (key formula blocks are missing/undecoded), so the mechanics of goal estimation are not clearly explained in the available text.", "evidence": "1) \"ToM-teachers able to 1. update a belief about the internal state (i.e. goal, intention, belief, sensory capacity) of an unknown learner through Bayesian inference based on observations of its behaviour in a simple environment, see Figure 1(A), and\" (Introduction).\n\n2) \"the teacher ignores the goal of the learner and its sensory capacity, but can infer them from a past trajectory of the learner in a simpler environment.\" (Introduction).\n\n3) \"To simulate a trajectory τ_i of learner L_i on M_j, one only needs to know the tuple (π_j, g_i, v_i, b_{i,j}^0).\" (Methods, 3.2 LEARNER).\n\n4) \"<!-- formula-not-decoded -->\" (appears in Methods where update/inference formulas would be expected), indicating missing/undecoded explicit equations.", "section": "Introduction; Methods (3.2 LEARNER)"}
{"claim": "The evaluation setup may minimize learner behavior uncertainty, limiting the evaluation's realism and generalizability.", "claim_type": "experimental", "paper_id": "WcSofkUVge", "paper_title": "Utility-based Adaptive Teaching Strategies using Bayesian Theory of Mind", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "RYHtZ3asnL", "reviewer": "Reviewer_3FPZ", "review_text": "Summary: The paper presents a method for adaptive teaching based on utility theory. The authors propose a model in which the teacher's actions are determined by the expected utility of each teaching action. This utility-based framework aims to adapt teaching strategies according to students' needs and progress. The paper also details the model's implementation in a educational setting and provides empirical evidence of its effectiveness.\n\nStrengths: 1. Integrating utility theory into adaptive teaching is a novel approach.\n2. Beyond estimating a student's skill level, the paper also aims to estimate the learner's internal goal, offering finer-grained teaching.\n3. The paper outlines the model's mathematical foundation, ensuring replicability for other researchers.\n4. The results have potential applications in personalized learning, AI-driven educational platforms, and adaptive curriculum design.\n\nWeaknesses: 1. The paper's formulation is ambiguous.\n\n    1. The decision to frame the learning task as a POMDP lacks clear justification. While this relates to the proposed method, more concrete examples are needed for justification.\n\n    2. The task formulation for the ToM-teacher isn't clearly presented in the manuscript. From my perspective, the teaching task resembles a POMDP, akin to references [1, 2]. The teacher doesn't fully know the learner's internal state but tries to optimize the student's performance by providing demonstrations. Unlike earlier works, this paper also seeks to estimate the learner's transition function parameters (goals and observation function). Clarifying the task's formulation is crucial for understanding the technical value of the proposed approach, which remains unclear.\n\n2. The paper's objective is unclear. The authors claim, \"The goal of this work is to study whether learner-specific teachers who model the learner’s internal state are more efficient than learner-agnostic ones.\" Given the vague problem definition, it's unclear if this captures the paper's essence. Is the primary takeaway that Bayesian ToM-teachers excel at understanding human internal state changes?\n\n3. The evaluation appears lacking. The paper notes that the teacher has a general idea of the student's policy, and during testing, the student's actions are driven by a basic decision tree. This might mean there's minimal uncertainty in the learner's behavior. It would be beneficial to incorporate human-subject experiments to see real-world impact.\n\n4. Based on points from point 1 and equation 3, the teaching policy is essentially a greedy policy. Over time, this might not be the best strategy and could lead to suboptimal results. Without a well-defined teaching task, such a policy is not well justified.\n\n[1] Srivastava et al., Assistive Teaching of Motor Control Tasks to Humans, 2022.\n\n[2] Yu et al., COACH: Cooperative Robot Teaching, 2022.\n\n\n**update after rebuttal:**\n\nI agree with Reviewer C7a4's comment regarding the absence of related works in the submission. To appropriately position this work in the context of existing literature, some major changes are needed in the manuscript. Therefore, I am maintaining my current rating.\n\nQuestions: see weaknesses", "labeling_timestamp": "2026-01-11T17:17:49.222215", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper makes simplifying assumptions that reduce variability in learner behavior (finite family of learners, a single shared policy, uniform initial beliefs, and that trajectories are simulated from a known tuple), which can minimize learner-behavior uncertainty and thus limit realism/generalizability of the evaluation.", "evidence": "We consider a finite family of agents L = { L i , i ∈ I } that we call learners . A learner L i is defined by a goal g i ∈ G and an observation function v i , i.e. L i = ( g i , v i ).\n\nUnless mentioned otherwise, we assume that the learner's initial belief b i,j 0 on the state of M j is uniform over the set of possible states S j B . In the experiments presented below, we additionally assume that all learners share a policy on the environment M j conditioned by a goal, an observation function and a belief:\n\nTo simulate a trajectory τ i of learner L i on M j , one only needs to know the tuple ( π j , g i , v i , b i,j 0 ). In practice, the learners use a single policy denoted π for all the considered GC-POMDPs.", "section": "3.2 LEARNER"}
{"claim": "The paper does not discuss alternative label integration methods for crowdsourcing nor clarify that no such alternatives exist.", "claim_type": "subjective", "paper_id": "wnPlJNiqfA", "paper_title": "KFNN: K-Free Nearest Neighbor For Crowdsourcing", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "RmGxXHTEuK", "reviewer": "Reviewer_MQSy", "review_text": "Comment: Dear authors,\n\nI have read the rebuttal, and thank you for considering my suggestions. Some of my concerns are well addressed, and the others aren't:\n\n(1) Q1 asks whether there are alternatives for label integration that can be used in crowdsourcing. If so, please discuss. Otherwise, please clarify that there are no such alternatives.\n\n(2) Q2: thank authors for acknowledging that these two terms are synonymous. That's why I have asked whether there are alternatives for the studied problem. As reviewed in the seminal survey work published in [R3], there are other options for this problem. Based on this reason, I have asked the authors to enhance the motivation of using KNN-based method instead of the others in Q1.\n\n(3) Q3 is generally satisfactory.\n\n(4) The claims for Q4 should be verified through experimental evaluations.\n\n(5) Q5 asks to clarify the type of crowdsourced task. Notice that, a ranking task in crowdsourcing still requires label integration (or ground truth inference). Thus, it is important to clearly define the application scope. From the current form of submission, the proposed method might be limited to only limited tasks types instead of general tasks in crowdsourcing. If this is the fact, the title and some other contents may need to be refined to avoid over-claiming.\n\n(6) Q6: as shown in the experimental results (Tables 1 and 2), the proposed method could perform worse than the selected baselines in either Macro-F1 score and integration accuracy under certain datasets by a large margin. Based on the current results, it seems that (1) different methods have their own pros and cons, and (2) the proposed method is not always the optimal among the compared ones.\n\n(7) Q7: thank you for acknowledging that the proposed method has not been verified by a real-world platform like AMT. Existing studies on crowdsourcing often conduct evaluations on a real-world platform, such as CrowdFlower (see Section 5.2 in [R2]), since real-world scenarios are more complex than a simulated experiment. Besides, the evaluation can be conducted under the double blind policy. Based on the current evaluations, it is unclear whether the proposed method can be effectively integrated in a real-world platform or not.\n\nOverall, I appreciate the efforts made in the rebuttal. I will take your responses into consideration when making my final decision.\n\nBest regards,", "labeling_timestamp": "2026-01-11T17:17:40.478167", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper's Related Work section explicitly discusses many alternative label integration methods (both non-neighbor-based and neighbor-based), so the claim that it does not discuss alternatives or clarify their nonexistence is false.", "evidence": "Depending on whether neighbor instances are leveraged or not, existing label integration algorithms can be divided into two categories. The first category of algorithms does not leverage neighbor instances... For example, [17] models the ability of each worker with a confusion matrix... The second category of algorithms performs label integration by leveraging the information from neighbor instances obtained by the KNN algorithm. For example, [13] proposes to use the labels assigned to the neighbor instances of an instance to augment this instance's multiple noisy labels...", "section": "2 Related work"}
{"claim": "The paper lacks motivation and justification for choosing a KNN-based method over other established label integration approaches reported in prior surveys.", "claim_type": "subjective", "paper_id": "wnPlJNiqfA", "paper_title": "KFNN: K-Free Nearest Neighbor For Crowdsourcing", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "RmGxXHTEuK", "reviewer": "Reviewer_MQSy", "review_text": "Comment: Dear authors,\n\nI have read the rebuttal, and thank you for considering my suggestions. Some of my concerns are well addressed, and the others aren't:\n\n(1) Q1 asks whether there are alternatives for label integration that can be used in crowdsourcing. If so, please discuss. Otherwise, please clarify that there are no such alternatives.\n\n(2) Q2: thank authors for acknowledging that these two terms are synonymous. That's why I have asked whether there are alternatives for the studied problem. As reviewed in the seminal survey work published in [R3], there are other options for this problem. Based on this reason, I have asked the authors to enhance the motivation of using KNN-based method instead of the others in Q1.\n\n(3) Q3 is generally satisfactory.\n\n(4) The claims for Q4 should be verified through experimental evaluations.\n\n(5) Q5 asks to clarify the type of crowdsourced task. Notice that, a ranking task in crowdsourcing still requires label integration (or ground truth inference). Thus, it is important to clearly define the application scope. From the current form of submission, the proposed method might be limited to only limited tasks types instead of general tasks in crowdsourcing. If this is the fact, the title and some other contents may need to be refined to avoid over-claiming.\n\n(6) Q6: as shown in the experimental results (Tables 1 and 2), the proposed method could perform worse than the selected baselines in either Macro-F1 score and integration accuracy under certain datasets by a large margin. Based on the current results, it seems that (1) different methods have their own pros and cons, and (2) the proposed method is not always the optimal among the compared ones.\n\n(7) Q7: thank you for acknowledging that the proposed method has not been verified by a real-world platform like AMT. Existing studies on crowdsourcing often conduct evaluations on a real-world platform, such as CrowdFlower (see Section 5.2 in [R2]), since real-world scenarios are more complex than a simulated experiment. Besides, the evaluation can be conducted under the double blind policy. Based on the current evaluations, it is unclear whether the proposed method can be effectively integrated in a real-world platform or not.\n\nOverall, I appreciate the efforts made in the rebuttal. I will take your responses into consideration when making my final decision.\n\nBest regards,", "labeling_timestamp": "2026-01-11T17:17:55.789380", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly motivates using neighbor-based (KNN) methods because limited noisy labels per instance hinder label integration, and prior work and theoretical/experimental results show leveraging neighbors improves performance; it then justifies improving KNN by removing the fixed-K assumption. Thus it does provide motivation and justification for a KNN-based approach.", "evidence": "1) \"Recent works have shown that leveraging neighbor instances can help alleviate this problem.\" (Abstract). 2) \"To alleviate this problem, recent works have begun to focus on leveraging neighbor instances [13, 14, 1]. These works successfully improve the performance of label integration by leveraging the information from neighbor instances obtained by the K-nearest neighbor (KNN) algorithm. However, due to the use of KNN, these algorithms all assume that each instance has the same neighborhood size. This assumption is difficult to hold because it defies common sense...\" (Introduction). 3) \"While simpler and more efficient, the first category of algorithms are limited in effectiveness because each instance can only obtain few noisy labels. Both experimental results and theoretical analysis demonstrate the effectiveness of the second category of algorithms in leveraging the information from neighbor instances. However, these algorithms all assume a fixed neighborhood size for each instance, which is often unrealistic and thus limits their performance. To further ensure that each instance has a free neighborhood size, this paper proposes a novel label integration algorithm called KFNN.\" (Related work).", "section": "Abstract, Introduction, Related work"}
{"claim": "Some specific claims (Q4) in the paper are not experimentally verified and require empirical evaluation to be supported.", "claim_type": "experimental", "paper_id": "wnPlJNiqfA", "paper_title": "KFNN: K-Free Nearest Neighbor For Crowdsourcing", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "RmGxXHTEuK", "reviewer": "Reviewer_MQSy", "review_text": "Comment: Dear authors,\n\nI have read the rebuttal, and thank you for considering my suggestions. Some of my concerns are well addressed, and the others aren't:\n\n(1) Q1 asks whether there are alternatives for label integration that can be used in crowdsourcing. If so, please discuss. Otherwise, please clarify that there are no such alternatives.\n\n(2) Q2: thank authors for acknowledging that these two terms are synonymous. That's why I have asked whether there are alternatives for the studied problem. As reviewed in the seminal survey work published in [R3], there are other options for this problem. Based on this reason, I have asked the authors to enhance the motivation of using KNN-based method instead of the others in Q1.\n\n(3) Q3 is generally satisfactory.\n\n(4) The claims for Q4 should be verified through experimental evaluations.\n\n(5) Q5 asks to clarify the type of crowdsourced task. Notice that, a ranking task in crowdsourcing still requires label integration (or ground truth inference). Thus, it is important to clearly define the application scope. From the current form of submission, the proposed method might be limited to only limited tasks types instead of general tasks in crowdsourcing. If this is the fact, the title and some other contents may need to be refined to avoid over-claiming.\n\n(6) Q6: as shown in the experimental results (Tables 1 and 2), the proposed method could perform worse than the selected baselines in either Macro-F1 score and integration accuracy under certain datasets by a large margin. Based on the current results, it seems that (1) different methods have their own pros and cons, and (2) the proposed method is not always the optimal among the compared ones.\n\n(7) Q7: thank you for acknowledging that the proposed method has not been verified by a real-world platform like AMT. Existing studies on crowdsourcing often conduct evaluations on a real-world platform, such as CrowdFlower (see Section 5.2 in [R2]), since real-world scenarios are more complex than a simulated experiment. Besides, the evaluation can be conducted under the double blind policy. Based on the current evaluations, it is unclear whether the proposed method can be effectively integrated in a real-world platform or not.\n\nOverall, I appreciate the efforts made in the rebuttal. I will take your responses into consideration when making my final decision.\n\nBest regards,", "labeling_timestamp": "2026-01-11T17:17:58.236215", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt claims that extensive experimental results validate KFNN (Abstract, Contributions), but the excerpt does not include the experimental section or any description of 'Q4' or which specific claims are meant. Because the experimental results and the specific 'Q4' claims are not present in the supplied content, it is impossible to verify whether those specific claims were experimentally evaluated.", "evidence": "\"Extensive experimental results demonstrate that KFNN significantly outperforms all the other state-of-the-art algorithms and exhibits greater robustness in various crowdsourcing scenarios.\"", "section": "Abstract"}
{"claim": "The paper fails to clearly define which type(s) of crowdsourcing tasks the proposed method targets.", "claim_type": "methodology", "paper_id": "wnPlJNiqfA", "paper_title": "KFNN: K-Free Nearest Neighbor For Crowdsourcing", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "RmGxXHTEuK", "reviewer": "Reviewer_MQSy", "review_text": "Comment: Dear authors,\n\nI have read the rebuttal, and thank you for considering my suggestions. Some of my concerns are well addressed, and the others aren't:\n\n(1) Q1 asks whether there are alternatives for label integration that can be used in crowdsourcing. If so, please discuss. Otherwise, please clarify that there are no such alternatives.\n\n(2) Q2: thank authors for acknowledging that these two terms are synonymous. That's why I have asked whether there are alternatives for the studied problem. As reviewed in the seminal survey work published in [R3], there are other options for this problem. Based on this reason, I have asked the authors to enhance the motivation of using KNN-based method instead of the others in Q1.\n\n(3) Q3 is generally satisfactory.\n\n(4) The claims for Q4 should be verified through experimental evaluations.\n\n(5) Q5 asks to clarify the type of crowdsourced task. Notice that, a ranking task in crowdsourcing still requires label integration (or ground truth inference). Thus, it is important to clearly define the application scope. From the current form of submission, the proposed method might be limited to only limited tasks types instead of general tasks in crowdsourcing. If this is the fact, the title and some other contents may need to be refined to avoid over-claiming.\n\n(6) Q6: as shown in the experimental results (Tables 1 and 2), the proposed method could perform worse than the selected baselines in either Macro-F1 score and integration accuracy under certain datasets by a large margin. Based on the current results, it seems that (1) different methods have their own pros and cons, and (2) the proposed method is not always the optimal among the compared ones.\n\n(7) Q7: thank you for acknowledging that the proposed method has not been verified by a real-world platform like AMT. Existing studies on crowdsourcing often conduct evaluations on a real-world platform, such as CrowdFlower (see Section 5.2 in [R2]), since real-world scenarios are more complex than a simulated experiment. Besides, the evaluation can be conducted under the double blind policy. Based on the current evaluations, it is unclear whether the proposed method can be effectively integrated in a real-world platform or not.\n\nOverall, I appreciate the efforts made in the rebuttal. I will take your responses into consideration when making my final decision.\n\nBest regards,", "labeling_timestamp": "2026-01-11T17:18:05.544640", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly frames the problem as crowdsourced label integration for classification: it defines multiple noisy labels drawn from a fixed set of class labels and discusses both binary and multi-class tasks. Thus it does specify the type of crowdsourcing tasks targeted (classification with repeated noisy annotations).", "evidence": "L_i denotes multiple noisy labels of x_i, which can be expressed as { l_ir }_{r=1}^R. R is the number of workers and l_ir denotes the label of x_i annotated by the r-th worker u_r. l_ir takes a value from a fixed set {-1, c_1, . . . , c_q, . . . , c_Q}, where Q is the number of classes, c_q denotes the q-th class and -1 indicates that u_r has not annotated x_i.", "section": "3.1 Preliminary"}
{"claim": "The proposed method may be applicable only to a limited set of task types, but the paper does not clarify these limitations.", "claim_type": "presentation", "paper_id": "wnPlJNiqfA", "paper_title": "KFNN: K-Free Nearest Neighbor For Crowdsourcing", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "RmGxXHTEuK", "reviewer": "Reviewer_MQSy", "review_text": "Comment: Dear authors,\n\nI have read the rebuttal, and thank you for considering my suggestions. Some of my concerns are well addressed, and the others aren't:\n\n(1) Q1 asks whether there are alternatives for label integration that can be used in crowdsourcing. If so, please discuss. Otherwise, please clarify that there are no such alternatives.\n\n(2) Q2: thank authors for acknowledging that these two terms are synonymous. That's why I have asked whether there are alternatives for the studied problem. As reviewed in the seminal survey work published in [R3], there are other options for this problem. Based on this reason, I have asked the authors to enhance the motivation of using KNN-based method instead of the others in Q1.\n\n(3) Q3 is generally satisfactory.\n\n(4) The claims for Q4 should be verified through experimental evaluations.\n\n(5) Q5 asks to clarify the type of crowdsourced task. Notice that, a ranking task in crowdsourcing still requires label integration (or ground truth inference). Thus, it is important to clearly define the application scope. From the current form of submission, the proposed method might be limited to only limited tasks types instead of general tasks in crowdsourcing. If this is the fact, the title and some other contents may need to be refined to avoid over-claiming.\n\n(6) Q6: as shown in the experimental results (Tables 1 and 2), the proposed method could perform worse than the selected baselines in either Macro-F1 score and integration accuracy under certain datasets by a large margin. Based on the current results, it seems that (1) different methods have their own pros and cons, and (2) the proposed method is not always the optimal among the compared ones.\n\n(7) Q7: thank you for acknowledging that the proposed method has not been verified by a real-world platform like AMT. Existing studies on crowdsourcing often conduct evaluations on a real-world platform, such as CrowdFlower (see Section 5.2 in [R2]), since real-world scenarios are more complex than a simulated experiment. Besides, the evaluation can be conducted under the double blind policy. Based on the current evaluations, it is unclear whether the proposed method can be effectively integrated in a real-world platform or not.\n\nOverall, I appreciate the efforts made in the rebuttal. I will take your responses into consideration when making my final decision.\n\nBest regards,", "labeling_timestamp": "2026-01-11T17:18:19.935094", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper consistently frames KFNN as operating on instances with attribute vectors and multiple noisy labels and explicitly uses Mahalanobis distances in the attribute space, which implies the method requires attribute-based tasks and may not apply where such attributes are absent. However, the paper does not explicitly state or discuss these applicability limitations anywhere, so the reviewer’s concern about lack of clarification is correct while the claim that it 'may be applicable only to a limited set of task types' is an inference rather than an explicitly stated limitation.", "evidence": "\"In KFNN, the neighborhood size of each instance is automatically determined based on its attributes and noisy labels.\" (Abstract)\n\n\"KFNN initially estimates a Mahalanobis distance distribution from the attribute space to model the relationship between each instance and all classes.\" (Abstract)\n\n\"Let D = { ( x_i , L_i ) } denote a crowdsourced dataset... x_i can be represented as { x_im }_{m=1}^M. Here, M is the dimension of attributes... L_i denotes multiple noisy labels of x_i...\" (3.1 Preliminary)\n\n\"KFNN automatically determines the optimal neighborhood size for each instance based on its attributes and noisy labels...\" (Related work / Introduction)", "section": "Abstract; Introduction; 3.1 Preliminary"}
{"claim": "The paper's title and certain content risk over-claiming general applicability without refining scope based on task limitations.", "claim_type": "presentation", "paper_id": "wnPlJNiqfA", "paper_title": "KFNN: K-Free Nearest Neighbor For Crowdsourcing", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "RmGxXHTEuK", "reviewer": "Reviewer_MQSy", "review_text": "Comment: Dear authors,\n\nI have read the rebuttal, and thank you for considering my suggestions. Some of my concerns are well addressed, and the others aren't:\n\n(1) Q1 asks whether there are alternatives for label integration that can be used in crowdsourcing. If so, please discuss. Otherwise, please clarify that there are no such alternatives.\n\n(2) Q2: thank authors for acknowledging that these two terms are synonymous. That's why I have asked whether there are alternatives for the studied problem. As reviewed in the seminal survey work published in [R3], there are other options for this problem. Based on this reason, I have asked the authors to enhance the motivation of using KNN-based method instead of the others in Q1.\n\n(3) Q3 is generally satisfactory.\n\n(4) The claims for Q4 should be verified through experimental evaluations.\n\n(5) Q5 asks to clarify the type of crowdsourced task. Notice that, a ranking task in crowdsourcing still requires label integration (or ground truth inference). Thus, it is important to clearly define the application scope. From the current form of submission, the proposed method might be limited to only limited tasks types instead of general tasks in crowdsourcing. If this is the fact, the title and some other contents may need to be refined to avoid over-claiming.\n\n(6) Q6: as shown in the experimental results (Tables 1 and 2), the proposed method could perform worse than the selected baselines in either Macro-F1 score and integration accuracy under certain datasets by a large margin. Based on the current results, it seems that (1) different methods have their own pros and cons, and (2) the proposed method is not always the optimal among the compared ones.\n\n(7) Q7: thank you for acknowledging that the proposed method has not been verified by a real-world platform like AMT. Existing studies on crowdsourcing often conduct evaluations on a real-world platform, such as CrowdFlower (see Section 5.2 in [R2]), since real-world scenarios are more complex than a simulated experiment. Besides, the evaluation can be conducted under the double blind policy. Based on the current evaluations, it is unclear whether the proposed method can be effectively integrated in a real-world platform or not.\n\nOverall, I appreciate the efforts made in the rebuttal. I will take your responses into consideration when making my final decision.\n\nBest regards,", "labeling_timestamp": "2026-01-11T17:18:37.286529", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper's title is explicitly scoped to crowdsourcing (so it does not by itself overclaim general applicability), but the abstract and contributions make broad, unqualified claims that KFNN \"significantly outperforms all the other state-of-the-art algorithms\" and is robust in \"various crowdsourcing scenarios\" without detailing task limitations or caveats in the provided text. Thus the reviewer's concern is partly supported: the content makes broad claims without refinement, while the title itself is scoped.", "evidence": "Title: KFNN: K-Free Nearest Neighbor For Crowdsourcing\nAbstract: \"Extensive experimental results demonstrate that KFNN significantly outperforms all the other state-of-the-art algorithms and exhibits greater robustness in various crowdsourcing scenarios.\"\nIntroduction (Contributions): \"- Extensive experimental results demonstrate that KFNN significantly outperforms all the other state-of-the-art label integration algorithms and exhibits greater robustness than existing algorithms in various crowdsourcing scenarios.\"", "section": "Title; Abstract; Introduction (Contributions)"}
{"claim": "Experimental results show the proposed method performs worse than selected baselines on some datasets in Macro-F1 and integration accuracy by large margins.", "claim_type": "experimental", "paper_id": "wnPlJNiqfA", "paper_title": "KFNN: K-Free Nearest Neighbor For Crowdsourcing", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "RmGxXHTEuK", "reviewer": "Reviewer_MQSy", "review_text": "Comment: Dear authors,\n\nI have read the rebuttal, and thank you for considering my suggestions. Some of my concerns are well addressed, and the others aren't:\n\n(1) Q1 asks whether there are alternatives for label integration that can be used in crowdsourcing. If so, please discuss. Otherwise, please clarify that there are no such alternatives.\n\n(2) Q2: thank authors for acknowledging that these two terms are synonymous. That's why I have asked whether there are alternatives for the studied problem. As reviewed in the seminal survey work published in [R3], there are other options for this problem. Based on this reason, I have asked the authors to enhance the motivation of using KNN-based method instead of the others in Q1.\n\n(3) Q3 is generally satisfactory.\n\n(4) The claims for Q4 should be verified through experimental evaluations.\n\n(5) Q5 asks to clarify the type of crowdsourced task. Notice that, a ranking task in crowdsourcing still requires label integration (or ground truth inference). Thus, it is important to clearly define the application scope. From the current form of submission, the proposed method might be limited to only limited tasks types instead of general tasks in crowdsourcing. If this is the fact, the title and some other contents may need to be refined to avoid over-claiming.\n\n(6) Q6: as shown in the experimental results (Tables 1 and 2), the proposed method could perform worse than the selected baselines in either Macro-F1 score and integration accuracy under certain datasets by a large margin. Based on the current results, it seems that (1) different methods have their own pros and cons, and (2) the proposed method is not always the optimal among the compared ones.\n\n(7) Q7: thank you for acknowledging that the proposed method has not been verified by a real-world platform like AMT. Existing studies on crowdsourcing often conduct evaluations on a real-world platform, such as CrowdFlower (see Section 5.2 in [R2]), since real-world scenarios are more complex than a simulated experiment. Besides, the evaluation can be conducted under the double blind policy. Based on the current evaluations, it is unclear whether the proposed method can be effectively integrated in a real-world platform or not.\n\nOverall, I appreciate the efforts made in the rebuttal. I will take your responses into consideration when making my final decision.\n\nBest regards,", "labeling_timestamp": "2026-01-11T17:18:25.702053", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper claims that KFNN 'significantly outperforms all the other state-of-the-art algorithms' in experiments; it does not report experimental results showing KFNN performs worse than selected baselines on some datasets.", "evidence": "Extensive experimental results demonstrate that KFNN significantly outperforms all the other state-of-the-art algorithms and exhibits greater robustness in various crowdsourcing scenarios.", "section": "Abstract"}
{"claim": "The evaluation indicates the proposed method is not consistently the optimal choice across the compared methods.", "claim_type": "experimental", "paper_id": "wnPlJNiqfA", "paper_title": "KFNN: K-Free Nearest Neighbor For Crowdsourcing", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "RmGxXHTEuK", "reviewer": "Reviewer_MQSy", "review_text": "Comment: Dear authors,\n\nI have read the rebuttal, and thank you for considering my suggestions. Some of my concerns are well addressed, and the others aren't:\n\n(1) Q1 asks whether there are alternatives for label integration that can be used in crowdsourcing. If so, please discuss. Otherwise, please clarify that there are no such alternatives.\n\n(2) Q2: thank authors for acknowledging that these two terms are synonymous. That's why I have asked whether there are alternatives for the studied problem. As reviewed in the seminal survey work published in [R3], there are other options for this problem. Based on this reason, I have asked the authors to enhance the motivation of using KNN-based method instead of the others in Q1.\n\n(3) Q3 is generally satisfactory.\n\n(4) The claims for Q4 should be verified through experimental evaluations.\n\n(5) Q5 asks to clarify the type of crowdsourced task. Notice that, a ranking task in crowdsourcing still requires label integration (or ground truth inference). Thus, it is important to clearly define the application scope. From the current form of submission, the proposed method might be limited to only limited tasks types instead of general tasks in crowdsourcing. If this is the fact, the title and some other contents may need to be refined to avoid over-claiming.\n\n(6) Q6: as shown in the experimental results (Tables 1 and 2), the proposed method could perform worse than the selected baselines in either Macro-F1 score and integration accuracy under certain datasets by a large margin. Based on the current results, it seems that (1) different methods have their own pros and cons, and (2) the proposed method is not always the optimal among the compared ones.\n\n(7) Q7: thank you for acknowledging that the proposed method has not been verified by a real-world platform like AMT. Existing studies on crowdsourcing often conduct evaluations on a real-world platform, such as CrowdFlower (see Section 5.2 in [R2]), since real-world scenarios are more complex than a simulated experiment. Besides, the evaluation can be conducted under the double blind policy. Based on the current evaluations, it is unclear whether the proposed method can be effectively integrated in a real-world platform or not.\n\nOverall, I appreciate the efforts made in the rebuttal. I will take your responses into consideration when making my final decision.\n\nBest regards,", "labeling_timestamp": "2026-01-11T17:18:27.968384", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper's abstract and contribution statements claim that KFNN 'significantly outperforms all the other state-of-the-art algorithms' and that extensive experiments demonstrate its superiority and robustness. The provided content contains no evaluation results indicating that KFNN is not consistently the best; instead, it asserts consistent superiority.", "evidence": "Extensive experimental results demonstrate that KFNN significantly outperforms all the other state-of-the-art algorithms and exhibits greater robustness in various crowdsourcing scenarios.", "section": "Abstract"}
{"claim": "The proposed method has not been evaluated on real-world crowdsourcing platforms such as Amazon Mechanical Turk or CrowdFlower.", "claim_type": "experimental", "paper_id": "wnPlJNiqfA", "paper_title": "KFNN: K-Free Nearest Neighbor For Crowdsourcing", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "RmGxXHTEuK", "reviewer": "Reviewer_MQSy", "review_text": "Comment: Dear authors,\n\nI have read the rebuttal, and thank you for considering my suggestions. Some of my concerns are well addressed, and the others aren't:\n\n(1) Q1 asks whether there are alternatives for label integration that can be used in crowdsourcing. If so, please discuss. Otherwise, please clarify that there are no such alternatives.\n\n(2) Q2: thank authors for acknowledging that these two terms are synonymous. That's why I have asked whether there are alternatives for the studied problem. As reviewed in the seminal survey work published in [R3], there are other options for this problem. Based on this reason, I have asked the authors to enhance the motivation of using KNN-based method instead of the others in Q1.\n\n(3) Q3 is generally satisfactory.\n\n(4) The claims for Q4 should be verified through experimental evaluations.\n\n(5) Q5 asks to clarify the type of crowdsourced task. Notice that, a ranking task in crowdsourcing still requires label integration (or ground truth inference). Thus, it is important to clearly define the application scope. From the current form of submission, the proposed method might be limited to only limited tasks types instead of general tasks in crowdsourcing. If this is the fact, the title and some other contents may need to be refined to avoid over-claiming.\n\n(6) Q6: as shown in the experimental results (Tables 1 and 2), the proposed method could perform worse than the selected baselines in either Macro-F1 score and integration accuracy under certain datasets by a large margin. Based on the current results, it seems that (1) different methods have their own pros and cons, and (2) the proposed method is not always the optimal among the compared ones.\n\n(7) Q7: thank you for acknowledging that the proposed method has not been verified by a real-world platform like AMT. Existing studies on crowdsourcing often conduct evaluations on a real-world platform, such as CrowdFlower (see Section 5.2 in [R2]), since real-world scenarios are more complex than a simulated experiment. Besides, the evaluation can be conducted under the double blind policy. Based on the current evaluations, it is unclear whether the proposed method can be effectively integrated in a real-world platform or not.\n\nOverall, I appreciate the efforts made in the rebuttal. I will take your responses into consideration when making my final decision.\n\nBest regards,", "labeling_timestamp": "2026-01-11T17:18:33.700499", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper text does not describe the experimental datasets or state that evaluations were run on Amazon Mechanical Turk or CrowdFlower. It only contains a general mention of crowdsourcing platforms and a claim of extensive experimental results, but no specific platforms or evaluation details are given in the supplied content.", "evidence": "\"Through crowdsourcing platforms such as Figure Eight and Clickworker, instances can be annotated by crowd workers at a low cost [2, 3].\"; \"Extensive experimental results demonstrate that KFNN significantly outperforms all the other state-of-the-art label integration algorithms and exhibits greater robustness than existing algorithms in various crowdsourcing scenarios.\"", "section": "Introduction; Abstract"}
{"claim": "Current simulated experiments do not establish whether the proposed method can be effectively integrated and perform well in complex real-world crowdsourcing scenarios.", "claim_type": "experimental", "paper_id": "wnPlJNiqfA", "paper_title": "KFNN: K-Free Nearest Neighbor For Crowdsourcing", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "RmGxXHTEuK", "reviewer": "Reviewer_MQSy", "review_text": "Comment: Dear authors,\n\nI have read the rebuttal, and thank you for considering my suggestions. Some of my concerns are well addressed, and the others aren't:\n\n(1) Q1 asks whether there are alternatives for label integration that can be used in crowdsourcing. If so, please discuss. Otherwise, please clarify that there are no such alternatives.\n\n(2) Q2: thank authors for acknowledging that these two terms are synonymous. That's why I have asked whether there are alternatives for the studied problem. As reviewed in the seminal survey work published in [R3], there are other options for this problem. Based on this reason, I have asked the authors to enhance the motivation of using KNN-based method instead of the others in Q1.\n\n(3) Q3 is generally satisfactory.\n\n(4) The claims for Q4 should be verified through experimental evaluations.\n\n(5) Q5 asks to clarify the type of crowdsourced task. Notice that, a ranking task in crowdsourcing still requires label integration (or ground truth inference). Thus, it is important to clearly define the application scope. From the current form of submission, the proposed method might be limited to only limited tasks types instead of general tasks in crowdsourcing. If this is the fact, the title and some other contents may need to be refined to avoid over-claiming.\n\n(6) Q6: as shown in the experimental results (Tables 1 and 2), the proposed method could perform worse than the selected baselines in either Macro-F1 score and integration accuracy under certain datasets by a large margin. Based on the current results, it seems that (1) different methods have their own pros and cons, and (2) the proposed method is not always the optimal among the compared ones.\n\n(7) Q7: thank you for acknowledging that the proposed method has not been verified by a real-world platform like AMT. Existing studies on crowdsourcing often conduct evaluations on a real-world platform, such as CrowdFlower (see Section 5.2 in [R2]), since real-world scenarios are more complex than a simulated experiment. Besides, the evaluation can be conducted under the double blind policy. Based on the current evaluations, it is unclear whether the proposed method can be effectively integrated in a real-world platform or not.\n\nOverall, I appreciate the efforts made in the rebuttal. I will take your responses into consideration when making my final decision.\n\nBest regards,", "labeling_timestamp": "2026-01-11T17:18:42.040117", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt (Abstract and Introduction) claims \"extensive experimental results\" demonstrating robustness in various crowdsourcing scenarios, but the excerpt does not include the experimental setup, datasets, or results details (e.g., whether experiments are simulated or on real-world crowdsourcing data). Therefore there is insufficient information in the supplied content to confirm the reviewer's assertion that the current simulated experiments fail to establish real-world effectiveness.", "evidence": "Extensive experimental results demonstrate that KFNN significantly outperforms all the other state-of-the-art algorithms and exhibits greater robustness in various crowdsourcing scenarios.", "section": "Abstract"}
{"claim": "The evaluation lacks real-world double-blind testing commonly used in crowdsourcing studies to validate performance under realistic conditions.", "claim_type": "methodology", "paper_id": "wnPlJNiqfA", "paper_title": "KFNN: K-Free Nearest Neighbor For Crowdsourcing", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "RmGxXHTEuK", "reviewer": "Reviewer_MQSy", "review_text": "Comment: Dear authors,\n\nI have read the rebuttal, and thank you for considering my suggestions. Some of my concerns are well addressed, and the others aren't:\n\n(1) Q1 asks whether there are alternatives for label integration that can be used in crowdsourcing. If so, please discuss. Otherwise, please clarify that there are no such alternatives.\n\n(2) Q2: thank authors for acknowledging that these two terms are synonymous. That's why I have asked whether there are alternatives for the studied problem. As reviewed in the seminal survey work published in [R3], there are other options for this problem. Based on this reason, I have asked the authors to enhance the motivation of using KNN-based method instead of the others in Q1.\n\n(3) Q3 is generally satisfactory.\n\n(4) The claims for Q4 should be verified through experimental evaluations.\n\n(5) Q5 asks to clarify the type of crowdsourced task. Notice that, a ranking task in crowdsourcing still requires label integration (or ground truth inference). Thus, it is important to clearly define the application scope. From the current form of submission, the proposed method might be limited to only limited tasks types instead of general tasks in crowdsourcing. If this is the fact, the title and some other contents may need to be refined to avoid over-claiming.\n\n(6) Q6: as shown in the experimental results (Tables 1 and 2), the proposed method could perform worse than the selected baselines in either Macro-F1 score and integration accuracy under certain datasets by a large margin. Based on the current results, it seems that (1) different methods have their own pros and cons, and (2) the proposed method is not always the optimal among the compared ones.\n\n(7) Q7: thank you for acknowledging that the proposed method has not been verified by a real-world platform like AMT. Existing studies on crowdsourcing often conduct evaluations on a real-world platform, such as CrowdFlower (see Section 5.2 in [R2]), since real-world scenarios are more complex than a simulated experiment. Besides, the evaluation can be conducted under the double blind policy. Based on the current evaluations, it is unclear whether the proposed method can be effectively integrated in a real-world platform or not.\n\nOverall, I appreciate the efforts made in the rebuttal. I will take your responses into consideration when making my final decision.\n\nBest regards,", "labeling_timestamp": "2026-01-11T17:18:51.534262", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt only states that there are \"extensive experimental results\" (Abstract) but does not include any evaluation or experimental methodology details (no Experiments/Evaluation section or description of real-world double-blind testing protocols) in the supplied content. Therefore there is insufficient information to determine whether real-world double-blind testing was performed.", "evidence": "Extensive experimental results demonstrate that KFNN significantly outperforms all the other state-of-the-art algorithms and exhibits greater robustness in various crowdsourcing scenarios.", "section": "Abstract"}
{"claim": "The paper does not compare its method against other label integration options identified in the seminal survey referenced by the reviewer.", "claim_type": "baseline", "paper_id": "wnPlJNiqfA", "paper_title": "KFNN: K-Free Nearest Neighbor For Crowdsourcing", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "RmGxXHTEuK", "reviewer": "Reviewer_MQSy", "review_text": "Comment: Dear authors,\n\nI have read the rebuttal, and thank you for considering my suggestions. Some of my concerns are well addressed, and the others aren't:\n\n(1) Q1 asks whether there are alternatives for label integration that can be used in crowdsourcing. If so, please discuss. Otherwise, please clarify that there are no such alternatives.\n\n(2) Q2: thank authors for acknowledging that these two terms are synonymous. That's why I have asked whether there are alternatives for the studied problem. As reviewed in the seminal survey work published in [R3], there are other options for this problem. Based on this reason, I have asked the authors to enhance the motivation of using KNN-based method instead of the others in Q1.\n\n(3) Q3 is generally satisfactory.\n\n(4) The claims for Q4 should be verified through experimental evaluations.\n\n(5) Q5 asks to clarify the type of crowdsourced task. Notice that, a ranking task in crowdsourcing still requires label integration (or ground truth inference). Thus, it is important to clearly define the application scope. From the current form of submission, the proposed method might be limited to only limited tasks types instead of general tasks in crowdsourcing. If this is the fact, the title and some other contents may need to be refined to avoid over-claiming.\n\n(6) Q6: as shown in the experimental results (Tables 1 and 2), the proposed method could perform worse than the selected baselines in either Macro-F1 score and integration accuracy under certain datasets by a large margin. Based on the current results, it seems that (1) different methods have their own pros and cons, and (2) the proposed method is not always the optimal among the compared ones.\n\n(7) Q7: thank you for acknowledging that the proposed method has not been verified by a real-world platform like AMT. Existing studies on crowdsourcing often conduct evaluations on a real-world platform, such as CrowdFlower (see Section 5.2 in [R2]), since real-world scenarios are more complex than a simulated experiment. Besides, the evaluation can be conducted under the double blind policy. Based on the current evaluations, it is unclear whether the proposed method can be effectively integrated in a real-world platform or not.\n\nOverall, I appreciate the efforts made in the rebuttal. I will take your responses into consideration when making my final decision.\n\nBest regards,", "labeling_timestamp": "2026-01-11T17:18:56.147055", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt claims extensive experimental results and lists related algorithms, but the excerpt does not include the experimental section or a detailed list of baselines actually used in comparisons. Therefore we cannot determine from the given content whether the authors compared KFNN against the specific label-integration options from the reviewer’s referenced seminal survey.", "evidence": "Abstract: \"Extensive experimental results demonstrate that KFNN significantly outperforms all the other state-of-the-art algorithms and exhibits greater robustness in various crowdsourcing scenarios.\"; Related work: \"Depending on whether neighbor instances are leveraged or not, existing label integration algorithms can be divided into two categories...\" (lists many prior methods such as [17]–[29], [13], [14], [1]).", "section": "Abstract; 2 Related work"}
{"claim": "The method requires additional meta-training, increasing training complexity and resource demands.", "claim_type": "methodology", "paper_id": "5RielfrDkP", "paper_title": "Learning Adaptive Multiresolution Transforms via Meta-Framelet-based Graph Convolutional Network", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "stCa35d4sd", "reviewer": "Reviewer_nDyN", "review_text": "Summary: This paper use meta-learning to build for adaptive framelet for GNN.\n\nThey evaluate the model's performance on various graph learning tasks.\n\nStrengths: 1. MM-FGCN is can learn adaptive multiresolution representation.\n\n2. Achieve STOA performance on graph learning tasks.\n\n3. parameterization of the meta-framelet generator uses use fewer parameters than previous method.\n\nWeaknesses: 1. Require additional meta training.\n2. The framelet is defined using a meta-band-pass filter based on polynomial splines, the model use Chebyshev approximation to circumventing the need for eigen-decomposition, but compared with graph-structure-based approach, Chebyshev approximation seems a more expensive.\n3. Following the previous point, it seems it's hard to use the model on large graphs. (ogbn tasks)\n4. The performance is not close to STOA, recent studies show better performance, e.g. [1],[2]\n\n[1] https://arxiv.org/abs/2305.10498\n[2] https://arxiv.org/abs/2105.07634\n\nQuestions: 1. Could you provide details on the time cost for meta-learning as well as the inference time for the model?\n2. Would it be possible to evaluate the model on larger datasets like ogbn? I did observe ogbg-molhiv, but I'm referring to graphs with a larger number of nodes rather than the total number of graphs.\n3. An detailed introduction to meta-learning would help readers in gaining a clearer understanding of the experimental setup.", "labeling_timestamp": "2026-01-11T17:19:04.570087", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper states it uses a \"meta-learner\" to generate customized framelet generators for each graph instance (Intro), which indicates an additional learned component. However, the provided content does not describe a separate meta-training procedure, nor does it quantify training complexity or resource requirements, so it is insufficient to conclude that the method requires additional meta-training that increases complexity/resources.", "evidence": "\"To acquire multiresolution representation suited to different graph instances and distributions, we introduce the Multiresolution MetaFrameletbased Graph Convolutional Network (MM-FGCN), facilitating comprehensive and adaptive multiresolution analysis across diverse graphs. ... To address this limitation, it is crucial to learn an adaptive multiresolution representation that can be automatically tailored to diverse graph instances and distributions. This motivates us to establish a meta-learner to generate the customized feature transform and multiresolution analysis strategy for each individual graph instance.\"", "section": "1 INTRODUCTION"}
{"claim": "The paper lacks reported time costs for meta-learning training and for model inference, preventing assessment of computational efficiency.", "claim_type": "quantitative", "paper_id": "5RielfrDkP", "paper_title": "Learning Adaptive Multiresolution Transforms via Meta-Framelet-based Graph Convolutional Network", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "stCa35d4sd", "reviewer": "Reviewer_nDyN", "review_text": "Summary: This paper use meta-learning to build for adaptive framelet for GNN.\n\nThey evaluate the model's performance on various graph learning tasks.\n\nStrengths: 1. MM-FGCN is can learn adaptive multiresolution representation.\n\n2. Achieve STOA performance on graph learning tasks.\n\n3. parameterization of the meta-framelet generator uses use fewer parameters than previous method.\n\nWeaknesses: 1. Require additional meta training.\n2. The framelet is defined using a meta-band-pass filter based on polynomial splines, the model use Chebyshev approximation to circumventing the need for eigen-decomposition, but compared with graph-structure-based approach, Chebyshev approximation seems a more expensive.\n3. Following the previous point, it seems it's hard to use the model on large graphs. (ogbn tasks)\n4. The performance is not close to STOA, recent studies show better performance, e.g. [1],[2]\n\n[1] https://arxiv.org/abs/2305.10498\n[2] https://arxiv.org/abs/2105.07634\n\nQuestions: 1. Could you provide details on the time cost for meta-learning as well as the inference time for the model?\n2. Would it be possible to evaluate the model on larger datasets like ogbn? I did observe ogbg-molhiv, but I'm referring to graphs with a larger number of nodes rather than the total number of graphs.\n3. An detailed introduction to meta-learning would help readers in gaining a clearer understanding of the experimental setup.", "labeling_timestamp": "2026-01-11T17:19:04.094191", "model": "gpt-5-mini", "label": "Supported", "justification": "The provided paper content describes experimental evaluation and claims state-of-the-art performance but contains no reported training or inference time costs, nor any metrics of computational efficiency or runtime. Thus the reviewer's claim that time costs for meta-learning training and model inference are not reported is supported by the paper text provided.", "evidence": "Abstract: \"Extensive experiments demonstrate that our MM-FGCN achieves SOTA performance on various graph learning tasks. The code is available on GitHub 1 .\"; Contributions: \"Extensive experiments show that our model achieves state-of-the-art performance compared to other baseline methods.\"", "section": "Abstract; Contributions (end of Introduction)"}
{"claim": "The Chebyshev approximation used to avoid eigen-decomposition appears more computationally expensive than graph-structure-based approaches.", "claim_type": "baseline", "paper_id": "5RielfrDkP", "paper_title": "Learning Adaptive Multiresolution Transforms via Meta-Framelet-based Graph Convolutional Network", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "stCa35d4sd", "reviewer": "Reviewer_nDyN", "review_text": "Summary: This paper use meta-learning to build for adaptive framelet for GNN.\n\nThey evaluate the model's performance on various graph learning tasks.\n\nStrengths: 1. MM-FGCN is can learn adaptive multiresolution representation.\n\n2. Achieve STOA performance on graph learning tasks.\n\n3. parameterization of the meta-framelet generator uses use fewer parameters than previous method.\n\nWeaknesses: 1. Require additional meta training.\n2. The framelet is defined using a meta-band-pass filter based on polynomial splines, the model use Chebyshev approximation to circumventing the need for eigen-decomposition, but compared with graph-structure-based approach, Chebyshev approximation seems a more expensive.\n3. Following the previous point, it seems it's hard to use the model on large graphs. (ogbn tasks)\n4. The performance is not close to STOA, recent studies show better performance, e.g. [1],[2]\n\n[1] https://arxiv.org/abs/2305.10498\n[2] https://arxiv.org/abs/2105.07634\n\nQuestions: 1. Could you provide details on the time cost for meta-learning as well as the inference time for the model?\n2. Would it be possible to evaluate the model on larger datasets like ogbn? I did observe ogbg-molhiv, but I'm referring to graphs with a larger number of nodes rather than the total number of graphs.\n3. An detailed introduction to meta-learning would help readers in gaining a clearer understanding of the experimental setup.", "labeling_timestamp": "2026-01-11T17:19:14.791927", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper content does not mention a Chebyshev approximation or analyze its computational cost relative to graph-structure-based approaches. Therefore there is insufficient information to determine whether the claim is true according to the paper.", "evidence": "Suppose the eigendecomposition of the graph Laplacian is L = UΛU ⊤ , the graph spectrum refers to the diagonal eigenvalue matrix Λ = diag( λ 1 , ..., λ n ) , and the spectral bases is the collection of eigenvectors U = ( u 1 , ..., u n ) .", "section": "3 PRELIMINARY — Spectral Graph Signal Processing"}
{"claim": "The model seems difficult to apply to large graphs such as ogbn datasets due to computational expense and scalability concerns.", "claim_type": "methodology", "paper_id": "5RielfrDkP", "paper_title": "Learning Adaptive Multiresolution Transforms via Meta-Framelet-based Graph Convolutional Network", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "stCa35d4sd", "reviewer": "Reviewer_nDyN", "review_text": "Summary: This paper use meta-learning to build for adaptive framelet for GNN.\n\nThey evaluate the model's performance on various graph learning tasks.\n\nStrengths: 1. MM-FGCN is can learn adaptive multiresolution representation.\n\n2. Achieve STOA performance on graph learning tasks.\n\n3. parameterization of the meta-framelet generator uses use fewer parameters than previous method.\n\nWeaknesses: 1. Require additional meta training.\n2. The framelet is defined using a meta-band-pass filter based on polynomial splines, the model use Chebyshev approximation to circumventing the need for eigen-decomposition, but compared with graph-structure-based approach, Chebyshev approximation seems a more expensive.\n3. Following the previous point, it seems it's hard to use the model on large graphs. (ogbn tasks)\n4. The performance is not close to STOA, recent studies show better performance, e.g. [1],[2]\n\n[1] https://arxiv.org/abs/2305.10498\n[2] https://arxiv.org/abs/2105.07634\n\nQuestions: 1. Could you provide details on the time cost for meta-learning as well as the inference time for the model?\n2. Would it be possible to evaluate the model on larger datasets like ogbn? I did observe ogbg-molhiv, but I'm referring to graphs with a larger number of nodes rather than the total number of graphs.\n3. An detailed introduction to meta-learning would help readers in gaining a clearer understanding of the experimental setup.", "labeling_timestamp": "2026-01-11T17:19:14.107734", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does not discuss computational complexity, runtime, or scalability to large benchmarks (e.g., ogbn). While it describes generating a meta-framelet generator per input graph instance, it does not state that this makes the approach impractical for large graphs nor reports experiments or complexity analyses on large-scale datasets.", "evidence": "For each input graph instance, the MMFGCN first generates the meta-framelet generator, which consists of a set of customized band-pass filters in the frequency domain.\nExtensive experiments show that our model achieves state-of-the-art performance compared to other baseline methods.", "section": "Introduction; Abstract"}
{"claim": "The evaluation omits large-node-count graph datasets like ogbn, limiting evidence of scalability and generalization to large graphs.", "claim_type": "experimental", "paper_id": "5RielfrDkP", "paper_title": "Learning Adaptive Multiresolution Transforms via Meta-Framelet-based Graph Convolutional Network", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "stCa35d4sd", "reviewer": "Reviewer_nDyN", "review_text": "Summary: This paper use meta-learning to build for adaptive framelet for GNN.\n\nThey evaluate the model's performance on various graph learning tasks.\n\nStrengths: 1. MM-FGCN is can learn adaptive multiresolution representation.\n\n2. Achieve STOA performance on graph learning tasks.\n\n3. parameterization of the meta-framelet generator uses use fewer parameters than previous method.\n\nWeaknesses: 1. Require additional meta training.\n2. The framelet is defined using a meta-band-pass filter based on polynomial splines, the model use Chebyshev approximation to circumventing the need for eigen-decomposition, but compared with graph-structure-based approach, Chebyshev approximation seems a more expensive.\n3. Following the previous point, it seems it's hard to use the model on large graphs. (ogbn tasks)\n4. The performance is not close to STOA, recent studies show better performance, e.g. [1],[2]\n\n[1] https://arxiv.org/abs/2305.10498\n[2] https://arxiv.org/abs/2105.07634\n\nQuestions: 1. Could you provide details on the time cost for meta-learning as well as the inference time for the model?\n2. Would it be possible to evaluate the model on larger datasets like ogbn? I did observe ogbg-molhiv, but I'm referring to graphs with a larger number of nodes rather than the total number of graphs.\n3. An detailed introduction to meta-learning would help readers in gaining a clearer understanding of the experimental setup.", "labeling_timestamp": "2026-01-11T17:19:19.713746", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt (Abstract, Introduction, Related Work, Preliminaries, and Contributions) does not include the Experiments section or any dataset list. There is no information in the given content about which datasets (e.g., ogbn) were used, so we cannot verify whether large-node-count graph datasets were omitted.", "evidence": "1) \"Extensive experiments demonstrate that our MM-FGCN achieves SOTA performance on various graph learning tasks. The code is available on GitHub.\" 2) \"Extensive experiments show that our model achieves state-of-the-art performance compared to other baseline methods.\"", "section": "Abstract / Contributions"}
{"claim": "The reported performance is not close to state-of-the-art; more recent studies (e.g., cited [1], [2]) show better performance.", "claim_type": "baseline", "paper_id": "5RielfrDkP", "paper_title": "Learning Adaptive Multiresolution Transforms via Meta-Framelet-based Graph Convolutional Network", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "stCa35d4sd", "reviewer": "Reviewer_nDyN", "review_text": "Summary: This paper use meta-learning to build for adaptive framelet for GNN.\n\nThey evaluate the model's performance on various graph learning tasks.\n\nStrengths: 1. MM-FGCN is can learn adaptive multiresolution representation.\n\n2. Achieve STOA performance on graph learning tasks.\n\n3. parameterization of the meta-framelet generator uses use fewer parameters than previous method.\n\nWeaknesses: 1. Require additional meta training.\n2. The framelet is defined using a meta-band-pass filter based on polynomial splines, the model use Chebyshev approximation to circumventing the need for eigen-decomposition, but compared with graph-structure-based approach, Chebyshev approximation seems a more expensive.\n3. Following the previous point, it seems it's hard to use the model on large graphs. (ogbn tasks)\n4. The performance is not close to STOA, recent studies show better performance, e.g. [1],[2]\n\n[1] https://arxiv.org/abs/2305.10498\n[2] https://arxiv.org/abs/2105.07634\n\nQuestions: 1. Could you provide details on the time cost for meta-learning as well as the inference time for the model?\n2. Would it be possible to evaluate the model on larger datasets like ogbn? I did observe ogbg-molhiv, but I'm referring to graphs with a larger number of nodes rather than the total number of graphs.\n3. An detailed introduction to meta-learning would help readers in gaining a clearer understanding of the experimental setup.", "labeling_timestamp": "2026-01-11T17:19:29.232745", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly claims that its method achieves state-of-the-art performance. The reviewer claim that reported performance is not close to SOTA contradicts the paper's stated results; the provided content includes explicit SOTA claims (though numerical results are not shown in the excerpt).", "evidence": "\"Extensive experiments demonstrate that our MM-FGCN achieves SOTA performance on various graph learning tasks.\"; \"Extensive experiments show that our model achieves state-of-the-art performance compared to other baseline methods.\"", "section": "Abstract; Contributions (end of Introduction)"}
{"claim": "The paper lacks a detailed introduction to meta-learning, hindering readers' understanding of the experimental setup and method rationale.", "claim_type": "presentation", "paper_id": "5RielfrDkP", "paper_title": "Learning Adaptive Multiresolution Transforms via Meta-Framelet-based Graph Convolutional Network", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "stCa35d4sd", "reviewer": "Reviewer_nDyN", "review_text": "Summary: This paper use meta-learning to build for adaptive framelet for GNN.\n\nThey evaluate the model's performance on various graph learning tasks.\n\nStrengths: 1. MM-FGCN is can learn adaptive multiresolution representation.\n\n2. Achieve STOA performance on graph learning tasks.\n\n3. parameterization of the meta-framelet generator uses use fewer parameters than previous method.\n\nWeaknesses: 1. Require additional meta training.\n2. The framelet is defined using a meta-band-pass filter based on polynomial splines, the model use Chebyshev approximation to circumventing the need for eigen-decomposition, but compared with graph-structure-based approach, Chebyshev approximation seems a more expensive.\n3. Following the previous point, it seems it's hard to use the model on large graphs. (ogbn tasks)\n4. The performance is not close to STOA, recent studies show better performance, e.g. [1],[2]\n\n[1] https://arxiv.org/abs/2305.10498\n[2] https://arxiv.org/abs/2105.07634\n\nQuestions: 1. Could you provide details on the time cost for meta-learning as well as the inference time for the model?\n2. Would it be possible to evaluate the model on larger datasets like ogbn? I did observe ogbg-molhiv, but I'm referring to graphs with a larger number of nodes rather than the total number of graphs.\n3. An detailed introduction to meta-learning would help readers in gaining a clearer understanding of the experimental setup.", "labeling_timestamp": "2026-01-11T17:19:34.075057", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper only briefly mentions a \"meta-learner\" in the Introduction to motivate the method but provides no dedicated background or detailed introduction to meta-learning in Related Work or Preliminary sections, indicating a lack of detailed meta-learning exposition.", "evidence": "Introduction: \"This motivates us to establish a meta-learner to generate the customized feature transform and multiresolution analysis strategy for each individual graph instance.\"  Preliminary (topics covered): \"Graph Representation Learning... Spectral Graph Signal Processing... Spectral Graph Multiresolution Analysis.\"", "section": "Introduction"}
{"claim": "The SparklesDialogueVG subset was generated using GPT-4, introducing dependency on the same model being evaluated.", "claim_type": "methodology", "paper_id": "oq5EF8parZ", "paper_title": "Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "WbH03mw5Tg", "reviewer": "Reviewer_eU8x", "review_text": "Summary: The paper introduces SparklesChat, a multimodal instruction following model for open-ended dialogues across multiple images. This is MiniGPT4 fine-tuned on the machine-generated dialogue dataset released in the paper called SparklesDialogue. This contains word-level interleaved multi-image and text interactions with up to 3 images during the first turn and 1 image during the second turn. SparklesDialogue consists of two subsets: 1) SparklesDialogueCC which contain images from CC3M and captions generated by MiniGPT4 2) SparklesDialogueVG which contain images from Visual Genome and descriptions from GPT-4, based on human-annotated captions, objects, and regions. SparklesEval is a new GPT-assisted benchmark with 150 dialogs, introduced to assess conversational competence across multiple images and dialogue turns, through criteria such as Image Understanding & Reasoning, Cross-Image & Cross-Turn Coherence, and Relevance & Completeness of Responses. SparklesChat outperforms MiniGPT-4 and gets marginally close GPT-4 on binary image selection task and the NLVR2 visual reasoning task. The paper contains ablation study on the effect of dialog turns and SparklesDialogue subsets during training.\n\n====\n\nUpdated final rating form 3 to 5 due to demonstration of improvement on LLaVA as well. The experiments section and total contributions are still weak.\n\nStrengths: 1.\tNew dataset SparklesDialogue for word-level interleaved multi-image and text interactions\n2.\tNew benchmark SparklesEval for word-level interleaved multi-image and text interactions\n3.\tDemonstration of improved performance over MiniGPT4\n\nWeaknesses: 1.\tSparklesDialogue contains subset SparklesDialogueVG, which was generated using GPT-4. The paper compares with performance of GPT-4 (method used to create the data set is also being evaluated on), while still performing worse although SparklesChat uses much richer image embedding. \n2.\tNo contribution in terms of novelty architecture. Main contribution is in the data set. \n3.\tOnly two turns per sample in the dataset. Longer sessions are probably more practical than more images per turn and limiting to just 2 turns. Dataset (that too, machine-generated) being the highlight of this paper, would have expected more.\n4.     Not clear how this extends to other approaches such as LLaVA. Results are shown only for Min-GPT4 extension.\n\nQuestions: Q1) Section 5.2 mentions, SparklesDialogueVG and SparklesEval use the same sources of images and captions. This is suspected to be one of the reasons why model trained on SparklesDialogueVG performs better than model trained on SparklesDialogueCC. Isnlt this a serious issue, especially since SparklesDialogueVG is claimed to be the high quality subset?\n\nMinor typo\n1.\tTable 2: Column title should be A2 under “Turn two”", "labeling_timestamp": "2026-01-11T17:19:49.254440", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly states that GPT-4 was used to generate the SparklesDialogue data (including VG descriptions) and also reports GPT-4's performance on SparklesEval, meaning the same model was used for data generation and evaluation, supporting the reviewer's claim.", "evidence": "\"We use OpenAI's GPT-4 (Openai, 2023) to simulate user-assistant conversations with visual capabilities by leveraging detailed image descriptions.\" \n\n\"SparklesDialogueVG is of high quality as the VG image descriptions generated by GPT-4 benefit from human-annotated captions, objects,\" \n\n\"SparklesChat scored 8.56 out of 10 on SparklesEval, substantially exceeding MiniGPT-4's score of 3.91 and nearing GPT-4's score of 9.26.\"", "section": "Section 4.1 (GPT-ASSISTED DATA CONSTRUCTION); Section 4.2 (STATISTICS AND CHARACTERISTICS); Abstract/Introduction"}
{"claim": "The paper compares model performance to GPT-4 despite GPT-4 being used to generate part of the training dataset.", "claim_type": "baseline", "paper_id": "oq5EF8parZ", "paper_title": "Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "WbH03mw5Tg", "reviewer": "Reviewer_eU8x", "review_text": "Summary: The paper introduces SparklesChat, a multimodal instruction following model for open-ended dialogues across multiple images. This is MiniGPT4 fine-tuned on the machine-generated dialogue dataset released in the paper called SparklesDialogue. This contains word-level interleaved multi-image and text interactions with up to 3 images during the first turn and 1 image during the second turn. SparklesDialogue consists of two subsets: 1) SparklesDialogueCC which contain images from CC3M and captions generated by MiniGPT4 2) SparklesDialogueVG which contain images from Visual Genome and descriptions from GPT-4, based on human-annotated captions, objects, and regions. SparklesEval is a new GPT-assisted benchmark with 150 dialogs, introduced to assess conversational competence across multiple images and dialogue turns, through criteria such as Image Understanding & Reasoning, Cross-Image & Cross-Turn Coherence, and Relevance & Completeness of Responses. SparklesChat outperforms MiniGPT-4 and gets marginally close GPT-4 on binary image selection task and the NLVR2 visual reasoning task. The paper contains ablation study on the effect of dialog turns and SparklesDialogue subsets during training.\n\n====\n\nUpdated final rating form 3 to 5 due to demonstration of improvement on LLaVA as well. The experiments section and total contributions are still weak.\n\nStrengths: 1.\tNew dataset SparklesDialogue for word-level interleaved multi-image and text interactions\n2.\tNew benchmark SparklesEval for word-level interleaved multi-image and text interactions\n3.\tDemonstration of improved performance over MiniGPT4\n\nWeaknesses: 1.\tSparklesDialogue contains subset SparklesDialogueVG, which was generated using GPT-4. The paper compares with performance of GPT-4 (method used to create the data set is also being evaluated on), while still performing worse although SparklesChat uses much richer image embedding. \n2.\tNo contribution in terms of novelty architecture. Main contribution is in the data set. \n3.\tOnly two turns per sample in the dataset. Longer sessions are probably more practical than more images per turn and limiting to just 2 turns. Dataset (that too, machine-generated) being the highlight of this paper, would have expected more.\n4.     Not clear how this extends to other approaches such as LLaVA. Results are shown only for Min-GPT4 extension.\n\nQuestions: Q1) Section 5.2 mentions, SparklesDialogueVG and SparklesEval use the same sources of images and captions. This is suspected to be one of the reasons why model trained on SparklesDialogueVG performs better than model trained on SparklesDialogueCC. Isnlt this a serious issue, especially since SparklesDialogueVG is claimed to be the high quality subset?\n\nMinor typo\n1.\tTable 2: Column title should be A2 under “Turn two”", "labeling_timestamp": "2026-01-11T17:19:39.098361", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly states that GPT-4 was used to generate the SparklesDialogue training data (Section 4.1 / Introduction) and also reports GPT-4's score as a comparison in the SparklesEval evaluation, so it does compare to GPT-4 despite GPT-4 contributing to the training dataset.", "evidence": "1) \"To support the training of SparklesChat, we introduce SparklesDialogue, the first machine-generated dialogue dataset designed for word-level interleaved multi-image and text interactions. We use OpenAI's GPT-4 (Openai, 2023) to simulate user-assistant conversations with visual capabilities by leveraging detailed image descriptions.\"; 2) \"In our SparklesEval benchmark, SparklesChat scores 8.56 out of 10, significantly exceeds MiniGPT-4's score of 3.91, and closely approaches GPT-4's score of 9.26.\"", "section": "Introduction; 4.1 GPT-ASSISTED DATA CONSTRUCTION (and Abstract)"}
{"claim": "SparklesChat performs worse than GPT-4 despite using much richer image embeddings.", "claim_type": "baseline", "paper_id": "oq5EF8parZ", "paper_title": "Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "WbH03mw5Tg", "reviewer": "Reviewer_eU8x", "review_text": "Summary: The paper introduces SparklesChat, a multimodal instruction following model for open-ended dialogues across multiple images. This is MiniGPT4 fine-tuned on the machine-generated dialogue dataset released in the paper called SparklesDialogue. This contains word-level interleaved multi-image and text interactions with up to 3 images during the first turn and 1 image during the second turn. SparklesDialogue consists of two subsets: 1) SparklesDialogueCC which contain images from CC3M and captions generated by MiniGPT4 2) SparklesDialogueVG which contain images from Visual Genome and descriptions from GPT-4, based on human-annotated captions, objects, and regions. SparklesEval is a new GPT-assisted benchmark with 150 dialogs, introduced to assess conversational competence across multiple images and dialogue turns, through criteria such as Image Understanding & Reasoning, Cross-Image & Cross-Turn Coherence, and Relevance & Completeness of Responses. SparklesChat outperforms MiniGPT-4 and gets marginally close GPT-4 on binary image selection task and the NLVR2 visual reasoning task. The paper contains ablation study on the effect of dialog turns and SparklesDialogue subsets during training.\n\n====\n\nUpdated final rating form 3 to 5 due to demonstration of improvement on LLaVA as well. The experiments section and total contributions are still weak.\n\nStrengths: 1.\tNew dataset SparklesDialogue for word-level interleaved multi-image and text interactions\n2.\tNew benchmark SparklesEval for word-level interleaved multi-image and text interactions\n3.\tDemonstration of improved performance over MiniGPT4\n\nWeaknesses: 1.\tSparklesDialogue contains subset SparklesDialogueVG, which was generated using GPT-4. The paper compares with performance of GPT-4 (method used to create the data set is also being evaluated on), while still performing worse although SparklesChat uses much richer image embedding. \n2.\tNo contribution in terms of novelty architecture. Main contribution is in the data set. \n3.\tOnly two turns per sample in the dataset. Longer sessions are probably more practical than more images per turn and limiting to just 2 turns. Dataset (that too, machine-generated) being the highlight of this paper, would have expected more.\n4.     Not clear how this extends to other approaches such as LLaVA. Results are shown only for Min-GPT4 extension.\n\nQuestions: Q1) Section 5.2 mentions, SparklesDialogueVG and SparklesEval use the same sources of images and captions. This is suspected to be one of the reasons why model trained on SparklesDialogueVG performs better than model trained on SparklesDialogueCC. Isnlt this a serious issue, especially since SparklesDialogueVG is claimed to be the high quality subset?\n\nMinor typo\n1.\tTable 2: Column title should be A2 under “Turn two”", "labeling_timestamp": "2026-01-11T17:20:00.648179", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper reports SparklesChat scoring 8.56 vs GPT-4's 9.26 on SparklesEval, so it does perform worse than GPT-4. However, the paper does not claim or provide evidence that SparklesChat uses \"much richer image embeddings\" than GPT-4; in fact, it notes the publicly accessible GPT-4 API only accepts text input (images were represented via descriptions) and thus no direct comparison of image embedding richness is provided.", "evidence": "1) \"SparklesChat scored 8.56 out of 10 on SparklesEval, substantially exceeding MiniGPT-4's score of 3.91 and nearing GPT-4's score of 9.26.\"  2) \"For image processing, we use the visual encoder from BLIP-2, combining a pretrained EVA-ViT in Vision Transformer (ViT) backbone with a pretrained Q-Former ... In SparklesChat, image representations of different images are embedded between text according to their positions in dialogues.\"  3) \"Given that the publicly accessible GPT-4 API only accepts text input, we represent images with detailed descriptions.\"", "section": "Abstract; Section 3 (Architecture); Section 4.1 (GPT-assisted data construction)"}
{"claim": "The paper presents no novel model architecture, with the primary contribution being the dataset rather than methodological innovation.", "claim_type": "novelty", "paper_id": "oq5EF8parZ", "paper_title": "Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "WbH03mw5Tg", "reviewer": "Reviewer_eU8x", "review_text": "Summary: The paper introduces SparklesChat, a multimodal instruction following model for open-ended dialogues across multiple images. This is MiniGPT4 fine-tuned on the machine-generated dialogue dataset released in the paper called SparklesDialogue. This contains word-level interleaved multi-image and text interactions with up to 3 images during the first turn and 1 image during the second turn. SparklesDialogue consists of two subsets: 1) SparklesDialogueCC which contain images from CC3M and captions generated by MiniGPT4 2) SparklesDialogueVG which contain images from Visual Genome and descriptions from GPT-4, based on human-annotated captions, objects, and regions. SparklesEval is a new GPT-assisted benchmark with 150 dialogs, introduced to assess conversational competence across multiple images and dialogue turns, through criteria such as Image Understanding & Reasoning, Cross-Image & Cross-Turn Coherence, and Relevance & Completeness of Responses. SparklesChat outperforms MiniGPT-4 and gets marginally close GPT-4 on binary image selection task and the NLVR2 visual reasoning task. The paper contains ablation study on the effect of dialog turns and SparklesDialogue subsets during training.\n\n====\n\nUpdated final rating form 3 to 5 due to demonstration of improvement on LLaVA as well. The experiments section and total contributions are still weak.\n\nStrengths: 1.\tNew dataset SparklesDialogue for word-level interleaved multi-image and text interactions\n2.\tNew benchmark SparklesEval for word-level interleaved multi-image and text interactions\n3.\tDemonstration of improved performance over MiniGPT4\n\nWeaknesses: 1.\tSparklesDialogue contains subset SparklesDialogueVG, which was generated using GPT-4. The paper compares with performance of GPT-4 (method used to create the data set is also being evaluated on), while still performing worse although SparklesChat uses much richer image embedding. \n2.\tNo contribution in terms of novelty architecture. Main contribution is in the data set. \n3.\tOnly two turns per sample in the dataset. Longer sessions are probably more practical than more images per turn and limiting to just 2 turns. Dataset (that too, machine-generated) being the highlight of this paper, would have expected more.\n4.     Not clear how this extends to other approaches such as LLaVA. Results are shown only for Min-GPT4 extension.\n\nQuestions: Q1) Section 5.2 mentions, SparklesDialogueVG and SparklesEval use the same sources of images and captions. This is suspected to be one of the reasons why model trained on SparklesDialogueVG performs better than model trained on SparklesDialogueCC. Isnlt this a serious issue, especially since SparklesDialogueVG is claimed to be the high quality subset?\n\nMinor typo\n1.\tTable 2: Column title should be A2 under “Turn two”", "labeling_timestamp": "2026-01-11T17:19:55.243892", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly claims an architectural innovation: embedding multiple image representations at the word level (rather than concatenating a single image representation), which it presents as a novel design for SparklesChat. While the dataset is a major contribution, the paper does describe a methodological/architectural change, so the reviewer's assertion of 'no novel model architecture' is contradicted by the paper.", "evidence": "“Unlike previous approaches such as MiniGPT-4 that takes the concatenation of a single image with sentence-level text as input ..., SparklesChat, as shown in Figure 1, integrates multiple images at the word level (e.g., 'Can you link the celebration occurring in IMAGE#2331159 and the dirt bike race in IMAGE#2330601 ?'). This innovation enables fine-grained integration of images and text, mimicking natural human communication more closely.”\n\n“... In SparklesChat, image representations of different images are embedded between text according to their positions in dialogues.”", "section": "Introduction; 3 SPARKLESCHAT (Architecture)"}
{"claim": "The dataset contains only two dialogue turns per sample, which limits modeling of longer conversational sessions.", "claim_type": "quantitative", "paper_id": "oq5EF8parZ", "paper_title": "Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "WbH03mw5Tg", "reviewer": "Reviewer_eU8x", "review_text": "Summary: The paper introduces SparklesChat, a multimodal instruction following model for open-ended dialogues across multiple images. This is MiniGPT4 fine-tuned on the machine-generated dialogue dataset released in the paper called SparklesDialogue. This contains word-level interleaved multi-image and text interactions with up to 3 images during the first turn and 1 image during the second turn. SparklesDialogue consists of two subsets: 1) SparklesDialogueCC which contain images from CC3M and captions generated by MiniGPT4 2) SparklesDialogueVG which contain images from Visual Genome and descriptions from GPT-4, based on human-annotated captions, objects, and regions. SparklesEval is a new GPT-assisted benchmark with 150 dialogs, introduced to assess conversational competence across multiple images and dialogue turns, through criteria such as Image Understanding & Reasoning, Cross-Image & Cross-Turn Coherence, and Relevance & Completeness of Responses. SparklesChat outperforms MiniGPT-4 and gets marginally close GPT-4 on binary image selection task and the NLVR2 visual reasoning task. The paper contains ablation study on the effect of dialog turns and SparklesDialogue subsets during training.\n\n====\n\nUpdated final rating form 3 to 5 due to demonstration of improvement on LLaVA as well. The experiments section and total contributions are still weak.\n\nStrengths: 1.\tNew dataset SparklesDialogue for word-level interleaved multi-image and text interactions\n2.\tNew benchmark SparklesEval for word-level interleaved multi-image and text interactions\n3.\tDemonstration of improved performance over MiniGPT4\n\nWeaknesses: 1.\tSparklesDialogue contains subset SparklesDialogueVG, which was generated using GPT-4. The paper compares with performance of GPT-4 (method used to create the data set is also being evaluated on), while still performing worse although SparklesChat uses much richer image embedding. \n2.\tNo contribution in terms of novelty architecture. Main contribution is in the data set. \n3.\tOnly two turns per sample in the dataset. Longer sessions are probably more practical than more images per turn and limiting to just 2 turns. Dataset (that too, machine-generated) being the highlight of this paper, would have expected more.\n4.     Not clear how this extends to other approaches such as LLaVA. Results are shown only for Min-GPT4 extension.\n\nQuestions: Q1) Section 5.2 mentions, SparklesDialogueVG and SparklesEval use the same sources of images and captions. This is suspected to be one of the reasons why model trained on SparklesDialogueVG performs better than model trained on SparklesDialogueCC. Isnlt this a serious issue, especially since SparklesDialogueVG is claimed to be the high quality subset?\n\nMinor typo\n1.\tTable 2: Column title should be A2 under “Turn two”", "labeling_timestamp": "2026-01-11T17:19:59.843860", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly states that SparklesDialogue is generated using a \"structured two-turn pattern\", so each dialogue sample contains two turns. However, the paper does not state that this design \"limits modeling of longer conversational sessions\"—that limitation is a reviewer inference not asserted by the paper.", "evidence": "We instruct GPT-4 to simulate realistic and diverse dialogues between a user and an assistant with advanced image understanding and reasoning capabilities following a structured two-turn pattern. In the first turn, the user initiates a reasonable and creative message regarding some images. In response, the assistant generates detailed answers that include comprehensive reasoning regarding the visual content. In the second turn, the user introduces a new image for further discussion, referencing both the new and previous images.", "section": "4.1 GPT-ASSISTED DATA CONSTRUCTION"}
{"claim": "Limiting samples to only two turns sacrifices longer-session practicality in favor of more images per turn.", "claim_type": "methodology", "paper_id": "oq5EF8parZ", "paper_title": "Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "WbH03mw5Tg", "reviewer": "Reviewer_eU8x", "review_text": "Summary: The paper introduces SparklesChat, a multimodal instruction following model for open-ended dialogues across multiple images. This is MiniGPT4 fine-tuned on the machine-generated dialogue dataset released in the paper called SparklesDialogue. This contains word-level interleaved multi-image and text interactions with up to 3 images during the first turn and 1 image during the second turn. SparklesDialogue consists of two subsets: 1) SparklesDialogueCC which contain images from CC3M and captions generated by MiniGPT4 2) SparklesDialogueVG which contain images from Visual Genome and descriptions from GPT-4, based on human-annotated captions, objects, and regions. SparklesEval is a new GPT-assisted benchmark with 150 dialogs, introduced to assess conversational competence across multiple images and dialogue turns, through criteria such as Image Understanding & Reasoning, Cross-Image & Cross-Turn Coherence, and Relevance & Completeness of Responses. SparklesChat outperforms MiniGPT-4 and gets marginally close GPT-4 on binary image selection task and the NLVR2 visual reasoning task. The paper contains ablation study on the effect of dialog turns and SparklesDialogue subsets during training.\n\n====\n\nUpdated final rating form 3 to 5 due to demonstration of improvement on LLaVA as well. The experiments section and total contributions are still weak.\n\nStrengths: 1.\tNew dataset SparklesDialogue for word-level interleaved multi-image and text interactions\n2.\tNew benchmark SparklesEval for word-level interleaved multi-image and text interactions\n3.\tDemonstration of improved performance over MiniGPT4\n\nWeaknesses: 1.\tSparklesDialogue contains subset SparklesDialogueVG, which was generated using GPT-4. The paper compares with performance of GPT-4 (method used to create the data set is also being evaluated on), while still performing worse although SparklesChat uses much richer image embedding. \n2.\tNo contribution in terms of novelty architecture. Main contribution is in the data set. \n3.\tOnly two turns per sample in the dataset. Longer sessions are probably more practical than more images per turn and limiting to just 2 turns. Dataset (that too, machine-generated) being the highlight of this paper, would have expected more.\n4.     Not clear how this extends to other approaches such as LLaVA. Results are shown only for Min-GPT4 extension.\n\nQuestions: Q1) Section 5.2 mentions, SparklesDialogueVG and SparklesEval use the same sources of images and captions. This is suspected to be one of the reasons why model trained on SparklesDialogueVG performs better than model trained on SparklesDialogueCC. Isnlt this a serious issue, especially since SparklesDialogueVG is claimed to be the high quality subset?\n\nMinor typo\n1.\tTable 2: Column title should be A2 under “Turn two”", "labeling_timestamp": "2026-01-11T17:20:01.564581", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly states that GPT-4 was instructed to generate dialogues following a \"structured two-turn pattern,\" so the factual claim that samples are limited to two turns is supported (Section 4.1). However, the paper does not state or argue that this choice \"sacrifices longer-session practicality in favor of more images per turn\" or discuss such a trade-off, so that interpretive consequence is not supported by the text.", "evidence": "We instruct GPT-4 to simulate realistic and diverse dialogues between a user and an assistant with advanced image understanding and reasoning capabilities following a structured two-turn pattern. In the first turn, the user initiates a reasonable and creative message regarding some images. ... In the second turn, the user introduces a new image for further discussion, referencing both the new and previous images.", "section": "4.1 GPT-ASSISTED DATA CONSTRUCTION"}
{"claim": "The dataset is machine-generated rather than human-created, weakening its value as the paper's principal contribution.", "claim_type": "novelty", "paper_id": "oq5EF8parZ", "paper_title": "Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "WbH03mw5Tg", "reviewer": "Reviewer_eU8x", "review_text": "Summary: The paper introduces SparklesChat, a multimodal instruction following model for open-ended dialogues across multiple images. This is MiniGPT4 fine-tuned on the machine-generated dialogue dataset released in the paper called SparklesDialogue. This contains word-level interleaved multi-image and text interactions with up to 3 images during the first turn and 1 image during the second turn. SparklesDialogue consists of two subsets: 1) SparklesDialogueCC which contain images from CC3M and captions generated by MiniGPT4 2) SparklesDialogueVG which contain images from Visual Genome and descriptions from GPT-4, based on human-annotated captions, objects, and regions. SparklesEval is a new GPT-assisted benchmark with 150 dialogs, introduced to assess conversational competence across multiple images and dialogue turns, through criteria such as Image Understanding & Reasoning, Cross-Image & Cross-Turn Coherence, and Relevance & Completeness of Responses. SparklesChat outperforms MiniGPT-4 and gets marginally close GPT-4 on binary image selection task and the NLVR2 visual reasoning task. The paper contains ablation study on the effect of dialog turns and SparklesDialogue subsets during training.\n\n====\n\nUpdated final rating form 3 to 5 due to demonstration of improvement on LLaVA as well. The experiments section and total contributions are still weak.\n\nStrengths: 1.\tNew dataset SparklesDialogue for word-level interleaved multi-image and text interactions\n2.\tNew benchmark SparklesEval for word-level interleaved multi-image and text interactions\n3.\tDemonstration of improved performance over MiniGPT4\n\nWeaknesses: 1.\tSparklesDialogue contains subset SparklesDialogueVG, which was generated using GPT-4. The paper compares with performance of GPT-4 (method used to create the data set is also being evaluated on), while still performing worse although SparklesChat uses much richer image embedding. \n2.\tNo contribution in terms of novelty architecture. Main contribution is in the data set. \n3.\tOnly two turns per sample in the dataset. Longer sessions are probably more practical than more images per turn and limiting to just 2 turns. Dataset (that too, machine-generated) being the highlight of this paper, would have expected more.\n4.     Not clear how this extends to other approaches such as LLaVA. Results are shown only for Min-GPT4 extension.\n\nQuestions: Q1) Section 5.2 mentions, SparklesDialogueVG and SparklesEval use the same sources of images and captions. This is suspected to be one of the reasons why model trained on SparklesDialogueVG performs better than model trained on SparklesDialogueCC. Isnlt this a serious issue, especially since SparklesDialogueVG is claimed to be the high quality subset?\n\nMinor typo\n1.\tTable 2: Column title should be A2 under “Turn two”", "labeling_timestamp": "2026-01-11T17:20:17.078356", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper clearly states the dataset (SparklesDialogue) is machine-generated using GPT-4, so the factual part of the claim is supported. However, the reviewer's assertion that this 'weakens its value as the paper's principal contribution' is an opinion not asserted by the paper; the paper presents the machine-generated dataset as a core contribution and provides experimental results and usage to argue its usefulness.", "evidence": "1) \"To support the training, we introduce SparklesDialogue , the first machine-generated dialogue dataset tailored for word-level interleaved multi-image and text interactions.\" \n2) \"To achieve this, we use GPT-4 as the primary tool in our dialogue data generation, given its capabilities to follow complex instructions and extensive world knowledge.\" \n3) \"We instruct GPT-4 to simulate realistic and diverse dialogues between a user and an assistant with advanced image understanding and reasoning capabilities following a structured two-turn pattern.\" \n4) \"Our experiments validate the effectiveness of SparklesChat in understanding and reasoning across multiple images and dialogue turns.\"", "section": "Abstract; 4.1 GPT-ASSISTED DATA CONSTRUCTION; Abstract (experiments sentence)"}
{"claim": "Results are reported only for MiniGPT-4 extensions, lacking experiments that evaluate applicability to other approaches like LLaVA.", "claim_type": "baseline", "paper_id": "oq5EF8parZ", "paper_title": "Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "WbH03mw5Tg", "reviewer": "Reviewer_eU8x", "review_text": "Summary: The paper introduces SparklesChat, a multimodal instruction following model for open-ended dialogues across multiple images. This is MiniGPT4 fine-tuned on the machine-generated dialogue dataset released in the paper called SparklesDialogue. This contains word-level interleaved multi-image and text interactions with up to 3 images during the first turn and 1 image during the second turn. SparklesDialogue consists of two subsets: 1) SparklesDialogueCC which contain images from CC3M and captions generated by MiniGPT4 2) SparklesDialogueVG which contain images from Visual Genome and descriptions from GPT-4, based on human-annotated captions, objects, and regions. SparklesEval is a new GPT-assisted benchmark with 150 dialogs, introduced to assess conversational competence across multiple images and dialogue turns, through criteria such as Image Understanding & Reasoning, Cross-Image & Cross-Turn Coherence, and Relevance & Completeness of Responses. SparklesChat outperforms MiniGPT-4 and gets marginally close GPT-4 on binary image selection task and the NLVR2 visual reasoning task. The paper contains ablation study on the effect of dialog turns and SparklesDialogue subsets during training.\n\n====\n\nUpdated final rating form 3 to 5 due to demonstration of improvement on LLaVA as well. The experiments section and total contributions are still weak.\n\nStrengths: 1.\tNew dataset SparklesDialogue for word-level interleaved multi-image and text interactions\n2.\tNew benchmark SparklesEval for word-level interleaved multi-image and text interactions\n3.\tDemonstration of improved performance over MiniGPT4\n\nWeaknesses: 1.\tSparklesDialogue contains subset SparklesDialogueVG, which was generated using GPT-4. The paper compares with performance of GPT-4 (method used to create the data set is also being evaluated on), while still performing worse although SparklesChat uses much richer image embedding. \n2.\tNo contribution in terms of novelty architecture. Main contribution is in the data set. \n3.\tOnly two turns per sample in the dataset. Longer sessions are probably more practical than more images per turn and limiting to just 2 turns. Dataset (that too, machine-generated) being the highlight of this paper, would have expected more.\n4.     Not clear how this extends to other approaches such as LLaVA. Results are shown only for Min-GPT4 extension.\n\nQuestions: Q1) Section 5.2 mentions, SparklesDialogueVG and SparklesEval use the same sources of images and captions. This is suspected to be one of the reasons why model trained on SparklesDialogueVG performs better than model trained on SparklesDialogueCC. Isnlt this a serious issue, especially since SparklesDialogueVG is claimed to be the high quality subset?\n\nMinor typo\n1.\tTable 2: Column title should be A2 under “Turn two”", "labeling_timestamp": "2026-01-11T17:20:26.334024", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper's quantitative results and comparisons report SparklesChat against MiniGPT-4 (and GPT-4) on BISON, NLVR2, and SparklesEval; it does not present experimental evaluations or comparisons involving other multimodal approaches such as LLaVA.", "evidence": "\"Specifically, SparklesChat outperformed MiniGPT-4 on established vision-language benchmarks, including the BISON binary image selection task and the NLVR2 visual reasoning task. Moreover, SparklesChat scored 8.56 out of 10 on SparklesEval, substantially exceeding MiniGPT-4's score of 3.91 and nearing GPT-4's score of 9.26.\"", "section": "Abstract"}
{"claim": "The experiments section is weak and does not convincingly demonstrate the claimed improvements across diverse settings.", "claim_type": "subjective", "paper_id": "oq5EF8parZ", "paper_title": "Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "WbH03mw5Tg", "reviewer": "Reviewer_eU8x", "review_text": "Summary: The paper introduces SparklesChat, a multimodal instruction following model for open-ended dialogues across multiple images. This is MiniGPT4 fine-tuned on the machine-generated dialogue dataset released in the paper called SparklesDialogue. This contains word-level interleaved multi-image and text interactions with up to 3 images during the first turn and 1 image during the second turn. SparklesDialogue consists of two subsets: 1) SparklesDialogueCC which contain images from CC3M and captions generated by MiniGPT4 2) SparklesDialogueVG which contain images from Visual Genome and descriptions from GPT-4, based on human-annotated captions, objects, and regions. SparklesEval is a new GPT-assisted benchmark with 150 dialogs, introduced to assess conversational competence across multiple images and dialogue turns, through criteria such as Image Understanding & Reasoning, Cross-Image & Cross-Turn Coherence, and Relevance & Completeness of Responses. SparklesChat outperforms MiniGPT-4 and gets marginally close GPT-4 on binary image selection task and the NLVR2 visual reasoning task. The paper contains ablation study on the effect of dialog turns and SparklesDialogue subsets during training.\n\n====\n\nUpdated final rating form 3 to 5 due to demonstration of improvement on LLaVA as well. The experiments section and total contributions are still weak.\n\nStrengths: 1.\tNew dataset SparklesDialogue for word-level interleaved multi-image and text interactions\n2.\tNew benchmark SparklesEval for word-level interleaved multi-image and text interactions\n3.\tDemonstration of improved performance over MiniGPT4\n\nWeaknesses: 1.\tSparklesDialogue contains subset SparklesDialogueVG, which was generated using GPT-4. The paper compares with performance of GPT-4 (method used to create the data set is also being evaluated on), while still performing worse although SparklesChat uses much richer image embedding. \n2.\tNo contribution in terms of novelty architecture. Main contribution is in the data set. \n3.\tOnly two turns per sample in the dataset. Longer sessions are probably more practical than more images per turn and limiting to just 2 turns. Dataset (that too, machine-generated) being the highlight of this paper, would have expected more.\n4.     Not clear how this extends to other approaches such as LLaVA. Results are shown only for Min-GPT4 extension.\n\nQuestions: Q1) Section 5.2 mentions, SparklesDialogueVG and SparklesEval use the same sources of images and captions. This is suspected to be one of the reasons why model trained on SparklesDialogueVG performs better than model trained on SparklesDialogueCC. Isnlt this a serious issue, especially since SparklesDialogueVG is claimed to be the high quality subset?\n\nMinor typo\n1.\tTable 2: Column title should be A2 under “Turn two”", "labeling_timestamp": "2026-01-11T17:20:30.326152", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper reports quantitative improvements on two standard vision-language benchmarks (BISON and NLVR2) and on its own SparklesEval benchmark, which supports some claimed gains. However, the experiments appear limited in scope (only two established V-L tasks plus an internal benchmark) and the excerpt contains no detailed ablations, broader task diversity, multiple competing baselines beyond MiniGPT-4 and GPT-4, or uncertainty estimates—so the claim that the experiments are weak / not convincingly demonstrating improvements across diverse settings is partially true.", "evidence": "“For quantitative evaluation, we validate the effectiveness of SparklesChat through extensive experiments. We conduct zero-shot evaluations on two standard vision-language tasks, including binary image selection on the BISON dataset (Hu et al., 2019) and visual reasoning on the NLVR2 dataset (Suhr et al., 2019). On the BISON dataset, SparklesChat achieved an accuracy of 56.7%, surpassing MiniGPT-4's 46.0%. On the NLVR2 dataset, SparklesChat reached an accuracy of 58.0%, outperforming MiniGPT-4's 51.3%. In our SparklesEval benchmark, SparklesChat scores 8.56 out of 10, significantly exceeds MiniGPT-4's score of 3.91, and closely approaches GPT-4's score of 9.26.”", "section": "Abstract / 1 Introduction"}
{"claim": "The overall contributions of the paper are weak, relying primarily on dataset creation rather than substantial technical advances.", "claim_type": "novelty", "paper_id": "oq5EF8parZ", "paper_title": "Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "WbH03mw5Tg", "reviewer": "Reviewer_eU8x", "review_text": "Summary: The paper introduces SparklesChat, a multimodal instruction following model for open-ended dialogues across multiple images. This is MiniGPT4 fine-tuned on the machine-generated dialogue dataset released in the paper called SparklesDialogue. This contains word-level interleaved multi-image and text interactions with up to 3 images during the first turn and 1 image during the second turn. SparklesDialogue consists of two subsets: 1) SparklesDialogueCC which contain images from CC3M and captions generated by MiniGPT4 2) SparklesDialogueVG which contain images from Visual Genome and descriptions from GPT-4, based on human-annotated captions, objects, and regions. SparklesEval is a new GPT-assisted benchmark with 150 dialogs, introduced to assess conversational competence across multiple images and dialogue turns, through criteria such as Image Understanding & Reasoning, Cross-Image & Cross-Turn Coherence, and Relevance & Completeness of Responses. SparklesChat outperforms MiniGPT-4 and gets marginally close GPT-4 on binary image selection task and the NLVR2 visual reasoning task. The paper contains ablation study on the effect of dialog turns and SparklesDialogue subsets during training.\n\n====\n\nUpdated final rating form 3 to 5 due to demonstration of improvement on LLaVA as well. The experiments section and total contributions are still weak.\n\nStrengths: 1.\tNew dataset SparklesDialogue for word-level interleaved multi-image and text interactions\n2.\tNew benchmark SparklesEval for word-level interleaved multi-image and text interactions\n3.\tDemonstration of improved performance over MiniGPT4\n\nWeaknesses: 1.\tSparklesDialogue contains subset SparklesDialogueVG, which was generated using GPT-4. The paper compares with performance of GPT-4 (method used to create the data set is also being evaluated on), while still performing worse although SparklesChat uses much richer image embedding. \n2.\tNo contribution in terms of novelty architecture. Main contribution is in the data set. \n3.\tOnly two turns per sample in the dataset. Longer sessions are probably more practical than more images per turn and limiting to just 2 turns. Dataset (that too, machine-generated) being the highlight of this paper, would have expected more.\n4.     Not clear how this extends to other approaches such as LLaVA. Results are shown only for Min-GPT4 extension.\n\nQuestions: Q1) Section 5.2 mentions, SparklesDialogueVG and SparklesEval use the same sources of images and captions. This is suspected to be one of the reasons why model trained on SparklesDialogueVG performs better than model trained on SparklesDialogueCC. Isnlt this a serious issue, especially since SparklesDialogueVG is claimed to be the high quality subset?\n\nMinor typo\n1.\tTable 2: Column title should be A2 under “Turn two”", "labeling_timestamp": "2026-01-11T17:20:35.512971", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper indeed emphasizes dataset and benchmark contributions (SparklesDialogue and SparklesEval) as central to the work, which supports the reviewer's point about dataset reliance. However, the paper also claims a technical contribution—integrating multiple images at the word level within the model architecture (SparklesChat) and demonstrating empirical gains—so the claim that contributions are primarily dataset-only is only partially true.", "evidence": "Abstract: \"To support the training, we introduce SparklesDialogue , the first machine-generated dialogue dataset tailored for word-level interleaved multi-image and text interactions. Furthermore, we construct SparklesEval , a GPT-assisted benchmark for quantitatively assessing a model's conversational competence across multiple images and dialogue turns.\"; Introduction: \"SparklesChat, as shown in Figure 1, integrates multiple images at the word level (e.g., \\\"Can you link the celebration occurring in IMAGE#2331159 and the dirt bike race in IMAGE#2330601 ?\\\"). This innovation enables fine-grained integration of images and text, mimicking natural human communication more closely.\"; Section 3 (Architecture): \"Only the projection layer is trainable in the model while other vision and language components are frozen.\"", "section": "Abstract; 1 INTRODUCTION; 3 SPARKLESCHAT"}
{"claim": "SparklesDialogueVG and SparklesEval use the same image and caption sources, creating potential overlap between training and evaluation sets.", "claim_type": "methodology", "paper_id": "oq5EF8parZ", "paper_title": "Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "WbH03mw5Tg", "reviewer": "Reviewer_eU8x", "review_text": "Summary: The paper introduces SparklesChat, a multimodal instruction following model for open-ended dialogues across multiple images. This is MiniGPT4 fine-tuned on the machine-generated dialogue dataset released in the paper called SparklesDialogue. This contains word-level interleaved multi-image and text interactions with up to 3 images during the first turn and 1 image during the second turn. SparklesDialogue consists of two subsets: 1) SparklesDialogueCC which contain images from CC3M and captions generated by MiniGPT4 2) SparklesDialogueVG which contain images from Visual Genome and descriptions from GPT-4, based on human-annotated captions, objects, and regions. SparklesEval is a new GPT-assisted benchmark with 150 dialogs, introduced to assess conversational competence across multiple images and dialogue turns, through criteria such as Image Understanding & Reasoning, Cross-Image & Cross-Turn Coherence, and Relevance & Completeness of Responses. SparklesChat outperforms MiniGPT-4 and gets marginally close GPT-4 on binary image selection task and the NLVR2 visual reasoning task. The paper contains ablation study on the effect of dialog turns and SparklesDialogue subsets during training.\n\n====\n\nUpdated final rating form 3 to 5 due to demonstration of improvement on LLaVA as well. The experiments section and total contributions are still weak.\n\nStrengths: 1.\tNew dataset SparklesDialogue for word-level interleaved multi-image and text interactions\n2.\tNew benchmark SparklesEval for word-level interleaved multi-image and text interactions\n3.\tDemonstration of improved performance over MiniGPT4\n\nWeaknesses: 1.\tSparklesDialogue contains subset SparklesDialogueVG, which was generated using GPT-4. The paper compares with performance of GPT-4 (method used to create the data set is also being evaluated on), while still performing worse although SparklesChat uses much richer image embedding. \n2.\tNo contribution in terms of novelty architecture. Main contribution is in the data set. \n3.\tOnly two turns per sample in the dataset. Longer sessions are probably more practical than more images per turn and limiting to just 2 turns. Dataset (that too, machine-generated) being the highlight of this paper, would have expected more.\n4.     Not clear how this extends to other approaches such as LLaVA. Results are shown only for Min-GPT4 extension.\n\nQuestions: Q1) Section 5.2 mentions, SparklesDialogueVG and SparklesEval use the same sources of images and captions. This is suspected to be one of the reasons why model trained on SparklesDialogueVG performs better than model trained on SparklesDialogueCC. Isnlt this a serious issue, especially since SparklesDialogueVG is claimed to be the high quality subset?\n\nMinor typo\n1.\tTable 2: Column title should be A2 under “Turn two”", "labeling_timestamp": "2026-01-11T17:20:24.584875", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper clearly states the image sources used for SparklesDialogueVG (Visual Genome) but does not specify the image or caption sources used for SparklesEval or whether evaluation images overlap with training. Therefore there is insufficient information to confirm the claimed overlap.", "evidence": "1) \"The respective detailed descriptions, provided in MiniGPT-4 (Zhu et al., 2023a) and SVIT (Zhao et al., 2023), correspond to image sources from Conceptual Captions (CC) (Sharma et al., 2018) and Visual Genome (VG) (Krishna et al., 2017).\"  2) \"Furthermore, we construct SparklesEval , a GPT-assisted benchmark to quantitatively evaluate a model's conversational competence in multimodal, open-ended dialogues across multiple images and dialogue turns.\"", "section": "4.2 STATISTICS AND CHARACTERISTICS; 4 SPARKLESDIALOGUE AND SPARKLESEVAL / ABSTRACT"}
{"claim": "Shared sources between SparklesDialogueVG and SparklesEval may explain why the VG-trained model outperforms the CC-trained model.", "claim_type": "experimental", "paper_id": "oq5EF8parZ", "paper_title": "Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "WbH03mw5Tg", "reviewer": "Reviewer_eU8x", "review_text": "Summary: The paper introduces SparklesChat, a multimodal instruction following model for open-ended dialogues across multiple images. This is MiniGPT4 fine-tuned on the machine-generated dialogue dataset released in the paper called SparklesDialogue. This contains word-level interleaved multi-image and text interactions with up to 3 images during the first turn and 1 image during the second turn. SparklesDialogue consists of two subsets: 1) SparklesDialogueCC which contain images from CC3M and captions generated by MiniGPT4 2) SparklesDialogueVG which contain images from Visual Genome and descriptions from GPT-4, based on human-annotated captions, objects, and regions. SparklesEval is a new GPT-assisted benchmark with 150 dialogs, introduced to assess conversational competence across multiple images and dialogue turns, through criteria such as Image Understanding & Reasoning, Cross-Image & Cross-Turn Coherence, and Relevance & Completeness of Responses. SparklesChat outperforms MiniGPT-4 and gets marginally close GPT-4 on binary image selection task and the NLVR2 visual reasoning task. The paper contains ablation study on the effect of dialog turns and SparklesDialogue subsets during training.\n\n====\n\nUpdated final rating form 3 to 5 due to demonstration of improvement on LLaVA as well. The experiments section and total contributions are still weak.\n\nStrengths: 1.\tNew dataset SparklesDialogue for word-level interleaved multi-image and text interactions\n2.\tNew benchmark SparklesEval for word-level interleaved multi-image and text interactions\n3.\tDemonstration of improved performance over MiniGPT4\n\nWeaknesses: 1.\tSparklesDialogue contains subset SparklesDialogueVG, which was generated using GPT-4. The paper compares with performance of GPT-4 (method used to create the data set is also being evaluated on), while still performing worse although SparklesChat uses much richer image embedding. \n2.\tNo contribution in terms of novelty architecture. Main contribution is in the data set. \n3.\tOnly two turns per sample in the dataset. Longer sessions are probably more practical than more images per turn and limiting to just 2 turns. Dataset (that too, machine-generated) being the highlight of this paper, would have expected more.\n4.     Not clear how this extends to other approaches such as LLaVA. Results are shown only for Min-GPT4 extension.\n\nQuestions: Q1) Section 5.2 mentions, SparklesDialogueVG and SparklesEval use the same sources of images and captions. This is suspected to be one of the reasons why model trained on SparklesDialogueVG performs better than model trained on SparklesDialogueCC. Isnlt this a serious issue, especially since SparklesDialogueVG is claimed to be the high quality subset?\n\nMinor typo\n1.\tTable 2: Column title should be A2 under “Turn two”", "labeling_timestamp": "2026-01-11T17:20:37.813351", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper states SparklesDialogue has two subsets (VG and CC) sourced from Visual Genome and Conceptual Captions (Section 4.2) and describes using candidate image descriptions from various image-text datasets (Section 4.1). However, the paper does not state that SparklesEval shares sources with SparklesDialogueVG nor provides a comparison showing a VG-trained model outperforming a CC-trained model or attributing such a difference to shared sources. Therefore there is insufficient evidence in the paper to confirm the reviewer’s claim.", "evidence": "“We use the data construction process introduced in subsection 4.1 to collect a robust and diverse dataset named SparklesDialogue, which consists of two subsets: SparklesDialogueCC and SparklesDialogueVG. The respective detailed descriptions, provided in MiniGPT-4 (Zhu et al., 2023a) and SVIT (Zhao et al., 2023), correspond to image sources from Conceptual Captions (CC) (Sharma et al., 2018) and Visual Genome (VG) (Krishna et al., 2017).”", "section": "4.2 STATISTICS AND CHARACTERISTICS"}
{"claim": "Claiming SparklesDialogueVG as a high-quality subset is questionable given its potential overlap with the evaluation set.", "claim_type": "other", "paper_id": "oq5EF8parZ", "paper_title": "Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "WbH03mw5Tg", "reviewer": "Reviewer_eU8x", "review_text": "Summary: The paper introduces SparklesChat, a multimodal instruction following model for open-ended dialogues across multiple images. This is MiniGPT4 fine-tuned on the machine-generated dialogue dataset released in the paper called SparklesDialogue. This contains word-level interleaved multi-image and text interactions with up to 3 images during the first turn and 1 image during the second turn. SparklesDialogue consists of two subsets: 1) SparklesDialogueCC which contain images from CC3M and captions generated by MiniGPT4 2) SparklesDialogueVG which contain images from Visual Genome and descriptions from GPT-4, based on human-annotated captions, objects, and regions. SparklesEval is a new GPT-assisted benchmark with 150 dialogs, introduced to assess conversational competence across multiple images and dialogue turns, through criteria such as Image Understanding & Reasoning, Cross-Image & Cross-Turn Coherence, and Relevance & Completeness of Responses. SparklesChat outperforms MiniGPT-4 and gets marginally close GPT-4 on binary image selection task and the NLVR2 visual reasoning task. The paper contains ablation study on the effect of dialog turns and SparklesDialogue subsets during training.\n\n====\n\nUpdated final rating form 3 to 5 due to demonstration of improvement on LLaVA as well. The experiments section and total contributions are still weak.\n\nStrengths: 1.\tNew dataset SparklesDialogue for word-level interleaved multi-image and text interactions\n2.\tNew benchmark SparklesEval for word-level interleaved multi-image and text interactions\n3.\tDemonstration of improved performance over MiniGPT4\n\nWeaknesses: 1.\tSparklesDialogue contains subset SparklesDialogueVG, which was generated using GPT-4. The paper compares with performance of GPT-4 (method used to create the data set is also being evaluated on), while still performing worse although SparklesChat uses much richer image embedding. \n2.\tNo contribution in terms of novelty architecture. Main contribution is in the data set. \n3.\tOnly two turns per sample in the dataset. Longer sessions are probably more practical than more images per turn and limiting to just 2 turns. Dataset (that too, machine-generated) being the highlight of this paper, would have expected more.\n4.     Not clear how this extends to other approaches such as LLaVA. Results are shown only for Min-GPT4 extension.\n\nQuestions: Q1) Section 5.2 mentions, SparklesDialogueVG and SparklesEval use the same sources of images and captions. This is suspected to be one of the reasons why model trained on SparklesDialogueVG performs better than model trained on SparklesDialogueCC. Isnlt this a serious issue, especially since SparklesDialogueVG is claimed to be the high quality subset?\n\nMinor typo\n1.\tTable 2: Column title should be A2 under “Turn two”", "labeling_timestamp": "2026-01-11T17:20:41.602184", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper explicitly claims SparklesDialogueVG is high-quality (Section 4.2) but does not discuss dataset overlap with evaluation sets or any measures to detect/avoid overlap. Therefore the reviewer’s concern about potential overlap with the evaluation set is not addressed by the paper, so it cannot be confirmed or contradicted from the provided text.", "evidence": "“SparklesDialogueVG is of high quality as the VG image descriptions generated by GPT-4 benefit from human-annotated captions, objects,”", "section": "4.2 STATISTICS AND CHARACTERISTICS"}
{"claim": "Table 2 contains a typographical error: the column title under 'Turn two' should be labeled 'A2'.", "claim_type": "presentation", "paper_id": "oq5EF8parZ", "paper_title": "Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "WbH03mw5Tg", "reviewer": "Reviewer_eU8x", "review_text": "Summary: The paper introduces SparklesChat, a multimodal instruction following model for open-ended dialogues across multiple images. This is MiniGPT4 fine-tuned on the machine-generated dialogue dataset released in the paper called SparklesDialogue. This contains word-level interleaved multi-image and text interactions with up to 3 images during the first turn and 1 image during the second turn. SparklesDialogue consists of two subsets: 1) SparklesDialogueCC which contain images from CC3M and captions generated by MiniGPT4 2) SparklesDialogueVG which contain images from Visual Genome and descriptions from GPT-4, based on human-annotated captions, objects, and regions. SparklesEval is a new GPT-assisted benchmark with 150 dialogs, introduced to assess conversational competence across multiple images and dialogue turns, through criteria such as Image Understanding & Reasoning, Cross-Image & Cross-Turn Coherence, and Relevance & Completeness of Responses. SparklesChat outperforms MiniGPT-4 and gets marginally close GPT-4 on binary image selection task and the NLVR2 visual reasoning task. The paper contains ablation study on the effect of dialog turns and SparklesDialogue subsets during training.\n\n====\n\nUpdated final rating form 3 to 5 due to demonstration of improvement on LLaVA as well. The experiments section and total contributions are still weak.\n\nStrengths: 1.\tNew dataset SparklesDialogue for word-level interleaved multi-image and text interactions\n2.\tNew benchmark SparklesEval for word-level interleaved multi-image and text interactions\n3.\tDemonstration of improved performance over MiniGPT4\n\nWeaknesses: 1.\tSparklesDialogue contains subset SparklesDialogueVG, which was generated using GPT-4. The paper compares with performance of GPT-4 (method used to create the data set is also being evaluated on), while still performing worse although SparklesChat uses much richer image embedding. \n2.\tNo contribution in terms of novelty architecture. Main contribution is in the data set. \n3.\tOnly two turns per sample in the dataset. Longer sessions are probably more practical than more images per turn and limiting to just 2 turns. Dataset (that too, machine-generated) being the highlight of this paper, would have expected more.\n4.     Not clear how this extends to other approaches such as LLaVA. Results are shown only for Min-GPT4 extension.\n\nQuestions: Q1) Section 5.2 mentions, SparklesDialogueVG and SparklesEval use the same sources of images and captions. This is suspected to be one of the reasons why model trained on SparklesDialogueVG performs better than model trained on SparklesDialogueCC. Isnlt this a serious issue, especially since SparklesDialogueVG is claimed to be the high quality subset?\n\nMinor typo\n1.\tTable 2: Column title should be A2 under “Turn two”", "labeling_timestamp": "2026-01-11T17:20:40.739577", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not include Table 2 or its contents, so there is insufficient information to verify whether a typographical error exists in that table's 'Turn two' column title.", "evidence": "\"We instruct GPT-4 to simulate realistic and diverse dialogues between a user and an assistant with advanced image understanding and reasoning capabilities by following a structured two-turn pattern. In the first turn, the user initiates a reasonable and creative message regarding some images. In response, the assistant generates detailed answers... In the second turn, the user introduces a new image for further discussion, referencing both the new and previous images.\"", "section": "4.1 GPT-ASSISTED DATA CONSTRUCTION"}
{"claim": "The authors used a 95:5 train-test split on the MD22 dataset, making their results incomparable to baseline models that used different predefined splits.", "claim_type": "baseline", "paper_id": "Y4mBaZu4vy", "paper_title": "The Importance of Being Scalable: Improving the Speed and Accuracy of Neural Network Interatomic Potentials Across Chemical Domains", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Zh7uivbHf7", "reviewer": "Reviewer_wbCz", "review_text": "Summary: This paper introduces the Efficient Graph Attention Potential (EGAP), a new architecture for Neural Network Interatomic Potentials (NNIP) designed for scalability and efficiency. The authors investigate scaling strategies for NNIP models and propose a model that leverages optimized self-attention mechanisms. They evaluate EGAP on the MD22 and OC20 datasets, claiming state-of-the-art performance and improved efficiency compared to existing models.\n\nStrengths: 1. The paper addresses an important challenge in the field of NNIPs by focusing on scalability and efficiency.\n2. The presentation of the model illustration is clear and appealing, aiding in understanding the proposed architecture.\n3. The authors provide an investigation into scaling strategies for NNIP models, which could be valuable for future research in this area.\n\nWeaknesses: 1. Dataset split inconsistency: There is a significant issue with the dataset split on the MD22 dataset. The authors mention using a 95:5 train-test split, stating: \"We use an EGAP model with six blocks and 48M parameters on each system and evaluate the performance on the test set (train-test split 95:5).\" However, this deviates from the splits used in previous works [1]. In prior studies, the training data was predefined (e.g., the AT-AT target utilized 3k training data, which is 15% of the total data). The 95:5 split in previous works was applied to the training data for train-validation splitting, not for train-test splitting. For instance, the AT-AT target data was split into 2,850 train, 150 validation, and 17,001 test samples. By utilizing a 95:5 train-test split, the authors have made their results incomparable to baseline models.\n\n2. Inconsistent dataset usage: The authors claim, \"The total dataset was used in the training and evaluation of EGAP, but the other models only used a subset of the dataset. We do not anticipate a significant difference in the performance of the models because the dataset is composed of very similar equilibrium structures, and improvement from increasing dataset size saturates quickly.\" This statement is problematic for two reasons:\n   a) The baseline models used the remaining data as a test set, not just a subset of the training data. By including more of the dataset in their training data, the authors have reduced the size of the test set.\n   b) The authors have effectively used a different train/validation/test set compared to baseline models, rather than just using additional training data.\n   c) The claim about performance differences with dataset size is not supported by evidence in the paper.\n\n3. Limited experimental scope: Experiments on the OC20 dataset are conducted only on the 2M split of S2EF, rather than the larger S2EF-All or S2EF-All+MD splits. Given that the main motivation is efficiency, it would be more fitting to test on the larger set. This limitation significantly weakens the paper's claims about scalability and efficiency.\n\n4. Inconclusive performance difference: On the OC20 2M split, EGAP underperforms on the force metric while outperforming on the energy metric. Since there is a regulation between these metrics during training, prioritizing one over the other can result in these outcomes even for the same model. This renders the performance difference unconvincing.\n\n5. Lack of scalability experiments: The authors didn't provide any scalability experiments in terms of model size, which leaves their scalability claims unsupported. Without demonstrating how the model performs as its size increases, it's difficult to evaluate the true scalability of EGAP.\n\n6. Result discrepancies: The results presented don't align with those provided in [1]. For example, in [1], the Tetrasaccharide target for VisNet-LSRM is reported as 0.1055 energy (kcal/mol) and 0.0767 forces (kcal/mol/Å), which equals 4.574914 energy (meV) = 0.05258521839 (meV/atom) and 3.326028 force (meV). However, the authors report these as 0.044 (meV/atom) and 5.0 force (meV). These discrepancies raise concerns about the accuracy of the reported results and the fairness of comparisons.\n\n\n\n[1] Li, Y., Wang, Y., Huang, L., Yang, H., Wei, X., Zhang, J., Wang, T., Wang, Z., Shao, B., and Liu, T.Y. (2024). Long-short-range message-passing: A physics-informed framework to capture non-local interaction for scalable molecular dynamics simulation. In The Twelfth International Conference on Learning Representations.\n\nQuestions: 1. Why didn't the authors follow the same dataset split procedure used by baseline models, especially for the MD22 dataset?\n2. Can the authors provide scalability experiments showing how EGAP performs as model size increases?\n3. Why didn't the authors test EGAP on larger splits of the OC20 dataset (S2EF-All or S2EF-All+MD) to better demonstrate its efficiency advantages?\n4. Can the authors explain the discrepancies between their reported results for baseline models and those in the original papers, particularly for the MD22 dataset?\n5. How do the authors justify their claim that increasing dataset size wouldn't significantly affect model performance, given that they used a different train/test split than baseline models?", "labeling_timestamp": "2026-01-11T17:20:48.944473", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not mention the MD22 dataset nor any 95:5 train-test split. The paper lists other datasets (OC20, OC22, SPICE, MPTrj) but gives no information about MD22 splits, so there is insufficient evidence to confirm the claim.", "evidence": "EScAIP also achieves state-of-the-art performance on a wide range of datasets including catalysts (OC20 and OC22), molecules (SPICE), and materials (MPTrj).", "section": "Abstract (and Related Works: Datasets for NNIP training)"}
{"claim": "The 95:5 split was applied to train-test in this work rather than to train-validation as in prior studies, causing inconsistent dataset partitioning.", "claim_type": "methodology", "paper_id": "Y4mBaZu4vy", "paper_title": "The Importance of Being Scalable: Improving the Speed and Accuracy of Neural Network Interatomic Potentials Across Chemical Domains", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Zh7uivbHf7", "reviewer": "Reviewer_wbCz", "review_text": "Summary: This paper introduces the Efficient Graph Attention Potential (EGAP), a new architecture for Neural Network Interatomic Potentials (NNIP) designed for scalability and efficiency. The authors investigate scaling strategies for NNIP models and propose a model that leverages optimized self-attention mechanisms. They evaluate EGAP on the MD22 and OC20 datasets, claiming state-of-the-art performance and improved efficiency compared to existing models.\n\nStrengths: 1. The paper addresses an important challenge in the field of NNIPs by focusing on scalability and efficiency.\n2. The presentation of the model illustration is clear and appealing, aiding in understanding the proposed architecture.\n3. The authors provide an investigation into scaling strategies for NNIP models, which could be valuable for future research in this area.\n\nWeaknesses: 1. Dataset split inconsistency: There is a significant issue with the dataset split on the MD22 dataset. The authors mention using a 95:5 train-test split, stating: \"We use an EGAP model with six blocks and 48M parameters on each system and evaluate the performance on the test set (train-test split 95:5).\" However, this deviates from the splits used in previous works [1]. In prior studies, the training data was predefined (e.g., the AT-AT target utilized 3k training data, which is 15% of the total data). The 95:5 split in previous works was applied to the training data for train-validation splitting, not for train-test splitting. For instance, the AT-AT target data was split into 2,850 train, 150 validation, and 17,001 test samples. By utilizing a 95:5 train-test split, the authors have made their results incomparable to baseline models.\n\n2. Inconsistent dataset usage: The authors claim, \"The total dataset was used in the training and evaluation of EGAP, but the other models only used a subset of the dataset. We do not anticipate a significant difference in the performance of the models because the dataset is composed of very similar equilibrium structures, and improvement from increasing dataset size saturates quickly.\" This statement is problematic for two reasons:\n   a) The baseline models used the remaining data as a test set, not just a subset of the training data. By including more of the dataset in their training data, the authors have reduced the size of the test set.\n   b) The authors have effectively used a different train/validation/test set compared to baseline models, rather than just using additional training data.\n   c) The claim about performance differences with dataset size is not supported by evidence in the paper.\n\n3. Limited experimental scope: Experiments on the OC20 dataset are conducted only on the 2M split of S2EF, rather than the larger S2EF-All or S2EF-All+MD splits. Given that the main motivation is efficiency, it would be more fitting to test on the larger set. This limitation significantly weakens the paper's claims about scalability and efficiency.\n\n4. Inconclusive performance difference: On the OC20 2M split, EGAP underperforms on the force metric while outperforming on the energy metric. Since there is a regulation between these metrics during training, prioritizing one over the other can result in these outcomes even for the same model. This renders the performance difference unconvincing.\n\n5. Lack of scalability experiments: The authors didn't provide any scalability experiments in terms of model size, which leaves their scalability claims unsupported. Without demonstrating how the model performs as its size increases, it's difficult to evaluate the true scalability of EGAP.\n\n6. Result discrepancies: The results presented don't align with those provided in [1]. For example, in [1], the Tetrasaccharide target for VisNet-LSRM is reported as 0.1055 energy (kcal/mol) and 0.0767 forces (kcal/mol/Å), which equals 4.574914 energy (meV) = 0.05258521839 (meV/atom) and 3.326028 force (meV). However, the authors report these as 0.044 (meV/atom) and 5.0 force (meV). These discrepancies raise concerns about the accuracy of the reported results and the fairness of comparisons.\n\n\n\n[1] Li, Y., Wang, Y., Huang, L., Yang, H., Wei, X., Zhang, J., Wang, T., Wang, Z., Shao, B., and Liu, T.Y. (2024). Long-short-range message-passing: A physics-informed framework to capture non-local interaction for scalable molecular dynamics simulation. In The Twelfth International Conference on Learning Representations.\n\nQuestions: 1. Why didn't the authors follow the same dataset split procedure used by baseline models, especially for the MD22 dataset?\n2. Can the authors provide scalability experiments showing how EGAP performs as model size increases?\n3. Why didn't the authors test EGAP on larger splits of the OC20 dataset (S2EF-All or S2EF-All+MD) to better demonstrate its efficiency advantages?\n4. Can the authors explain the discrepancies between their reported results for baseline models and those in the original papers, particularly for the MD22 dataset?\n5. How do the authors justify their claim that increasing dataset size wouldn't significantly affect model performance, given that they used a different train/test split than baseline models?", "labeling_timestamp": "2026-01-11T17:20:57.582962", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does not report any dataset partition using a 95:5 split nor explicitly state whether a 95:5 split was applied to train-test versus train-validation. The text only refers to experiments on the OC20 2M split and to evaluation on a 'held-out validation set', but gives no information about a 95:5 partitioning or a change from prior studies.", "evidence": "“After training EScAIP on different datasets, we find that EScAIP accurately predicts the forces on the rotated systems, as indicated by high cosine similarity scores (≥0.999).” and “We conduct experiments using a leading NNIP architecture, the EquiformerV2 model [Liao et al., 2024], on the Open Catalyst 2020 (OC20) Dataset [Chanussot et al., 2021] 2M split to evaluate the performance of different scaling strategies.”", "section": "Abstract; Section 3 (Investigation on How to Scale Neural Network Interatomic Potentials)"}
{"claim": "Authors trained EGAP on the entire MD22 dataset while baseline models reserved substantial test sets, effectively reducing the authors' test set size.", "claim_type": "baseline", "paper_id": "Y4mBaZu4vy", "paper_title": "The Importance of Being Scalable: Improving the Speed and Accuracy of Neural Network Interatomic Potentials Across Chemical Domains", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Zh7uivbHf7", "reviewer": "Reviewer_wbCz", "review_text": "Summary: This paper introduces the Efficient Graph Attention Potential (EGAP), a new architecture for Neural Network Interatomic Potentials (NNIP) designed for scalability and efficiency. The authors investigate scaling strategies for NNIP models and propose a model that leverages optimized self-attention mechanisms. They evaluate EGAP on the MD22 and OC20 datasets, claiming state-of-the-art performance and improved efficiency compared to existing models.\n\nStrengths: 1. The paper addresses an important challenge in the field of NNIPs by focusing on scalability and efficiency.\n2. The presentation of the model illustration is clear and appealing, aiding in understanding the proposed architecture.\n3. The authors provide an investigation into scaling strategies for NNIP models, which could be valuable for future research in this area.\n\nWeaknesses: 1. Dataset split inconsistency: There is a significant issue with the dataset split on the MD22 dataset. The authors mention using a 95:5 train-test split, stating: \"We use an EGAP model with six blocks and 48M parameters on each system and evaluate the performance on the test set (train-test split 95:5).\" However, this deviates from the splits used in previous works [1]. In prior studies, the training data was predefined (e.g., the AT-AT target utilized 3k training data, which is 15% of the total data). The 95:5 split in previous works was applied to the training data for train-validation splitting, not for train-test splitting. For instance, the AT-AT target data was split into 2,850 train, 150 validation, and 17,001 test samples. By utilizing a 95:5 train-test split, the authors have made their results incomparable to baseline models.\n\n2. Inconsistent dataset usage: The authors claim, \"The total dataset was used in the training and evaluation of EGAP, but the other models only used a subset of the dataset. We do not anticipate a significant difference in the performance of the models because the dataset is composed of very similar equilibrium structures, and improvement from increasing dataset size saturates quickly.\" This statement is problematic for two reasons:\n   a) The baseline models used the remaining data as a test set, not just a subset of the training data. By including more of the dataset in their training data, the authors have reduced the size of the test set.\n   b) The authors have effectively used a different train/validation/test set compared to baseline models, rather than just using additional training data.\n   c) The claim about performance differences with dataset size is not supported by evidence in the paper.\n\n3. Limited experimental scope: Experiments on the OC20 dataset are conducted only on the 2M split of S2EF, rather than the larger S2EF-All or S2EF-All+MD splits. Given that the main motivation is efficiency, it would be more fitting to test on the larger set. This limitation significantly weakens the paper's claims about scalability and efficiency.\n\n4. Inconclusive performance difference: On the OC20 2M split, EGAP underperforms on the force metric while outperforming on the energy metric. Since there is a regulation between these metrics during training, prioritizing one over the other can result in these outcomes even for the same model. This renders the performance difference unconvincing.\n\n5. Lack of scalability experiments: The authors didn't provide any scalability experiments in terms of model size, which leaves their scalability claims unsupported. Without demonstrating how the model performs as its size increases, it's difficult to evaluate the true scalability of EGAP.\n\n6. Result discrepancies: The results presented don't align with those provided in [1]. For example, in [1], the Tetrasaccharide target for VisNet-LSRM is reported as 0.1055 energy (kcal/mol) and 0.0767 forces (kcal/mol/Å), which equals 4.574914 energy (meV) = 0.05258521839 (meV/atom) and 3.326028 force (meV). However, the authors report these as 0.044 (meV/atom) and 5.0 force (meV). These discrepancies raise concerns about the accuracy of the reported results and the fairness of comparisons.\n\n\n\n[1] Li, Y., Wang, Y., Huang, L., Yang, H., Wei, X., Zhang, J., Wang, T., Wang, Z., Shao, B., and Liu, T.Y. (2024). Long-short-range message-passing: A physics-informed framework to capture non-local interaction for scalable molecular dynamics simulation. In The Twelfth International Conference on Learning Representations.\n\nQuestions: 1. Why didn't the authors follow the same dataset split procedure used by baseline models, especially for the MD22 dataset?\n2. Can the authors provide scalability experiments showing how EGAP performs as model size increases?\n3. Why didn't the authors test EGAP on larger splits of the OC20 dataset (S2EF-All or S2EF-All+MD) to better demonstrate its efficiency advantages?\n4. Can the authors explain the discrepancies between their reported results for baseline models and those in the original papers, particularly for the MD22 dataset?\n5. How do the authors justify their claim that increasing dataset size wouldn't significantly affect model performance, given that they used a different train/test split than baseline models?", "labeling_timestamp": "2026-01-11T17:20:55.631869", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper text provided does not mention EGAP or the MD22 dataset, nor does it describe training on an entire MD22 dataset or how baseline models split MD22. Therefore there is insufficient information in the paper to confirm or refute the reviewer's claim.", "evidence": "\"EScAIP also achieves state-of-the-art performance on a wide range of datasets including catalysts (OC20 and OC22), molecules (SPICE), and materials (MPTrj).\"", "section": "Abstract"}
{"claim": "The authors used a different train/validation/test partitioning than baseline models instead of only adding extra training data, invalidating direct performance comparisons.", "claim_type": "baseline", "paper_id": "Y4mBaZu4vy", "paper_title": "The Importance of Being Scalable: Improving the Speed and Accuracy of Neural Network Interatomic Potentials Across Chemical Domains", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Zh7uivbHf7", "reviewer": "Reviewer_wbCz", "review_text": "Summary: This paper introduces the Efficient Graph Attention Potential (EGAP), a new architecture for Neural Network Interatomic Potentials (NNIP) designed for scalability and efficiency. The authors investigate scaling strategies for NNIP models and propose a model that leverages optimized self-attention mechanisms. They evaluate EGAP on the MD22 and OC20 datasets, claiming state-of-the-art performance and improved efficiency compared to existing models.\n\nStrengths: 1. The paper addresses an important challenge in the field of NNIPs by focusing on scalability and efficiency.\n2. The presentation of the model illustration is clear and appealing, aiding in understanding the proposed architecture.\n3. The authors provide an investigation into scaling strategies for NNIP models, which could be valuable for future research in this area.\n\nWeaknesses: 1. Dataset split inconsistency: There is a significant issue with the dataset split on the MD22 dataset. The authors mention using a 95:5 train-test split, stating: \"We use an EGAP model with six blocks and 48M parameters on each system and evaluate the performance on the test set (train-test split 95:5).\" However, this deviates from the splits used in previous works [1]. In prior studies, the training data was predefined (e.g., the AT-AT target utilized 3k training data, which is 15% of the total data). The 95:5 split in previous works was applied to the training data for train-validation splitting, not for train-test splitting. For instance, the AT-AT target data was split into 2,850 train, 150 validation, and 17,001 test samples. By utilizing a 95:5 train-test split, the authors have made their results incomparable to baseline models.\n\n2. Inconsistent dataset usage: The authors claim, \"The total dataset was used in the training and evaluation of EGAP, but the other models only used a subset of the dataset. We do not anticipate a significant difference in the performance of the models because the dataset is composed of very similar equilibrium structures, and improvement from increasing dataset size saturates quickly.\" This statement is problematic for two reasons:\n   a) The baseline models used the remaining data as a test set, not just a subset of the training data. By including more of the dataset in their training data, the authors have reduced the size of the test set.\n   b) The authors have effectively used a different train/validation/test set compared to baseline models, rather than just using additional training data.\n   c) The claim about performance differences with dataset size is not supported by evidence in the paper.\n\n3. Limited experimental scope: Experiments on the OC20 dataset are conducted only on the 2M split of S2EF, rather than the larger S2EF-All or S2EF-All+MD splits. Given that the main motivation is efficiency, it would be more fitting to test on the larger set. This limitation significantly weakens the paper's claims about scalability and efficiency.\n\n4. Inconclusive performance difference: On the OC20 2M split, EGAP underperforms on the force metric while outperforming on the energy metric. Since there is a regulation between these metrics during training, prioritizing one over the other can result in these outcomes even for the same model. This renders the performance difference unconvincing.\n\n5. Lack of scalability experiments: The authors didn't provide any scalability experiments in terms of model size, which leaves their scalability claims unsupported. Without demonstrating how the model performs as its size increases, it's difficult to evaluate the true scalability of EGAP.\n\n6. Result discrepancies: The results presented don't align with those provided in [1]. For example, in [1], the Tetrasaccharide target for VisNet-LSRM is reported as 0.1055 energy (kcal/mol) and 0.0767 forces (kcal/mol/Å), which equals 4.574914 energy (meV) = 0.05258521839 (meV/atom) and 3.326028 force (meV). However, the authors report these as 0.044 (meV/atom) and 5.0 force (meV). These discrepancies raise concerns about the accuracy of the reported results and the fairness of comparisons.\n\n\n\n[1] Li, Y., Wang, Y., Huang, L., Yang, H., Wei, X., Zhang, J., Wang, T., Wang, Z., Shao, B., and Liu, T.Y. (2024). Long-short-range message-passing: A physics-informed framework to capture non-local interaction for scalable molecular dynamics simulation. In The Twelfth International Conference on Learning Representations.\n\nQuestions: 1. Why didn't the authors follow the same dataset split procedure used by baseline models, especially for the MD22 dataset?\n2. Can the authors provide scalability experiments showing how EGAP performs as model size increases?\n3. Why didn't the authors test EGAP on larger splits of the OC20 dataset (S2EF-All or S2EF-All+MD) to better demonstrate its efficiency advantages?\n4. Can the authors explain the discrepancies between their reported results for baseline models and those in the original papers, particularly for the MD22 dataset?\n5. How do the authors justify their claim that increasing dataset size wouldn't significantly affect model performance, given that they used a different train/test split than baseline models?", "labeling_timestamp": "2026-01-11T17:21:06.304861", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does not state that the authors changed the train/validation/test partitions used by baseline models. It mentions using standard dataset splits (e.g., OC20 2M split) and reports comparisons to baselines, but provides no explicit information that a different partitioning was used instead of merely adding extra training data.", "evidence": "We conduct experiments using a leading NNIP architecture, the EquiformerV2 model [Liao et al., 2024], on the Open Catalyst 2020 (OC20) Dataset [Chanussot et al., 2021] 2M split to evaluate the performance of different scaling strategies.", "section": "3 Investigation on How to Scale Neural Network Interatomic Potentials"}
{"claim": "The authors assert that performance saturates quickly with dataset size but provide no experimental evidence or analyses to support this claim.", "claim_type": "experimental", "paper_id": "Y4mBaZu4vy", "paper_title": "The Importance of Being Scalable: Improving the Speed and Accuracy of Neural Network Interatomic Potentials Across Chemical Domains", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Zh7uivbHf7", "reviewer": "Reviewer_wbCz", "review_text": "Summary: This paper introduces the Efficient Graph Attention Potential (EGAP), a new architecture for Neural Network Interatomic Potentials (NNIP) designed for scalability and efficiency. The authors investigate scaling strategies for NNIP models and propose a model that leverages optimized self-attention mechanisms. They evaluate EGAP on the MD22 and OC20 datasets, claiming state-of-the-art performance and improved efficiency compared to existing models.\n\nStrengths: 1. The paper addresses an important challenge in the field of NNIPs by focusing on scalability and efficiency.\n2. The presentation of the model illustration is clear and appealing, aiding in understanding the proposed architecture.\n3. The authors provide an investigation into scaling strategies for NNIP models, which could be valuable for future research in this area.\n\nWeaknesses: 1. Dataset split inconsistency: There is a significant issue with the dataset split on the MD22 dataset. The authors mention using a 95:5 train-test split, stating: \"We use an EGAP model with six blocks and 48M parameters on each system and evaluate the performance on the test set (train-test split 95:5).\" However, this deviates from the splits used in previous works [1]. In prior studies, the training data was predefined (e.g., the AT-AT target utilized 3k training data, which is 15% of the total data). The 95:5 split in previous works was applied to the training data for train-validation splitting, not for train-test splitting. For instance, the AT-AT target data was split into 2,850 train, 150 validation, and 17,001 test samples. By utilizing a 95:5 train-test split, the authors have made their results incomparable to baseline models.\n\n2. Inconsistent dataset usage: The authors claim, \"The total dataset was used in the training and evaluation of EGAP, but the other models only used a subset of the dataset. We do not anticipate a significant difference in the performance of the models because the dataset is composed of very similar equilibrium structures, and improvement from increasing dataset size saturates quickly.\" This statement is problematic for two reasons:\n   a) The baseline models used the remaining data as a test set, not just a subset of the training data. By including more of the dataset in their training data, the authors have reduced the size of the test set.\n   b) The authors have effectively used a different train/validation/test set compared to baseline models, rather than just using additional training data.\n   c) The claim about performance differences with dataset size is not supported by evidence in the paper.\n\n3. Limited experimental scope: Experiments on the OC20 dataset are conducted only on the 2M split of S2EF, rather than the larger S2EF-All or S2EF-All+MD splits. Given that the main motivation is efficiency, it would be more fitting to test on the larger set. This limitation significantly weakens the paper's claims about scalability and efficiency.\n\n4. Inconclusive performance difference: On the OC20 2M split, EGAP underperforms on the force metric while outperforming on the energy metric. Since there is a regulation between these metrics during training, prioritizing one over the other can result in these outcomes even for the same model. This renders the performance difference unconvincing.\n\n5. Lack of scalability experiments: The authors didn't provide any scalability experiments in terms of model size, which leaves their scalability claims unsupported. Without demonstrating how the model performs as its size increases, it's difficult to evaluate the true scalability of EGAP.\n\n6. Result discrepancies: The results presented don't align with those provided in [1]. For example, in [1], the Tetrasaccharide target for VisNet-LSRM is reported as 0.1055 energy (kcal/mol) and 0.0767 forces (kcal/mol/Å), which equals 4.574914 energy (meV) = 0.05258521839 (meV/atom) and 3.326028 force (meV). However, the authors report these as 0.044 (meV/atom) and 5.0 force (meV). These discrepancies raise concerns about the accuracy of the reported results and the fairness of comparisons.\n\n\n\n[1] Li, Y., Wang, Y., Huang, L., Yang, H., Wei, X., Zhang, J., Wang, T., Wang, Z., Shao, B., and Liu, T.Y. (2024). Long-short-range message-passing: A physics-informed framework to capture non-local interaction for scalable molecular dynamics simulation. In The Twelfth International Conference on Learning Representations.\n\nQuestions: 1. Why didn't the authors follow the same dataset split procedure used by baseline models, especially for the MD22 dataset?\n2. Can the authors provide scalability experiments showing how EGAP performs as model size increases?\n3. Why didn't the authors test EGAP on larger splits of the OC20 dataset (S2EF-All or S2EF-All+MD) to better demonstrate its efficiency advantages?\n4. Can the authors explain the discrepancies between their reported results for baseline models and those in the original papers, particularly for the MD22 dataset?\n5. How do the authors justify their claim that increasing dataset size wouldn't significantly affect model performance, given that they used a different train/test split than baseline models?", "labeling_timestamp": "2026-01-11T17:21:25.417767", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper does not assert that performance saturates quickly with dataset size. Instead, it argues that imposing complex domain constraints may cause performance to plateau and presents ablation studies and evidence about scaling with model parameters and compute. Thus the reviewer's statement that the authors claim dataset-size saturation (and provide no supporting evidence) is not an accurate description of the paper.", "evidence": "We contend that these increasingly complex domain constraints inhibit the scaling ability of NNIPs, and such strategies are likely to cause model performance to plateau in the long run.", "section": "Introduction"}
{"claim": "Experiments on OC20 were performed only on the 2M S2EF split instead of larger S2EF-All or S2EF-All+MD splits, weakening scalability and efficiency claims.", "claim_type": "experimental", "paper_id": "Y4mBaZu4vy", "paper_title": "The Importance of Being Scalable: Improving the Speed and Accuracy of Neural Network Interatomic Potentials Across Chemical Domains", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Zh7uivbHf7", "reviewer": "Reviewer_wbCz", "review_text": "Summary: This paper introduces the Efficient Graph Attention Potential (EGAP), a new architecture for Neural Network Interatomic Potentials (NNIP) designed for scalability and efficiency. The authors investigate scaling strategies for NNIP models and propose a model that leverages optimized self-attention mechanisms. They evaluate EGAP on the MD22 and OC20 datasets, claiming state-of-the-art performance and improved efficiency compared to existing models.\n\nStrengths: 1. The paper addresses an important challenge in the field of NNIPs by focusing on scalability and efficiency.\n2. The presentation of the model illustration is clear and appealing, aiding in understanding the proposed architecture.\n3. The authors provide an investigation into scaling strategies for NNIP models, which could be valuable for future research in this area.\n\nWeaknesses: 1. Dataset split inconsistency: There is a significant issue with the dataset split on the MD22 dataset. The authors mention using a 95:5 train-test split, stating: \"We use an EGAP model with six blocks and 48M parameters on each system and evaluate the performance on the test set (train-test split 95:5).\" However, this deviates from the splits used in previous works [1]. In prior studies, the training data was predefined (e.g., the AT-AT target utilized 3k training data, which is 15% of the total data). The 95:5 split in previous works was applied to the training data for train-validation splitting, not for train-test splitting. For instance, the AT-AT target data was split into 2,850 train, 150 validation, and 17,001 test samples. By utilizing a 95:5 train-test split, the authors have made their results incomparable to baseline models.\n\n2. Inconsistent dataset usage: The authors claim, \"The total dataset was used in the training and evaluation of EGAP, but the other models only used a subset of the dataset. We do not anticipate a significant difference in the performance of the models because the dataset is composed of very similar equilibrium structures, and improvement from increasing dataset size saturates quickly.\" This statement is problematic for two reasons:\n   a) The baseline models used the remaining data as a test set, not just a subset of the training data. By including more of the dataset in their training data, the authors have reduced the size of the test set.\n   b) The authors have effectively used a different train/validation/test set compared to baseline models, rather than just using additional training data.\n   c) The claim about performance differences with dataset size is not supported by evidence in the paper.\n\n3. Limited experimental scope: Experiments on the OC20 dataset are conducted only on the 2M split of S2EF, rather than the larger S2EF-All or S2EF-All+MD splits. Given that the main motivation is efficiency, it would be more fitting to test on the larger set. This limitation significantly weakens the paper's claims about scalability and efficiency.\n\n4. Inconclusive performance difference: On the OC20 2M split, EGAP underperforms on the force metric while outperforming on the energy metric. Since there is a regulation between these metrics during training, prioritizing one over the other can result in these outcomes even for the same model. This renders the performance difference unconvincing.\n\n5. Lack of scalability experiments: The authors didn't provide any scalability experiments in terms of model size, which leaves their scalability claims unsupported. Without demonstrating how the model performs as its size increases, it's difficult to evaluate the true scalability of EGAP.\n\n6. Result discrepancies: The results presented don't align with those provided in [1]. For example, in [1], the Tetrasaccharide target for VisNet-LSRM is reported as 0.1055 energy (kcal/mol) and 0.0767 forces (kcal/mol/Å), which equals 4.574914 energy (meV) = 0.05258521839 (meV/atom) and 3.326028 force (meV). However, the authors report these as 0.044 (meV/atom) and 5.0 force (meV). These discrepancies raise concerns about the accuracy of the reported results and the fairness of comparisons.\n\n\n\n[1] Li, Y., Wang, Y., Huang, L., Yang, H., Wei, X., Zhang, J., Wang, T., Wang, Z., Shao, B., and Liu, T.Y. (2024). Long-short-range message-passing: A physics-informed framework to capture non-local interaction for scalable molecular dynamics simulation. In The Twelfth International Conference on Learning Representations.\n\nQuestions: 1. Why didn't the authors follow the same dataset split procedure used by baseline models, especially for the MD22 dataset?\n2. Can the authors provide scalability experiments showing how EGAP performs as model size increases?\n3. Why didn't the authors test EGAP on larger splits of the OC20 dataset (S2EF-All or S2EF-All+MD) to better demonstrate its efficiency advantages?\n4. Can the authors explain the discrepancies between their reported results for baseline models and those in the original papers, particularly for the MD22 dataset?\n5. How do the authors justify their claim that increasing dataset size wouldn't significantly affect model performance, given that they used a different train/test split than baseline models?", "labeling_timestamp": "2026-01-11T17:21:14.324687", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly states that the ablation experiments on OC20 were conducted on the 2M split and Figure 2 captions report results on the OC20 2M dataset, and the paper does not report experiments on the larger S2EF-All or S2EF-All+MD splits in the provided content. The reviewer's evaluative claim that this choice \"weakens scalability and efficiency claims\" is an interpretation not directly asserted or evaluated in the paper, so that part is not determined from the text.", "evidence": "We conduct experiments using a leading NNIP architecture, the EquiformerV2 model [Liao et al., 2024], on the Open Catalyst 2020 (OC20) Dataset [Chanussot et al., 2021] 2M split to evaluate the performance of different scaling strategies.\n\nFigure 2: Results of ablation study of EquiformerV2 [Liao et al., 2024] on the OC20 2M dataset.", "section": "3 Investigation on How to Scale Neural Network Interatomic Potentials (and Figure 2 caption)"}
{"claim": "On the OC20 2M split, EGAP underperforms on force metrics while outperforming on energy metrics, making the reported performance advantage inconclusive.", "claim_type": "experimental", "paper_id": "Y4mBaZu4vy", "paper_title": "The Importance of Being Scalable: Improving the Speed and Accuracy of Neural Network Interatomic Potentials Across Chemical Domains", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Zh7uivbHf7", "reviewer": "Reviewer_wbCz", "review_text": "Summary: This paper introduces the Efficient Graph Attention Potential (EGAP), a new architecture for Neural Network Interatomic Potentials (NNIP) designed for scalability and efficiency. The authors investigate scaling strategies for NNIP models and propose a model that leverages optimized self-attention mechanisms. They evaluate EGAP on the MD22 and OC20 datasets, claiming state-of-the-art performance and improved efficiency compared to existing models.\n\nStrengths: 1. The paper addresses an important challenge in the field of NNIPs by focusing on scalability and efficiency.\n2. The presentation of the model illustration is clear and appealing, aiding in understanding the proposed architecture.\n3. The authors provide an investigation into scaling strategies for NNIP models, which could be valuable for future research in this area.\n\nWeaknesses: 1. Dataset split inconsistency: There is a significant issue with the dataset split on the MD22 dataset. The authors mention using a 95:5 train-test split, stating: \"We use an EGAP model with six blocks and 48M parameters on each system and evaluate the performance on the test set (train-test split 95:5).\" However, this deviates from the splits used in previous works [1]. In prior studies, the training data was predefined (e.g., the AT-AT target utilized 3k training data, which is 15% of the total data). The 95:5 split in previous works was applied to the training data for train-validation splitting, not for train-test splitting. For instance, the AT-AT target data was split into 2,850 train, 150 validation, and 17,001 test samples. By utilizing a 95:5 train-test split, the authors have made their results incomparable to baseline models.\n\n2. Inconsistent dataset usage: The authors claim, \"The total dataset was used in the training and evaluation of EGAP, but the other models only used a subset of the dataset. We do not anticipate a significant difference in the performance of the models because the dataset is composed of very similar equilibrium structures, and improvement from increasing dataset size saturates quickly.\" This statement is problematic for two reasons:\n   a) The baseline models used the remaining data as a test set, not just a subset of the training data. By including more of the dataset in their training data, the authors have reduced the size of the test set.\n   b) The authors have effectively used a different train/validation/test set compared to baseline models, rather than just using additional training data.\n   c) The claim about performance differences with dataset size is not supported by evidence in the paper.\n\n3. Limited experimental scope: Experiments on the OC20 dataset are conducted only on the 2M split of S2EF, rather than the larger S2EF-All or S2EF-All+MD splits. Given that the main motivation is efficiency, it would be more fitting to test on the larger set. This limitation significantly weakens the paper's claims about scalability and efficiency.\n\n4. Inconclusive performance difference: On the OC20 2M split, EGAP underperforms on the force metric while outperforming on the energy metric. Since there is a regulation between these metrics during training, prioritizing one over the other can result in these outcomes even for the same model. This renders the performance difference unconvincing.\n\n5. Lack of scalability experiments: The authors didn't provide any scalability experiments in terms of model size, which leaves their scalability claims unsupported. Without demonstrating how the model performs as its size increases, it's difficult to evaluate the true scalability of EGAP.\n\n6. Result discrepancies: The results presented don't align with those provided in [1]. For example, in [1], the Tetrasaccharide target for VisNet-LSRM is reported as 0.1055 energy (kcal/mol) and 0.0767 forces (kcal/mol/Å), which equals 4.574914 energy (meV) = 0.05258521839 (meV/atom) and 3.326028 force (meV). However, the authors report these as 0.044 (meV/atom) and 5.0 force (meV). These discrepancies raise concerns about the accuracy of the reported results and the fairness of comparisons.\n\n\n\n[1] Li, Y., Wang, Y., Huang, L., Yang, H., Wei, X., Zhang, J., Wang, T., Wang, Z., Shao, B., and Liu, T.Y. (2024). Long-short-range message-passing: A physics-informed framework to capture non-local interaction for scalable molecular dynamics simulation. In The Twelfth International Conference on Learning Representations.\n\nQuestions: 1. Why didn't the authors follow the same dataset split procedure used by baseline models, especially for the MD22 dataset?\n2. Can the authors provide scalability experiments showing how EGAP performs as model size increases?\n3. Why didn't the authors test EGAP on larger splits of the OC20 dataset (S2EF-All or S2EF-All+MD) to better demonstrate its efficiency advantages?\n4. Can the authors explain the discrepancies between their reported results for baseline models and those in the original papers, particularly for the MD22 dataset?\n5. How do the authors justify their claim that increasing dataset size wouldn't significantly affect model performance, given that they used a different train/test split than baseline models?", "labeling_timestamp": "2026-01-11T17:21:20.427088", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does not mention a model named \"EGAP.\" It reports results for EScAIP and an ablation of EquiformerV2 on the OC20 2M split (showing energy and force MAEs), but provides no results or discussion for a model called EGAP, so the claim cannot be verified from this paper.", "evidence": "Abstract: \"EScAIPalso achieves state-of-the-art performance on a wide range of datasets including catalysts (OC20 and OC22), molecules (SPICE), and materials (MPTrj).\"; Figure 2 caption (Section 3): \"Results of ablation study of EquiformerV2 [Liao et al., 2024] on the OC20 2M dataset. Energy (eV) and force (eV/Å) mean absolute error (MAE) are reported, along with the model's parameter counts.\"", "section": "Abstract; Section 3 (Figure 2 caption)"}
{"claim": "Because training trades off energy and force predictions, prioritizing one metric can explain observed differences, so metric differences do not demonstrate overall superiority.", "claim_type": "methodology", "paper_id": "Y4mBaZu4vy", "paper_title": "The Importance of Being Scalable: Improving the Speed and Accuracy of Neural Network Interatomic Potentials Across Chemical Domains", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Zh7uivbHf7", "reviewer": "Reviewer_wbCz", "review_text": "Summary: This paper introduces the Efficient Graph Attention Potential (EGAP), a new architecture for Neural Network Interatomic Potentials (NNIP) designed for scalability and efficiency. The authors investigate scaling strategies for NNIP models and propose a model that leverages optimized self-attention mechanisms. They evaluate EGAP on the MD22 and OC20 datasets, claiming state-of-the-art performance and improved efficiency compared to existing models.\n\nStrengths: 1. The paper addresses an important challenge in the field of NNIPs by focusing on scalability and efficiency.\n2. The presentation of the model illustration is clear and appealing, aiding in understanding the proposed architecture.\n3. The authors provide an investigation into scaling strategies for NNIP models, which could be valuable for future research in this area.\n\nWeaknesses: 1. Dataset split inconsistency: There is a significant issue with the dataset split on the MD22 dataset. The authors mention using a 95:5 train-test split, stating: \"We use an EGAP model with six blocks and 48M parameters on each system and evaluate the performance on the test set (train-test split 95:5).\" However, this deviates from the splits used in previous works [1]. In prior studies, the training data was predefined (e.g., the AT-AT target utilized 3k training data, which is 15% of the total data). The 95:5 split in previous works was applied to the training data for train-validation splitting, not for train-test splitting. For instance, the AT-AT target data was split into 2,850 train, 150 validation, and 17,001 test samples. By utilizing a 95:5 train-test split, the authors have made their results incomparable to baseline models.\n\n2. Inconsistent dataset usage: The authors claim, \"The total dataset was used in the training and evaluation of EGAP, but the other models only used a subset of the dataset. We do not anticipate a significant difference in the performance of the models because the dataset is composed of very similar equilibrium structures, and improvement from increasing dataset size saturates quickly.\" This statement is problematic for two reasons:\n   a) The baseline models used the remaining data as a test set, not just a subset of the training data. By including more of the dataset in their training data, the authors have reduced the size of the test set.\n   b) The authors have effectively used a different train/validation/test set compared to baseline models, rather than just using additional training data.\n   c) The claim about performance differences with dataset size is not supported by evidence in the paper.\n\n3. Limited experimental scope: Experiments on the OC20 dataset are conducted only on the 2M split of S2EF, rather than the larger S2EF-All or S2EF-All+MD splits. Given that the main motivation is efficiency, it would be more fitting to test on the larger set. This limitation significantly weakens the paper's claims about scalability and efficiency.\n\n4. Inconclusive performance difference: On the OC20 2M split, EGAP underperforms on the force metric while outperforming on the energy metric. Since there is a regulation between these metrics during training, prioritizing one over the other can result in these outcomes even for the same model. This renders the performance difference unconvincing.\n\n5. Lack of scalability experiments: The authors didn't provide any scalability experiments in terms of model size, which leaves their scalability claims unsupported. Without demonstrating how the model performs as its size increases, it's difficult to evaluate the true scalability of EGAP.\n\n6. Result discrepancies: The results presented don't align with those provided in [1]. For example, in [1], the Tetrasaccharide target for VisNet-LSRM is reported as 0.1055 energy (kcal/mol) and 0.0767 forces (kcal/mol/Å), which equals 4.574914 energy (meV) = 0.05258521839 (meV/atom) and 3.326028 force (meV). However, the authors report these as 0.044 (meV/atom) and 5.0 force (meV). These discrepancies raise concerns about the accuracy of the reported results and the fairness of comparisons.\n\n\n\n[1] Li, Y., Wang, Y., Huang, L., Yang, H., Wei, X., Zhang, J., Wang, T., Wang, Z., Shao, B., and Liu, T.Y. (2024). Long-short-range message-passing: A physics-informed framework to capture non-local interaction for scalable molecular dynamics simulation. In The Twelfth International Conference on Learning Representations.\n\nQuestions: 1. Why didn't the authors follow the same dataset split procedure used by baseline models, especially for the MD22 dataset?\n2. Can the authors provide scalability experiments showing how EGAP performs as model size increases?\n3. Why didn't the authors test EGAP on larger splits of the OC20 dataset (S2EF-All or S2EF-All+MD) to better demonstrate its efficiency advantages?\n4. Can the authors explain the discrepancies between their reported results for baseline models and those in the original papers, particularly for the MD22 dataset?\n5. How do the authors justify their claim that increasing dataset size wouldn't significantly affect model performance, given that they used a different train/test split than baseline models?", "labeling_timestamp": "2026-01-11T17:21:28.528096", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper reports both energy and force MAE and compares models (see Figure 2 and Fig.1 captions), but it does not state or analyze that training involves an explicit trade-off between optimizing energy versus forces or that prioritizing one metric explains observed differences. There is no discussion in the provided text attributing metric differences to such a trade-off.", "evidence": "\"These models are usually trained to predict the system energy and per-atom force based on system properties, including atomic numbers and positions.\"; \"Energy (eV) and force (eV/Å) mean absolute error (MAE) are reported, along with the model's parameter counts.\"; \"Results with Energy MAE can be found in the Appendix Fig. 7.\"", "section": "Related Works; Figure 2 caption; Figure 1 caption"}
{"claim": "The authors did not provide experiments varying model size or parameter count, so their scalability claims lack empirical support.", "claim_type": "experimental", "paper_id": "Y4mBaZu4vy", "paper_title": "The Importance of Being Scalable: Improving the Speed and Accuracy of Neural Network Interatomic Potentials Across Chemical Domains", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Zh7uivbHf7", "reviewer": "Reviewer_wbCz", "review_text": "Summary: This paper introduces the Efficient Graph Attention Potential (EGAP), a new architecture for Neural Network Interatomic Potentials (NNIP) designed for scalability and efficiency. The authors investigate scaling strategies for NNIP models and propose a model that leverages optimized self-attention mechanisms. They evaluate EGAP on the MD22 and OC20 datasets, claiming state-of-the-art performance and improved efficiency compared to existing models.\n\nStrengths: 1. The paper addresses an important challenge in the field of NNIPs by focusing on scalability and efficiency.\n2. The presentation of the model illustration is clear and appealing, aiding in understanding the proposed architecture.\n3. The authors provide an investigation into scaling strategies for NNIP models, which could be valuable for future research in this area.\n\nWeaknesses: 1. Dataset split inconsistency: There is a significant issue with the dataset split on the MD22 dataset. The authors mention using a 95:5 train-test split, stating: \"We use an EGAP model with six blocks and 48M parameters on each system and evaluate the performance on the test set (train-test split 95:5).\" However, this deviates from the splits used in previous works [1]. In prior studies, the training data was predefined (e.g., the AT-AT target utilized 3k training data, which is 15% of the total data). The 95:5 split in previous works was applied to the training data for train-validation splitting, not for train-test splitting. For instance, the AT-AT target data was split into 2,850 train, 150 validation, and 17,001 test samples. By utilizing a 95:5 train-test split, the authors have made their results incomparable to baseline models.\n\n2. Inconsistent dataset usage: The authors claim, \"The total dataset was used in the training and evaluation of EGAP, but the other models only used a subset of the dataset. We do not anticipate a significant difference in the performance of the models because the dataset is composed of very similar equilibrium structures, and improvement from increasing dataset size saturates quickly.\" This statement is problematic for two reasons:\n   a) The baseline models used the remaining data as a test set, not just a subset of the training data. By including more of the dataset in their training data, the authors have reduced the size of the test set.\n   b) The authors have effectively used a different train/validation/test set compared to baseline models, rather than just using additional training data.\n   c) The claim about performance differences with dataset size is not supported by evidence in the paper.\n\n3. Limited experimental scope: Experiments on the OC20 dataset are conducted only on the 2M split of S2EF, rather than the larger S2EF-All or S2EF-All+MD splits. Given that the main motivation is efficiency, it would be more fitting to test on the larger set. This limitation significantly weakens the paper's claims about scalability and efficiency.\n\n4. Inconclusive performance difference: On the OC20 2M split, EGAP underperforms on the force metric while outperforming on the energy metric. Since there is a regulation between these metrics during training, prioritizing one over the other can result in these outcomes even for the same model. This renders the performance difference unconvincing.\n\n5. Lack of scalability experiments: The authors didn't provide any scalability experiments in terms of model size, which leaves their scalability claims unsupported. Without demonstrating how the model performs as its size increases, it's difficult to evaluate the true scalability of EGAP.\n\n6. Result discrepancies: The results presented don't align with those provided in [1]. For example, in [1], the Tetrasaccharide target for VisNet-LSRM is reported as 0.1055 energy (kcal/mol) and 0.0767 forces (kcal/mol/Å), which equals 4.574914 energy (meV) = 0.05258521839 (meV/atom) and 3.326028 force (meV). However, the authors report these as 0.044 (meV/atom) and 5.0 force (meV). These discrepancies raise concerns about the accuracy of the reported results and the fairness of comparisons.\n\n\n\n[1] Li, Y., Wang, Y., Huang, L., Yang, H., Wei, X., Zhang, J., Wang, T., Wang, Z., Shao, B., and Liu, T.Y. (2024). Long-short-range message-passing: A physics-informed framework to capture non-local interaction for scalable molecular dynamics simulation. In The Twelfth International Conference on Learning Representations.\n\nQuestions: 1. Why didn't the authors follow the same dataset split procedure used by baseline models, especially for the MD22 dataset?\n2. Can the authors provide scalability experiments showing how EGAP performs as model size increases?\n3. Why didn't the authors test EGAP on larger splits of the OC20 dataset (S2EF-All or S2EF-All+MD) to better demonstrate its efficiency advantages?\n4. Can the authors explain the discrepancies between their reported results for baseline models and those in the original papers, particularly for the MD22 dataset?\n5. How do the authors justify their claim that increasing dataset size wouldn't significantly affect model performance, given that they used a different train/test split than baseline models?", "labeling_timestamp": "2026-01-11T17:21:26.945581", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper reports an ablation study that explicitly varies how parameters are added (e.g., via attention vs. spherical channels) and controls for total parameter counts to evaluate scaling; thus the authors did provide experiments varying model size/parameter count to support their scaling claims.", "evidence": "“To answer this question, we conduct an initial ablation study to identify which components in NNIPs are most conducive to scaling.”\n\n“Figure 2: Results of ablation study of EquiformerV2 [Liao et al., 2024] on the OC20 2M dataset... We look at scaling parameters through the attention mechanisms ( AT ) and spherical channels ( SC ) for the original L =2 and L =4 models, such that the number of parameters is approximately equal to the original L =6 model. Scaling parameters in different ways affects the overall energy and forces error, and increasing attention parameters is particularly effective in improving model performance ( More AT ). We also modify the architecture to be invariant ( L =0 ), allowing us to examine the effects of excluding rotational equivariance while controlling for the number of parameters ( Invariant BOO ). After controlling for parameter counts, many of the models have comparable error to the original L =6 model.”", "section": "1 Introduction; 3 Investigation on How to Scale Neural Network Interatomic Potentials (Figure 2 / 3.1)"}
{"claim": "The paper reports baseline results for VisNet-LSRM Tetrasaccharide that do not match values in Li et al. (2024), indicating numerical discrepancies.", "claim_type": "baseline", "paper_id": "Y4mBaZu4vy", "paper_title": "The Importance of Being Scalable: Improving the Speed and Accuracy of Neural Network Interatomic Potentials Across Chemical Domains", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Zh7uivbHf7", "reviewer": "Reviewer_wbCz", "review_text": "Summary: This paper introduces the Efficient Graph Attention Potential (EGAP), a new architecture for Neural Network Interatomic Potentials (NNIP) designed for scalability and efficiency. The authors investigate scaling strategies for NNIP models and propose a model that leverages optimized self-attention mechanisms. They evaluate EGAP on the MD22 and OC20 datasets, claiming state-of-the-art performance and improved efficiency compared to existing models.\n\nStrengths: 1. The paper addresses an important challenge in the field of NNIPs by focusing on scalability and efficiency.\n2. The presentation of the model illustration is clear and appealing, aiding in understanding the proposed architecture.\n3. The authors provide an investigation into scaling strategies for NNIP models, which could be valuable for future research in this area.\n\nWeaknesses: 1. Dataset split inconsistency: There is a significant issue with the dataset split on the MD22 dataset. The authors mention using a 95:5 train-test split, stating: \"We use an EGAP model with six blocks and 48M parameters on each system and evaluate the performance on the test set (train-test split 95:5).\" However, this deviates from the splits used in previous works [1]. In prior studies, the training data was predefined (e.g., the AT-AT target utilized 3k training data, which is 15% of the total data). The 95:5 split in previous works was applied to the training data for train-validation splitting, not for train-test splitting. For instance, the AT-AT target data was split into 2,850 train, 150 validation, and 17,001 test samples. By utilizing a 95:5 train-test split, the authors have made their results incomparable to baseline models.\n\n2. Inconsistent dataset usage: The authors claim, \"The total dataset was used in the training and evaluation of EGAP, but the other models only used a subset of the dataset. We do not anticipate a significant difference in the performance of the models because the dataset is composed of very similar equilibrium structures, and improvement from increasing dataset size saturates quickly.\" This statement is problematic for two reasons:\n   a) The baseline models used the remaining data as a test set, not just a subset of the training data. By including more of the dataset in their training data, the authors have reduced the size of the test set.\n   b) The authors have effectively used a different train/validation/test set compared to baseline models, rather than just using additional training data.\n   c) The claim about performance differences with dataset size is not supported by evidence in the paper.\n\n3. Limited experimental scope: Experiments on the OC20 dataset are conducted only on the 2M split of S2EF, rather than the larger S2EF-All or S2EF-All+MD splits. Given that the main motivation is efficiency, it would be more fitting to test on the larger set. This limitation significantly weakens the paper's claims about scalability and efficiency.\n\n4. Inconclusive performance difference: On the OC20 2M split, EGAP underperforms on the force metric while outperforming on the energy metric. Since there is a regulation between these metrics during training, prioritizing one over the other can result in these outcomes even for the same model. This renders the performance difference unconvincing.\n\n5. Lack of scalability experiments: The authors didn't provide any scalability experiments in terms of model size, which leaves their scalability claims unsupported. Without demonstrating how the model performs as its size increases, it's difficult to evaluate the true scalability of EGAP.\n\n6. Result discrepancies: The results presented don't align with those provided in [1]. For example, in [1], the Tetrasaccharide target for VisNet-LSRM is reported as 0.1055 energy (kcal/mol) and 0.0767 forces (kcal/mol/Å), which equals 4.574914 energy (meV) = 0.05258521839 (meV/atom) and 3.326028 force (meV). However, the authors report these as 0.044 (meV/atom) and 5.0 force (meV). These discrepancies raise concerns about the accuracy of the reported results and the fairness of comparisons.\n\n\n\n[1] Li, Y., Wang, Y., Huang, L., Yang, H., Wei, X., Zhang, J., Wang, T., Wang, Z., Shao, B., and Liu, T.Y. (2024). Long-short-range message-passing: A physics-informed framework to capture non-local interaction for scalable molecular dynamics simulation. In The Twelfth International Conference on Learning Representations.\n\nQuestions: 1. Why didn't the authors follow the same dataset split procedure used by baseline models, especially for the MD22 dataset?\n2. Can the authors provide scalability experiments showing how EGAP performs as model size increases?\n3. Why didn't the authors test EGAP on larger splits of the OC20 dataset (S2EF-All or S2EF-All+MD) to better demonstrate its efficiency advantages?\n4. Can the authors explain the discrepancies between their reported results for baseline models and those in the original papers, particularly for the MD22 dataset?\n5. How do the authors justify their claim that increasing dataset size wouldn't significantly affect model performance, given that they used a different train/test split than baseline models?", "labeling_timestamp": "2026-01-11T17:21:41.614049", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper text provided makes no reference to VisNet-LSRM, Tetrasaccharide, or Li et al. (2024). The manuscript focuses on EScAIP, scaling studies, and datasets such as OC20/OC22, SPICE, and MPTrj, so there is insufficient information to verify the reviewer’s claim about baseline numbers for VisNet-LSRM Tetrasaccharide or discrepancies with Li et al. (2024).", "evidence": "\"EScAIPalso achieves state-of-the-art performance on a wide range of datasets including catalysts (OC20 and OC22), molecules (SPICE), and materials (MPTrj).\"", "section": "Abstract"}
{"claim": "Discrepancies between the authors' reported baseline numbers and the original papers raise concerns about the accuracy and fairness of the reported comparisons.", "claim_type": "baseline", "paper_id": "Y4mBaZu4vy", "paper_title": "The Importance of Being Scalable: Improving the Speed and Accuracy of Neural Network Interatomic Potentials Across Chemical Domains", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "Zh7uivbHf7", "reviewer": "Reviewer_wbCz", "review_text": "Summary: This paper introduces the Efficient Graph Attention Potential (EGAP), a new architecture for Neural Network Interatomic Potentials (NNIP) designed for scalability and efficiency. The authors investigate scaling strategies for NNIP models and propose a model that leverages optimized self-attention mechanisms. They evaluate EGAP on the MD22 and OC20 datasets, claiming state-of-the-art performance and improved efficiency compared to existing models.\n\nStrengths: 1. The paper addresses an important challenge in the field of NNIPs by focusing on scalability and efficiency.\n2. The presentation of the model illustration is clear and appealing, aiding in understanding the proposed architecture.\n3. The authors provide an investigation into scaling strategies for NNIP models, which could be valuable for future research in this area.\n\nWeaknesses: 1. Dataset split inconsistency: There is a significant issue with the dataset split on the MD22 dataset. The authors mention using a 95:5 train-test split, stating: \"We use an EGAP model with six blocks and 48M parameters on each system and evaluate the performance on the test set (train-test split 95:5).\" However, this deviates from the splits used in previous works [1]. In prior studies, the training data was predefined (e.g., the AT-AT target utilized 3k training data, which is 15% of the total data). The 95:5 split in previous works was applied to the training data for train-validation splitting, not for train-test splitting. For instance, the AT-AT target data was split into 2,850 train, 150 validation, and 17,001 test samples. By utilizing a 95:5 train-test split, the authors have made their results incomparable to baseline models.\n\n2. Inconsistent dataset usage: The authors claim, \"The total dataset was used in the training and evaluation of EGAP, but the other models only used a subset of the dataset. We do not anticipate a significant difference in the performance of the models because the dataset is composed of very similar equilibrium structures, and improvement from increasing dataset size saturates quickly.\" This statement is problematic for two reasons:\n   a) The baseline models used the remaining data as a test set, not just a subset of the training data. By including more of the dataset in their training data, the authors have reduced the size of the test set.\n   b) The authors have effectively used a different train/validation/test set compared to baseline models, rather than just using additional training data.\n   c) The claim about performance differences with dataset size is not supported by evidence in the paper.\n\n3. Limited experimental scope: Experiments on the OC20 dataset are conducted only on the 2M split of S2EF, rather than the larger S2EF-All or S2EF-All+MD splits. Given that the main motivation is efficiency, it would be more fitting to test on the larger set. This limitation significantly weakens the paper's claims about scalability and efficiency.\n\n4. Inconclusive performance difference: On the OC20 2M split, EGAP underperforms on the force metric while outperforming on the energy metric. Since there is a regulation between these metrics during training, prioritizing one over the other can result in these outcomes even for the same model. This renders the performance difference unconvincing.\n\n5. Lack of scalability experiments: The authors didn't provide any scalability experiments in terms of model size, which leaves their scalability claims unsupported. Without demonstrating how the model performs as its size increases, it's difficult to evaluate the true scalability of EGAP.\n\n6. Result discrepancies: The results presented don't align with those provided in [1]. For example, in [1], the Tetrasaccharide target for VisNet-LSRM is reported as 0.1055 energy (kcal/mol) and 0.0767 forces (kcal/mol/Å), which equals 4.574914 energy (meV) = 0.05258521839 (meV/atom) and 3.326028 force (meV). However, the authors report these as 0.044 (meV/atom) and 5.0 force (meV). These discrepancies raise concerns about the accuracy of the reported results and the fairness of comparisons.\n\n\n\n[1] Li, Y., Wang, Y., Huang, L., Yang, H., Wei, X., Zhang, J., Wang, T., Wang, Z., Shao, B., and Liu, T.Y. (2024). Long-short-range message-passing: A physics-informed framework to capture non-local interaction for scalable molecular dynamics simulation. In The Twelfth International Conference on Learning Representations.\n\nQuestions: 1. Why didn't the authors follow the same dataset split procedure used by baseline models, especially for the MD22 dataset?\n2. Can the authors provide scalability experiments showing how EGAP performs as model size increases?\n3. Why didn't the authors test EGAP on larger splits of the OC20 dataset (S2EF-All or S2EF-All+MD) to better demonstrate its efficiency advantages?\n4. Can the authors explain the discrepancies between their reported results for baseline models and those in the original papers, particularly for the MD22 dataset?\n5. How do the authors justify their claim that increasing dataset size wouldn't significantly affect model performance, given that they used a different train/test split than baseline models?", "labeling_timestamp": "2026-01-11T17:21:45.882940", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper reports baseline comparisons and ablations (e.g., Abstract, Section 3 and Figure 2) but does not mention or document any discrepancies between its reported baseline numbers and those in the original papers. Because the provided content neither acknowledges nor shows evidence of mismatched baseline values, there is insufficient information to verify the reviewer's claim.", "evidence": "Abstract: \"EScAIP also achieves state-of-the-art performance on a wide range of datasets including catalysts (OC20 and OC22), molecules (SPICE), and materials (MPTrj).\"; Figure 2 caption / Section 3: \"The leftmost column shows the original results from [Liao et al., 2024], where different L had a different number of trainable parameters. ... After controlling for parameter counts, many of the models have comparable error to the original L =6 model.\"", "section": "Abstract; Section 3 (Figure 2)"}
{"claim": "The authors' interpretation of SDA [1] is incorrect and misrepresents how SDA constructs intermediate posteriors and uses likelihood functions.", "claim_type": "methodology", "paper_id": "BOrut7M2X7", "paper_title": "Divide-and-Conquer Posterior Sampling for Denoising Diffusion priors", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "IgYyLGCtuC", "reviewer": "Reviewer_af3x", "review_text": "Comment: Thank you for your response and taking the time to implement [1].\n\nI do not agree with your interpretation of SDA [1]. Even though $\\hat{p}(y \\mid x_t)$ is an approximation of $p(y \\mid x_t)$, it is also a likelihood function and therefore defines a series of intermediate posteriors $\\hat{p}(x_t \\mid y)$. The DDIM step (predictor) is an approximation of the transition from $\\hat{p}(x_t \\mid y)$ to $\\hat{p}(x_{t - \\Delta} \\mid y)$ and the $C$ Langevin steps (corrector) \"ensure\" a sample from $\\hat{p}(x_{t - \\Delta} \\mid y)$. In this light, DCPS and SDA's sampling algorithm are quite similar, especially regarding the Langevin steps which are used to \"correct\" the transition errors. These similarities should be discussed in the main text. There are also major differences, obviously. Notably,\n1. The likelihood function $g^l_m(x_m)$ is chosen as $\\mathbb{E}[g_{k_l}(x_{k_l}) \\mid x_m]$ instead of a (very) rough approximation to $\\mathbb{E}[g_0(x_0) \\mid x_m]$.\n2. The \"predictor\" step spans several diffusion steps and consists in a sequence of variational Gaussian approximations, which is indeed quite different from a DDIM step.\n\nConcerning the choice of potentials, I very much agree that bad potentials could lead to unstable sampling. However, I still believe that most of the quality gains of DCPS are due to the Langevin steps and not to the potentials $g^l_m(x)$. My intuition is that potentials $g_m(x)$ defined as in Eq. (3.9), (3.10) or (3.11) but with $m$ instead of $k_l$ would work nearly as well.", "labeling_timestamp": "2026-01-11T17:21:45.753475", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper text does not mention SDA [1] or discuss its construction of intermediate posteriors or use of likelihood functions. Because the paper does not address SDA, there is insufficient information to decide whether the authors' interpretation of SDA is incorrect or a misrepresentation.", "evidence": "“Recent works that aim to sample from the posterior (2.3) generally employ the FK representation (2.4). These studies, however, adopt varying auxiliary potentials [10, 47, 60, 4, 54, 59].”\n\n“For the Diffusion Posterior Sampling (DPS) algorithm [10], the point mass approximation δ ˆ x 0 | k ( x k ) of p 0 | k ( ·| x k ) results in the estimate ∇ x k log g 0 (ˆ x 0 | k ( x k )) of ∇ x k log g ⋆ k ( x k ).”", "section": "2 Posterior sampling with DDM prior"}
{"claim": "The approximated likelihood \\hat{p}(y|x_t) should be treated as a likelihood function that defines a series of intermediate posteriors \\hat{p}(x_t|y) across diffusion timesteps.", "claim_type": "methodology", "paper_id": "BOrut7M2X7", "paper_title": "Divide-and-Conquer Posterior Sampling for Denoising Diffusion priors", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "IgYyLGCtuC", "reviewer": "Reviewer_af3x", "review_text": "Comment: Thank you for your response and taking the time to implement [1].\n\nI do not agree with your interpretation of SDA [1]. Even though $\\hat{p}(y \\mid x_t)$ is an approximation of $p(y \\mid x_t)$, it is also a likelihood function and therefore defines a series of intermediate posteriors $\\hat{p}(x_t \\mid y)$. The DDIM step (predictor) is an approximation of the transition from $\\hat{p}(x_t \\mid y)$ to $\\hat{p}(x_{t - \\Delta} \\mid y)$ and the $C$ Langevin steps (corrector) \"ensure\" a sample from $\\hat{p}(x_{t - \\Delta} \\mid y)$. In this light, DCPS and SDA's sampling algorithm are quite similar, especially regarding the Langevin steps which are used to \"correct\" the transition errors. These similarities should be discussed in the main text. There are also major differences, obviously. Notably,\n1. The likelihood function $g^l_m(x_m)$ is chosen as $\\mathbb{E}[g_{k_l}(x_{k_l}) \\mid x_m]$ instead of a (very) rough approximation to $\\mathbb{E}[g_0(x_0) \\mid x_m]$.\n2. The \"predictor\" step spans several diffusion steps and consists in a sequence of variational Gaussian approximations, which is indeed quite different from a DDIM step.\n\nConcerning the choice of potentials, I very much agree that bad potentials could lead to unstable sampling. However, I still believe that most of the quality gains of DCPS are due to the Langevin steps and not to the potentials $g^l_m(x)$. My intuition is that potentials $g_m(x)$ defined as in Eq. (3.9), (3.10) or (3.11) but with $m$ instead of $k_l$ would work nearly as well.", "labeling_timestamp": "2026-01-11T17:21:59.431345", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly treats the likelihood g0(y|x0) and its time-k marginal g*_k(x_k)=∫ g0(x0) p0|k(x0|x_k) dx0 as potentials that define a sequence of intermediate (time-k) marginals/posteriors via a twisted Feynman–Kac/backward Markov decomposition. It also describes common approximations (e.g. DPS point-mass or Gaussian approximations of p0|k) as approximations of these likelihood-induced potentials, which consequently define approximated intermediate posteriors across timesteps.", "evidence": "\"Let g0 be a nonnegative function on R^{d_x}. When solving Bayesian inverse problems, g0 is taken as the likelihood... Our objective is to sample from the posterior distribution ...\"; \"define the potentials g^*_k(x_k) := ∫ g_0(x_0) p_{0|k}(x_0|x_k) dx_0.\"; \"we twist ... the backward transitions p_{k|k+1} by artificial positive potentials (g_k) ... This allows the posterior of interest to be expressed as the time-zero marginal distribution of a time-reversed FK model\"; \"For the Diffusion Posterior Sampling (DPS) algorithm [10], the point mass approximation δ_{â_{0|k}(x_k)} of p_{0|k}(·|x_k) results in the estimate ∇_{x_k} log g_0(â_{0|k}(x_k)) of ∇_{x_k} log g^*_k(x_k).\"", "section": "2 Posterior sampling with DDM prior"}
{"claim": "The DDIM predictor approximates the transition from \\hat{p}(x_t|y) to \\hat{p}(x_{t-\\Delta}|y), while the C Langevin corrector steps aim to produce samples from the resulting posterior.", "claim_type": "methodology", "paper_id": "BOrut7M2X7", "paper_title": "Divide-and-Conquer Posterior Sampling for Denoising Diffusion priors", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "IgYyLGCtuC", "reviewer": "Reviewer_af3x", "review_text": "Comment: Thank you for your response and taking the time to implement [1].\n\nI do not agree with your interpretation of SDA [1]. Even though $\\hat{p}(y \\mid x_t)$ is an approximation of $p(y \\mid x_t)$, it is also a likelihood function and therefore defines a series of intermediate posteriors $\\hat{p}(x_t \\mid y)$. The DDIM step (predictor) is an approximation of the transition from $\\hat{p}(x_t \\mid y)$ to $\\hat{p}(x_{t - \\Delta} \\mid y)$ and the $C$ Langevin steps (corrector) \"ensure\" a sample from $\\hat{p}(x_{t - \\Delta} \\mid y)$. In this light, DCPS and SDA's sampling algorithm are quite similar, especially regarding the Langevin steps which are used to \"correct\" the transition errors. These similarities should be discussed in the main text. There are also major differences, obviously. Notably,\n1. The likelihood function $g^l_m(x_m)$ is chosen as $\\mathbb{E}[g_{k_l}(x_{k_l}) \\mid x_m]$ instead of a (very) rough approximation to $\\mathbb{E}[g_0(x_0) \\mid x_m]$.\n2. The \"predictor\" step spans several diffusion steps and consists in a sequence of variational Gaussian approximations, which is indeed quite different from a DDIM step.\n\nConcerning the choice of potentials, I very much agree that bad potentials could lead to unstable sampling. However, I still believe that most of the quality gains of DCPS are due to the Langevin steps and not to the potentials $g^l_m(x)$. My intuition is that potentials $g_m(x)$ defined as in Eq. (3.9), (3.10) or (3.11) but with $m$ instead of $k_l$ would work nearly as well.", "labeling_timestamp": "2026-01-11T17:21:55.723407", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does not discuss DDIM predictors or the specific \"C Langevin corrector\" predictor-corrector terminology. It does mention using Langevin iterations as part of constructing draws in the DCPS scheme, but it does not state that a DDIM predictor approximates transitions between ̂p(x_t|y) and ̂p(x_{t-Δ}|y), nor that \"C Langevin corrector\" steps aim to produce samples from the resulting posterior. Therefore the reviewer's specific claim cannot be confirmed from the paper.", "evidence": "Starting with a sample from π k ℓ +1 , a draw from π k ℓ is formed by a combination of Langevin iterations and the simulation of an inhomogeneous Markov chain.", "section": "Introduction"}
{"claim": "DCPS and SDA sampling algorithms are similar in that both employ Langevin correction steps to reduce transition errors during sampling.", "claim_type": "methodology", "paper_id": "BOrut7M2X7", "paper_title": "Divide-and-Conquer Posterior Sampling for Denoising Diffusion priors", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "IgYyLGCtuC", "reviewer": "Reviewer_af3x", "review_text": "Comment: Thank you for your response and taking the time to implement [1].\n\nI do not agree with your interpretation of SDA [1]. Even though $\\hat{p}(y \\mid x_t)$ is an approximation of $p(y \\mid x_t)$, it is also a likelihood function and therefore defines a series of intermediate posteriors $\\hat{p}(x_t \\mid y)$. The DDIM step (predictor) is an approximation of the transition from $\\hat{p}(x_t \\mid y)$ to $\\hat{p}(x_{t - \\Delta} \\mid y)$ and the $C$ Langevin steps (corrector) \"ensure\" a sample from $\\hat{p}(x_{t - \\Delta} \\mid y)$. In this light, DCPS and SDA's sampling algorithm are quite similar, especially regarding the Langevin steps which are used to \"correct\" the transition errors. These similarities should be discussed in the main text. There are also major differences, obviously. Notably,\n1. The likelihood function $g^l_m(x_m)$ is chosen as $\\mathbb{E}[g_{k_l}(x_{k_l}) \\mid x_m]$ instead of a (very) rough approximation to $\\mathbb{E}[g_0(x_0) \\mid x_m]$.\n2. The \"predictor\" step spans several diffusion steps and consists in a sequence of variational Gaussian approximations, which is indeed quite different from a DDIM step.\n\nConcerning the choice of potentials, I very much agree that bad potentials could lead to unstable sampling. However, I still believe that most of the quality gains of DCPS are due to the Langevin steps and not to the potentials $g^l_m(x)$. My intuition is that potentials $g_m(x)$ defined as in Eq. (3.9), (3.10) or (3.11) but with $m$ instead of $k_l$ would work nearly as well.", "labeling_timestamp": "2026-01-11T17:22:13.775962", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper explicitly states that DCPS uses Langevin iterations as part of forming draws (Introduction). However, the manuscript does not mention or describe any algorithm named 'SDA' or compare DCPS to an 'SDA' method, so it provides no evidence that SDA also employs Langevin correction steps.", "evidence": "Starting with a sample from π_{k_{ℓ+1}}, a draw from π_{k_ℓ} is formed by a combination of Langevin iterations and the simulation of an inhomogeneous Markov chain.", "section": "1 Introduction"}
{"claim": "The manuscript does not discuss the substantive similarities between DCPS and SDA, particularly both methods' use of Langevin steps to correct transition errors.", "claim_type": "subjective", "paper_id": "BOrut7M2X7", "paper_title": "Divide-and-Conquer Posterior Sampling for Denoising Diffusion priors", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "IgYyLGCtuC", "reviewer": "Reviewer_af3x", "review_text": "Comment: Thank you for your response and taking the time to implement [1].\n\nI do not agree with your interpretation of SDA [1]. Even though $\\hat{p}(y \\mid x_t)$ is an approximation of $p(y \\mid x_t)$, it is also a likelihood function and therefore defines a series of intermediate posteriors $\\hat{p}(x_t \\mid y)$. The DDIM step (predictor) is an approximation of the transition from $\\hat{p}(x_t \\mid y)$ to $\\hat{p}(x_{t - \\Delta} \\mid y)$ and the $C$ Langevin steps (corrector) \"ensure\" a sample from $\\hat{p}(x_{t - \\Delta} \\mid y)$. In this light, DCPS and SDA's sampling algorithm are quite similar, especially regarding the Langevin steps which are used to \"correct\" the transition errors. These similarities should be discussed in the main text. There are also major differences, obviously. Notably,\n1. The likelihood function $g^l_m(x_m)$ is chosen as $\\mathbb{E}[g_{k_l}(x_{k_l}) \\mid x_m]$ instead of a (very) rough approximation to $\\mathbb{E}[g_0(x_0) \\mid x_m]$.\n2. The \"predictor\" step spans several diffusion steps and consists in a sequence of variational Gaussian approximations, which is indeed quite different from a DDIM step.\n\nConcerning the choice of potentials, I very much agree that bad potentials could lead to unstable sampling. However, I still believe that most of the quality gains of DCPS are due to the Langevin steps and not to the potentials $g^l_m(x)$. My intuition is that potentials $g_m(x)$ defined as in Eq. (3.9), (3.10) or (3.11) but with $m$ instead of $k_l$ would work nearly as well.", "labeling_timestamp": "2026-01-11T17:22:22.276800", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes DCPS as using Langevin iterations to correct transitions (Introduction / Sec. 3), but it does not mention or compare to SDA anywhere in the presented related-work discussion (Sec. 2 and Appendix B references). Thus the manuscript does not discuss substantive similarities between DCPS and SDA.", "evidence": [{"quote": "Starting with a sample from π k ℓ +1 , a draw from π k ℓ is formed by a combination of Langevin iterations and the simulation of an inhomogeneous Markov chain.", "section": "Introduction"}, {"quote": "Recent works have focused on developing tractable approximations of p 0 | k ( ·| x k ) . For the Diffusion Posterior Sampling (DPS) algorithm [10], ... Alternatively, [47] proposed the Pseudoinverse-Guided Diffusion Model (ΠGDM), which uses a Gaussian approximation ... We discuss in more depth the related works in Appendix B.", "section": "Section 2 (Posterior sampling with DDM prior)"}], "section": "Introduction; Section 2"}
{"claim": "DCPS defines the likelihood g^l_m(x_m) as the conditional expectation E[g_{k_l}(x_{k_l}) | x_m], rather than the rough approximation to E[g_0(x_0) | x_m] used in SDA.", "claim_type": "methodology", "paper_id": "BOrut7M2X7", "paper_title": "Divide-and-Conquer Posterior Sampling for Denoising Diffusion priors", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "IgYyLGCtuC", "reviewer": "Reviewer_af3x", "review_text": "Comment: Thank you for your response and taking the time to implement [1].\n\nI do not agree with your interpretation of SDA [1]. Even though $\\hat{p}(y \\mid x_t)$ is an approximation of $p(y \\mid x_t)$, it is also a likelihood function and therefore defines a series of intermediate posteriors $\\hat{p}(x_t \\mid y)$. The DDIM step (predictor) is an approximation of the transition from $\\hat{p}(x_t \\mid y)$ to $\\hat{p}(x_{t - \\Delta} \\mid y)$ and the $C$ Langevin steps (corrector) \"ensure\" a sample from $\\hat{p}(x_{t - \\Delta} \\mid y)$. In this light, DCPS and SDA's sampling algorithm are quite similar, especially regarding the Langevin steps which are used to \"correct\" the transition errors. These similarities should be discussed in the main text. There are also major differences, obviously. Notably,\n1. The likelihood function $g^l_m(x_m)$ is chosen as $\\mathbb{E}[g_{k_l}(x_{k_l}) \\mid x_m]$ instead of a (very) rough approximation to $\\mathbb{E}[g_0(x_0) \\mid x_m]$.\n2. The \"predictor\" step spans several diffusion steps and consists in a sequence of variational Gaussian approximations, which is indeed quite different from a DDIM step.\n\nConcerning the choice of potentials, I very much agree that bad potentials could lead to unstable sampling. However, I still believe that most of the quality gains of DCPS are due to the Langevin steps and not to the potentials $g^l_m(x)$. My intuition is that potentials $g_m(x)$ defined as in Eq. (3.9), (3.10) or (3.11) but with $m$ instead of $k_l$ would work nearly as well.", "labeling_timestamp": "2026-01-11T17:22:32.951089", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper defines the potentials used for backward twisting as conditional expectations of earlier potentials (i.e., g*_k(x_k) = E[g_0(X_0) | X_k = x_k]) and explicitly notes the recursion g*_{k+1}(x_{k+1}) = E[g*_k(X_k) | X_{k+1} = x_{k+1}]. It also contrasts this with DPS, which uses a point-mass approximation of p_{0|k} (a rough approximation to E[g_0|x_k]). Thus the paper supports the claim that DCPS uses intermediate (conditional expectation) potentials rather than the crude point-mass approximation used in DPS/SDA.", "evidence": "“define the potentials g⋆_k(x_k) := ∫ g_0(x_0) p_{0|k}(x_0 | x_k) d x_0. Note that these potentials satisfy the recursion g⋆_{k+1}(x_{k+1}) = ∫ g⋆_k(x_k) p_{k|k+1}(x_k | x_{k+1}) d x_k.”\n\n“For the Diffusion Posterior Sampling (DPS) algorithm [10], the point mass approximation δ_{ˆx_{0|k}(x_k)} of p_{0|k}(·|x_k) results in the estimate ∇_{x_k} log g_0(ˆx_{0|k}(x_k)) of ∇_{x_k} log g⋆_k(x_k).”", "section": "Section 2 (Posterior sampling)"}
{"claim": "The DCPS predictor spans multiple diffusion steps and uses a sequence of variational Gaussian approximations, which is substantially different from a single-step DDIM predictor.", "claim_type": "methodology", "paper_id": "BOrut7M2X7", "paper_title": "Divide-and-Conquer Posterior Sampling for Denoising Diffusion priors", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "IgYyLGCtuC", "reviewer": "Reviewer_af3x", "review_text": "Comment: Thank you for your response and taking the time to implement [1].\n\nI do not agree with your interpretation of SDA [1]. Even though $\\hat{p}(y \\mid x_t)$ is an approximation of $p(y \\mid x_t)$, it is also a likelihood function and therefore defines a series of intermediate posteriors $\\hat{p}(x_t \\mid y)$. The DDIM step (predictor) is an approximation of the transition from $\\hat{p}(x_t \\mid y)$ to $\\hat{p}(x_{t - \\Delta} \\mid y)$ and the $C$ Langevin steps (corrector) \"ensure\" a sample from $\\hat{p}(x_{t - \\Delta} \\mid y)$. In this light, DCPS and SDA's sampling algorithm are quite similar, especially regarding the Langevin steps which are used to \"correct\" the transition errors. These similarities should be discussed in the main text. There are also major differences, obviously. Notably,\n1. The likelihood function $g^l_m(x_m)$ is chosen as $\\mathbb{E}[g_{k_l}(x_{k_l}) \\mid x_m]$ instead of a (very) rough approximation to $\\mathbb{E}[g_0(x_0) \\mid x_m]$.\n2. The \"predictor\" step spans several diffusion steps and consists in a sequence of variational Gaussian approximations, which is indeed quite different from a DDIM step.\n\nConcerning the choice of potentials, I very much agree that bad potentials could lead to unstable sampling. However, I still believe that most of the quality gains of DCPS are due to the Langevin steps and not to the potentials $g^l_m(x)$. My intuition is that potentials $g_m(x)$ defined as in Eq. (3.9), (3.10) or (3.11) but with $m$ instead of $k_l$ would work nearly as well.", "labeling_timestamp": "2026-01-11T17:22:29.743366", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper clearly states that DCPS constructs a sequence of intermediate (time-reversed) Markov distributions spanning portions of the diffusion path and that these inhomogeneous Markov chains are \"approximately sampled using Gaussian variational inference\" (supporting the multi-step and variational-Gaussian aspects). However, the paper does not discuss or compare DCPS to a single-step DDIM predictor, so the claim that this is \"substantially different from a single-step DDIM predictor\" cannot be confirmed from the paper text provided.", "evidence": "“instead of targeting the given posterior by a single simulation run through the full backward decomposition, our proposed scheme targets backward a sequence ( π^k_ℓ )_{ℓ=0}^L of distributions along the path measure leading to the target posterior distribution… π^k_ℓ is expressed as the final marginal distribution of a time-reversed inhomogeneous Markov chain of moderate length k_{ℓ+1}-k_ℓ ∈ N^* … This chain, whose transition densities are intractable, is approximately sampled using Gaussian variational inference. The rationale behind our approach stems from the observation that the Gaussian approximation error can be reduced by shortening the length of the intermediate FK path measures (i.e., by increasing L).”", "section": "1 Introduction"}
{"claim": "Poorly chosen potentials g^l_m(x) can lead to unstable sampling behavior and degrade the method's robustness.", "claim_type": "methodology", "paper_id": "BOrut7M2X7", "paper_title": "Divide-and-Conquer Posterior Sampling for Denoising Diffusion priors", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "IgYyLGCtuC", "reviewer": "Reviewer_af3x", "review_text": "Comment: Thank you for your response and taking the time to implement [1].\n\nI do not agree with your interpretation of SDA [1]. Even though $\\hat{p}(y \\mid x_t)$ is an approximation of $p(y \\mid x_t)$, it is also a likelihood function and therefore defines a series of intermediate posteriors $\\hat{p}(x_t \\mid y)$. The DDIM step (predictor) is an approximation of the transition from $\\hat{p}(x_t \\mid y)$ to $\\hat{p}(x_{t - \\Delta} \\mid y)$ and the $C$ Langevin steps (corrector) \"ensure\" a sample from $\\hat{p}(x_{t - \\Delta} \\mid y)$. In this light, DCPS and SDA's sampling algorithm are quite similar, especially regarding the Langevin steps which are used to \"correct\" the transition errors. These similarities should be discussed in the main text. There are also major differences, obviously. Notably,\n1. The likelihood function $g^l_m(x_m)$ is chosen as $\\mathbb{E}[g_{k_l}(x_{k_l}) \\mid x_m]$ instead of a (very) rough approximation to $\\mathbb{E}[g_0(x_0) \\mid x_m]$.\n2. The \"predictor\" step spans several diffusion steps and consists in a sequence of variational Gaussian approximations, which is indeed quite different from a DDIM step.\n\nConcerning the choice of potentials, I very much agree that bad potentials could lead to unstable sampling. However, I still believe that most of the quality gains of DCPS are due to the Langevin steps and not to the potentials $g^l_m(x)$. My intuition is that potentials $g_m(x)$ defined as in Eq. (3.9), (3.10) or (3.11) but with $m$ instead of $k_l$ would work nearly as well.", "labeling_timestamp": "2026-01-11T17:22:22.425568", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly states that sampling effectiveness depends heavily on the choice of intermediate potentials and notes sensitivity/vulnerability (e.g., mode collapse) when sampling is constrained, which supports the reviewer's claim that poorly chosen potentials can destabilize sampling and degrade robustness.", "evidence": "\"The effectiveness of this technique depends heavily on the choice of intermediate potentials ( g_k )_{n k =1}, as discussed in [54, 59, 7, 16].\" \n\n\"However, SMC methods require a number of samples proportional and often exponential in the dimensionality of the problems hence limiting their application in these setups due to the resulting probabitive memory cost [2]. On the other hand, reducing the number of samples makes them vulnerable to mode collapse.\" \n\n\"their associated iterative sampling schemes can be computationally intensive and exhibit high sensitivity to the choice of hyperparameters; see e.g. [24].\"", "section": "Section 2: Posterior sampling with DDM prior (and Introduction)"}
{"claim": "The reviewer believes that most observed quality gains in DCPS are attributable to the Langevin corrector steps rather than to the specific design of the potentials g^l_m(x).", "claim_type": "experimental", "paper_id": "BOrut7M2X7", "paper_title": "Divide-and-Conquer Posterior Sampling for Denoising Diffusion priors", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "IgYyLGCtuC", "reviewer": "Reviewer_af3x", "review_text": "Comment: Thank you for your response and taking the time to implement [1].\n\nI do not agree with your interpretation of SDA [1]. Even though $\\hat{p}(y \\mid x_t)$ is an approximation of $p(y \\mid x_t)$, it is also a likelihood function and therefore defines a series of intermediate posteriors $\\hat{p}(x_t \\mid y)$. The DDIM step (predictor) is an approximation of the transition from $\\hat{p}(x_t \\mid y)$ to $\\hat{p}(x_{t - \\Delta} \\mid y)$ and the $C$ Langevin steps (corrector) \"ensure\" a sample from $\\hat{p}(x_{t - \\Delta} \\mid y)$. In this light, DCPS and SDA's sampling algorithm are quite similar, especially regarding the Langevin steps which are used to \"correct\" the transition errors. These similarities should be discussed in the main text. There are also major differences, obviously. Notably,\n1. The likelihood function $g^l_m(x_m)$ is chosen as $\\mathbb{E}[g_{k_l}(x_{k_l}) \\mid x_m]$ instead of a (very) rough approximation to $\\mathbb{E}[g_0(x_0) \\mid x_m]$.\n2. The \"predictor\" step spans several diffusion steps and consists in a sequence of variational Gaussian approximations, which is indeed quite different from a DDIM step.\n\nConcerning the choice of potentials, I very much agree that bad potentials could lead to unstable sampling. However, I still believe that most of the quality gains of DCPS are due to the Langevin steps and not to the potentials $g^l_m(x)$. My intuition is that potentials $g_m(x)$ defined as in Eq. (3.9), (3.10) or (3.11) but with $m$ instead of $k_l$ would work nearly as well.", "labeling_timestamp": "2026-01-11T17:22:36.925508", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper describes that draws are formed by a combination of Langevin iterations and simulation of an inhomogeneous Markov chain and emphasizes improvements from the divide-and-conquer potentials, bridge-kernel smoothing, and Gaussian variational inference. It does not present an ablation or a statement attributing most observed quality gains specifically to the Langevin corrector steps versus the potentials' design, so the claim cannot be confirmed from the paper.", "evidence": "1) \"Starting with a sample from π^{k_{ℓ+1}}, a draw from π^{k_ℓ} is formed by a combination of Langevin iterations and the simulation of an inhomogeneous Markov chain.\"  \n2) \"To sum up our contribution, we - show that the existing approximations of the Markovian backward decomposition can be improved using a bridge-kernel smoothing technique - design a novel divide-and-conquer sampling approach that enables efficient bias-reduced sampling from the posterior ... - propose a new technique to efficiently generate approximate samples from the backward decomposition using Gaussian variational inference.\"", "section": "Introduction; 3 The DCPS algorithm"}
{"claim": "Potentials defined as in Eq.(3.9)-(3.11) but using index m instead of k_l would likely perform nearly as well, suggesting the potential choice may not be the critical factor.", "claim_type": "experimental", "paper_id": "BOrut7M2X7", "paper_title": "Divide-and-Conquer Posterior Sampling for Denoising Diffusion priors", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "IgYyLGCtuC", "reviewer": "Reviewer_af3x", "review_text": "Comment: Thank you for your response and taking the time to implement [1].\n\nI do not agree with your interpretation of SDA [1]. Even though $\\hat{p}(y \\mid x_t)$ is an approximation of $p(y \\mid x_t)$, it is also a likelihood function and therefore defines a series of intermediate posteriors $\\hat{p}(x_t \\mid y)$. The DDIM step (predictor) is an approximation of the transition from $\\hat{p}(x_t \\mid y)$ to $\\hat{p}(x_{t - \\Delta} \\mid y)$ and the $C$ Langevin steps (corrector) \"ensure\" a sample from $\\hat{p}(x_{t - \\Delta} \\mid y)$. In this light, DCPS and SDA's sampling algorithm are quite similar, especially regarding the Langevin steps which are used to \"correct\" the transition errors. These similarities should be discussed in the main text. There are also major differences, obviously. Notably,\n1. The likelihood function $g^l_m(x_m)$ is chosen as $\\mathbb{E}[g_{k_l}(x_{k_l}) \\mid x_m]$ instead of a (very) rough approximation to $\\mathbb{E}[g_0(x_0) \\mid x_m]$.\n2. The \"predictor\" step spans several diffusion steps and consists in a sequence of variational Gaussian approximations, which is indeed quite different from a DDIM step.\n\nConcerning the choice of potentials, I very much agree that bad potentials could lead to unstable sampling. However, I still believe that most of the quality gains of DCPS are due to the Langevin steps and not to the potentials $g^l_m(x)$. My intuition is that potentials $g_m(x)$ defined as in Eq. (3.9), (3.10) or (3.11) but with $m$ instead of $k_l$ would work nearly as well.", "labeling_timestamp": "2026-01-11T17:22:45.122738", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper repeatedly emphasizes that the choice of intermediate potentials is important and central to performance (Section 2 and Introduction). It proposes a sequence of increasingly complex potentials that converge to the target and argues that effectiveness depends heavily on the potential choice, which contradicts the reviewer's claim that a simple index change would not matter.", "evidence": "1) \"These distributions are induced by a sequence of increasingly complex potentials and converge to the target distribution.\" 2) \"The effectiveness of this technique depends heavily on the choice of intermediate potentials ( g_k )_{n k =1}, as discussed in [54, 59, 7, 16].\"", "section": "Introduction; Section 2 (Posterior sampling with DDM prior)"}
{"claim": "The authors misuse the term 'generation-time strategy', which typically refers to decoding methods like beam search or top-k sampling.", "claim_type": "methodology", "paper_id": "0bMmZ3fkCk", "paper_title": "NEFTune: Noisy Embeddings Improve Instruction Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vmTbtNCVEl", "reviewer": "Reviewer_h2qY", "review_text": "Comment: 1) \" generation-time strategy \" This term is usually used for strategy such as beam search decoding, top-k sampling etc. So the result in table-5 does not support the claim.\n\n2) \"Std values\" I am not really convinced by the result in table 6 because of the following reason. By changing the noise statistics the results still hold, which is counter intuitive since there has to be an inflection point beyond which adding noise should hurt the performance. There has to be a detailed study of how and why this algorithm is working. I understand the experiments are running for standard deviation however for this settings of the paper reporting the standard deviation is important. The setting assumed is low resource setting for LLMs and standard deviation defines the reliability of the algorithm proposed. I strongly encourage the authors to report them. \"For AlpacaEval, the reported Std Error is less than 2%.\" this is a vague statement and I would want to see all the numbers.\n\n3) As I mentioned above in the weakness, the authors did not clearly explain why their algorithm works. For instance, in the work stated above [2], they propose to represent a word embedding vector as a linear combination of all the related word embedding vectors weighted by the entries of a similarity matrix. The authors compare the proposed method with a baseline, BFTSS top-k random S, where they add random noise to the embedding representation of a word that improves robustness. This baseline outperforms the vanilla method however lags behind their method. I see NEFTune to be on the same lines as this work. However in their work, adding random noise outperforms vanilla in only some settings. So as I mentioned above, I urge the authors to inspect what is the noise statistics that improves the performance, why is it contributing to the better performance and also when it will deteriorate the performance. Another suggestion is, in the current form of the work, the authors are just adding noise to the embedding representation. This is uncontrolled noise and the interpretability of the method is less. I suggest the authors to add the baseline where the embedding vector of a word is represented as a linear combination of all the related word embedding vectors weighted by the entries of a similarity matrix initialized using cosine similarity of some pre-trained model embedding matrix [2]. They do not need to learn for the task-dependent similarity matrix. I believe from table 6 if the method works with a range of noise initialization then the suggested baseline should perform at par or even better as observed in [2]. It also brings some interpretability to the table.\n\n[2] \"Bi-level Finetuning with Task-dependent Similarity Structure for Low-resource Training.\" Findings of the Association for Computational Linguistics: ACL 2023. 2023.\n\nI appreciate the authors for their research and especially recognizing the need to regularize to boost the performance. However, the current form of the work, as I mentioned in my previous comments, does not provide any understanding of why NEFTune works. The approach seems to be inspired from the previous methods in the literature, though a detailed comparison with those methods is not offered.  The standard deviation is necessary for methods dealing with low-resource. However, the results look promising and the paper is more on the empirical end. \n\nNevertheless, since all my comments are not adequately answered, I would like to reduce my score from 8->6. That said, I recommend the authors to revise the work thoroughly since I believe it has a good potential.", "labeling_timestamp": "2026-01-11T17:22:39.105773", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does not use or discuss the phrase 'generation-time strategy' anywhere in the provided content. Instead, it describes NEFTune as a training-time (fine-tuning) augmentation that adds noise to embeddings during the forward pass, so there is no evidence that the authors used or misused the term referring to decoding methods.", "evidence": "\"In this paper, we propose to add random noise to the embedding vectors of the training data during the forward pass of fine-tuning.\" (Introduction)\n\n\"NEFTune then departs from standard training by adding a random noise vector to the embeddings.\" (Section 2)", "section": "1 INTRODUCTION; 2 NEFTUNE : NOISY EMBEDDING INSTRUCTION FINETUNING"}
{"claim": "The results presented in Table 5 do not support the paper's claim about a 'generation-time strategy'.", "claim_type": "experimental", "paper_id": "0bMmZ3fkCk", "paper_title": "NEFTune: Noisy Embeddings Improve Instruction Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vmTbtNCVEl", "reviewer": "Reviewer_h2qY", "review_text": "Comment: 1) \" generation-time strategy \" This term is usually used for strategy such as beam search decoding, top-k sampling etc. So the result in table-5 does not support the claim.\n\n2) \"Std values\" I am not really convinced by the result in table 6 because of the following reason. By changing the noise statistics the results still hold, which is counter intuitive since there has to be an inflection point beyond which adding noise should hurt the performance. There has to be a detailed study of how and why this algorithm is working. I understand the experiments are running for standard deviation however for this settings of the paper reporting the standard deviation is important. The setting assumed is low resource setting for LLMs and standard deviation defines the reliability of the algorithm proposed. I strongly encourage the authors to report them. \"For AlpacaEval, the reported Std Error is less than 2%.\" this is a vague statement and I would want to see all the numbers.\n\n3) As I mentioned above in the weakness, the authors did not clearly explain why their algorithm works. For instance, in the work stated above [2], they propose to represent a word embedding vector as a linear combination of all the related word embedding vectors weighted by the entries of a similarity matrix. The authors compare the proposed method with a baseline, BFTSS top-k random S, where they add random noise to the embedding representation of a word that improves robustness. This baseline outperforms the vanilla method however lags behind their method. I see NEFTune to be on the same lines as this work. However in their work, adding random noise outperforms vanilla in only some settings. So as I mentioned above, I urge the authors to inspect what is the noise statistics that improves the performance, why is it contributing to the better performance and also when it will deteriorate the performance. Another suggestion is, in the current form of the work, the authors are just adding noise to the embedding representation. This is uncontrolled noise and the interpretability of the method is less. I suggest the authors to add the baseline where the embedding vector of a word is represented as a linear combination of all the related word embedding vectors weighted by the entries of a similarity matrix initialized using cosine similarity of some pre-trained model embedding matrix [2]. They do not need to learn for the task-dependent similarity matrix. I believe from table 6 if the method works with a range of noise initialization then the suggested baseline should perform at par or even better as observed in [2]. It also brings some interpretability to the table.\n\n[2] \"Bi-level Finetuning with Task-dependent Similarity Structure for Low-resource Training.\" Findings of the Association for Computational Linguistics: ACL 2023. 2023.\n\nI appreciate the authors for their research and especially recognizing the need to regularize to boost the performance. However, the current form of the work, as I mentioned in my previous comments, does not provide any understanding of why NEFTune works. The approach seems to be inspired from the previous methods in the literature, though a detailed comparison with those methods is not offered.  The standard deviation is necessary for methods dealing with low-resource. However, the results look promising and the paper is more on the empirical end. \n\nNevertheless, since all my comments are not adequately answered, I would like to reduce my score from 8->6. That said, I recommend the authors to revise the work thoroughly since I believe it has a good potential.", "labeling_timestamp": "2026-01-11T17:22:51.554401", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not contain a Table 5 nor any mention of a 'generation-time strategy', so there is insufficient information to verify whether Table 5 supports or contradicts the paper's claim about such a strategy.", "evidence": "Table 1: AlpacaEval Win Rate versus Text-Davinci-003 for LLaMA-2 trained on different datasets, using GPT-4 as the evaluator, showing an average improvement of 15 %across all datasets over three seeds.\n\nTable 2: LLaMA-2-Chat (7B), LLaMA-2 (13B), and LLaMA-2 (70B) can be finetuned further to improve performance.", "section": "3.3 EVALUATION / 4 RESULTS"}
{"claim": "Table 6 fails to report standard deviations for the experiments, preventing assessment of result reliability in low-resource settings.", "claim_type": "experimental", "paper_id": "0bMmZ3fkCk", "paper_title": "NEFTune: Noisy Embeddings Improve Instruction Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vmTbtNCVEl", "reviewer": "Reviewer_h2qY", "review_text": "Comment: 1) \" generation-time strategy \" This term is usually used for strategy such as beam search decoding, top-k sampling etc. So the result in table-5 does not support the claim.\n\n2) \"Std values\" I am not really convinced by the result in table 6 because of the following reason. By changing the noise statistics the results still hold, which is counter intuitive since there has to be an inflection point beyond which adding noise should hurt the performance. There has to be a detailed study of how and why this algorithm is working. I understand the experiments are running for standard deviation however for this settings of the paper reporting the standard deviation is important. The setting assumed is low resource setting for LLMs and standard deviation defines the reliability of the algorithm proposed. I strongly encourage the authors to report them. \"For AlpacaEval, the reported Std Error is less than 2%.\" this is a vague statement and I would want to see all the numbers.\n\n3) As I mentioned above in the weakness, the authors did not clearly explain why their algorithm works. For instance, in the work stated above [2], they propose to represent a word embedding vector as a linear combination of all the related word embedding vectors weighted by the entries of a similarity matrix. The authors compare the proposed method with a baseline, BFTSS top-k random S, where they add random noise to the embedding representation of a word that improves robustness. This baseline outperforms the vanilla method however lags behind their method. I see NEFTune to be on the same lines as this work. However in their work, adding random noise outperforms vanilla in only some settings. So as I mentioned above, I urge the authors to inspect what is the noise statistics that improves the performance, why is it contributing to the better performance and also when it will deteriorate the performance. Another suggestion is, in the current form of the work, the authors are just adding noise to the embedding representation. This is uncontrolled noise and the interpretability of the method is less. I suggest the authors to add the baseline where the embedding vector of a word is represented as a linear combination of all the related word embedding vectors weighted by the entries of a similarity matrix initialized using cosine similarity of some pre-trained model embedding matrix [2]. They do not need to learn for the task-dependent similarity matrix. I believe from table 6 if the method works with a range of noise initialization then the suggested baseline should perform at par or even better as observed in [2]. It also brings some interpretability to the table.\n\n[2] \"Bi-level Finetuning with Task-dependent Similarity Structure for Low-resource Training.\" Findings of the Association for Computational Linguistics: ACL 2023. 2023.\n\nI appreciate the authors for their research and especially recognizing the need to regularize to boost the performance. However, the current form of the work, as I mentioned in my previous comments, does not provide any understanding of why NEFTune works. The approach seems to be inspired from the previous methods in the literature, though a detailed comparison with those methods is not offered.  The standard deviation is necessary for methods dealing with low-resource. However, the results look promising and the paper is more on the empirical end. \n\nNevertheless, since all my comments are not adequately answered, I would like to reduce my score from 8->6. That said, I recommend the authors to revise the work thoroughly since I believe it has a good potential.", "labeling_timestamp": "2026-01-11T17:22:58.147402", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The reviewer refers specifically to Table 6, but the provided paper excerpt does not include Table 6, so we cannot verify whether that table reports standard deviations. The excerpt does show that some tables (e.g., Table 1) include ± values across seeds, but this does not allow confirming the contents or formatting of Table 6.", "evidence": "Table 1: AlpacaEval Win Rate versus Text-Davinci-003 for LLaMA-2 trained on different datasets, using GPT-4 as the evaluator, showing an average improvement of 15 %across all datasets over three seeds.\n\n| Llama-2 7B + NEFTune | 29.91 ± 0 . 40 65.70 ± 0 . 52 |", "section": "3.3 EVALUATION / Table 1"}
{"claim": "The paper reports stable performance across changed noise statistics, which is counterintuitive because excessive noise should eventually degrade model performance.", "claim_type": "experimental", "paper_id": "0bMmZ3fkCk", "paper_title": "NEFTune: Noisy Embeddings Improve Instruction Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vmTbtNCVEl", "reviewer": "Reviewer_h2qY", "review_text": "Comment: 1) \" generation-time strategy \" This term is usually used for strategy such as beam search decoding, top-k sampling etc. So the result in table-5 does not support the claim.\n\n2) \"Std values\" I am not really convinced by the result in table 6 because of the following reason. By changing the noise statistics the results still hold, which is counter intuitive since there has to be an inflection point beyond which adding noise should hurt the performance. There has to be a detailed study of how and why this algorithm is working. I understand the experiments are running for standard deviation however for this settings of the paper reporting the standard deviation is important. The setting assumed is low resource setting for LLMs and standard deviation defines the reliability of the algorithm proposed. I strongly encourage the authors to report them. \"For AlpacaEval, the reported Std Error is less than 2%.\" this is a vague statement and I would want to see all the numbers.\n\n3) As I mentioned above in the weakness, the authors did not clearly explain why their algorithm works. For instance, in the work stated above [2], they propose to represent a word embedding vector as a linear combination of all the related word embedding vectors weighted by the entries of a similarity matrix. The authors compare the proposed method with a baseline, BFTSS top-k random S, where they add random noise to the embedding representation of a word that improves robustness. This baseline outperforms the vanilla method however lags behind their method. I see NEFTune to be on the same lines as this work. However in their work, adding random noise outperforms vanilla in only some settings. So as I mentioned above, I urge the authors to inspect what is the noise statistics that improves the performance, why is it contributing to the better performance and also when it will deteriorate the performance. Another suggestion is, in the current form of the work, the authors are just adding noise to the embedding representation. This is uncontrolled noise and the interpretability of the method is less. I suggest the authors to add the baseline where the embedding vector of a word is represented as a linear combination of all the related word embedding vectors weighted by the entries of a similarity matrix initialized using cosine similarity of some pre-trained model embedding matrix [2]. They do not need to learn for the task-dependent similarity matrix. I believe from table 6 if the method works with a range of noise initialization then the suggested baseline should perform at par or even better as observed in [2]. It also brings some interpretability to the table.\n\n[2] \"Bi-level Finetuning with Task-dependent Similarity Structure for Low-resource Training.\" Findings of the Association for Computational Linguistics: ACL 2023. 2023.\n\nI appreciate the authors for their research and especially recognizing the need to regularize to boost the performance. However, the current form of the work, as I mentioned in my previous comments, does not provide any understanding of why NEFTune works. The approach seems to be inspired from the previous methods in the literature, though a detailed comparison with those methods is not offered.  The standard deviation is necessary for methods dealing with low-resource. However, the results look promising and the paper is more on the empirical end. \n\nNevertheless, since all my comments are not adequately answered, I would like to reduce my score from 8->6. That said, I recommend the authors to revise the work thoroughly since I believe it has a good potential.", "labeling_timestamp": "2026-01-11T17:23:04.197957", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper describes the noise generation procedure and notes a coarse hyperparameter sweep to choose a noise scale (α), but the provided content does not report experiments that vary the noise statistics (distribution or scale) and measure performance stability across those changes. Thus the claim that the paper reports stable performance across changed noise statistics cannot be confirmed from the paper text shown.", "evidence": "“The noise is generated by sampling iid uniform entries, each in the range [ -1 , 1] , and then scaling the entire noise vector by a factor of α/ √ Ld, ... results in a random vector with an expected Euclidean magnitude of approximately α/ √ 3 .” (Section 2)\n\n“We set our hyperparameters through a coarse sweep on LLaMA-1 ( 7 B) trained on the Alpaca dataset, where we see 6% improvement over the standard Alpaca model. We use these as the defaults on all models.” (Section 3.2)\n\n“Figure 3 shows that scores remain stable and that NEFTune preserves model capabilities.” (Section 4, but this refers to task capability stability, not noise-statistics variation.)", "section": "Section 2 (NEFTune), Section 3.2 (Experimental setup), Section 4 (Results)"}
{"claim": "The authors provide only a vague statement that AlpacaEval's standard error is less than 2% without reporting the actual numeric values across experiments.", "claim_type": "quantitative", "paper_id": "0bMmZ3fkCk", "paper_title": "NEFTune: Noisy Embeddings Improve Instruction Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vmTbtNCVEl", "reviewer": "Reviewer_h2qY", "review_text": "Comment: 1) \" generation-time strategy \" This term is usually used for strategy such as beam search decoding, top-k sampling etc. So the result in table-5 does not support the claim.\n\n2) \"Std values\" I am not really convinced by the result in table 6 because of the following reason. By changing the noise statistics the results still hold, which is counter intuitive since there has to be an inflection point beyond which adding noise should hurt the performance. There has to be a detailed study of how and why this algorithm is working. I understand the experiments are running for standard deviation however for this settings of the paper reporting the standard deviation is important. The setting assumed is low resource setting for LLMs and standard deviation defines the reliability of the algorithm proposed. I strongly encourage the authors to report them. \"For AlpacaEval, the reported Std Error is less than 2%.\" this is a vague statement and I would want to see all the numbers.\n\n3) As I mentioned above in the weakness, the authors did not clearly explain why their algorithm works. For instance, in the work stated above [2], they propose to represent a word embedding vector as a linear combination of all the related word embedding vectors weighted by the entries of a similarity matrix. The authors compare the proposed method with a baseline, BFTSS top-k random S, where they add random noise to the embedding representation of a word that improves robustness. This baseline outperforms the vanilla method however lags behind their method. I see NEFTune to be on the same lines as this work. However in their work, adding random noise outperforms vanilla in only some settings. So as I mentioned above, I urge the authors to inspect what is the noise statistics that improves the performance, why is it contributing to the better performance and also when it will deteriorate the performance. Another suggestion is, in the current form of the work, the authors are just adding noise to the embedding representation. This is uncontrolled noise and the interpretability of the method is less. I suggest the authors to add the baseline where the embedding vector of a word is represented as a linear combination of all the related word embedding vectors weighted by the entries of a similarity matrix initialized using cosine similarity of some pre-trained model embedding matrix [2]. They do not need to learn for the task-dependent similarity matrix. I believe from table 6 if the method works with a range of noise initialization then the suggested baseline should perform at par or even better as observed in [2]. It also brings some interpretability to the table.\n\n[2] \"Bi-level Finetuning with Task-dependent Similarity Structure for Low-resource Training.\" Findings of the Association for Computational Linguistics: ACL 2023. 2023.\n\nI appreciate the authors for their research and especially recognizing the need to regularize to boost the performance. However, the current form of the work, as I mentioned in my previous comments, does not provide any understanding of why NEFTune works. The approach seems to be inspired from the previous methods in the literature, though a detailed comparison with those methods is not offered.  The standard deviation is necessary for methods dealing with low-resource. However, the results look promising and the paper is more on the empirical end. \n\nNevertheless, since all my comments are not adequately answered, I would like to reduce my score from 8->6. That said, I recommend the authors to revise the work thoroughly since I believe it has a good potential.", "labeling_timestamp": "2026-01-11T17:23:04.232773", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper reports numeric uncertainty (±) values for AlpacaEval Win Rates in Table 1 (over three seeds), rather than only a vague statement that standard error is <2%.", "evidence": "Table 1: AlpacaEval Win Rate versus Text-Davinci-003 for LLaMA-2 trained on different datasets, using GPT-4 as the evaluator, showing an average improvement of 15 %across all datasets over three seeds.\n\n| Llama-2 7B + NEFTune | 29.91 ± 0 . 40 65.70 ± 0 . 52 | 70.19 ± 0 . 08 80.36 ± 0 . 47 | 70.57 ± 1 . 38 76.22 ± 0 . 67 | 60.95 ± 1 . 52 69.96 ± 0 . 47 | 57.91 73.06 |", "section": "3.3 EVALUATION / Table 1"}
{"claim": "The manuscript does not explain why NEFTune improves performance, lacking a mechanistic or theoretical justification of the method.", "claim_type": "methodology", "paper_id": "0bMmZ3fkCk", "paper_title": "NEFTune: Noisy Embeddings Improve Instruction Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vmTbtNCVEl", "reviewer": "Reviewer_h2qY", "review_text": "Comment: 1) \" generation-time strategy \" This term is usually used for strategy such as beam search decoding, top-k sampling etc. So the result in table-5 does not support the claim.\n\n2) \"Std values\" I am not really convinced by the result in table 6 because of the following reason. By changing the noise statistics the results still hold, which is counter intuitive since there has to be an inflection point beyond which adding noise should hurt the performance. There has to be a detailed study of how and why this algorithm is working. I understand the experiments are running for standard deviation however for this settings of the paper reporting the standard deviation is important. The setting assumed is low resource setting for LLMs and standard deviation defines the reliability of the algorithm proposed. I strongly encourage the authors to report them. \"For AlpacaEval, the reported Std Error is less than 2%.\" this is a vague statement and I would want to see all the numbers.\n\n3) As I mentioned above in the weakness, the authors did not clearly explain why their algorithm works. For instance, in the work stated above [2], they propose to represent a word embedding vector as a linear combination of all the related word embedding vectors weighted by the entries of a similarity matrix. The authors compare the proposed method with a baseline, BFTSS top-k random S, where they add random noise to the embedding representation of a word that improves robustness. This baseline outperforms the vanilla method however lags behind their method. I see NEFTune to be on the same lines as this work. However in their work, adding random noise outperforms vanilla in only some settings. So as I mentioned above, I urge the authors to inspect what is the noise statistics that improves the performance, why is it contributing to the better performance and also when it will deteriorate the performance. Another suggestion is, in the current form of the work, the authors are just adding noise to the embedding representation. This is uncontrolled noise and the interpretability of the method is less. I suggest the authors to add the baseline where the embedding vector of a word is represented as a linear combination of all the related word embedding vectors weighted by the entries of a similarity matrix initialized using cosine similarity of some pre-trained model embedding matrix [2]. They do not need to learn for the task-dependent similarity matrix. I believe from table 6 if the method works with a range of noise initialization then the suggested baseline should perform at par or even better as observed in [2]. It also brings some interpretability to the table.\n\n[2] \"Bi-level Finetuning with Task-dependent Similarity Structure for Low-resource Training.\" Findings of the Association for Computational Linguistics: ACL 2023. 2023.\n\nI appreciate the authors for their research and especially recognizing the need to regularize to boost the performance. However, the current form of the work, as I mentioned in my previous comments, does not provide any understanding of why NEFTune works. The approach seems to be inspired from the previous methods in the literature, though a detailed comparison with those methods is not offered.  The standard deviation is necessary for methods dealing with low-resource. However, the results look promising and the paper is more on the empirical end. \n\nNevertheless, since all my comments are not adequately answered, I would like to reduce my score from 8->6. That said, I recommend the authors to revise the work thoroughly since I believe it has a good potential.", "labeling_timestamp": "2026-01-11T17:23:12.353703", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper provides empirical results showing NEFTune improves performance and notes adopting a noise-scaling rule from prior adversarial-noise work, but it does not offer a mechanistic or theoretical explanation for why adding noise to embeddings yields the observed gains. The text frames the method as a simple trick and describes implementation details and empirical outcomes without deriving or hypothesizing a causal mechanism.", "evidence": "\"In this paper, we propose to add random noise to the embedding vectors of the training data during the forward pass of fine-tuning. We show that this simple trick can improve the outcome of instruction fine-tuning, often by a large margin, with no additional compute or data overhead.\" (Introduction)\n\n\"While our proposed scheme is non-adversarial, we adopt the noise scaling rules from these works.\" (1.1 Related Work)\n\n\"Nevertheless, it is surprising that the conversation quality of such a refined chat model can be so dramatically improved.\" (4 Results, NEFTune Can Improve Chat Models)", "section": "Introduction, 1.1 Related Work, 2 NEFTUNE, 4 Results"}
{"claim": "NEFTune injects uncontrolled noise into embedding representations, reducing interpretability of why performance improves.", "claim_type": "methodology", "paper_id": "0bMmZ3fkCk", "paper_title": "NEFTune: Noisy Embeddings Improve Instruction Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vmTbtNCVEl", "reviewer": "Reviewer_h2qY", "review_text": "Comment: 1) \" generation-time strategy \" This term is usually used for strategy such as beam search decoding, top-k sampling etc. So the result in table-5 does not support the claim.\n\n2) \"Std values\" I am not really convinced by the result in table 6 because of the following reason. By changing the noise statistics the results still hold, which is counter intuitive since there has to be an inflection point beyond which adding noise should hurt the performance. There has to be a detailed study of how and why this algorithm is working. I understand the experiments are running for standard deviation however for this settings of the paper reporting the standard deviation is important. The setting assumed is low resource setting for LLMs and standard deviation defines the reliability of the algorithm proposed. I strongly encourage the authors to report them. \"For AlpacaEval, the reported Std Error is less than 2%.\" this is a vague statement and I would want to see all the numbers.\n\n3) As I mentioned above in the weakness, the authors did not clearly explain why their algorithm works. For instance, in the work stated above [2], they propose to represent a word embedding vector as a linear combination of all the related word embedding vectors weighted by the entries of a similarity matrix. The authors compare the proposed method with a baseline, BFTSS top-k random S, where they add random noise to the embedding representation of a word that improves robustness. This baseline outperforms the vanilla method however lags behind their method. I see NEFTune to be on the same lines as this work. However in their work, adding random noise outperforms vanilla in only some settings. So as I mentioned above, I urge the authors to inspect what is the noise statistics that improves the performance, why is it contributing to the better performance and also when it will deteriorate the performance. Another suggestion is, in the current form of the work, the authors are just adding noise to the embedding representation. This is uncontrolled noise and the interpretability of the method is less. I suggest the authors to add the baseline where the embedding vector of a word is represented as a linear combination of all the related word embedding vectors weighted by the entries of a similarity matrix initialized using cosine similarity of some pre-trained model embedding matrix [2]. They do not need to learn for the task-dependent similarity matrix. I believe from table 6 if the method works with a range of noise initialization then the suggested baseline should perform at par or even better as observed in [2]. It also brings some interpretability to the table.\n\n[2] \"Bi-level Finetuning with Task-dependent Similarity Structure for Low-resource Training.\" Findings of the Association for Computational Linguistics: ACL 2023. 2023.\n\nI appreciate the authors for their research and especially recognizing the need to regularize to boost the performance. However, the current form of the work, as I mentioned in my previous comments, does not provide any understanding of why NEFTune works. The approach seems to be inspired from the previous methods in the literature, though a detailed comparison with those methods is not offered.  The standard deviation is necessary for methods dealing with low-resource. However, the results look promising and the paper is more on the empirical end. \n\nNevertheless, since all my comments are not adequately answered, I would like to reduce my score from 8->6. That said, I recommend the authors to revise the work thoroughly since I believe it has a good potential.", "labeling_timestamp": "2026-01-11T17:23:14.487607", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper does state that NEFTune adds random noise to embedding vectors (so the claim that noise is injected is true), but it explicitly describes a controlled noise generation and scaling scheme (so characterizing the noise as “uncontrolled” is incorrect). The paper does not claim that NEFTune reduces interpretability or provide analysis showing decreased interpretability, so that part is unsupported/undetermined.", "evidence": "Abstract: \"NEFTune adds noise to the embedding vectors during training.\"  Section 2: \"The noise is generated by sampling iid uniform entries, each in the range [-1, 1], and then scaling the entire noise vector by a factor of α/√Ld... This scaling rule... results in a random vector with an expected Euclidean magnitude of approximately α/√3.\"", "section": "Abstract; Section 2 (NEFTune : Noisy Embedding Instruction Fine Tuning)"}
{"claim": "The authors do not analyze which noise statistics improve performance nor identify conditions where added noise will deteriorate results.", "claim_type": "experimental", "paper_id": "0bMmZ3fkCk", "paper_title": "NEFTune: Noisy Embeddings Improve Instruction Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vmTbtNCVEl", "reviewer": "Reviewer_h2qY", "review_text": "Comment: 1) \" generation-time strategy \" This term is usually used for strategy such as beam search decoding, top-k sampling etc. So the result in table-5 does not support the claim.\n\n2) \"Std values\" I am not really convinced by the result in table 6 because of the following reason. By changing the noise statistics the results still hold, which is counter intuitive since there has to be an inflection point beyond which adding noise should hurt the performance. There has to be a detailed study of how and why this algorithm is working. I understand the experiments are running for standard deviation however for this settings of the paper reporting the standard deviation is important. The setting assumed is low resource setting for LLMs and standard deviation defines the reliability of the algorithm proposed. I strongly encourage the authors to report them. \"For AlpacaEval, the reported Std Error is less than 2%.\" this is a vague statement and I would want to see all the numbers.\n\n3) As I mentioned above in the weakness, the authors did not clearly explain why their algorithm works. For instance, in the work stated above [2], they propose to represent a word embedding vector as a linear combination of all the related word embedding vectors weighted by the entries of a similarity matrix. The authors compare the proposed method with a baseline, BFTSS top-k random S, where they add random noise to the embedding representation of a word that improves robustness. This baseline outperforms the vanilla method however lags behind their method. I see NEFTune to be on the same lines as this work. However in their work, adding random noise outperforms vanilla in only some settings. So as I mentioned above, I urge the authors to inspect what is the noise statistics that improves the performance, why is it contributing to the better performance and also when it will deteriorate the performance. Another suggestion is, in the current form of the work, the authors are just adding noise to the embedding representation. This is uncontrolled noise and the interpretability of the method is less. I suggest the authors to add the baseline where the embedding vector of a word is represented as a linear combination of all the related word embedding vectors weighted by the entries of a similarity matrix initialized using cosine similarity of some pre-trained model embedding matrix [2]. They do not need to learn for the task-dependent similarity matrix. I believe from table 6 if the method works with a range of noise initialization then the suggested baseline should perform at par or even better as observed in [2]. It also brings some interpretability to the table.\n\n[2] \"Bi-level Finetuning with Task-dependent Similarity Structure for Low-resource Training.\" Findings of the Association for Computational Linguistics: ACL 2023. 2023.\n\nI appreciate the authors for their research and especially recognizing the need to regularize to boost the performance. However, the current form of the work, as I mentioned in my previous comments, does not provide any understanding of why NEFTune works. The approach seems to be inspired from the previous methods in the literature, though a detailed comparison with those methods is not offered.  The standard deviation is necessary for methods dealing with low-resource. However, the results look promising and the paper is more on the empirical end. \n\nNevertheless, since all my comments are not adequately answered, I would like to reduce my score from 8->6. That said, I recommend the authors to revise the work thoroughly since I believe it has a good potential.", "labeling_timestamp": "2026-01-11T17:23:29.496122", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper specifies a single noise scheme (iid uniform in [-1,1] scaled by α/√(Ld)) and only mentions a coarse hyperparameter sweep for α; it does not compare different noise statistics or systematically identify conditions where noise worsens performance (only an unquantified note that some capabilities 'may be affected').", "evidence": "Section 2: \"The noise is generated by sampling iid uniform entries, each in the range [ -1 , 1] , and then scaling the entire noise vector by a factor of α/ √ Ld...\"; Section 3.2: \"We set our hyperparameters through a coarse sweep on LLaMA-1 ( 7 B) trained on the Alpaca dataset, where we see 6% improvement over the standard Alpaca model. We use these as the defaults on all models.\"; Section 4: \"we note that some capabilities of this checkpoint model may be affected like its ability to refrain from outputting toxic behavior.\"", "section": "Section 2 (Noise formulation); Section 3.2 (Hyperparameter sweep); Section 4 (Results / NEFTune Can Improve Chat Models)"}
{"claim": "The paper lacks a baseline that represents each embedding as a cosine-similarity-weighted linear combination of related embeddings, as used in prior work.", "claim_type": "baseline", "paper_id": "0bMmZ3fkCk", "paper_title": "NEFTune: Noisy Embeddings Improve Instruction Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vmTbtNCVEl", "reviewer": "Reviewer_h2qY", "review_text": "Comment: 1) \" generation-time strategy \" This term is usually used for strategy such as beam search decoding, top-k sampling etc. So the result in table-5 does not support the claim.\n\n2) \"Std values\" I am not really convinced by the result in table 6 because of the following reason. By changing the noise statistics the results still hold, which is counter intuitive since there has to be an inflection point beyond which adding noise should hurt the performance. There has to be a detailed study of how and why this algorithm is working. I understand the experiments are running for standard deviation however for this settings of the paper reporting the standard deviation is important. The setting assumed is low resource setting for LLMs and standard deviation defines the reliability of the algorithm proposed. I strongly encourage the authors to report them. \"For AlpacaEval, the reported Std Error is less than 2%.\" this is a vague statement and I would want to see all the numbers.\n\n3) As I mentioned above in the weakness, the authors did not clearly explain why their algorithm works. For instance, in the work stated above [2], they propose to represent a word embedding vector as a linear combination of all the related word embedding vectors weighted by the entries of a similarity matrix. The authors compare the proposed method with a baseline, BFTSS top-k random S, where they add random noise to the embedding representation of a word that improves robustness. This baseline outperforms the vanilla method however lags behind their method. I see NEFTune to be on the same lines as this work. However in their work, adding random noise outperforms vanilla in only some settings. So as I mentioned above, I urge the authors to inspect what is the noise statistics that improves the performance, why is it contributing to the better performance and also when it will deteriorate the performance. Another suggestion is, in the current form of the work, the authors are just adding noise to the embedding representation. This is uncontrolled noise and the interpretability of the method is less. I suggest the authors to add the baseline where the embedding vector of a word is represented as a linear combination of all the related word embedding vectors weighted by the entries of a similarity matrix initialized using cosine similarity of some pre-trained model embedding matrix [2]. They do not need to learn for the task-dependent similarity matrix. I believe from table 6 if the method works with a range of noise initialization then the suggested baseline should perform at par or even better as observed in [2]. It also brings some interpretability to the table.\n\n[2] \"Bi-level Finetuning with Task-dependent Similarity Structure for Low-resource Training.\" Findings of the Association for Computational Linguistics: ACL 2023. 2023.\n\nI appreciate the authors for their research and especially recognizing the need to regularize to boost the performance. However, the current form of the work, as I mentioned in my previous comments, does not provide any understanding of why NEFTune works. The approach seems to be inspired from the previous methods in the literature, though a detailed comparison with those methods is not offered.  The standard deviation is necessary for methods dealing with low-resource. However, the results look promising and the paper is more on the empirical end. \n\nNevertheless, since all my comments are not adequately answered, I would like to reduce my score from 8->6. That said, I recommend the authors to revise the work thoroughly since I believe it has a good potential.", "labeling_timestamp": "2026-01-11T17:23:32.818871", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper only evaluates NEFTune (adding random noise to embeddings) and compares to standard finetuning/adversarial-noise references; it does not present or evaluate a baseline that replaces each embedding by a cosine-similarity-weighted linear combination of related embeddings.", "evidence": "\"NEFTune then departs from standard training by adding a random noise vector to the embeddings. The noise is generated by sampling iid uniform entries, each in the range [-1, 1], and then scaling the entire noise vector by a factor of α/√Ld...\"", "section": "Section 2 (NEFTune: Noisy Embedding Instruction Finetuning)"}
{"claim": "The authors did not include a detailed comparison with earlier related methods such as bi-level finetuning with task-dependent similarity structure.", "claim_type": "baseline", "paper_id": "0bMmZ3fkCk", "paper_title": "NEFTune: Noisy Embeddings Improve Instruction Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vmTbtNCVEl", "reviewer": "Reviewer_h2qY", "review_text": "Comment: 1) \" generation-time strategy \" This term is usually used for strategy such as beam search decoding, top-k sampling etc. So the result in table-5 does not support the claim.\n\n2) \"Std values\" I am not really convinced by the result in table 6 because of the following reason. By changing the noise statistics the results still hold, which is counter intuitive since there has to be an inflection point beyond which adding noise should hurt the performance. There has to be a detailed study of how and why this algorithm is working. I understand the experiments are running for standard deviation however for this settings of the paper reporting the standard deviation is important. The setting assumed is low resource setting for LLMs and standard deviation defines the reliability of the algorithm proposed. I strongly encourage the authors to report them. \"For AlpacaEval, the reported Std Error is less than 2%.\" this is a vague statement and I would want to see all the numbers.\n\n3) As I mentioned above in the weakness, the authors did not clearly explain why their algorithm works. For instance, in the work stated above [2], they propose to represent a word embedding vector as a linear combination of all the related word embedding vectors weighted by the entries of a similarity matrix. The authors compare the proposed method with a baseline, BFTSS top-k random S, where they add random noise to the embedding representation of a word that improves robustness. This baseline outperforms the vanilla method however lags behind their method. I see NEFTune to be on the same lines as this work. However in their work, adding random noise outperforms vanilla in only some settings. So as I mentioned above, I urge the authors to inspect what is the noise statistics that improves the performance, why is it contributing to the better performance and also when it will deteriorate the performance. Another suggestion is, in the current form of the work, the authors are just adding noise to the embedding representation. This is uncontrolled noise and the interpretability of the method is less. I suggest the authors to add the baseline where the embedding vector of a word is represented as a linear combination of all the related word embedding vectors weighted by the entries of a similarity matrix initialized using cosine similarity of some pre-trained model embedding matrix [2]. They do not need to learn for the task-dependent similarity matrix. I believe from table 6 if the method works with a range of noise initialization then the suggested baseline should perform at par or even better as observed in [2]. It also brings some interpretability to the table.\n\n[2] \"Bi-level Finetuning with Task-dependent Similarity Structure for Low-resource Training.\" Findings of the Association for Computational Linguistics: ACL 2023. 2023.\n\nI appreciate the authors for their research and especially recognizing the need to regularize to boost the performance. However, the current form of the work, as I mentioned in my previous comments, does not provide any understanding of why NEFTune works. The approach seems to be inspired from the previous methods in the literature, though a detailed comparison with those methods is not offered.  The standard deviation is necessary for methods dealing with low-resource. However, the results look promising and the paper is more on the empirical end. \n\nNevertheless, since all my comments are not adequately answered, I would like to reduce my score from 8->6. That said, I recommend the authors to revise the work thoroughly since I believe it has a good potential.", "labeling_timestamp": "2026-01-11T17:23:30.034454", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper's Related Work discusses noisy-input and adversarial methods (e.g., FreeLB) and describes dataset and baseline comparisons, but it does not mention or provide a detailed empirical comparison to methods like bi-level finetuning with task-dependent similarity structure.", "evidence": "It should be noted that noisy inputs have been used to improve models in various ways. The first instance of noise being used to improve language models was the FreeLB method by Zhu et al. (2019), who observed that adversarial perturbations boosted the performance of MLM models.", "section": "1.1 RELATED WORK"}
{"claim": "The authors did not test the suggested similarity-initialized embedding combination baseline, which might match or outperform NEFTune according to prior observations.", "claim_type": "baseline", "paper_id": "0bMmZ3fkCk", "paper_title": "NEFTune: Noisy Embeddings Improve Instruction Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vmTbtNCVEl", "reviewer": "Reviewer_h2qY", "review_text": "Comment: 1) \" generation-time strategy \" This term is usually used for strategy such as beam search decoding, top-k sampling etc. So the result in table-5 does not support the claim.\n\n2) \"Std values\" I am not really convinced by the result in table 6 because of the following reason. By changing the noise statistics the results still hold, which is counter intuitive since there has to be an inflection point beyond which adding noise should hurt the performance. There has to be a detailed study of how and why this algorithm is working. I understand the experiments are running for standard deviation however for this settings of the paper reporting the standard deviation is important. The setting assumed is low resource setting for LLMs and standard deviation defines the reliability of the algorithm proposed. I strongly encourage the authors to report them. \"For AlpacaEval, the reported Std Error is less than 2%.\" this is a vague statement and I would want to see all the numbers.\n\n3) As I mentioned above in the weakness, the authors did not clearly explain why their algorithm works. For instance, in the work stated above [2], they propose to represent a word embedding vector as a linear combination of all the related word embedding vectors weighted by the entries of a similarity matrix. The authors compare the proposed method with a baseline, BFTSS top-k random S, where they add random noise to the embedding representation of a word that improves robustness. This baseline outperforms the vanilla method however lags behind their method. I see NEFTune to be on the same lines as this work. However in their work, adding random noise outperforms vanilla in only some settings. So as I mentioned above, I urge the authors to inspect what is the noise statistics that improves the performance, why is it contributing to the better performance and also when it will deteriorate the performance. Another suggestion is, in the current form of the work, the authors are just adding noise to the embedding representation. This is uncontrolled noise and the interpretability of the method is less. I suggest the authors to add the baseline where the embedding vector of a word is represented as a linear combination of all the related word embedding vectors weighted by the entries of a similarity matrix initialized using cosine similarity of some pre-trained model embedding matrix [2]. They do not need to learn for the task-dependent similarity matrix. I believe from table 6 if the method works with a range of noise initialization then the suggested baseline should perform at par or even better as observed in [2]. It also brings some interpretability to the table.\n\n[2] \"Bi-level Finetuning with Task-dependent Similarity Structure for Low-resource Training.\" Findings of the Association for Computational Linguistics: ACL 2023. 2023.\n\nI appreciate the authors for their research and especially recognizing the need to regularize to boost the performance. However, the current form of the work, as I mentioned in my previous comments, does not provide any understanding of why NEFTune works. The approach seems to be inspired from the previous methods in the literature, though a detailed comparison with those methods is not offered.  The standard deviation is necessary for methods dealing with low-resource. However, the results look promising and the paper is more on the empirical end. \n\nNevertheless, since all my comments are not adequately answered, I would like to reduce my score from 8->6. That said, I recommend the authors to revise the work thoroughly since I believe it has a good potential.", "labeling_timestamp": "2026-01-11T17:23:41.513905", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes and evaluates only the NEFTune approach (adding random noise to embeddings) and standard finetuning baselines across datasets; it does not describe or evaluate a 'similarity-initialized embedding combination' baseline. There is no mention of testing such an alternative in the methods, experiments, or results sections.", "evidence": "\"NEFTune adds noise to the embedding vectors during training.\" (ABSTRACT)\n\"NEFTune then departs from standard training by adding a random noise vector to the embeddings.\" (Section 2 NEFTUNE)", "section": "ABSTRACT; Section 2 NEFTUNE"}
{"claim": "The paper does not study the range of noise initializations and their impact on the robustness and reliability of the proposed method.", "claim_type": "experimental", "paper_id": "0bMmZ3fkCk", "paper_title": "NEFTune: Noisy Embeddings Improve Instruction Finetuning", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vmTbtNCVEl", "reviewer": "Reviewer_h2qY", "review_text": "Comment: 1) \" generation-time strategy \" This term is usually used for strategy such as beam search decoding, top-k sampling etc. So the result in table-5 does not support the claim.\n\n2) \"Std values\" I am not really convinced by the result in table 6 because of the following reason. By changing the noise statistics the results still hold, which is counter intuitive since there has to be an inflection point beyond which adding noise should hurt the performance. There has to be a detailed study of how and why this algorithm is working. I understand the experiments are running for standard deviation however for this settings of the paper reporting the standard deviation is important. The setting assumed is low resource setting for LLMs and standard deviation defines the reliability of the algorithm proposed. I strongly encourage the authors to report them. \"For AlpacaEval, the reported Std Error is less than 2%.\" this is a vague statement and I would want to see all the numbers.\n\n3) As I mentioned above in the weakness, the authors did not clearly explain why their algorithm works. For instance, in the work stated above [2], they propose to represent a word embedding vector as a linear combination of all the related word embedding vectors weighted by the entries of a similarity matrix. The authors compare the proposed method with a baseline, BFTSS top-k random S, where they add random noise to the embedding representation of a word that improves robustness. This baseline outperforms the vanilla method however lags behind their method. I see NEFTune to be on the same lines as this work. However in their work, adding random noise outperforms vanilla in only some settings. So as I mentioned above, I urge the authors to inspect what is the noise statistics that improves the performance, why is it contributing to the better performance and also when it will deteriorate the performance. Another suggestion is, in the current form of the work, the authors are just adding noise to the embedding representation. This is uncontrolled noise and the interpretability of the method is less. I suggest the authors to add the baseline where the embedding vector of a word is represented as a linear combination of all the related word embedding vectors weighted by the entries of a similarity matrix initialized using cosine similarity of some pre-trained model embedding matrix [2]. They do not need to learn for the task-dependent similarity matrix. I believe from table 6 if the method works with a range of noise initialization then the suggested baseline should perform at par or even better as observed in [2]. It also brings some interpretability to the table.\n\n[2] \"Bi-level Finetuning with Task-dependent Similarity Structure for Low-resource Training.\" Findings of the Association for Computational Linguistics: ACL 2023. 2023.\n\nI appreciate the authors for their research and especially recognizing the need to regularize to boost the performance. However, the current form of the work, as I mentioned in my previous comments, does not provide any understanding of why NEFTune works. The approach seems to be inspired from the previous methods in the literature, though a detailed comparison with those methods is not offered.  The standard deviation is necessary for methods dealing with low-resource. However, the results look promising and the paper is more on the empirical end. \n\nNevertheless, since all my comments are not adequately answered, I would like to reduce my score from 8->6. That said, I recommend the authors to revise the work thoroughly since I believe it has a good potential.", "labeling_timestamp": "2026-01-11T17:23:37.888652", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper defines a single noise generation scheme (iid uniform in [-1,1] scaled by α/√(Ld)) and notes only a 'coarse sweep' to set hyperparameters; it does not present a systematic study or ablation over different noise initializations, distributions, or detailed robustness/reliability analyses in the provided content.", "evidence": "“The noise is generated by sampling iid uniform entries, each in the range [-1, 1], and then scaling the entire noise vector by a factor of α/√Ld, where L is the sequence length, d is the embedding dimension, and α is a tunable parameter.”\n\n“We set our hyperparameters through a coarse sweep on LLaMA-1 (7B) trained on the Alpaca dataset, where we see 6% improvement over the standard Alpaca model. We use these as the defaults on all models.”", "section": "Section 2 (NEFTune : Noisy Embedding Instruction Finetuning) and Section 3.2 (Instruction Finetuning Datasets / Experimental set-up)"}
{"claim": "The experimental settings derived from HighwayEnv are too restrictive to extrapolate how LLM-based reasoning with retrieval and few-shot prompting would perform on diverse real-world scenes.", "claim_type": "subjective", "paper_id": "OqTMUPuLuC", "paper_title": "DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "xN8k5wi0lW", "reviewer": "Reviewer_48LR", "review_text": "Summary: The paper presents a framework for utilizing the few shot and self-correction capabilities of LLMs for the task of AV planning, where the following abilities of the framework are highlighted:\n- Store successful experiences in memory and leverage them to improve future rollouts through similarity retrieval and usage in few-shot prompting.\n- Ability to learn from unsuccessful experiences (ones with collisions) by applying LLM self-correction and storing the modified experience among the successful experiences in the memory\n\nThe above components, dubbed as reasoning and reflection modules respectively, are integrated along with memory in a closed loop setting without any back-propagation objective. \n\nA number of prompting techniques including chain-of-thought and few-shot prompting are used to get better reasoning. The environment used for experiments (Highway-Env) only requires four discrete decisions, hence the LLM is prompted to select one amongst these four decisions for each frame after going through CoT reasoning.\n\nThe experiments are used to demonstrate the following key claims:\n- The memory module combined with few-shot prompting provides much better results than using no memory module (zero-shot) or using lesser shots. \n- The more the number of experiences in the memory, the better.\n- The ability to generalize is better with more few-shot experiences fed into the LLM\n- Adding successful and corrected experiences both help in improving performance\n- Better generalization capability compared to RL method GRAD.\n\nStrengths: - The motivating idea of human knowledge distillation for AV planning is sound, interesting, and under-explored.\n\n- The overall framework formulation towards leveraging LLMs via appropriate prompting, retrieval, and self-correction is interesting and well set up. It would have been exciting to see formulations for LLMs assisting planning stacks (instead of directly doing discrete action decision making) - which could be much more valuable to existing systems.  \n\n- The flywheel effect created from storing both successful and unsuccessful + corrected experiences in memory is an important contribution.\n\n- The paper provides a good foundation for other exciting work to build upon, especially with the promise to open source upon acceptance.\n\n- The experiments are fairly extensive towards investigating all the different components of the proposed framework.\n\nWeaknesses: -  One of the main proposed advantages is better generalization through instilling human knowledge-driven capabilities instead of a data-driven only approach. However, the experimental settings derived from HighwayEnv are too restrictive to help extrapolate how such LLM based reasoning would perform on diverse new scenes using retrieval + few shot prompting. While it is perfectly fine to work with restricted settings and smaller datasets for new research work, the bridge to answer the most interesting questions is too long.\n\n- As mentioned in strengths section, providing directions and initial experiments on assisting planning stacks (instead of directly doing discrete action decision making) could provide a lot of value.\n\n- The experiment settings used to demonstrate generalization are not too convincing. The number of lanes and traffic density is changed, but this is still an extremely similar environment where the retrieved few-shot scenarios could be nearly directly applicable.\n\n- Under the above setting, it is possible that with a large enough memory module the task reduces to simply copying the answer from one of the retrieved experiences. It would be good to see a baseline where the decision from one of the retrieved experiences is used as is (voting with mixture of experts or winner takes all)\n\n- The metric movement with CitySim in Figure 7b and Table 1 correction row do not seem significant to make the corresponding claims?\n\n- Nit: The key frame sampling for successful experiences seems like an important detail that has not been explained.\n\n- Minor nit: The claim for this being the first work addressing AV planning via leveraging LLMs might need to be revised with recent papers like GPT driver (depending on chronology).\n\nQuestions: - What kind of diverse interactions do we get from the Highway-env simulator? Would it be possible to evaluate the framework under more interactive / challenging conditions, especially wrt agent interactions? It would be interesting to see the generalization to intersections, interactions with peds, aggressive agents etc.\n\n- The correction experiences intuitively should provide a strong boost to performance since they are akin to hard example mining and injecting reasoning about the negative outcomes. However the corresponding results in Table 1 do not show strong improvements. Is it possible understudied and warrants more extensive experimentations?", "labeling_timestamp": "2026-01-11T17:24:02.198043", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpts describe a closed-loop driving environment, few-shot retrieval from a Memory module, and evaluation against RL baselines, but they do not mention HighwayEnv or give details of the experimental environment setup. Therefore there is insufficient information to determine whether the experiments were derived from HighwayEnv or whether those settings are too restrictive to extrapolate to diverse real-world scenes.", "evidence": "\"We design a closed-loop driving environment and prove that DiLu can perform better and better with the experience accumulated in the memory module.\"; \"Remarkably, with only 40 memory items, DiLu achieves comparable performance to the reinforcement learning (RL) based methods that have extensively trained over 600,000 episodes, but with a much stronger generalization ability to diverse scenarios.\"; \"DiLu's ability to directly acquire experiences from real-world datasets highlights its potential to be deployed on practical autonomous driving systems.\"; \"a prompt generator is employed to combine this scenario description with the few-shot experiences of similar situations, which retrieved from the Memory module.\"", "section": "1 Introduction; 3.1 Overview; 3.2 Memory module"}
{"claim": "The paper lacks experiments or formulations demonstrating how LLMs could assist existing planning stacks instead of directly making discrete action decisions.", "claim_type": "experimental", "paper_id": "OqTMUPuLuC", "paper_title": "DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "xN8k5wi0lW", "reviewer": "Reviewer_48LR", "review_text": "Summary: The paper presents a framework for utilizing the few shot and self-correction capabilities of LLMs for the task of AV planning, where the following abilities of the framework are highlighted:\n- Store successful experiences in memory and leverage them to improve future rollouts through similarity retrieval and usage in few-shot prompting.\n- Ability to learn from unsuccessful experiences (ones with collisions) by applying LLM self-correction and storing the modified experience among the successful experiences in the memory\n\nThe above components, dubbed as reasoning and reflection modules respectively, are integrated along with memory in a closed loop setting without any back-propagation objective. \n\nA number of prompting techniques including chain-of-thought and few-shot prompting are used to get better reasoning. The environment used for experiments (Highway-Env) only requires four discrete decisions, hence the LLM is prompted to select one amongst these four decisions for each frame after going through CoT reasoning.\n\nThe experiments are used to demonstrate the following key claims:\n- The memory module combined with few-shot prompting provides much better results than using no memory module (zero-shot) or using lesser shots. \n- The more the number of experiences in the memory, the better.\n- The ability to generalize is better with more few-shot experiences fed into the LLM\n- Adding successful and corrected experiences both help in improving performance\n- Better generalization capability compared to RL method GRAD.\n\nStrengths: - The motivating idea of human knowledge distillation for AV planning is sound, interesting, and under-explored.\n\n- The overall framework formulation towards leveraging LLMs via appropriate prompting, retrieval, and self-correction is interesting and well set up. It would have been exciting to see formulations for LLMs assisting planning stacks (instead of directly doing discrete action decision making) - which could be much more valuable to existing systems.  \n\n- The flywheel effect created from storing both successful and unsuccessful + corrected experiences in memory is an important contribution.\n\n- The paper provides a good foundation for other exciting work to build upon, especially with the promise to open source upon acceptance.\n\n- The experiments are fairly extensive towards investigating all the different components of the proposed framework.\n\nWeaknesses: -  One of the main proposed advantages is better generalization through instilling human knowledge-driven capabilities instead of a data-driven only approach. However, the experimental settings derived from HighwayEnv are too restrictive to help extrapolate how such LLM based reasoning would perform on diverse new scenes using retrieval + few shot prompting. While it is perfectly fine to work with restricted settings and smaller datasets for new research work, the bridge to answer the most interesting questions is too long.\n\n- As mentioned in strengths section, providing directions and initial experiments on assisting planning stacks (instead of directly doing discrete action decision making) could provide a lot of value.\n\n- The experiment settings used to demonstrate generalization are not too convincing. The number of lanes and traffic density is changed, but this is still an extremely similar environment where the retrieved few-shot scenarios could be nearly directly applicable.\n\n- Under the above setting, it is possible that with a large enough memory module the task reduces to simply copying the answer from one of the retrieved experiences. It would be good to see a baseline where the decision from one of the retrieved experiences is used as is (voting with mixture of experts or winner takes all)\n\n- The metric movement with CitySim in Figure 7b and Table 1 correction row do not seem significant to make the corresponding claims?\n\n- Nit: The key frame sampling for successful experiences seems like an important detail that has not been explained.\n\n- Minor nit: The claim for this being the first work addressing AV planning via leveraging LLMs might need to be revised with recent papers like GPT driver (depending on chronology).\n\nQuestions: - What kind of diverse interactions do we get from the Highway-env simulator? Would it be possible to evaluate the framework under more interactive / challenging conditions, especially wrt agent interactions? It would be interesting to see the generalization to intersections, interactions with peds, aggressive agents etc.\n\n- The correction experiences intuitively should provide a strong boost to performance since they are akin to hard example mining and injecting reasoning about the negative outcomes. However the corresponding results in Table 1 do not show strong improvements. Is it possible understudied and warrants more extensive experimentations?", "labeling_timestamp": "2026-01-11T17:23:56.799189", "model": "gpt-5-mini", "label": "Supported", "justification": "Throughout the paper the LLM is described and evaluated as directly producing driving decisions (via a 'decision decoder' and controlling the ego vehicle). The paper describes no experiments, formulations, or system variants that show how an LLM could be used to assist or augment an existing planning stack instead of directly outputting discrete actions.", "evidence": "1) \"These prompts are then fed into an out-of-the-box Large Language Model (LLM), and the decision decoder make an action by decoding LLM's response.\"  (Section 3.1 Overview)\n2) \"It then employs the Reflection Module to identify safe and unsafe decisions produced by the Reasoning Module, subsequently refining them into correct decisions using the knowledge embedded in the LLM.\"  (Introduction)\n3) \"In DiLu, the Reasoning module can observe the environment, generate prompts by combining scenario descriptions and experiences in the Memory module, and decode responses from the LLM to finish decision-making.\"  (Figure 2 caption / Methodology)", "section": "3.1 Overview; Introduction"}
{"claim": "The generalization experiments are unconvincing because only lane count and traffic density change, leaving environments extremely similar and possibly directly applicable retrieved few-shot scenarios.", "claim_type": "experimental", "paper_id": "OqTMUPuLuC", "paper_title": "DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "xN8k5wi0lW", "reviewer": "Reviewer_48LR", "review_text": "Summary: The paper presents a framework for utilizing the few shot and self-correction capabilities of LLMs for the task of AV planning, where the following abilities of the framework are highlighted:\n- Store successful experiences in memory and leverage them to improve future rollouts through similarity retrieval and usage in few-shot prompting.\n- Ability to learn from unsuccessful experiences (ones with collisions) by applying LLM self-correction and storing the modified experience among the successful experiences in the memory\n\nThe above components, dubbed as reasoning and reflection modules respectively, are integrated along with memory in a closed loop setting without any back-propagation objective. \n\nA number of prompting techniques including chain-of-thought and few-shot prompting are used to get better reasoning. The environment used for experiments (Highway-Env) only requires four discrete decisions, hence the LLM is prompted to select one amongst these four decisions for each frame after going through CoT reasoning.\n\nThe experiments are used to demonstrate the following key claims:\n- The memory module combined with few-shot prompting provides much better results than using no memory module (zero-shot) or using lesser shots. \n- The more the number of experiences in the memory, the better.\n- The ability to generalize is better with more few-shot experiences fed into the LLM\n- Adding successful and corrected experiences both help in improving performance\n- Better generalization capability compared to RL method GRAD.\n\nStrengths: - The motivating idea of human knowledge distillation for AV planning is sound, interesting, and under-explored.\n\n- The overall framework formulation towards leveraging LLMs via appropriate prompting, retrieval, and self-correction is interesting and well set up. It would have been exciting to see formulations for LLMs assisting planning stacks (instead of directly doing discrete action decision making) - which could be much more valuable to existing systems.  \n\n- The flywheel effect created from storing both successful and unsuccessful + corrected experiences in memory is an important contribution.\n\n- The paper provides a good foundation for other exciting work to build upon, especially with the promise to open source upon acceptance.\n\n- The experiments are fairly extensive towards investigating all the different components of the proposed framework.\n\nWeaknesses: -  One of the main proposed advantages is better generalization through instilling human knowledge-driven capabilities instead of a data-driven only approach. However, the experimental settings derived from HighwayEnv are too restrictive to help extrapolate how such LLM based reasoning would perform on diverse new scenes using retrieval + few shot prompting. While it is perfectly fine to work with restricted settings and smaller datasets for new research work, the bridge to answer the most interesting questions is too long.\n\n- As mentioned in strengths section, providing directions and initial experiments on assisting planning stacks (instead of directly doing discrete action decision making) could provide a lot of value.\n\n- The experiment settings used to demonstrate generalization are not too convincing. The number of lanes and traffic density is changed, but this is still an extremely similar environment where the retrieved few-shot scenarios could be nearly directly applicable.\n\n- Under the above setting, it is possible that with a large enough memory module the task reduces to simply copying the answer from one of the retrieved experiences. It would be good to see a baseline where the decision from one of the retrieved experiences is used as is (voting with mixture of experts or winner takes all)\n\n- The metric movement with CitySim in Figure 7b and Table 1 correction row do not seem significant to make the corresponding claims?\n\n- Nit: The key frame sampling for successful experiences seems like an important detail that has not been explained.\n\n- Minor nit: The claim for this being the first work addressing AV planning via leveraging LLMs might need to be revised with recent papers like GPT driver (depending on chronology).\n\nQuestions: - What kind of diverse interactions do we get from the Highway-env simulator? Would it be possible to evaluate the framework under more interactive / challenging conditions, especially wrt agent interactions? It would be interesting to see the generalization to intersections, interactions with peds, aggressive agents etc.\n\n- The correction experiences intuitively should provide a strong boost to performance since they are akin to hard example mining and injecting reasoning about the negative outcomes. However the corresponding results in Table 1 do not show strong improvements. Is it possible understudied and warrants more extensive experimentations?", "labeling_timestamp": "2026-01-11T17:24:17.717721", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper claims DiLu has stronger generalization (Introduction) and describes few-shot memory recall (Memory module), but the provided content does not describe the specific generalization experiment setups or state that only lane count and traffic density were varied. Therefore there is insufficient information to confirm the reviewer's claim about which factors changed and whether retrieved few-shot scenarios were directly applicable.", "evidence": "1) \"Remarkably, with only 40 memory items, DiLu achieves comparable performance to the reinforcement learning (RL) based methods that have extensively trained over 600,000 episodes, but with a much stronger generalization ability to diverse scenarios.\" 2) \"This key is then clustered and searched to find the closest scenarios (Johnson et al., 2019) in the memory module and their corresponding reasoning processes, or memories. These recalled memories are provided to the agent in a few-shot format to assist in making accurate reasoning and decisions for the current scenario.\"", "section": "1 Introduction; 3.2 Memory module"}
{"claim": "With a sufficiently large memory module, the task may degenerate to copying an answer from a retrieved experience rather than demonstrating true reasoning or generalization.", "claim_type": "experimental", "paper_id": "OqTMUPuLuC", "paper_title": "DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "xN8k5wi0lW", "reviewer": "Reviewer_48LR", "review_text": "Summary: The paper presents a framework for utilizing the few shot and self-correction capabilities of LLMs for the task of AV planning, where the following abilities of the framework are highlighted:\n- Store successful experiences in memory and leverage them to improve future rollouts through similarity retrieval and usage in few-shot prompting.\n- Ability to learn from unsuccessful experiences (ones with collisions) by applying LLM self-correction and storing the modified experience among the successful experiences in the memory\n\nThe above components, dubbed as reasoning and reflection modules respectively, are integrated along with memory in a closed loop setting without any back-propagation objective. \n\nA number of prompting techniques including chain-of-thought and few-shot prompting are used to get better reasoning. The environment used for experiments (Highway-Env) only requires four discrete decisions, hence the LLM is prompted to select one amongst these four decisions for each frame after going through CoT reasoning.\n\nThe experiments are used to demonstrate the following key claims:\n- The memory module combined with few-shot prompting provides much better results than using no memory module (zero-shot) or using lesser shots. \n- The more the number of experiences in the memory, the better.\n- The ability to generalize is better with more few-shot experiences fed into the LLM\n- Adding successful and corrected experiences both help in improving performance\n- Better generalization capability compared to RL method GRAD.\n\nStrengths: - The motivating idea of human knowledge distillation for AV planning is sound, interesting, and under-explored.\n\n- The overall framework formulation towards leveraging LLMs via appropriate prompting, retrieval, and self-correction is interesting and well set up. It would have been exciting to see formulations for LLMs assisting planning stacks (instead of directly doing discrete action decision making) - which could be much more valuable to existing systems.  \n\n- The flywheel effect created from storing both successful and unsuccessful + corrected experiences in memory is an important contribution.\n\n- The paper provides a good foundation for other exciting work to build upon, especially with the promise to open source upon acceptance.\n\n- The experiments are fairly extensive towards investigating all the different components of the proposed framework.\n\nWeaknesses: -  One of the main proposed advantages is better generalization through instilling human knowledge-driven capabilities instead of a data-driven only approach. However, the experimental settings derived from HighwayEnv are too restrictive to help extrapolate how such LLM based reasoning would perform on diverse new scenes using retrieval + few shot prompting. While it is perfectly fine to work with restricted settings and smaller datasets for new research work, the bridge to answer the most interesting questions is too long.\n\n- As mentioned in strengths section, providing directions and initial experiments on assisting planning stacks (instead of directly doing discrete action decision making) could provide a lot of value.\n\n- The experiment settings used to demonstrate generalization are not too convincing. The number of lanes and traffic density is changed, but this is still an extremely similar environment where the retrieved few-shot scenarios could be nearly directly applicable.\n\n- Under the above setting, it is possible that with a large enough memory module the task reduces to simply copying the answer from one of the retrieved experiences. It would be good to see a baseline where the decision from one of the retrieved experiences is used as is (voting with mixture of experts or winner takes all)\n\n- The metric movement with CitySim in Figure 7b and Table 1 correction row do not seem significant to make the corresponding claims?\n\n- Nit: The key frame sampling for successful experiences seems like an important detail that has not been explained.\n\n- Minor nit: The claim for this being the first work addressing AV planning via leveraging LLMs might need to be revised with recent papers like GPT driver (depending on chronology).\n\nQuestions: - What kind of diverse interactions do we get from the Highway-env simulator? Would it be possible to evaluate the framework under more interactive / challenging conditions, especially wrt agent interactions? It would be interesting to see the generalization to intersections, interactions with peds, aggressive agents etc.\n\n- The correction experiences intuitively should provide a strong boost to performance since they are akin to hard example mining and injecting reasoning about the negative outcomes. However the corresponding results in Table 1 do not show strong improvements. Is it possible understudied and warrants more extensive experimentations?", "labeling_timestamp": "2026-01-11T17:24:03.335493", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper describes retrieval from a Memory module and using few-shot recalled experiences to guide LLM decision-making (Sections 3.1–3.2), but it does not state or analyze the risk that a sufficiently large memory causes the task to degenerate into mere copying from retrieved experiences rather than reasoning or generalization. Thus the claim is not addressed by the paper.", "evidence": "\"Before making a decision, the current driving scenario is embedded into a vector, which serves as the memory key. This key is then clustered and searched to find the closest scenarios (Johnson et al., 2019) in the memory module and their corresponding reasoning processes, or memories. These recalled memories are provided to the agent in a few-shot format to assist in making accurate reasoning and decisions for the current scenario.\"", "section": "3.2 Memory module"}
{"claim": "The metric changes reported for CitySim (Figure 7b) and the Table 1 correction row appear too small to substantiate the corresponding performance improvement claims.", "claim_type": "quantitative", "paper_id": "OqTMUPuLuC", "paper_title": "DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "xN8k5wi0lW", "reviewer": "Reviewer_48LR", "review_text": "Summary: The paper presents a framework for utilizing the few shot and self-correction capabilities of LLMs for the task of AV planning, where the following abilities of the framework are highlighted:\n- Store successful experiences in memory and leverage them to improve future rollouts through similarity retrieval and usage in few-shot prompting.\n- Ability to learn from unsuccessful experiences (ones with collisions) by applying LLM self-correction and storing the modified experience among the successful experiences in the memory\n\nThe above components, dubbed as reasoning and reflection modules respectively, are integrated along with memory in a closed loop setting without any back-propagation objective. \n\nA number of prompting techniques including chain-of-thought and few-shot prompting are used to get better reasoning. The environment used for experiments (Highway-Env) only requires four discrete decisions, hence the LLM is prompted to select one amongst these four decisions for each frame after going through CoT reasoning.\n\nThe experiments are used to demonstrate the following key claims:\n- The memory module combined with few-shot prompting provides much better results than using no memory module (zero-shot) or using lesser shots. \n- The more the number of experiences in the memory, the better.\n- The ability to generalize is better with more few-shot experiences fed into the LLM\n- Adding successful and corrected experiences both help in improving performance\n- Better generalization capability compared to RL method GRAD.\n\nStrengths: - The motivating idea of human knowledge distillation for AV planning is sound, interesting, and under-explored.\n\n- The overall framework formulation towards leveraging LLMs via appropriate prompting, retrieval, and self-correction is interesting and well set up. It would have been exciting to see formulations for LLMs assisting planning stacks (instead of directly doing discrete action decision making) - which could be much more valuable to existing systems.  \n\n- The flywheel effect created from storing both successful and unsuccessful + corrected experiences in memory is an important contribution.\n\n- The paper provides a good foundation for other exciting work to build upon, especially with the promise to open source upon acceptance.\n\n- The experiments are fairly extensive towards investigating all the different components of the proposed framework.\n\nWeaknesses: -  One of the main proposed advantages is better generalization through instilling human knowledge-driven capabilities instead of a data-driven only approach. However, the experimental settings derived from HighwayEnv are too restrictive to help extrapolate how such LLM based reasoning would perform on diverse new scenes using retrieval + few shot prompting. While it is perfectly fine to work with restricted settings and smaller datasets for new research work, the bridge to answer the most interesting questions is too long.\n\n- As mentioned in strengths section, providing directions and initial experiments on assisting planning stacks (instead of directly doing discrete action decision making) could provide a lot of value.\n\n- The experiment settings used to demonstrate generalization are not too convincing. The number of lanes and traffic density is changed, but this is still an extremely similar environment where the retrieved few-shot scenarios could be nearly directly applicable.\n\n- Under the above setting, it is possible that with a large enough memory module the task reduces to simply copying the answer from one of the retrieved experiences. It would be good to see a baseline where the decision from one of the retrieved experiences is used as is (voting with mixture of experts or winner takes all)\n\n- The metric movement with CitySim in Figure 7b and Table 1 correction row do not seem significant to make the corresponding claims?\n\n- Nit: The key frame sampling for successful experiences seems like an important detail that has not been explained.\n\n- Minor nit: The claim for this being the first work addressing AV planning via leveraging LLMs might need to be revised with recent papers like GPT driver (depending on chronology).\n\nQuestions: - What kind of diverse interactions do we get from the Highway-env simulator? Would it be possible to evaluate the framework under more interactive / challenging conditions, especially wrt agent interactions? It would be interesting to see the generalization to intersections, interactions with peds, aggressive agents etc.\n\n- The correction experiences intuitively should provide a strong boost to performance since they are akin to hard example mining and injecting reasoning about the negative outcomes. However the corresponding results in Table 1 do not show strong improvements. Is it possible understudied and warrants more extensive experimentations?", "labeling_timestamp": "2026-01-11T17:24:17.306777", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The reviewer refers to specific results in Figure 7b and Table 1, but the provided paper excerpt does not contain Figure 7b, Table 1, or detailed numerical results for CitySim; therefore it is impossible to verify whether the reported metric changes are too small to substantiate the claimed improvements.", "evidence": "“Extensive experiments demonstrate that the proposed framework DiLu can leverage LLM to make proper decisions for the autonomous driving system. We design a closed-loop driving environment and prove that DiLu can perform better and better with the experience accumulated in the memory module. Remarkably, with only 40 memory items, DiLu achieves comparable performance to the reinforcement learning (RL) based methods that have extensively trained over 600,000 episodes, but with a much stronger generalization ability to diverse scenarios.”", "section": "Introduction"}
{"claim": "The key frame sampling procedure for successful experiences is an important implementation detail that the paper does not explain.", "claim_type": "methodology", "paper_id": "OqTMUPuLuC", "paper_title": "DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "xN8k5wi0lW", "reviewer": "Reviewer_48LR", "review_text": "Summary: The paper presents a framework for utilizing the few shot and self-correction capabilities of LLMs for the task of AV planning, where the following abilities of the framework are highlighted:\n- Store successful experiences in memory and leverage them to improve future rollouts through similarity retrieval and usage in few-shot prompting.\n- Ability to learn from unsuccessful experiences (ones with collisions) by applying LLM self-correction and storing the modified experience among the successful experiences in the memory\n\nThe above components, dubbed as reasoning and reflection modules respectively, are integrated along with memory in a closed loop setting without any back-propagation objective. \n\nA number of prompting techniques including chain-of-thought and few-shot prompting are used to get better reasoning. The environment used for experiments (Highway-Env) only requires four discrete decisions, hence the LLM is prompted to select one amongst these four decisions for each frame after going through CoT reasoning.\n\nThe experiments are used to demonstrate the following key claims:\n- The memory module combined with few-shot prompting provides much better results than using no memory module (zero-shot) or using lesser shots. \n- The more the number of experiences in the memory, the better.\n- The ability to generalize is better with more few-shot experiences fed into the LLM\n- Adding successful and corrected experiences both help in improving performance\n- Better generalization capability compared to RL method GRAD.\n\nStrengths: - The motivating idea of human knowledge distillation for AV planning is sound, interesting, and under-explored.\n\n- The overall framework formulation towards leveraging LLMs via appropriate prompting, retrieval, and self-correction is interesting and well set up. It would have been exciting to see formulations for LLMs assisting planning stacks (instead of directly doing discrete action decision making) - which could be much more valuable to existing systems.  \n\n- The flywheel effect created from storing both successful and unsuccessful + corrected experiences in memory is an important contribution.\n\n- The paper provides a good foundation for other exciting work to build upon, especially with the promise to open source upon acceptance.\n\n- The experiments are fairly extensive towards investigating all the different components of the proposed framework.\n\nWeaknesses: -  One of the main proposed advantages is better generalization through instilling human knowledge-driven capabilities instead of a data-driven only approach. However, the experimental settings derived from HighwayEnv are too restrictive to help extrapolate how such LLM based reasoning would perform on diverse new scenes using retrieval + few shot prompting. While it is perfectly fine to work with restricted settings and smaller datasets for new research work, the bridge to answer the most interesting questions is too long.\n\n- As mentioned in strengths section, providing directions and initial experiments on assisting planning stacks (instead of directly doing discrete action decision making) could provide a lot of value.\n\n- The experiment settings used to demonstrate generalization are not too convincing. The number of lanes and traffic density is changed, but this is still an extremely similar environment where the retrieved few-shot scenarios could be nearly directly applicable.\n\n- Under the above setting, it is possible that with a large enough memory module the task reduces to simply copying the answer from one of the retrieved experiences. It would be good to see a baseline where the decision from one of the retrieved experiences is used as is (voting with mixture of experts or winner takes all)\n\n- The metric movement with CitySim in Figure 7b and Table 1 correction row do not seem significant to make the corresponding claims?\n\n- Nit: The key frame sampling for successful experiences seems like an important detail that has not been explained.\n\n- Minor nit: The claim for this being the first work addressing AV planning via leveraging LLMs might need to be revised with recent papers like GPT driver (depending on chronology).\n\nQuestions: - What kind of diverse interactions do we get from the Highway-env simulator? Would it be possible to evaluate the framework under more interactive / challenging conditions, especially wrt agent interactions? It would be interesting to see the generalization to intersections, interactions with peds, aggressive agents etc.\n\n- The correction experiences intuitively should provide a strong boost to performance since they are akin to hard example mining and injecting reasoning about the negative outcomes. However the corresponding results in Table 1 do not show strong improvements. Is it possible understudied and warrants more extensive experimentations?", "labeling_timestamp": "2026-01-11T17:24:19.846789", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper's Memory module (Section 3.2) describes initializing a small set of scenarios, embedding scene descriptions, clustering and retrieving nearest memories, and storing scene-description→reasoning pairs, but it does not describe any procedure for sampling key frames from successful experiences or how such key-frame selection is performed.", "evidence": "\"Initialization : The Initialization of memory module is akin to a human attending driving school before hitting the road. We select a few scenarios and manually outline the correct reasoning and decision-making processes for these situations to form the initial memory.\" \"Memory recall : At each decision frame, the agent receives a textual description of the driving scenario. Before making a decision, the current driving scenario is embedded into a vector, which serves as the memory key. This key is then clustered and searched to find the closest scenarios (Johnson et al., 2019) in the memory module and their corresponding reasoning processes, or memories.\" ", "section": "3.2 Memory module / Memory recall"}
{"claim": "The claim of being the first work addressing AV planning via LLMs may be inaccurate given recent related papers such as GPT Driver and should be revised.", "claim_type": "novelty", "paper_id": "OqTMUPuLuC", "paper_title": "DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "xN8k5wi0lW", "reviewer": "Reviewer_48LR", "review_text": "Summary: The paper presents a framework for utilizing the few shot and self-correction capabilities of LLMs for the task of AV planning, where the following abilities of the framework are highlighted:\n- Store successful experiences in memory and leverage them to improve future rollouts through similarity retrieval and usage in few-shot prompting.\n- Ability to learn from unsuccessful experiences (ones with collisions) by applying LLM self-correction and storing the modified experience among the successful experiences in the memory\n\nThe above components, dubbed as reasoning and reflection modules respectively, are integrated along with memory in a closed loop setting without any back-propagation objective. \n\nA number of prompting techniques including chain-of-thought and few-shot prompting are used to get better reasoning. The environment used for experiments (Highway-Env) only requires four discrete decisions, hence the LLM is prompted to select one amongst these four decisions for each frame after going through CoT reasoning.\n\nThe experiments are used to demonstrate the following key claims:\n- The memory module combined with few-shot prompting provides much better results than using no memory module (zero-shot) or using lesser shots. \n- The more the number of experiences in the memory, the better.\n- The ability to generalize is better with more few-shot experiences fed into the LLM\n- Adding successful and corrected experiences both help in improving performance\n- Better generalization capability compared to RL method GRAD.\n\nStrengths: - The motivating idea of human knowledge distillation for AV planning is sound, interesting, and under-explored.\n\n- The overall framework formulation towards leveraging LLMs via appropriate prompting, retrieval, and self-correction is interesting and well set up. It would have been exciting to see formulations for LLMs assisting planning stacks (instead of directly doing discrete action decision making) - which could be much more valuable to existing systems.  \n\n- The flywheel effect created from storing both successful and unsuccessful + corrected experiences in memory is an important contribution.\n\n- The paper provides a good foundation for other exciting work to build upon, especially with the promise to open source upon acceptance.\n\n- The experiments are fairly extensive towards investigating all the different components of the proposed framework.\n\nWeaknesses: -  One of the main proposed advantages is better generalization through instilling human knowledge-driven capabilities instead of a data-driven only approach. However, the experimental settings derived from HighwayEnv are too restrictive to help extrapolate how such LLM based reasoning would perform on diverse new scenes using retrieval + few shot prompting. While it is perfectly fine to work with restricted settings and smaller datasets for new research work, the bridge to answer the most interesting questions is too long.\n\n- As mentioned in strengths section, providing directions and initial experiments on assisting planning stacks (instead of directly doing discrete action decision making) could provide a lot of value.\n\n- The experiment settings used to demonstrate generalization are not too convincing. The number of lanes and traffic density is changed, but this is still an extremely similar environment where the retrieved few-shot scenarios could be nearly directly applicable.\n\n- Under the above setting, it is possible that with a large enough memory module the task reduces to simply copying the answer from one of the retrieved experiences. It would be good to see a baseline where the decision from one of the retrieved experiences is used as is (voting with mixture of experts or winner takes all)\n\n- The metric movement with CitySim in Figure 7b and Table 1 correction row do not seem significant to make the corresponding claims?\n\n- Nit: The key frame sampling for successful experiences seems like an important detail that has not been explained.\n\n- Minor nit: The claim for this being the first work addressing AV planning via leveraging LLMs might need to be revised with recent papers like GPT driver (depending on chronology).\n\nQuestions: - What kind of diverse interactions do we get from the Highway-env simulator? Would it be possible to evaluate the framework under more interactive / challenging conditions, especially wrt agent interactions? It would be interesting to see the generalization to intersections, interactions with peds, aggressive agents etc.\n\n- The correction experiences intuitively should provide a strong boost to performance since they are akin to hard example mining and injecting reasoning about the negative outcomes. However the corresponding results in Table 1 do not show strong improvements. Is it possible understudied and warrants more extensive experimentations?", "labeling_timestamp": "2026-01-11T17:24:26.842942", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper explicitly claims to be the first to \"leverage knowledge-driven capability in decision-making for autonomous vehicles,\" but it does not discuss or cite recent works such as GPT Driver. From the paper content alone we cannot determine whether the reviewer's assertion of inaccuracy (given external papers) is correct.", "evidence": "To the best of our knowledge, we are the first to leverage knowledge-driven capability in decision-making for autonomous vehicles.", "section": "Abstract / Contributions (Introduction)"}
{"claim": "Highway-Env may not produce sufficiently diverse interactive agent interactions, limiting evaluation of the framework's robustness to varied agent behaviors.", "claim_type": "experimental", "paper_id": "OqTMUPuLuC", "paper_title": "DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "xN8k5wi0lW", "reviewer": "Reviewer_48LR", "review_text": "Summary: The paper presents a framework for utilizing the few shot and self-correction capabilities of LLMs for the task of AV planning, where the following abilities of the framework are highlighted:\n- Store successful experiences in memory and leverage them to improve future rollouts through similarity retrieval and usage in few-shot prompting.\n- Ability to learn from unsuccessful experiences (ones with collisions) by applying LLM self-correction and storing the modified experience among the successful experiences in the memory\n\nThe above components, dubbed as reasoning and reflection modules respectively, are integrated along with memory in a closed loop setting without any back-propagation objective. \n\nA number of prompting techniques including chain-of-thought and few-shot prompting are used to get better reasoning. The environment used for experiments (Highway-Env) only requires four discrete decisions, hence the LLM is prompted to select one amongst these four decisions for each frame after going through CoT reasoning.\n\nThe experiments are used to demonstrate the following key claims:\n- The memory module combined with few-shot prompting provides much better results than using no memory module (zero-shot) or using lesser shots. \n- The more the number of experiences in the memory, the better.\n- The ability to generalize is better with more few-shot experiences fed into the LLM\n- Adding successful and corrected experiences both help in improving performance\n- Better generalization capability compared to RL method GRAD.\n\nStrengths: - The motivating idea of human knowledge distillation for AV planning is sound, interesting, and under-explored.\n\n- The overall framework formulation towards leveraging LLMs via appropriate prompting, retrieval, and self-correction is interesting and well set up. It would have been exciting to see formulations for LLMs assisting planning stacks (instead of directly doing discrete action decision making) - which could be much more valuable to existing systems.  \n\n- The flywheel effect created from storing both successful and unsuccessful + corrected experiences in memory is an important contribution.\n\n- The paper provides a good foundation for other exciting work to build upon, especially with the promise to open source upon acceptance.\n\n- The experiments are fairly extensive towards investigating all the different components of the proposed framework.\n\nWeaknesses: -  One of the main proposed advantages is better generalization through instilling human knowledge-driven capabilities instead of a data-driven only approach. However, the experimental settings derived from HighwayEnv are too restrictive to help extrapolate how such LLM based reasoning would perform on diverse new scenes using retrieval + few shot prompting. While it is perfectly fine to work with restricted settings and smaller datasets for new research work, the bridge to answer the most interesting questions is too long.\n\n- As mentioned in strengths section, providing directions and initial experiments on assisting planning stacks (instead of directly doing discrete action decision making) could provide a lot of value.\n\n- The experiment settings used to demonstrate generalization are not too convincing. The number of lanes and traffic density is changed, but this is still an extremely similar environment where the retrieved few-shot scenarios could be nearly directly applicable.\n\n- Under the above setting, it is possible that with a large enough memory module the task reduces to simply copying the answer from one of the retrieved experiences. It would be good to see a baseline where the decision from one of the retrieved experiences is used as is (voting with mixture of experts or winner takes all)\n\n- The metric movement with CitySim in Figure 7b and Table 1 correction row do not seem significant to make the corresponding claims?\n\n- Nit: The key frame sampling for successful experiences seems like an important detail that has not been explained.\n\n- Minor nit: The claim for this being the first work addressing AV planning via leveraging LLMs might need to be revised with recent papers like GPT driver (depending on chronology).\n\nQuestions: - What kind of diverse interactions do we get from the Highway-env simulator? Would it be possible to evaluate the framework under more interactive / challenging conditions, especially wrt agent interactions? It would be interesting to see the generalization to intersections, interactions with peds, aggressive agents etc.\n\n- The correction experiences intuitively should provide a strong boost to performance since they are akin to hard example mining and injecting reasoning about the negative outcomes. However the corresponding results in Table 1 do not show strong improvements. Is it possible understudied and warrants more extensive experimentations?", "labeling_timestamp": "2026-01-11T17:24:33.654859", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does not mention Highway-Env or discuss whether the environment used produces diverse interactive agent interactions. It only states that the authors designed a \"closed-loop driving environment\" and describes the DiLu framework (Environment, Reasoning, Reflection, Memory) without referring to Highway-Env or analyzing diversity of other agents' behaviors.", "evidence": "\"We design a closed-loop driving environment and prove that DiLu can perform better and better with the experience accumulated in the memory module.\"; \"DiLu consists of four core modules: Environment, Reasoning, Reflection, and Memory.\"", "section": "Introduction; Methodology (Sections 1 and 3)"}
{"claim": "The framework is not evaluated under more interactive or challenging conditions like intersections, pedestrian interactions, or aggressive agent behaviors.", "claim_type": "experimental", "paper_id": "OqTMUPuLuC", "paper_title": "DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "xN8k5wi0lW", "reviewer": "Reviewer_48LR", "review_text": "Summary: The paper presents a framework for utilizing the few shot and self-correction capabilities of LLMs for the task of AV planning, where the following abilities of the framework are highlighted:\n- Store successful experiences in memory and leverage them to improve future rollouts through similarity retrieval and usage in few-shot prompting.\n- Ability to learn from unsuccessful experiences (ones with collisions) by applying LLM self-correction and storing the modified experience among the successful experiences in the memory\n\nThe above components, dubbed as reasoning and reflection modules respectively, are integrated along with memory in a closed loop setting without any back-propagation objective. \n\nA number of prompting techniques including chain-of-thought and few-shot prompting are used to get better reasoning. The environment used for experiments (Highway-Env) only requires four discrete decisions, hence the LLM is prompted to select one amongst these four decisions for each frame after going through CoT reasoning.\n\nThe experiments are used to demonstrate the following key claims:\n- The memory module combined with few-shot prompting provides much better results than using no memory module (zero-shot) or using lesser shots. \n- The more the number of experiences in the memory, the better.\n- The ability to generalize is better with more few-shot experiences fed into the LLM\n- Adding successful and corrected experiences both help in improving performance\n- Better generalization capability compared to RL method GRAD.\n\nStrengths: - The motivating idea of human knowledge distillation for AV planning is sound, interesting, and under-explored.\n\n- The overall framework formulation towards leveraging LLMs via appropriate prompting, retrieval, and self-correction is interesting and well set up. It would have been exciting to see formulations for LLMs assisting planning stacks (instead of directly doing discrete action decision making) - which could be much more valuable to existing systems.  \n\n- The flywheel effect created from storing both successful and unsuccessful + corrected experiences in memory is an important contribution.\n\n- The paper provides a good foundation for other exciting work to build upon, especially with the promise to open source upon acceptance.\n\n- The experiments are fairly extensive towards investigating all the different components of the proposed framework.\n\nWeaknesses: -  One of the main proposed advantages is better generalization through instilling human knowledge-driven capabilities instead of a data-driven only approach. However, the experimental settings derived from HighwayEnv are too restrictive to help extrapolate how such LLM based reasoning would perform on diverse new scenes using retrieval + few shot prompting. While it is perfectly fine to work with restricted settings and smaller datasets for new research work, the bridge to answer the most interesting questions is too long.\n\n- As mentioned in strengths section, providing directions and initial experiments on assisting planning stacks (instead of directly doing discrete action decision making) could provide a lot of value.\n\n- The experiment settings used to demonstrate generalization are not too convincing. The number of lanes and traffic density is changed, but this is still an extremely similar environment where the retrieved few-shot scenarios could be nearly directly applicable.\n\n- Under the above setting, it is possible that with a large enough memory module the task reduces to simply copying the answer from one of the retrieved experiences. It would be good to see a baseline where the decision from one of the retrieved experiences is used as is (voting with mixture of experts or winner takes all)\n\n- The metric movement with CitySim in Figure 7b and Table 1 correction row do not seem significant to make the corresponding claims?\n\n- Nit: The key frame sampling for successful experiences seems like an important detail that has not been explained.\n\n- Minor nit: The claim for this being the first work addressing AV planning via leveraging LLMs might need to be revised with recent papers like GPT driver (depending on chronology).\n\nQuestions: - What kind of diverse interactions do we get from the Highway-env simulator? Would it be possible to evaluate the framework under more interactive / challenging conditions, especially wrt agent interactions? It would be interesting to see the generalization to intersections, interactions with peds, aggressive agents etc.\n\n- The correction experiences intuitively should provide a strong boost to performance since they are akin to hard example mining and injecting reasoning about the negative outcomes. However the corresponding results in Table 1 do not show strong improvements. Is it possible understudied and warrants more extensive experimentations?", "labeling_timestamp": "2026-01-11T17:24:37.044102", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper content describes a closed-loop driving environment and claims generalization to diverse scenarios (Introduction, Abstract), but contains no experimental details or lists of evaluated scenario types (e.g., intersections, pedestrians, aggressive agents). Therefore we cannot confirm from the supplied text whether those specific challenging conditions were evaluated.", "evidence": "We design a closed-loop driving environment and prove that DiLu can perform better and better with the experience accumulated in the memory module.", "section": "1 Introduction"}
{"claim": "Correction experiences, which should boost performance, do not show strong improvements in Table 1 according to the reviewer.", "claim_type": "experimental", "paper_id": "OqTMUPuLuC", "paper_title": "DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "xN8k5wi0lW", "reviewer": "Reviewer_48LR", "review_text": "Summary: The paper presents a framework for utilizing the few shot and self-correction capabilities of LLMs for the task of AV planning, where the following abilities of the framework are highlighted:\n- Store successful experiences in memory and leverage them to improve future rollouts through similarity retrieval and usage in few-shot prompting.\n- Ability to learn from unsuccessful experiences (ones with collisions) by applying LLM self-correction and storing the modified experience among the successful experiences in the memory\n\nThe above components, dubbed as reasoning and reflection modules respectively, are integrated along with memory in a closed loop setting without any back-propagation objective. \n\nA number of prompting techniques including chain-of-thought and few-shot prompting are used to get better reasoning. The environment used for experiments (Highway-Env) only requires four discrete decisions, hence the LLM is prompted to select one amongst these four decisions for each frame after going through CoT reasoning.\n\nThe experiments are used to demonstrate the following key claims:\n- The memory module combined with few-shot prompting provides much better results than using no memory module (zero-shot) or using lesser shots. \n- The more the number of experiences in the memory, the better.\n- The ability to generalize is better with more few-shot experiences fed into the LLM\n- Adding successful and corrected experiences both help in improving performance\n- Better generalization capability compared to RL method GRAD.\n\nStrengths: - The motivating idea of human knowledge distillation for AV planning is sound, interesting, and under-explored.\n\n- The overall framework formulation towards leveraging LLMs via appropriate prompting, retrieval, and self-correction is interesting and well set up. It would have been exciting to see formulations for LLMs assisting planning stacks (instead of directly doing discrete action decision making) - which could be much more valuable to existing systems.  \n\n- The flywheel effect created from storing both successful and unsuccessful + corrected experiences in memory is an important contribution.\n\n- The paper provides a good foundation for other exciting work to build upon, especially with the promise to open source upon acceptance.\n\n- The experiments are fairly extensive towards investigating all the different components of the proposed framework.\n\nWeaknesses: -  One of the main proposed advantages is better generalization through instilling human knowledge-driven capabilities instead of a data-driven only approach. However, the experimental settings derived from HighwayEnv are too restrictive to help extrapolate how such LLM based reasoning would perform on diverse new scenes using retrieval + few shot prompting. While it is perfectly fine to work with restricted settings and smaller datasets for new research work, the bridge to answer the most interesting questions is too long.\n\n- As mentioned in strengths section, providing directions and initial experiments on assisting planning stacks (instead of directly doing discrete action decision making) could provide a lot of value.\n\n- The experiment settings used to demonstrate generalization are not too convincing. The number of lanes and traffic density is changed, but this is still an extremely similar environment where the retrieved few-shot scenarios could be nearly directly applicable.\n\n- Under the above setting, it is possible that with a large enough memory module the task reduces to simply copying the answer from one of the retrieved experiences. It would be good to see a baseline where the decision from one of the retrieved experiences is used as is (voting with mixture of experts or winner takes all)\n\n- The metric movement with CitySim in Figure 7b and Table 1 correction row do not seem significant to make the corresponding claims?\n\n- Nit: The key frame sampling for successful experiences seems like an important detail that has not been explained.\n\n- Minor nit: The claim for this being the first work addressing AV planning via leveraging LLMs might need to be revised with recent papers like GPT driver (depending on chronology).\n\nQuestions: - What kind of diverse interactions do we get from the Highway-env simulator? Would it be possible to evaluate the framework under more interactive / challenging conditions, especially wrt agent interactions? It would be interesting to see the generalization to intersections, interactions with peds, aggressive agents etc.\n\n- The correction experiences intuitively should provide a strong boost to performance since they are akin to hard example mining and injecting reasoning about the negative outcomes. However the corresponding results in Table 1 do not show strong improvements. Is it possible understudied and warrants more extensive experimentations?", "labeling_timestamp": "2026-01-11T17:24:34.308823", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not include Table 1 or its numeric results, so the reviewer's specific claim about Table 1 cannot be verified from the given content. The paper does state that memory-based experiences improve performance (e.g., 40 memory items yield strong results), but without Table 1 we cannot confirm whether correction experiences show weak improvements in that table.", "evidence": "\"Remarkably, with only 40 memory items, DiLu achieves comparable performance to the reinforcement learning (RL) based methods that have extensively trained over 600,000 episodes, but with a much stronger generalization ability to diverse scenarios.\"", "section": "1 Introduction"}
{"claim": "The paper does not include a baseline implementing mixture-of-experts voting or winner-takes-all retrieval to compare against the proposed memory-based few-shot approach.", "claim_type": "baseline", "paper_id": "OqTMUPuLuC", "paper_title": "DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "xN8k5wi0lW", "reviewer": "Reviewer_48LR", "review_text": "Summary: The paper presents a framework for utilizing the few shot and self-correction capabilities of LLMs for the task of AV planning, where the following abilities of the framework are highlighted:\n- Store successful experiences in memory and leverage them to improve future rollouts through similarity retrieval and usage in few-shot prompting.\n- Ability to learn from unsuccessful experiences (ones with collisions) by applying LLM self-correction and storing the modified experience among the successful experiences in the memory\n\nThe above components, dubbed as reasoning and reflection modules respectively, are integrated along with memory in a closed loop setting without any back-propagation objective. \n\nA number of prompting techniques including chain-of-thought and few-shot prompting are used to get better reasoning. The environment used for experiments (Highway-Env) only requires four discrete decisions, hence the LLM is prompted to select one amongst these four decisions for each frame after going through CoT reasoning.\n\nThe experiments are used to demonstrate the following key claims:\n- The memory module combined with few-shot prompting provides much better results than using no memory module (zero-shot) or using lesser shots. \n- The more the number of experiences in the memory, the better.\n- The ability to generalize is better with more few-shot experiences fed into the LLM\n- Adding successful and corrected experiences both help in improving performance\n- Better generalization capability compared to RL method GRAD.\n\nStrengths: - The motivating idea of human knowledge distillation for AV planning is sound, interesting, and under-explored.\n\n- The overall framework formulation towards leveraging LLMs via appropriate prompting, retrieval, and self-correction is interesting and well set up. It would have been exciting to see formulations for LLMs assisting planning stacks (instead of directly doing discrete action decision making) - which could be much more valuable to existing systems.  \n\n- The flywheel effect created from storing both successful and unsuccessful + corrected experiences in memory is an important contribution.\n\n- The paper provides a good foundation for other exciting work to build upon, especially with the promise to open source upon acceptance.\n\n- The experiments are fairly extensive towards investigating all the different components of the proposed framework.\n\nWeaknesses: -  One of the main proposed advantages is better generalization through instilling human knowledge-driven capabilities instead of a data-driven only approach. However, the experimental settings derived from HighwayEnv are too restrictive to help extrapolate how such LLM based reasoning would perform on diverse new scenes using retrieval + few shot prompting. While it is perfectly fine to work with restricted settings and smaller datasets for new research work, the bridge to answer the most interesting questions is too long.\n\n- As mentioned in strengths section, providing directions and initial experiments on assisting planning stacks (instead of directly doing discrete action decision making) could provide a lot of value.\n\n- The experiment settings used to demonstrate generalization are not too convincing. The number of lanes and traffic density is changed, but this is still an extremely similar environment where the retrieved few-shot scenarios could be nearly directly applicable.\n\n- Under the above setting, it is possible that with a large enough memory module the task reduces to simply copying the answer from one of the retrieved experiences. It would be good to see a baseline where the decision from one of the retrieved experiences is used as is (voting with mixture of experts or winner takes all)\n\n- The metric movement with CitySim in Figure 7b and Table 1 correction row do not seem significant to make the corresponding claims?\n\n- Nit: The key frame sampling for successful experiences seems like an important detail that has not been explained.\n\n- Minor nit: The claim for this being the first work addressing AV planning via leveraging LLMs might need to be revised with recent papers like GPT driver (depending on chronology).\n\nQuestions: - What kind of diverse interactions do we get from the Highway-env simulator? Would it be possible to evaluate the framework under more interactive / challenging conditions, especially wrt agent interactions? It would be interesting to see the generalization to intersections, interactions with peds, aggressive agents etc.\n\n- The correction experiences intuitively should provide a strong boost to performance since they are akin to hard example mining and injecting reasoning about the negative outcomes. However the corresponding results in Table 1 do not show strong improvements. Is it possible understudied and warrants more extensive experimentations?", "labeling_timestamp": "2026-01-11T17:24:47.833969", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes comparisons to reinforcement-learning baselines and details its memory recall (nearest/clustered retrieval) mechanism, but it does not mention or implement a baseline using mixture-of-experts voting or a winner-takes-all retrieval strategy for comparison.", "evidence": "\"Remarkably, with only 40 memory items, DiLu achieves comparable performance to the reinforcement learning (RL) based methods that have extensively trained over 600,000 episodes, but with a much stronger generalization ability to diverse scenarios.\"; \"Memory recall : At each decision frame, the agent receives a textual description of the driving scenario. Before making a decision, the current driving scenario is embedded into a vector, which serves as the memory key. This key is then clustered and searched to find the closest scenarios (Johnson et al., 2019) in the memory module and their corresponding reasoning processes, or memories.\"", "section": "Introduction; 3.2 Memory module"}
{"claim": "The impact of correction experiences appears understudied and warrants more extensive experimentation to validate their claimed benefits.", "claim_type": "novelty", "paper_id": "OqTMUPuLuC", "paper_title": "DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "xN8k5wi0lW", "reviewer": "Reviewer_48LR", "review_text": "Summary: The paper presents a framework for utilizing the few shot and self-correction capabilities of LLMs for the task of AV planning, where the following abilities of the framework are highlighted:\n- Store successful experiences in memory and leverage them to improve future rollouts through similarity retrieval and usage in few-shot prompting.\n- Ability to learn from unsuccessful experiences (ones with collisions) by applying LLM self-correction and storing the modified experience among the successful experiences in the memory\n\nThe above components, dubbed as reasoning and reflection modules respectively, are integrated along with memory in a closed loop setting without any back-propagation objective. \n\nA number of prompting techniques including chain-of-thought and few-shot prompting are used to get better reasoning. The environment used for experiments (Highway-Env) only requires four discrete decisions, hence the LLM is prompted to select one amongst these four decisions for each frame after going through CoT reasoning.\n\nThe experiments are used to demonstrate the following key claims:\n- The memory module combined with few-shot prompting provides much better results than using no memory module (zero-shot) or using lesser shots. \n- The more the number of experiences in the memory, the better.\n- The ability to generalize is better with more few-shot experiences fed into the LLM\n- Adding successful and corrected experiences both help in improving performance\n- Better generalization capability compared to RL method GRAD.\n\nStrengths: - The motivating idea of human knowledge distillation for AV planning is sound, interesting, and under-explored.\n\n- The overall framework formulation towards leveraging LLMs via appropriate prompting, retrieval, and self-correction is interesting and well set up. It would have been exciting to see formulations for LLMs assisting planning stacks (instead of directly doing discrete action decision making) - which could be much more valuable to existing systems.  \n\n- The flywheel effect created from storing both successful and unsuccessful + corrected experiences in memory is an important contribution.\n\n- The paper provides a good foundation for other exciting work to build upon, especially with the promise to open source upon acceptance.\n\n- The experiments are fairly extensive towards investigating all the different components of the proposed framework.\n\nWeaknesses: -  One of the main proposed advantages is better generalization through instilling human knowledge-driven capabilities instead of a data-driven only approach. However, the experimental settings derived from HighwayEnv are too restrictive to help extrapolate how such LLM based reasoning would perform on diverse new scenes using retrieval + few shot prompting. While it is perfectly fine to work with restricted settings and smaller datasets for new research work, the bridge to answer the most interesting questions is too long.\n\n- As mentioned in strengths section, providing directions and initial experiments on assisting planning stacks (instead of directly doing discrete action decision making) could provide a lot of value.\n\n- The experiment settings used to demonstrate generalization are not too convincing. The number of lanes and traffic density is changed, but this is still an extremely similar environment where the retrieved few-shot scenarios could be nearly directly applicable.\n\n- Under the above setting, it is possible that with a large enough memory module the task reduces to simply copying the answer from one of the retrieved experiences. It would be good to see a baseline where the decision from one of the retrieved experiences is used as is (voting with mixture of experts or winner takes all)\n\n- The metric movement with CitySim in Figure 7b and Table 1 correction row do not seem significant to make the corresponding claims?\n\n- Nit: The key frame sampling for successful experiences seems like an important detail that has not been explained.\n\n- Minor nit: The claim for this being the first work addressing AV planning via leveraging LLMs might need to be revised with recent papers like GPT driver (depending on chronology).\n\nQuestions: - What kind of diverse interactions do we get from the Highway-env simulator? Would it be possible to evaluate the framework under more interactive / challenging conditions, especially wrt agent interactions? It would be interesting to see the generalization to intersections, interactions with peds, aggressive agents etc.\n\n- The correction experiences intuitively should provide a strong boost to performance since they are akin to hard example mining and injecting reasoning about the negative outcomes. However the corresponding results in Table 1 do not show strong improvements. Is it possible understudied and warrants more extensive experimentations?", "labeling_timestamp": "2026-01-11T17:25:01.027397", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes a Reflection module that produces corrected (revised) experiences and a Memory module that accumulates experiences, but the provided content does not present focused experiments or ablation studies that isolate or quantify the specific impact of those correction-derived experiences. The experiments reported claim overall benefits of memory accumulation and generalization, yet do not specifically validate the contribution of correction experiences.", "evidence": "\"Subsequently, we employ the Reflection module to assess past decision sequences, categorizing them as either safe or unsafe. The unsafe decisions are revised and these refined decisions are finally updated back into the Memory module.\" \n\n\"Extensive experiments prove DiLu's capability to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods.\"", "section": "3.1 Overview; Abstract"}
{"claim": "The authors do not impose Donsker conditions, raising concern about attaining root-n convergence of the proposed semiparametric estimator.", "claim_type": "methodology", "paper_id": "MqEQbvPvkE", "paper_title": "Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "kfNah4HeNr", "reviewer": "Reviewer_xEjj", "review_text": "Summary: This study provides a strong framework for treatment effect estimation based on the semiparametric theory. The authors develop a novel optimization problem for treatment effect estimation and show the asymptotic properties.\n\nStrengths: My major curiosity lies in the proof of Theorem 2. As discussed in the literature of double machine learning (cf. Chernozhukov et al. (2018)), to attain $\\sqrt{n}$-convergence of semiparametric estimators, we usually impose the Donsker condition for (nonparametric) nuisance estimators. However, it seems that the authors do not impose such assumptions. I am checking the proof, but could the authors provide intuitive reasons for the results? If this result is true, I believe that this is the theoretical strength of this study.\n\nThe above is also my concern because the posited assumptions are too weak to show the results. We usually impose some properties such as smoothness on the nuisance estimators to discuss convergence rates. Even if we obtain desirable convergence rates, neural network models usually do not satisfy the Donsker conditions. Therefore, I am afraid of missing assumptions or errorrness in the proof (I need to confirm the proof but have not yet done it...).\n\nWeaknesses: See above.\n\nQuestions: - Is Assumption 2.2 sufficient? If $p(a|x) \\propto 1/n$ for some $a$, then the results do not hold, I think $p(a|x)$ should be lower bounded by a positive constant independent of $n$.\n- In Theorem 2, what does $\\to$ indicate? Convergence of non-random variables or convergence in probability?\n- In Theorem 2, what does $O(r_1(n))$ in $\\|\\hat{\\mu} - \\mu\\|_\\infty$ mean? Should it be $O_P$?\n- Sugiyama et al. (2012) discussed the density-ratio estimation, and its interest differs from this study. Furthermore, it has been known that Eq. (8) can be used to estimate the propensity score. I think the citation may not be appropriate.\n- Does this study relate to automatic debiased learning proposed by Chernozhukov et al. (2022)?\n- \"for some function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\" should be for \"for some measurable function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\"?\n- Does the shift-response function is the same as the standard average treatment effect?\n- Some citations are missing.", "labeling_timestamp": "2026-01-11T17:25:02.358227", "model": "gpt-5-mini", "label": "Supported", "justification": "Nowhere in the provided paper text do the authors state or impose Donsker (empirical process) conditions; they assert efficiency under \"standard regularity conditions\" and claim an estimator attains an 'efficient' rate, but do not list the empirical-process-type assumptions (e.g., Donsker) that are commonly required to guarantee root-n asymptotic linearity. Thus the reviewer's factual claim that Donsker conditions are not imposed is supported, and the concern about root-n convergence is reasonable given the lack of such stated conditions.", "evidence": "1) \"Given standard regularity conditions, these guarantees imply that the SRF is consistently estimated when either the outcome model or the density-ratio model for the exposure shift is correctly specified, and achieves the best possible efficiency rate when both models are correctly specified.\" 2) \"As we have suggested earlier, an estimator of ψ can be derived from estimators of the outcome and density ratio functions. Using TR, we will obtain an estimator ˆψ_tr with the guarantees that ∥ˆψ_tr - ψ∥_2 converges in probability at an 'efficient' rate according to the prevailing semiparametric efficiency theory surrounding robust causal effect estimation (Kennedy, 2022).\"", "section": "Contributions (Introduction) and Section 3 (TRESNET: TARGETED REGULARIZATION...)"}
{"claim": "The paper's posited assumptions appear too weak to establish the stated results; smoothness or other properties on nuisance estimators are typically required for convergence rates.", "claim_type": "methodology", "paper_id": "MqEQbvPvkE", "paper_title": "Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "kfNah4HeNr", "reviewer": "Reviewer_xEjj", "review_text": "Summary: This study provides a strong framework for treatment effect estimation based on the semiparametric theory. The authors develop a novel optimization problem for treatment effect estimation and show the asymptotic properties.\n\nStrengths: My major curiosity lies in the proof of Theorem 2. As discussed in the literature of double machine learning (cf. Chernozhukov et al. (2018)), to attain $\\sqrt{n}$-convergence of semiparametric estimators, we usually impose the Donsker condition for (nonparametric) nuisance estimators. However, it seems that the authors do not impose such assumptions. I am checking the proof, but could the authors provide intuitive reasons for the results? If this result is true, I believe that this is the theoretical strength of this study.\n\nThe above is also my concern because the posited assumptions are too weak to show the results. We usually impose some properties such as smoothness on the nuisance estimators to discuss convergence rates. Even if we obtain desirable convergence rates, neural network models usually do not satisfy the Donsker conditions. Therefore, I am afraid of missing assumptions or errorrness in the proof (I need to confirm the proof but have not yet done it...).\n\nWeaknesses: See above.\n\nQuestions: - Is Assumption 2.2 sufficient? If $p(a|x) \\propto 1/n$ for some $a$, then the results do not hold, I think $p(a|x)$ should be lower bounded by a positive constant independent of $n$.\n- In Theorem 2, what does $\\to$ indicate? Convergence of non-random variables or convergence in probability?\n- In Theorem 2, what does $O(r_1(n))$ in $\\|\\hat{\\mu} - \\mu\\|_\\infty$ mean? Should it be $O_P$?\n- Sugiyama et al. (2012) discussed the density-ratio estimation, and its interest differs from this study. Furthermore, it has been known that Eq. (8) can be used to estimate the propensity score. I think the citation may not be appropriate.\n- Does this study relate to automatic debiased learning proposed by Chernozhukov et al. (2022)?\n- \"for some function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\" should be for \"for some measurable function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\"?\n- Does the shift-response function is the same as the standard average treatment effect?\n- Some citations are missing.", "labeling_timestamp": "2026-01-11T17:25:04.959115", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper states only high-level \"standard regularity conditions\" and standard causal assumptions (consistency, unconfoundedness, positivity) but does not state explicit smoothness or convergence-rate conditions on nuisance estimators that would be needed to justify the claimed convergence/efficiency results.", "evidence": [{"quote": "Given standard regularity conditions, these guarantees imply that the SRF is consistently estimated when either the outcome model or the density-ratio model for the exposure shift is correctly specified, and achieves the best possible efficiency rate when both models are correctly specified.", "section": "Introduction / Contributions"}, {"quote": "The target estimand ψ can be expressed as a functional of the observable data distribution under standard assumptions, which are: 1 Consistency and no interference ... Assumption 2.1 (Unconfoundedness) ... Assumption 2.2 (Positivity).", "section": "Section 2: Causal identification"}, {"quote": "Using TR, we will obtain an estimator ˆψ_tr with the guarantees that ∥ˆψ_tr - ψ∥_2 converges in probability at an 'efficient' rate according to the prevailing semiparametric efficiency theory surrounding robust causal effect estimation (Kennedy, 2022).", "section": "Section 3: TRESNET"}], "section": ""}
{"claim": "Neural network nuisance estimators usually do not satisfy the Donsker condition, questioning applicability of the theoretical results when neural networks are used.", "claim_type": "methodology", "paper_id": "MqEQbvPvkE", "paper_title": "Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "kfNah4HeNr", "reviewer": "Reviewer_xEjj", "review_text": "Summary: This study provides a strong framework for treatment effect estimation based on the semiparametric theory. The authors develop a novel optimization problem for treatment effect estimation and show the asymptotic properties.\n\nStrengths: My major curiosity lies in the proof of Theorem 2. As discussed in the literature of double machine learning (cf. Chernozhukov et al. (2018)), to attain $\\sqrt{n}$-convergence of semiparametric estimators, we usually impose the Donsker condition for (nonparametric) nuisance estimators. However, it seems that the authors do not impose such assumptions. I am checking the proof, but could the authors provide intuitive reasons for the results? If this result is true, I believe that this is the theoretical strength of this study.\n\nThe above is also my concern because the posited assumptions are too weak to show the results. We usually impose some properties such as smoothness on the nuisance estimators to discuss convergence rates. Even if we obtain desirable convergence rates, neural network models usually do not satisfy the Donsker conditions. Therefore, I am afraid of missing assumptions or errorrness in the proof (I need to confirm the proof but have not yet done it...).\n\nWeaknesses: See above.\n\nQuestions: - Is Assumption 2.2 sufficient? If $p(a|x) \\propto 1/n$ for some $a$, then the results do not hold, I think $p(a|x)$ should be lower bounded by a positive constant independent of $n$.\n- In Theorem 2, what does $\\to$ indicate? Convergence of non-random variables or convergence in probability?\n- In Theorem 2, what does $O(r_1(n))$ in $\\|\\hat{\\mu} - \\mu\\|_\\infty$ mean? Should it be $O_P$?\n- Sugiyama et al. (2012) discussed the density-ratio estimation, and its interest differs from this study. Furthermore, it has been known that Eq. (8) can be used to estimate the propensity score. I think the citation may not be appropriate.\n- Does this study relate to automatic debiased learning proposed by Chernozhukov et al. (2022)?\n- \"for some function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\" should be for \"for some measurable function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\"?\n- Does the shift-response function is the same as the standard average treatment effect?\n- Some citations are missing.", "labeling_timestamp": "2026-01-11T17:24:59.798691", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does not discuss the Donsker condition or whether neural-network-based nuisance estimators satisfy it. It only invokes unspecified “standard regularity conditions” for its theoretical guarantees, so the reviewer’s claim about Donsker violations by neural nets is not addressed in the text.", "evidence": "\"Given standard regularity conditions, these guarantees imply that the SRF is consistently estimated when either the outcome model or the density-ratio model for the exposure shift is correctly specified, and achieves the best possible efficiency rate when both models are correctly specified.\"", "section": "1 INTRODUCTION (Contributions paragraph)"}
{"claim": "The proof of Theorem 2 may contain errors or omit necessary assumptions required for the claimed convergence.", "claim_type": "methodology", "paper_id": "MqEQbvPvkE", "paper_title": "Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "kfNah4HeNr", "reviewer": "Reviewer_xEjj", "review_text": "Summary: This study provides a strong framework for treatment effect estimation based on the semiparametric theory. The authors develop a novel optimization problem for treatment effect estimation and show the asymptotic properties.\n\nStrengths: My major curiosity lies in the proof of Theorem 2. As discussed in the literature of double machine learning (cf. Chernozhukov et al. (2018)), to attain $\\sqrt{n}$-convergence of semiparametric estimators, we usually impose the Donsker condition for (nonparametric) nuisance estimators. However, it seems that the authors do not impose such assumptions. I am checking the proof, but could the authors provide intuitive reasons for the results? If this result is true, I believe that this is the theoretical strength of this study.\n\nThe above is also my concern because the posited assumptions are too weak to show the results. We usually impose some properties such as smoothness on the nuisance estimators to discuss convergence rates. Even if we obtain desirable convergence rates, neural network models usually do not satisfy the Donsker conditions. Therefore, I am afraid of missing assumptions or errorrness in the proof (I need to confirm the proof but have not yet done it...).\n\nWeaknesses: See above.\n\nQuestions: - Is Assumption 2.2 sufficient? If $p(a|x) \\propto 1/n$ for some $a$, then the results do not hold, I think $p(a|x)$ should be lower bounded by a positive constant independent of $n$.\n- In Theorem 2, what does $\\to$ indicate? Convergence of non-random variables or convergence in probability?\n- In Theorem 2, what does $O(r_1(n))$ in $\\|\\hat{\\mu} - \\mu\\|_\\infty$ mean? Should it be $O_P$?\n- Sugiyama et al. (2012) discussed the density-ratio estimation, and its interest differs from this study. Furthermore, it has been known that Eq. (8) can be used to estimate the propensity score. I think the citation may not be appropriate.\n- Does this study relate to automatic debiased learning proposed by Chernozhukov et al. (2022)?\n- \"for some function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\" should be for \"for some measurable function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\"?\n- Does the shift-response function is the same as the standard average treatment effect?\n- Some citations are missing.", "labeling_timestamp": "2026-01-11T17:25:11.989414", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt makes theoretical claims about convergence and efficiency (e.g., targeted regularization yields double robustness and efficient convergence) but does not include Theorem 2 or its proof in the supplied content, so we cannot verify whether the alleged errors or omitted assumptions in Theorem 2 exist.", "evidence": "1) \"First, we use a TR loss that specifically targets SRFs, ensuring that our estimates retain the properties we have come to expect from TR methods such as asymptotic efficiency and double robustness (Kennedy, 2016).\" (Contributions)\n2) \"As we have suggested earlier, an estimator of ψ can be derived from estimators of the outcome and density ratio functions. Using TR, we will obtain an estimator ˆψ_tr with the guarantees that ∥ˆψ_tr - ψ∥_2 converges in probability at an 'efficient' rate according to the prevailing semiparametric efficiency theory surrounding robust causal effect estimation (Kennedy, 2022).\" (Section 3)", "section": "Abstract/Contributions; Section 3"}
{"claim": "If p(a|x) is proportional to 1/n for some action a, the paper's results likely fail; p(a|x) should be lower bounded by a positive constant independent of n.", "claim_type": "quantitative", "paper_id": "MqEQbvPvkE", "paper_title": "Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "kfNah4HeNr", "reviewer": "Reviewer_xEjj", "review_text": "Summary: This study provides a strong framework for treatment effect estimation based on the semiparametric theory. The authors develop a novel optimization problem for treatment effect estimation and show the asymptotic properties.\n\nStrengths: My major curiosity lies in the proof of Theorem 2. As discussed in the literature of double machine learning (cf. Chernozhukov et al. (2018)), to attain $\\sqrt{n}$-convergence of semiparametric estimators, we usually impose the Donsker condition for (nonparametric) nuisance estimators. However, it seems that the authors do not impose such assumptions. I am checking the proof, but could the authors provide intuitive reasons for the results? If this result is true, I believe that this is the theoretical strength of this study.\n\nThe above is also my concern because the posited assumptions are too weak to show the results. We usually impose some properties such as smoothness on the nuisance estimators to discuss convergence rates. Even if we obtain desirable convergence rates, neural network models usually do not satisfy the Donsker conditions. Therefore, I am afraid of missing assumptions or errorrness in the proof (I need to confirm the proof but have not yet done it...).\n\nWeaknesses: See above.\n\nQuestions: - Is Assumption 2.2 sufficient? If $p(a|x) \\propto 1/n$ for some $a$, then the results do not hold, I think $p(a|x)$ should be lower bounded by a positive constant independent of $n$.\n- In Theorem 2, what does $\\to$ indicate? Convergence of non-random variables or convergence in probability?\n- In Theorem 2, what does $O(r_1(n))$ in $\\|\\hat{\\mu} - \\mu\\|_\\infty$ mean? Should it be $O_P$?\n- Sugiyama et al. (2012) discussed the density-ratio estimation, and its interest differs from this study. Furthermore, it has been known that Eq. (8) can be used to estimate the propensity score. I think the citation may not be appropriate.\n- Does this study relate to automatic debiased learning proposed by Chernozhukov et al. (2022)?\n- \"for some function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\" should be for \"for some measurable function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\"?\n- Does the shift-response function is the same as the standard average treatment effect?\n- Some citations are missing.", "labeling_timestamp": "2026-01-11T17:25:31.840120", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper imposes a positivity condition in Assumption 2.2 that requires the density ratio w( x,a)=˜p(a|x)/p(a|x) be uniformly bounded (w<M). It does not state or analyze a requirement that p(a|x) be lower-bounded by a positive constant independent of n, nor does it discuss scenarios where p(a|x) scales like 1/n or sample-size-dependent vanishing probabilities. Therefore the specific reviewer claim about p(a|x) ∝ 1/n causing failure is not addressed in the paper.", "evidence": "Assumption 2.2 (Positivity). Let w(x,a) = ˜p(a|x)/p(a|x). Then, there exist a constant M > 0 such that w(x,a) < M for all (a,x) such that p(a|x) > 0.\n\nThe second assumption implies that the density ratio w(x,a) is well-defined and behaved. Notice that ψ = E[µ(X,A) w(X,A)] by the importance sampling formula.\n\nIntuitively, Assumption 2.2 prohibits extreme cases when the shifted exposures take value outside the practical domain of the observed exposure, in which case counterfactual estimation is impossible (Muñoz & Van Der Laan, 2012).", "section": "2 PROBLEM STATEMENT: THE CAUSAL EFFECT OF AN EXPOSURE SHIFT"}
{"claim": "Assumption 2.2 may be insufficient to guarantee the stated results in cases where propensity probabilities vanish with sample size.", "claim_type": "methodology", "paper_id": "MqEQbvPvkE", "paper_title": "Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "kfNah4HeNr", "reviewer": "Reviewer_xEjj", "review_text": "Summary: This study provides a strong framework for treatment effect estimation based on the semiparametric theory. The authors develop a novel optimization problem for treatment effect estimation and show the asymptotic properties.\n\nStrengths: My major curiosity lies in the proof of Theorem 2. As discussed in the literature of double machine learning (cf. Chernozhukov et al. (2018)), to attain $\\sqrt{n}$-convergence of semiparametric estimators, we usually impose the Donsker condition for (nonparametric) nuisance estimators. However, it seems that the authors do not impose such assumptions. I am checking the proof, but could the authors provide intuitive reasons for the results? If this result is true, I believe that this is the theoretical strength of this study.\n\nThe above is also my concern because the posited assumptions are too weak to show the results. We usually impose some properties such as smoothness on the nuisance estimators to discuss convergence rates. Even if we obtain desirable convergence rates, neural network models usually do not satisfy the Donsker conditions. Therefore, I am afraid of missing assumptions or errorrness in the proof (I need to confirm the proof but have not yet done it...).\n\nWeaknesses: See above.\n\nQuestions: - Is Assumption 2.2 sufficient? If $p(a|x) \\propto 1/n$ for some $a$, then the results do not hold, I think $p(a|x)$ should be lower bounded by a positive constant independent of $n$.\n- In Theorem 2, what does $\\to$ indicate? Convergence of non-random variables or convergence in probability?\n- In Theorem 2, what does $O(r_1(n))$ in $\\|\\hat{\\mu} - \\mu\\|_\\infty$ mean? Should it be $O_P$?\n- Sugiyama et al. (2012) discussed the density-ratio estimation, and its interest differs from this study. Furthermore, it has been known that Eq. (8) can be used to estimate the propensity score. I think the citation may not be appropriate.\n- Does this study relate to automatic debiased learning proposed by Chernozhukov et al. (2022)?\n- \"for some function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\" should be for \"for some measurable function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\"?\n- Does the shift-response function is the same as the standard average treatment effect?\n- Some citations are missing.", "labeling_timestamp": "2026-01-11T17:25:32.525203", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper states Assumption 2.2 as a boundedness condition on the density ratio w(x,a) and asserts this makes w “well-defined and behaved” (Section 2), but it does not discuss sequences of data-generating distributions or cases where the generalized propensity p(a|x) vanishes with sample size (no uniform-in-n positivity or related rates are provided). Therefore the paper does not confirm or refute the reviewer’s concern that Assumption 2.2 may be insufficient in such vanishing-propensity scenarios.", "evidence": "Assumption 2.2 (Positivity). Let w(x,a) = ˜p(a|x)/p(a|x). Then, there exist a constant M > 0 such that w(x,a) < M for all (a,x) such that p(a|x) > 0.\n\n\"The second assumption implies that the density ratio w(x,a) is well-defined and behaved.\"", "section": "2 PROBLEM STATEMENT: THE CAUSAL EFFECT OF AN EXPOSURE SHIFT (Causal identification / Assumption 2.2)"}
{"claim": "In Theorem 2, the meaning of the symbol '→' is unclear: the paper does not specify whether it denotes deterministic convergence or convergence in probability.", "claim_type": "methodology", "paper_id": "MqEQbvPvkE", "paper_title": "Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "kfNah4HeNr", "reviewer": "Reviewer_xEjj", "review_text": "Summary: This study provides a strong framework for treatment effect estimation based on the semiparametric theory. The authors develop a novel optimization problem for treatment effect estimation and show the asymptotic properties.\n\nStrengths: My major curiosity lies in the proof of Theorem 2. As discussed in the literature of double machine learning (cf. Chernozhukov et al. (2018)), to attain $\\sqrt{n}$-convergence of semiparametric estimators, we usually impose the Donsker condition for (nonparametric) nuisance estimators. However, it seems that the authors do not impose such assumptions. I am checking the proof, but could the authors provide intuitive reasons for the results? If this result is true, I believe that this is the theoretical strength of this study.\n\nThe above is also my concern because the posited assumptions are too weak to show the results. We usually impose some properties such as smoothness on the nuisance estimators to discuss convergence rates. Even if we obtain desirable convergence rates, neural network models usually do not satisfy the Donsker conditions. Therefore, I am afraid of missing assumptions or errorrness in the proof (I need to confirm the proof but have not yet done it...).\n\nWeaknesses: See above.\n\nQuestions: - Is Assumption 2.2 sufficient? If $p(a|x) \\propto 1/n$ for some $a$, then the results do not hold, I think $p(a|x)$ should be lower bounded by a positive constant independent of $n$.\n- In Theorem 2, what does $\\to$ indicate? Convergence of non-random variables or convergence in probability?\n- In Theorem 2, what does $O(r_1(n))$ in $\\|\\hat{\\mu} - \\mu\\|_\\infty$ mean? Should it be $O_P$?\n- Sugiyama et al. (2012) discussed the density-ratio estimation, and its interest differs from this study. Furthermore, it has been known that Eq. (8) can be used to estimate the propensity score. I think the citation may not be appropriate.\n- Does this study relate to automatic debiased learning proposed by Chernozhukov et al. (2022)?\n- \"for some function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\" should be for \"for some measurable function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\"?\n- Does the shift-response function is the same as the standard average treatment effect?\n- Some citations are missing.", "labeling_timestamp": "2026-01-11T17:25:15.150561", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not include Theorem 2 or an instance of the symbol '→' used in Theorem 2, so it is not possible to verify whether the paper clarifies whether '→' denotes deterministic convergence or convergence in probability. The excerpt does use the phrase 'converges in probability' elsewhere, but that does not resolve the ambiguity about the notation in Theorem 2.", "evidence": "“...we will obtain an estimator ˆψ_tr with the guarantees that ∥ˆψ_tr - ψ∥_2 converges in probability at an 'efficient' rate according to the prevailing semiparametric efficiency theory...”", "section": "3 TRESNET: TARGETED REGULARIZATION FOR ESTIMATING THE CAUSAL EFFECTS OF EXPOSURE SHIFTS WITH NEURAL NETWORKS"}
{"claim": "The notation O(r_1(n)) for ||\\hat{\\mu} - \\mu||_\\infty in Theorem 2 should likely be stochastic order O_P, but the manuscript does not clarify this distinction.", "claim_type": "methodology", "paper_id": "MqEQbvPvkE", "paper_title": "Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "kfNah4HeNr", "reviewer": "Reviewer_xEjj", "review_text": "Summary: This study provides a strong framework for treatment effect estimation based on the semiparametric theory. The authors develop a novel optimization problem for treatment effect estimation and show the asymptotic properties.\n\nStrengths: My major curiosity lies in the proof of Theorem 2. As discussed in the literature of double machine learning (cf. Chernozhukov et al. (2018)), to attain $\\sqrt{n}$-convergence of semiparametric estimators, we usually impose the Donsker condition for (nonparametric) nuisance estimators. However, it seems that the authors do not impose such assumptions. I am checking the proof, but could the authors provide intuitive reasons for the results? If this result is true, I believe that this is the theoretical strength of this study.\n\nThe above is also my concern because the posited assumptions are too weak to show the results. We usually impose some properties such as smoothness on the nuisance estimators to discuss convergence rates. Even if we obtain desirable convergence rates, neural network models usually do not satisfy the Donsker conditions. Therefore, I am afraid of missing assumptions or errorrness in the proof (I need to confirm the proof but have not yet done it...).\n\nWeaknesses: See above.\n\nQuestions: - Is Assumption 2.2 sufficient? If $p(a|x) \\propto 1/n$ for some $a$, then the results do not hold, I think $p(a|x)$ should be lower bounded by a positive constant independent of $n$.\n- In Theorem 2, what does $\\to$ indicate? Convergence of non-random variables or convergence in probability?\n- In Theorem 2, what does $O(r_1(n))$ in $\\|\\hat{\\mu} - \\mu\\|_\\infty$ mean? Should it be $O_P$?\n- Sugiyama et al. (2012) discussed the density-ratio estimation, and its interest differs from this study. Furthermore, it has been known that Eq. (8) can be used to estimate the propensity score. I think the citation may not be appropriate.\n- Does this study relate to automatic debiased learning proposed by Chernozhukov et al. (2022)?\n- \"for some function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\" should be for \"for some measurable function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\"?\n- Does the shift-response function is the same as the standard average treatment effect?\n- Some citations are missing.", "labeling_timestamp": "2026-01-11T17:25:17.780108", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided excerpt does not include Theorem 2 or the specific notation 'O(r_1(n))' for ||\\hat{\\mu}-\\mu||_\\infty, so it is impossible to verify whether the manuscript used deterministic O(·) or stochastic O_P(·) notation or whether the distinction was clarified. The paper does use probabilistic language elsewhere (e.g., 'converges in probability'), but that does not establish how the big-O notation is used in Theorem 2.", "evidence": "we will obtain an estimator ˆ ψ tr with the guarantees that ∥ ˆ ψ tr -ψ ∥ 2 converges in probability at an 'efficient' rate according to the prevailing semiparametric efficiency theory surrounding robust causal effect estimation (Kennedy, 2022).", "section": "3 TRESNET: TARGETED REGULARIZATION FOR ESTIMATING THE CAUSAL EFFECTS OF EXPOSURE SHIFTS WITH NEURAL NETWORKS"}
{"claim": "Citation of Sugiyama et al. (2012) may be inappropriate because their density-ratio estimation focus differs and Equation (8) is already known to estimate propensity scores.", "claim_type": "novelty", "paper_id": "MqEQbvPvkE", "paper_title": "Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "kfNah4HeNr", "reviewer": "Reviewer_xEjj", "review_text": "Summary: This study provides a strong framework for treatment effect estimation based on the semiparametric theory. The authors develop a novel optimization problem for treatment effect estimation and show the asymptotic properties.\n\nStrengths: My major curiosity lies in the proof of Theorem 2. As discussed in the literature of double machine learning (cf. Chernozhukov et al. (2018)), to attain $\\sqrt{n}$-convergence of semiparametric estimators, we usually impose the Donsker condition for (nonparametric) nuisance estimators. However, it seems that the authors do not impose such assumptions. I am checking the proof, but could the authors provide intuitive reasons for the results? If this result is true, I believe that this is the theoretical strength of this study.\n\nThe above is also my concern because the posited assumptions are too weak to show the results. We usually impose some properties such as smoothness on the nuisance estimators to discuss convergence rates. Even if we obtain desirable convergence rates, neural network models usually do not satisfy the Donsker conditions. Therefore, I am afraid of missing assumptions or errorrness in the proof (I need to confirm the proof but have not yet done it...).\n\nWeaknesses: See above.\n\nQuestions: - Is Assumption 2.2 sufficient? If $p(a|x) \\propto 1/n$ for some $a$, then the results do not hold, I think $p(a|x)$ should be lower bounded by a positive constant independent of $n$.\n- In Theorem 2, what does $\\to$ indicate? Convergence of non-random variables or convergence in probability?\n- In Theorem 2, what does $O(r_1(n))$ in $\\|\\hat{\\mu} - \\mu\\|_\\infty$ mean? Should it be $O_P$?\n- Sugiyama et al. (2012) discussed the density-ratio estimation, and its interest differs from this study. Furthermore, it has been known that Eq. (8) can be used to estimate the propensity score. I think the citation may not be appropriate.\n- Does this study relate to automatic debiased learning proposed by Chernozhukov et al. (2022)?\n- \"for some function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\" should be for \"for some measurable function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\"?\n- Does the shift-response function is the same as the standard average treatment effect?\n- Some citations are missing.", "labeling_timestamp": "2026-01-11T17:25:42.311148", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not include any citation to Sugiyama et al. (2012) nor does it display or label an Equation (8), so we cannot verify whether that citation is present or inappropriate, or whether Equation (8) corresponds to propensity-score estimation. The paper does discuss density-ratio estimation (w) and states that estimators of the outcome and density-ratio suffice to estimate the estimand (Section 2), but this does not allow assessment of the specific claim about Sugiyama (2012) or Equation (8).", "evidence": "Let w ( x , a ) = ˜ p ( a | x ) /p ( a | x ) .\n\nTherefore, estimators of µ and w will suffice to estimate ψ .", "section": "2 PROBLEM STATEMENT / Causal identification"}
{"claim": "The manuscript does not discuss whether the proposed method relates to automatic debiased learning proposed by Chernozhukov et al. (2022).", "claim_type": "other", "paper_id": "MqEQbvPvkE", "paper_title": "Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "kfNah4HeNr", "reviewer": "Reviewer_xEjj", "review_text": "Summary: This study provides a strong framework for treatment effect estimation based on the semiparametric theory. The authors develop a novel optimization problem for treatment effect estimation and show the asymptotic properties.\n\nStrengths: My major curiosity lies in the proof of Theorem 2. As discussed in the literature of double machine learning (cf. Chernozhukov et al. (2018)), to attain $\\sqrt{n}$-convergence of semiparametric estimators, we usually impose the Donsker condition for (nonparametric) nuisance estimators. However, it seems that the authors do not impose such assumptions. I am checking the proof, but could the authors provide intuitive reasons for the results? If this result is true, I believe that this is the theoretical strength of this study.\n\nThe above is also my concern because the posited assumptions are too weak to show the results. We usually impose some properties such as smoothness on the nuisance estimators to discuss convergence rates. Even if we obtain desirable convergence rates, neural network models usually do not satisfy the Donsker conditions. Therefore, I am afraid of missing assumptions or errorrness in the proof (I need to confirm the proof but have not yet done it...).\n\nWeaknesses: See above.\n\nQuestions: - Is Assumption 2.2 sufficient? If $p(a|x) \\propto 1/n$ for some $a$, then the results do not hold, I think $p(a|x)$ should be lower bounded by a positive constant independent of $n$.\n- In Theorem 2, what does $\\to$ indicate? Convergence of non-random variables or convergence in probability?\n- In Theorem 2, what does $O(r_1(n))$ in $\\|\\hat{\\mu} - \\mu\\|_\\infty$ mean? Should it be $O_P$?\n- Sugiyama et al. (2012) discussed the density-ratio estimation, and its interest differs from this study. Furthermore, it has been known that Eq. (8) can be used to estimate the propensity score. I think the citation may not be appropriate.\n- Does this study relate to automatic debiased learning proposed by Chernozhukov et al. (2022)?\n- \"for some function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\" should be for \"for some measurable function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\"?\n- Does the shift-response function is the same as the standard average treatment effect?\n- Some citations are missing.", "labeling_timestamp": "2026-01-11T17:25:42.655790", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper discusses efficient influence functions, Neyman orthogonality, and double machine learning (Kennedy, 2022) and targeted regularization (Shi et al., 2019), but it does not mention Chernozhukov et al. (2022) or 'automatic debiased learning'.", "evidence": "“EIFs are also referred to as Neyman orthogonal scores in the double machine learning literature (Kennedy, 2022). Targeted regularization (TR) (Shi et al., 2019) links EIF estimation methods to neural network architectures and optimization.”", "section": "1 INTRODUCTION (Related work paragraph)"}
{"claim": "The phrase 'for some function η: X×A→R' should specify 'measurable function' for mathematical rigor, but the paper omits this qualification.", "claim_type": "methodology", "paper_id": "MqEQbvPvkE", "paper_title": "Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "kfNah4HeNr", "reviewer": "Reviewer_xEjj", "review_text": "Summary: This study provides a strong framework for treatment effect estimation based on the semiparametric theory. The authors develop a novel optimization problem for treatment effect estimation and show the asymptotic properties.\n\nStrengths: My major curiosity lies in the proof of Theorem 2. As discussed in the literature of double machine learning (cf. Chernozhukov et al. (2018)), to attain $\\sqrt{n}$-convergence of semiparametric estimators, we usually impose the Donsker condition for (nonparametric) nuisance estimators. However, it seems that the authors do not impose such assumptions. I am checking the proof, but could the authors provide intuitive reasons for the results? If this result is true, I believe that this is the theoretical strength of this study.\n\nThe above is also my concern because the posited assumptions are too weak to show the results. We usually impose some properties such as smoothness on the nuisance estimators to discuss convergence rates. Even if we obtain desirable convergence rates, neural network models usually do not satisfy the Donsker conditions. Therefore, I am afraid of missing assumptions or errorrness in the proof (I need to confirm the proof but have not yet done it...).\n\nWeaknesses: See above.\n\nQuestions: - Is Assumption 2.2 sufficient? If $p(a|x) \\propto 1/n$ for some $a$, then the results do not hold, I think $p(a|x)$ should be lower bounded by a positive constant independent of $n$.\n- In Theorem 2, what does $\\to$ indicate? Convergence of non-random variables or convergence in probability?\n- In Theorem 2, what does $O(r_1(n))$ in $\\|\\hat{\\mu} - \\mu\\|_\\infty$ mean? Should it be $O_P$?\n- Sugiyama et al. (2012) discussed the density-ratio estimation, and its interest differs from this study. Furthermore, it has been known that Eq. (8) can be used to estimate the propensity score. I think the citation may not be appropriate.\n- Does this study relate to automatic debiased learning proposed by Chernozhukov et al. (2022)?\n- \"for some function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\" should be for \"for some measurable function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\"?\n- Does the shift-response function is the same as the standard average treatment effect?\n- Some citations are missing.", "labeling_timestamp": "2026-01-11T17:25:37.989604", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not contain the exact phrase 'for some function η: X×A→R', so we cannot confirm whether the paper omitted the qualifier 'measurable' in that phrasing. The paper does define functions (e.g., µ(x,a)=E[Y_a|X=x]) without an explicit measurability statement in the shown sections, but this does not prove the specific phrase in the reviewer's claim appears elsewhere in the full paper or is missing the qualifier.", "evidence": "The conditional expectation of the potential outcomes is defined as µ ( x , a ) = E [ Y a | X = x ] .", "section": "2 PROBLEM STATEMENT: THE CAUSAL EFFECT OF AN EXPOSURE SHIFT"}
{"claim": "It is unclear whether the paper's shift-response function is equivalent to the standard average treatment effect; the manuscript does not clarify this relationship.", "claim_type": "methodology", "paper_id": "MqEQbvPvkE", "paper_title": "Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "kfNah4HeNr", "reviewer": "Reviewer_xEjj", "review_text": "Summary: This study provides a strong framework for treatment effect estimation based on the semiparametric theory. The authors develop a novel optimization problem for treatment effect estimation and show the asymptotic properties.\n\nStrengths: My major curiosity lies in the proof of Theorem 2. As discussed in the literature of double machine learning (cf. Chernozhukov et al. (2018)), to attain $\\sqrt{n}$-convergence of semiparametric estimators, we usually impose the Donsker condition for (nonparametric) nuisance estimators. However, it seems that the authors do not impose such assumptions. I am checking the proof, but could the authors provide intuitive reasons for the results? If this result is true, I believe that this is the theoretical strength of this study.\n\nThe above is also my concern because the posited assumptions are too weak to show the results. We usually impose some properties such as smoothness on the nuisance estimators to discuss convergence rates. Even if we obtain desirable convergence rates, neural network models usually do not satisfy the Donsker conditions. Therefore, I am afraid of missing assumptions or errorrness in the proof (I need to confirm the proof but have not yet done it...).\n\nWeaknesses: See above.\n\nQuestions: - Is Assumption 2.2 sufficient? If $p(a|x) \\propto 1/n$ for some $a$, then the results do not hold, I think $p(a|x)$ should be lower bounded by a positive constant independent of $n$.\n- In Theorem 2, what does $\\to$ indicate? Convergence of non-random variables or convergence in probability?\n- In Theorem 2, what does $O(r_1(n))$ in $\\|\\hat{\\mu} - \\mu\\|_\\infty$ mean? Should it be $O_P$?\n- Sugiyama et al. (2012) discussed the density-ratio estimation, and its interest differs from this study. Furthermore, it has been known that Eq. (8) can be used to estimate the propensity score. I think the citation may not be appropriate.\n- Does this study relate to automatic debiased learning proposed by Chernozhukov et al. (2022)?\n- \"for some function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\" should be for \"for some measurable function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\"?\n- Does the shift-response function is the same as the standard average treatment effect?\n- Some citations are missing.", "labeling_timestamp": "2026-01-11T17:25:56.956325", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states that the SRF is not equivalent to standard causal effects and clarifies its relation to other estimands (ATE and ERF), so the reviewer's claim that the manuscript does not clarify the relationship is contradicted by the text.", "evidence": "“This estimand cannot be expressed in terms of traditional causal effects such as the average treatment effect (ATE) or an exposure-response function (ERF) (Muñoz & Van Der Laan, 2012).”\n\n“Mathematically, the ERF ξ can be written as the mapping ξ(a) = E[µ(X,A) | A = a]. One can consider ERFs as a limiting case of SRFs when ˜p is a point mass distribution centered at a fixed treatment value assigned equally to all units.”", "section": "Section 2: Problem Statement: The Causal Effect of an Exposure Shift"}
{"claim": "The manuscript omits some relevant citations needed to properly contextualize related work and prior methods.", "claim_type": "subjective", "paper_id": "MqEQbvPvkE", "paper_title": "Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "kfNah4HeNr", "reviewer": "Reviewer_xEjj", "review_text": "Summary: This study provides a strong framework for treatment effect estimation based on the semiparametric theory. The authors develop a novel optimization problem for treatment effect estimation and show the asymptotic properties.\n\nStrengths: My major curiosity lies in the proof of Theorem 2. As discussed in the literature of double machine learning (cf. Chernozhukov et al. (2018)), to attain $\\sqrt{n}$-convergence of semiparametric estimators, we usually impose the Donsker condition for (nonparametric) nuisance estimators. However, it seems that the authors do not impose such assumptions. I am checking the proof, but could the authors provide intuitive reasons for the results? If this result is true, I believe that this is the theoretical strength of this study.\n\nThe above is also my concern because the posited assumptions are too weak to show the results. We usually impose some properties such as smoothness on the nuisance estimators to discuss convergence rates. Even if we obtain desirable convergence rates, neural network models usually do not satisfy the Donsker conditions. Therefore, I am afraid of missing assumptions or errorrness in the proof (I need to confirm the proof but have not yet done it...).\n\nWeaknesses: See above.\n\nQuestions: - Is Assumption 2.2 sufficient? If $p(a|x) \\propto 1/n$ for some $a$, then the results do not hold, I think $p(a|x)$ should be lower bounded by a positive constant independent of $n$.\n- In Theorem 2, what does $\\to$ indicate? Convergence of non-random variables or convergence in probability?\n- In Theorem 2, what does $O(r_1(n))$ in $\\|\\hat{\\mu} - \\mu\\|_\\infty$ mean? Should it be $O_P$?\n- Sugiyama et al. (2012) discussed the density-ratio estimation, and its interest differs from this study. Furthermore, it has been known that Eq. (8) can be used to estimate the propensity score. I think the citation may not be appropriate.\n- Does this study relate to automatic debiased learning proposed by Chernozhukov et al. (2022)?\n- \"for some function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\" should be for \"for some measurable function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\"?\n- Does the shift-response function is the same as the standard average treatment effect?\n- Some citations are missing.", "labeling_timestamp": "2026-01-11T17:25:54.218275", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper contains a related-work discussion and cites multiple prior methods (e.g., targeted regularization, DRAGONNET, VCNET, semiparametric theory, and air-pollution ERF studies). From the provided content there is no evidence or author acknowledgement that relevant citations were omitted, nor is there an exhaustive bibliography to verify missing references; therefore it cannot be determined from the manuscript excerpt whether some relevant citations are indeed omitted.", "evidence": "Related work Recent papers have most often estimated causal effects relating air pollution to elder mortality using exposure-response functions (ERFs); see for example Wu et al. (2020); Bahadori et al. (2022); Josey et al., (2023). However, none of the methods implemented in these studies target an SRF estimand. ... Targeted regularization (TR) (Shi et al., 2019) links EIF estimation methods to neural network architectures and optimization. Recent uses of TR include the DRAGONNET (Shi et al., 2019), which introduced TR for targeting the average treatment effect of a binary treatment, and the VCNET (Nie et al., 2021), for targeting the exposure-response function (ERF) of a continuous exposure.", "section": "Related work / Introduction"}
{"claim": "The paper lacks an intuitive, high-level explanation for why Theorem 2 holds despite not imposing typical nuisance estimator conditions.", "claim_type": "presentation", "paper_id": "MqEQbvPvkE", "paper_title": "Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "kfNah4HeNr", "reviewer": "Reviewer_xEjj", "review_text": "Summary: This study provides a strong framework for treatment effect estimation based on the semiparametric theory. The authors develop a novel optimization problem for treatment effect estimation and show the asymptotic properties.\n\nStrengths: My major curiosity lies in the proof of Theorem 2. As discussed in the literature of double machine learning (cf. Chernozhukov et al. (2018)), to attain $\\sqrt{n}$-convergence of semiparametric estimators, we usually impose the Donsker condition for (nonparametric) nuisance estimators. However, it seems that the authors do not impose such assumptions. I am checking the proof, but could the authors provide intuitive reasons for the results? If this result is true, I believe that this is the theoretical strength of this study.\n\nThe above is also my concern because the posited assumptions are too weak to show the results. We usually impose some properties such as smoothness on the nuisance estimators to discuss convergence rates. Even if we obtain desirable convergence rates, neural network models usually do not satisfy the Donsker conditions. Therefore, I am afraid of missing assumptions or errorrness in the proof (I need to confirm the proof but have not yet done it...).\n\nWeaknesses: See above.\n\nQuestions: - Is Assumption 2.2 sufficient? If $p(a|x) \\propto 1/n$ for some $a$, then the results do not hold, I think $p(a|x)$ should be lower bounded by a positive constant independent of $n$.\n- In Theorem 2, what does $\\to$ indicate? Convergence of non-random variables or convergence in probability?\n- In Theorem 2, what does $O(r_1(n))$ in $\\|\\hat{\\mu} - \\mu\\|_\\infty$ mean? Should it be $O_P$?\n- Sugiyama et al. (2012) discussed the density-ratio estimation, and its interest differs from this study. Furthermore, it has been known that Eq. (8) can be used to estimate the propensity score. I think the citation may not be appropriate.\n- Does this study relate to automatic debiased learning proposed by Chernozhukov et al. (2022)?\n- \"for some function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\" should be for \"for some measurable function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\"?\n- Does the shift-response function is the same as the standard average treatment effect?\n- Some citations are missing.", "labeling_timestamp": "2026-01-11T17:25:58.471600", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided excerpts state theoretical guarantees (double robustness and efficiency) and refer to 'standard regularity conditions' (Introduction/Contributions) and introduce the EIF (Section 3), but Theorem 2 itself and any accompanying intuitive, high-level explanation are not present in the supplied text. Therefore there is insufficient information to determine whether the paper lacks the specific intuitive explanation the reviewer claims is missing.", "evidence": "Given standard regularity conditions, these guarantees imply that the SRF is consistently estimated when either the outcome model or the density-ratio model for the exposure shift is correctly specified, and achieves the best possible efficiency rate when both models are correctly specified.", "section": "Introduction / Contributions"}
{"claim": "The paper fails to explain how it attains root-n convergence without imposing Donsker or alternative sufficient conditions on nuisance estimators.", "claim_type": "methodology", "paper_id": "MqEQbvPvkE", "paper_title": "Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "kfNah4HeNr", "reviewer": "Reviewer_xEjj", "review_text": "Summary: This study provides a strong framework for treatment effect estimation based on the semiparametric theory. The authors develop a novel optimization problem for treatment effect estimation and show the asymptotic properties.\n\nStrengths: My major curiosity lies in the proof of Theorem 2. As discussed in the literature of double machine learning (cf. Chernozhukov et al. (2018)), to attain $\\sqrt{n}$-convergence of semiparametric estimators, we usually impose the Donsker condition for (nonparametric) nuisance estimators. However, it seems that the authors do not impose such assumptions. I am checking the proof, but could the authors provide intuitive reasons for the results? If this result is true, I believe that this is the theoretical strength of this study.\n\nThe above is also my concern because the posited assumptions are too weak to show the results. We usually impose some properties such as smoothness on the nuisance estimators to discuss convergence rates. Even if we obtain desirable convergence rates, neural network models usually do not satisfy the Donsker conditions. Therefore, I am afraid of missing assumptions or errorrness in the proof (I need to confirm the proof but have not yet done it...).\n\nWeaknesses: See above.\n\nQuestions: - Is Assumption 2.2 sufficient? If $p(a|x) \\propto 1/n$ for some $a$, then the results do not hold, I think $p(a|x)$ should be lower bounded by a positive constant independent of $n$.\n- In Theorem 2, what does $\\to$ indicate? Convergence of non-random variables or convergence in probability?\n- In Theorem 2, what does $O(r_1(n))$ in $\\|\\hat{\\mu} - \\mu\\|_\\infty$ mean? Should it be $O_P$?\n- Sugiyama et al. (2012) discussed the density-ratio estimation, and its interest differs from this study. Furthermore, it has been known that Eq. (8) can be used to estimate the propensity score. I think the citation may not be appropriate.\n- Does this study relate to automatic debiased learning proposed by Chernozhukov et al. (2022)?\n- \"for some function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\" should be for \"for some measurable function $\\eta:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}$\"?\n- Does the shift-response function is the same as the standard average treatment effect?\n- Some citations are missing.", "labeling_timestamp": "2026-01-11T17:26:09.362747", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper asserts root-n / semiparametric efficiency guarantees for the estimator under “standard regularity conditions” and via targeted regularization (Sections 'Contributions' and '3 TRESNET'), but the provided text does not state the specific empirical-process / Donsker conditions nor alternative sufficient conditions (e.g., cross-fitting or sample-splitting) on nuisance estimators that would justify root-n asymptotics for modern, flexible learners.", "evidence": "1) \"Given standard regularity conditions, these guarantees imply that the SRF is consistently estimated when either the outcome model or the density-ratio model for the exposure shift is correctly specified, and achieves the best possible efficiency rate when both models are correctly specified.\" 2) \"As we have suggested earlier, an estimator of ψ can be derived from estimators of the outcome and density ratio functions. Using TR, we will obtain an estimator ˆψ_tr with the guarantees that ∥ˆψ_tr -ψ∥_2 converges in probability at an 'efficient' rate according to the prevailing semiparametric efficiency theory (Kennedy, 2022).\"", "section": "Contributions (Introduction) and 3 TRESNET"}
{"claim": "MGDL exhibits loss spikes during training, indicating potential instability in the method.", "claim_type": "experimental", "paper_id": "IoRT7EhFap", "paper_title": "Addressing Spectral Bias of Deep Neural Networks by Multi-Grade Deep Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "3b0MU64PcM", "reviewer": "Reviewer_otLk", "review_text": "Comment: For the weakness: Our concern, \"There are concerns about the performance and stability of MGDL, as evidenced by loss spikes and lack of significant loss reduction in real-world tasks (Figure 8),\" was not clearly addressed, nor were additional numerical experiments provided to alleviate our doubts. Additionally, there is inconsistency in the use of commas in the numerical values in Table 1, such as \"32,401\" and \"27, 817.\"\n\nFor question2: Can I understand it this way: When we divide the network into multiple grades, the cost of forward propagation increases during training. For example, when training the n-th grade of the network, the forward propagation process requires additional computations from the previous n-1 grades, resulting in the computational cost of forward propagation growing quadratically.", "labeling_timestamp": "2026-01-11T17:26:08.246318", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper content does not report or discuss training loss spikes or any instability of MGDL. The paper describes MGDL's training procedure and claims empirical efficacy (Abstract, Introduction, Section 2) but contains no statements, training curves, or analysis mentioning loss spikes or instability.", "evidence": "1) \"Our study reveals that MGDL excels at representing functions containing high-frequency information.\" (Abstract)\n2) \"When D is relatively small, for example, D < 5, we call N_D an SNN. It is well-recognized that training an SNN is notably easier than training a DNN.\" (Section 2: Proposed Approach and Multi-Grade Learning Model)", "section": "Abstract; Section 2 (Proposed Approach and Multi-Grade Learning Model)"}
{"claim": "MGDL shows a lack of significant loss reduction on real-world tasks, as indicated by Figure 8.", "claim_type": "experimental", "paper_id": "IoRT7EhFap", "paper_title": "Addressing Spectral Bias of Deep Neural Networks by Multi-Grade Deep Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "3b0MU64PcM", "reviewer": "Reviewer_otLk", "review_text": "Comment: For the weakness: Our concern, \"There are concerns about the performance and stability of MGDL, as evidenced by loss spikes and lack of significant loss reduction in real-world tasks (Figure 8),\" was not clearly addressed, nor were additional numerical experiments provided to alleviate our doubts. Additionally, there is inconsistency in the use of commas in the numerical values in Table 1, such as \"32,401\" and \"27, 817.\"\n\nFor question2: Can I understand it this way: When we divide the network into multiple grades, the cost of forward propagation increases during training. For example, when training the n-th grade of the network, the forward propagation process requires additional computations from the previous n-1 grades, resulting in the computational cost of forward propagation growing quadratically.", "labeling_timestamp": "2026-01-11T17:26:20.942987", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not include Figure 8 or any figures/tables showing loss reduction on real-world tasks. The excerpt only shows Figure 1 and textual mentions of experiments (e.g., MNIST and colored images), so there is insufficient information to verify the reviewer's claim about Figure 8.", "evidence": "1) \"We apply MGDL to synthetic, manifold, colored images, and MNIST datasets, all characterized by presence of high-frequency features.\" (Abstract)\n2) \"Figure 1: Spectrum comparison of f := f1 + f2 ◦ f1 + f3 ◦ f2 ◦ f1 + f4 ◦ f3 ◦ f2 ◦ f1 and fj: Amplitude versus one-side frequency plots for function f (Left) and fj for j ∈ N4 (Right).\" (Figure 1 caption)", "section": "Abstract and Figure 1 caption (Introduction/Section 2)"}
{"claim": "The authors did not clearly address the performance and stability concerns, nor provide additional numerical experiments to alleviate these doubts.", "claim_type": "experimental", "paper_id": "IoRT7EhFap", "paper_title": "Addressing Spectral Bias of Deep Neural Networks by Multi-Grade Deep Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "3b0MU64PcM", "reviewer": "Reviewer_otLk", "review_text": "Comment: For the weakness: Our concern, \"There are concerns about the performance and stability of MGDL, as evidenced by loss spikes and lack of significant loss reduction in real-world tasks (Figure 8),\" was not clearly addressed, nor were additional numerical experiments provided to alleviate our doubts. Additionally, there is inconsistency in the use of commas in the numerical values in Table 1, such as \"32,401\" and \"27, 817.\"\n\nFor question2: Can I understand it this way: When we divide the network into multiple grades, the cost of forward propagation increases during training. For example, when training the n-th grade of the network, the forward propagation process requires additional computations from the previous n-1 grades, resulting in the computational cost of forward propagation growing quadratically.", "labeling_timestamp": "2026-01-11T17:26:37.491337", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper does include multiple numerical experiments (1D, 2D manifold, colored images, and MNIST) demonstrating efficacy, so the reviewer's claim that no additional numerical experiments were provided is contradicted. However, the paper does not present an explicit analysis or discussion of stability (e.g., convergence, robustness, sensitivity, or runtime) or dedicated performance/stability experiments, so the portion of the claim about not clearly addressing performance/stability concerns is accurate.", "evidence": "\"We apply MGDL to synthetic, manifold, colored images, and MNIST datasets, all characterized by presence of high-frequency features. Our study reveals that MGDL excels at representing functions containing high-frequency information.\" (Abstract)\n\n\"We demonstrate the efficacy of the proposed approach in four experiments with one-dimensional synthetic data, two-dimensional manifold data, two-dimensional colored images, and very high-dimensional modified National Institute of Standards and Technology (MNIST) data.\" (Introduction)\n\n\"Contributions of this paper include: (a) ... (b) ... (c) We successfully apply the proposed approach to synthetic data in 1 and 2 dimensions and real data in 2 and 784 dimensions, showing that it can effectively address the spectral bias issue.\" (Contributions)", "section": "Abstract; Introduction; Contributions"}
{"claim": "Table 1 contains inconsistent comma usage in numerical values, for example '32,401' versus '27, 817', indicating presentation errors.", "claim_type": "presentation", "paper_id": "IoRT7EhFap", "paper_title": "Addressing Spectral Bias of Deep Neural Networks by Multi-Grade Deep Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "3b0MU64PcM", "reviewer": "Reviewer_otLk", "review_text": "Comment: For the weakness: Our concern, \"There are concerns about the performance and stability of MGDL, as evidenced by loss spikes and lack of significant loss reduction in real-world tasks (Figure 8),\" was not clearly addressed, nor were additional numerical experiments provided to alleviate our doubts. Additionally, there is inconsistency in the use of commas in the numerical values in Table 1, such as \"32,401\" and \"27, 817.\"\n\nFor question2: Can I understand it this way: When we divide the network into multiple grades, the cost of forward propagation increases during training. For example, when training the n-th grade of the network, the forward propagation process requires additional computations from the previous n-1 grades, resulting in the computational cost of forward propagation growing quadratically.", "labeling_timestamp": "2026-01-11T17:26:17.026583", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt (Abstract, Introduction, and Section 2) does not include Table 1 or any table content, so there is insufficient information to verify the claim about inconsistent comma usage in Table 1.", "evidence": "\"Figure 1: Spectrum comparison of f := f 1 + f 2 ◦ f 1 + f 3 ◦ f 2 ◦ f 1 + f 4 ◦ f 3 ◦ f 2 ◦ f 1 and f j : Amplitude versus one-side frequency plots for function f (Left) and f j for j ∈ N 4 (Right).\"", "section": "Section 2 (Provided content up through Section 2)"}
{"claim": "Dividing the network into multiple grades increases the forward propagation computational cost during training.", "claim_type": "quantitative", "paper_id": "IoRT7EhFap", "paper_title": "Addressing Spectral Bias of Deep Neural Networks by Multi-Grade Deep Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "3b0MU64PcM", "reviewer": "Reviewer_otLk", "review_text": "Comment: For the weakness: Our concern, \"There are concerns about the performance and stability of MGDL, as evidenced by loss spikes and lack of significant loss reduction in real-world tasks (Figure 8),\" was not clearly addressed, nor were additional numerical experiments provided to alleviate our doubts. Additionally, there is inconsistency in the use of commas in the numerical values in Table 1, such as \"32,401\" and \"27, 817.\"\n\nFor question2: Can I understand it this way: When we divide the network into multiple grades, the cost of forward propagation increases during training. For example, when training the n-th grade of the network, the forward propagation process requires additional computations from the previous n-1 grades, resulting in the computational cost of forward propagation growing quadratically.", "labeling_timestamp": "2026-01-11T17:26:36.560532", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper describes the MGDL training procedure (splitting into grades and training shallow networks sequentially) and notes that training SNNs is easier, but it does not state or analyze whether forward propagation computational cost during training increases when dividing the network into multiple grades.", "evidence": "Abstract: \"MGDL model, a recently introduced model that trains a DNN incrementally, grade by grade, a current grade learning from the residue of the previous grade only an SNN (with trainable parameters) composed with the SNNs (with fixed parameters) trained in the preceding grades as features.\"; Section 1: \"When D is relatively small, for example, D < 5, we call N_D an SNN. It is well-recognized that training an SNN is notably easier than training a DNN.\"; Section 2: \"Following [43], we split a DNN with depth D into L grades, with L < D, each of which learns an SNN N_{D_l}, defined as (1), with depth D_l, from the residue {e_l^ℓ}_{ℓ=1}^N of the previous grade, where 1 < D_l < D and ∑_{l=1}^L D_l = D + L - 1.\"", "section": "Abstract; 1 Introduction; 2 Proposed Approach and Multi-Grade Learning Model"}
{"claim": "When training the n-th grade, forward propagation requires additional computations from the previous n-1 grades.", "claim_type": "methodology", "paper_id": "IoRT7EhFap", "paper_title": "Addressing Spectral Bias of Deep Neural Networks by Multi-Grade Deep Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "3b0MU64PcM", "reviewer": "Reviewer_otLk", "review_text": "Comment: For the weakness: Our concern, \"There are concerns about the performance and stability of MGDL, as evidenced by loss spikes and lack of significant loss reduction in real-world tasks (Figure 8),\" was not clearly addressed, nor were additional numerical experiments provided to alleviate our doubts. Additionally, there is inconsistency in the use of commas in the numerical values in Table 1, such as \"32,401\" and \"27, 817.\"\n\nFor question2: Can I understand it this way: When we divide the network into multiple grades, the cost of forward propagation increases during training. For example, when training the n-th grade of the network, the forward propagation process requires additional computations from the previous n-1 grades, resulting in the computational cost of forward propagation growing quadratically.", "labeling_timestamp": "2026-01-11T17:26:30.810416", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly states that each new grade's network is composed with the SNNs trained in previous grades (with fixed parameters) and defines the recursive form g_{l+1}(Θ_{l+1}; x) as a composition involving H_{...}(Θ*_previous; ·). Thus forward propagation during training of grade n uses computations from the previous n-1 grades.", "evidence": "Abstract: \"a current grade learning from the residue of the previous grade only an SNN (with trainable parameters) composed with the SNNs (with fixed parameters) trained in the preceding grades as features.\" Section 2: \"We define recursively g_1(Θ_1; x) := N_{D_1}(Θ_1; x), g_{l+1}(Θ_{l+1}; x) := N_{D_{l+1}}(Θ_{l+1}; ·) ◦ H_{D_l-1}(Θ^*_l; ·) ◦ . . . ◦ H_{D_1-1}(Θ^*_1; ·)\"", "section": "Abstract; Section 2"}
{"claim": "The forward propagation computational cost therefore grows roughly quadratically with the number of grades.", "claim_type": "quantitative", "paper_id": "IoRT7EhFap", "paper_title": "Addressing Spectral Bias of Deep Neural Networks by Multi-Grade Deep Learning", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "3b0MU64PcM", "reviewer": "Reviewer_otLk", "review_text": "Comment: For the weakness: Our concern, \"There are concerns about the performance and stability of MGDL, as evidenced by loss spikes and lack of significant loss reduction in real-world tasks (Figure 8),\" was not clearly addressed, nor were additional numerical experiments provided to alleviate our doubts. Additionally, there is inconsistency in the use of commas in the numerical values in Table 1, such as \"32,401\" and \"27, 817.\"\n\nFor question2: Can I understand it this way: When we divide the network into multiple grades, the cost of forward propagation increases during training. For example, when training the n-th grade of the network, the forward propagation process requires additional computations from the previous n-1 grades, resulting in the computational cost of forward propagation growing quadratically.", "labeling_timestamp": "2026-01-11T17:26:33.341346", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper text describes the MGDL architecture and how grades compose SNNs, but it does not state or quantify how forward propagation computational cost scales with the number of grades (no claim of 'roughly quadratic' growth appears in the provided sections).", "evidence": ["Abstract: \"a current grade learning from the residue of the previous grade only an SNN (with trainable parameters) composed with the SNNs (with fixed parameters) trained in the preceding grades as features.\"", "Section 2: \"we split a DNN with depth D into L grades, with L < D, each of which learns an SNN N_{D_l}, defined as (1), with depth D_l, from the residue {e_l^ℓ}_{ℓ=1}^N of the previous grade, where 1 < D_l < D and ∑_{l=1}^L D_l = D + L - 1.\""], "section": "Abstract; 2 Proposed Approach and Multi-Grade Learning Model"}
{"claim": "The paper does not report FLOPs and runtime measurements for 2K and 4K resolutions and should include them in the final version.", "claim_type": "experimental", "paper_id": "4s5UsBUsUS", "paper_title": "VFIMamba: Video Frame Interpolation with State Space Models", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "JJvrOo4QDl", "reviewer": "Reviewer_9Hn5", "review_text": "Comment: I thank the authors for the answers, and highly appreciate the extensive additional results and verification of the numbers. Minor detail, it would be nice if in the final version the FLOPs and runtime will also be added for 2K/4K. The additional provided details on training and additional ablations provided in the rebuttal pdf confirm the contribution of the paper and I still recommend the paper for acceptance.", "labeling_timestamp": "2026-01-11T17:26:41.829367", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper reports PSNR improvements for 2K and 4K on X-TEST but contains no FLOPs counts or runtime/latency measurements for 2K or 4K in the provided content, nor any table or section reporting such benchmarks.", "evidence": "Abstract: \"In particular, on the X-TEST dataset, VFIMamba demonstrates a noteworthy improvement of 0.80 dB for 4K frames and 0.96 dB for 2K frames.\"  Introduction: \"...model speed is also of paramount importance.\"", "section": "Abstract; 1 Introduction"}
{"claim": "The paper's overall novelty is very limited because it largely reuses existing ideas from graph neural networks and prior graph transformer work.", "claim_type": "novelty", "paper_id": "ffNrpcBpi6", "paper_title": "Graph Convolutions Enrich the Self-Attention in Transformers!", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BnHhKrAJht", "reviewer": "Reviewer_bzWT", "review_text": "Summary: To overcome the oversmoothing effects in general transformer settings, the author considers to treat the attention module as the graph filter and propose to use a polynomial graph filter (graph signal processing) techniques to alleviate oversmoothing. Specifically,  the author considers the induced attention matrix from features as the adjacency matrix and construct a filter that allows exhibiting high-pass and low-pass filter. As low-pass filters provides smoothing tendency and high-pass filter provides edge tendency, the author leverages an adaptive filter to alleviate oversmoothing effects on Transformer domain.\n\nStrengths: 1. The idea is clear and well understood, the author leverages the idea in GNN fields to the transformer models.\n2. The high-pass and low-pass filter is analyzed and the author used an approximation to approximate the polynomial filter.\n3. The experiments covers many different applications of transformers.\n\nWeaknesses: 1. The novelty is very limited. The idea of building adaptive high-pass, low-pass filter is well studied in GNN field and has been leveraged in many works to solve oversmoothing in GNN. The idea of attention module as graph filter is also proposed in many Graph transformer models. The main contribution of this paper is simply change the direction from using attention as graph filters to use graph filters as attentions, which is not very interesting.\n\nQuestions: In the Theorem F.1, the description is not very clear. I assume  \"the sum w0 +w1σi +wkσi^{k}  will be smaller than g(σi)\" indicates that g(σi) will become smaller as i increase, but this depends on the scale of w1 and wk, even as σi^{k}->0 with i is large, if the rate wk/w1 is large, such tendency can only be guaranteed asymptotically, in the limited i case, this doesn't always hold true. Please correct me if I am wrong.", "labeling_timestamp": "2026-01-11T17:27:22.719132", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly builds on prior GSP/GNN ideas (it interprets self-attention as a graph filter and positions its filter as an extension of existing matrix-polynomial filters), which supports the reviewer's point that existing ideas are reused. However, the authors also propose a concrete new GFSA design (identity + two polynomial terms with a Taylor approximation), provide theoretical analysis (Theorem 3.1), and emphasize distinctiveness and broad empirical evaluation across multiple domains, which contradicts the claim that novelty is 'very limited.' Thus the reviewer's statement is only partially accurate.", "evidence": ["In general, a graph filter on a graph G is represented by a polynomial expression based on its adjacency or Laplacian matrix - in this regard, the existing self-attention mechanism can be understood as the simplest graph filter with ¯A only, where ¯A ∈ [0,1]^{n×n} means a learned attention matrix and n is the number of input tokens.", "In comparison with them, our proposed graph filter is distinctive in the following aspects: i) our proposed filter is more effective and shows better performance with comparable computational overheads, ii) our proposed filter is well-aligned with recent advancements in the GCN community - in other words, some graph filters used by recent advanced GCN methods are special cases of our proposed graph filter, which is not the case for prior works, and iii) other methods were typically studied for certain domains only whereas we test our method in 6 domains - for instance, DiversePatch [25] works only for Vision Transformers (ViTs).", "We propose a graph filter-based self-attention (GFSA) mechanism, integrating an identity term and two polynomial terms for general yet effective than the simple self-attention mechanism (Section 3)."], "section": "Introduction; 2.2 Self-Attention and Graph Convolutional Filter; 3 Graph Filter-based Self-Attention Layers"}
{"claim": "Adaptive high-pass and low-pass filters have already been well studied in the graph neural network literature and used to address oversmoothing.", "claim_type": "novelty", "paper_id": "ffNrpcBpi6", "paper_title": "Graph Convolutions Enrich the Self-Attention in Transformers!", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BnHhKrAJht", "reviewer": "Reviewer_bzWT", "review_text": "Summary: To overcome the oversmoothing effects in general transformer settings, the author considers to treat the attention module as the graph filter and propose to use a polynomial graph filter (graph signal processing) techniques to alleviate oversmoothing. Specifically,  the author considers the induced attention matrix from features as the adjacency matrix and construct a filter that allows exhibiting high-pass and low-pass filter. As low-pass filters provides smoothing tendency and high-pass filter provides edge tendency, the author leverages an adaptive filter to alleviate oversmoothing effects on Transformer domain.\n\nStrengths: 1. The idea is clear and well understood, the author leverages the idea in GNN fields to the transformer models.\n2. The high-pass and low-pass filter is analyzed and the author used an approximation to approximate the polynomial filter.\n3. The experiments covers many different applications of transformers.\n\nWeaknesses: 1. The novelty is very limited. The idea of building adaptive high-pass, low-pass filter is well studied in GNN field and has been leveraged in many works to solve oversmoothing in GNN. The idea of attention module as graph filter is also proposed in many Graph transformer models. The main contribution of this paper is simply change the direction from using attention as graph filters to use graph filters as attentions, which is not very interesting.\n\nQuestions: In the Theorem F.1, the description is not very clear. I assume  \"the sum w0 +w1σi +wkσi^{k}  will be smaller than g(σi)\" indicates that g(σi) will become smaller as i increase, but this depends on the scale of w1 and wk, even as σi^{k}->0 with i is large, if the rate wk/w1 is large, such tendency can only be guaranteed asymptotically, in the limited i case, this doesn't always hold true. Please correct me if I am wrong.", "labeling_timestamp": "2026-01-11T17:26:58.539523", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper cites prior work that uses high-frequency information and discusses graph filters in GCNs and oversmoothing (supporting that related filters have been studied). However, it does not explicitly state that adaptive high-pass and low-pass filters have been \"well studied\" in the GNN literature nor that such adaptive filters are already widely used specifically to address oversmoothing; it only notes some prior studies and that some GCN filters are special cases of their proposal.", "evidence": "1) \"There exist a couple of prior works to enrich the self-attention mechanism with high frequency information [80, 4].\" 2) \"our proposed filter is well-aligned with recent advancements in the GCN community - in other words, some graph filters used by recent advanced GCN methods are special cases of our proposed graph filter...\" 3) \"There are studies on utilizing high frequency information via frequency domain analyses [80, 4], but they are not designed on top of graph filtering perspectives.\"", "section": "Introduction; Section 2.3"}
{"claim": "Treating the attention module as a graph filter has already been proposed in many prior Graph Transformer models.", "claim_type": "novelty", "paper_id": "ffNrpcBpi6", "paper_title": "Graph Convolutions Enrich the Self-Attention in Transformers!", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BnHhKrAJht", "reviewer": "Reviewer_bzWT", "review_text": "Summary: To overcome the oversmoothing effects in general transformer settings, the author considers to treat the attention module as the graph filter and propose to use a polynomial graph filter (graph signal processing) techniques to alleviate oversmoothing. Specifically,  the author considers the induced attention matrix from features as the adjacency matrix and construct a filter that allows exhibiting high-pass and low-pass filter. As low-pass filters provides smoothing tendency and high-pass filter provides edge tendency, the author leverages an adaptive filter to alleviate oversmoothing effects on Transformer domain.\n\nStrengths: 1. The idea is clear and well understood, the author leverages the idea in GNN fields to the transformer models.\n2. The high-pass and low-pass filter is analyzed and the author used an approximation to approximate the polynomial filter.\n3. The experiments covers many different applications of transformers.\n\nWeaknesses: 1. The novelty is very limited. The idea of building adaptive high-pass, low-pass filter is well studied in GNN field and has been leveraged in many works to solve oversmoothing in GNN. The idea of attention module as graph filter is also proposed in many Graph transformer models. The main contribution of this paper is simply change the direction from using attention as graph filters to use graph filters as attentions, which is not very interesting.\n\nQuestions: In the Theorem F.1, the description is not very clear. I assume  \"the sum w0 +w1σi +wkσi^{k}  will be smaller than g(σi)\" indicates that g(σi) will become smaller as i increase, but this depends on the scale of w1 and wk, even as σi^{k}->0 with i is large, if the rate wk/w1 is large, such tendency can only be guaranteed asymptotically, in the limited i case, this doesn't always hold true. Please correct me if I am wrong.", "labeling_timestamp": "2026-01-11T17:27:05.917683", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper acknowledges prior work linking self-attention to graph adjacency/graph convolution (citing Shi et al. [71] and others) and notes prior methods to enrich attention with high-frequency information, but the authors still frame their specific graph-filter formulation and its novelty as a contribution. Thus the claim that this perspective has been proposed before is supported in part, but the paper also claims novelty in their particular graph-filter design.", "evidence": "\"Shi et al. [71] revealed an analogy between the self-attention and the residual graph convolutional network (GCN)...\"; \"The self-attention matrix used in Transformers has the form of symmetrically normalized adjacency matrix where each token become a node [71, 28] ... In other words, ¯A = D^{-1}A\"; \"There exist a couple of prior works to enrich the self-attention mechanism with high frequency information [80, 4].\"", "section": "2.2 Self-Attention and Graph Convolutional Filter (also Introduction)"}
{"claim": "The paper's main contribution appears to be reversing the direction—using graph filters as attentions instead of attentions as graph filters—which is not an interesting or substantial advance.", "claim_type": "novelty", "paper_id": "ffNrpcBpi6", "paper_title": "Graph Convolutions Enrich the Self-Attention in Transformers!", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BnHhKrAJht", "reviewer": "Reviewer_bzWT", "review_text": "Summary: To overcome the oversmoothing effects in general transformer settings, the author considers to treat the attention module as the graph filter and propose to use a polynomial graph filter (graph signal processing) techniques to alleviate oversmoothing. Specifically,  the author considers the induced attention matrix from features as the adjacency matrix and construct a filter that allows exhibiting high-pass and low-pass filter. As low-pass filters provides smoothing tendency and high-pass filter provides edge tendency, the author leverages an adaptive filter to alleviate oversmoothing effects on Transformer domain.\n\nStrengths: 1. The idea is clear and well understood, the author leverages the idea in GNN fields to the transformer models.\n2. The high-pass and low-pass filter is analyzed and the author used an approximation to approximate the polynomial filter.\n3. The experiments covers many different applications of transformers.\n\nWeaknesses: 1. The novelty is very limited. The idea of building adaptive high-pass, low-pass filter is well studied in GNN field and has been leveraged in many works to solve oversmoothing in GNN. The idea of attention module as graph filter is also proposed in many Graph transformer models. The main contribution of this paper is simply change the direction from using attention as graph filters to use graph filters as attentions, which is not very interesting.\n\nQuestions: In the Theorem F.1, the description is not very clear. I assume  \"the sum w0 +w1σi +wkσi^{k}  will be smaller than g(σi)\" indicates that g(σi) will become smaller as i increase, but this depends on the scale of w1 and wk, even as σi^{k}->0 with i is large, if the rate wk/w1 is large, such tendency can only be guaranteed asymptotically, in the limited i case, this doesn't always hold true. Please correct me if I am wrong.", "labeling_timestamp": "2026-01-11T17:27:10.490027", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper's primary contribution is indeed proposing a graph-filter-based self-attention (GFSA) that builds on viewing self-attention through a graph-signal-processing lens — i.e., it designs a graph filter to serve as the attention operator. However, the reviewer wording that this is a 'reversing the direction' (as if the paper switches from treating attentions as graph filters to treating graph filters as attentions) is a mischaracterization: the paper explicitly interprets self-attention as a simple graph filter and then generalizes that filter. The reviewer’s claim that this is 'not an interesting or substantial advance' is subjective and not something the paper can confirm or refute objectively.", "evidence": "Abstract: \"We interpret the original self-attention as a simple graph filter and redesign it from a graph signal processing (GSP) perspective. We propose a graph-filter-based self-attention (GFSA) ...\"; Section 3: \"one can consider that our proposed graph filter is the very next complicated filter after the one used by the original self-attention mechanism.\"", "section": "Abstract; Section 3"}
{"claim": "Theorem F.1's statement that w0 + w1σi + ... < g(σi) implies g(σi) decreases with i depends on the relative scales of weights w1 and wk, so it is not generally guaranteed.", "claim_type": "methodology", "paper_id": "ffNrpcBpi6", "paper_title": "Graph Convolutions Enrich the Self-Attention in Transformers!", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BnHhKrAJht", "reviewer": "Reviewer_bzWT", "review_text": "Summary: To overcome the oversmoothing effects in general transformer settings, the author considers to treat the attention module as the graph filter and propose to use a polynomial graph filter (graph signal processing) techniques to alleviate oversmoothing. Specifically,  the author considers the induced attention matrix from features as the adjacency matrix and construct a filter that allows exhibiting high-pass and low-pass filter. As low-pass filters provides smoothing tendency and high-pass filter provides edge tendency, the author leverages an adaptive filter to alleviate oversmoothing effects on Transformer domain.\n\nStrengths: 1. The idea is clear and well understood, the author leverages the idea in GNN fields to the transformer models.\n2. The high-pass and low-pass filter is analyzed and the author used an approximation to approximate the polynomial filter.\n3. The experiments covers many different applications of transformers.\n\nWeaknesses: 1. The novelty is very limited. The idea of building adaptive high-pass, low-pass filter is well studied in GNN field and has been leveraged in many works to solve oversmoothing in GNN. The idea of attention module as graph filter is also proposed in many Graph transformer models. The main contribution of this paper is simply change the direction from using attention as graph filters to use graph filters as attentions, which is not very interesting.\n\nQuestions: In the Theorem F.1, the description is not very clear. I assume  \"the sum w0 +w1σi +wkσi^{k}  will be smaller than g(σi)\" indicates that g(σi) will become smaller as i increase, but this depends on the scale of w1 and wk, even as σi^{k}->0 with i is large, if the rate wk/w1 is large, such tendency can only be guaranteed asymptotically, in the limited i case, this doesn't always hold true. Please correct me if I am wrong.", "labeling_timestamp": "2026-01-11T17:27:13.507977", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not include Theorem F.1 or the specific inequality and monotonicity claim referenced by the reviewer, so there is insufficient information to verify whether the theorem's implication depends on relative weight scales. The excerpt includes Theorem 3.1 (about w0, w1, wK) and a pointer to Appendices E and F, but Appendix F (and Theorem F.1) text is not present here.", "evidence": "1) \"Theorem 3.1 (Filter characteristics based on coefficient values). ... Consider the polynomial graph filter defined by ∑_K_{k=0} w_k ¯A^k , where w_2, w_3, ..., w_{K-1} = 0 and only w_0, w_1, and w_K are non-zero. If the coefficients w_k for k = 0, 1, K are positive and their sum is 1, then the polynomial filter acts as a low-pass filter, attenuating high-frequency components and promoting smoothness across the graph.\" 2) \"See more discussion in Appendices E and F.\"", "section": "Section 3"}
{"claim": "Even though σi^k tends to zero as i increases, a large ratio wk/w1 can prevent g(σi) from decreasing except asymptotically, so the finite-i behavior is not guaranteed.", "claim_type": "methodology", "paper_id": "ffNrpcBpi6", "paper_title": "Graph Convolutions Enrich the Self-Attention in Transformers!", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BnHhKrAJht", "reviewer": "Reviewer_bzWT", "review_text": "Summary: To overcome the oversmoothing effects in general transformer settings, the author considers to treat the attention module as the graph filter and propose to use a polynomial graph filter (graph signal processing) techniques to alleviate oversmoothing. Specifically,  the author considers the induced attention matrix from features as the adjacency matrix and construct a filter that allows exhibiting high-pass and low-pass filter. As low-pass filters provides smoothing tendency and high-pass filter provides edge tendency, the author leverages an adaptive filter to alleviate oversmoothing effects on Transformer domain.\n\nStrengths: 1. The idea is clear and well understood, the author leverages the idea in GNN fields to the transformer models.\n2. The high-pass and low-pass filter is analyzed and the author used an approximation to approximate the polynomial filter.\n3. The experiments covers many different applications of transformers.\n\nWeaknesses: 1. The novelty is very limited. The idea of building adaptive high-pass, low-pass filter is well studied in GNN field and has been leveraged in many works to solve oversmoothing in GNN. The idea of attention module as graph filter is also proposed in many Graph transformer models. The main contribution of this paper is simply change the direction from using attention as graph filters to use graph filters as attentions, which is not very interesting.\n\nQuestions: In the Theorem F.1, the description is not very clear. I assume  \"the sum w0 +w1σi +wkσi^{k}  will be smaller than g(σi)\" indicates that g(σi) will become smaller as i increase, but this depends on the scale of w1 and wk, even as σi^{k}->0 with i is large, if the rate wk/w1 is large, such tendency can only be guaranteed asymptotically, in the limited i case, this doesn't always hold true. Please correct me if I am wrong.", "labeling_timestamp": "2026-01-11T17:27:40.197016", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper discusses singular-value-domain analysis of polynomial graph filters and states that the filter behavior depends on coefficient values (Section 2.2 and Section 3). However, it does not state or analyze the specific claim about σ_i^k tending to zero with i and how a large ratio w_k/w_1 can prevent g(σ_i) from decreasing for finite i. The provided text contains no explicit derivation or statement that confirms or contradicts that finite-i behavior assertion.", "evidence": "1) \"Being inspired by Zou et al. [106] and Maskey et al. [49], we rely on the singular value domain analysis to understand the low/high-pass characteristics of filters on directed graphs (cf. Fig. 2). See more discussion in Appendices E and F.\" 2) \"From the GSP perspective, using ¯A as the shift operator, a graph filter can be represented as a matrix polynomial filter... We aim to design this matrix polynomial filter using the two lowest-order terms and one high-order term in Eq. (3). The following theorem shows that, despite using the three terms, the filter can be either a low-pass filter or a high-pass filter, depending on the coefficient values.\" 3) \"Consider the polynomial graph filter defined by ∑_{k=0}^K w_k ¯A^k, where w_2, w_3, ..., w_{K-1} = 0 and only w_0, w_1, and w_K are non-zero.\"", "section": "Section 2.2 and Section 3"}
{"claim": "Theorem F.1 requires correction or clearer justification regarding how the weight scales affect the claimed monotonic decrease of g(σi) with increasing i.", "claim_type": "methodology", "paper_id": "ffNrpcBpi6", "paper_title": "Graph Convolutions Enrich the Self-Attention in Transformers!", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "BnHhKrAJht", "reviewer": "Reviewer_bzWT", "review_text": "Summary: To overcome the oversmoothing effects in general transformer settings, the author considers to treat the attention module as the graph filter and propose to use a polynomial graph filter (graph signal processing) techniques to alleviate oversmoothing. Specifically,  the author considers the induced attention matrix from features as the adjacency matrix and construct a filter that allows exhibiting high-pass and low-pass filter. As low-pass filters provides smoothing tendency and high-pass filter provides edge tendency, the author leverages an adaptive filter to alleviate oversmoothing effects on Transformer domain.\n\nStrengths: 1. The idea is clear and well understood, the author leverages the idea in GNN fields to the transformer models.\n2. The high-pass and low-pass filter is analyzed and the author used an approximation to approximate the polynomial filter.\n3. The experiments covers many different applications of transformers.\n\nWeaknesses: 1. The novelty is very limited. The idea of building adaptive high-pass, low-pass filter is well studied in GNN field and has been leveraged in many works to solve oversmoothing in GNN. The idea of attention module as graph filter is also proposed in many Graph transformer models. The main contribution of this paper is simply change the direction from using attention as graph filters to use graph filters as attentions, which is not very interesting.\n\nQuestions: In the Theorem F.1, the description is not very clear. I assume  \"the sum w0 +w1σi +wkσi^{k}  will be smaller than g(σi)\" indicates that g(σi) will become smaller as i increase, but this depends on the scale of w1 and wk, even as σi^{k}->0 with i is large, if the rate wk/w1 is large, such tendency can only be guaranteed asymptotically, in the limited i case, this doesn't always hold true. Please correct me if I am wrong.", "labeling_timestamp": "2026-01-11T17:27:39.383083", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not include Appendix F or Theorem F.1, so there is insufficient information to evaluate whether Theorem F.1 needs correction or clearer justification about how weight scales affect the claimed monotonic decrease of g(σi). The main text only references Appendices E and F but does not present the theorem or its proof.", "evidence": "\"See more discussion in Appendices E and F.\"", "section": "2.2 Self-Attention and Graph Convolutional Filter"}
{"claim": "Encoder-based customized text-to-image generation is not novel given prior literature.", "claim_type": "novelty", "paper_id": "VzPGV19Bnp", "paper_title": "Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "DR2C8HbrUg", "reviewer": "Reviewer_qTeR", "review_text": "Summary: The authors observe that the commonly used regularization to avoid overfitting in customized text-to-image generative models may lead to the loss of detailed information on the customized objects. The authors propose to balance the influences of the prompt condition and the customization condition S* instead of applying regularization to avoid overfitting, ensuring the preservation of customized details and flexibility to work with diverse prompts.\n\nStrengths: - Motivated by interesting observations\n- Novel idea of removing regularization to preserve detailed information\n- Computational resource-efficient approach\n\nWeaknesses: - Encoder-based customized Text-to-Image generation is not new and missing comparisons with important previous works: [1][2][3][4]\n- The important term \"independent conditions\" is not clearly defined.\n- Figure 9 shows that before fine-tuning, fusion sampling even leads to worse identity-preserving performance than baseline sampling, implying that performance gain in identity similarity mainly comes from fine-tuning.\n- In Figure 4, the proposed method does not present visually better results compared to E4T.\n\n\n[1] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. ELITE: Encoding visual concepts into textual embeddings for customized text-to-image generation. arXiv preprint arXiv:2302.13848, 2023.\n[2] Chen, Wenhu and Hu, Hexiang and Li, Yandong and Ruiz, Nataniel and Jia, Xuhui and Chang, Ming-Wei and Cohen, William W. Subject-driven Text-to-Image Generation via Apprenticeship Learning. NeurIPS 2023.\n[3] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or. Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models. arXiv preprint arXiv:2302.12228 (2023). \n[4] Xuhui Jia, Yang Zhao, Kelvin C.K. Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, Yu-Chuan Su. Taming Encoder for Zero Fine-tuning Image Customization with Text-to-Image Diffusion Models. arXiv preprint arXiv:2304.02642, 2023.\n\nQuestions: - The authors aim at \"detail preservation\" but do not clearly explain what information these details include. What is the difference between detail preservation and identity preservation?\n- Minor issue of presentation. Figures 6 and 7 on page 8 are squeezed and Figure 7 has slightly occluded the caption of Figure 6. May rearrange for better visualization.", "labeling_timestamp": "2026-01-11T17:27:37.069280", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly cites prior work that uses an encoder to map a reference image to a word embedding (e.g., E4T and related works), indicating that encoder-based customization is already present in the literature; the paper's claimed novelty lies in the fusion sampling approach and avoiding regularization rather than the mere use of an encoder.", "evidence": "Introduction: \"Gal et al. (2022; 2023); Wei et al. (2023) propose to encode the novel concept of user input image in a word embedding, which is obtained by an optimization method or from an encoder network.\"; Section 2: \"Some existing works (Gal et al., 2022; 2023) also try to obtain S* for customized generation. However, regularization is often applied in these works. For instance, E4T (Gal et al., 2023) proposes to use an encoder to generate S*, which is optimized with ... where the L2 norm of S* is regularized.\"", "section": "Introduction; Section 2 (Methodology)"}
{"claim": "The paper does not compare its method against important prior works ELITE, Subject-driven Apprenticeship Learning, Encoder-based Domain Tuning, and Taming Encoder.", "claim_type": "baseline", "paper_id": "VzPGV19Bnp", "paper_title": "Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "DR2C8HbrUg", "reviewer": "Reviewer_qTeR", "review_text": "Summary: The authors observe that the commonly used regularization to avoid overfitting in customized text-to-image generative models may lead to the loss of detailed information on the customized objects. The authors propose to balance the influences of the prompt condition and the customization condition S* instead of applying regularization to avoid overfitting, ensuring the preservation of customized details and flexibility to work with diverse prompts.\n\nStrengths: - Motivated by interesting observations\n- Novel idea of removing regularization to preserve detailed information\n- Computational resource-efficient approach\n\nWeaknesses: - Encoder-based customized Text-to-Image generation is not new and missing comparisons with important previous works: [1][2][3][4]\n- The important term \"independent conditions\" is not clearly defined.\n- Figure 9 shows that before fine-tuning, fusion sampling even leads to worse identity-preserving performance than baseline sampling, implying that performance gain in identity similarity mainly comes from fine-tuning.\n- In Figure 4, the proposed method does not present visually better results compared to E4T.\n\n\n[1] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. ELITE: Encoding visual concepts into textual embeddings for customized text-to-image generation. arXiv preprint arXiv:2302.13848, 2023.\n[2] Chen, Wenhu and Hu, Hexiang and Li, Yandong and Ruiz, Nataniel and Jia, Xuhui and Chang, Ming-Wei and Cohen, William W. Subject-driven Text-to-Image Generation via Apprenticeship Learning. NeurIPS 2023.\n[3] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or. Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models. arXiv preprint arXiv:2302.12228 (2023). \n[4] Xuhui Jia, Yang Zhao, Kelvin C.K. Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, Yu-Chuan Su. Taming Encoder for Zero Fine-tuning Image Customization with Text-to-Image Diffusion Models. arXiv preprint arXiv:2304.02642, 2023.\n\nQuestions: - The authors aim at \"detail preservation\" but do not clearly explain what information these details include. What is the difference between detail preservation and identity preservation?\n- Minor issue of presentation. Figures 6 and 7 on page 8 are squeezed and Figure 7 has slightly occluded the caption of Figure 6. May rearrange for better visualization.", "labeling_timestamp": "2026-01-11T17:27:37.178343", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper's experimental comparisons list only Stable Diffusion 2, Textual Inversion, DreamBooth, and E4T; it does not mention or compare against ELITE, Subject-driven Apprenticeship Learning, Encoder-based Domain Tuning, or Taming Encoder.", "evidence": "We conduct extensive experiments to compare the proposed framework with several baseline methods including Stable Diffusion 2 (Rombach et al., 2021), Textual Inversion (Gal et al., 2022), DreamBooth (Ruiz et al., 2022) and E4T (Gal et al., 2023).", "section": "3 EXPERIMENTS"}
{"claim": "The term 'independent conditions' is not clearly defined in the paper.", "claim_type": "presentation", "paper_id": "VzPGV19Bnp", "paper_title": "Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "DR2C8HbrUg", "reviewer": "Reviewer_qTeR", "review_text": "Summary: The authors observe that the commonly used regularization to avoid overfitting in customized text-to-image generative models may lead to the loss of detailed information on the customized objects. The authors propose to balance the influences of the prompt condition and the customization condition S* instead of applying regularization to avoid overfitting, ensuring the preservation of customized details and flexibility to work with diverse prompts.\n\nStrengths: - Motivated by interesting observations\n- Novel idea of removing regularization to preserve detailed information\n- Computational resource-efficient approach\n\nWeaknesses: - Encoder-based customized Text-to-Image generation is not new and missing comparisons with important previous works: [1][2][3][4]\n- The important term \"independent conditions\" is not clearly defined.\n- Figure 9 shows that before fine-tuning, fusion sampling even leads to worse identity-preserving performance than baseline sampling, implying that performance gain in identity similarity mainly comes from fine-tuning.\n- In Figure 4, the proposed method does not present visually better results compared to E4T.\n\n\n[1] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. ELITE: Encoding visual concepts into textual embeddings for customized text-to-image generation. arXiv preprint arXiv:2302.13848, 2023.\n[2] Chen, Wenhu and Hu, Hexiang and Li, Yandong and Ruiz, Nataniel and Jia, Xuhui and Chang, Ming-Wei and Cohen, William W. Subject-driven Text-to-Image Generation via Apprenticeship Learning. NeurIPS 2023.\n[3] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or. Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models. arXiv preprint arXiv:2302.12228 (2023). \n[4] Xuhui Jia, Yang Zhao, Kelvin C.K. Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, Yu-Chuan Su. Taming Encoder for Zero Fine-tuning Image Customization with Text-to-Image Diffusion Models. arXiv preprint arXiv:2304.02642, 2023.\n\nQuestions: - The authors aim at \"detail preservation\" but do not clearly explain what information these details include. What is the difference between detail preservation and identity preservation?\n- Minor issue of presentation. Figures 6 and 7 on page 8 are squeezed and Figure 7 has slightly occluded the caption of Figure 6. May rearrange for better visualization.", "labeling_timestamp": "2026-01-11T17:27:34.879604", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly introduces and uses the assumption that the conditions are independent: it states 'We begin by assuming that S* and C are independent' and proceeds to derive formulas under that assumption in Section 2.1. Thus the term (as used in the paper) is defined as the probabilistic independence of S* and C.", "evidence": "“Sampling with independent conditions We begin by assuming that S ∗ and C are independent.”\n\n“Since we assume that S ∗ , C are independent, we can further rewrite the above as”", "section": "2.1 FUSION SAMPLING"}
{"claim": "Figure 9 indicates fusion sampling before fine-tuning yields worse identity-preserving performance than baseline sampling.", "claim_type": "baseline", "paper_id": "VzPGV19Bnp", "paper_title": "Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "DR2C8HbrUg", "reviewer": "Reviewer_qTeR", "review_text": "Summary: The authors observe that the commonly used regularization to avoid overfitting in customized text-to-image generative models may lead to the loss of detailed information on the customized objects. The authors propose to balance the influences of the prompt condition and the customization condition S* instead of applying regularization to avoid overfitting, ensuring the preservation of customized details and flexibility to work with diverse prompts.\n\nStrengths: - Motivated by interesting observations\n- Novel idea of removing regularization to preserve detailed information\n- Computational resource-efficient approach\n\nWeaknesses: - Encoder-based customized Text-to-Image generation is not new and missing comparisons with important previous works: [1][2][3][4]\n- The important term \"independent conditions\" is not clearly defined.\n- Figure 9 shows that before fine-tuning, fusion sampling even leads to worse identity-preserving performance than baseline sampling, implying that performance gain in identity similarity mainly comes from fine-tuning.\n- In Figure 4, the proposed method does not present visually better results compared to E4T.\n\n\n[1] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. ELITE: Encoding visual concepts into textual embeddings for customized text-to-image generation. arXiv preprint arXiv:2302.13848, 2023.\n[2] Chen, Wenhu and Hu, Hexiang and Li, Yandong and Ruiz, Nataniel and Jia, Xuhui and Chang, Ming-Wei and Cohen, William W. Subject-driven Text-to-Image Generation via Apprenticeship Learning. NeurIPS 2023.\n[3] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or. Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models. arXiv preprint arXiv:2302.12228 (2023). \n[4] Xuhui Jia, Yang Zhao, Kelvin C.K. Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, Yu-Chuan Su. Taming Encoder for Zero Fine-tuning Image Customization with Text-to-Image Diffusion Models. arXiv preprint arXiv:2304.02642, 2023.\n\nQuestions: - The authors aim at \"detail preservation\" but do not clearly explain what information these details include. What is the difference between detail preservation and identity preservation?\n- Minor issue of presentation. Figures 6 and 7 on page 8 are squeezed and Figure 7 has slightly occluded the caption of Figure 6. May rearrange for better visualization.", "labeling_timestamp": "2026-01-11T17:28:03.719485", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not include Figure 9 nor any results or text explicitly reporting that 'fusion sampling before fine-tuning yields worse identity-preserving performance than baseline sampling.' The paper describes Fusion Sampling and mentions experiments, but no figure 9 or matching comparison is present in the given content, so the claim cannot be verified.", "evidence": "1) \"Figure 3: The performance of customized generation is impacted by the level of regularization.\" 2) \"We conduct extensive experiments to compare the proposed framework with several baseline methods including Stable Diffusion 2 (Rombach et al., 2021), Textual Inversion (Gal et al., 2022), DreamBooth (Ruiz et al., 2022) and E4T (Gal et al., 2023).\" 3) \"To solve this problem, we propose Fusion Sampling, which consists of two stages at each timestep t: a fusion stage which encodes information from both S* and C into x_t with an updated x_t, and a refinement stage which predicts x_{t-1} based on Equation 6.\"", "section": "Figures/Experiments; 2.1 FUSION SAMPLING; 3 EXPERIMENTS"}
{"claim": "The observed identity similarity performance gain appears to primarily result from fine-tuning rather than the fusion sampling method.", "claim_type": "experimental", "paper_id": "VzPGV19Bnp", "paper_title": "Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "DR2C8HbrUg", "reviewer": "Reviewer_qTeR", "review_text": "Summary: The authors observe that the commonly used regularization to avoid overfitting in customized text-to-image generative models may lead to the loss of detailed information on the customized objects. The authors propose to balance the influences of the prompt condition and the customization condition S* instead of applying regularization to avoid overfitting, ensuring the preservation of customized details and flexibility to work with diverse prompts.\n\nStrengths: - Motivated by interesting observations\n- Novel idea of removing regularization to preserve detailed information\n- Computational resource-efficient approach\n\nWeaknesses: - Encoder-based customized Text-to-Image generation is not new and missing comparisons with important previous works: [1][2][3][4]\n- The important term \"independent conditions\" is not clearly defined.\n- Figure 9 shows that before fine-tuning, fusion sampling even leads to worse identity-preserving performance than baseline sampling, implying that performance gain in identity similarity mainly comes from fine-tuning.\n- In Figure 4, the proposed method does not present visually better results compared to E4T.\n\n\n[1] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. ELITE: Encoding visual concepts into textual embeddings for customized text-to-image generation. arXiv preprint arXiv:2302.13848, 2023.\n[2] Chen, Wenhu and Hu, Hexiang and Li, Yandong and Ruiz, Nataniel and Jia, Xuhui and Chang, Ming-Wei and Cohen, William W. Subject-driven Text-to-Image Generation via Apprenticeship Learning. NeurIPS 2023.\n[3] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or. Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models. arXiv preprint arXiv:2302.12228 (2023). \n[4] Xuhui Jia, Yang Zhao, Kelvin C.K. Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, Yu-Chuan Su. Taming Encoder for Zero Fine-tuning Image Customization with Text-to-Image Diffusion Models. arXiv preprint arXiv:2304.02642, 2023.\n\nQuestions: - The authors aim at \"detail preservation\" but do not clearly explain what information these details include. What is the difference between detail preservation and identity preservation?\n- Minor issue of presentation. Figures 6 and 7 on page 8 are squeezed and Figure 7 has slightly occluded the caption of Figure 6. May rearrange for better visualization.", "labeling_timestamp": "2026-01-11T17:28:11.540837", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper attributes improved detail preservation to the combination of a PromptNet (trained without regularization) and the proposed Fusion Sampling, and it argues Fusion Sampling is necessary to use unregularized embeddings. However, the provided content does not include experimental results or ablations that isolate the effect of fine-tuning versus the effect of Fusion Sampling on identity-similarity metrics, so it is not possible to determine from the paper excerpt whether the observed identity similarity gain is primarily due to fine-tuning rather than Fusion Sampling.", "evidence": "1) \"Different from previous methods, our ProFusion propose to tackle the potential over-fitting problem by the proposed Fusion Sampling method at inference instead of using regularization.\" (Abstract / Introduction)\n\n2) \"Given a PromptNet trained without regularization which can map input image ¯x0 into word embedding S*, our goal is to successfully perform customized generation which preserves details of ¯x0, and meets the requirements specified in arbitrary prompt containing S*.\" (Section 2.1 FUSION SAMPLING)\n\n3) \"Sampling with independent conditions can be regarded as a special case of Fusion Sampling with m = 0. We set m = 1 in practice because it works well enough.\" (Section 2.1 FUSION SAMPLING)", "section": "Abstract; Section 2.1 FUSION SAMPLING"}
{"claim": "In Figure 4, the proposed method does not present visually better results compared to E4T.", "claim_type": "experimental", "paper_id": "VzPGV19Bnp", "paper_title": "Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "DR2C8HbrUg", "reviewer": "Reviewer_qTeR", "review_text": "Summary: The authors observe that the commonly used regularization to avoid overfitting in customized text-to-image generative models may lead to the loss of detailed information on the customized objects. The authors propose to balance the influences of the prompt condition and the customization condition S* instead of applying regularization to avoid overfitting, ensuring the preservation of customized details and flexibility to work with diverse prompts.\n\nStrengths: - Motivated by interesting observations\n- Novel idea of removing regularization to preserve detailed information\n- Computational resource-efficient approach\n\nWeaknesses: - Encoder-based customized Text-to-Image generation is not new and missing comparisons with important previous works: [1][2][3][4]\n- The important term \"independent conditions\" is not clearly defined.\n- Figure 9 shows that before fine-tuning, fusion sampling even leads to worse identity-preserving performance than baseline sampling, implying that performance gain in identity similarity mainly comes from fine-tuning.\n- In Figure 4, the proposed method does not present visually better results compared to E4T.\n\n\n[1] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. ELITE: Encoding visual concepts into textual embeddings for customized text-to-image generation. arXiv preprint arXiv:2302.13848, 2023.\n[2] Chen, Wenhu and Hu, Hexiang and Li, Yandong and Ruiz, Nataniel and Jia, Xuhui and Chang, Ming-Wei and Cohen, William W. Subject-driven Text-to-Image Generation via Apprenticeship Learning. NeurIPS 2023.\n[3] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or. Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models. arXiv preprint arXiv:2302.12228 (2023). \n[4] Xuhui Jia, Yang Zhao, Kelvin C.K. Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, Yu-Chuan Su. Taming Encoder for Zero Fine-tuning Image Customization with Text-to-Image Diffusion Models. arXiv preprint arXiv:2304.02642, 2023.\n\nQuestions: - The authors aim at \"detail preservation\" but do not clearly explain what information these details include. What is the difference between detail preservation and identity preservation?\n- Minor issue of presentation. Figures 6 and 7 on page 8 are squeezed and Figure 7 has slightly occluded the caption of Figure 6. May rearrange for better visualization.", "labeling_timestamp": "2026-01-11T17:28:04.736233", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does compare against E4T (so the comparison exists), but the excerpt does not include Figure 4 or any description/textual summary of what Figure 4 shows. Therefore we cannot verify whether in Figure 4 the proposed method is visually worse than E4T from the available content.", "evidence": "\"We conduct extensive experiments to compare the proposed framework with several baseline methods including Stable Diffusion 2 (Rombach et al., 2021), Textual Inversion (Gal et al., 2022), DreamBooth (Ruiz et al., 2022) and E4T (Gal et al., 2023).\"", "section": "3 EXPERIMENTS"}
{"claim": "The paper does not clearly explain which specific object details are meant by 'detail preservation'.", "claim_type": "presentation", "paper_id": "VzPGV19Bnp", "paper_title": "Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "DR2C8HbrUg", "reviewer": "Reviewer_qTeR", "review_text": "Summary: The authors observe that the commonly used regularization to avoid overfitting in customized text-to-image generative models may lead to the loss of detailed information on the customized objects. The authors propose to balance the influences of the prompt condition and the customization condition S* instead of applying regularization to avoid overfitting, ensuring the preservation of customized details and flexibility to work with diverse prompts.\n\nStrengths: - Motivated by interesting observations\n- Novel idea of removing regularization to preserve detailed information\n- Computational resource-efficient approach\n\nWeaknesses: - Encoder-based customized Text-to-Image generation is not new and missing comparisons with important previous works: [1][2][3][4]\n- The important term \"independent conditions\" is not clearly defined.\n- Figure 9 shows that before fine-tuning, fusion sampling even leads to worse identity-preserving performance than baseline sampling, implying that performance gain in identity similarity mainly comes from fine-tuning.\n- In Figure 4, the proposed method does not present visually better results compared to E4T.\n\n\n[1] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. ELITE: Encoding visual concepts into textual embeddings for customized text-to-image generation. arXiv preprint arXiv:2302.13848, 2023.\n[2] Chen, Wenhu and Hu, Hexiang and Li, Yandong and Ruiz, Nataniel and Jia, Xuhui and Chang, Ming-Wei and Cohen, William W. Subject-driven Text-to-Image Generation via Apprenticeship Learning. NeurIPS 2023.\n[3] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or. Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models. arXiv preprint arXiv:2302.12228 (2023). \n[4] Xuhui Jia, Yang Zhao, Kelvin C.K. Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, Yu-Chuan Su. Taming Encoder for Zero Fine-tuning Image Customization with Text-to-Image Diffusion Models. arXiv preprint arXiv:2304.02642, 2023.\n\nQuestions: - The authors aim at \"detail preservation\" but do not clearly explain what information these details include. What is the difference between detail preservation and identity preservation?\n- Minor issue of presentation. Figures 6 and 7 on page 8 are squeezed and Figure 7 has slightly occluded the caption of Figure 6. May rearrange for better visualization.", "labeling_timestamp": "2026-01-11T17:28:02.380186", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper repeatedly refers to 'detailed information' and 'fine-grained details' but never defines which specific object attributes (e.g., color, texture, shape, pose, small parts) are meant by 'detail preservation' or gives an explicit definition or list. References to detail preservation are generic and no operational definition or concrete examples of the particular details preserved are provided in the cited sections.", "evidence": "Abstract: \"it may restrict the model capability, resulting in the loss of detailed information and inferior performance.\"; Contributions (Introduction): \"The absence of regularization also allows the proposed framework to achieve enhanced preservation of fine-grained details;\"; Section 2 (Methodology): \"our goal is to successfully perform customized generation which preserves details of ¯ x0 , and meets the requirements specified in arbitrary prompt containing S ∗ .\"; Section 2 discussion of Figure 3: \"Small regularization leads to less information loss, which results in better detail preservation.\"", "section": "Abstract; Introduction (Contributions); Section 2 (Methodology and Figure 3 discussion)"}
{"claim": "The manuscript does not explain the difference between detail preservation and identity preservation.", "claim_type": "methodology", "paper_id": "VzPGV19Bnp", "paper_title": "Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "DR2C8HbrUg", "reviewer": "Reviewer_qTeR", "review_text": "Summary: The authors observe that the commonly used regularization to avoid overfitting in customized text-to-image generative models may lead to the loss of detailed information on the customized objects. The authors propose to balance the influences of the prompt condition and the customization condition S* instead of applying regularization to avoid overfitting, ensuring the preservation of customized details and flexibility to work with diverse prompts.\n\nStrengths: - Motivated by interesting observations\n- Novel idea of removing regularization to preserve detailed information\n- Computational resource-efficient approach\n\nWeaknesses: - Encoder-based customized Text-to-Image generation is not new and missing comparisons with important previous works: [1][2][3][4]\n- The important term \"independent conditions\" is not clearly defined.\n- Figure 9 shows that before fine-tuning, fusion sampling even leads to worse identity-preserving performance than baseline sampling, implying that performance gain in identity similarity mainly comes from fine-tuning.\n- In Figure 4, the proposed method does not present visually better results compared to E4T.\n\n\n[1] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. ELITE: Encoding visual concepts into textual embeddings for customized text-to-image generation. arXiv preprint arXiv:2302.13848, 2023.\n[2] Chen, Wenhu and Hu, Hexiang and Li, Yandong and Ruiz, Nataniel and Jia, Xuhui and Chang, Ming-Wei and Cohen, William W. Subject-driven Text-to-Image Generation via Apprenticeship Learning. NeurIPS 2023.\n[3] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or. Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models. arXiv preprint arXiv:2302.12228 (2023). \n[4] Xuhui Jia, Yang Zhao, Kelvin C.K. Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, Yu-Chuan Su. Taming Encoder for Zero Fine-tuning Image Customization with Text-to-Image Diffusion Models. arXiv preprint arXiv:2304.02642, 2023.\n\nQuestions: - The authors aim at \"detail preservation\" but do not clearly explain what information these details include. What is the difference between detail preservation and identity preservation?\n- Minor issue of presentation. Figures 6 and 7 on page 8 are squeezed and Figure 7 has slightly occluded the caption of Figure 6. May rearrange for better visualization.", "labeling_timestamp": "2026-01-11T17:28:08.679814", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper repeatedly discusses and defines 'detail' or 'fine-grained detail' preservation (Abstract, Introduction, Methodology) but does not mention or define 'identity preservation' nor contrast the two concepts anywhere in the provided text. Thus the claim that the manuscript does not explain the difference is correct.", "evidence": "1) \"The absence of regularization also allows the proposed framework to achieve enhanced preservation of fine-grained details;\" (Abstract/Introduction contributions). 2) \"our goal is to successfully perform customized generation which preserves details of ¯ x_0, and meets the requirements specified in arbitrary prompt containing S^* .\" (Section 2, Methodology).", "section": "Abstract; Introduction; 2 METHODOLOGY"}
{"claim": "Figures 6 and 7 are squeezed and Figure 7 slightly occludes the caption of Figure 6, impairing visualization.", "claim_type": "presentation", "paper_id": "VzPGV19Bnp", "paper_title": "Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "DR2C8HbrUg", "reviewer": "Reviewer_qTeR", "review_text": "Summary: The authors observe that the commonly used regularization to avoid overfitting in customized text-to-image generative models may lead to the loss of detailed information on the customized objects. The authors propose to balance the influences of the prompt condition and the customization condition S* instead of applying regularization to avoid overfitting, ensuring the preservation of customized details and flexibility to work with diverse prompts.\n\nStrengths: - Motivated by interesting observations\n- Novel idea of removing regularization to preserve detailed information\n- Computational resource-efficient approach\n\nWeaknesses: - Encoder-based customized Text-to-Image generation is not new and missing comparisons with important previous works: [1][2][3][4]\n- The important term \"independent conditions\" is not clearly defined.\n- Figure 9 shows that before fine-tuning, fusion sampling even leads to worse identity-preserving performance than baseline sampling, implying that performance gain in identity similarity mainly comes from fine-tuning.\n- In Figure 4, the proposed method does not present visually better results compared to E4T.\n\n\n[1] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. ELITE: Encoding visual concepts into textual embeddings for customized text-to-image generation. arXiv preprint arXiv:2302.13848, 2023.\n[2] Chen, Wenhu and Hu, Hexiang and Li, Yandong and Ruiz, Nataniel and Jia, Xuhui and Chang, Ming-Wei and Cohen, William W. Subject-driven Text-to-Image Generation via Apprenticeship Learning. NeurIPS 2023.\n[3] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or. Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models. arXiv preprint arXiv:2302.12228 (2023). \n[4] Xuhui Jia, Yang Zhao, Kelvin C.K. Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, Yu-Chuan Su. Taming Encoder for Zero Fine-tuning Image Customization with Text-to-Image Diffusion Models. arXiv preprint arXiv:2304.02642, 2023.\n\nQuestions: - The authors aim at \"detail preservation\" but do not clearly explain what information these details include. What is the difference between detail preservation and identity preservation?\n- Minor issue of presentation. Figures 6 and 7 on page 8 are squeezed and Figure 7 has slightly occluded the caption of Figure 6. May rearrange for better visualization.", "labeling_timestamp": "2026-01-11T17:28:22.992617", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not contain Figures 6 or 7, so the claim about their layout and occlusion cannot be verified from the supplied content.", "evidence": "\"Figure 1: Generated examples from the proposed framework. Given only single testing image from arbitrary downstream domain, ProFusion can perform customized generation in 5 to 25 seconds.\"; \"Figure 2: Illustration of the proposed framework. A PromptNet will be trained to output an embedding S ∗ based on input image. Then Fusion Sampling performs customized generation based on S ∗ and arbitrary text input.\"; \"Figure 3: The performance of customized generation is impacted by the level of regularization.\"", "section": "Introduction; Methodology"}
{"claim": "The paper lacks theoretical guarantees for the proposed CS-Fluctuation metric and its use in preventing overfitting.", "claim_type": "methodology", "paper_id": "TzWLecXH6I", "paper_title": "Towards Personalized AI: Early-stopping Low-Rank Adaptation of Foundation Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "QrZww2Zdjd", "reviewer": "Reviewer_fqrQ", "review_text": "Summary: This paper aims to address the overfitting issue that arises when fine-tuning a pre-trained foundation model using Low-Rank Adaption (LoRA). Particularly, the authors proposed a new metric called CS-Fluctuation, based on the cosine similarity between the fixed model weight and the added trainable weight using personal data.\n\nStrengths: 1. The proposed method CS-Fluctuation is very simple and kind of reasonable from the case study in this paper. \n1. The proposed method are demonstrated in various benchmark foundation models.\n\nWeaknesses: 1. The proposed method lacks intuitive understanding and theoretical guarantee. It is hard to fully understanding why the proposed method is reasonable, especially why the metric is based on the cosine similarity between the model weights?\n1. The scale of the metric ($\\approx1e-7$) is too small and changes from data to data. Some normalization is needed for the metric. \n1. The experiment demonstration is monotonous, only CS-Fluctuation vs. training steps is showcased, more aspects about the proposed method should be presented to justify the claims. \n1. No qualitative comparisons. It hard to judge the superiority of the proposed method.\n\nQuestions: 1. In figure 1, what is the connection between Epoch and training steps? It seems the first two epoch is sufficient for model fine-tuning from the figure?\n1. From the figure 3, it seems there is no clear signal which training steps is better. Why not early stop the method at the first $K$ epoch? set $K=2$ according figure 1.", "labeling_timestamp": "2026-01-11T17:28:28.267621", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes CS-Fluctuation empirically and reports experiments that corroborate its effectiveness, but it contains no theoretical analysis, theorems, proofs, or formal guarantees about the metric or its ability to prevent overfitting.", "evidence": "Abstract: \"Empirically, we leverage various types of personalized data to conduct customization experiments on both vision and language foundation models, which corroborates the effectiveness of CS-Fluctuation in early stopping the LoRA fine-tuning.\"; Section 3.2: \"We leverage CS-Fluctuation to early stop the fine-tuning process to return a well performing LoRA model. Over the fine-tuning iterations, we empirically observe that the CS-Fluctuations behave like transverse waves...\"", "section": "Abstract; Section 3.2 CS-FLUCTUATION"}
{"claim": "The paper does not provide an intuitive explanation for using cosine similarity between fixed model weights and trainable LoRA weights.", "claim_type": "methodology", "paper_id": "TzWLecXH6I", "paper_title": "Towards Personalized AI: Early-stopping Low-Rank Adaptation of Foundation Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "QrZww2Zdjd", "reviewer": "Reviewer_fqrQ", "review_text": "Summary: This paper aims to address the overfitting issue that arises when fine-tuning a pre-trained foundation model using Low-Rank Adaption (LoRA). Particularly, the authors proposed a new metric called CS-Fluctuation, based on the cosine similarity between the fixed model weight and the added trainable weight using personal data.\n\nStrengths: 1. The proposed method CS-Fluctuation is very simple and kind of reasonable from the case study in this paper. \n1. The proposed method are demonstrated in various benchmark foundation models.\n\nWeaknesses: 1. The proposed method lacks intuitive understanding and theoretical guarantee. It is hard to fully understanding why the proposed method is reasonable, especially why the metric is based on the cosine similarity between the model weights?\n1. The scale of the metric ($\\approx1e-7$) is too small and changes from data to data. Some normalization is needed for the metric. \n1. The experiment demonstration is monotonous, only CS-Fluctuation vs. training steps is showcased, more aspects about the proposed method should be presented to justify the claims. \n1. No qualitative comparisons. It hard to judge the superiority of the proposed method.\n\nQuestions: 1. In figure 1, what is the connection between Epoch and training steps? It seems the first two epoch is sufficient for model fine-tuning from the figure?\n1. From the figure 3, it seems there is no clear signal which training steps is better. Why not early stop the method at the first $K$ epoch? set $K=2$ according figure 1.", "labeling_timestamp": "2026-01-11T17:28:35.182712", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes and motivates using cosine similarity (CS) via empirical observations (abrupt-to-stable CS behavior and using CS-Fluctuation to find a turning point) but does not provide a theoretical or intuitive explanation for why CS between frozen weights and LoRA weights should indicate overfitting or the learning phases.", "evidence": ["Abstract: \"we leverage Low-Rank Adaptation (LoRA) to fit the small scale of the personalized data while monitoring the cosine similarity of the parameter changes between the LoRA branch and its corresponding layer. When the changes become steady, we observe the onset of overfitting issue which becomes increasingly severe as fine-tuning progresses.\"", "Introduction: \"During the LoRA fine-tuning for both LDM and LLM, we observe that CS undergoes abrupt changes before settling into a more gradual and stable pattern... Interestingly, the turning point of overfitting happens exactly at the transitional point of CS from 'abrupt' changes to 'stable' changes... Thus, to aid in locating the turning point (grey dashed lines), we propose a new metric called CS-Fluctuation...\"", "Section 3.2: \"CS-Fluctuation is computed based on the Cosine Similarity (CS) between frozen parameters in a pretrained foundation model (W) and their counterparts in LoRA (BA).\" and \"We empirically observe that the CS-Fluctuations behave like transverse waves... The first valley... Therefore, we take the second valley as the turning point to signal the early stopping...\""], "section": "Abstract; Introduction; 3.2 CS-Fluctuation"}
{"claim": "The CS-Fluctuation metric has an extremely small scale (≈1e-7) that varies across datasets, and the paper does not propose normalization.", "claim_type": "quantitative", "paper_id": "TzWLecXH6I", "paper_title": "Towards Personalized AI: Early-stopping Low-Rank Adaptation of Foundation Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "QrZww2Zdjd", "reviewer": "Reviewer_fqrQ", "review_text": "Summary: This paper aims to address the overfitting issue that arises when fine-tuning a pre-trained foundation model using Low-Rank Adaption (LoRA). Particularly, the authors proposed a new metric called CS-Fluctuation, based on the cosine similarity between the fixed model weight and the added trainable weight using personal data.\n\nStrengths: 1. The proposed method CS-Fluctuation is very simple and kind of reasonable from the case study in this paper. \n1. The proposed method are demonstrated in various benchmark foundation models.\n\nWeaknesses: 1. The proposed method lacks intuitive understanding and theoretical guarantee. It is hard to fully understanding why the proposed method is reasonable, especially why the metric is based on the cosine similarity between the model weights?\n1. The scale of the metric ($\\approx1e-7$) is too small and changes from data to data. Some normalization is needed for the metric. \n1. The experiment demonstration is monotonous, only CS-Fluctuation vs. training steps is showcased, more aspects about the proposed method should be presented to justify the claims. \n1. No qualitative comparisons. It hard to judge the superiority of the proposed method.\n\nQuestions: 1. In figure 1, what is the connection between Epoch and training steps? It seems the first two epoch is sufficient for model fine-tuning from the figure?\n1. From the figure 3, it seems there is no clear signal which training steps is better. Why not early stop the method at the first $K$ epoch? set $K=2$ according figure 1.", "labeling_timestamp": "2026-01-11T17:28:32.295641", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states a normalization step for CS-Fluctuation (dividing the variance by the learning rate), which contradicts the reviewer's claim that the paper does not propose normalization. The paper does not report any numeric scale like ~1e-7 for CS-Fluctuation or claim dataset-varying magnitudes, so that part of the reviewer's statement is unsupported by the paper.", "evidence": "The CS-Fluctuation is then the variance value of smoothed CS slope, as shown in Equation 4. We divide the variance by lr to normalize and eliminate the effect of lr scale on CS fluctuations.", "section": "3.2 CS-FLUCTUATION: TRACKING LEARNING STATUS TO AVOID OVERFITTING"}
{"claim": "Experiments only present CS-Fluctuation plotted against training steps, lacking additional analyses such as performance metrics, ablations, or robustness evaluations.", "claim_type": "experimental", "paper_id": "TzWLecXH6I", "paper_title": "Towards Personalized AI: Early-stopping Low-Rank Adaptation of Foundation Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "QrZww2Zdjd", "reviewer": "Reviewer_fqrQ", "review_text": "Summary: This paper aims to address the overfitting issue that arises when fine-tuning a pre-trained foundation model using Low-Rank Adaption (LoRA). Particularly, the authors proposed a new metric called CS-Fluctuation, based on the cosine similarity between the fixed model weight and the added trainable weight using personal data.\n\nStrengths: 1. The proposed method CS-Fluctuation is very simple and kind of reasonable from the case study in this paper. \n1. The proposed method are demonstrated in various benchmark foundation models.\n\nWeaknesses: 1. The proposed method lacks intuitive understanding and theoretical guarantee. It is hard to fully understanding why the proposed method is reasonable, especially why the metric is based on the cosine similarity between the model weights?\n1. The scale of the metric ($\\approx1e-7$) is too small and changes from data to data. Some normalization is needed for the metric. \n1. The experiment demonstration is monotonous, only CS-Fluctuation vs. training steps is showcased, more aspects about the proposed method should be presented to justify the claims. \n1. No qualitative comparisons. It hard to judge the superiority of the proposed method.\n\nQuestions: 1. In figure 1, what is the connection between Epoch and training steps? It seems the first two epoch is sufficient for model fine-tuning from the figure?\n1. From the figure 3, it seems there is no clear signal which training steps is better. Why not early stop the method at the first $K$ epoch? set $K=2$ according figure 1.", "labeling_timestamp": "2026-01-11T17:28:37.912622", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpts describe CS-Fluctuation and state that empirical experiments were conducted and that LoRA models at the turning points \"demonstrated better performance\", but the excerpts do not contain the experimental results, metrics, ablation studies, or robustness evaluations themselves. Therefore there is insufficient information in the supplied content to confirm the reviewer’s claim that experiments only show CS-Fluctuation plots and lack other analyses.", "evidence": "\"Empirically, we leverage various types of personalized data to conduct customization experiments on both vision and language foundation models, which corroborates the effectiveness of CS-Fluctuation in early stopping the LoRA fine-tuning.\"; \"In practice, the LoRA models corresponding to these turning points demonstrated better performance in most cases.\"", "section": "Abstract and Introduction"}
{"claim": "The paper omits qualitative comparisons with alternative methods, making it difficult to judge the proposed method's superiority.", "claim_type": "baseline", "paper_id": "TzWLecXH6I", "paper_title": "Towards Personalized AI: Early-stopping Low-Rank Adaptation of Foundation Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "QrZww2Zdjd", "reviewer": "Reviewer_fqrQ", "review_text": "Summary: This paper aims to address the overfitting issue that arises when fine-tuning a pre-trained foundation model using Low-Rank Adaption (LoRA). Particularly, the authors proposed a new metric called CS-Fluctuation, based on the cosine similarity between the fixed model weight and the added trainable weight using personal data.\n\nStrengths: 1. The proposed method CS-Fluctuation is very simple and kind of reasonable from the case study in this paper. \n1. The proposed method are demonstrated in various benchmark foundation models.\n\nWeaknesses: 1. The proposed method lacks intuitive understanding and theoretical guarantee. It is hard to fully understanding why the proposed method is reasonable, especially why the metric is based on the cosine similarity between the model weights?\n1. The scale of the metric ($\\approx1e-7$) is too small and changes from data to data. Some normalization is needed for the metric. \n1. The experiment demonstration is monotonous, only CS-Fluctuation vs. training steps is showcased, more aspects about the proposed method should be presented to justify the claims. \n1. No qualitative comparisons. It hard to judge the superiority of the proposed method.\n\nQuestions: 1. In figure 1, what is the connection between Epoch and training steps? It seems the first two epoch is sufficient for model fine-tuning from the figure?\n1. From the figure 3, it seems there is no clear signal which training steps is better. Why not early stop the method at the first $K$ epoch? set $K=2$ according figure 1.", "labeling_timestamp": "2026-01-11T17:28:56.907411", "model": "gpt-5-mini", "label": "Supported", "justification": "Throughout the provided content the paper describes experiments demonstrating the effectiveness of CS-Fluctuation but does not present qualitative comparisons against alternative methods or baselines; claims of better performance are made without describing comparative qualitative analyses.", "evidence": "\"Empirically, we leverage various types of personalized data to conduct customization experiments on both vision and language foundation models, which corroborates the effectiveness of CS-Fluctuation in early stopping the LoRA fine-tuning.\" \"In practice, the LoRA models corresponding to these turning points demonstrated better performance in most cases.\"", "section": "Abstract / Introduction"}
{"claim": "Figure 1 does not clarify the relationship between 'Epoch' and 'training steps', leaving the mapping and interpretation ambiguous.", "claim_type": "presentation", "paper_id": "TzWLecXH6I", "paper_title": "Towards Personalized AI: Early-stopping Low-Rank Adaptation of Foundation Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "QrZww2Zdjd", "reviewer": "Reviewer_fqrQ", "review_text": "Summary: This paper aims to address the overfitting issue that arises when fine-tuning a pre-trained foundation model using Low-Rank Adaption (LoRA). Particularly, the authors proposed a new metric called CS-Fluctuation, based on the cosine similarity between the fixed model weight and the added trainable weight using personal data.\n\nStrengths: 1. The proposed method CS-Fluctuation is very simple and kind of reasonable from the case study in this paper. \n1. The proposed method are demonstrated in various benchmark foundation models.\n\nWeaknesses: 1. The proposed method lacks intuitive understanding and theoretical guarantee. It is hard to fully understanding why the proposed method is reasonable, especially why the metric is based on the cosine similarity between the model weights?\n1. The scale of the metric ($\\approx1e-7$) is too small and changes from data to data. Some normalization is needed for the metric. \n1. The experiment demonstration is monotonous, only CS-Fluctuation vs. training steps is showcased, more aspects about the proposed method should be presented to justify the claims. \n1. No qualitative comparisons. It hard to judge the superiority of the proposed method.\n\nQuestions: 1. In figure 1, what is the connection between Epoch and training steps? It seems the first two epoch is sufficient for model fine-tuning from the figure?\n1. From the figure 3, it seems there is no clear signal which training steps is better. Why not early stop the method at the first $K$ epoch? set $K=2$ according figure 1.", "labeling_timestamp": "2026-01-11T17:28:53.030641", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper's Figure 1 refers to an 'Epoch' (e.g., 'Epoch 2') but the text defines CS measurements in terms of iteration/index j and batch-wise moving windows without specifying how iterations map to epochs or how many training steps constitute an epoch. Thus the figure does not clarify the relationship between 'Epoch' and 'training steps', making the mapping ambiguous.", "evidence": "Figure 1 caption: 'The grey dashed line is the turning point that is located by our proposed algorithm, e.g., the Epoch 2 in red in Fig.1(a) is where the turning point is located.'; Section 3.2: 'where j is the index of iteration steps in fine-tuning process, and M is the size of moving window.'; Section 3.2: 'Note that in order to calculate the CS of the iteration j , we need to calculate M more iterations of fine-tuning.'", "section": "Introduction (Figure 1 caption) and Section 3.2 (CS-FLUCTUATION)"}
{"claim": "Figure 1 appears to suggest that the first two epochs are sufficient for fine-tuning, but the paper does not discuss or validate early stopping.", "claim_type": "experimental", "paper_id": "TzWLecXH6I", "paper_title": "Towards Personalized AI: Early-stopping Low-Rank Adaptation of Foundation Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "QrZww2Zdjd", "reviewer": "Reviewer_fqrQ", "review_text": "Summary: This paper aims to address the overfitting issue that arises when fine-tuning a pre-trained foundation model using Low-Rank Adaption (LoRA). Particularly, the authors proposed a new metric called CS-Fluctuation, based on the cosine similarity between the fixed model weight and the added trainable weight using personal data.\n\nStrengths: 1. The proposed method CS-Fluctuation is very simple and kind of reasonable from the case study in this paper. \n1. The proposed method are demonstrated in various benchmark foundation models.\n\nWeaknesses: 1. The proposed method lacks intuitive understanding and theoretical guarantee. It is hard to fully understanding why the proposed method is reasonable, especially why the metric is based on the cosine similarity between the model weights?\n1. The scale of the metric ($\\approx1e-7$) is too small and changes from data to data. Some normalization is needed for the metric. \n1. The experiment demonstration is monotonous, only CS-Fluctuation vs. training steps is showcased, more aspects about the proposed method should be presented to justify the claims. \n1. No qualitative comparisons. It hard to judge the superiority of the proposed method.\n\nQuestions: 1. In figure 1, what is the connection between Epoch and training steps? It seems the first two epoch is sufficient for model fine-tuning from the figure?\n1. From the figure 3, it seems there is no clear signal which training steps is better. Why not early stop the method at the first $K$ epoch? set $K=2$ according figure 1.", "labeling_timestamp": "2026-01-11T17:28:57.471023", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly proposes and discusses early stopping via the CS-Fluctuation metric, describes an algorithm for early stopping (Algorithm 1), and reports empirical validation that CS-Fluctuation identifies turning points to early-stop fine-tuning. Figure 1 does show an example turning point at 'Epoch 2', but contrary to the claim the paper does discuss and validate early stopping.", "evidence": "Abstract: \"we introduce a new monitoring metric (CS-Fluctuation) to facilitate early stopping the fine-tuning process.\" | Figure 1 caption: \"The grey dashed line is the turning point that is located by our proposed algorithm, e.g., the Epoch 2 in red in Fig.1(a) is where the turning point is located.\" | Section 3.2: \"In this section, we propose a CS-Fluctuation metric to monitor the LoRA fine-tuning process. Once CS-Fluctuation becomes steady and small, we early stop the fine-tuning process to avoid overfitting.\"", "section": "Abstract, Figure 1, Section 3.2"}
{"claim": "Figure 3 shows no clear signal indicating which training steps yield better results, undermining guidance for selecting optimal stopping points.", "claim_type": "experimental", "paper_id": "TzWLecXH6I", "paper_title": "Towards Personalized AI: Early-stopping Low-Rank Adaptation of Foundation Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "QrZww2Zdjd", "reviewer": "Reviewer_fqrQ", "review_text": "Summary: This paper aims to address the overfitting issue that arises when fine-tuning a pre-trained foundation model using Low-Rank Adaption (LoRA). Particularly, the authors proposed a new metric called CS-Fluctuation, based on the cosine similarity between the fixed model weight and the added trainable weight using personal data.\n\nStrengths: 1. The proposed method CS-Fluctuation is very simple and kind of reasonable from the case study in this paper. \n1. The proposed method are demonstrated in various benchmark foundation models.\n\nWeaknesses: 1. The proposed method lacks intuitive understanding and theoretical guarantee. It is hard to fully understanding why the proposed method is reasonable, especially why the metric is based on the cosine similarity between the model weights?\n1. The scale of the metric ($\\approx1e-7$) is too small and changes from data to data. Some normalization is needed for the metric. \n1. The experiment demonstration is monotonous, only CS-Fluctuation vs. training steps is showcased, more aspects about the proposed method should be presented to justify the claims. \n1. No qualitative comparisons. It hard to judge the superiority of the proposed method.\n\nQuestions: 1. In figure 1, what is the connection between Epoch and training steps? It seems the first two epoch is sufficient for model fine-tuning from the figure?\n1. From the figure 3, it seems there is no clear signal which training steps is better. Why not early stop the method at the first $K$ epoch? set $K=2$ according figure 1.", "labeling_timestamp": "2026-01-11T17:29:08.624039", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper content does not include or describe any Figure 3, so there is insufficient information to verify the reviewer's claim about Figure 3. The paper does discuss Figure 1 as showing turning points and CS-Fluctuation signals for early stopping (supporting that some figures do show signals), but no Figure 3 is present in the supplied text to confirm or refute the reviewer's statement.", "evidence": "\"As shown in Figure 1, an individual wants to incorporate the personalized photos or texts into a foundation model...\" and \"Figure 1: LoRA of a LDM and LLM. As the LoRA fine-tuning progresses, the cosine similarity (CS) between LoRA layer and its corresponding original layers undergoes abrupt changes before settling into a more gradual and stable pattern. Our proposed monitoring metric-CS-Fluctuation-monoter the fluctuations of the CS changes.\"", "section": "1 INTRODUCTION"}
{"claim": "The paper does not evaluate or justify early stopping (for example stopping at K=2 epochs), despite figures implying it could be effective.", "claim_type": "experimental", "paper_id": "TzWLecXH6I", "paper_title": "Towards Personalized AI: Early-stopping Low-Rank Adaptation of Foundation Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "QrZww2Zdjd", "reviewer": "Reviewer_fqrQ", "review_text": "Summary: This paper aims to address the overfitting issue that arises when fine-tuning a pre-trained foundation model using Low-Rank Adaption (LoRA). Particularly, the authors proposed a new metric called CS-Fluctuation, based on the cosine similarity between the fixed model weight and the added trainable weight using personal data.\n\nStrengths: 1. The proposed method CS-Fluctuation is very simple and kind of reasonable from the case study in this paper. \n1. The proposed method are demonstrated in various benchmark foundation models.\n\nWeaknesses: 1. The proposed method lacks intuitive understanding and theoretical guarantee. It is hard to fully understanding why the proposed method is reasonable, especially why the metric is based on the cosine similarity between the model weights?\n1. The scale of the metric ($\\approx1e-7$) is too small and changes from data to data. Some normalization is needed for the metric. \n1. The experiment demonstration is monotonous, only CS-Fluctuation vs. training steps is showcased, more aspects about the proposed method should be presented to justify the claims. \n1. No qualitative comparisons. It hard to judge the superiority of the proposed method.\n\nQuestions: 1. In figure 1, what is the connection between Epoch and training steps? It seems the first two epoch is sufficient for model fine-tuning from the figure?\n1. From the figure 3, it seems there is no clear signal which training steps is better. Why not early stop the method at the first $K$ epoch? set $K=2$ according figure 1.", "labeling_timestamp": "2026-01-11T17:29:20.528890", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper proposes and motivates CS-Fluctuation as an early-stopping criterion and states that experiments corroborate its effectiveness, but the provided excerpt does not include concrete evaluation details, quantitative results, or a justification specifically analyzing early stopping at K=2 epochs. Therefore there is insufficient information in the supplied content to confirm the reviewer's claim.", "evidence": "Abstract: \"we introduce a new monitoring metric (CS-Fluctuation) to facilitate early stopping the fine-tuning process.\"; Figure 1 caption: \"The grey dashed line is the turning point that is located by our proposed algorithm, e.g., the Epoch 2 in red in Fig.1(a) is where the turning point is located.\"; Section 3.2: \"We leverage CS-Fluctuation to early stop the fine-tuning process to return a well performing LoRA model... The first valley of waves often happens... Therefore, we take the second valley as the turning point to signal the early stopping of the fine-tuning process.\"", "section": "Abstract; Figure 1 caption (Introduction); Section 3.2 (Method)"}
{"claim": "The revision did not adequately address the reviewer's core concerns and instead referenced other reviewers' comments without resolving issues.", "claim_type": "subjective", "paper_id": "npJQ6qS4bg", "paper_title": "Understanding and Minimising Outlier Features in Transformer Training", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "YeBeK8zEL0", "reviewer": "Reviewer_Lb1w", "review_text": "Comment: Thank you for your response. However, it appears that the revision did not adequately address the core concerns, instead referencing unhelpful comments from other reviewers. The citations from other evaluations do not substantiate your responses unless they are accompanied by supporting evidence. Consequently, my concerns remain unresolved, and I have retained my original score.\n\n1. **Unsatisfactory Writing Quality**: The manuscript suffers from numerous stylistic and structural weaknesses, making it difficult to follow. Starting in Section 3, from Line 87-103, the paragraph introduces background on normalization but fails to signal this topic in the opening sentence. Line 104-177, abruptly begins with \"In Figure 2\" without a transition from the previous paragraph, leaving the reader unclear about the figure's contect...... Limited by time and energy, the reviewer can not point out all the places that writing can be improved since there are too many. Most of the paragraphs in the document lack a coherent top-down structure.\n\n2. **Insufficient Support for Architectural Contribution**: The \"Outlier Protected transformer block\" is presented as a highlighted innovation, yet its description is confined to a single paragraph (Lines 146-163). For an effective presentation of a new architecture, consider the detailed exposition found in Section 4 of Reference [14] (frequently mentioned by the author). If there are substantial results supporting the proposed architecture, they should be prominently featured, akin to how Tables 2 and 3 are presented in [14]. The lack of a unified section detailing the experimental approach, akin to Section 5 in Reference [14], makes it challenging for the readers to effectively understand the context and validity of your findings. \n\n3. **Insignificant Observation**: My concern regarding the relationship between Outlier Features (OF) and Signal Propagation has not been addressed. You provide several opposing subjective comments but fail to engage with the core issue. Specifically, you claim a significant observation that Signal Propagation relates to OF, yet both measures you discuss (Kurtosis and the Gram matrix) are mathematical functions of the same variable, X. The inherent mathematical relationship between these functions should not be surprising, and you have not clarified why this finding is noteworthy.\n\n4. **Insufficient Fundamental Insight**: The title and abstract of your paper promises a deep understanding of why OFs emerge during training. However, the empirical analysis over optimization choices in Section 5, while useful, does not address this core question from a fundamental perspective. An effective discussion should integrate more profound insights into the nature and implications of OFs within neural network dynamics. To enhance the paper, one possible analysis technique can be associated with the neural tangent kernel or training dynamics. For guidance on conducting this type of rigorous research, I recommend referring to [A,B,C], which illustrate how scholars **deeply** uncover the underlying causes of phenomena during training. \n\nOverall, the manuscript requires significant improvements in writing, experimental design, and technical depth to meet the publication standards.\n\n[A] Kumar, A., Raghunathan, A., Jones, R. M., Ma, T., & Liang, P. Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution. In International Conference on Learning Representations 2022.\n\n[B] Kou, Y., Chen, Z., Chen, Y., & Gu, Q. Benign overfitting in two-layer ReLU convolutional neural networks. In International Conference on Machine Learning (pp. 17615-17659). PMLR 2023.\n\n[C] Mahankali, A. V., Hashimoto, T., & Ma, T. One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention. In The Twelfth International Conference on Learning Representations 2024.", "labeling_timestamp": "2026-01-11T17:29:18.755914", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper is the research manuscript and contains no discussion of a revision or any author responses to reviewers. There is no information in the paper text about addressing reviewer comments or referencing other reviewers' remarks, so the claim about the revision's adequacy cannot be verified from the paper content.", "evidence": "\"Our contributions Overall, we show that OFE can be mitigated relative to standard practices, and highlight key design choices to do so.\"", "section": "Introduction"}
{"claim": "Citations from other evaluations do not substantiate the authors' responses unless accompanied by supporting evidence from the current paper.", "claim_type": "subjective", "paper_id": "npJQ6qS4bg", "paper_title": "Understanding and Minimising Outlier Features in Transformer Training", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "YeBeK8zEL0", "reviewer": "Reviewer_Lb1w", "review_text": "Comment: Thank you for your response. However, it appears that the revision did not adequately address the core concerns, instead referencing unhelpful comments from other reviewers. The citations from other evaluations do not substantiate your responses unless they are accompanied by supporting evidence. Consequently, my concerns remain unresolved, and I have retained my original score.\n\n1. **Unsatisfactory Writing Quality**: The manuscript suffers from numerous stylistic and structural weaknesses, making it difficult to follow. Starting in Section 3, from Line 87-103, the paragraph introduces background on normalization but fails to signal this topic in the opening sentence. Line 104-177, abruptly begins with \"In Figure 2\" without a transition from the previous paragraph, leaving the reader unclear about the figure's contect...... Limited by time and energy, the reviewer can not point out all the places that writing can be improved since there are too many. Most of the paragraphs in the document lack a coherent top-down structure.\n\n2. **Insufficient Support for Architectural Contribution**: The \"Outlier Protected transformer block\" is presented as a highlighted innovation, yet its description is confined to a single paragraph (Lines 146-163). For an effective presentation of a new architecture, consider the detailed exposition found in Section 4 of Reference [14] (frequently mentioned by the author). If there are substantial results supporting the proposed architecture, they should be prominently featured, akin to how Tables 2 and 3 are presented in [14]. The lack of a unified section detailing the experimental approach, akin to Section 5 in Reference [14], makes it challenging for the readers to effectively understand the context and validity of your findings. \n\n3. **Insignificant Observation**: My concern regarding the relationship between Outlier Features (OF) and Signal Propagation has not been addressed. You provide several opposing subjective comments but fail to engage with the core issue. Specifically, you claim a significant observation that Signal Propagation relates to OF, yet both measures you discuss (Kurtosis and the Gram matrix) are mathematical functions of the same variable, X. The inherent mathematical relationship between these functions should not be surprising, and you have not clarified why this finding is noteworthy.\n\n4. **Insufficient Fundamental Insight**: The title and abstract of your paper promises a deep understanding of why OFs emerge during training. However, the empirical analysis over optimization choices in Section 5, while useful, does not address this core question from a fundamental perspective. An effective discussion should integrate more profound insights into the nature and implications of OFs within neural network dynamics. To enhance the paper, one possible analysis technique can be associated with the neural tangent kernel or training dynamics. For guidance on conducting this type of rigorous research, I recommend referring to [A,B,C], which illustrate how scholars **deeply** uncover the underlying causes of phenomena during training. \n\nOverall, the manuscript requires significant improvements in writing, experimental design, and technical depth to meet the publication standards.\n\n[A] Kumar, A., Raghunathan, A., Jones, R. M., Ma, T., & Liang, P. Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution. In International Conference on Learning Representations 2022.\n\n[B] Kou, Y., Chen, Z., Chen, Y., & Gu, Q. Benign overfitting in two-layer ReLU convolutional neural networks. In International Conference on Machine Learning (pp. 17615-17659). PMLR 2023.\n\n[C] Mahankali, A. V., Hashimoto, T., & Ma, T. One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention. In The Twelfth International Conference on Learning Representations 2024.", "labeling_timestamp": "2026-01-11T17:29:29.762956", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper cites prior work and presents its own experiments and metrics, but it does not explicitly assert the normative claim that citations from other evaluations are insufficient unless accompanied by supporting evidence in the current paper. Thus the paper does not directly confirm or refute the reviewer's statement.", "evidence": "“Though progress has been made [1, 13, 14, 10, 15], the mechanisms behind OFE remain largely unknown.”", "section": "Introduction"}
{"claim": "The manuscript contains numerous stylistic and structural weaknesses that make it difficult to follow.", "claim_type": "presentation", "paper_id": "npJQ6qS4bg", "paper_title": "Understanding and Minimising Outlier Features in Transformer Training", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "YeBeK8zEL0", "reviewer": "Reviewer_Lb1w", "review_text": "Comment: Thank you for your response. However, it appears that the revision did not adequately address the core concerns, instead referencing unhelpful comments from other reviewers. The citations from other evaluations do not substantiate your responses unless they are accompanied by supporting evidence. Consequently, my concerns remain unresolved, and I have retained my original score.\n\n1. **Unsatisfactory Writing Quality**: The manuscript suffers from numerous stylistic and structural weaknesses, making it difficult to follow. Starting in Section 3, from Line 87-103, the paragraph introduces background on normalization but fails to signal this topic in the opening sentence. Line 104-177, abruptly begins with \"In Figure 2\" without a transition from the previous paragraph, leaving the reader unclear about the figure's contect...... Limited by time and energy, the reviewer can not point out all the places that writing can be improved since there are too many. Most of the paragraphs in the document lack a coherent top-down structure.\n\n2. **Insufficient Support for Architectural Contribution**: The \"Outlier Protected transformer block\" is presented as a highlighted innovation, yet its description is confined to a single paragraph (Lines 146-163). For an effective presentation of a new architecture, consider the detailed exposition found in Section 4 of Reference [14] (frequently mentioned by the author). If there are substantial results supporting the proposed architecture, they should be prominently featured, akin to how Tables 2 and 3 are presented in [14]. The lack of a unified section detailing the experimental approach, akin to Section 5 in Reference [14], makes it challenging for the readers to effectively understand the context and validity of your findings. \n\n3. **Insignificant Observation**: My concern regarding the relationship between Outlier Features (OF) and Signal Propagation has not been addressed. You provide several opposing subjective comments but fail to engage with the core issue. Specifically, you claim a significant observation that Signal Propagation relates to OF, yet both measures you discuss (Kurtosis and the Gram matrix) are mathematical functions of the same variable, X. The inherent mathematical relationship between these functions should not be surprising, and you have not clarified why this finding is noteworthy.\n\n4. **Insufficient Fundamental Insight**: The title and abstract of your paper promises a deep understanding of why OFs emerge during training. However, the empirical analysis over optimization choices in Section 5, while useful, does not address this core question from a fundamental perspective. An effective discussion should integrate more profound insights into the nature and implications of OFs within neural network dynamics. To enhance the paper, one possible analysis technique can be associated with the neural tangent kernel or training dynamics. For guidance on conducting this type of rigorous research, I recommend referring to [A,B,C], which illustrate how scholars **deeply** uncover the underlying causes of phenomena during training. \n\nOverall, the manuscript requires significant improvements in writing, experimental design, and technical depth to meet the publication standards.\n\n[A] Kumar, A., Raghunathan, A., Jones, R. M., Ma, T., & Liang, P. Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution. In International Conference on Learning Representations 2022.\n\n[B] Kou, Y., Chen, Z., Chen, Y., & Gu, Q. Benign overfitting in two-layer ReLU convolutional neural networks. In International Conference on Machine Learning (pp. 17615-17659). PMLR 2023.\n\n[C] Mahankali, A. V., Hashimoto, T., & Ma, T. One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention. In The Twelfth International Conference on Learning Representations 2024.", "labeling_timestamp": "2026-01-11T17:29:18.640642", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper presents a clear, standard structure (Abstract, Introduction, numbered sections with methods and experiments) and explicitly states its contributions and experimental setup, which contradicts the reviewer's claim that it contains numerous stylistic and structural weaknesses making it difficult to follow.", "evidence": "Abstract: \"Outlier Features (OFs) are neurons whose activation magnitudes significantly exceed the average over a neural network's (NN) width...\" | Introduction: \"In this paper, we tackle OFE from two related angles: by (1) proposing interventions to minimise OFE ... and (2) enhancing our understanding of why OFs appear during training.\" | Contributions: \"Overall, we show that OFE can be mitigated relative to standard practices, and highlight key design choices to do so. We start by introducing OFs, and in particular quantitative metrics to measure OFs in Sec 2. In Sec 3, we study the role of normalisation layers for OFE...\"", "section": "Abstract; 1 Introduction; Contributions paragraph (end of Introduction)"}
{"claim": "Section 3 Lines 87-103 introduce background on normalization but fail to signal that topic in the opening sentence.", "claim_type": "presentation", "paper_id": "npJQ6qS4bg", "paper_title": "Understanding and Minimising Outlier Features in Transformer Training", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "YeBeK8zEL0", "reviewer": "Reviewer_Lb1w", "review_text": "Comment: Thank you for your response. However, it appears that the revision did not adequately address the core concerns, instead referencing unhelpful comments from other reviewers. The citations from other evaluations do not substantiate your responses unless they are accompanied by supporting evidence. Consequently, my concerns remain unresolved, and I have retained my original score.\n\n1. **Unsatisfactory Writing Quality**: The manuscript suffers from numerous stylistic and structural weaknesses, making it difficult to follow. Starting in Section 3, from Line 87-103, the paragraph introduces background on normalization but fails to signal this topic in the opening sentence. Line 104-177, abruptly begins with \"In Figure 2\" without a transition from the previous paragraph, leaving the reader unclear about the figure's contect...... Limited by time and energy, the reviewer can not point out all the places that writing can be improved since there are too many. Most of the paragraphs in the document lack a coherent top-down structure.\n\n2. **Insufficient Support for Architectural Contribution**: The \"Outlier Protected transformer block\" is presented as a highlighted innovation, yet its description is confined to a single paragraph (Lines 146-163). For an effective presentation of a new architecture, consider the detailed exposition found in Section 4 of Reference [14] (frequently mentioned by the author). If there are substantial results supporting the proposed architecture, they should be prominently featured, akin to how Tables 2 and 3 are presented in [14]. The lack of a unified section detailing the experimental approach, akin to Section 5 in Reference [14], makes it challenging for the readers to effectively understand the context and validity of your findings. \n\n3. **Insignificant Observation**: My concern regarding the relationship between Outlier Features (OF) and Signal Propagation has not been addressed. You provide several opposing subjective comments but fail to engage with the core issue. Specifically, you claim a significant observation that Signal Propagation relates to OF, yet both measures you discuss (Kurtosis and the Gram matrix) are mathematical functions of the same variable, X. The inherent mathematical relationship between these functions should not be surprising, and you have not clarified why this finding is noteworthy.\n\n4. **Insufficient Fundamental Insight**: The title and abstract of your paper promises a deep understanding of why OFs emerge during training. However, the empirical analysis over optimization choices in Section 5, while useful, does not address this core question from a fundamental perspective. An effective discussion should integrate more profound insights into the nature and implications of OFs within neural network dynamics. To enhance the paper, one possible analysis technique can be associated with the neural tangent kernel or training dynamics. For guidance on conducting this type of rigorous research, I recommend referring to [A,B,C], which illustrate how scholars **deeply** uncover the underlying causes of phenomena during training. \n\nOverall, the manuscript requires significant improvements in writing, experimental design, and technical depth to meet the publication standards.\n\n[A] Kumar, A., Raghunathan, A., Jones, R. M., Ma, T., & Liang, P. Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution. In International Conference on Learning Representations 2022.\n\n[B] Kou, Y., Chen, Z., Chen, Y., & Gu, Q. Benign overfitting in two-layer ReLU convolutional neural networks. In International Conference on Machine Learning (pp. 17615-17659). PMLR 2023.\n\n[C] Mahankali, A. V., Hashimoto, T., & Ma, T. One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention. In The Twelfth International Conference on Learning Representations 2024.", "labeling_timestamp": "2026-01-11T17:29:29.253552", "model": "gpt-5-mini", "label": "Contradicted", "justification": "Section 3's opening sentence explicitly introduces normalization: it names Layer Normalisation and states LN belongs to the family of normalisation layers used in sequence models, so the topic is clearly signalled.", "evidence": "“Several works have highlighted the architectural choice of Layer Normalisation (LN) [44] as a cause of OFE [1, 7, 15]. LN belongs to a family of normalisation (Norm) layers commonly used in sequence models, which normalise a representation vector x ∈ R d across the width dimension independently”", "section": "3 Normalisation Layers and Outlier Features"}
{"claim": "Lines 104-177 begin abruptly with 'In Figure 2' without a transition, leaving the reader unclear about the figure's context.", "claim_type": "presentation", "paper_id": "npJQ6qS4bg", "paper_title": "Understanding and Minimising Outlier Features in Transformer Training", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "YeBeK8zEL0", "reviewer": "Reviewer_Lb1w", "review_text": "Comment: Thank you for your response. However, it appears that the revision did not adequately address the core concerns, instead referencing unhelpful comments from other reviewers. The citations from other evaluations do not substantiate your responses unless they are accompanied by supporting evidence. Consequently, my concerns remain unresolved, and I have retained my original score.\n\n1. **Unsatisfactory Writing Quality**: The manuscript suffers from numerous stylistic and structural weaknesses, making it difficult to follow. Starting in Section 3, from Line 87-103, the paragraph introduces background on normalization but fails to signal this topic in the opening sentence. Line 104-177, abruptly begins with \"In Figure 2\" without a transition from the previous paragraph, leaving the reader unclear about the figure's contect...... Limited by time and energy, the reviewer can not point out all the places that writing can be improved since there are too many. Most of the paragraphs in the document lack a coherent top-down structure.\n\n2. **Insufficient Support for Architectural Contribution**: The \"Outlier Protected transformer block\" is presented as a highlighted innovation, yet its description is confined to a single paragraph (Lines 146-163). For an effective presentation of a new architecture, consider the detailed exposition found in Section 4 of Reference [14] (frequently mentioned by the author). If there are substantial results supporting the proposed architecture, they should be prominently featured, akin to how Tables 2 and 3 are presented in [14]. The lack of a unified section detailing the experimental approach, akin to Section 5 in Reference [14], makes it challenging for the readers to effectively understand the context and validity of your findings. \n\n3. **Insignificant Observation**: My concern regarding the relationship between Outlier Features (OF) and Signal Propagation has not been addressed. You provide several opposing subjective comments but fail to engage with the core issue. Specifically, you claim a significant observation that Signal Propagation relates to OF, yet both measures you discuss (Kurtosis and the Gram matrix) are mathematical functions of the same variable, X. The inherent mathematical relationship between these functions should not be surprising, and you have not clarified why this finding is noteworthy.\n\n4. **Insufficient Fundamental Insight**: The title and abstract of your paper promises a deep understanding of why OFs emerge during training. However, the empirical analysis over optimization choices in Section 5, while useful, does not address this core question from a fundamental perspective. An effective discussion should integrate more profound insights into the nature and implications of OFs within neural network dynamics. To enhance the paper, one possible analysis technique can be associated with the neural tangent kernel or training dynamics. For guidance on conducting this type of rigorous research, I recommend referring to [A,B,C], which illustrate how scholars **deeply** uncover the underlying causes of phenomena during training. \n\nOverall, the manuscript requires significant improvements in writing, experimental design, and technical depth to meet the publication standards.\n\n[A] Kumar, A., Raghunathan, A., Jones, R. M., Ma, T., & Liang, P. Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution. In International Conference on Learning Representations 2022.\n\n[B] Kou, Y., Chen, Z., Chen, Y., & Gu, Q. Benign overfitting in two-layer ReLU convolutional neural networks. In International Conference on Machine Learning (pp. 17615-17659). PMLR 2023.\n\n[C] Mahankali, A. V., Hashimoto, T., & Ma, T. One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention. In The Twelfth International Conference on Learning Representations 2024.", "labeling_timestamp": "2026-01-11T17:29:47.034796", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper introduces Figure 2 with a preceding sentence that explicitly references and contextualises the figure, and the figure caption further explains what is being plotted. The surrounding text continues after the figure, showing the figure is integrated into the discussion rather than presented abruptly or without transition.", "evidence": "We compare these different Norms in Fig 2, where we see that independent of Norm choice, all Pre-Norm transformers incur OFE: the peak kurtosis during training across Norms is over 4 orders of magnitude larger than initialisation.\n\nFigure 2: Kurtosis becomes large (i.e. OFE) when training with different Norms at 130M scale. We plot the residual stream entering\n\n...the 2nd of 6 blocks. Other layers in Fig 14. Having established that removing trainable weights in Norms still results in OFE, the next question we ask is: how does removing standard Norms entirely influence Outlier Feature emergence ?", "section": "3 Normalisation Layers and Outlier Features"}
{"claim": "Most paragraphs lack a coherent top-down structure, reducing overall document readability.", "claim_type": "presentation", "paper_id": "npJQ6qS4bg", "paper_title": "Understanding and Minimising Outlier Features in Transformer Training", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "YeBeK8zEL0", "reviewer": "Reviewer_Lb1w", "review_text": "Comment: Thank you for your response. However, it appears that the revision did not adequately address the core concerns, instead referencing unhelpful comments from other reviewers. The citations from other evaluations do not substantiate your responses unless they are accompanied by supporting evidence. Consequently, my concerns remain unresolved, and I have retained my original score.\n\n1. **Unsatisfactory Writing Quality**: The manuscript suffers from numerous stylistic and structural weaknesses, making it difficult to follow. Starting in Section 3, from Line 87-103, the paragraph introduces background on normalization but fails to signal this topic in the opening sentence. Line 104-177, abruptly begins with \"In Figure 2\" without a transition from the previous paragraph, leaving the reader unclear about the figure's contect...... Limited by time and energy, the reviewer can not point out all the places that writing can be improved since there are too many. Most of the paragraphs in the document lack a coherent top-down structure.\n\n2. **Insufficient Support for Architectural Contribution**: The \"Outlier Protected transformer block\" is presented as a highlighted innovation, yet its description is confined to a single paragraph (Lines 146-163). For an effective presentation of a new architecture, consider the detailed exposition found in Section 4 of Reference [14] (frequently mentioned by the author). If there are substantial results supporting the proposed architecture, they should be prominently featured, akin to how Tables 2 and 3 are presented in [14]. The lack of a unified section detailing the experimental approach, akin to Section 5 in Reference [14], makes it challenging for the readers to effectively understand the context and validity of your findings. \n\n3. **Insignificant Observation**: My concern regarding the relationship between Outlier Features (OF) and Signal Propagation has not been addressed. You provide several opposing subjective comments but fail to engage with the core issue. Specifically, you claim a significant observation that Signal Propagation relates to OF, yet both measures you discuss (Kurtosis and the Gram matrix) are mathematical functions of the same variable, X. The inherent mathematical relationship between these functions should not be surprising, and you have not clarified why this finding is noteworthy.\n\n4. **Insufficient Fundamental Insight**: The title and abstract of your paper promises a deep understanding of why OFs emerge during training. However, the empirical analysis over optimization choices in Section 5, while useful, does not address this core question from a fundamental perspective. An effective discussion should integrate more profound insights into the nature and implications of OFs within neural network dynamics. To enhance the paper, one possible analysis technique can be associated with the neural tangent kernel or training dynamics. For guidance on conducting this type of rigorous research, I recommend referring to [A,B,C], which illustrate how scholars **deeply** uncover the underlying causes of phenomena during training. \n\nOverall, the manuscript requires significant improvements in writing, experimental design, and technical depth to meet the publication standards.\n\n[A] Kumar, A., Raghunathan, A., Jones, R. M., Ma, T., & Liang, P. Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution. In International Conference on Learning Representations 2022.\n\n[B] Kou, Y., Chen, Z., Chen, Y., & Gu, Q. Benign overfitting in two-layer ReLU convolutional neural networks. In International Conference on Machine Learning (pp. 17615-17659). PMLR 2023.\n\n[C] Mahankali, A. V., Hashimoto, T., & Ma, T. One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention. In The Twelfth International Conference on Learning Representations 2024.", "labeling_timestamp": "2026-01-11T17:29:44.258186", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does not comment on its own paragraph-level coherence or overall readability, and the provided excerpts show a conventional, sectioned structure (Abstract, Introduction, numbered sections) rather than evidence that 'most paragraphs lack a coherent top-down structure'. The reviewer’s claim is a subjective quality assessment not addressed or verifiable from the paper text alone.", "evidence": "“Outlier Features (OFs) are neurons whose activation magnitudes significantly exceed the average over a neural network's (NN) width.”\n\n“Overall, we show that OFE can be mitigated relative to standard practices, and highlight key design choices to do so. We start by introducing OFs, and in particular quantitative metrics to measure OFs in Sec 2. In Sec 3, we study the role of normalisation layers for OFE, and find that existing hypotheses do not fully capture the OF phenomenon.”", "section": "Abstract; Introduction (Our contributions paragraph)"}
{"claim": "The 'Outlier Protected transformer block' is described only in a single paragraph (Lines 146-163), providing insufficient architectural detail.", "claim_type": "methodology", "paper_id": "npJQ6qS4bg", "paper_title": "Understanding and Minimising Outlier Features in Transformer Training", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "YeBeK8zEL0", "reviewer": "Reviewer_Lb1w", "review_text": "Comment: Thank you for your response. However, it appears that the revision did not adequately address the core concerns, instead referencing unhelpful comments from other reviewers. The citations from other evaluations do not substantiate your responses unless they are accompanied by supporting evidence. Consequently, my concerns remain unresolved, and I have retained my original score.\n\n1. **Unsatisfactory Writing Quality**: The manuscript suffers from numerous stylistic and structural weaknesses, making it difficult to follow. Starting in Section 3, from Line 87-103, the paragraph introduces background on normalization but fails to signal this topic in the opening sentence. Line 104-177, abruptly begins with \"In Figure 2\" without a transition from the previous paragraph, leaving the reader unclear about the figure's contect...... Limited by time and energy, the reviewer can not point out all the places that writing can be improved since there are too many. Most of the paragraphs in the document lack a coherent top-down structure.\n\n2. **Insufficient Support for Architectural Contribution**: The \"Outlier Protected transformer block\" is presented as a highlighted innovation, yet its description is confined to a single paragraph (Lines 146-163). For an effective presentation of a new architecture, consider the detailed exposition found in Section 4 of Reference [14] (frequently mentioned by the author). If there are substantial results supporting the proposed architecture, they should be prominently featured, akin to how Tables 2 and 3 are presented in [14]. The lack of a unified section detailing the experimental approach, akin to Section 5 in Reference [14], makes it challenging for the readers to effectively understand the context and validity of your findings. \n\n3. **Insignificant Observation**: My concern regarding the relationship between Outlier Features (OF) and Signal Propagation has not been addressed. You provide several opposing subjective comments but fail to engage with the core issue. Specifically, you claim a significant observation that Signal Propagation relates to OF, yet both measures you discuss (Kurtosis and the Gram matrix) are mathematical functions of the same variable, X. The inherent mathematical relationship between these functions should not be surprising, and you have not clarified why this finding is noteworthy.\n\n4. **Insufficient Fundamental Insight**: The title and abstract of your paper promises a deep understanding of why OFs emerge during training. However, the empirical analysis over optimization choices in Section 5, while useful, does not address this core question from a fundamental perspective. An effective discussion should integrate more profound insights into the nature and implications of OFs within neural network dynamics. To enhance the paper, one possible analysis technique can be associated with the neural tangent kernel or training dynamics. For guidance on conducting this type of rigorous research, I recommend referring to [A,B,C], which illustrate how scholars **deeply** uncover the underlying causes of phenomena during training. \n\nOverall, the manuscript requires significant improvements in writing, experimental design, and technical depth to meet the publication standards.\n\n[A] Kumar, A., Raghunathan, A., Jones, R. M., Ma, T., & Liang, P. Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution. In International Conference on Learning Representations 2022.\n\n[B] Kou, Y., Chen, Z., Chen, Y., & Gu, Q. Benign overfitting in two-layer ReLU convolutional neural networks. In International Conference on Machine Learning (pp. 17615-17659). PMLR 2023.\n\n[C] Mahankali, A. V., Hashimoto, T., & Ma, T. One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention. In The Twelfth International Conference on Learning Representations 2024.", "labeling_timestamp": "2026-01-11T17:29:40.571245", "model": "gpt-5-mini", "label": "Supported", "justification": "The main text only briefly introduces the Outlier Protected block in Sec. 3 without giving architectural details in the provided content; no detailed schematic, equations, or layer-wise description are present in the excerpt, consistent with the reviewer's claim of an insufficient single-paragraph description.", "evidence": "\"We proceed to show that removing normalisation through our Outlier Protected transformer block minimises OFs, without loss of convergence speed or training stability compared to standard transformer blocks.\"", "section": "3 Normalisation Layers and Outlier Features"}
{"claim": "The paper lacks a detailed exposition of the proposed architecture comparable to Section 4 of Reference [14].", "claim_type": "methodology", "paper_id": "npJQ6qS4bg", "paper_title": "Understanding and Minimising Outlier Features in Transformer Training", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "YeBeK8zEL0", "reviewer": "Reviewer_Lb1w", "review_text": "Comment: Thank you for your response. However, it appears that the revision did not adequately address the core concerns, instead referencing unhelpful comments from other reviewers. The citations from other evaluations do not substantiate your responses unless they are accompanied by supporting evidence. Consequently, my concerns remain unresolved, and I have retained my original score.\n\n1. **Unsatisfactory Writing Quality**: The manuscript suffers from numerous stylistic and structural weaknesses, making it difficult to follow. Starting in Section 3, from Line 87-103, the paragraph introduces background on normalization but fails to signal this topic in the opening sentence. Line 104-177, abruptly begins with \"In Figure 2\" without a transition from the previous paragraph, leaving the reader unclear about the figure's contect...... Limited by time and energy, the reviewer can not point out all the places that writing can be improved since there are too many. Most of the paragraphs in the document lack a coherent top-down structure.\n\n2. **Insufficient Support for Architectural Contribution**: The \"Outlier Protected transformer block\" is presented as a highlighted innovation, yet its description is confined to a single paragraph (Lines 146-163). For an effective presentation of a new architecture, consider the detailed exposition found in Section 4 of Reference [14] (frequently mentioned by the author). If there are substantial results supporting the proposed architecture, they should be prominently featured, akin to how Tables 2 and 3 are presented in [14]. The lack of a unified section detailing the experimental approach, akin to Section 5 in Reference [14], makes it challenging for the readers to effectively understand the context and validity of your findings. \n\n3. **Insignificant Observation**: My concern regarding the relationship between Outlier Features (OF) and Signal Propagation has not been addressed. You provide several opposing subjective comments but fail to engage with the core issue. Specifically, you claim a significant observation that Signal Propagation relates to OF, yet both measures you discuss (Kurtosis and the Gram matrix) are mathematical functions of the same variable, X. The inherent mathematical relationship between these functions should not be surprising, and you have not clarified why this finding is noteworthy.\n\n4. **Insufficient Fundamental Insight**: The title and abstract of your paper promises a deep understanding of why OFs emerge during training. However, the empirical analysis over optimization choices in Section 5, while useful, does not address this core question from a fundamental perspective. An effective discussion should integrate more profound insights into the nature and implications of OFs within neural network dynamics. To enhance the paper, one possible analysis technique can be associated with the neural tangent kernel or training dynamics. For guidance on conducting this type of rigorous research, I recommend referring to [A,B,C], which illustrate how scholars **deeply** uncover the underlying causes of phenomena during training. \n\nOverall, the manuscript requires significant improvements in writing, experimental design, and technical depth to meet the publication standards.\n\n[A] Kumar, A., Raghunathan, A., Jones, R. M., Ma, T., & Liang, P. Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution. In International Conference on Learning Representations 2022.\n\n[B] Kou, Y., Chen, Z., Chen, Y., & Gu, Q. Benign overfitting in two-layer ReLU convolutional neural networks. In International Conference on Machine Learning (pp. 17615-17659). PMLR 2023.\n\n[C] Mahankali, A. V., Hashimoto, T., & Ma, T. One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention. In The Twelfth International Conference on Learning Representations 2024.", "labeling_timestamp": "2026-01-11T17:29:48.352555", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does describe a proposed architecture (the Outlier Protected block) in the main text and refers to implementation/transformer block details in Appendix A.1, but the claim requires a comparison to 'Section 4 of Reference [14]'. The provided paper content does not include Reference [14] or its Section 4, so there is insufficient information to determine whether the exposition here is comparable to that specific section of Ref [14].", "evidence": "Abstract: \"...we introduce a novel unnormalised transformer block, the Outlier Protected block...\"; Sec 3: \"Having established that removing trainable weights in Norms still results in OFE, the next question we ask is: how does removing standard Norms entirely influence Outlier Feature emergence ?\"; \"...to answer the above question, we must first review different hypotheses for the benefits of Norms in transformer training dynamics in order to motivate a novel transformer block that matches the Pre-Norm block in convergence speed, while eschewing standard Norm layers.\"; \"...in App A.1 we discuss transformer blocks...\"", "section": "Abstract; Section 3; Appendix A.1"}
{"claim": "There is no unified section detailing the experimental approach akin to Section 5 of Reference [14], hindering understanding of context and validity.", "claim_type": "methodology", "paper_id": "npJQ6qS4bg", "paper_title": "Understanding and Minimising Outlier Features in Transformer Training", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "YeBeK8zEL0", "reviewer": "Reviewer_Lb1w", "review_text": "Comment: Thank you for your response. However, it appears that the revision did not adequately address the core concerns, instead referencing unhelpful comments from other reviewers. The citations from other evaluations do not substantiate your responses unless they are accompanied by supporting evidence. Consequently, my concerns remain unresolved, and I have retained my original score.\n\n1. **Unsatisfactory Writing Quality**: The manuscript suffers from numerous stylistic and structural weaknesses, making it difficult to follow. Starting in Section 3, from Line 87-103, the paragraph introduces background on normalization but fails to signal this topic in the opening sentence. Line 104-177, abruptly begins with \"In Figure 2\" without a transition from the previous paragraph, leaving the reader unclear about the figure's contect...... Limited by time and energy, the reviewer can not point out all the places that writing can be improved since there are too many. Most of the paragraphs in the document lack a coherent top-down structure.\n\n2. **Insufficient Support for Architectural Contribution**: The \"Outlier Protected transformer block\" is presented as a highlighted innovation, yet its description is confined to a single paragraph (Lines 146-163). For an effective presentation of a new architecture, consider the detailed exposition found in Section 4 of Reference [14] (frequently mentioned by the author). If there are substantial results supporting the proposed architecture, they should be prominently featured, akin to how Tables 2 and 3 are presented in [14]. The lack of a unified section detailing the experimental approach, akin to Section 5 in Reference [14], makes it challenging for the readers to effectively understand the context and validity of your findings. \n\n3. **Insignificant Observation**: My concern regarding the relationship between Outlier Features (OF) and Signal Propagation has not been addressed. You provide several opposing subjective comments but fail to engage with the core issue. Specifically, you claim a significant observation that Signal Propagation relates to OF, yet both measures you discuss (Kurtosis and the Gram matrix) are mathematical functions of the same variable, X. The inherent mathematical relationship between these functions should not be surprising, and you have not clarified why this finding is noteworthy.\n\n4. **Insufficient Fundamental Insight**: The title and abstract of your paper promises a deep understanding of why OFs emerge during training. However, the empirical analysis over optimization choices in Section 5, while useful, does not address this core question from a fundamental perspective. An effective discussion should integrate more profound insights into the nature and implications of OFs within neural network dynamics. To enhance the paper, one possible analysis technique can be associated with the neural tangent kernel or training dynamics. For guidance on conducting this type of rigorous research, I recommend referring to [A,B,C], which illustrate how scholars **deeply** uncover the underlying causes of phenomena during training. \n\nOverall, the manuscript requires significant improvements in writing, experimental design, and technical depth to meet the publication standards.\n\n[A] Kumar, A., Raghunathan, A., Jones, R. M., Ma, T., & Liang, P. Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution. In International Conference on Learning Representations 2022.\n\n[B] Kou, Y., Chen, Z., Chen, Y., & Gu, Q. Benign overfitting in two-layer ReLU convolutional neural networks. In International Conference on Machine Learning (pp. 17615-17659). PMLR 2023.\n\n[C] Mahankali, A. V., Hashimoto, T., & Ma, T. One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention. In The Twelfth International Conference on Learning Representations 2024.", "labeling_timestamp": "2026-01-11T17:29:53.327330", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper does provide a unified experimental description: an \"Experimental Setup\" paragraph appears in Section 2 outlining tasks, datasets, default architecture, optimiser, scales, and references to App D and E for further experimental details. Thus the claim that there is no unified section is false.", "evidence": "\"Experimental Setup Throughout this work, we train transformers on the next-token language modelling task, and study OFs, on a range of datasets, including: 1) CodeParrot, 5 2) Languini Books [39], 3) BookCorpus [40] and English Wikipedia, 6 and 4) FineWeb-Edu [41]. ... In terms of architecture and optimiser, our default choices are the Pre-Norm transformer (App A.1) and AdamW [43] respectively ... Our default architecture scale has width d = 768 and 6 layers, giving around 130M parameters, but we demonstrate our findings continue to hold at larger scales (up to 7B parameters) in Secs 3 and 6. ... Further experimental details and results beyond the main paper can be found in Apps D and E respectively.\"", "section": "2 Problem Setting (Experimental Setup paragraph)"}
{"claim": "The paper does not prominently present substantial results supporting the proposed architecture, similar to how Tables 2 and 3 are presented in Reference [14].", "claim_type": "experimental", "paper_id": "npJQ6qS4bg", "paper_title": "Understanding and Minimising Outlier Features in Transformer Training", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "YeBeK8zEL0", "reviewer": "Reviewer_Lb1w", "review_text": "Comment: Thank you for your response. However, it appears that the revision did not adequately address the core concerns, instead referencing unhelpful comments from other reviewers. The citations from other evaluations do not substantiate your responses unless they are accompanied by supporting evidence. Consequently, my concerns remain unresolved, and I have retained my original score.\n\n1. **Unsatisfactory Writing Quality**: The manuscript suffers from numerous stylistic and structural weaknesses, making it difficult to follow. Starting in Section 3, from Line 87-103, the paragraph introduces background on normalization but fails to signal this topic in the opening sentence. Line 104-177, abruptly begins with \"In Figure 2\" without a transition from the previous paragraph, leaving the reader unclear about the figure's contect...... Limited by time and energy, the reviewer can not point out all the places that writing can be improved since there are too many. Most of the paragraphs in the document lack a coherent top-down structure.\n\n2. **Insufficient Support for Architectural Contribution**: The \"Outlier Protected transformer block\" is presented as a highlighted innovation, yet its description is confined to a single paragraph (Lines 146-163). For an effective presentation of a new architecture, consider the detailed exposition found in Section 4 of Reference [14] (frequently mentioned by the author). If there are substantial results supporting the proposed architecture, they should be prominently featured, akin to how Tables 2 and 3 are presented in [14]. The lack of a unified section detailing the experimental approach, akin to Section 5 in Reference [14], makes it challenging for the readers to effectively understand the context and validity of your findings. \n\n3. **Insignificant Observation**: My concern regarding the relationship between Outlier Features (OF) and Signal Propagation has not been addressed. You provide several opposing subjective comments but fail to engage with the core issue. Specifically, you claim a significant observation that Signal Propagation relates to OF, yet both measures you discuss (Kurtosis and the Gram matrix) are mathematical functions of the same variable, X. The inherent mathematical relationship between these functions should not be surprising, and you have not clarified why this finding is noteworthy.\n\n4. **Insufficient Fundamental Insight**: The title and abstract of your paper promises a deep understanding of why OFs emerge during training. However, the empirical analysis over optimization choices in Section 5, while useful, does not address this core question from a fundamental perspective. An effective discussion should integrate more profound insights into the nature and implications of OFs within neural network dynamics. To enhance the paper, one possible analysis technique can be associated with the neural tangent kernel or training dynamics. For guidance on conducting this type of rigorous research, I recommend referring to [A,B,C], which illustrate how scholars **deeply** uncover the underlying causes of phenomena during training. \n\nOverall, the manuscript requires significant improvements in writing, experimental design, and technical depth to meet the publication standards.\n\n[A] Kumar, A., Raghunathan, A., Jones, R. M., Ma, T., & Liang, P. Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution. In International Conference on Learning Representations 2022.\n\n[B] Kou, Y., Chen, Z., Chen, Y., & Gu, Q. Benign overfitting in two-layer ReLU convolutional neural networks. In International Conference on Machine Learning (pp. 17615-17659). PMLR 2023.\n\n[C] Mahankali, A. V., Hashimoto, T., & Ma, T. One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention. In The Twelfth International Conference on Learning Representations 2024.", "labeling_timestamp": "2026-01-11T17:30:13.964552", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper does present prominent, quantitative results supporting the proposed Outlier Protected (OP) block and its combination with a non-diagonal preconditioner (SOAP). The Abstract reports concrete int8 quantisation perplexity numbers for OP/SOAP versus a baseline, Sec. 3 claims the OP block minimises OFs without loss of convergence speed, and Sec. 6 is cited as demonstrating performance up to 7B parameters—indicating substantive empirical support for the proposed architecture.", "evidence": "Abstract: \"As highlights, we introduce a novel unnormalised transformer block, the Outlier Protected block, and present a previously unknown benefit of non-diagonal preconditioning optimisers, finding both approaches to significantly reduce OFs and improve quantisation without compromising convergence speed, at scales of up to 7B parameters. Notably, our combination of OP block and non-diagonal preconditioner (SOAP) achieves 14.87 weight-and-activation int8 perplexity (from 14.71 in standard precision), compared to 63.4 int8 perplexity (from 16.00) with a default OF-prone combination of Pre-Norm model and Adam, when quantising OPT-125m models post-training.\"  Also: \"We proceed to show that removing normalisation through our Outlier Protected transformer block minimises OFs, without loss of convergence speed or training stability compared to standard transformer blocks.\"", "section": "Abstract; Sec 3; Sec 6"}
{"claim": "The claimed relationship between Signal Propagation and Outlier Features is unsurprising because both Kurtosis and the Gram matrix are functions of the same variable X.", "claim_type": "novelty", "paper_id": "npJQ6qS4bg", "paper_title": "Understanding and Minimising Outlier Features in Transformer Training", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "YeBeK8zEL0", "reviewer": "Reviewer_Lb1w", "review_text": "Comment: Thank you for your response. However, it appears that the revision did not adequately address the core concerns, instead referencing unhelpful comments from other reviewers. The citations from other evaluations do not substantiate your responses unless they are accompanied by supporting evidence. Consequently, my concerns remain unresolved, and I have retained my original score.\n\n1. **Unsatisfactory Writing Quality**: The manuscript suffers from numerous stylistic and structural weaknesses, making it difficult to follow. Starting in Section 3, from Line 87-103, the paragraph introduces background on normalization but fails to signal this topic in the opening sentence. Line 104-177, abruptly begins with \"In Figure 2\" without a transition from the previous paragraph, leaving the reader unclear about the figure's contect...... Limited by time and energy, the reviewer can not point out all the places that writing can be improved since there are too many. Most of the paragraphs in the document lack a coherent top-down structure.\n\n2. **Insufficient Support for Architectural Contribution**: The \"Outlier Protected transformer block\" is presented as a highlighted innovation, yet its description is confined to a single paragraph (Lines 146-163). For an effective presentation of a new architecture, consider the detailed exposition found in Section 4 of Reference [14] (frequently mentioned by the author). If there are substantial results supporting the proposed architecture, they should be prominently featured, akin to how Tables 2 and 3 are presented in [14]. The lack of a unified section detailing the experimental approach, akin to Section 5 in Reference [14], makes it challenging for the readers to effectively understand the context and validity of your findings. \n\n3. **Insignificant Observation**: My concern regarding the relationship between Outlier Features (OF) and Signal Propagation has not been addressed. You provide several opposing subjective comments but fail to engage with the core issue. Specifically, you claim a significant observation that Signal Propagation relates to OF, yet both measures you discuss (Kurtosis and the Gram matrix) are mathematical functions of the same variable, X. The inherent mathematical relationship between these functions should not be surprising, and you have not clarified why this finding is noteworthy.\n\n4. **Insufficient Fundamental Insight**: The title and abstract of your paper promises a deep understanding of why OFs emerge during training. However, the empirical analysis over optimization choices in Section 5, while useful, does not address this core question from a fundamental perspective. An effective discussion should integrate more profound insights into the nature and implications of OFs within neural network dynamics. To enhance the paper, one possible analysis technique can be associated with the neural tangent kernel or training dynamics. For guidance on conducting this type of rigorous research, I recommend referring to [A,B,C], which illustrate how scholars **deeply** uncover the underlying causes of phenomena during training. \n\nOverall, the manuscript requires significant improvements in writing, experimental design, and technical depth to meet the publication standards.\n\n[A] Kumar, A., Raghunathan, A., Jones, R. M., Ma, T., & Liang, P. Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution. In International Conference on Learning Representations 2022.\n\n[B] Kou, Y., Chen, Z., Chen, Y., & Gu, Q. Benign overfitting in two-layer ReLU convolutional neural networks. In International Conference on Machine Learning (pp. 17615-17659). PMLR 2023.\n\n[C] Mahankali, A. V., Hashimoto, T., & Ma, T. One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention. In The Twelfth International Conference on Learning Representations 2024.", "labeling_timestamp": "2026-01-11T17:30:18.927356", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does link kurtosis (Kurt(X)) to the activation matrix X and states that this formulation allows linking OFs and signal propagation (Sec. 2 and Sec. 4). However the paper excerpt does not mention the Gram matrix or claim that the Gram matrix is used to explain the Signal Propagation–OF relationship. Thus the reviewer’s rationale (that the relationship is unsurprising because both Kurtosis and the Gram matrix are functions of the same X) is not supported or addressed by the paper as given.", "evidence": "\"Consider an activation matrix X ∈ R n × d obtained from some neural network layer...\" (Sec. 2)\n\n\"Let Kurt ( X ) be the ratio of the fourth moment m4 to the squared second moment m2 over the empirical distribution of s...\" (Sec. 2)\n\n\"Variants of Kurt ( X ) have previously been proposed..., but our formulation in Eq (1) aggregates activations over inputs first, which allows us to link OFs and signal propagation in Sec 4.\" (Sec. 2)\n\n\"In Sec 4, we consolidate our findings by identifying signal propagation as an important object that can predict OFs during training, and that choices that improve signal propagation during training also minimise OFE.\" (Introduction / Contributions)", "section": "Sec. 2 and Sec. 4 (as cited in the Introduction)"}
{"claim": "The authors responded with subjective opposing comments rather than directly engaging the core issue about the relationship between OF and Signal Propagation.", "claim_type": "subjective", "paper_id": "npJQ6qS4bg", "paper_title": "Understanding and Minimising Outlier Features in Transformer Training", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "YeBeK8zEL0", "reviewer": "Reviewer_Lb1w", "review_text": "Comment: Thank you for your response. However, it appears that the revision did not adequately address the core concerns, instead referencing unhelpful comments from other reviewers. The citations from other evaluations do not substantiate your responses unless they are accompanied by supporting evidence. Consequently, my concerns remain unresolved, and I have retained my original score.\n\n1. **Unsatisfactory Writing Quality**: The manuscript suffers from numerous stylistic and structural weaknesses, making it difficult to follow. Starting in Section 3, from Line 87-103, the paragraph introduces background on normalization but fails to signal this topic in the opening sentence. Line 104-177, abruptly begins with \"In Figure 2\" without a transition from the previous paragraph, leaving the reader unclear about the figure's contect...... Limited by time and energy, the reviewer can not point out all the places that writing can be improved since there are too many. Most of the paragraphs in the document lack a coherent top-down structure.\n\n2. **Insufficient Support for Architectural Contribution**: The \"Outlier Protected transformer block\" is presented as a highlighted innovation, yet its description is confined to a single paragraph (Lines 146-163). For an effective presentation of a new architecture, consider the detailed exposition found in Section 4 of Reference [14] (frequently mentioned by the author). If there are substantial results supporting the proposed architecture, they should be prominently featured, akin to how Tables 2 and 3 are presented in [14]. The lack of a unified section detailing the experimental approach, akin to Section 5 in Reference [14], makes it challenging for the readers to effectively understand the context and validity of your findings. \n\n3. **Insignificant Observation**: My concern regarding the relationship between Outlier Features (OF) and Signal Propagation has not been addressed. You provide several opposing subjective comments but fail to engage with the core issue. Specifically, you claim a significant observation that Signal Propagation relates to OF, yet both measures you discuss (Kurtosis and the Gram matrix) are mathematical functions of the same variable, X. The inherent mathematical relationship between these functions should not be surprising, and you have not clarified why this finding is noteworthy.\n\n4. **Insufficient Fundamental Insight**: The title and abstract of your paper promises a deep understanding of why OFs emerge during training. However, the empirical analysis over optimization choices in Section 5, while useful, does not address this core question from a fundamental perspective. An effective discussion should integrate more profound insights into the nature and implications of OFs within neural network dynamics. To enhance the paper, one possible analysis technique can be associated with the neural tangent kernel or training dynamics. For guidance on conducting this type of rigorous research, I recommend referring to [A,B,C], which illustrate how scholars **deeply** uncover the underlying causes of phenomena during training. \n\nOverall, the manuscript requires significant improvements in writing, experimental design, and technical depth to meet the publication standards.\n\n[A] Kumar, A., Raghunathan, A., Jones, R. M., Ma, T., & Liang, P. Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution. In International Conference on Learning Representations 2022.\n\n[B] Kou, Y., Chen, Z., Chen, Y., & Gu, Q. Benign overfitting in two-layer ReLU convolutional neural networks. In International Conference on Machine Learning (pp. 17615-17659). PMLR 2023.\n\n[C] Mahankali, A. V., Hashimoto, T., & Ma, T. One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention. In The Twelfth International Conference on Learning Representations 2024.", "labeling_timestamp": "2026-01-11T17:30:19.051823", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states that it studies and links signal propagation to Outlier Feature Emergence and devotes a section to consolidating findings around signal propagation predicting OFs; it therefore directly engages the core issue rather than only offering subjective opposing comments.", "evidence": "\"In Sec 4, we consolidate our findings by identifying signal propagation as an important object that can predict OFs during training, and that choices that improve signal propagation during training also minimise OFE.\"", "section": "Introduction / Sec 4 (as referenced in Introduction); Sec 3 (Normalisation Layers and Outlier Features)"}
{"claim": "The title and abstract promise a deep understanding of why Outlier Features emerge, but the paper fails to provide that fundamental explanation.", "claim_type": "subjective", "paper_id": "npJQ6qS4bg", "paper_title": "Understanding and Minimising Outlier Features in Transformer Training", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "YeBeK8zEL0", "reviewer": "Reviewer_Lb1w", "review_text": "Comment: Thank you for your response. However, it appears that the revision did not adequately address the core concerns, instead referencing unhelpful comments from other reviewers. The citations from other evaluations do not substantiate your responses unless they are accompanied by supporting evidence. Consequently, my concerns remain unresolved, and I have retained my original score.\n\n1. **Unsatisfactory Writing Quality**: The manuscript suffers from numerous stylistic and structural weaknesses, making it difficult to follow. Starting in Section 3, from Line 87-103, the paragraph introduces background on normalization but fails to signal this topic in the opening sentence. Line 104-177, abruptly begins with \"In Figure 2\" without a transition from the previous paragraph, leaving the reader unclear about the figure's contect...... Limited by time and energy, the reviewer can not point out all the places that writing can be improved since there are too many. Most of the paragraphs in the document lack a coherent top-down structure.\n\n2. **Insufficient Support for Architectural Contribution**: The \"Outlier Protected transformer block\" is presented as a highlighted innovation, yet its description is confined to a single paragraph (Lines 146-163). For an effective presentation of a new architecture, consider the detailed exposition found in Section 4 of Reference [14] (frequently mentioned by the author). If there are substantial results supporting the proposed architecture, they should be prominently featured, akin to how Tables 2 and 3 are presented in [14]. The lack of a unified section detailing the experimental approach, akin to Section 5 in Reference [14], makes it challenging for the readers to effectively understand the context and validity of your findings. \n\n3. **Insignificant Observation**: My concern regarding the relationship between Outlier Features (OF) and Signal Propagation has not been addressed. You provide several opposing subjective comments but fail to engage with the core issue. Specifically, you claim a significant observation that Signal Propagation relates to OF, yet both measures you discuss (Kurtosis and the Gram matrix) are mathematical functions of the same variable, X. The inherent mathematical relationship between these functions should not be surprising, and you have not clarified why this finding is noteworthy.\n\n4. **Insufficient Fundamental Insight**: The title and abstract of your paper promises a deep understanding of why OFs emerge during training. However, the empirical analysis over optimization choices in Section 5, while useful, does not address this core question from a fundamental perspective. An effective discussion should integrate more profound insights into the nature and implications of OFs within neural network dynamics. To enhance the paper, one possible analysis technique can be associated with the neural tangent kernel or training dynamics. For guidance on conducting this type of rigorous research, I recommend referring to [A,B,C], which illustrate how scholars **deeply** uncover the underlying causes of phenomena during training. \n\nOverall, the manuscript requires significant improvements in writing, experimental design, and technical depth to meet the publication standards.\n\n[A] Kumar, A., Raghunathan, A., Jones, R. M., Ma, T., & Liang, P. Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution. In International Conference on Learning Representations 2022.\n\n[B] Kou, Y., Chen, Z., Chen, Y., & Gu, Q. Benign overfitting in two-layer ReLU convolutional neural networks. In International Conference on Machine Learning (pp. 17615-17659). PMLR 2023.\n\n[C] Mahankali, A. V., Hashimoto, T., & Ma, T. One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention. In The Twelfth International Conference on Learning Representations 2024.", "labeling_timestamp": "2026-01-11T17:30:29.317555", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states it investigates causes of Outlier Feature Emergence and reports mechanistic insights (e.g. identifying signal propagation and optimisation choices as predictors/causes), so the reviewer's claim that it \"fails to provide that fundamental explanation\" contradicts the paper's own stated contributions and findings.", "evidence": "Abstract: \"Our work focuses on the above questions, first identifying several quantitative metrics, such as the kurtosis over neuron activation norms, to measure OFs. With these metrics, we study how architectural and optimisation choices influence OFs, and provide practical insights to minimise OFs during training.\" Introduction: \"we tackle OFE from two related angles: by (1) proposing interventions to minimise OFE ... using insights motivated through (2) enhancing our understanding of why OFs appear during training.\" Later summary: \"In Sec 4, we consolidate our findings by identifying signal propagation as an important object that can predict OFs during training, and that choices that improve signal propagation during training also minimise OFE.\"", "section": "Abstract / Introduction / Section 4"}
{"claim": "The empirical analysis over optimization choices in Section 5 does not provide fundamental insights into why Outlier Features emerge during training.", "claim_type": "subjective", "paper_id": "npJQ6qS4bg", "paper_title": "Understanding and Minimising Outlier Features in Transformer Training", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "YeBeK8zEL0", "reviewer": "Reviewer_Lb1w", "review_text": "Comment: Thank you for your response. However, it appears that the revision did not adequately address the core concerns, instead referencing unhelpful comments from other reviewers. The citations from other evaluations do not substantiate your responses unless they are accompanied by supporting evidence. Consequently, my concerns remain unresolved, and I have retained my original score.\n\n1. **Unsatisfactory Writing Quality**: The manuscript suffers from numerous stylistic and structural weaknesses, making it difficult to follow. Starting in Section 3, from Line 87-103, the paragraph introduces background on normalization but fails to signal this topic in the opening sentence. Line 104-177, abruptly begins with \"In Figure 2\" without a transition from the previous paragraph, leaving the reader unclear about the figure's contect...... Limited by time and energy, the reviewer can not point out all the places that writing can be improved since there are too many. Most of the paragraphs in the document lack a coherent top-down structure.\n\n2. **Insufficient Support for Architectural Contribution**: The \"Outlier Protected transformer block\" is presented as a highlighted innovation, yet its description is confined to a single paragraph (Lines 146-163). For an effective presentation of a new architecture, consider the detailed exposition found in Section 4 of Reference [14] (frequently mentioned by the author). If there are substantial results supporting the proposed architecture, they should be prominently featured, akin to how Tables 2 and 3 are presented in [14]. The lack of a unified section detailing the experimental approach, akin to Section 5 in Reference [14], makes it challenging for the readers to effectively understand the context and validity of your findings. \n\n3. **Insignificant Observation**: My concern regarding the relationship between Outlier Features (OF) and Signal Propagation has not been addressed. You provide several opposing subjective comments but fail to engage with the core issue. Specifically, you claim a significant observation that Signal Propagation relates to OF, yet both measures you discuss (Kurtosis and the Gram matrix) are mathematical functions of the same variable, X. The inherent mathematical relationship between these functions should not be surprising, and you have not clarified why this finding is noteworthy.\n\n4. **Insufficient Fundamental Insight**: The title and abstract of your paper promises a deep understanding of why OFs emerge during training. However, the empirical analysis over optimization choices in Section 5, while useful, does not address this core question from a fundamental perspective. An effective discussion should integrate more profound insights into the nature and implications of OFs within neural network dynamics. To enhance the paper, one possible analysis technique can be associated with the neural tangent kernel or training dynamics. For guidance on conducting this type of rigorous research, I recommend referring to [A,B,C], which illustrate how scholars **deeply** uncover the underlying causes of phenomena during training. \n\nOverall, the manuscript requires significant improvements in writing, experimental design, and technical depth to meet the publication standards.\n\n[A] Kumar, A., Raghunathan, A., Jones, R. M., Ma, T., & Liang, P. Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution. In International Conference on Learning Representations 2022.\n\n[B] Kou, Y., Chen, Z., Chen, Y., & Gu, Q. Benign overfitting in two-layer ReLU convolutional neural networks. In International Conference on Machine Learning (pp. 17615-17659). PMLR 2023.\n\n[C] Mahankali, A. V., Hashimoto, T., & Ma, T. One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention. In The Twelfth International Conference on Learning Representations 2024.", "labeling_timestamp": "2026-01-11T17:30:24.047635", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes Section 5 as an empirical study of optimisation hyperparameters (not as providing a fundamental mechanistic explanation), while it explicitly locates the core explanatory insight about why OFs appear in Section 4 (signal propagation). Thus Section 5 provides empirical/practical findings (e.g. on adaptive learning rates) rather than a fundamental explanation of OF emergence.", "evidence": "“In Sec 5, we consider optimisation hyperparameters, and highlight the importance of large diagonal adaptive learning rates for OFE.”\n\n“In Sec 4, we consolidate our findings by identifying signal propagation as an important object that can predict OFs during training, and that choices that improve signal propagation during training also minimise OFE.”", "section": "Section 5 (optimisation hyperparameters) and Section 4 (signal propagation)"}
{"claim": "The paper lacks rigorous analytical approaches, such as neural tangent kernel or training-dynamics analyses, to uncover underlying causes of Outlier Features.", "claim_type": "methodology", "paper_id": "npJQ6qS4bg", "paper_title": "Understanding and Minimising Outlier Features in Transformer Training", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "YeBeK8zEL0", "reviewer": "Reviewer_Lb1w", "review_text": "Comment: Thank you for your response. However, it appears that the revision did not adequately address the core concerns, instead referencing unhelpful comments from other reviewers. The citations from other evaluations do not substantiate your responses unless they are accompanied by supporting evidence. Consequently, my concerns remain unresolved, and I have retained my original score.\n\n1. **Unsatisfactory Writing Quality**: The manuscript suffers from numerous stylistic and structural weaknesses, making it difficult to follow. Starting in Section 3, from Line 87-103, the paragraph introduces background on normalization but fails to signal this topic in the opening sentence. Line 104-177, abruptly begins with \"In Figure 2\" without a transition from the previous paragraph, leaving the reader unclear about the figure's contect...... Limited by time and energy, the reviewer can not point out all the places that writing can be improved since there are too many. Most of the paragraphs in the document lack a coherent top-down structure.\n\n2. **Insufficient Support for Architectural Contribution**: The \"Outlier Protected transformer block\" is presented as a highlighted innovation, yet its description is confined to a single paragraph (Lines 146-163). For an effective presentation of a new architecture, consider the detailed exposition found in Section 4 of Reference [14] (frequently mentioned by the author). If there are substantial results supporting the proposed architecture, they should be prominently featured, akin to how Tables 2 and 3 are presented in [14]. The lack of a unified section detailing the experimental approach, akin to Section 5 in Reference [14], makes it challenging for the readers to effectively understand the context and validity of your findings. \n\n3. **Insignificant Observation**: My concern regarding the relationship between Outlier Features (OF) and Signal Propagation has not been addressed. You provide several opposing subjective comments but fail to engage with the core issue. Specifically, you claim a significant observation that Signal Propagation relates to OF, yet both measures you discuss (Kurtosis and the Gram matrix) are mathematical functions of the same variable, X. The inherent mathematical relationship between these functions should not be surprising, and you have not clarified why this finding is noteworthy.\n\n4. **Insufficient Fundamental Insight**: The title and abstract of your paper promises a deep understanding of why OFs emerge during training. However, the empirical analysis over optimization choices in Section 5, while useful, does not address this core question from a fundamental perspective. An effective discussion should integrate more profound insights into the nature and implications of OFs within neural network dynamics. To enhance the paper, one possible analysis technique can be associated with the neural tangent kernel or training dynamics. For guidance on conducting this type of rigorous research, I recommend referring to [A,B,C], which illustrate how scholars **deeply** uncover the underlying causes of phenomena during training. \n\nOverall, the manuscript requires significant improvements in writing, experimental design, and technical depth to meet the publication standards.\n\n[A] Kumar, A., Raghunathan, A., Jones, R. M., Ma, T., & Liang, P. Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution. In International Conference on Learning Representations 2022.\n\n[B] Kou, Y., Chen, Z., Chen, Y., & Gu, Q. Benign overfitting in two-layer ReLU convolutional neural networks. In International Conference on Machine Learning (pp. 17615-17659). PMLR 2023.\n\n[C] Mahankali, A. V., Hashimoto, T., & Ma, T. One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention. In The Twelfth International Conference on Learning Representations 2024.", "labeling_timestamp": "2026-01-11T17:30:37.591861", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper provides some analytical discussion of training dynamics (notably identifying signal propagation as an object that can predict OFs in Sec. 4) but does not present more formal analytical frameworks like Neural Tangent Kernel (NTK) analyses or detailed training-dynamics derivations. Thus the reviewer’s claim is partially correct: rigorous NTK-style analysis is absent, but the paper does include some training-dynamics-related analysis (signal propagation).", "evidence": "1) \"we consolidate our findings by identifying signal propagation as an important object that can predict OFs during training\"; 2) \"Though progress has been made [1, 13, 14, 10, 15], the mechanisms behind OFE remain largely unknown.\"", "section": "Introduction / Sec 4 (summary in Introduction)"}
{"claim": "There are too many writing issues throughout the document for the reviewer to enumerate exhaustively, indicating pervasive presentation problems.", "claim_type": "presentation", "paper_id": "npJQ6qS4bg", "paper_title": "Understanding and Minimising Outlier Features in Transformer Training", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "YeBeK8zEL0", "reviewer": "Reviewer_Lb1w", "review_text": "Comment: Thank you for your response. However, it appears that the revision did not adequately address the core concerns, instead referencing unhelpful comments from other reviewers. The citations from other evaluations do not substantiate your responses unless they are accompanied by supporting evidence. Consequently, my concerns remain unresolved, and I have retained my original score.\n\n1. **Unsatisfactory Writing Quality**: The manuscript suffers from numerous stylistic and structural weaknesses, making it difficult to follow. Starting in Section 3, from Line 87-103, the paragraph introduces background on normalization but fails to signal this topic in the opening sentence. Line 104-177, abruptly begins with \"In Figure 2\" without a transition from the previous paragraph, leaving the reader unclear about the figure's contect...... Limited by time and energy, the reviewer can not point out all the places that writing can be improved since there are too many. Most of the paragraphs in the document lack a coherent top-down structure.\n\n2. **Insufficient Support for Architectural Contribution**: The \"Outlier Protected transformer block\" is presented as a highlighted innovation, yet its description is confined to a single paragraph (Lines 146-163). For an effective presentation of a new architecture, consider the detailed exposition found in Section 4 of Reference [14] (frequently mentioned by the author). If there are substantial results supporting the proposed architecture, they should be prominently featured, akin to how Tables 2 and 3 are presented in [14]. The lack of a unified section detailing the experimental approach, akin to Section 5 in Reference [14], makes it challenging for the readers to effectively understand the context and validity of your findings. \n\n3. **Insignificant Observation**: My concern regarding the relationship between Outlier Features (OF) and Signal Propagation has not been addressed. You provide several opposing subjective comments but fail to engage with the core issue. Specifically, you claim a significant observation that Signal Propagation relates to OF, yet both measures you discuss (Kurtosis and the Gram matrix) are mathematical functions of the same variable, X. The inherent mathematical relationship between these functions should not be surprising, and you have not clarified why this finding is noteworthy.\n\n4. **Insufficient Fundamental Insight**: The title and abstract of your paper promises a deep understanding of why OFs emerge during training. However, the empirical analysis over optimization choices in Section 5, while useful, does not address this core question from a fundamental perspective. An effective discussion should integrate more profound insights into the nature and implications of OFs within neural network dynamics. To enhance the paper, one possible analysis technique can be associated with the neural tangent kernel or training dynamics. For guidance on conducting this type of rigorous research, I recommend referring to [A,B,C], which illustrate how scholars **deeply** uncover the underlying causes of phenomena during training. \n\nOverall, the manuscript requires significant improvements in writing, experimental design, and technical depth to meet the publication standards.\n\n[A] Kumar, A., Raghunathan, A., Jones, R. M., Ma, T., & Liang, P. Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution. In International Conference on Learning Representations 2022.\n\n[B] Kou, Y., Chen, Z., Chen, Y., & Gu, Q. Benign overfitting in two-layer ReLU convolutional neural networks. In International Conference on Machine Learning (pp. 17615-17659). PMLR 2023.\n\n[C] Mahankali, A. V., Hashimoto, T., & Ma, T. One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention. In The Twelfth International Conference on Learning Representations 2024.", "labeling_timestamp": "2026-01-11T17:30:50.711458", "model": "gpt-5-mini", "label": "Supported", "justification": "The provided paper text contains multiple, clear presentation problems across sections (broken/missing equations, image placeholders, and spacing/formatting errors), consistent with a pervasive writing/presentation issue.", "evidence": "Abstract: \"...little is known behind why OFs emerge during training , nor how one can minimise them .\" | Section 2: \"<!-- formula-not-decoded -->\" | Figure 1 area: \"Figure 1: Outlier Features appear in open-source transformers [16] during training, as measured by our Kurtosis metric Eq (1). Our work investigates the design\\n\\n<!-- image -->\" | Section 3: \"<!-- formula-not-decoded -->\" | Figure 2 area: \"Figure 2: Kurtosis becomes large (i.e. OFE) when training with different Norms at 130M scale. We plot the residual stream entering\\n\\n<!-- image -->\"", "section": "Abstract; Section 2; Section 3; Figures"}
{"claim": "The authors have not clarified why the mathematical relationship between Kurtosis and the Gram matrix should be considered a noteworthy finding.", "claim_type": "novelty", "paper_id": "npJQ6qS4bg", "paper_title": "Understanding and Minimising Outlier Features in Transformer Training", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "YeBeK8zEL0", "reviewer": "Reviewer_Lb1w", "review_text": "Comment: Thank you for your response. However, it appears that the revision did not adequately address the core concerns, instead referencing unhelpful comments from other reviewers. The citations from other evaluations do not substantiate your responses unless they are accompanied by supporting evidence. Consequently, my concerns remain unresolved, and I have retained my original score.\n\n1. **Unsatisfactory Writing Quality**: The manuscript suffers from numerous stylistic and structural weaknesses, making it difficult to follow. Starting in Section 3, from Line 87-103, the paragraph introduces background on normalization but fails to signal this topic in the opening sentence. Line 104-177, abruptly begins with \"In Figure 2\" without a transition from the previous paragraph, leaving the reader unclear about the figure's contect...... Limited by time and energy, the reviewer can not point out all the places that writing can be improved since there are too many. Most of the paragraphs in the document lack a coherent top-down structure.\n\n2. **Insufficient Support for Architectural Contribution**: The \"Outlier Protected transformer block\" is presented as a highlighted innovation, yet its description is confined to a single paragraph (Lines 146-163). For an effective presentation of a new architecture, consider the detailed exposition found in Section 4 of Reference [14] (frequently mentioned by the author). If there are substantial results supporting the proposed architecture, they should be prominently featured, akin to how Tables 2 and 3 are presented in [14]. The lack of a unified section detailing the experimental approach, akin to Section 5 in Reference [14], makes it challenging for the readers to effectively understand the context and validity of your findings. \n\n3. **Insignificant Observation**: My concern regarding the relationship between Outlier Features (OF) and Signal Propagation has not been addressed. You provide several opposing subjective comments but fail to engage with the core issue. Specifically, you claim a significant observation that Signal Propagation relates to OF, yet both measures you discuss (Kurtosis and the Gram matrix) are mathematical functions of the same variable, X. The inherent mathematical relationship between these functions should not be surprising, and you have not clarified why this finding is noteworthy.\n\n4. **Insufficient Fundamental Insight**: The title and abstract of your paper promises a deep understanding of why OFs emerge during training. However, the empirical analysis over optimization choices in Section 5, while useful, does not address this core question from a fundamental perspective. An effective discussion should integrate more profound insights into the nature and implications of OFs within neural network dynamics. To enhance the paper, one possible analysis technique can be associated with the neural tangent kernel or training dynamics. For guidance on conducting this type of rigorous research, I recommend referring to [A,B,C], which illustrate how scholars **deeply** uncover the underlying causes of phenomena during training. \n\nOverall, the manuscript requires significant improvements in writing, experimental design, and technical depth to meet the publication standards.\n\n[A] Kumar, A., Raghunathan, A., Jones, R. M., Ma, T., & Liang, P. Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution. In International Conference on Learning Representations 2022.\n\n[B] Kou, Y., Chen, Z., Chen, Y., & Gu, Q. Benign overfitting in two-layer ReLU convolutional neural networks. In International Conference on Machine Learning (pp. 17615-17659). PMLR 2023.\n\n[C] Mahankali, A. V., Hashimoto, T., & Ma, T. One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention. In The Twelfth International Conference on Learning Representations 2024.", "labeling_timestamp": "2026-01-11T17:31:04.396912", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt defines the Kurtosis metric and states that this formulation allows a link to signal propagation in Sec. 4, but the excerpt does not include Sec. 4 or any explicit derivation/discussion connecting Kurtosis to the Gram matrix or an explanation of why such a mathematical relationship would be noteworthy. Therefore there is insufficient information to determine whether the authors clarified that point.", "evidence": "\"Variants of Kurt ( X ) have previously been proposed [14, 3], but our formulation in Eq (1) aggregates activations over inputs first, which allows us to link OFs and signal propagation in Sec 4.\"", "section": "2 Problem Setting"}
{"claim": "Comparing domain-specific SotA methods trained on Objaverse directly with DiffGS trained on ShapeNet-fitted data is unfair.", "claim_type": "baseline", "paper_id": "6zROYoHlcp", "paper_title": "DiffGS: Functional Gaussian Splatting Diffusion", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "jMbtMURYue", "reviewer": "Reviewer_p5pi", "review_text": "Comment: I appreciate the authors for their careful and detailed rebuttal with additional experiments that have resolved many of my initial concerns. I have a few remaining questions regarding the rebuttal.\n\n---\n\n**Global-Q1: Evaluation Against Other SOTA 3D Generative Models**\n\nRegarding that the domain-specific SotA methods compared in the additional experiments such as LGM are trained with Objaverse dataset, it might be unfair to directly compare this with ShapeNet-fitted DiffGS for the generation of ShapeNet data. Would you share your thoughts on how DiffGS can be better (in terms of diversity and quality) if it is scaled to Objaverse dataset based on the shown experiments?\n\n---\n\n**Q7: Justification for Training a Domain-Dedicated 3D Generator**\n\nAccording to Appendix C of the manuscript, I see training for ShapeNet already takes a week for a 8-GPU server. I think in order to make the contribution of this work practically useful, the method should be scaled up to at least Objaverse-scale datasets (perhaps in the future. I do not believe it is adequate to mandate conference papers to train on huge datasets, so I am not requiring the authors to do the experiments.) I am a little bit suspicious on the computational feasibility of this scaling up process. For example, training a 100-GPU cluster for six month for getting a domain-specific Gaussian splatting generator would be a waste of time compared to having a 10-minutes image-to-3DGS generator such as LucidDreamer [1]. Would you persuade me that the proposed approach is computationally feasible and practically meaningful?\n\n---\n\nThese are the last of my concerns.\n\n[1] Yixun Liang, et al., LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching, CVPR 2024.", "labeling_timestamp": "2026-01-11T17:31:03.189768", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper states experiments are conducted on ShapeNet and DeepFashion3D and claims improvements over state-of-the-art methods, but the provided content does not mention Objaverse or any direct comparison where competing methods were trained on Objaverse while DiffGS was trained on ShapeNet-fitted data. Therefore there is insufficient information in the paper excerpt to confirm the reviewer's claim about unfair comparisons.", "evidence": "\"We conduct comprehensive experiments on both synthetic ShapeNet dataset and real-world DeepFashion3D dataset, which demonstrate our non-trivial improvements over the state-of-the-art methods.\"", "section": "Introduction"}
{"claim": "The paper lacks evaluation of DiffGS when scaled to the Objaverse dataset, leaving unclear whether diversity and quality would improve at that scale.", "claim_type": "experimental", "paper_id": "6zROYoHlcp", "paper_title": "DiffGS: Functional Gaussian Splatting Diffusion", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "jMbtMURYue", "reviewer": "Reviewer_p5pi", "review_text": "Comment: I appreciate the authors for their careful and detailed rebuttal with additional experiments that have resolved many of my initial concerns. I have a few remaining questions regarding the rebuttal.\n\n---\n\n**Global-Q1: Evaluation Against Other SOTA 3D Generative Models**\n\nRegarding that the domain-specific SotA methods compared in the additional experiments such as LGM are trained with Objaverse dataset, it might be unfair to directly compare this with ShapeNet-fitted DiffGS for the generation of ShapeNet data. Would you share your thoughts on how DiffGS can be better (in terms of diversity and quality) if it is scaled to Objaverse dataset based on the shown experiments?\n\n---\n\n**Q7: Justification for Training a Domain-Dedicated 3D Generator**\n\nAccording to Appendix C of the manuscript, I see training for ShapeNet already takes a week for a 8-GPU server. I think in order to make the contribution of this work practically useful, the method should be scaled up to at least Objaverse-scale datasets (perhaps in the future. I do not believe it is adequate to mandate conference papers to train on huge datasets, so I am not requiring the authors to do the experiments.) I am a little bit suspicious on the computational feasibility of this scaling up process. For example, training a 100-GPU cluster for six month for getting a domain-specific Gaussian splatting generator would be a waste of time compared to having a 10-minutes image-to-3DGS generator such as LucidDreamer [1]. Would you persuade me that the proposed approach is computationally feasible and practically meaningful?\n\n---\n\nThese are the last of my concerns.\n\n[1] Yixun Liang, et al., LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching, CVPR 2024.", "labeling_timestamp": "2026-01-11T17:30:59.919843", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper reports experiments only on ShapeNet and DeepFashion3D and does not mention or evaluate scaling to the Objaverse dataset, so the claim that it lacks Objaverse-scale evaluation is accurate.", "evidence": "We conduct comprehensive experiments on both synthetic ShapeNet dataset and real-world DeepFashion3D dataset, which demonstrate our non-trivial improvements over the state-of-the-art methods.", "section": "1 Introduction"}
{"claim": "Appendix C indicates training on ShapeNet takes one week on an 8-GPU server.", "claim_type": "quantitative", "paper_id": "6zROYoHlcp", "paper_title": "DiffGS: Functional Gaussian Splatting Diffusion", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "jMbtMURYue", "reviewer": "Reviewer_p5pi", "review_text": "Comment: I appreciate the authors for their careful and detailed rebuttal with additional experiments that have resolved many of my initial concerns. I have a few remaining questions regarding the rebuttal.\n\n---\n\n**Global-Q1: Evaluation Against Other SOTA 3D Generative Models**\n\nRegarding that the domain-specific SotA methods compared in the additional experiments such as LGM are trained with Objaverse dataset, it might be unfair to directly compare this with ShapeNet-fitted DiffGS for the generation of ShapeNet data. Would you share your thoughts on how DiffGS can be better (in terms of diversity and quality) if it is scaled to Objaverse dataset based on the shown experiments?\n\n---\n\n**Q7: Justification for Training a Domain-Dedicated 3D Generator**\n\nAccording to Appendix C of the manuscript, I see training for ShapeNet already takes a week for a 8-GPU server. I think in order to make the contribution of this work practically useful, the method should be scaled up to at least Objaverse-scale datasets (perhaps in the future. I do not believe it is adequate to mandate conference papers to train on huge datasets, so I am not requiring the authors to do the experiments.) I am a little bit suspicious on the computational feasibility of this scaling up process. For example, training a 100-GPU cluster for six month for getting a domain-specific Gaussian splatting generator would be a waste of time compared to having a 10-minutes image-to-3DGS generator such as LucidDreamer [1]. Would you persuade me that the proposed approach is computationally feasible and practically meaningful?\n\n---\n\nThese are the last of my concerns.\n\n[1] Yixun Liang, et al., LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching, CVPR 2024.", "labeling_timestamp": "2026-01-11T17:30:59.559108", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not include Appendix C or any statement specifying training time or GPU hardware for ShapeNet; therefore the claim cannot be verified from the given content.", "evidence": "We conduct comprehensive experiments on both synthetic ShapeNet dataset and real-world DeepFashion3D dataset, which demonstrate our non-trivial improvements over the state-of-the-art methods.", "section": "Introduction"}
{"claim": "Scaling the proposed method to Objaverse-scale datasets may be computationally infeasible or practically meaningless.", "claim_type": "subjective", "paper_id": "6zROYoHlcp", "paper_title": "DiffGS: Functional Gaussian Splatting Diffusion", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "jMbtMURYue", "reviewer": "Reviewer_p5pi", "review_text": "Comment: I appreciate the authors for their careful and detailed rebuttal with additional experiments that have resolved many of my initial concerns. I have a few remaining questions regarding the rebuttal.\n\n---\n\n**Global-Q1: Evaluation Against Other SOTA 3D Generative Models**\n\nRegarding that the domain-specific SotA methods compared in the additional experiments such as LGM are trained with Objaverse dataset, it might be unfair to directly compare this with ShapeNet-fitted DiffGS for the generation of ShapeNet data. Would you share your thoughts on how DiffGS can be better (in terms of diversity and quality) if it is scaled to Objaverse dataset based on the shown experiments?\n\n---\n\n**Q7: Justification for Training a Domain-Dedicated 3D Generator**\n\nAccording to Appendix C of the manuscript, I see training for ShapeNet already takes a week for a 8-GPU server. I think in order to make the contribution of this work practically useful, the method should be scaled up to at least Objaverse-scale datasets (perhaps in the future. I do not believe it is adequate to mandate conference papers to train on huge datasets, so I am not requiring the authors to do the experiments.) I am a little bit suspicious on the computational feasibility of this scaling up process. For example, training a 100-GPU cluster for six month for getting a domain-specific Gaussian splatting generator would be a waste of time compared to having a 10-minutes image-to-3DGS generator such as LucidDreamer [1]. Would you persuade me that the proposed approach is computationally feasible and practically meaningful?\n\n---\n\nThese are the last of my concerns.\n\n[1] Yixun Liang, et al., LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching, CVPR 2024.", "labeling_timestamp": "2026-01-11T17:31:22.039891", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly claims DiffGS is efficient and scalable and can generate Gaussian primitives at arbitrary numbers. It presents scalability and efficiency as core advantages and evaluates on ShapeNet and DeepFashion3D, and does not state or provide evidence that scaling to very large datasets (e.g., Objaverse-scale) is infeasible or meaningless.", "evidence": "Abstract: \"DiffGS is a powerful and efficient 3D generative model which is capable of generating Gaussian primitives at arbitrary numbers for high-fidelity rendering with rasterization.\" Introduction (summary of contributions): \"4) Scalability, we scalably generate Gaussian primitives at arbitrary numbers.\" Section 3 (Method): \"We introduce DiffGS, a novel diffusion-based generative model for general 3D Gaussian Splatting, which is capable of efficiently generating high-quality Gaussian primitives at arbitrary numbers.\" Introduction (experiments): \"We conduct comprehensive experiments on both synthetic ShapeNet dataset and real-world DeepFashion3D dataset...\"", "section": "Abstract; Introduction; Section 3 (Method)"}
{"claim": "Training a 100-GPU cluster for six months to obtain a domain-specific Gaussian splatting generator may be wasteful compared to a 10-minute image-to-3DGS method like LucidDreamer.", "claim_type": "quantitative", "paper_id": "6zROYoHlcp", "paper_title": "DiffGS: Functional Gaussian Splatting Diffusion", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "jMbtMURYue", "reviewer": "Reviewer_p5pi", "review_text": "Comment: I appreciate the authors for their careful and detailed rebuttal with additional experiments that have resolved many of my initial concerns. I have a few remaining questions regarding the rebuttal.\n\n---\n\n**Global-Q1: Evaluation Against Other SOTA 3D Generative Models**\n\nRegarding that the domain-specific SotA methods compared in the additional experiments such as LGM are trained with Objaverse dataset, it might be unfair to directly compare this with ShapeNet-fitted DiffGS for the generation of ShapeNet data. Would you share your thoughts on how DiffGS can be better (in terms of diversity and quality) if it is scaled to Objaverse dataset based on the shown experiments?\n\n---\n\n**Q7: Justification for Training a Domain-Dedicated 3D Generator**\n\nAccording to Appendix C of the manuscript, I see training for ShapeNet already takes a week for a 8-GPU server. I think in order to make the contribution of this work practically useful, the method should be scaled up to at least Objaverse-scale datasets (perhaps in the future. I do not believe it is adequate to mandate conference papers to train on huge datasets, so I am not requiring the authors to do the experiments.) I am a little bit suspicious on the computational feasibility of this scaling up process. For example, training a 100-GPU cluster for six month for getting a domain-specific Gaussian splatting generator would be a waste of time compared to having a 10-minutes image-to-3DGS generator such as LucidDreamer [1]. Would you persuade me that the proposed approach is computationally feasible and practically meaningful?\n\n---\n\nThese are the last of my concerns.\n\n[1] Yixun Liang, et al., LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching, CVPR 2024.", "labeling_timestamp": "2026-01-11T17:31:28.448471", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does not report training on a 100‑GPU cluster for six months, nor does it mention or compare to LucidDreamer or a 10‑minute image→3DGS method. It only makes a general claim of efficiency without the specific compute/time comparison in the reviewer claim.", "evidence": "\"1) Efficiency, we design DiffGS based on Gaussian Splatting and Latent Diffusion Models, which shows significant efficiency in model training, inference and shape rendering.\"", "section": "1 Introduction"}
{"claim": "The rebuttal does not convincingly demonstrate that the approach is computationally feasible or practically meaningful at large dataset scales.", "claim_type": "experimental", "paper_id": "6zROYoHlcp", "paper_title": "DiffGS: Functional Gaussian Splatting Diffusion", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "jMbtMURYue", "reviewer": "Reviewer_p5pi", "review_text": "Comment: I appreciate the authors for their careful and detailed rebuttal with additional experiments that have resolved many of my initial concerns. I have a few remaining questions regarding the rebuttal.\n\n---\n\n**Global-Q1: Evaluation Against Other SOTA 3D Generative Models**\n\nRegarding that the domain-specific SotA methods compared in the additional experiments such as LGM are trained with Objaverse dataset, it might be unfair to directly compare this with ShapeNet-fitted DiffGS for the generation of ShapeNet data. Would you share your thoughts on how DiffGS can be better (in terms of diversity and quality) if it is scaled to Objaverse dataset based on the shown experiments?\n\n---\n\n**Q7: Justification for Training a Domain-Dedicated 3D Generator**\n\nAccording to Appendix C of the manuscript, I see training for ShapeNet already takes a week for a 8-GPU server. I think in order to make the contribution of this work practically useful, the method should be scaled up to at least Objaverse-scale datasets (perhaps in the future. I do not believe it is adequate to mandate conference papers to train on huge datasets, so I am not requiring the authors to do the experiments.) I am a little bit suspicious on the computational feasibility of this scaling up process. For example, training a 100-GPU cluster for six month for getting a domain-specific Gaussian splatting generator would be a waste of time compared to having a 10-minutes image-to-3DGS generator such as LucidDreamer [1]. Would you persuade me that the proposed approach is computationally feasible and practically meaningful?\n\n---\n\nThese are the last of my concerns.\n\n[1] Yixun Liang, et al., LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching, CVPR 2024.", "labeling_timestamp": "2026-01-11T17:31:21.658524", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The reviewer claim refers to the rebuttal, which is not part of the provided paper content. The paper itself makes claims of efficiency and scalability (Introduction) but the provided excerpts do not include detailed large-scale computational benchmarks or a rebuttal, so whether a rebuttal 'does not convincingly demonstrate' feasibility at large scales cannot be verified from the paper.", "evidence": "“We systematically summarize the superiority of DiffGS in terms of: 1) Efficiency, we design DiffGS based on Gaussian Splatting and Latent Diffusion Models, which shows significant efficiency in model training, inference and shape rendering. … 4) Scalability, we scalably generate Gaussian primitives at arbitrary numbers. We conduct comprehensive experiments on both synthetic ShapeNet dataset and real-world DeepFashion3D dataset, which demonstrate our non-trivial improvements over the state-of-the-art methods.”", "section": "1 Introduction"}
{"claim": "The manuscript lacks explicit computational cost estimates or experiments demonstrating scalability from ShapeNet to Objaverse.", "claim_type": "experimental", "paper_id": "6zROYoHlcp", "paper_title": "DiffGS: Functional Gaussian Splatting Diffusion", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "jMbtMURYue", "reviewer": "Reviewer_p5pi", "review_text": "Comment: I appreciate the authors for their careful and detailed rebuttal with additional experiments that have resolved many of my initial concerns. I have a few remaining questions regarding the rebuttal.\n\n---\n\n**Global-Q1: Evaluation Against Other SOTA 3D Generative Models**\n\nRegarding that the domain-specific SotA methods compared in the additional experiments such as LGM are trained with Objaverse dataset, it might be unfair to directly compare this with ShapeNet-fitted DiffGS for the generation of ShapeNet data. Would you share your thoughts on how DiffGS can be better (in terms of diversity and quality) if it is scaled to Objaverse dataset based on the shown experiments?\n\n---\n\n**Q7: Justification for Training a Domain-Dedicated 3D Generator**\n\nAccording to Appendix C of the manuscript, I see training for ShapeNet already takes a week for a 8-GPU server. I think in order to make the contribution of this work practically useful, the method should be scaled up to at least Objaverse-scale datasets (perhaps in the future. I do not believe it is adequate to mandate conference papers to train on huge datasets, so I am not requiring the authors to do the experiments.) I am a little bit suspicious on the computational feasibility of this scaling up process. For example, training a 100-GPU cluster for six month for getting a domain-specific Gaussian splatting generator would be a waste of time compared to having a 10-minutes image-to-3DGS generator such as LucidDreamer [1]. Would you persuade me that the proposed approach is computationally feasible and practically meaningful?\n\n---\n\nThese are the last of my concerns.\n\n[1] Yixun Liang, et al., LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching, CVPR 2024.", "labeling_timestamp": "2026-01-11T17:31:25.286619", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes experiments on ShapeNet and DeepFashion3D and claims efficiency, but the provided content contains no explicit computational cost estimates (e.g., training/inference time, memory, FLOPs) nor any experiments or results evaluating scalability from ShapeNet to Objaverse.", "evidence": "\"We conduct comprehensive experiments on both synthetic ShapeNet dataset and real-world DeepFashion3D dataset, which demonstrate our non-trivial improvements over the state-of-the-art methods.\"\" and \"We systematically summarize the superiority of DiffGS in terms of: 1) Efficiency, we design DiffGS based on Gaussian Splatting and Latent Diffusion Models, which shows significant efficiency in model training, inference and shape rendering.\"", "section": "1 Introduction"}
{"claim": "The paper only compares against 'pickup' methods and does not include state-of-the-art video generation baselines for comparison.", "claim_type": "baseline", "paper_id": "9zHxXaYEgw", "paper_title": "LEO: Generative Latent Image Animator for Human Video Synthesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "57fAR2ze7f", "reviewer": "Reviewer_NikU", "review_text": "Summary: This work introduces a diffusion-based method for video generation. The proposed method leverages a flow-based image animator to learn motion representations thus enabling disentangle motion from appearance. An LDM is designed to learn the motion distribution by providing the starting motion α1 as the condition.\n\nStrengths: 1. This work tries to solve the challenging issue of disentangling motion from appearance. The method is well-motivated and the proposal method is simple to understand.\n2. A Linear Motion Condition (LMC) mechanism is designed in cLMDM to condition the generative process with the first motion code α1.\n3. Qualitative results show the ability to generate long videos and enable disentanglement of motion and appearance.\n\nWeaknesses: 1. The author only includes pickup methods for comparison, STOA methods are not included for comparison. Recent methods, such as MoStGAN-V, VDM, Video-LDM, VideoFactory, and Make-A-Video, should be included for comparison.\n\n2. The author should include experiments on more challenging datasets, such as MSR-VTT and UCF101.\n\nQuestions: see weakness.", "labeling_timestamp": "2026-01-11T17:31:28.177352", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper includes comparisons to recent state-of-the-art video generation methods (e.g., DIGAN, TATS, StyleGAN-V) rather than only 'pickup' methods.", "evidence": "Figure 3: Qualitative Comparison. We qualitatively compare LEO with DIGAN, TATS, StyleGAN-V on short video generation.", "section": "Qualitative Comparison (Figure 3)"}
{"claim": "The paper does not compare against recent methods such as MoStGAN-V, VDM, Video-LDM, VideoFactory, and Make-A-Video.", "claim_type": "baseline", "paper_id": "9zHxXaYEgw", "paper_title": "LEO: Generative Latent Image Animator for Human Video Synthesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "57fAR2ze7f", "reviewer": "Reviewer_NikU", "review_text": "Summary: This work introduces a diffusion-based method for video generation. The proposed method leverages a flow-based image animator to learn motion representations thus enabling disentangle motion from appearance. An LDM is designed to learn the motion distribution by providing the starting motion α1 as the condition.\n\nStrengths: 1. This work tries to solve the challenging issue of disentangling motion from appearance. The method is well-motivated and the proposal method is simple to understand.\n2. A Linear Motion Condition (LMC) mechanism is designed in cLMDM to condition the generative process with the first motion code α1.\n3. Qualitative results show the ability to generate long videos and enable disentanglement of motion and appearance.\n\nWeaknesses: 1. The author only includes pickup methods for comparison, STOA methods are not included for comparison. Recent methods, such as MoStGAN-V, VDM, Video-LDM, VideoFactory, and Make-A-Video, should be included for comparison.\n\n2. The author should include experiments on more challenging datasets, such as MSR-VTT and UCF101.\n\nQuestions: see weakness.", "labeling_timestamp": "2026-01-11T17:31:46.098379", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper's qualitative comparison (Figure 3) only lists DIGAN, TATS, and StyleGAN-V; there is no experimental comparison shown against MoStGAN-V, VDM, Video-LDM, VideoFactory, or Make-A-Video in the provided content.", "evidence": "Figure 3: Qualitative Comparison. We qualitatively compare LEO with DIGAN, TATS, StyleGAN-V on short video generation.", "section": "Figure 3 (Qualitative Comparison); Related Works"}
{"claim": "The experimental evaluation lacks results on more challenging datasets such as MSR-VTT and UCF101.", "claim_type": "experimental", "paper_id": "9zHxXaYEgw", "paper_title": "LEO: Generative Latent Image Animator for Human Video Synthesis", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "57fAR2ze7f", "reviewer": "Reviewer_NikU", "review_text": "Summary: This work introduces a diffusion-based method for video generation. The proposed method leverages a flow-based image animator to learn motion representations thus enabling disentangle motion from appearance. An LDM is designed to learn the motion distribution by providing the starting motion α1 as the condition.\n\nStrengths: 1. This work tries to solve the challenging issue of disentangling motion from appearance. The method is well-motivated and the proposal method is simple to understand.\n2. A Linear Motion Condition (LMC) mechanism is designed in cLMDM to condition the generative process with the first motion code α1.\n3. Qualitative results show the ability to generate long videos and enable disentanglement of motion and appearance.\n\nWeaknesses: 1. The author only includes pickup methods for comparison, STOA methods are not included for comparison. Recent methods, such as MoStGAN-V, VDM, Video-LDM, VideoFactory, and Make-A-Video, should be included for comparison.\n\n2. The author should include experiments on more challenging datasets, such as MSR-VTT and UCF101.\n\nQuestions: see weakness.", "labeling_timestamp": "2026-01-11T17:31:43.210725", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper's experiments are reported only on three human video datasets (TaichiHD, FaceForensics, CelebV-HQ). It does not mention evaluations on MSR-VTT or UCF101, so the claim that the evaluation lacks results on those datasets is accurate.", "evidence": "Abstract: \"Extensive quantitative and qualitative analysis suggests that LEO significantly improves coherent synthesis of human videos over previous methods on the datasets TaichiHD, FaceForensics and CelebV-HQ.\" \nIntroduction: \"To evaluate LEO, we conduct extensive experiments pertained to three human video datasets, including TaichiHD (Siarohin et al., 2019), FaceForensics (R¨ossler et al., 2018), and CelebV-HQ (Zhu et al., 2022).\"", "section": "Abstract; 1 INTRODUCTION"}
{"claim": "The method requires an efficient text-to-image diffusion model with a small number of inference steps, which may hinder applying the method to other base models.", "claim_type": "methodology", "paper_id": "E6ZodZu0HQ", "paper_title": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "hG6rPjvWXW", "reviewer": "Reviewer_bfEK", "review_text": "Summary: The paper propose a tuning-free method for customization text-to-image diffusion model. Particularly, the author propose to utilize efficient diffusion model (SDXL-lightning in this case) to generate samples during training. Then, they adopt contrastive alignment loss to preserve the identity of subject in the input.\n\nStrengths: - The paper is fairly well-written and easy to read.\n- The proposed method is well-motivated and sound.\n- The ablation experiment demonstrates the effectiveness of each proposed components.\n- Reported results seem quite impressive.\n\nWeaknesses: - Since the model requires an efficient text-to-image diffusion model (in terms of number of inference steps), it can hinder the application of  the introduced method with other base models.\n\nQuestions: - Since the author roll out full diffusion path (4 steps) during training, how much memory is required for training (e.g., compared to existing methods)? \n- How does the model perform if we set the number of diffusion step to (1, 2, 6)? (and how much memory footprint)\n- Does this method work for subject other than human as input (e.g., dog, cat, etc) ?", "labeling_timestamp": "2026-01-11T17:31:53.521341", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly requires a Lightning T2I branch that leverages fast sampling (SDXL-Lightning) to generate high-quality images from pure noise in a few steps (4 steps), so the first part of the claim is supported (see Introduction, Related Work, Methods). However, the paper does not state or discuss that this requirement will hinder applying the method to other base models — that limitation is not asserted or analyzed in the paper, so the second part is not supported by the paper.", "evidence": "Introduction: \"Specifically, we introduce a Lightning T2I branch alongside the standard diffusion-denoising training branch. Leveraging recent fast sampling methods [26, 38, 23], the lighting T2I branch can generate high-quality images from pure noise with a limited and manageable number of steps.\"; Related Work (Fast Sampling): \"The Lightning T2I training branch we introduce leverages the SDXL-Lightning [23] acceleration technology, thus enabling us to generate high-quality images from pure noise in just 4 steps.\"; Methods (3.2 Basic Settings): \"We build our model based on the pre-trained SDXL [30], which is a SOTA T2I latent diffusion model.\"", "section": "Introduction; Related Work (Fast Sampling); Methods (3.2 Basic Settings)"}
{"claim": "The paper does not report the memory required for training when rolling out the full four-step diffusion path, making comparison to existing methods unclear.", "claim_type": "experimental", "paper_id": "E6ZodZu0HQ", "paper_title": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "hG6rPjvWXW", "reviewer": "Reviewer_bfEK", "review_text": "Summary: The paper propose a tuning-free method for customization text-to-image diffusion model. Particularly, the author propose to utilize efficient diffusion model (SDXL-lightning in this case) to generate samples during training. Then, they adopt contrastive alignment loss to preserve the identity of subject in the input.\n\nStrengths: - The paper is fairly well-written and easy to read.\n- The proposed method is well-motivated and sound.\n- The ablation experiment demonstrates the effectiveness of each proposed components.\n- Reported results seem quite impressive.\n\nWeaknesses: - Since the model requires an efficient text-to-image diffusion model (in terms of number of inference steps), it can hinder the application of  the introduced method with other base models.\n\nQuestions: - Since the author roll out full diffusion path (4 steps) during training, how much memory is required for training (e.g., compared to existing methods)? \n- How does the model perform if we set the number of diffusion step to (1, 2, 6)? (and how much memory footprint)\n- Does this method work for subject other than human as input (e.g., dog, cat, etc) ?", "labeling_timestamp": "2026-01-11T17:31:53.185342", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes using a 4-step Lightning T2I branch (mentions ‘4 steps’) but does not report any memory requirements or GPU/memory measurements for training with the full 4-step rollout, so the reviewer's claim that memory usage is not reported is correct.", "evidence": "1) \"thus enabling us to generate high-quality images from pure noise in just 4 steps.\" 2) \"(4 in this paper).\"", "section": "Section 2 (Fast Sampling of Diffusion Models) and Section 3 (Figure 2 caption / Methods)"}
{"claim": "The paper does not evaluate model performance when using alternative numbers of diffusion steps such as one, two, or six.", "claim_type": "experimental", "paper_id": "E6ZodZu0HQ", "paper_title": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "hG6rPjvWXW", "reviewer": "Reviewer_bfEK", "review_text": "Summary: The paper propose a tuning-free method for customization text-to-image diffusion model. Particularly, the author propose to utilize efficient diffusion model (SDXL-lightning in this case) to generate samples during training. Then, they adopt contrastive alignment loss to preserve the identity of subject in the input.\n\nStrengths: - The paper is fairly well-written and easy to read.\n- The proposed method is well-motivated and sound.\n- The ablation experiment demonstrates the effectiveness of each proposed components.\n- Reported results seem quite impressive.\n\nWeaknesses: - Since the model requires an efficient text-to-image diffusion model (in terms of number of inference steps), it can hinder the application of  the introduced method with other base models.\n\nQuestions: - Since the author roll out full diffusion path (4 steps) during training, how much memory is required for training (e.g., compared to existing methods)? \n- How does the model perform if we set the number of diffusion step to (1, 2, 6)? (and how much memory footprint)\n- Does this method work for subject other than human as input (e.g., dog, cat, etc) ?", "labeling_timestamp": "2026-01-11T17:31:47.966851", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper consistently describes using a 4-step Lightning T2I branch and does not report evaluations or ablations for other step counts (e.g., 1, 2, or 6). No experimental results or analyses varying the number of diffusion steps are presented in the provided content.", "evidence": "\"thus enabling us to generate high-quality images from pure noise in just 4 steps.\"; \"(4 in this paper).\"", "section": "Related Work and Methods (Figure 2 / Section 3)"}
{"claim": "The paper does not report the memory footprint for training or inference when using one, two, or six diffusion steps.", "claim_type": "experimental", "paper_id": "E6ZodZu0HQ", "paper_title": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "hG6rPjvWXW", "reviewer": "Reviewer_bfEK", "review_text": "Summary: The paper propose a tuning-free method for customization text-to-image diffusion model. Particularly, the author propose to utilize efficient diffusion model (SDXL-lightning in this case) to generate samples during training. Then, they adopt contrastive alignment loss to preserve the identity of subject in the input.\n\nStrengths: - The paper is fairly well-written and easy to read.\n- The proposed method is well-motivated and sound.\n- The ablation experiment demonstrates the effectiveness of each proposed components.\n- Reported results seem quite impressive.\n\nWeaknesses: - Since the model requires an efficient text-to-image diffusion model (in terms of number of inference steps), it can hinder the application of  the introduced method with other base models.\n\nQuestions: - Since the author roll out full diffusion path (4 steps) during training, how much memory is required for training (e.g., compared to existing methods)? \n- How does the model perform if we set the number of diffusion step to (1, 2, 6)? (and how much memory footprint)\n- Does this method work for subject other than human as input (e.g., dog, cat, etc) ?", "labeling_timestamp": "2026-01-11T17:32:09.814142", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper discusses fast sampling and explicitly states the Lightning T2I branch uses 4 steps, but it does not present any measurements or reported memory footprints for training or inference at 1, 2, or 6 diffusion steps (no tables, figures, or text giving memory usage).", "evidence": "\"thus enabling us to generate high-quality images from pure noise in just 4 steps.\"; \"In this branch, we construct contrastive paths with and without ID injection and introduce an alignment loss... (4 in this paper).\"", "section": "Section 2 (Fast Sampling of Diffusion Models) and Section 3 (Methods / Figure 2 caption)"}
{"claim": "The paper does not evaluate whether the tuning-free method generalizes to non-human subjects such as dogs or cats.", "claim_type": "experimental", "paper_id": "E6ZodZu0HQ", "paper_title": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "hG6rPjvWXW", "reviewer": "Reviewer_bfEK", "review_text": "Summary: The paper propose a tuning-free method for customization text-to-image diffusion model. Particularly, the author propose to utilize efficient diffusion model (SDXL-lightning in this case) to generate samples during training. Then, they adopt contrastive alignment loss to preserve the identity of subject in the input.\n\nStrengths: - The paper is fairly well-written and easy to read.\n- The proposed method is well-motivated and sound.\n- The ablation experiment demonstrates the effectiveness of each proposed components.\n- Reported results seem quite impressive.\n\nWeaknesses: - Since the model requires an efficient text-to-image diffusion model (in terms of number of inference steps), it can hinder the application of  the introduced method with other base models.\n\nQuestions: - Since the author roll out full diffusion path (4 steps) during training, how much memory is required for training (e.g., compared to existing methods)? \n- How does the model perform if we set the number of diffusion step to (1, 2, 6)? (and how much memory footprint)\n- Does this method work for subject other than human as input (e.g., dog, cat, etc) ?", "labeling_timestamp": "2026-01-11T17:32:02.928280", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper exclusively discusses identity (ID) customization for human faces and uses face recognition backbones and face extraction as the ID condition. It contains no experiments or evaluation on non-human subjects (e.g., dogs or cats), so the claim that it does not evaluate generalization to non-human subjects is accurate.", "evidence": "\"the face extracted from the same image is employed as the ID condition C id .\" \"Given our human sensitivity to faces, maintaining a high degree of ID fidelity is crucial in ID customization tasks.\" \"Our ID encoder employs two commonly used backbones within the ID customization domain: the face recognition model [6] and the CLIP image encoder [32], to extract ID features.\"", "section": "Figure 2 caption; Introduction; Section 3.2 Basic Settings"}
{"claim": "The graph, node, and edge encoding functions used in the paper are simple and inefficient.", "claim_type": "subjective", "paper_id": "IuXR1CCrSi", "paper_title": "Talk like a Graph: Encoding Graphs for Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "xykE9kudcU", "reviewer": "Reviewer_FDET", "review_text": "Summary: The paper addresses the problem of reasoning on graphs with large language\nmodels (LLMs) and provides a comprehensive exploration of encoding graph-\nstructured data as text that can use LLMs. The paper claims that the LLM\nperformance in graph reasoning tasks varies on three crucial fronts: (1) the\nmethod used to encode the graph, (2) the nature of the graph task itself, and\n(3) the inherent structure of the graph. The paper has provided comprehensive\nexperiments on graph reasoning using LLMs by providing them with text prompts\nthat are constructed from the graphs. In these, the paper analyzes the effect of\na variety of graph-to-text encoding and question encoding functions as well as\ngraph structures on LLMs performance. Different methods such as Zero-shot,\nFew-shot, and Chain-of-Thought methods have been considered for prompting.\nTo analyze the impact of different graph structures on performance, the paper\nhas generated random graphs using previous approaches.\n\nStrengths: - The paper has provided detailed discussions of their results along with\nreasonable and meaningful conclusions.\n\n- The paper is also well-organized and easy to read.\n\n- The experiments are comprehensive as they include important factors\nthat can impact the performance of LLMs on graph reasoning. These\nare encoding the input graph to text, the structure of the input graph,\nrephrasing the question, complexity of the LLM, and prompting method.\n\nWeaknesses: - The graph, node, and edge encoding functions are simple and inefficient.\nThe paper could use more advanced and recent graph-to-text generation\ntechniques (i.e. [1]). Evaluating only the defined encoding methods cannot\nsupport the general claims about the power of LLMs in graph reasoning.\n\n\n- The proposed graph encoding approaches are similar i.e. the Friendship,\nPolitician, Social network, GOT, and SP all depict alternative ways of\nstating two nodes are “connected”. Therefore, evaluating them shows\nthe power of LLMs in interpreting the names rather than exhibiting their\nability to understand underlying relations and exploit neighborhoods within\na graph. This could have been considered in increasing the diversity of\nencoding functions.\n\n- It might be good to introduce previous random graph generation methods.\nAdding some detail of these methods (even in the appendix) can be helpful\nto understand how they are different.\n\n- The proposed benchmark tasks (except for edge existence) do not involve\nreasoning. They can be inferred without reasoning (by counting, simple arithmetic operations, and memorizing the graph structure). More\nchallenging tasks (e.g., node classification) can enrich the experiments.\n\n- In Experiment 2, authors compare question and application rephrasing\nmethods, while the difference between these two is not clear. Authors can\nadd a few examples of rephrasing a question with these methods in the\nmain body or appendix of their paper.\n\n[1] Yi Luan Mirella Lapata Rik Koncel-Kedziorski, Dhanush Bekal and Han-\nnaneh Hajishirzi. Text Generation from Knowledge Graphs with Graph\nTransformers. In NAACL, 2019.\n\nQuestions: It would be great if some of the points raised in the weakness section are addressed.", "labeling_timestamp": "2026-01-11T17:32:14.731914", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper analyzes multiple graph/node/edge encoding functions, reports that the choice of encoding substantially affects LLM performance, and finds that certain encodings (e.g., incident encoding) outperform others by large margins. It therefore does not characterize the encodings as merely simple and inefficient; instead it presents them as meaningful design choices that can significantly boost performance.", "evidence": "\"Graph encoding function has significant impact on LLM reasoning. As the results indicate, the choice of the graph encoding function has a significant impact on the performance of LLMs on graph-related tasks. For instance, for finding connected nodes to a node in a graph, adjacency achieves 19 . 8% accuracy and incident achieves 53 . 8% accuracy. For both node degree and connected nodes, incident encoding outperforms the rest of the encoding functions. This is likely because the incident encoding encodes the graph structure in a way that makes the relevant information more accessible, i.e. , in close proximity, to the LLM.\"", "section": "3.1.1 RESULTS"}
{"claim": "The paper does not use advanced, recent graph-to-text generation techniques such as graph transformers.", "claim_type": "methodology", "paper_id": "IuXR1CCrSi", "paper_title": "Talk like a Graph: Encoding Graphs for Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "xykE9kudcU", "reviewer": "Reviewer_FDET", "review_text": "Summary: The paper addresses the problem of reasoning on graphs with large language\nmodels (LLMs) and provides a comprehensive exploration of encoding graph-\nstructured data as text that can use LLMs. The paper claims that the LLM\nperformance in graph reasoning tasks varies on three crucial fronts: (1) the\nmethod used to encode the graph, (2) the nature of the graph task itself, and\n(3) the inherent structure of the graph. The paper has provided comprehensive\nexperiments on graph reasoning using LLMs by providing them with text prompts\nthat are constructed from the graphs. In these, the paper analyzes the effect of\na variety of graph-to-text encoding and question encoding functions as well as\ngraph structures on LLMs performance. Different methods such as Zero-shot,\nFew-shot, and Chain-of-Thought methods have been considered for prompting.\nTo analyze the impact of different graph structures on performance, the paper\nhas generated random graphs using previous approaches.\n\nStrengths: - The paper has provided detailed discussions of their results along with\nreasonable and meaningful conclusions.\n\n- The paper is also well-organized and easy to read.\n\n- The experiments are comprehensive as they include important factors\nthat can impact the performance of LLMs on graph reasoning. These\nare encoding the input graph to text, the structure of the input graph,\nrephrasing the question, complexity of the LLM, and prompting method.\n\nWeaknesses: - The graph, node, and edge encoding functions are simple and inefficient.\nThe paper could use more advanced and recent graph-to-text generation\ntechniques (i.e. [1]). Evaluating only the defined encoding methods cannot\nsupport the general claims about the power of LLMs in graph reasoning.\n\n\n- The proposed graph encoding approaches are similar i.e. the Friendship,\nPolitician, Social network, GOT, and SP all depict alternative ways of\nstating two nodes are “connected”. Therefore, evaluating them shows\nthe power of LLMs in interpreting the names rather than exhibiting their\nability to understand underlying relations and exploit neighborhoods within\na graph. This could have been considered in increasing the diversity of\nencoding functions.\n\n- It might be good to introduce previous random graph generation methods.\nAdding some detail of these methods (even in the appendix) can be helpful\nto understand how they are different.\n\n- The proposed benchmark tasks (except for edge existence) do not involve\nreasoning. They can be inferred without reasoning (by counting, simple arithmetic operations, and memorizing the graph structure). More\nchallenging tasks (e.g., node classification) can enrich the experiments.\n\n- In Experiment 2, authors compare question and application rephrasing\nmethods, while the difference between these two is not clear. Authors can\nadd a few examples of rephrasing a question with these methods in the\nmain body or appendix of their paper.\n\n[1] Yi Luan Mirella Lapata Rik Koncel-Kedziorski, Dhanush Bekal and Han-\nnaneh Hajishirzi. Text Generation from Knowledge Graphs with Graph\nTransformers. In NAACL, 2019.\n\nQuestions: It would be great if some of the points raised in the weakness section are addressed.", "labeling_timestamp": "2026-01-11T17:32:17.008001", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper specifically studies representing graphs as text for black-box LLM consumption and explicitly states it does not adopt methods that require modifying model internals (which would include graph-transformer style approaches). The experiments and methods focus on text encodings and prompting heuristics rather than advanced graph-to-text models.", "evidence": "However, these methods require access to the internals of the model (its weights or gradients), which can limit their applicability in many settings. In this work, we are instead interested in the case where f ( . ) and its parameters are fixed, and the system is available only for use in a black box setup where the LLM only consumes and produces text ( i.e. , the LLM f : W ↦→ W ).", "section": "2.1 PROMPT ENGINEERING"}
{"claim": "Evaluating only the paper's defined encoding methods cannot support general claims about LLMs' graph reasoning capabilities.", "claim_type": "subjective", "paper_id": "IuXR1CCrSi", "paper_title": "Talk like a Graph: Encoding Graphs for Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "xykE9kudcU", "reviewer": "Reviewer_FDET", "review_text": "Summary: The paper addresses the problem of reasoning on graphs with large language\nmodels (LLMs) and provides a comprehensive exploration of encoding graph-\nstructured data as text that can use LLMs. The paper claims that the LLM\nperformance in graph reasoning tasks varies on three crucial fronts: (1) the\nmethod used to encode the graph, (2) the nature of the graph task itself, and\n(3) the inherent structure of the graph. The paper has provided comprehensive\nexperiments on graph reasoning using LLMs by providing them with text prompts\nthat are constructed from the graphs. In these, the paper analyzes the effect of\na variety of graph-to-text encoding and question encoding functions as well as\ngraph structures on LLMs performance. Different methods such as Zero-shot,\nFew-shot, and Chain-of-Thought methods have been considered for prompting.\nTo analyze the impact of different graph structures on performance, the paper\nhas generated random graphs using previous approaches.\n\nStrengths: - The paper has provided detailed discussions of their results along with\nreasonable and meaningful conclusions.\n\n- The paper is also well-organized and easy to read.\n\n- The experiments are comprehensive as they include important factors\nthat can impact the performance of LLMs on graph reasoning. These\nare encoding the input graph to text, the structure of the input graph,\nrephrasing the question, complexity of the LLM, and prompting method.\n\nWeaknesses: - The graph, node, and edge encoding functions are simple and inefficient.\nThe paper could use more advanced and recent graph-to-text generation\ntechniques (i.e. [1]). Evaluating only the defined encoding methods cannot\nsupport the general claims about the power of LLMs in graph reasoning.\n\n\n- The proposed graph encoding approaches are similar i.e. the Friendship,\nPolitician, Social network, GOT, and SP all depict alternative ways of\nstating two nodes are “connected”. Therefore, evaluating them shows\nthe power of LLMs in interpreting the names rather than exhibiting their\nability to understand underlying relations and exploit neighborhoods within\na graph. This could have been considered in increasing the diversity of\nencoding functions.\n\n- It might be good to introduce previous random graph generation methods.\nAdding some detail of these methods (even in the appendix) can be helpful\nto understand how they are different.\n\n- The proposed benchmark tasks (except for edge existence) do not involve\nreasoning. They can be inferred without reasoning (by counting, simple arithmetic operations, and memorizing the graph structure). More\nchallenging tasks (e.g., node classification) can enrich the experiments.\n\n- In Experiment 2, authors compare question and application rephrasing\nmethods, while the difference between these two is not clear. Authors can\nadd a few examples of rephrasing a question with these methods in the\nmain body or appendix of their paper.\n\n[1] Yi Luan Mirella Lapata Rik Koncel-Kedziorski, Dhanush Bekal and Han-\nnaneh Hajishirzi. Text Generation from Knowledge Graphs with Graph\nTransformers. In NAACL, 2019.\n\nQuestions: It would be great if some of the points raised in the weakness section are addressed.", "labeling_timestamp": "2026-01-11T17:32:27.370994", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper itself draws broad conclusions about LLMs' graph reasoning based on the evaluated encoding methods (e.g., stating that LLMs perform poorly on basic graph tasks and that encoding choice significantly affects performance). Thus it does not support the reviewer's assertion that evaluating only the defined encoders cannot support general claims; instead the paper uses those evaluations to make general claims about LLM behavior.", "evidence": "\"LLMs perform poorly on basic graph tasks.\"; \"The graph encoding function has a significant impact on LLM graph reasoning.\"; \"we perform the first comprehensive study of encoding graph-structured data as text for consumption by LLMs. We show that LLM performance on graph reasoning tasks varies on three fundamental levels: (1) the graph encoding method, (2) the nature of the graph task itself, and (3) ... the very structure of the graph considered.\"", "section": "Abstract; Section 3.1 RESULTS"}
{"claim": "The proposed encoding templates (Friendship, Politician, Social network, GOT, SP) are too similar and only restate that two nodes are \"connected\".", "claim_type": "methodology", "paper_id": "IuXR1CCrSi", "paper_title": "Talk like a Graph: Encoding Graphs for Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "xykE9kudcU", "reviewer": "Reviewer_FDET", "review_text": "Summary: The paper addresses the problem of reasoning on graphs with large language\nmodels (LLMs) and provides a comprehensive exploration of encoding graph-\nstructured data as text that can use LLMs. The paper claims that the LLM\nperformance in graph reasoning tasks varies on three crucial fronts: (1) the\nmethod used to encode the graph, (2) the nature of the graph task itself, and\n(3) the inherent structure of the graph. The paper has provided comprehensive\nexperiments on graph reasoning using LLMs by providing them with text prompts\nthat are constructed from the graphs. In these, the paper analyzes the effect of\na variety of graph-to-text encoding and question encoding functions as well as\ngraph structures on LLMs performance. Different methods such as Zero-shot,\nFew-shot, and Chain-of-Thought methods have been considered for prompting.\nTo analyze the impact of different graph structures on performance, the paper\nhas generated random graphs using previous approaches.\n\nStrengths: - The paper has provided detailed discussions of their results along with\nreasonable and meaningful conclusions.\n\n- The paper is also well-organized and easy to read.\n\n- The experiments are comprehensive as they include important factors\nthat can impact the performance of LLMs on graph reasoning. These\nare encoding the input graph to text, the structure of the input graph,\nrephrasing the question, complexity of the LLM, and prompting method.\n\nWeaknesses: - The graph, node, and edge encoding functions are simple and inefficient.\nThe paper could use more advanced and recent graph-to-text generation\ntechniques (i.e. [1]). Evaluating only the defined encoding methods cannot\nsupport the general claims about the power of LLMs in graph reasoning.\n\n\n- The proposed graph encoding approaches are similar i.e. the Friendship,\nPolitician, Social network, GOT, and SP all depict alternative ways of\nstating two nodes are “connected”. Therefore, evaluating them shows\nthe power of LLMs in interpreting the names rather than exhibiting their\nability to understand underlying relations and exploit neighborhoods within\na graph. This could have been considered in increasing the diversity of\nencoding functions.\n\n- It might be good to introduce previous random graph generation methods.\nAdding some detail of these methods (even in the appendix) can be helpful\nto understand how they are different.\n\n- The proposed benchmark tasks (except for edge existence) do not involve\nreasoning. They can be inferred without reasoning (by counting, simple arithmetic operations, and memorizing the graph structure). More\nchallenging tasks (e.g., node classification) can enrich the experiments.\n\n- In Experiment 2, authors compare question and application rephrasing\nmethods, while the difference between these two is not clear. Authors can\nadd a few examples of rephrasing a question with these methods in the\nmain body or appendix of their paper.\n\n[1] Yi Luan Mirella Lapata Rik Koncel-Kedziorski, Dhanush Bekal and Han-\nnaneh Hajishirzi. Text Generation from Knowledge Graphs with Graph\nTransformers. In NAACL, 2019.\n\nQuestions: It would be great if some of the points raised in the weakness section are addressed.", "labeling_timestamp": "2026-01-11T17:32:38.718417", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly studies multiple distinct graph encoding functions and reports that different encodings capture different aspects of graph structure and lead to substantially different LLM performance (e.g., adjacency vs incident). This contradicts the reviewer's claim that the templates are all too similar and only restate connectivity.", "evidence": "\"the choice of the graph encoding function has a significant impact on the performance of LLMs on graph-related tasks.\" \"different encoding functions capture different aspects of the graph structure. For instance, for finding connected nodes to a node in a graph, adjacency achieves 19.8% accuracy and incident achieves 53.8% accuracy. For both node degree and connected nodes, incident encoding outperforms the rest of the encoding functions.\"", "section": "3.1.1 RESULTS"}
{"claim": "Using similar encoding templates primarily measures LLMs' ability to interpret names rather than their ability to understand underlying relations or exploit graph neighborhoods.", "claim_type": "experimental", "paper_id": "IuXR1CCrSi", "paper_title": "Talk like a Graph: Encoding Graphs for Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "xykE9kudcU", "reviewer": "Reviewer_FDET", "review_text": "Summary: The paper addresses the problem of reasoning on graphs with large language\nmodels (LLMs) and provides a comprehensive exploration of encoding graph-\nstructured data as text that can use LLMs. The paper claims that the LLM\nperformance in graph reasoning tasks varies on three crucial fronts: (1) the\nmethod used to encode the graph, (2) the nature of the graph task itself, and\n(3) the inherent structure of the graph. The paper has provided comprehensive\nexperiments on graph reasoning using LLMs by providing them with text prompts\nthat are constructed from the graphs. In these, the paper analyzes the effect of\na variety of graph-to-text encoding and question encoding functions as well as\ngraph structures on LLMs performance. Different methods such as Zero-shot,\nFew-shot, and Chain-of-Thought methods have been considered for prompting.\nTo analyze the impact of different graph structures on performance, the paper\nhas generated random graphs using previous approaches.\n\nStrengths: - The paper has provided detailed discussions of their results along with\nreasonable and meaningful conclusions.\n\n- The paper is also well-organized and easy to read.\n\n- The experiments are comprehensive as they include important factors\nthat can impact the performance of LLMs on graph reasoning. These\nare encoding the input graph to text, the structure of the input graph,\nrephrasing the question, complexity of the LLM, and prompting method.\n\nWeaknesses: - The graph, node, and edge encoding functions are simple and inefficient.\nThe paper could use more advanced and recent graph-to-text generation\ntechniques (i.e. [1]). Evaluating only the defined encoding methods cannot\nsupport the general claims about the power of LLMs in graph reasoning.\n\n\n- The proposed graph encoding approaches are similar i.e. the Friendship,\nPolitician, Social network, GOT, and SP all depict alternative ways of\nstating two nodes are “connected”. Therefore, evaluating them shows\nthe power of LLMs in interpreting the names rather than exhibiting their\nability to understand underlying relations and exploit neighborhoods within\na graph. This could have been considered in increasing the diversity of\nencoding functions.\n\n- It might be good to introduce previous random graph generation methods.\nAdding some detail of these methods (even in the appendix) can be helpful\nto understand how they are different.\n\n- The proposed benchmark tasks (except for edge existence) do not involve\nreasoning. They can be inferred without reasoning (by counting, simple arithmetic operations, and memorizing the graph structure). More\nchallenging tasks (e.g., node classification) can enrich the experiments.\n\n- In Experiment 2, authors compare question and application rephrasing\nmethods, while the difference between these two is not clear. Authors can\nadd a few examples of rephrasing a question with these methods in the\nmain body or appendix of their paper.\n\n[1] Yi Luan Mirella Lapata Rik Koncel-Kedziorski, Dhanush Bekal and Han-\nnaneh Hajishirzi. Text Generation from Knowledge Graphs with Graph\nTransformers. In NAACL, 2019.\n\nQuestions: It would be great if some of the points raised in the weakness section are addressed.", "labeling_timestamp": "2026-01-11T17:32:45.893771", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper shows that the choice of encoding (including node labels) strongly affects LLM performance — e.g., integer node encoding improves arithmetic-style outputs — which supports the reviewer's point that label interpretation matters. However, the paper also attributes performance differences to how encodings present relational structure (e.g., incident encoding puts relevant information in close proximity and improves connected-node accuracy), indicating encodings also measure ability to access/exploit neighborhood/relations. Thus the claim is only partially aligned with the paper.", "evidence": "“Integer node encoding improves arithmetic performance. Another finding here is that integer encoding of nodes ( e.g. , node 0 ) can improve the performance of LLMs on integer output tasks, such” ; “Graph encoding function has significant impact on LLM reasoning. ... For instance, for finding connected nodes to a node in a graph, adjacency achieves 19 . 8% accuracy and incident achieves 53 . 8% accuracy. For both node degree and connected nodes, incident encoding outperforms the rest of the encoding functions. This is likely because the incident encoding encodes the graph structure in a way that makes the relevant information more accessible, i.e. , in close proximity, to the LLM.”", "section": "3.1.1 RESULTS"}
{"claim": "The encoding functions lack diversity and fail to evaluate models' ability to exploit neighborhood information in graphs.", "claim_type": "methodology", "paper_id": "IuXR1CCrSi", "paper_title": "Talk like a Graph: Encoding Graphs for Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "xykE9kudcU", "reviewer": "Reviewer_FDET", "review_text": "Summary: The paper addresses the problem of reasoning on graphs with large language\nmodels (LLMs) and provides a comprehensive exploration of encoding graph-\nstructured data as text that can use LLMs. The paper claims that the LLM\nperformance in graph reasoning tasks varies on three crucial fronts: (1) the\nmethod used to encode the graph, (2) the nature of the graph task itself, and\n(3) the inherent structure of the graph. The paper has provided comprehensive\nexperiments on graph reasoning using LLMs by providing them with text prompts\nthat are constructed from the graphs. In these, the paper analyzes the effect of\na variety of graph-to-text encoding and question encoding functions as well as\ngraph structures on LLMs performance. Different methods such as Zero-shot,\nFew-shot, and Chain-of-Thought methods have been considered for prompting.\nTo analyze the impact of different graph structures on performance, the paper\nhas generated random graphs using previous approaches.\n\nStrengths: - The paper has provided detailed discussions of their results along with\nreasonable and meaningful conclusions.\n\n- The paper is also well-organized and easy to read.\n\n- The experiments are comprehensive as they include important factors\nthat can impact the performance of LLMs on graph reasoning. These\nare encoding the input graph to text, the structure of the input graph,\nrephrasing the question, complexity of the LLM, and prompting method.\n\nWeaknesses: - The graph, node, and edge encoding functions are simple and inefficient.\nThe paper could use more advanced and recent graph-to-text generation\ntechniques (i.e. [1]). Evaluating only the defined encoding methods cannot\nsupport the general claims about the power of LLMs in graph reasoning.\n\n\n- The proposed graph encoding approaches are similar i.e. the Friendship,\nPolitician, Social network, GOT, and SP all depict alternative ways of\nstating two nodes are “connected”. Therefore, evaluating them shows\nthe power of LLMs in interpreting the names rather than exhibiting their\nability to understand underlying relations and exploit neighborhoods within\na graph. This could have been considered in increasing the diversity of\nencoding functions.\n\n- It might be good to introduce previous random graph generation methods.\nAdding some detail of these methods (even in the appendix) can be helpful\nto understand how they are different.\n\n- The proposed benchmark tasks (except for edge existence) do not involve\nreasoning. They can be inferred without reasoning (by counting, simple arithmetic operations, and memorizing the graph structure). More\nchallenging tasks (e.g., node classification) can enrich the experiments.\n\n- In Experiment 2, authors compare question and application rephrasing\nmethods, while the difference between these two is not clear. Authors can\nadd a few examples of rephrasing a question with these methods in the\nmain body or appendix of their paper.\n\n[1] Yi Luan Mirella Lapata Rik Koncel-Kedziorski, Dhanush Bekal and Han-\nnaneh Hajishirzi. Text Generation from Knowledge Graphs with Graph\nTransformers. In NAACL, 2019.\n\nQuestions: It would be great if some of the points raised in the weakness section are addressed.", "labeling_timestamp": "2026-01-11T17:32:55.162978", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly studies multiple distinct graph encoding functions (e.g., adjacency, incident, integer node encoding) and evaluates tasks that require neighborhood information (connected nodes, node degree). It reports large performance differences across encodings—showing that certain encodings (incident) better expose neighborhood information and improve LLM performance—so the claim that encodings lack diversity and fail to evaluate neighborhood exploitation is contradicted.", "evidence": "1) \"R3 : Incident graph encoding outperforms the rest in most of the setups ( § 3.1).\"  2) \"Graph encoding function has significant impact on LLM reasoning. As the results indicate, the choice of the graph encoding function has a significant impact on the performance of LLMs on graph-related tasks. ... For instance, for finding connected nodes to a node in a graph, adjacency achieves 19 . 8% accuracy and incident achieves 53 . 8% accuracy.\"", "section": "3.1 / 3.1.1 Results"}
{"claim": "The paper does not describe the previous random graph generation methods used to create test graphs.", "claim_type": "methodology", "paper_id": "IuXR1CCrSi", "paper_title": "Talk like a Graph: Encoding Graphs for Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "xykE9kudcU", "reviewer": "Reviewer_FDET", "review_text": "Summary: The paper addresses the problem of reasoning on graphs with large language\nmodels (LLMs) and provides a comprehensive exploration of encoding graph-\nstructured data as text that can use LLMs. The paper claims that the LLM\nperformance in graph reasoning tasks varies on three crucial fronts: (1) the\nmethod used to encode the graph, (2) the nature of the graph task itself, and\n(3) the inherent structure of the graph. The paper has provided comprehensive\nexperiments on graph reasoning using LLMs by providing them with text prompts\nthat are constructed from the graphs. In these, the paper analyzes the effect of\na variety of graph-to-text encoding and question encoding functions as well as\ngraph structures on LLMs performance. Different methods such as Zero-shot,\nFew-shot, and Chain-of-Thought methods have been considered for prompting.\nTo analyze the impact of different graph structures on performance, the paper\nhas generated random graphs using previous approaches.\n\nStrengths: - The paper has provided detailed discussions of their results along with\nreasonable and meaningful conclusions.\n\n- The paper is also well-organized and easy to read.\n\n- The experiments are comprehensive as they include important factors\nthat can impact the performance of LLMs on graph reasoning. These\nare encoding the input graph to text, the structure of the input graph,\nrephrasing the question, complexity of the LLM, and prompting method.\n\nWeaknesses: - The graph, node, and edge encoding functions are simple and inefficient.\nThe paper could use more advanced and recent graph-to-text generation\ntechniques (i.e. [1]). Evaluating only the defined encoding methods cannot\nsupport the general claims about the power of LLMs in graph reasoning.\n\n\n- The proposed graph encoding approaches are similar i.e. the Friendship,\nPolitician, Social network, GOT, and SP all depict alternative ways of\nstating two nodes are “connected”. Therefore, evaluating them shows\nthe power of LLMs in interpreting the names rather than exhibiting their\nability to understand underlying relations and exploit neighborhoods within\na graph. This could have been considered in increasing the diversity of\nencoding functions.\n\n- It might be good to introduce previous random graph generation methods.\nAdding some detail of these methods (even in the appendix) can be helpful\nto understand how they are different.\n\n- The proposed benchmark tasks (except for edge existence) do not involve\nreasoning. They can be inferred without reasoning (by counting, simple arithmetic operations, and memorizing the graph structure). More\nchallenging tasks (e.g., node classification) can enrich the experiments.\n\n- In Experiment 2, authors compare question and application rephrasing\nmethods, while the difference between these two is not clear. Authors can\nadd a few examples of rephrasing a question with these methods in the\nmain body or appendix of their paper.\n\n[1] Yi Luan Mirella Lapata Rik Koncel-Kedziorski, Dhanush Bekal and Han-\nnaneh Hajishirzi. Text Generation from Knowledge Graphs with Graph\nTransformers. In NAACL, 2019.\n\nQuestions: It would be great if some of the points raised in the weakness section are addressed.", "labeling_timestamp": "2026-01-11T17:32:33.879825", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states the random graph model used (Erdős–Rényi) for experiments and points to code and appendices for dataset generation details, so it does describe the random graph generation methods.", "evidence": "\"Graph structure . We briefly note that the design of this experiment follows that of Wang et al. (2023), who use Erd˝ os-R´ enyi (ER) graphs (Erd˝ os & R´ enyi, 1959).\"", "section": "3 TALK LIKE A GRAPH: ENCODING GRAPHS VIA TEXT (Graph structure)"}
{"claim": "Most proposed benchmark tasks (except edge existence) do not require reasoning.", "claim_type": "methodology", "paper_id": "IuXR1CCrSi", "paper_title": "Talk like a Graph: Encoding Graphs for Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "xykE9kudcU", "reviewer": "Reviewer_FDET", "review_text": "Summary: The paper addresses the problem of reasoning on graphs with large language\nmodels (LLMs) and provides a comprehensive exploration of encoding graph-\nstructured data as text that can use LLMs. The paper claims that the LLM\nperformance in graph reasoning tasks varies on three crucial fronts: (1) the\nmethod used to encode the graph, (2) the nature of the graph task itself, and\n(3) the inherent structure of the graph. The paper has provided comprehensive\nexperiments on graph reasoning using LLMs by providing them with text prompts\nthat are constructed from the graphs. In these, the paper analyzes the effect of\na variety of graph-to-text encoding and question encoding functions as well as\ngraph structures on LLMs performance. Different methods such as Zero-shot,\nFew-shot, and Chain-of-Thought methods have been considered for prompting.\nTo analyze the impact of different graph structures on performance, the paper\nhas generated random graphs using previous approaches.\n\nStrengths: - The paper has provided detailed discussions of their results along with\nreasonable and meaningful conclusions.\n\n- The paper is also well-organized and easy to read.\n\n- The experiments are comprehensive as they include important factors\nthat can impact the performance of LLMs on graph reasoning. These\nare encoding the input graph to text, the structure of the input graph,\nrephrasing the question, complexity of the LLM, and prompting method.\n\nWeaknesses: - The graph, node, and edge encoding functions are simple and inefficient.\nThe paper could use more advanced and recent graph-to-text generation\ntechniques (i.e. [1]). Evaluating only the defined encoding methods cannot\nsupport the general claims about the power of LLMs in graph reasoning.\n\n\n- The proposed graph encoding approaches are similar i.e. the Friendship,\nPolitician, Social network, GOT, and SP all depict alternative ways of\nstating two nodes are “connected”. Therefore, evaluating them shows\nthe power of LLMs in interpreting the names rather than exhibiting their\nability to understand underlying relations and exploit neighborhoods within\na graph. This could have been considered in increasing the diversity of\nencoding functions.\n\n- It might be good to introduce previous random graph generation methods.\nAdding some detail of these methods (even in the appendix) can be helpful\nto understand how they are different.\n\n- The proposed benchmark tasks (except for edge existence) do not involve\nreasoning. They can be inferred without reasoning (by counting, simple arithmetic operations, and memorizing the graph structure). More\nchallenging tasks (e.g., node classification) can enrich the experiments.\n\n- In Experiment 2, authors compare question and application rephrasing\nmethods, while the difference between these two is not clear. Authors can\nadd a few examples of rephrasing a question with these methods in the\nmain body or appendix of their paper.\n\n[1] Yi Luan Mirella Lapata Rik Koncel-Kedziorski, Dhanush Bekal and Han-\nnaneh Hajishirzi. Text Generation from Knowledge Graphs with Graph\nTransformers. In NAACL, 2019.\n\nQuestions: It would be great if some of the points raised in the weakness section are addressed.", "labeling_timestamp": "2026-01-11T17:32:58.657571", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper states that the evaluated basic graph tasks do not require multi-hop reasoning (i.e., are 'simple tasks'), which supports the reviewer's general point that most benchmark tasks do not require reasoning. However, the paper does not single out edge existence as the sole exception; edge existence is listed among the basic tasks and is mentioned as a failure case, but the paper does not claim it uniquely requires reasoning. Thus the reviewer's exception for edge existence is not supported by the paper.", "evidence": "1) \"Simple prompts are best for simple tasks. We see that ZERO-COT prompting has worse model performance than ZERO-SHOT prompting on basic graph tasks. This is likely because ZERO-SHOT prompting is sufficient for these tasks, which do not require multi-hop reasoning.\" 2) \"In this experiment, we measure the performance of pre-trained LLMs on graph tasks: edge existence, node degree, node count, edge count, connected nodes, and cycle check.\" 3) \"LLMs perform poorly on basic graph tasks. ... This is especially interesting for edge existence and cycle check ... Therefore. LLMs perform worse than the majority baseline.\"", "section": "3.1.1 RESULTS"}
{"claim": "The tasks can be solved by counting, simple arithmetic, or memorizing the graph structure instead of real reasoning.", "claim_type": "methodology", "paper_id": "IuXR1CCrSi", "paper_title": "Talk like a Graph: Encoding Graphs for Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "xykE9kudcU", "reviewer": "Reviewer_FDET", "review_text": "Summary: The paper addresses the problem of reasoning on graphs with large language\nmodels (LLMs) and provides a comprehensive exploration of encoding graph-\nstructured data as text that can use LLMs. The paper claims that the LLM\nperformance in graph reasoning tasks varies on three crucial fronts: (1) the\nmethod used to encode the graph, (2) the nature of the graph task itself, and\n(3) the inherent structure of the graph. The paper has provided comprehensive\nexperiments on graph reasoning using LLMs by providing them with text prompts\nthat are constructed from the graphs. In these, the paper analyzes the effect of\na variety of graph-to-text encoding and question encoding functions as well as\ngraph structures on LLMs performance. Different methods such as Zero-shot,\nFew-shot, and Chain-of-Thought methods have been considered for prompting.\nTo analyze the impact of different graph structures on performance, the paper\nhas generated random graphs using previous approaches.\n\nStrengths: - The paper has provided detailed discussions of their results along with\nreasonable and meaningful conclusions.\n\n- The paper is also well-organized and easy to read.\n\n- The experiments are comprehensive as they include important factors\nthat can impact the performance of LLMs on graph reasoning. These\nare encoding the input graph to text, the structure of the input graph,\nrephrasing the question, complexity of the LLM, and prompting method.\n\nWeaknesses: - The graph, node, and edge encoding functions are simple and inefficient.\nThe paper could use more advanced and recent graph-to-text generation\ntechniques (i.e. [1]). Evaluating only the defined encoding methods cannot\nsupport the general claims about the power of LLMs in graph reasoning.\n\n\n- The proposed graph encoding approaches are similar i.e. the Friendship,\nPolitician, Social network, GOT, and SP all depict alternative ways of\nstating two nodes are “connected”. Therefore, evaluating them shows\nthe power of LLMs in interpreting the names rather than exhibiting their\nability to understand underlying relations and exploit neighborhoods within\na graph. This could have been considered in increasing the diversity of\nencoding functions.\n\n- It might be good to introduce previous random graph generation methods.\nAdding some detail of these methods (even in the appendix) can be helpful\nto understand how they are different.\n\n- The proposed benchmark tasks (except for edge existence) do not involve\nreasoning. They can be inferred without reasoning (by counting, simple arithmetic operations, and memorizing the graph structure). More\nchallenging tasks (e.g., node classification) can enrich the experiments.\n\n- In Experiment 2, authors compare question and application rephrasing\nmethods, while the difference between these two is not clear. Authors can\nadd a few examples of rephrasing a question with these methods in the\nmain body or appendix of their paper.\n\n[1] Yi Luan Mirella Lapata Rik Koncel-Kedziorski, Dhanush Bekal and Han-\nnaneh Hajishirzi. Text Generation from Knowledge Graphs with Graph\nTransformers. In NAACL, 2019.\n\nQuestions: It would be great if some of the points raised in the weakness section are addressed.", "labeling_timestamp": "2026-01-11T17:33:00.750783", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly states many evaluated graph tasks are \"basic\" and do not require multi-hop reasoning, and notes integer encoding helps arithmetic-like outputs, implying counting/simple arithmetic can suffice. However, the paper does not claim these tasks are solvable merely by \"memorizing the graph structure,\" nor does it assert that no reasoning is required beyond simple operations—so the reviewer claim overstates what the paper asserts.", "evidence": "1) \"Simple prompts are best for simple tasks. We see that ZERO-COT prompting has worse model performance than ZERO-SHOT prompting on basic graph tasks. This is likely because ZERO-SHOT prompting is sufficient for these tasks, which do not require multi-hop reasoning.\" 2) \"Integer node encoding improves arithmetic performance.\"", "section": "3.1.1 RESULTS"}
{"claim": "More challenging tasks, such as node classification, are missing from the experiments.", "claim_type": "experimental", "paper_id": "IuXR1CCrSi", "paper_title": "Talk like a Graph: Encoding Graphs for Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "xykE9kudcU", "reviewer": "Reviewer_FDET", "review_text": "Summary: The paper addresses the problem of reasoning on graphs with large language\nmodels (LLMs) and provides a comprehensive exploration of encoding graph-\nstructured data as text that can use LLMs. The paper claims that the LLM\nperformance in graph reasoning tasks varies on three crucial fronts: (1) the\nmethod used to encode the graph, (2) the nature of the graph task itself, and\n(3) the inherent structure of the graph. The paper has provided comprehensive\nexperiments on graph reasoning using LLMs by providing them with text prompts\nthat are constructed from the graphs. In these, the paper analyzes the effect of\na variety of graph-to-text encoding and question encoding functions as well as\ngraph structures on LLMs performance. Different methods such as Zero-shot,\nFew-shot, and Chain-of-Thought methods have been considered for prompting.\nTo analyze the impact of different graph structures on performance, the paper\nhas generated random graphs using previous approaches.\n\nStrengths: - The paper has provided detailed discussions of their results along with\nreasonable and meaningful conclusions.\n\n- The paper is also well-organized and easy to read.\n\n- The experiments are comprehensive as they include important factors\nthat can impact the performance of LLMs on graph reasoning. These\nare encoding the input graph to text, the structure of the input graph,\nrephrasing the question, complexity of the LLM, and prompting method.\n\nWeaknesses: - The graph, node, and edge encoding functions are simple and inefficient.\nThe paper could use more advanced and recent graph-to-text generation\ntechniques (i.e. [1]). Evaluating only the defined encoding methods cannot\nsupport the general claims about the power of LLMs in graph reasoning.\n\n\n- The proposed graph encoding approaches are similar i.e. the Friendship,\nPolitician, Social network, GOT, and SP all depict alternative ways of\nstating two nodes are “connected”. Therefore, evaluating them shows\nthe power of LLMs in interpreting the names rather than exhibiting their\nability to understand underlying relations and exploit neighborhoods within\na graph. This could have been considered in increasing the diversity of\nencoding functions.\n\n- It might be good to introduce previous random graph generation methods.\nAdding some detail of these methods (even in the appendix) can be helpful\nto understand how they are different.\n\n- The proposed benchmark tasks (except for edge existence) do not involve\nreasoning. They can be inferred without reasoning (by counting, simple arithmetic operations, and memorizing the graph structure). More\nchallenging tasks (e.g., node classification) can enrich the experiments.\n\n- In Experiment 2, authors compare question and application rephrasing\nmethods, while the difference between these two is not clear. Authors can\nadd a few examples of rephrasing a question with these methods in the\nmain body or appendix of their paper.\n\n[1] Yi Luan Mirella Lapata Rik Koncel-Kedziorski, Dhanush Bekal and Han-\nnaneh Hajishirzi. Text Generation from Knowledge Graphs with Graph\nTransformers. In NAACL, 2019.\n\nQuestions: It would be great if some of the points raised in the weakness section are addressed.", "labeling_timestamp": "2026-01-11T17:32:54.447746", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper's experiments (GraphQA) evaluate basic graph tasks—edge existence, node degree, node count, edge count, connected nodes, and cycle check—and do not include node classification or other more challenging tasks.", "evidence": "In this experiment, we measure the performance of pre-trained LLMs on graph tasks: edge existence , node degree , node count , edge count , connected nodes , and cycle check .", "section": "3.1 EXPERIMENT 1: VARYING GRAPH ENCODING FUNCTION"}
{"claim": "The distinction between \"question rephrasing\" and \"application rephrasing\" methods in Experiment 2 is unclear.", "claim_type": "methodology", "paper_id": "IuXR1CCrSi", "paper_title": "Talk like a Graph: Encoding Graphs for Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "xykE9kudcU", "reviewer": "Reviewer_FDET", "review_text": "Summary: The paper addresses the problem of reasoning on graphs with large language\nmodels (LLMs) and provides a comprehensive exploration of encoding graph-\nstructured data as text that can use LLMs. The paper claims that the LLM\nperformance in graph reasoning tasks varies on three crucial fronts: (1) the\nmethod used to encode the graph, (2) the nature of the graph task itself, and\n(3) the inherent structure of the graph. The paper has provided comprehensive\nexperiments on graph reasoning using LLMs by providing them with text prompts\nthat are constructed from the graphs. In these, the paper analyzes the effect of\na variety of graph-to-text encoding and question encoding functions as well as\ngraph structures on LLMs performance. Different methods such as Zero-shot,\nFew-shot, and Chain-of-Thought methods have been considered for prompting.\nTo analyze the impact of different graph structures on performance, the paper\nhas generated random graphs using previous approaches.\n\nStrengths: - The paper has provided detailed discussions of their results along with\nreasonable and meaningful conclusions.\n\n- The paper is also well-organized and easy to read.\n\n- The experiments are comprehensive as they include important factors\nthat can impact the performance of LLMs on graph reasoning. These\nare encoding the input graph to text, the structure of the input graph,\nrephrasing the question, complexity of the LLM, and prompting method.\n\nWeaknesses: - The graph, node, and edge encoding functions are simple and inefficient.\nThe paper could use more advanced and recent graph-to-text generation\ntechniques (i.e. [1]). Evaluating only the defined encoding methods cannot\nsupport the general claims about the power of LLMs in graph reasoning.\n\n\n- The proposed graph encoding approaches are similar i.e. the Friendship,\nPolitician, Social network, GOT, and SP all depict alternative ways of\nstating two nodes are “connected”. Therefore, evaluating them shows\nthe power of LLMs in interpreting the names rather than exhibiting their\nability to understand underlying relations and exploit neighborhoods within\na graph. This could have been considered in increasing the diversity of\nencoding functions.\n\n- It might be good to introduce previous random graph generation methods.\nAdding some detail of these methods (even in the appendix) can be helpful\nto understand how they are different.\n\n- The proposed benchmark tasks (except for edge existence) do not involve\nreasoning. They can be inferred without reasoning (by counting, simple arithmetic operations, and memorizing the graph structure). More\nchallenging tasks (e.g., node classification) can enrich the experiments.\n\n- In Experiment 2, authors compare question and application rephrasing\nmethods, while the difference between these two is not clear. Authors can\nadd a few examples of rephrasing a question with these methods in the\nmain body or appendix of their paper.\n\n[1] Yi Luan Mirella Lapata Rik Koncel-Kedziorski, Dhanush Bekal and Han-\nnaneh Hajishirzi. Text Generation from Knowledge Graphs with Graph\nTransformers. In NAACL, 2019.\n\nQuestions: It would be great if some of the points raised in the weakness section are addressed.", "labeling_timestamp": "2026-01-11T17:33:05.325108", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt defines a question rephrasing function q(Q) (Section 2.1) but does not include Experiment 2 or any description of an 'application rephrasing' method. Therefore there is insufficient information in the supplied content to assess whether the distinction in Experiment 2 is unclear.", "evidence": "“we introduce the graph encoding function g ( G ) and question rephrasing function q ( Q ) , where g : G ↦→ W and q : W ↦→ W ...” (Section 2.1). The excerpt contains descriptions of prompting heuristics and Experiment 1, but no Experiment 2 or 'application rephrasing' discussion is present.", "section": "2.1 PROMPT ENGINEERING"}
{"claim": "The paper does not provide concrete examples of rephrasing a question with the compared methods in the main text or appendix.", "claim_type": "baseline", "paper_id": "IuXR1CCrSi", "paper_title": "Talk like a Graph: Encoding Graphs for Large Language Models", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "xykE9kudcU", "reviewer": "Reviewer_FDET", "review_text": "Summary: The paper addresses the problem of reasoning on graphs with large language\nmodels (LLMs) and provides a comprehensive exploration of encoding graph-\nstructured data as text that can use LLMs. The paper claims that the LLM\nperformance in graph reasoning tasks varies on three crucial fronts: (1) the\nmethod used to encode the graph, (2) the nature of the graph task itself, and\n(3) the inherent structure of the graph. The paper has provided comprehensive\nexperiments on graph reasoning using LLMs by providing them with text prompts\nthat are constructed from the graphs. In these, the paper analyzes the effect of\na variety of graph-to-text encoding and question encoding functions as well as\ngraph structures on LLMs performance. Different methods such as Zero-shot,\nFew-shot, and Chain-of-Thought methods have been considered for prompting.\nTo analyze the impact of different graph structures on performance, the paper\nhas generated random graphs using previous approaches.\n\nStrengths: - The paper has provided detailed discussions of their results along with\nreasonable and meaningful conclusions.\n\n- The paper is also well-organized and easy to read.\n\n- The experiments are comprehensive as they include important factors\nthat can impact the performance of LLMs on graph reasoning. These\nare encoding the input graph to text, the structure of the input graph,\nrephrasing the question, complexity of the LLM, and prompting method.\n\nWeaknesses: - The graph, node, and edge encoding functions are simple and inefficient.\nThe paper could use more advanced and recent graph-to-text generation\ntechniques (i.e. [1]). Evaluating only the defined encoding methods cannot\nsupport the general claims about the power of LLMs in graph reasoning.\n\n\n- The proposed graph encoding approaches are similar i.e. the Friendship,\nPolitician, Social network, GOT, and SP all depict alternative ways of\nstating two nodes are “connected”. Therefore, evaluating them shows\nthe power of LLMs in interpreting the names rather than exhibiting their\nability to understand underlying relations and exploit neighborhoods within\na graph. This could have been considered in increasing the diversity of\nencoding functions.\n\n- It might be good to introduce previous random graph generation methods.\nAdding some detail of these methods (even in the appendix) can be helpful\nto understand how they are different.\n\n- The proposed benchmark tasks (except for edge existence) do not involve\nreasoning. They can be inferred without reasoning (by counting, simple arithmetic operations, and memorizing the graph structure). More\nchallenging tasks (e.g., node classification) can enrich the experiments.\n\n- In Experiment 2, authors compare question and application rephrasing\nmethods, while the difference between these two is not clear. Authors can\nadd a few examples of rephrasing a question with these methods in the\nmain body or appendix of their paper.\n\n[1] Yi Luan Mirella Lapata Rik Koncel-Kedziorski, Dhanush Bekal and Han-\nnaneh Hajishirzi. Text Generation from Knowledge Graphs with Graph\nTransformers. In NAACL, 2019.\n\nQuestions: It would be great if some of the points raised in the weakness section are addressed.", "labeling_timestamp": "2026-01-11T17:33:12.827052", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper does provide concrete example rephrasings for prompting methods in the main text (e.g., the exact phrases used for ZERO-COT and Bag prompting), so the reviewer's claim that no concrete examples are given is false.", "evidence": "'As suggested by the original paper, we used \"Let's think step by step\".'; 'Bag prompting (COT-BAG) ... It works by appending \"Let's construct a graph with the nodes and edges first\" to the graph description.'", "section": "2.2 PROMPTING HEURISTICS"}
{"claim": "The system with the most human-like metrics achieves only 72% task accuracy, while chance is 50% and humans would be expected to achieve nearly 100% accuracy.", "claim_type": "experimental", "paper_id": "2wlNnIqCb7", "paper_title": "Bridging semantics and pragmatics in information-theoretic emergent communication", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "ZUU7oKQGvi", "reviewer": "Reviewer_zhL9", "review_text": "Summary: This paper investigates emergent communication in artificial agents and how properties of the emergent language compares to properties of human language. Specifically, it aims to present a framework to study pragmatic language emergence in systems with different learning objectives, in order to determine constraints that lead to human-like linguistic systems. For training, they use a reference-game setup based in naturalistic images. Based on the closest similarity in complexity, lexicon size and lowest normalized information distance compared to the human baseline, they argue that agents need to optimize for all: utility, informativeness, and complexity.\n\nStrengths: - Well thought-through dataset preprocessing and task formulation\n- Relevant and interesting topic for the computational linguistics and computation cognitive science community\n- Thoughtful setup and analyses that are informative for understanding what the agents learn\n\nWeaknesses: - The paper overall argues for the necessity of a trade-off of different communicative pressures in order to learn a human-like emergent language -- as measured by the system's similarity to human language in complexity, lexicon size and Normalized Information Distance (NID). Based on these parameters, the authors argue that utility, informativeness, and complexity need to be jointly optimized for in order for a human-like language system to emerge. However, the best-performing system (according to the  complexity, lexicon size, and NID metrics) only achieves a final task performance of 72% -- If I understand the task setup correctly, the baseline chance performance is 50% and people should be expected to achieve nearly 100% in performance. The utility-only objective achieves an accuracy of 95% but does so with a larger lexicon size, worse NID and differently looking complexity. So why do the complexity/lexicon size/NID metrics hold so much more value than the actual task performance? **It seems to me that this argument can really just be made when all systems succeed on the task equally and conditioned on that, we can inspect which learning constraints appear to lead to the most human-like system.** And learning constraints that don't lead to task success while others do should already disqualify from further inspection. I find this methodologically problematic and to me this is a fundamental flaw in the argument. (It also severely lacks discussion in the paper.)\n- Minor: The paper writing could generally be improved. Firstly, I find Figure 2 quite confusing as to what the labels mean and when something is masked. Secondly, there are various typos in the paper (e.g., lines 90, 153, 158, 170, 179) and confusing enumeration (line 125-129).\n\nQuestions: - Could you elaborate on what naturalistic images afford compared to more controllable environments (e.g., more interesting lexicon size?)\n- What is the basis for the claim that utility and informativeness are only partially aligned? (line 245)\n- There is an argument brought forth that the agents are pressured to overcompensate because of the lack of syntax (lines 231-233). Could you elaborate on this a bit more? Why can syntax not emerge and wouldn't that then be a fundamental constraint on the comparability of the system to human language (especially when it comes to lexicon size as a metric)?\n- In the utility-only condition, the agents learn to solve the task well, but the authors argue that the listener doesn't learn to reconstruct robust and non-contextual semantics. How is it possible that the model still generalizes so well?\n- How do the pragmatic and semantic condition map onto the results? I understood the paper to say that the agents were trained in the pragmatic setting where it's the listener's goal to pick out the intended target based on two options. The utility evaluation seems to measure the listener's success to pick out the correct target. What is the semantic setup used for?", "labeling_timestamp": "2026-01-11T17:33:16.218901", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The reviewer asserts a specific numeric result (72% task accuracy, chance 50%, humans ~100%), but the provided paper excerpt does not report any numeric task-accuracy values or direct human vs. model accuracy comparisons. The paper states that it assesses pragmatic competence and human-likeness but gives no performance numbers in the supplied content, so the claim cannot be verified from the paper text provided.", "evidence": "Abstract: \"At test time, we assess the pragmatic competence (i.e., utility) of the agents as well as the 'human-likeness' of the shared emergent lexicon that they converged on, by invoking it in a lexical semantic setting...\"", "section": "Abstract"}
{"claim": "The paper values complexity, lexicon size, and NID over actual task performance without adequately justifying that choice when systems differ in task success.", "claim_type": "methodology", "paper_id": "2wlNnIqCb7", "paper_title": "Bridging semantics and pragmatics in information-theoretic emergent communication", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "ZUU7oKQGvi", "reviewer": "Reviewer_zhL9", "review_text": "Summary: This paper investigates emergent communication in artificial agents and how properties of the emergent language compares to properties of human language. Specifically, it aims to present a framework to study pragmatic language emergence in systems with different learning objectives, in order to determine constraints that lead to human-like linguistic systems. For training, they use a reference-game setup based in naturalistic images. Based on the closest similarity in complexity, lexicon size and lowest normalized information distance compared to the human baseline, they argue that agents need to optimize for all: utility, informativeness, and complexity.\n\nStrengths: - Well thought-through dataset preprocessing and task formulation\n- Relevant and interesting topic for the computational linguistics and computation cognitive science community\n- Thoughtful setup and analyses that are informative for understanding what the agents learn\n\nWeaknesses: - The paper overall argues for the necessity of a trade-off of different communicative pressures in order to learn a human-like emergent language -- as measured by the system's similarity to human language in complexity, lexicon size and Normalized Information Distance (NID). Based on these parameters, the authors argue that utility, informativeness, and complexity need to be jointly optimized for in order for a human-like language system to emerge. However, the best-performing system (according to the  complexity, lexicon size, and NID metrics) only achieves a final task performance of 72% -- If I understand the task setup correctly, the baseline chance performance is 50% and people should be expected to achieve nearly 100% in performance. The utility-only objective achieves an accuracy of 95% but does so with a larger lexicon size, worse NID and differently looking complexity. So why do the complexity/lexicon size/NID metrics hold so much more value than the actual task performance? **It seems to me that this argument can really just be made when all systems succeed on the task equally and conditioned on that, we can inspect which learning constraints appear to lead to the most human-like system.** And learning constraints that don't lead to task success while others do should already disqualify from further inspection. I find this methodologically problematic and to me this is a fundamental flaw in the argument. (It also severely lacks discussion in the paper.)\n- Minor: The paper writing could generally be improved. Firstly, I find Figure 2 quite confusing as to what the labels mean and when something is masked. Secondly, there are various typos in the paper (e.g., lines 90, 153, 158, 170, 179) and confusing enumeration (line 125-129).\n\nQuestions: - Could you elaborate on what naturalistic images afford compared to more controllable environments (e.g., more interesting lexicon size?)\n- What is the basis for the claim that utility and informativeness are only partially aligned? (line 245)\n- There is an argument brought forth that the agents are pressured to overcompensate because of the lack of syntax (lines 231-233). Could you elaborate on this a bit more? Why can syntax not emerge and wouldn't that then be a fundamental constraint on the comparability of the system to human language (especially when it comes to lexicon size as a metric)?\n- In the utility-only condition, the agents learn to solve the task well, but the authors argue that the listener doesn't learn to reconstruct robust and non-contextual semantics. How is it possible that the model still generalizes so well?\n- How do the pragmatic and semantic condition map onto the results? I understood the paper to say that the agents were trained in the pragmatic setting where it's the listener's goal to pick out the intended target based on two options. The utility evaluation seems to measure the listener's success to pick out the correct target. What is the semantic setup used for?", "labeling_timestamp": "2026-01-11T17:33:32.903499", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly integrates task utility with information-theoretic pressures (complexity/informativeness) and evaluates both pragmatic competence (utility) and lexical properties. It motivates this choice with prior IB theory and empirical work and reports that informativeness drives emergence while utility and complexity remain important. Thus it does not merely value lexicon size/complexity/NID over task performance without justification.", "evidence": "Abstract: \"we extend a recent information-theoretic framework for emergent communication in artificial agents, which integrates utility maximization, associated with pragmatics, with general communicative constraints...\" Introduction: \"we assess the pragmatic competence (i.e., utility) of the agents as well as the 'human-likeness' of the shared emergent lexicon that they converged on... Given the substantial empirical evidence that human semantic systems are pressured to optimize the IB tradeoff, we predict that a shared human-like lexicon will not emerge when agents are guided solely by utility maximization but rather when they are guided by a non-trivial tradeoff between optimizing utility, informativeness, and complexity.\" Also: \"Interestingly, pressure for communicative informativeness, rather than (non-communicative) task utility, appears as the main driver of emergent communication, but weaker pressures to minimize complexity and maximize utility are still crucial for achieving human-like properties of the lexicon.\"", "section": "Abstract; Introduction"}
{"claim": "It is methodologically problematic to compare emergent language human-likeness when different systems do not all succeed equally on the task.", "claim_type": "methodology", "paper_id": "2wlNnIqCb7", "paper_title": "Bridging semantics and pragmatics in information-theoretic emergent communication", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "ZUU7oKQGvi", "reviewer": "Reviewer_zhL9", "review_text": "Summary: This paper investigates emergent communication in artificial agents and how properties of the emergent language compares to properties of human language. Specifically, it aims to present a framework to study pragmatic language emergence in systems with different learning objectives, in order to determine constraints that lead to human-like linguistic systems. For training, they use a reference-game setup based in naturalistic images. Based on the closest similarity in complexity, lexicon size and lowest normalized information distance compared to the human baseline, they argue that agents need to optimize for all: utility, informativeness, and complexity.\n\nStrengths: - Well thought-through dataset preprocessing and task formulation\n- Relevant and interesting topic for the computational linguistics and computation cognitive science community\n- Thoughtful setup and analyses that are informative for understanding what the agents learn\n\nWeaknesses: - The paper overall argues for the necessity of a trade-off of different communicative pressures in order to learn a human-like emergent language -- as measured by the system's similarity to human language in complexity, lexicon size and Normalized Information Distance (NID). Based on these parameters, the authors argue that utility, informativeness, and complexity need to be jointly optimized for in order for a human-like language system to emerge. However, the best-performing system (according to the  complexity, lexicon size, and NID metrics) only achieves a final task performance of 72% -- If I understand the task setup correctly, the baseline chance performance is 50% and people should be expected to achieve nearly 100% in performance. The utility-only objective achieves an accuracy of 95% but does so with a larger lexicon size, worse NID and differently looking complexity. So why do the complexity/lexicon size/NID metrics hold so much more value than the actual task performance? **It seems to me that this argument can really just be made when all systems succeed on the task equally and conditioned on that, we can inspect which learning constraints appear to lead to the most human-like system.** And learning constraints that don't lead to task success while others do should already disqualify from further inspection. I find this methodologically problematic and to me this is a fundamental flaw in the argument. (It also severely lacks discussion in the paper.)\n- Minor: The paper writing could generally be improved. Firstly, I find Figure 2 quite confusing as to what the labels mean and when something is masked. Secondly, there are various typos in the paper (e.g., lines 90, 153, 158, 170, 179) and confusing enumeration (line 125-129).\n\nQuestions: - Could you elaborate on what naturalistic images afford compared to more controllable environments (e.g., more interesting lexicon size?)\n- What is the basis for the claim that utility and informativeness are only partially aligned? (line 245)\n- There is an argument brought forth that the agents are pressured to overcompensate because of the lack of syntax (lines 231-233). Could you elaborate on this a bit more? Why can syntax not emerge and wouldn't that then be a fundamental constraint on the comparability of the system to human language (especially when it comes to lexicon size as a metric)?\n- In the utility-only condition, the agents learn to solve the task well, but the authors argue that the listener doesn't learn to reconstruct robust and non-contextual semantics. How is it possible that the model still generalizes so well?\n- How do the pragmatic and semantic condition map onto the results? I understood the paper to say that the agents were trained in the pragmatic setting where it's the listener's goal to pick out the intended target based on two options. The utility evaluation seems to measure the listener's success to pick out the correct target. What is the semantic setup used for?", "labeling_timestamp": "2026-01-11T17:33:22.220161", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does not explicitly claim that comparing human-likeness across systems with differing task success is methodologically problematic. It reports that it evaluates both pragmatic competence (utility) and human-likeness of emergent lexica, but does not discuss the methodological issue of comparing human-likeness when systems do not all succeed equally on the task.", "evidence": "At test time, we assess the pragmatic competence (i.e., utility) of the agents as well as the 'human-likeness' of the shared emergent lexicon that they converged on, by invoking it in a lexical semantic setting.", "section": "Introduction / Abstract"}
{"claim": "Learning constraints that prevent a system from achieving task success should disqualify that system from further inspection, but the paper does not follow this principle.", "claim_type": "subjective", "paper_id": "2wlNnIqCb7", "paper_title": "Bridging semantics and pragmatics in information-theoretic emergent communication", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "ZUU7oKQGvi", "reviewer": "Reviewer_zhL9", "review_text": "Summary: This paper investigates emergent communication in artificial agents and how properties of the emergent language compares to properties of human language. Specifically, it aims to present a framework to study pragmatic language emergence in systems with different learning objectives, in order to determine constraints that lead to human-like linguistic systems. For training, they use a reference-game setup based in naturalistic images. Based on the closest similarity in complexity, lexicon size and lowest normalized information distance compared to the human baseline, they argue that agents need to optimize for all: utility, informativeness, and complexity.\n\nStrengths: - Well thought-through dataset preprocessing and task formulation\n- Relevant and interesting topic for the computational linguistics and computation cognitive science community\n- Thoughtful setup and analyses that are informative for understanding what the agents learn\n\nWeaknesses: - The paper overall argues for the necessity of a trade-off of different communicative pressures in order to learn a human-like emergent language -- as measured by the system's similarity to human language in complexity, lexicon size and Normalized Information Distance (NID). Based on these parameters, the authors argue that utility, informativeness, and complexity need to be jointly optimized for in order for a human-like language system to emerge. However, the best-performing system (according to the  complexity, lexicon size, and NID metrics) only achieves a final task performance of 72% -- If I understand the task setup correctly, the baseline chance performance is 50% and people should be expected to achieve nearly 100% in performance. The utility-only objective achieves an accuracy of 95% but does so with a larger lexicon size, worse NID and differently looking complexity. So why do the complexity/lexicon size/NID metrics hold so much more value than the actual task performance? **It seems to me that this argument can really just be made when all systems succeed on the task equally and conditioned on that, we can inspect which learning constraints appear to lead to the most human-like system.** And learning constraints that don't lead to task success while others do should already disqualify from further inspection. I find this methodologically problematic and to me this is a fundamental flaw in the argument. (It also severely lacks discussion in the paper.)\n- Minor: The paper writing could generally be improved. Firstly, I find Figure 2 quite confusing as to what the labels mean and when something is masked. Secondly, there are various typos in the paper (e.g., lines 90, 153, 158, 170, 179) and confusing enumeration (line 125-129).\n\nQuestions: - Could you elaborate on what naturalistic images afford compared to more controllable environments (e.g., more interesting lexicon size?)\n- What is the basis for the claim that utility and informativeness are only partially aligned? (line 245)\n- There is an argument brought forth that the agents are pressured to overcompensate because of the lack of syntax (lines 231-233). Could you elaborate on this a bit more? Why can syntax not emerge and wouldn't that then be a fundamental constraint on the comparability of the system to human language (especially when it comes to lexicon size as a metric)?\n- In the utility-only condition, the agents learn to solve the task well, but the authors argue that the listener doesn't learn to reconstruct robust and non-contextual semantics. How is it possible that the model still generalizes so well?\n- How do the pragmatic and semantic condition map onto the results? I understood the paper to say that the agents were trained in the pragmatic setting where it's the listener's goal to pick out the intended target based on two options. The utility evaluation seems to measure the listener's success to pick out the correct target. What is the semantic setup used for?", "labeling_timestamp": "2026-01-11T17:34:14.041602", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly trains and evaluates agents under information-theoretic constraints that trade off utility, informativeness, and complexity, and it evaluates both pragmatic competence and emergent lexicon even when agents are not solely utility-optimized. It does not adopt any principle of disqualifying systems that fail to achieve task-optimal utility; instead it studies the resulting systems across the tradeoff space.", "evidence": "1) \"we predict that a shared human-like lexicon will not emerge when agents are guided solely by utility maximization but rather when they are guided by a non-trivial tradeoff between optimizing utility, informativeness, and complexity.\" 2) \"agents are trained via self-play, without any supervision or exposure to human languages. At test time, we assess the pragmatic competence (i.e., utility) of the agents as well as the 'human-likeness' of the shared emergent lexicon that they converged on, by invoking it in a lexical semantic setting.\" 3) \"an objective function that integrates utility maximization with the IB principle... yielding a multi-objective optimization problem that trades off maximizing task-specific utilities with maximizing task-agnostic communicative informativeness and minimizing communicative complexity.\"", "section": "Introduction"}
{"claim": "The utility-only objective achieves about 95% accuracy but produces a larger lexicon, worse NID, and different complexity, a trade-off the paper does not reconcile adequately.", "claim_type": "experimental", "paper_id": "2wlNnIqCb7", "paper_title": "Bridging semantics and pragmatics in information-theoretic emergent communication", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "ZUU7oKQGvi", "reviewer": "Reviewer_zhL9", "review_text": "Summary: This paper investigates emergent communication in artificial agents and how properties of the emergent language compares to properties of human language. Specifically, it aims to present a framework to study pragmatic language emergence in systems with different learning objectives, in order to determine constraints that lead to human-like linguistic systems. For training, they use a reference-game setup based in naturalistic images. Based on the closest similarity in complexity, lexicon size and lowest normalized information distance compared to the human baseline, they argue that agents need to optimize for all: utility, informativeness, and complexity.\n\nStrengths: - Well thought-through dataset preprocessing and task formulation\n- Relevant and interesting topic for the computational linguistics and computation cognitive science community\n- Thoughtful setup and analyses that are informative for understanding what the agents learn\n\nWeaknesses: - The paper overall argues for the necessity of a trade-off of different communicative pressures in order to learn a human-like emergent language -- as measured by the system's similarity to human language in complexity, lexicon size and Normalized Information Distance (NID). Based on these parameters, the authors argue that utility, informativeness, and complexity need to be jointly optimized for in order for a human-like language system to emerge. However, the best-performing system (according to the  complexity, lexicon size, and NID metrics) only achieves a final task performance of 72% -- If I understand the task setup correctly, the baseline chance performance is 50% and people should be expected to achieve nearly 100% in performance. The utility-only objective achieves an accuracy of 95% but does so with a larger lexicon size, worse NID and differently looking complexity. So why do the complexity/lexicon size/NID metrics hold so much more value than the actual task performance? **It seems to me that this argument can really just be made when all systems succeed on the task equally and conditioned on that, we can inspect which learning constraints appear to lead to the most human-like system.** And learning constraints that don't lead to task success while others do should already disqualify from further inspection. I find this methodologically problematic and to me this is a fundamental flaw in the argument. (It also severely lacks discussion in the paper.)\n- Minor: The paper writing could generally be improved. Firstly, I find Figure 2 quite confusing as to what the labels mean and when something is masked. Secondly, there are various typos in the paper (e.g., lines 90, 153, 158, 170, 179) and confusing enumeration (line 125-129).\n\nQuestions: - Could you elaborate on what naturalistic images afford compared to more controllable environments (e.g., more interesting lexicon size?)\n- What is the basis for the claim that utility and informativeness are only partially aligned? (line 245)\n- There is an argument brought forth that the agents are pressured to overcompensate because of the lack of syntax (lines 231-233). Could you elaborate on this a bit more? Why can syntax not emerge and wouldn't that then be a fundamental constraint on the comparability of the system to human language (especially when it comes to lexicon size as a metric)?\n- In the utility-only condition, the agents learn to solve the task well, but the authors argue that the listener doesn't learn to reconstruct robust and non-contextual semantics. How is it possible that the model still generalizes so well?\n- How do the pragmatic and semantic condition map onto the results? I understood the paper to say that the agents were trained in the pragmatic setting where it's the listener's goal to pick out the intended target based on two options. The utility evaluation seems to measure the listener's success to pick out the correct target. What is the semantic setup used for?", "labeling_timestamp": "2026-01-11T17:33:39.407684", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpts describe the hypothesis and high-level findings (that utility-only guidance is insufficient and a tradeoff with information-theoretic pressures yields human-like lexica), but they do not include the empirical results, numerical accuracy values (≈95%), or quantitative comparisons of lexicon size, NID, and complexity needed to verify the reviewer's specific claims.", "evidence": "Abstract: \"we test this approach in a rich visual domain of naturalistic images, and find that key human-like properties of the lexicon emerge when agents are guided by both context-specific utility and general communicative pressures...\"\n1 Introduction: \"Given the substantial empirical evidence that human semantic systems are pressured to optimize the IB tradeoff, we predict that a shared human-like lexicon will not emerge when agents are guided solely by utility maximization but rather when they are guided by a non-trivial tradeoff between optimizing utility, informativeness, and complexity.\"", "section": "Abstract; 1 Introduction"}
{"claim": "The paper severely lacks discussion addressing the core methodological flaw of prioritizing human-likeness metrics over task performance differences.", "claim_type": "methodology", "paper_id": "2wlNnIqCb7", "paper_title": "Bridging semantics and pragmatics in information-theoretic emergent communication", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "ZUU7oKQGvi", "reviewer": "Reviewer_zhL9", "review_text": "Summary: This paper investigates emergent communication in artificial agents and how properties of the emergent language compares to properties of human language. Specifically, it aims to present a framework to study pragmatic language emergence in systems with different learning objectives, in order to determine constraints that lead to human-like linguistic systems. For training, they use a reference-game setup based in naturalistic images. Based on the closest similarity in complexity, lexicon size and lowest normalized information distance compared to the human baseline, they argue that agents need to optimize for all: utility, informativeness, and complexity.\n\nStrengths: - Well thought-through dataset preprocessing and task formulation\n- Relevant and interesting topic for the computational linguistics and computation cognitive science community\n- Thoughtful setup and analyses that are informative for understanding what the agents learn\n\nWeaknesses: - The paper overall argues for the necessity of a trade-off of different communicative pressures in order to learn a human-like emergent language -- as measured by the system's similarity to human language in complexity, lexicon size and Normalized Information Distance (NID). Based on these parameters, the authors argue that utility, informativeness, and complexity need to be jointly optimized for in order for a human-like language system to emerge. However, the best-performing system (according to the  complexity, lexicon size, and NID metrics) only achieves a final task performance of 72% -- If I understand the task setup correctly, the baseline chance performance is 50% and people should be expected to achieve nearly 100% in performance. The utility-only objective achieves an accuracy of 95% but does so with a larger lexicon size, worse NID and differently looking complexity. So why do the complexity/lexicon size/NID metrics hold so much more value than the actual task performance? **It seems to me that this argument can really just be made when all systems succeed on the task equally and conditioned on that, we can inspect which learning constraints appear to lead to the most human-like system.** And learning constraints that don't lead to task success while others do should already disqualify from further inspection. I find this methodologically problematic and to me this is a fundamental flaw in the argument. (It also severely lacks discussion in the paper.)\n- Minor: The paper writing could generally be improved. Firstly, I find Figure 2 quite confusing as to what the labels mean and when something is masked. Secondly, there are various typos in the paper (e.g., lines 90, 153, 158, 170, 179) and confusing enumeration (line 125-129).\n\nQuestions: - Could you elaborate on what naturalistic images afford compared to more controllable environments (e.g., more interesting lexicon size?)\n- What is the basis for the claim that utility and informativeness are only partially aligned? (line 245)\n- There is an argument brought forth that the agents are pressured to overcompensate because of the lack of syntax (lines 231-233). Could you elaborate on this a bit more? Why can syntax not emerge and wouldn't that then be a fundamental constraint on the comparability of the system to human language (especially when it comes to lexicon size as a metric)?\n- In the utility-only condition, the agents learn to solve the task well, but the authors argue that the listener doesn't learn to reconstruct robust and non-contextual semantics. How is it possible that the model still generalizes so well?\n- How do the pragmatic and semantic condition map onto the results? I understood the paper to say that the agents were trained in the pragmatic setting where it's the listener's goal to pick out the intended target based on two options. The utility evaluation seems to measure the listener's success to pick out the correct target. What is the semantic setup used for?", "labeling_timestamp": "2026-01-11T17:34:18.182786", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly evaluates both task performance (pragmatic competence / utility) and human-likeness, and discusses their tradeoffs and relative roles (informativeness vs utility). Thus it does not \"severely lack discussion\" of prioritizing human-likeness over task performance; instead it analyzes the interaction and tradeoffs between these objectives.", "evidence": "1) \"At test time, we assess the pragmatic competence (i.e., utility) of the agents as well as the 'human-likeness' of the shared emergent lexicon that they converged on...\" (Abstract/Introduction).\n2) \"we predict that a shared human-like lexicon will not emerge when agents are guided solely by utility maximization but rather when they are guided by a non-trivial tradeoff between optimizing utility, informativeness, and complexity.\" (Introduction).\n3) \"Interestingly, pressure for communicative informativeness, rather than (non-communicative) task utility, appears as the main driver of emergent communication, but weaker pressures to minimize complexity and maximize utility are still crucial for achieving human-like properties of the lexicon.\" (Introduction).", "section": "Abstract / Introduction"}
{"claim": "Figure 2 is confusing about label meanings and when elements are masked, and the paper does not clarify this figure sufficiently.", "claim_type": "presentation", "paper_id": "2wlNnIqCb7", "paper_title": "Bridging semantics and pragmatics in information-theoretic emergent communication", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "ZUU7oKQGvi", "reviewer": "Reviewer_zhL9", "review_text": "Summary: This paper investigates emergent communication in artificial agents and how properties of the emergent language compares to properties of human language. Specifically, it aims to present a framework to study pragmatic language emergence in systems with different learning objectives, in order to determine constraints that lead to human-like linguistic systems. For training, they use a reference-game setup based in naturalistic images. Based on the closest similarity in complexity, lexicon size and lowest normalized information distance compared to the human baseline, they argue that agents need to optimize for all: utility, informativeness, and complexity.\n\nStrengths: - Well thought-through dataset preprocessing and task formulation\n- Relevant and interesting topic for the computational linguistics and computation cognitive science community\n- Thoughtful setup and analyses that are informative for understanding what the agents learn\n\nWeaknesses: - The paper overall argues for the necessity of a trade-off of different communicative pressures in order to learn a human-like emergent language -- as measured by the system's similarity to human language in complexity, lexicon size and Normalized Information Distance (NID). Based on these parameters, the authors argue that utility, informativeness, and complexity need to be jointly optimized for in order for a human-like language system to emerge. However, the best-performing system (according to the  complexity, lexicon size, and NID metrics) only achieves a final task performance of 72% -- If I understand the task setup correctly, the baseline chance performance is 50% and people should be expected to achieve nearly 100% in performance. The utility-only objective achieves an accuracy of 95% but does so with a larger lexicon size, worse NID and differently looking complexity. So why do the complexity/lexicon size/NID metrics hold so much more value than the actual task performance? **It seems to me that this argument can really just be made when all systems succeed on the task equally and conditioned on that, we can inspect which learning constraints appear to lead to the most human-like system.** And learning constraints that don't lead to task success while others do should already disqualify from further inspection. I find this methodologically problematic and to me this is a fundamental flaw in the argument. (It also severely lacks discussion in the paper.)\n- Minor: The paper writing could generally be improved. Firstly, I find Figure 2 quite confusing as to what the labels mean and when something is masked. Secondly, there are various typos in the paper (e.g., lines 90, 153, 158, 170, 179) and confusing enumeration (line 125-129).\n\nQuestions: - Could you elaborate on what naturalistic images afford compared to more controllable environments (e.g., more interesting lexicon size?)\n- What is the basis for the claim that utility and informativeness are only partially aligned? (line 245)\n- There is an argument brought forth that the agents are pressured to overcompensate because of the lack of syntax (lines 231-233). Could you elaborate on this a bit more? Why can syntax not emerge and wouldn't that then be a fundamental constraint on the comparability of the system to human language (especially when it comes to lexicon size as a metric)?\n- In the utility-only condition, the agents learn to solve the task well, but the authors argue that the listener doesn't learn to reconstruct robust and non-contextual semantics. How is it possible that the model still generalizes so well?\n- How do the pragmatic and semantic condition map onto the results? I understood the paper to say that the agents were trained in the pragmatic setting where it's the listener's goal to pick out the intended target based on two options. The utility evaluation seems to measure the listener's success to pick out the correct target. What is the semantic setup used for?", "labeling_timestamp": "2026-01-11T17:34:08.251598", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly explains the meanings of labels and the masking behavior in the Figure 2 caption and in the main text (Section 3). The caption clearly states which inputs are observed or masked at test time, and Section 3 describes the pragmatic setting where both agents observe (x0, x1) and the speaker knows the target vs. distractor.", "evidence": "\"Figure 2: Communication model for the co-evolution of semantics and pragmatics (see Section 3). Agents are trained in a pragmatic setting, where both observe inputs x0, x1 as shared context; one input is randomly selected as target xt for the speaker. After training, the agent's emergent communication systems are evaluated in a lexical semantics setting, without shared context; the speaker observes only xt while xd is masked, and both inputs are masked for the listener.\" \n\n\"In our pragmatic setting both agents observe a shared context (x0, x1), while the speaker also observes which referent is the target t and which is a distractor d\"", "section": "Figure 2 caption; Section 3"}
{"claim": "The manuscript contains multiple typos and confusing enumeration (e.g., lines 90, 125-129, 153, 158, 170, 179) that reduce readability.", "claim_type": "presentation", "paper_id": "2wlNnIqCb7", "paper_title": "Bridging semantics and pragmatics in information-theoretic emergent communication", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "ZUU7oKQGvi", "reviewer": "Reviewer_zhL9", "review_text": "Summary: This paper investigates emergent communication in artificial agents and how properties of the emergent language compares to properties of human language. Specifically, it aims to present a framework to study pragmatic language emergence in systems with different learning objectives, in order to determine constraints that lead to human-like linguistic systems. For training, they use a reference-game setup based in naturalistic images. Based on the closest similarity in complexity, lexicon size and lowest normalized information distance compared to the human baseline, they argue that agents need to optimize for all: utility, informativeness, and complexity.\n\nStrengths: - Well thought-through dataset preprocessing and task formulation\n- Relevant and interesting topic for the computational linguistics and computation cognitive science community\n- Thoughtful setup and analyses that are informative for understanding what the agents learn\n\nWeaknesses: - The paper overall argues for the necessity of a trade-off of different communicative pressures in order to learn a human-like emergent language -- as measured by the system's similarity to human language in complexity, lexicon size and Normalized Information Distance (NID). Based on these parameters, the authors argue that utility, informativeness, and complexity need to be jointly optimized for in order for a human-like language system to emerge. However, the best-performing system (according to the  complexity, lexicon size, and NID metrics) only achieves a final task performance of 72% -- If I understand the task setup correctly, the baseline chance performance is 50% and people should be expected to achieve nearly 100% in performance. The utility-only objective achieves an accuracy of 95% but does so with a larger lexicon size, worse NID and differently looking complexity. So why do the complexity/lexicon size/NID metrics hold so much more value than the actual task performance? **It seems to me that this argument can really just be made when all systems succeed on the task equally and conditioned on that, we can inspect which learning constraints appear to lead to the most human-like system.** And learning constraints that don't lead to task success while others do should already disqualify from further inspection. I find this methodologically problematic and to me this is a fundamental flaw in the argument. (It also severely lacks discussion in the paper.)\n- Minor: The paper writing could generally be improved. Firstly, I find Figure 2 quite confusing as to what the labels mean and when something is masked. Secondly, there are various typos in the paper (e.g., lines 90, 153, 158, 170, 179) and confusing enumeration (line 125-129).\n\nQuestions: - Could you elaborate on what naturalistic images afford compared to more controllable environments (e.g., more interesting lexicon size?)\n- What is the basis for the claim that utility and informativeness are only partially aligned? (line 245)\n- There is an argument brought forth that the agents are pressured to overcompensate because of the lack of syntax (lines 231-233). Could you elaborate on this a bit more? Why can syntax not emerge and wouldn't that then be a fundamental constraint on the comparability of the system to human language (especially when it comes to lexicon size as a metric)?\n- In the utility-only condition, the agents learn to solve the task well, but the authors argue that the listener doesn't learn to reconstruct robust and non-contextual semantics. How is it possible that the model still generalizes so well?\n- How do the pragmatic and semantic condition map onto the results? I understood the paper to say that the agents were trained in the pragmatic setting where it's the listener's goal to pick out the intended target based on two options. The utility evaluation seems to measure the listener's success to pick out the correct target. What is the semantic setup used for?", "labeling_timestamp": "2026-01-11T17:33:56.462316", "model": "gpt-5-mini", "label": "Supported", "justification": "The manuscript text contains multiple typographical and formatting issues (misplaced author/footnote lines, stray HTML comments, extra spaces before punctuation, and inconsistent word forms/hyphenation) that impair readability. These occur across the Introduction and Related work sections and match the reviewer's characterization of multiple typos and confusing enumeration.", "evidence": "Examples from the paper: (1) 'lexical semantics .' (extra space before period); (2) 'general informationtheoretic constraints' (missing hyphen/space); (3) 'subfield in cognitive linguistics.' (should be 'subfields'); (4) author/footnote lines appear mid-paragraph: '∗ Work finished prior to joining Apple' followed later by author entries 'Roger P. Levy MIT rplevy@mit.edu Noga Zaslavsky NYU nogaz@nyu.edu'; (5) inline HTML comment tokens: '<!-- image -->'.", "section": "Introduction; Related work"}
{"claim": "The paper does not explain what naturalistic images afford compared to more controllable environments, particularly regarding effects on lexicon size.", "claim_type": "subjective", "paper_id": "2wlNnIqCb7", "paper_title": "Bridging semantics and pragmatics in information-theoretic emergent communication", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "ZUU7oKQGvi", "reviewer": "Reviewer_zhL9", "review_text": "Summary: This paper investigates emergent communication in artificial agents and how properties of the emergent language compares to properties of human language. Specifically, it aims to present a framework to study pragmatic language emergence in systems with different learning objectives, in order to determine constraints that lead to human-like linguistic systems. For training, they use a reference-game setup based in naturalistic images. Based on the closest similarity in complexity, lexicon size and lowest normalized information distance compared to the human baseline, they argue that agents need to optimize for all: utility, informativeness, and complexity.\n\nStrengths: - Well thought-through dataset preprocessing and task formulation\n- Relevant and interesting topic for the computational linguistics and computation cognitive science community\n- Thoughtful setup and analyses that are informative for understanding what the agents learn\n\nWeaknesses: - The paper overall argues for the necessity of a trade-off of different communicative pressures in order to learn a human-like emergent language -- as measured by the system's similarity to human language in complexity, lexicon size and Normalized Information Distance (NID). Based on these parameters, the authors argue that utility, informativeness, and complexity need to be jointly optimized for in order for a human-like language system to emerge. However, the best-performing system (according to the  complexity, lexicon size, and NID metrics) only achieves a final task performance of 72% -- If I understand the task setup correctly, the baseline chance performance is 50% and people should be expected to achieve nearly 100% in performance. The utility-only objective achieves an accuracy of 95% but does so with a larger lexicon size, worse NID and differently looking complexity. So why do the complexity/lexicon size/NID metrics hold so much more value than the actual task performance? **It seems to me that this argument can really just be made when all systems succeed on the task equally and conditioned on that, we can inspect which learning constraints appear to lead to the most human-like system.** And learning constraints that don't lead to task success while others do should already disqualify from further inspection. I find this methodologically problematic and to me this is a fundamental flaw in the argument. (It also severely lacks discussion in the paper.)\n- Minor: The paper writing could generally be improved. Firstly, I find Figure 2 quite confusing as to what the labels mean and when something is masked. Secondly, there are various typos in the paper (e.g., lines 90, 153, 158, 170, 179) and confusing enumeration (line 125-129).\n\nQuestions: - Could you elaborate on what naturalistic images afford compared to more controllable environments (e.g., more interesting lexicon size?)\n- What is the basis for the claim that utility and informativeness are only partially aligned? (line 245)\n- There is an argument brought forth that the agents are pressured to overcompensate because of the lack of syntax (lines 231-233). Could you elaborate on this a bit more? Why can syntax not emerge and wouldn't that then be a fundamental constraint on the comparability of the system to human language (especially when it comes to lexicon size as a metric)?\n- In the utility-only condition, the agents learn to solve the task well, but the authors argue that the listener doesn't learn to reconstruct robust and non-contextual semantics. How is it possible that the model still generalizes so well?\n- How do the pragmatic and semantic condition map onto the results? I understood the paper to say that the agents were trained in the pragmatic setting where it's the listener's goal to pick out the intended target based on two options. The utility evaluation seems to measure the listener's success to pick out the correct target. What is the semantic setup used for?", "labeling_timestamp": "2026-01-11T17:34:10.666847", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper mentions using a rich naturalistic visual domain (ManyNames) and contrasts it with prior small domains, but it does not explain what naturalistic images specifically afford compared to more controllable environments, nor does it discuss how that choice affects emergent lexicon size.", "evidence": "1) \"To this end, we consider a rich visual domain of naturalistic images provided by the ManyNames dataset [18], which also contains free-naming human data generated by native English speakers. This domain allows us to train agents across many different conversational contexts and then evaluate their emergent lexicon with respect to the English naming data.\" 2) \"Brochhagen et al. [28] developed a game-theoretic model for the evolution of the division of labor between semantics and pragmatics, and tested the model in a relatively small domain. In comparison, we consider here different learning dynamics and objective function, employ state-of-the-art deep-learning tools for training agents at scale, and evaluate our approach with respect to actual human data in a rich domain of naturalistic images [18].\"", "section": "Introduction; Related work"}
{"claim": "The manuscript provides no clear basis for the claim that utility and informativeness are only partially aligned.", "claim_type": "experimental", "paper_id": "2wlNnIqCb7", "paper_title": "Bridging semantics and pragmatics in information-theoretic emergent communication", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "ZUU7oKQGvi", "reviewer": "Reviewer_zhL9", "review_text": "Summary: This paper investigates emergent communication in artificial agents and how properties of the emergent language compares to properties of human language. Specifically, it aims to present a framework to study pragmatic language emergence in systems with different learning objectives, in order to determine constraints that lead to human-like linguistic systems. For training, they use a reference-game setup based in naturalistic images. Based on the closest similarity in complexity, lexicon size and lowest normalized information distance compared to the human baseline, they argue that agents need to optimize for all: utility, informativeness, and complexity.\n\nStrengths: - Well thought-through dataset preprocessing and task formulation\n- Relevant and interesting topic for the computational linguistics and computation cognitive science community\n- Thoughtful setup and analyses that are informative for understanding what the agents learn\n\nWeaknesses: - The paper overall argues for the necessity of a trade-off of different communicative pressures in order to learn a human-like emergent language -- as measured by the system's similarity to human language in complexity, lexicon size and Normalized Information Distance (NID). Based on these parameters, the authors argue that utility, informativeness, and complexity need to be jointly optimized for in order for a human-like language system to emerge. However, the best-performing system (according to the  complexity, lexicon size, and NID metrics) only achieves a final task performance of 72% -- If I understand the task setup correctly, the baseline chance performance is 50% and people should be expected to achieve nearly 100% in performance. The utility-only objective achieves an accuracy of 95% but does so with a larger lexicon size, worse NID and differently looking complexity. So why do the complexity/lexicon size/NID metrics hold so much more value than the actual task performance? **It seems to me that this argument can really just be made when all systems succeed on the task equally and conditioned on that, we can inspect which learning constraints appear to lead to the most human-like system.** And learning constraints that don't lead to task success while others do should already disqualify from further inspection. I find this methodologically problematic and to me this is a fundamental flaw in the argument. (It also severely lacks discussion in the paper.)\n- Minor: The paper writing could generally be improved. Firstly, I find Figure 2 quite confusing as to what the labels mean and when something is masked. Secondly, there are various typos in the paper (e.g., lines 90, 153, 158, 170, 179) and confusing enumeration (line 125-129).\n\nQuestions: - Could you elaborate on what naturalistic images afford compared to more controllable environments (e.g., more interesting lexicon size?)\n- What is the basis for the claim that utility and informativeness are only partially aligned? (line 245)\n- There is an argument brought forth that the agents are pressured to overcompensate because of the lack of syntax (lines 231-233). Could you elaborate on this a bit more? Why can syntax not emerge and wouldn't that then be a fundamental constraint on the comparability of the system to human language (especially when it comes to lexicon size as a metric)?\n- In the utility-only condition, the agents learn to solve the task well, but the authors argue that the listener doesn't learn to reconstruct robust and non-contextual semantics. How is it possible that the model still generalizes so well?\n- How do the pragmatic and semantic condition map onto the results? I understood the paper to say that the agents were trained in the pragmatic setting where it's the listener's goal to pick out the intended target based on two options. The utility evaluation seems to measure the listener's success to pick out the correct target. What is the semantic setup used for?", "labeling_timestamp": "2026-01-11T17:34:22.243745", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper asserts that informativeness, rather than task utility, is the main driver while utility plays a weaker role, but the provided manuscript excerpt states this conclusion without presenting accompanying empirical analyses, metrics, or detailed results in the shown sections to substantiate the partial alignment claim.", "evidence": "\"Interestingly, pressure for communicative informativeness, rather than (non-communicative) task utility, appears as the main driver of emergent communication, but weaker pressures to minimize complexity and maximize utility are still crucial for achieving human-like properties of the lexicon.\"", "section": "Introduction (end of Section 1)"}
{"claim": "The paper argues agents overcompensate because of a lack of syntax but does not justify why syntax cannot emerge in these agents.", "claim_type": "methodology", "paper_id": "2wlNnIqCb7", "paper_title": "Bridging semantics and pragmatics in information-theoretic emergent communication", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "ZUU7oKQGvi", "reviewer": "Reviewer_zhL9", "review_text": "Summary: This paper investigates emergent communication in artificial agents and how properties of the emergent language compares to properties of human language. Specifically, it aims to present a framework to study pragmatic language emergence in systems with different learning objectives, in order to determine constraints that lead to human-like linguistic systems. For training, they use a reference-game setup based in naturalistic images. Based on the closest similarity in complexity, lexicon size and lowest normalized information distance compared to the human baseline, they argue that agents need to optimize for all: utility, informativeness, and complexity.\n\nStrengths: - Well thought-through dataset preprocessing and task formulation\n- Relevant and interesting topic for the computational linguistics and computation cognitive science community\n- Thoughtful setup and analyses that are informative for understanding what the agents learn\n\nWeaknesses: - The paper overall argues for the necessity of a trade-off of different communicative pressures in order to learn a human-like emergent language -- as measured by the system's similarity to human language in complexity, lexicon size and Normalized Information Distance (NID). Based on these parameters, the authors argue that utility, informativeness, and complexity need to be jointly optimized for in order for a human-like language system to emerge. However, the best-performing system (according to the  complexity, lexicon size, and NID metrics) only achieves a final task performance of 72% -- If I understand the task setup correctly, the baseline chance performance is 50% and people should be expected to achieve nearly 100% in performance. The utility-only objective achieves an accuracy of 95% but does so with a larger lexicon size, worse NID and differently looking complexity. So why do the complexity/lexicon size/NID metrics hold so much more value than the actual task performance? **It seems to me that this argument can really just be made when all systems succeed on the task equally and conditioned on that, we can inspect which learning constraints appear to lead to the most human-like system.** And learning constraints that don't lead to task success while others do should already disqualify from further inspection. I find this methodologically problematic and to me this is a fundamental flaw in the argument. (It also severely lacks discussion in the paper.)\n- Minor: The paper writing could generally be improved. Firstly, I find Figure 2 quite confusing as to what the labels mean and when something is masked. Secondly, there are various typos in the paper (e.g., lines 90, 153, 158, 170, 179) and confusing enumeration (line 125-129).\n\nQuestions: - Could you elaborate on what naturalistic images afford compared to more controllable environments (e.g., more interesting lexicon size?)\n- What is the basis for the claim that utility and informativeness are only partially aligned? (line 245)\n- There is an argument brought forth that the agents are pressured to overcompensate because of the lack of syntax (lines 231-233). Could you elaborate on this a bit more? Why can syntax not emerge and wouldn't that then be a fundamental constraint on the comparability of the system to human language (especially when it comes to lexicon size as a metric)?\n- In the utility-only condition, the agents learn to solve the task well, but the authors argue that the listener doesn't learn to reconstruct robust and non-contextual semantics. How is it possible that the model still generalizes so well?\n- How do the pragmatic and semantic condition map onto the results? I understood the paper to say that the agents were trained in the pragmatic setting where it's the listener's goal to pick out the intended target based on two options. The utility evaluation seems to measure the listener's success to pick out the correct target. What is the semantic setup used for?", "labeling_timestamp": "2026-01-11T17:34:35.117935", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper focuses on the co-evolution of lexical semantics and pragmatic reasoning, information-theoretic constraints, and emergent lexica. It does not discuss agents 'overcompensating' nor does it mention syntactic structure or claim that syntax cannot emerge, so the reviewer's claim cannot be verified from the text.", "evidence": "“Our work focuses on the underexplored interface between these two aspects of language, departing from the traditional assumption that the lexicon is given a-priori and shared among pragmatic interlocutors.”\n\n“we are interested in how pragmatics and lexical semantic may emerge without any human supervision.”", "section": "Related work / Introduction"}
{"claim": "The authors do not address that the absence of emergent syntax could fundamentally undermine comparability to human language when using lexicon size as a metric.", "claim_type": "methodology", "paper_id": "2wlNnIqCb7", "paper_title": "Bridging semantics and pragmatics in information-theoretic emergent communication", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "ZUU7oKQGvi", "reviewer": "Reviewer_zhL9", "review_text": "Summary: This paper investigates emergent communication in artificial agents and how properties of the emergent language compares to properties of human language. Specifically, it aims to present a framework to study pragmatic language emergence in systems with different learning objectives, in order to determine constraints that lead to human-like linguistic systems. For training, they use a reference-game setup based in naturalistic images. Based on the closest similarity in complexity, lexicon size and lowest normalized information distance compared to the human baseline, they argue that agents need to optimize for all: utility, informativeness, and complexity.\n\nStrengths: - Well thought-through dataset preprocessing and task formulation\n- Relevant and interesting topic for the computational linguistics and computation cognitive science community\n- Thoughtful setup and analyses that are informative for understanding what the agents learn\n\nWeaknesses: - The paper overall argues for the necessity of a trade-off of different communicative pressures in order to learn a human-like emergent language -- as measured by the system's similarity to human language in complexity, lexicon size and Normalized Information Distance (NID). Based on these parameters, the authors argue that utility, informativeness, and complexity need to be jointly optimized for in order for a human-like language system to emerge. However, the best-performing system (according to the  complexity, lexicon size, and NID metrics) only achieves a final task performance of 72% -- If I understand the task setup correctly, the baseline chance performance is 50% and people should be expected to achieve nearly 100% in performance. The utility-only objective achieves an accuracy of 95% but does so with a larger lexicon size, worse NID and differently looking complexity. So why do the complexity/lexicon size/NID metrics hold so much more value than the actual task performance? **It seems to me that this argument can really just be made when all systems succeed on the task equally and conditioned on that, we can inspect which learning constraints appear to lead to the most human-like system.** And learning constraints that don't lead to task success while others do should already disqualify from further inspection. I find this methodologically problematic and to me this is a fundamental flaw in the argument. (It also severely lacks discussion in the paper.)\n- Minor: The paper writing could generally be improved. Firstly, I find Figure 2 quite confusing as to what the labels mean and when something is masked. Secondly, there are various typos in the paper (e.g., lines 90, 153, 158, 170, 179) and confusing enumeration (line 125-129).\n\nQuestions: - Could you elaborate on what naturalistic images afford compared to more controllable environments (e.g., more interesting lexicon size?)\n- What is the basis for the claim that utility and informativeness are only partially aligned? (line 245)\n- There is an argument brought forth that the agents are pressured to overcompensate because of the lack of syntax (lines 231-233). Could you elaborate on this a bit more? Why can syntax not emerge and wouldn't that then be a fundamental constraint on the comparability of the system to human language (especially when it comes to lexicon size as a metric)?\n- In the utility-only condition, the agents learn to solve the task well, but the authors argue that the listener doesn't learn to reconstruct robust and non-contextual semantics. How is it possible that the model still generalizes so well?\n- How do the pragmatic and semantic condition map onto the results? I understood the paper to say that the agents were trained in the pragmatic setting where it's the listener's goal to pick out the intended target based on two options. The utility evaluation seems to measure the listener's success to pick out the correct target. What is the semantic setup used for?", "labeling_timestamp": "2026-01-11T17:34:35.863397", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly evaluates emergent lexicon properties (size, complexity, alignment with English) and focuses on semantics and pragmatics, but it contains no discussion of syntactic structure or how the absence of emergent syntax might affect comparability to human language when using lexicon size as a metric.", "evidence": "'we find that human-like properties of the lexicon (its size, complexity, and alignment with English speakers), together with high pragmatic competence, emerge when agents are guided by both context-specific utility and general communicative pressures as derived from the IB principle.'", "section": "1 Introduction"}
{"claim": "The authors claim the listener under the utility-only condition does not learn robust, non-contextual semantics but do not explain how the model nonetheless generalizes well.", "claim_type": "experimental", "paper_id": "2wlNnIqCb7", "paper_title": "Bridging semantics and pragmatics in information-theoretic emergent communication", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "ZUU7oKQGvi", "reviewer": "Reviewer_zhL9", "review_text": "Summary: This paper investigates emergent communication in artificial agents and how properties of the emergent language compares to properties of human language. Specifically, it aims to present a framework to study pragmatic language emergence in systems with different learning objectives, in order to determine constraints that lead to human-like linguistic systems. For training, they use a reference-game setup based in naturalistic images. Based on the closest similarity in complexity, lexicon size and lowest normalized information distance compared to the human baseline, they argue that agents need to optimize for all: utility, informativeness, and complexity.\n\nStrengths: - Well thought-through dataset preprocessing and task formulation\n- Relevant and interesting topic for the computational linguistics and computation cognitive science community\n- Thoughtful setup and analyses that are informative for understanding what the agents learn\n\nWeaknesses: - The paper overall argues for the necessity of a trade-off of different communicative pressures in order to learn a human-like emergent language -- as measured by the system's similarity to human language in complexity, lexicon size and Normalized Information Distance (NID). Based on these parameters, the authors argue that utility, informativeness, and complexity need to be jointly optimized for in order for a human-like language system to emerge. However, the best-performing system (according to the  complexity, lexicon size, and NID metrics) only achieves a final task performance of 72% -- If I understand the task setup correctly, the baseline chance performance is 50% and people should be expected to achieve nearly 100% in performance. The utility-only objective achieves an accuracy of 95% but does so with a larger lexicon size, worse NID and differently looking complexity. So why do the complexity/lexicon size/NID metrics hold so much more value than the actual task performance? **It seems to me that this argument can really just be made when all systems succeed on the task equally and conditioned on that, we can inspect which learning constraints appear to lead to the most human-like system.** And learning constraints that don't lead to task success while others do should already disqualify from further inspection. I find this methodologically problematic and to me this is a fundamental flaw in the argument. (It also severely lacks discussion in the paper.)\n- Minor: The paper writing could generally be improved. Firstly, I find Figure 2 quite confusing as to what the labels mean and when something is masked. Secondly, there are various typos in the paper (e.g., lines 90, 153, 158, 170, 179) and confusing enumeration (line 125-129).\n\nQuestions: - Could you elaborate on what naturalistic images afford compared to more controllable environments (e.g., more interesting lexicon size?)\n- What is the basis for the claim that utility and informativeness are only partially aligned? (line 245)\n- There is an argument brought forth that the agents are pressured to overcompensate because of the lack of syntax (lines 231-233). Could you elaborate on this a bit more? Why can syntax not emerge and wouldn't that then be a fundamental constraint on the comparability of the system to human language (especially when it comes to lexicon size as a metric)?\n- In the utility-only condition, the agents learn to solve the task well, but the authors argue that the listener doesn't learn to reconstruct robust and non-contextual semantics. How is it possible that the model still generalizes so well?\n- How do the pragmatic and semantic condition map onto the results? I understood the paper to say that the agents were trained in the pragmatic setting where it's the listener's goal to pick out the intended target based on two options. The utility evaluation seems to measure the listener's success to pick out the correct target. What is the semantic setup used for?", "labeling_timestamp": "2026-01-11T17:35:00.971104", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly states that a human-like (non-contextual) lexicon does not emerge when agents are guided solely by utility maximization (supporting the first part of the reviewer claim). However, the paper does not make or substantively discuss a claim that the model nevertheless \"generalizes well\" under the utility-only condition, nor does it provide an explanation for such a phenomenon; the only discussion of improved out-of-distribution generalization refers to prior work on the VQ-VIB method, not an explanation within this paper (so the second part of the reviewer claim is not supported by the paper).", "evidence": [{"quote": "Given the substantial empirical evidence that human semantic systems are pressured to optimize the IB tradeoff, we predict that a shared human-like lexicon will not emerge when agents are guided solely by utility maximization but rather when they are guided by a non-trivial tradeoff between optimizing utility, informativeness, and complexity.", "section": "1 Introduction"}, {"quote": "In support of our prediction, we find that human-like properties of the lexicon (its size, complexity, and alignment with English speakers), together with high pragmatic competence, emerge when agents are guided by both context-specific utility and general communicative pressures as derived from the IB principle.", "section": "1 Introduction"}, {"quote": "Tucker et al. [4] showed how to directly integrate the IB objective function with utility maximization in emergent communication and demonstrated the advantages of this framework for faster convergence rates and better out-of-distribution generalization [31]. However, they did not study the co-emergence of semantics and pragmatics.", "section": "2 Related work"}], "section": ""}
{"claim": "The paper fails to clearly explain how the pragmatic and semantic training conditions map onto the reported results and what specific semantic setup was used.", "claim_type": "methodology", "paper_id": "2wlNnIqCb7", "paper_title": "Bridging semantics and pragmatics in information-theoretic emergent communication", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "ZUU7oKQGvi", "reviewer": "Reviewer_zhL9", "review_text": "Summary: This paper investigates emergent communication in artificial agents and how properties of the emergent language compares to properties of human language. Specifically, it aims to present a framework to study pragmatic language emergence in systems with different learning objectives, in order to determine constraints that lead to human-like linguistic systems. For training, they use a reference-game setup based in naturalistic images. Based on the closest similarity in complexity, lexicon size and lowest normalized information distance compared to the human baseline, they argue that agents need to optimize for all: utility, informativeness, and complexity.\n\nStrengths: - Well thought-through dataset preprocessing and task formulation\n- Relevant and interesting topic for the computational linguistics and computation cognitive science community\n- Thoughtful setup and analyses that are informative for understanding what the agents learn\n\nWeaknesses: - The paper overall argues for the necessity of a trade-off of different communicative pressures in order to learn a human-like emergent language -- as measured by the system's similarity to human language in complexity, lexicon size and Normalized Information Distance (NID). Based on these parameters, the authors argue that utility, informativeness, and complexity need to be jointly optimized for in order for a human-like language system to emerge. However, the best-performing system (according to the  complexity, lexicon size, and NID metrics) only achieves a final task performance of 72% -- If I understand the task setup correctly, the baseline chance performance is 50% and people should be expected to achieve nearly 100% in performance. The utility-only objective achieves an accuracy of 95% but does so with a larger lexicon size, worse NID and differently looking complexity. So why do the complexity/lexicon size/NID metrics hold so much more value than the actual task performance? **It seems to me that this argument can really just be made when all systems succeed on the task equally and conditioned on that, we can inspect which learning constraints appear to lead to the most human-like system.** And learning constraints that don't lead to task success while others do should already disqualify from further inspection. I find this methodologically problematic and to me this is a fundamental flaw in the argument. (It also severely lacks discussion in the paper.)\n- Minor: The paper writing could generally be improved. Firstly, I find Figure 2 quite confusing as to what the labels mean and when something is masked. Secondly, there are various typos in the paper (e.g., lines 90, 153, 158, 170, 179) and confusing enumeration (line 125-129).\n\nQuestions: - Could you elaborate on what naturalistic images afford compared to more controllable environments (e.g., more interesting lexicon size?)\n- What is the basis for the claim that utility and informativeness are only partially aligned? (line 245)\n- There is an argument brought forth that the agents are pressured to overcompensate because of the lack of syntax (lines 231-233). Could you elaborate on this a bit more? Why can syntax not emerge and wouldn't that then be a fundamental constraint on the comparability of the system to human language (especially when it comes to lexicon size as a metric)?\n- In the utility-only condition, the agents learn to solve the task well, but the authors argue that the listener doesn't learn to reconstruct robust and non-contextual semantics. How is it possible that the model still generalizes so well?\n- How do the pragmatic and semantic condition map onto the results? I understood the paper to say that the agents were trained in the pragmatic setting where it's the listener's goal to pick out the intended target based on two options. The utility evaluation seems to measure the listener's success to pick out the correct target. What is the semantic setup used for?", "labeling_timestamp": "2026-01-11T17:34:51.161445", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly describes (1) training in a pragmatic setting with shared context and a target/distractor setup, and (2) evaluating the emergent lexicon in a lexical semantic (no-context) setting. These descriptions map the pragmatic/semantic conditions to the reported evaluations and specify the semantic evaluation setup.", "evidence": "Figure 2: \"Agents are trained in a pragmatic setting, where both observe inputs x0, x1 as shared context; one input is randomly selected as target xt for the speaker. After training, the agent's emergent communication systems are evaluated in a lexical semantics setting, without shared context; the speaker observes only xt while xd is masked, and both inputs are masked for the listener.\"", "section": "3 A unifying model for the co-evolution of semantics and pragmatics (Figure 2 caption)"}
{"claim": "The motivation for using the proposed strategy is insufficiently analyzed and lacks validation.", "claim_type": "subjective", "paper_id": "5HQhYiGnYb", "paper_title": "FIDE: Frequency-Inflated Conditional Diffusion Model for Extreme-Aware Time Series Generation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "0RMF9eYWmh", "reviewer": "Reviewer_4b7m", "review_text": "Comment: Thanks. I am not satisfied with the author's rebuttal. there are still a lot of issues that need to be clarified here.\n\nQ1: The motivation for using this strategy requires further analysis and validation.\n\nQ3: Based on the numerical experimental results, it is difficult to determine whether the performance improvement of the proposed method compared to diffusionTS is due to capturing extreme values. The differences between the two methods need further explanation. Why can't diffusionTS capture extreme values, but your method can? What is the motivation and principle behind this? You should use Wilcoxon-Holm analysis the results in Table 1 to demonstrate the advantages and disadvantages of the proposed method.\n\nQ4: What were the experimental setups for diffusionTS, Fourier-Flows, and RealNVP? How is the fairness of the comparison ensured?\n\nW1: Wilcoxon-Holm analysis should be used to demonstrate that the results in Table 2 indeed show a significant performance improvement. The mean values alone do not sufficiently indicate that your results are superior. I also do not think it is a good idea to place the ablation experiments in the supplementary materials.\n\nThe core challenge in time series generation tasks is the scarcity of data, yet diffusion models require large amounts of data for training. Please verify the performance of the proposed method on small-scale datasets, such as 10% of the stock dataset. I am certain that diffusionTS cannot handle this; please provide an explanation and validation. What is the significance of generating time series in the context of large-scale data?", "labeling_timestamp": "2026-01-11T17:34:47.053013", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper presents both analytical motivation and empirical validation for the proposed high-frequency inflation strategy: it connects block maxima to high-frequency components (Introduction and Section 3), provides a formal theorem justifying rapid dissipation and how inflation counteracts it (Theorems 1 and 2, Section 3 and 4.1), and shows empirical illustrations (Figures 2 and 3) demonstrating the effect.", "evidence": "“Our key observation is that unusually large block maxima values, often linked to abrupt temporal changes, are strongly associated with high-frequency components of the time series.”; “To substantiate our observations, the theorem below offers a rigorous justification for the rapid dissipation of block maxima during the forward process of the diffusion model (see Appendix C for proof and details).”; “Theorem 2. ... the ratio of high-frequency and low-frequency components after inflation and perturbations is: ... Thus, by applying high-frequency inflation, the high-frequency components including abrupt block maxima will be preserved by a factor of γ compared to the previous case.”; “Figure 3b shows how inflating the high-frequency components helps in preserving the block maxima values for longer iterations of the diffusion model.”", "section": "Introduction; Section 3 (On the Rapid Dissipation of Block Maxima in Diffusion Models); Section 4.1 (High Frequency Components Inflation)"}
{"claim": "The numerical results do not make it clear whether performance improvement over diffusionTS is due to capturing extreme values.", "claim_type": "experimental", "paper_id": "5HQhYiGnYb", "paper_title": "FIDE: Frequency-Inflated Conditional Diffusion Model for Extreme-Aware Time Series Generation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "0RMF9eYWmh", "reviewer": "Reviewer_4b7m", "review_text": "Comment: Thanks. I am not satisfied with the author's rebuttal. there are still a lot of issues that need to be clarified here.\n\nQ1: The motivation for using this strategy requires further analysis and validation.\n\nQ3: Based on the numerical experimental results, it is difficult to determine whether the performance improvement of the proposed method compared to diffusionTS is due to capturing extreme values. The differences between the two methods need further explanation. Why can't diffusionTS capture extreme values, but your method can? What is the motivation and principle behind this? You should use Wilcoxon-Holm analysis the results in Table 1 to demonstrate the advantages and disadvantages of the proposed method.\n\nQ4: What were the experimental setups for diffusionTS, Fourier-Flows, and RealNVP? How is the fairness of the comparison ensured?\n\nW1: Wilcoxon-Holm analysis should be used to demonstrate that the results in Table 2 indeed show a significant performance improvement. The mean values alone do not sufficiently indicate that your results are superior. I also do not think it is a good idea to place the ablation experiments in the supplementary materials.\n\nThe core challenge in time series generation tasks is the scarcity of data, yet diffusion models require large amounts of data for training. Please verify the performance of the proposed method on small-scale datasets, such as 10% of the stock dataset. I am certain that diffusionTS cannot handle this; please provide an explanation and validation. What is the significance of generating time series in the context of large-scale data?", "labeling_timestamp": "2026-01-11T17:35:06.503118", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt discusses improvements and presents qualitative observations and the proposed FIDE method, but it does not include the numerical experimental results or specific comparisons with the baseline 'diffusionTS' in the supplied text. Therefore there is insufficient information to determine whether any reported performance improvement over diffusionTS is specifically due to better capture of extreme values.", "evidence": "Abstract: \"Experimental results on real-world and synthetic data showcase the efficacy of FIDE over baseline methods, highlighting its potential in advancing Generative AI for time series analysis, specifically in accurately modeling extreme events.\" \nSection 3: \"Figure 3a illustrates this phenomenon. By tracking the evolution of residuals, or the differences between the original and perturbed time series generated by DDPM, we observe a discernible pattern...\"", "section": "Abstract; Section 3 (On the Rapid Dissipation of Block Maxima in Diffusion Models)"}
{"claim": "The paper does not explain the specific differences between the proposed method and diffusionTS.", "claim_type": "baseline", "paper_id": "5HQhYiGnYb", "paper_title": "FIDE: Frequency-Inflated Conditional Diffusion Model for Extreme-Aware Time Series Generation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "0RMF9eYWmh", "reviewer": "Reviewer_4b7m", "review_text": "Comment: Thanks. I am not satisfied with the author's rebuttal. there are still a lot of issues that need to be clarified here.\n\nQ1: The motivation for using this strategy requires further analysis and validation.\n\nQ3: Based on the numerical experimental results, it is difficult to determine whether the performance improvement of the proposed method compared to diffusionTS is due to capturing extreme values. The differences between the two methods need further explanation. Why can't diffusionTS capture extreme values, but your method can? What is the motivation and principle behind this? You should use Wilcoxon-Holm analysis the results in Table 1 to demonstrate the advantages and disadvantages of the proposed method.\n\nQ4: What were the experimental setups for diffusionTS, Fourier-Flows, and RealNVP? How is the fairness of the comparison ensured?\n\nW1: Wilcoxon-Holm analysis should be used to demonstrate that the results in Table 2 indeed show a significant performance improvement. The mean values alone do not sufficiently indicate that your results are superior. I also do not think it is a good idea to place the ablation experiments in the supplementary materials.\n\nThe core challenge in time series generation tasks is the scarcity of data, yet diffusion models require large amounts of data for training. Please verify the performance of the proposed method on small-scale datasets, such as 10% of the stock dataset. I am certain that diffusionTS cannot handle this; please provide an explanation and validation. What is the significance of generating time series in the context of large-scale data?", "labeling_timestamp": "2026-01-11T17:34:58.279238", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper discusses DDPM and diffusion models broadly (Introduction, Section 3) but contains no mention of 'DiffusionTS' or any comparison explaining specific differences to it, so the claim that it does not explain specific differences is accurate.", "evidence": "\"To illustrate the difficulty of modeling the distribution of block maxima, Figure 1 shows the result of applying the Denoising Diffusion Probabilistic Model (DDPM) [12] to a synthetic AR(1) dataset.\"; \"While there has been growing research on applying diffusion models for time series [20, 2], their ability to preserve the distribution of extreme values remains largely underexplored.\"", "section": "1 Introduction"}
{"claim": "The authors do not explain why diffusionTS cannot capture extreme values while the proposed method can.", "claim_type": "methodology", "paper_id": "5HQhYiGnYb", "paper_title": "FIDE: Frequency-Inflated Conditional Diffusion Model for Extreme-Aware Time Series Generation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "0RMF9eYWmh", "reviewer": "Reviewer_4b7m", "review_text": "Comment: Thanks. I am not satisfied with the author's rebuttal. there are still a lot of issues that need to be clarified here.\n\nQ1: The motivation for using this strategy requires further analysis and validation.\n\nQ3: Based on the numerical experimental results, it is difficult to determine whether the performance improvement of the proposed method compared to diffusionTS is due to capturing extreme values. The differences between the two methods need further explanation. Why can't diffusionTS capture extreme values, but your method can? What is the motivation and principle behind this? You should use Wilcoxon-Holm analysis the results in Table 1 to demonstrate the advantages and disadvantages of the proposed method.\n\nQ4: What were the experimental setups for diffusionTS, Fourier-Flows, and RealNVP? How is the fairness of the comparison ensured?\n\nW1: Wilcoxon-Holm analysis should be used to demonstrate that the results in Table 2 indeed show a significant performance improvement. The mean values alone do not sufficiently indicate that your results are superior. I also do not think it is a good idea to place the ablation experiments in the supplementary materials.\n\nThe core challenge in time series generation tasks is the scarcity of data, yet diffusion models require large amounts of data for training. Please verify the performance of the proposed method on small-scale datasets, such as 10% of the stock dataset. I am certain that diffusionTS cannot handle this; please provide an explanation and validation. What is the significance of generating time series in the context of large-scale data?", "labeling_timestamp": "2026-01-11T17:35:23.807552", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly explains both why existing diffusion models struggle to capture extremes (rapid dissipation of high-frequency components during the forward/noise addition process) and why the proposed method can address this (a high-frequency inflation strategy, conditional modeling on block maxima, and a GEV-based regularizer). These explanations appear in Section 3 and Section 4.1.", "evidence": "Section 3: \"Our first key observation reveals a connection between block maxima with abrupt changes and the high-frequency components of many real-world time series... Block maxima... are intrinsically linked to the high-frequency components of the data.\" \"Our second key observation unveils a concerning behavior of diffusion models: the addition of noise diminishes high frequency components, i.e., block maxima, at a faster rate compared to other values in the signal.\" \"Theorem 1. ... the ratio of high-frequency and low-frequency components after perturbation during the forward process of the diffusion model is:\"\n\nSection 4.1: \"In order to counteract the rapid decay of high-frequency components in the frequency domain while adding noise in the forward process of DDPM, we present a strategy for high-frequency inflation.\" \"Our goal is to inflate the topκ frequency components... where γ > 1 is the inflation weight.\" \"Theorem 2. ... by applying high-frequency inflation, the high-frequency components including abrupt block maxima will be preserved by a factor of γ compared to the previous case.\"", "section": "Section 3 (On the Rapid Dissipation of Block Maxima in Diffusion Models) and Section 4.1 (High Frequency Components Inflation)"}
{"claim": "The motivation and underlying principle for why the proposed method captures extreme values are not provided.", "claim_type": "subjective", "paper_id": "5HQhYiGnYb", "paper_title": "FIDE: Frequency-Inflated Conditional Diffusion Model for Extreme-Aware Time Series Generation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "0RMF9eYWmh", "reviewer": "Reviewer_4b7m", "review_text": "Comment: Thanks. I am not satisfied with the author's rebuttal. there are still a lot of issues that need to be clarified here.\n\nQ1: The motivation for using this strategy requires further analysis and validation.\n\nQ3: Based on the numerical experimental results, it is difficult to determine whether the performance improvement of the proposed method compared to diffusionTS is due to capturing extreme values. The differences between the two methods need further explanation. Why can't diffusionTS capture extreme values, but your method can? What is the motivation and principle behind this? You should use Wilcoxon-Holm analysis the results in Table 1 to demonstrate the advantages and disadvantages of the proposed method.\n\nQ4: What were the experimental setups for diffusionTS, Fourier-Flows, and RealNVP? How is the fairness of the comparison ensured?\n\nW1: Wilcoxon-Holm analysis should be used to demonstrate that the results in Table 2 indeed show a significant performance improvement. The mean values alone do not sufficiently indicate that your results are superior. I also do not think it is a good idea to place the ablation experiments in the supplementary materials.\n\nThe core challenge in time series generation tasks is the scarcity of data, yet diffusion models require large amounts of data for training. Please verify the performance of the proposed method on small-scale datasets, such as 10% of the stock dataset. I am certain that diffusionTS cannot handle this; please provide an explanation and validation. What is the significance of generating time series in the context of large-scale data?", "labeling_timestamp": "2026-01-11T17:35:19.747000", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states the motivation and underlying principle: extreme block maxima are tied to high-frequency components, diffusion noise attenuates those high-frequency components faster, and thus the authors propose inflating high-frequency components (and conditioning on block maxima / using GEV regularization) to preserve extremes. These points appear in the Introduction, Section 3, and Section 4.1.", "evidence": "Our key observation is that unusually large block maxima values, often linked to abrupt temporal changes, are strongly associated with high-frequency components of the time series. As the diffusion-based generative model gradually introduces noise with a linearly increasing variance schedule, it slowly diminishes the long-term trends (low-frequency components) of the time series while quickly attenuating the high-frequency components. These high-frequency components are crucial for reproducing extreme block maxima values.", "section": "Introduction (Section 1)"}
{"claim": "The authors did not perform Wilcoxon–Holm statistical analysis on the results in Table 1 to demonstrate significance.", "claim_type": "experimental", "paper_id": "5HQhYiGnYb", "paper_title": "FIDE: Frequency-Inflated Conditional Diffusion Model for Extreme-Aware Time Series Generation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "0RMF9eYWmh", "reviewer": "Reviewer_4b7m", "review_text": "Comment: Thanks. I am not satisfied with the author's rebuttal. there are still a lot of issues that need to be clarified here.\n\nQ1: The motivation for using this strategy requires further analysis and validation.\n\nQ3: Based on the numerical experimental results, it is difficult to determine whether the performance improvement of the proposed method compared to diffusionTS is due to capturing extreme values. The differences between the two methods need further explanation. Why can't diffusionTS capture extreme values, but your method can? What is the motivation and principle behind this? You should use Wilcoxon-Holm analysis the results in Table 1 to demonstrate the advantages and disadvantages of the proposed method.\n\nQ4: What were the experimental setups for diffusionTS, Fourier-Flows, and RealNVP? How is the fairness of the comparison ensured?\n\nW1: Wilcoxon-Holm analysis should be used to demonstrate that the results in Table 2 indeed show a significant performance improvement. The mean values alone do not sufficiently indicate that your results are superior. I also do not think it is a good idea to place the ablation experiments in the supplementary materials.\n\nThe core challenge in time series generation tasks is the scarcity of data, yet diffusion models require large amounts of data for training. Please verify the performance of the proposed method on small-scale datasets, such as 10% of the stock dataset. I am certain that diffusionTS cannot handle this; please provide an explanation and validation. What is the significance of generating time series in the context of large-scale data?", "labeling_timestamp": "2026-01-11T17:35:27.435972", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper content (Abstract through Section 4.2) does not include Table 1 or any mention of performing a Wilcoxon–Holm statistical test. Because the experimental/results section and Table 1 are not present in the supplied excerpt, there is insufficient information to confirm the reviewer’s claim that the authors did not perform the Wilcoxon–Holm analysis on Table 1.", "evidence": "\"Experimental results on real-world and synthetic data showcase the efficacy of FIDE over baseline methods, highlighting its potential in advancing Generative AI for time series analysis, specifically in accurately modeling extreme events.\"", "section": "Abstract (content provided covers Abstract through Section 4.2)"}
{"claim": "The experimental setups for diffusionTS, Fourier-Flows, and RealNVP are not described, preventing reproducibility and fair comparison.", "claim_type": "baseline", "paper_id": "5HQhYiGnYb", "paper_title": "FIDE: Frequency-Inflated Conditional Diffusion Model for Extreme-Aware Time Series Generation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "0RMF9eYWmh", "reviewer": "Reviewer_4b7m", "review_text": "Comment: Thanks. I am not satisfied with the author's rebuttal. there are still a lot of issues that need to be clarified here.\n\nQ1: The motivation for using this strategy requires further analysis and validation.\n\nQ3: Based on the numerical experimental results, it is difficult to determine whether the performance improvement of the proposed method compared to diffusionTS is due to capturing extreme values. The differences between the two methods need further explanation. Why can't diffusionTS capture extreme values, but your method can? What is the motivation and principle behind this? You should use Wilcoxon-Holm analysis the results in Table 1 to demonstrate the advantages and disadvantages of the proposed method.\n\nQ4: What were the experimental setups for diffusionTS, Fourier-Flows, and RealNVP? How is the fairness of the comparison ensured?\n\nW1: Wilcoxon-Holm analysis should be used to demonstrate that the results in Table 2 indeed show a significant performance improvement. The mean values alone do not sufficiently indicate that your results are superior. I also do not think it is a good idea to place the ablation experiments in the supplementary materials.\n\nThe core challenge in time series generation tasks is the scarcity of data, yet diffusion models require large amounts of data for training. Please verify the performance of the proposed method on small-scale datasets, such as 10% of the stock dataset. I am certain that diffusionTS cannot handle this; please provide an explanation and validation. What is the significance of generating time series in the context of large-scale data?", "labeling_timestamp": "2026-01-11T17:35:22.072245", "model": "gpt-5-mini", "label": "Supported", "justification": "The provided paper content mentions experimental comparisons to baseline methods but contains no descriptions of experimental setups or implementation details for diffusionTS, Fourier-Flows, or RealNVP. There is no Experiments/Implementation section or any baseline-specific setup information in the supplied sections, preventing reproduction or fair comparison based on the provided text.", "evidence": "Abstract: \"Experimental results on real-world and synthetic data showcase the efficacy of FIDE over baseline methods, highlighting its potential in advancing Generative AI for time series analysis, specifically in accurately modeling extreme events.\"", "section": "Abstract"}
{"claim": "The manuscript does not explain how fairness of the comparison between methods is ensured.", "claim_type": "baseline", "paper_id": "5HQhYiGnYb", "paper_title": "FIDE: Frequency-Inflated Conditional Diffusion Model for Extreme-Aware Time Series Generation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "0RMF9eYWmh", "reviewer": "Reviewer_4b7m", "review_text": "Comment: Thanks. I am not satisfied with the author's rebuttal. there are still a lot of issues that need to be clarified here.\n\nQ1: The motivation for using this strategy requires further analysis and validation.\n\nQ3: Based on the numerical experimental results, it is difficult to determine whether the performance improvement of the proposed method compared to diffusionTS is due to capturing extreme values. The differences between the two methods need further explanation. Why can't diffusionTS capture extreme values, but your method can? What is the motivation and principle behind this? You should use Wilcoxon-Holm analysis the results in Table 1 to demonstrate the advantages and disadvantages of the proposed method.\n\nQ4: What were the experimental setups for diffusionTS, Fourier-Flows, and RealNVP? How is the fairness of the comparison ensured?\n\nW1: Wilcoxon-Holm analysis should be used to demonstrate that the results in Table 2 indeed show a significant performance improvement. The mean values alone do not sufficiently indicate that your results are superior. I also do not think it is a good idea to place the ablation experiments in the supplementary materials.\n\nThe core challenge in time series generation tasks is the scarcity of data, yet diffusion models require large amounts of data for training. Please verify the performance of the proposed method on small-scale datasets, such as 10% of the stock dataset. I am certain that diffusionTS cannot handle this; please provide an explanation and validation. What is the significance of generating time series in the context of large-scale data?", "labeling_timestamp": "2026-01-11T17:35:35.372348", "model": "gpt-5-mini", "label": "Supported", "justification": "The provided manuscript text (Abstract, Introduction, and Sections 2–4) mentions experimental comparisons but does not include any description of how fairness between methods was ensured (no evaluation protocol, baseline selection criteria, hyperparameter tuning/selection procedures, or other controls are described in the supplied content).", "evidence": "\"Experimental results on real-world and synthetic data showcase the efficacy of FIDE over baseline methods, highlighting its potential in advancing Generative AI for time series analysis, specifically in accurately modeling extreme events.\"", "section": "Abstract"}
{"claim": "Table 2 lacks Wilcoxon–Holm significance testing, so reported mean values alone do not demonstrate superior performance.", "claim_type": "experimental", "paper_id": "5HQhYiGnYb", "paper_title": "FIDE: Frequency-Inflated Conditional Diffusion Model for Extreme-Aware Time Series Generation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "0RMF9eYWmh", "reviewer": "Reviewer_4b7m", "review_text": "Comment: Thanks. I am not satisfied with the author's rebuttal. there are still a lot of issues that need to be clarified here.\n\nQ1: The motivation for using this strategy requires further analysis and validation.\n\nQ3: Based on the numerical experimental results, it is difficult to determine whether the performance improvement of the proposed method compared to diffusionTS is due to capturing extreme values. The differences between the two methods need further explanation. Why can't diffusionTS capture extreme values, but your method can? What is the motivation and principle behind this? You should use Wilcoxon-Holm analysis the results in Table 1 to demonstrate the advantages and disadvantages of the proposed method.\n\nQ4: What were the experimental setups for diffusionTS, Fourier-Flows, and RealNVP? How is the fairness of the comparison ensured?\n\nW1: Wilcoxon-Holm analysis should be used to demonstrate that the results in Table 2 indeed show a significant performance improvement. The mean values alone do not sufficiently indicate that your results are superior. I also do not think it is a good idea to place the ablation experiments in the supplementary materials.\n\nThe core challenge in time series generation tasks is the scarcity of data, yet diffusion models require large amounts of data for training. Please verify the performance of the proposed method on small-scale datasets, such as 10% of the stock dataset. I am certain that diffusionTS cannot handle this; please provide an explanation and validation. What is the significance of generating time series in the context of large-scale data?", "labeling_timestamp": "2026-01-11T17:35:43.597409", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt does not include Table 2 or any experimental tables, nor does it mention Wilcoxon–Holm testing. Therefore we cannot verify the reviewer’s claim about the contents of Table 2 or the presence/absence of significance testing from the supplied text.", "evidence": "\"Experimental results on real-world and synthetic data showcase the efficacy of FIDE over baseline methods, highlighting its potential in advancing Generative AI for time series analysis, specifically in accurately modeling extreme events.\"", "section": "Abstract"}
{"claim": "Ablation experiments are placed in the supplementary materials, which reduces clarity and is inappropriate according to the reviewer.", "claim_type": "presentation", "paper_id": "5HQhYiGnYb", "paper_title": "FIDE: Frequency-Inflated Conditional Diffusion Model for Extreme-Aware Time Series Generation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "0RMF9eYWmh", "reviewer": "Reviewer_4b7m", "review_text": "Comment: Thanks. I am not satisfied with the author's rebuttal. there are still a lot of issues that need to be clarified here.\n\nQ1: The motivation for using this strategy requires further analysis and validation.\n\nQ3: Based on the numerical experimental results, it is difficult to determine whether the performance improvement of the proposed method compared to diffusionTS is due to capturing extreme values. The differences between the two methods need further explanation. Why can't diffusionTS capture extreme values, but your method can? What is the motivation and principle behind this? You should use Wilcoxon-Holm analysis the results in Table 1 to demonstrate the advantages and disadvantages of the proposed method.\n\nQ4: What were the experimental setups for diffusionTS, Fourier-Flows, and RealNVP? How is the fairness of the comparison ensured?\n\nW1: Wilcoxon-Holm analysis should be used to demonstrate that the results in Table 2 indeed show a significant performance improvement. The mean values alone do not sufficiently indicate that your results are superior. I also do not think it is a good idea to place the ablation experiments in the supplementary materials.\n\nThe core challenge in time series generation tasks is the scarcity of data, yet diffusion models require large amounts of data for training. Please verify the performance of the proposed method on small-scale datasets, such as 10% of the stock dataset. I am certain that diffusionTS cannot handle this; please provide an explanation and validation. What is the significance of generating time series in the context of large-scale data?", "labeling_timestamp": "2026-01-11T17:35:53.335222", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper references appendices for additional analysis and proofs (Appendix B and C) but does not mention any ablation experiments or state that ablations are placed in supplementary materials. Therefore the specific claim about ablation experiments being placed in the supplementary materials cannot be confirmed from the provided text.", "evidence": ["A more detailed analysis supporting this argument is given in Appendix B.", "see Appendix C for proof and details"], "section": ["3 On the Rapid Dissipation of Block Maxima in Diffusion Models", "4.1 High Frequency Components Inflation"]}
{"claim": "The paper fails to address the core challenge of time series data scarcity and does not test model performance under limited-data regimes.", "claim_type": "experimental", "paper_id": "5HQhYiGnYb", "paper_title": "FIDE: Frequency-Inflated Conditional Diffusion Model for Extreme-Aware Time Series Generation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "0RMF9eYWmh", "reviewer": "Reviewer_4b7m", "review_text": "Comment: Thanks. I am not satisfied with the author's rebuttal. there are still a lot of issues that need to be clarified here.\n\nQ1: The motivation for using this strategy requires further analysis and validation.\n\nQ3: Based on the numerical experimental results, it is difficult to determine whether the performance improvement of the proposed method compared to diffusionTS is due to capturing extreme values. The differences between the two methods need further explanation. Why can't diffusionTS capture extreme values, but your method can? What is the motivation and principle behind this? You should use Wilcoxon-Holm analysis the results in Table 1 to demonstrate the advantages and disadvantages of the proposed method.\n\nQ4: What were the experimental setups for diffusionTS, Fourier-Flows, and RealNVP? How is the fairness of the comparison ensured?\n\nW1: Wilcoxon-Holm analysis should be used to demonstrate that the results in Table 2 indeed show a significant performance improvement. The mean values alone do not sufficiently indicate that your results are superior. I also do not think it is a good idea to place the ablation experiments in the supplementary materials.\n\nThe core challenge in time series generation tasks is the scarcity of data, yet diffusion models require large amounts of data for training. Please verify the performance of the proposed method on small-scale datasets, such as 10% of the stock dataset. I am certain that diffusionTS cannot handle this; please provide an explanation and validation. What is the significance of generating time series in the context of large-scale data?", "labeling_timestamp": "2026-01-11T17:35:53.221453", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper's abstract, introduction, and methodological sections focus on preserving extreme values (block maxima) and techniques for high-frequency inflation and GEV regularization. Nowhere in the provided content does the paper discuss the core challenge of data scarcity or present experiments that evaluate model performance under limited-data regimes.", "evidence": "Abstract: \"Experimental results on real-world and synthetic data showcase the efficacy of FIDE over baseline methods, highlighting its potential in advancing Generative AI for time series analysis, specifically in accurately modeling extreme events.\" \nIntroduction: \"In this study, we examine how effectively diffusion models preserve extreme values in the form of block maxima...\" \nSection 4 (overview): Description of high-frequency inflation and conditional diffusion (no mention of limited-data evaluation).", "section": "Abstract; 1 Introduction; 4 Proposed Framework"}
{"claim": "The authors did not verify the proposed method's performance on small-scale datasets such as 10% of the stock dataset.", "claim_type": "experimental", "paper_id": "5HQhYiGnYb", "paper_title": "FIDE: Frequency-Inflated Conditional Diffusion Model for Extreme-Aware Time Series Generation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "0RMF9eYWmh", "reviewer": "Reviewer_4b7m", "review_text": "Comment: Thanks. I am not satisfied with the author's rebuttal. there are still a lot of issues that need to be clarified here.\n\nQ1: The motivation for using this strategy requires further analysis and validation.\n\nQ3: Based on the numerical experimental results, it is difficult to determine whether the performance improvement of the proposed method compared to diffusionTS is due to capturing extreme values. The differences between the two methods need further explanation. Why can't diffusionTS capture extreme values, but your method can? What is the motivation and principle behind this? You should use Wilcoxon-Holm analysis the results in Table 1 to demonstrate the advantages and disadvantages of the proposed method.\n\nQ4: What were the experimental setups for diffusionTS, Fourier-Flows, and RealNVP? How is the fairness of the comparison ensured?\n\nW1: Wilcoxon-Holm analysis should be used to demonstrate that the results in Table 2 indeed show a significant performance improvement. The mean values alone do not sufficiently indicate that your results are superior. I also do not think it is a good idea to place the ablation experiments in the supplementary materials.\n\nThe core challenge in time series generation tasks is the scarcity of data, yet diffusion models require large amounts of data for training. Please verify the performance of the proposed method on small-scale datasets, such as 10% of the stock dataset. I am certain that diffusionTS cannot handle this; please provide an explanation and validation. What is the significance of generating time series in the context of large-scale data?", "labeling_timestamp": "2026-01-11T17:35:58.634395", "model": "gpt-5-mini", "label": "Supported", "justification": "The provided paper text only states that experiments were run on \"real-world and synthetic data\" but gives no mention of a stock dataset or of experiments on small-scale subsets (e.g., 10% of a stock dataset). Therefore the claim that the authors did not verify performance on such a small-scale (10% stock) dataset is consistent with the paper content provided.", "evidence": "Experimental results on real-world and synthetic data showcase the efficacy of FIDE over baseline methods, highlighting its potential in advancing Generative AI for time series analysis, specifically in accurately modeling extreme events.", "section": "Abstract"}
{"claim": "The paper does not provide empirical validation or explanation that diffusionTS cannot handle small-scale datasets.", "claim_type": "experimental", "paper_id": "5HQhYiGnYb", "paper_title": "FIDE: Frequency-Inflated Conditional Diffusion Model for Extreme-Aware Time Series Generation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "0RMF9eYWmh", "reviewer": "Reviewer_4b7m", "review_text": "Comment: Thanks. I am not satisfied with the author's rebuttal. there are still a lot of issues that need to be clarified here.\n\nQ1: The motivation for using this strategy requires further analysis and validation.\n\nQ3: Based on the numerical experimental results, it is difficult to determine whether the performance improvement of the proposed method compared to diffusionTS is due to capturing extreme values. The differences between the two methods need further explanation. Why can't diffusionTS capture extreme values, but your method can? What is the motivation and principle behind this? You should use Wilcoxon-Holm analysis the results in Table 1 to demonstrate the advantages and disadvantages of the proposed method.\n\nQ4: What were the experimental setups for diffusionTS, Fourier-Flows, and RealNVP? How is the fairness of the comparison ensured?\n\nW1: Wilcoxon-Holm analysis should be used to demonstrate that the results in Table 2 indeed show a significant performance improvement. The mean values alone do not sufficiently indicate that your results are superior. I also do not think it is a good idea to place the ablation experiments in the supplementary materials.\n\nThe core challenge in time series generation tasks is the scarcity of data, yet diffusion models require large amounts of data for training. Please verify the performance of the proposed method on small-scale datasets, such as 10% of the stock dataset. I am certain that diffusionTS cannot handle this; please provide an explanation and validation. What is the significance of generating time series in the context of large-scale data?", "labeling_timestamp": "2026-01-11T17:35:59.562196", "model": "gpt-5-mini", "label": "Supported", "justification": "The provided paper text makes no mention of 'diffusionTS' nor of experiments or analysis specific to small-scale datasets. Therefore the reviewer's claim that the paper does not provide empirical validation or explanation that diffusionTS cannot handle small-scale datasets is accurate with respect to the content shown.", "evidence": "\"Experimental results on real-world and synthetic data showcase the efficacy of FIDE over baseline methods, highlighting its potential in advancing Generative AI for time series analysis, specifically in accurately modeling extreme events.\"", "section": "Abstract"}
{"claim": "The significance and motivation for generating time series when large-scale data is available are not explained.", "claim_type": "subjective", "paper_id": "5HQhYiGnYb", "paper_title": "FIDE: Frequency-Inflated Conditional Diffusion Model for Extreme-Aware Time Series Generation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "0RMF9eYWmh", "reviewer": "Reviewer_4b7m", "review_text": "Comment: Thanks. I am not satisfied with the author's rebuttal. there are still a lot of issues that need to be clarified here.\n\nQ1: The motivation for using this strategy requires further analysis and validation.\n\nQ3: Based on the numerical experimental results, it is difficult to determine whether the performance improvement of the proposed method compared to diffusionTS is due to capturing extreme values. The differences between the two methods need further explanation. Why can't diffusionTS capture extreme values, but your method can? What is the motivation and principle behind this? You should use Wilcoxon-Holm analysis the results in Table 1 to demonstrate the advantages and disadvantages of the proposed method.\n\nQ4: What were the experimental setups for diffusionTS, Fourier-Flows, and RealNVP? How is the fairness of the comparison ensured?\n\nW1: Wilcoxon-Holm analysis should be used to demonstrate that the results in Table 2 indeed show a significant performance improvement. The mean values alone do not sufficiently indicate that your results are superior. I also do not think it is a good idea to place the ablation experiments in the supplementary materials.\n\nThe core challenge in time series generation tasks is the scarcity of data, yet diffusion models require large amounts of data for training. Please verify the performance of the proposed method on small-scale datasets, such as 10% of the stock dataset. I am certain that diffusionTS cannot handle this; please provide an explanation and validation. What is the significance of generating time series in the context of large-scale data?", "labeling_timestamp": "2026-01-11T17:36:06.174303", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper gives general motivations for time series generation (data augmentation, learning distributions, uncertainty estimation, and applications needing extreme-value modeling) but does not discuss or justify why one should generate synthetic time series specifically in the case where large-scale real data is already available. There is no discussion of the trade-offs or rationale for generation when ample data exists.", "evidence": "\"Effective generative modeling of these extremes is important as it aids in learning the underlying data distribution, facilitating data augmentation, and improving uncertainty estimation, all of which are crucial for developing robust risk management strategies and enhancing disaster preparedness measures.\"", "section": "1 Introduction"}
{"claim": "Using an MLP component sacrifices interpretability by obscuring mechanistic understanding of the model's neuron-like operations.", "claim_type": "subjective", "paper_id": "vE1e1mLJ0U", "paper_title": "The Expressive Leaky Memory Neuron: an Efficient and Expressive Phenomenological Neuron Model Can Solve Long-Horizon Tasks.", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "2c423ciTlo", "reviewer": "Reviewer_27bc", "review_text": "Summary: This paper proposes artificial neural network models that incorporates important inductive bias (i.e. leaky memory dynamics, nonlinear synaptic integration) inspired from biological cortical neurons. The model is aimed to achieve two types of goals, one is to match the spike-level dynamics of pyramidal neuron, and the second is evaluated on bio-inspired tasks to evaluate temporal integration. For evaluation, they compare the model with others SOTA baselines (SNN, LSTM, Transformer) are evaluated on multiple biological inspired datasets and long sequence modeling task. It shows the benefits of efficiency in parameterization, and comparable or better performance compared to SOTA models. Meanwhile, the hyper-parameter tuning studies show overlap with previous literatures.\n\nStrengths: 1. The paper is well-motivated, and take the reductionist view to minimize the parameters from a more detailed modeling for cortical neurons, and aim to address the computational efficiency needs of standard models.\n2. Solid evaluations on multiple biological inspired datasets, and compared with multiple SOTA models, and follow by multiple hyperparameter tuning studies. \n3. The biological realism side shows interesting overlaps with previous neuroscience literatures.\n4. The results show the model is capable of achieving better accuracy with efficiency and fewer parameters than traditional deep model LSTM. The finding about simplification does not sacrifice the predictive performance is valuable.\n5. The paper is well-written, and organized in a good structure.\n\nWeaknesses: 1. This paper is aimed to balance the trade-off between fidelity, efficiency and biological realism. It did a fair job while still failed to capture some important aspects. For example, using MLP sacrifices the interpretability and biological realism to compensate accuracy. On the other hand, the model still sacrifices the accuracy and has a big performance gap when compared to SOTA models (S4 and Mega). \n2. Scalability of the method: as scaling law plays a big role for improving transformer's predictivity, one concern is that transformer might perform better with increasing number of parameters. However, it might not be the same for ELM. As firstly shown in Fig 3, the accuracy quickly saturates with simply increasing $d_m$ and $d_{mlp}$, and not able to get further improved to minimize the performance gap between ELM and Mega.\n3. Only one ML task is evaluated, more evaluations and benchmarks needed to demonstrate contributions in addressing long-range sequence modeling.\n4. As shown in Table S1, large number of hyper-parameters still needed in advance or be tuned based on prior knowledge from neuroscience literatures.\n5. The efficiency side might be over-claimed, as Table 1 shows ELM still requires 100k-200k parameters?\n\nQuestions: 1. What other critical components might be helpful to improve ELM model accuracy?\n2. After applying sparse regularization or quantization to S4 and Mega to match the number of parameters to ELM, how much accuracy drop they will have?\n3. Is the model able to be trained with other biological plausible learning rules instead of BPTT? How they might end up with different parameters?", "labeling_timestamp": "2026-01-11T17:36:20.692316", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly states that the MLP and synapse parameters are not designed for direct biophysical interpretability and that the MLP cannot provide a mechanistic explanation of neuronal phenomena, which matches the reviewer's claim that using an MLP obscures mechanistic understanding.", "evidence": "1) \"individual parameters of the ELM neuron are not designed for direct biophysical interpretability.\"  \n2) \"Despite the biological inspiration, the MLP and synapses are only intended to capture the neuron analogous plasticity and dendritic nonlinearity , and cannot give a mechanistic explanation of these phenomena in neuron.\"", "section": "2 THE EXPRESSIVE LEAKY MEMORY NEURON (The integration mechanism dynamics)"}
{"claim": "Incorporating an MLP sacrifices biological realism because it departs from biologically plausible neuron computations.", "claim_type": "subjective", "paper_id": "vE1e1mLJ0U", "paper_title": "The Expressive Leaky Memory Neuron: an Efficient and Expressive Phenomenological Neuron Model Can Solve Long-Horizon Tasks.", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "2c423ciTlo", "reviewer": "Reviewer_27bc", "review_text": "Summary: This paper proposes artificial neural network models that incorporates important inductive bias (i.e. leaky memory dynamics, nonlinear synaptic integration) inspired from biological cortical neurons. The model is aimed to achieve two types of goals, one is to match the spike-level dynamics of pyramidal neuron, and the second is evaluated on bio-inspired tasks to evaluate temporal integration. For evaluation, they compare the model with others SOTA baselines (SNN, LSTM, Transformer) are evaluated on multiple biological inspired datasets and long sequence modeling task. It shows the benefits of efficiency in parameterization, and comparable or better performance compared to SOTA models. Meanwhile, the hyper-parameter tuning studies show overlap with previous literatures.\n\nStrengths: 1. The paper is well-motivated, and take the reductionist view to minimize the parameters from a more detailed modeling for cortical neurons, and aim to address the computational efficiency needs of standard models.\n2. Solid evaluations on multiple biological inspired datasets, and compared with multiple SOTA models, and follow by multiple hyperparameter tuning studies. \n3. The biological realism side shows interesting overlaps with previous neuroscience literatures.\n4. The results show the model is capable of achieving better accuracy with efficiency and fewer parameters than traditional deep model LSTM. The finding about simplification does not sacrifice the predictive performance is valuable.\n5. The paper is well-written, and organized in a good structure.\n\nWeaknesses: 1. This paper is aimed to balance the trade-off between fidelity, efficiency and biological realism. It did a fair job while still failed to capture some important aspects. For example, using MLP sacrifices the interpretability and biological realism to compensate accuracy. On the other hand, the model still sacrifices the accuracy and has a big performance gap when compared to SOTA models (S4 and Mega). \n2. Scalability of the method: as scaling law plays a big role for improving transformer's predictivity, one concern is that transformer might perform better with increasing number of parameters. However, it might not be the same for ELM. As firstly shown in Fig 3, the accuracy quickly saturates with simply increasing $d_m$ and $d_{mlp}$, and not able to get further improved to minimize the performance gap between ELM and Mega.\n3. Only one ML task is evaluated, more evaluations and benchmarks needed to demonstrate contributions in addressing long-range sequence modeling.\n4. As shown in Table S1, large number of hyper-parameters still needed in advance or be tuned based on prior knowledge from neuroscience literatures.\n5. The efficiency side might be over-claimed, as Table 1 shows ELM still requires 100k-200k parameters?\n\nQuestions: 1. What other critical components might be helpful to improve ELM model accuracy?\n2. After applying sparse regularization or quantization to S4 and Mega to match the number of parameters to ELM, how much accuracy drop they will have?\n3. Is the model able to be trained with other biological plausible learning rules instead of BPTT? How they might end up with different parameters?", "labeling_timestamp": "2026-01-11T17:36:27.281228", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly acknowledges that it abstracts away low-level biological processes for computational efficiency and that the MLP is used as a phenomenological proxy for dendritic nonlinearity and plasticity rather than a mechanistic, biophysically plausible implementation. This indicates that using an MLP reduces biological realism.", "evidence": "\"While biologically inspired, low-level biological processes are abstracted away for computational efficiency, and consequently, individual parameters of the ELM neuron are not designed for direct biophysical interpretability.\" \n\n\"Despite the biological inspiration, the MLP and synapses are only intended to capture the neuron analogous plasticity and dendritic nonlinearity , and cannot give a mechanistic explanation of these phenomena in neuron.\"", "section": "1 INTRODUCTION; 2 THE EXPRESSIVE LEAKY MEMORY NEURON - The integration mechanism dynamics"}
{"claim": "The proposed model exhibits a large accuracy gap compared with state-of-the-art models S4 and Mega on evaluated tasks.", "claim_type": "experimental", "paper_id": "vE1e1mLJ0U", "paper_title": "The Expressive Leaky Memory Neuron: an Efficient and Expressive Phenomenological Neuron Model Can Solve Long-Horizon Tasks.", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "2c423ciTlo", "reviewer": "Reviewer_27bc", "review_text": "Summary: This paper proposes artificial neural network models that incorporates important inductive bias (i.e. leaky memory dynamics, nonlinear synaptic integration) inspired from biological cortical neurons. The model is aimed to achieve two types of goals, one is to match the spike-level dynamics of pyramidal neuron, and the second is evaluated on bio-inspired tasks to evaluate temporal integration. For evaluation, they compare the model with others SOTA baselines (SNN, LSTM, Transformer) are evaluated on multiple biological inspired datasets and long sequence modeling task. It shows the benefits of efficiency in parameterization, and comparable or better performance compared to SOTA models. Meanwhile, the hyper-parameter tuning studies show overlap with previous literatures.\n\nStrengths: 1. The paper is well-motivated, and take the reductionist view to minimize the parameters from a more detailed modeling for cortical neurons, and aim to address the computational efficiency needs of standard models.\n2. Solid evaluations on multiple biological inspired datasets, and compared with multiple SOTA models, and follow by multiple hyperparameter tuning studies. \n3. The biological realism side shows interesting overlaps with previous neuroscience literatures.\n4. The results show the model is capable of achieving better accuracy with efficiency and fewer parameters than traditional deep model LSTM. The finding about simplification does not sacrifice the predictive performance is valuable.\n5. The paper is well-written, and organized in a good structure.\n\nWeaknesses: 1. This paper is aimed to balance the trade-off between fidelity, efficiency and biological realism. It did a fair job while still failed to capture some important aspects. For example, using MLP sacrifices the interpretability and biological realism to compensate accuracy. On the other hand, the model still sacrifices the accuracy and has a big performance gap when compared to SOTA models (S4 and Mega). \n2. Scalability of the method: as scaling law plays a big role for improving transformer's predictivity, one concern is that transformer might perform better with increasing number of parameters. However, it might not be the same for ELM. As firstly shown in Fig 3, the accuracy quickly saturates with simply increasing $d_m$ and $d_{mlp}$, and not able to get further improved to minimize the performance gap between ELM and Mega.\n3. Only one ML task is evaluated, more evaluations and benchmarks needed to demonstrate contributions in addressing long-range sequence modeling.\n4. As shown in Table S1, large number of hyper-parameters still needed in advance or be tuned based on prior knowledge from neuroscience literatures.\n5. The efficiency side might be over-claimed, as Table 1 shows ELM still requires 100k-200k parameters?\n\nQuestions: 1. What other critical components might be helpful to improve ELM model accuracy?\n2. After applying sparse regularization or quantization to S4 and Mega to match the number of parameters to ELM, how much accuracy drop they will have?\n3. Is the model able to be trained with other biological plausible learning rules instead of BPTT? How they might end up with different parameters?", "labeling_timestamp": "2026-01-11T17:36:29.700026", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper's provided content reports comparisons to classic Transformer and Chrono-LSTM models (and mentions outperforming them on LRA and solving Pathfinder-X), but it does not mention or report results versus S4 or Mega in the excerpts given. Therefore there is insufficient information to determine the claimed large accuracy gap relative to S4 and Mega.", "evidence": "the ELM neuron displays substantial long-range processing capabilities, reliably outperforming the classic Transformer or Chrono-LSTM architectures on LRA, and even solving the Pathfinder-X task with over 70% accuracy (16k context length).", "section": "Abstract"}
{"claim": "ELM's accuracy quickly saturates as dm and dmlp increase, preventing further improvements and maintaining a performance gap with Mega.", "claim_type": "experimental", "paper_id": "vE1e1mLJ0U", "paper_title": "The Expressive Leaky Memory Neuron: an Efficient and Expressive Phenomenological Neuron Model Can Solve Long-Horizon Tasks.", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "2c423ciTlo", "reviewer": "Reviewer_27bc", "review_text": "Summary: This paper proposes artificial neural network models that incorporates important inductive bias (i.e. leaky memory dynamics, nonlinear synaptic integration) inspired from biological cortical neurons. The model is aimed to achieve two types of goals, one is to match the spike-level dynamics of pyramidal neuron, and the second is evaluated on bio-inspired tasks to evaluate temporal integration. For evaluation, they compare the model with others SOTA baselines (SNN, LSTM, Transformer) are evaluated on multiple biological inspired datasets and long sequence modeling task. It shows the benefits of efficiency in parameterization, and comparable or better performance compared to SOTA models. Meanwhile, the hyper-parameter tuning studies show overlap with previous literatures.\n\nStrengths: 1. The paper is well-motivated, and take the reductionist view to minimize the parameters from a more detailed modeling for cortical neurons, and aim to address the computational efficiency needs of standard models.\n2. Solid evaluations on multiple biological inspired datasets, and compared with multiple SOTA models, and follow by multiple hyperparameter tuning studies. \n3. The biological realism side shows interesting overlaps with previous neuroscience literatures.\n4. The results show the model is capable of achieving better accuracy with efficiency and fewer parameters than traditional deep model LSTM. The finding about simplification does not sacrifice the predictive performance is valuable.\n5. The paper is well-written, and organized in a good structure.\n\nWeaknesses: 1. This paper is aimed to balance the trade-off between fidelity, efficiency and biological realism. It did a fair job while still failed to capture some important aspects. For example, using MLP sacrifices the interpretability and biological realism to compensate accuracy. On the other hand, the model still sacrifices the accuracy and has a big performance gap when compared to SOTA models (S4 and Mega). \n2. Scalability of the method: as scaling law plays a big role for improving transformer's predictivity, one concern is that transformer might perform better with increasing number of parameters. However, it might not be the same for ELM. As firstly shown in Fig 3, the accuracy quickly saturates with simply increasing $d_m$ and $d_{mlp}$, and not able to get further improved to minimize the performance gap between ELM and Mega.\n3. Only one ML task is evaluated, more evaluations and benchmarks needed to demonstrate contributions in addressing long-range sequence modeling.\n4. As shown in Table S1, large number of hyper-parameters still needed in advance or be tuned based on prior knowledge from neuroscience literatures.\n5. The efficiency side might be over-claimed, as Table 1 shows ELM still requires 100k-200k parameters?\n\nQuestions: 1. What other critical components might be helpful to improve ELM model accuracy?\n2. After applying sparse regularization or quantization to S4 and Mega to match the number of parameters to ELM, how much accuracy drop they will have?\n3. Is the model able to be trained with other biological plausible learning rules instead of BPTT? How they might end up with different parameters?", "labeling_timestamp": "2026-01-11T17:36:44.559785", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does not state that ELM's accuracy \"quickly saturates\" as dm and dmlp increase; rather it emphasizes that larger numbers of memory units and richer MLP integration improve long-range performance. The manuscript also does not mention or compare to a model named \"Mega,\" so the claimed persistent performance gap with Mega cannot be verified from the provided text.", "evidence": "1) \"Leveraging a larger number of memory units with sufficiently long timescales, and correspondingly sophisticated synaptic integration, the ELM neuron displays substantial long-range processing capabilities, reliably outperforming the classic Transformer or Chrono-LSTM architectures on LRA, and even solving the Pathfinder-X task with over 70% accuracy (16k context length).\" (Abstract)\n2) \"In the ELM neuron, we achieve this by making the number of memory units d m a hyper-parameter and equipping each of them with a τ m (always learnable), setting it apart most other computational neuroscience models.\" (Section 2: The memory unit dynamics)", "section": "Abstract; Section 2 (The memory unit dynamics)"}
{"claim": "Transformers are likely to benefit more from increasing parameter counts than ELM, raising concerns about the method's scalability with model size.", "claim_type": "experimental", "paper_id": "vE1e1mLJ0U", "paper_title": "The Expressive Leaky Memory Neuron: an Efficient and Expressive Phenomenological Neuron Model Can Solve Long-Horizon Tasks.", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "2c423ciTlo", "reviewer": "Reviewer_27bc", "review_text": "Summary: This paper proposes artificial neural network models that incorporates important inductive bias (i.e. leaky memory dynamics, nonlinear synaptic integration) inspired from biological cortical neurons. The model is aimed to achieve two types of goals, one is to match the spike-level dynamics of pyramidal neuron, and the second is evaluated on bio-inspired tasks to evaluate temporal integration. For evaluation, they compare the model with others SOTA baselines (SNN, LSTM, Transformer) are evaluated on multiple biological inspired datasets and long sequence modeling task. It shows the benefits of efficiency in parameterization, and comparable or better performance compared to SOTA models. Meanwhile, the hyper-parameter tuning studies show overlap with previous literatures.\n\nStrengths: 1. The paper is well-motivated, and take the reductionist view to minimize the parameters from a more detailed modeling for cortical neurons, and aim to address the computational efficiency needs of standard models.\n2. Solid evaluations on multiple biological inspired datasets, and compared with multiple SOTA models, and follow by multiple hyperparameter tuning studies. \n3. The biological realism side shows interesting overlaps with previous neuroscience literatures.\n4. The results show the model is capable of achieving better accuracy with efficiency and fewer parameters than traditional deep model LSTM. The finding about simplification does not sacrifice the predictive performance is valuable.\n5. The paper is well-written, and organized in a good structure.\n\nWeaknesses: 1. This paper is aimed to balance the trade-off between fidelity, efficiency and biological realism. It did a fair job while still failed to capture some important aspects. For example, using MLP sacrifices the interpretability and biological realism to compensate accuracy. On the other hand, the model still sacrifices the accuracy and has a big performance gap when compared to SOTA models (S4 and Mega). \n2. Scalability of the method: as scaling law plays a big role for improving transformer's predictivity, one concern is that transformer might perform better with increasing number of parameters. However, it might not be the same for ELM. As firstly shown in Fig 3, the accuracy quickly saturates with simply increasing $d_m$ and $d_{mlp}$, and not able to get further improved to minimize the performance gap between ELM and Mega.\n3. Only one ML task is evaluated, more evaluations and benchmarks needed to demonstrate contributions in addressing long-range sequence modeling.\n4. As shown in Table S1, large number of hyper-parameters still needed in advance or be tuned based on prior knowledge from neuroscience literatures.\n5. The efficiency side might be over-claimed, as Table 1 shows ELM still requires 100k-200k parameters?\n\nQuestions: 1. What other critical components might be helpful to improve ELM model accuracy?\n2. After applying sparse regularization or quantization to S4 and Mega to match the number of parameters to ELM, how much accuracy drop they will have?\n3. Is the model able to be trained with other biological plausible learning rules instead of BPTT? How they might end up with different parameters?", "labeling_timestamp": "2026-01-11T17:36:32.530910", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper compares ELM to Transformers on LRA (reporting ELM often outperforms classic Transformer) and emphasizes ELM's parameter efficiency versus TCNs, but it does not present any experiments or analysis about how performance of Transformers versus ELM changes with increasing parameter count or discuss relative scalability with model size.", "evidence": "\"the ELM neuron displays substantial long-range processing capabilities, reliably outperforming the classic Transformer or Chrono-LSTM architectures on LRA\" | \"our ELM neuron is expressive enough to accurately match the spike level I/O of a detailed biophysical model ... with a few thousand parameters, in stark contrast to the millions of parameters required by temporal convolutional networks.\"", "section": "Abstract; 1 INTRODUCTION"}
{"claim": "The paper evaluates only one machine learning task, lacking broader benchmarks to demonstrate contributions for long-range sequence modeling.", "claim_type": "experimental", "paper_id": "vE1e1mLJ0U", "paper_title": "The Expressive Leaky Memory Neuron: an Efficient and Expressive Phenomenological Neuron Model Can Solve Long-Horizon Tasks.", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "2c423ciTlo", "reviewer": "Reviewer_27bc", "review_text": "Summary: This paper proposes artificial neural network models that incorporates important inductive bias (i.e. leaky memory dynamics, nonlinear synaptic integration) inspired from biological cortical neurons. The model is aimed to achieve two types of goals, one is to match the spike-level dynamics of pyramidal neuron, and the second is evaluated on bio-inspired tasks to evaluate temporal integration. For evaluation, they compare the model with others SOTA baselines (SNN, LSTM, Transformer) are evaluated on multiple biological inspired datasets and long sequence modeling task. It shows the benefits of efficiency in parameterization, and comparable or better performance compared to SOTA models. Meanwhile, the hyper-parameter tuning studies show overlap with previous literatures.\n\nStrengths: 1. The paper is well-motivated, and take the reductionist view to minimize the parameters from a more detailed modeling for cortical neurons, and aim to address the computational efficiency needs of standard models.\n2. Solid evaluations on multiple biological inspired datasets, and compared with multiple SOTA models, and follow by multiple hyperparameter tuning studies. \n3. The biological realism side shows interesting overlaps with previous neuroscience literatures.\n4. The results show the model is capable of achieving better accuracy with efficiency and fewer parameters than traditional deep model LSTM. The finding about simplification does not sacrifice the predictive performance is valuable.\n5. The paper is well-written, and organized in a good structure.\n\nWeaknesses: 1. This paper is aimed to balance the trade-off between fidelity, efficiency and biological realism. It did a fair job while still failed to capture some important aspects. For example, using MLP sacrifices the interpretability and biological realism to compensate accuracy. On the other hand, the model still sacrifices the accuracy and has a big performance gap when compared to SOTA models (S4 and Mega). \n2. Scalability of the method: as scaling law plays a big role for improving transformer's predictivity, one concern is that transformer might perform better with increasing number of parameters. However, it might not be the same for ELM. As firstly shown in Fig 3, the accuracy quickly saturates with simply increasing $d_m$ and $d_{mlp}$, and not able to get further improved to minimize the performance gap between ELM and Mega.\n3. Only one ML task is evaluated, more evaluations and benchmarks needed to demonstrate contributions in addressing long-range sequence modeling.\n4. As shown in Table S1, large number of hyper-parameters still needed in advance or be tuned based on prior knowledge from neuroscience literatures.\n5. The efficiency side might be over-claimed, as Table 1 shows ELM still requires 100k-200k parameters?\n\nQuestions: 1. What other critical components might be helpful to improve ELM model accuracy?\n2. After applying sparse regularization or quantization to S4 and Mega to match the number of parameters to ELM, how much accuracy drop they will have?\n3. Is the model able to be trained with other biological plausible learning rules instead of BPTT? How they might end up with different parameters?", "labeling_timestamp": "2026-01-11T17:36:41.780618", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper evaluates multiple long-range sequence modeling benchmarks (Long Range Arena datasets, a neuromorphic SHD-Adding task, and Pathfinder-X), so the claim that it evaluates only one machine learning task is false.", "evidence": "Abstract: \"we evaluate it on various tasks with demanding temporal structures, including the Long Range Arena (LRA) datasets, as well as a novel neuromorphic dataset based on the Spiking Heidelberg Digits dataset (SHD-Adding).\"; Introduction (Sec. 1): \"We subsequently evaluate the ELM neuron on the well-established long sequence modeling LRA benchmarks from the machine learning literature, including the notoriously challenging Pathfinder-X task, where it achieves over 70% accuracy...\"", "section": "Abstract; Introduction (Section 1)"}
{"claim": "The model requires a large number of hyperparameters that must be specified in advance or tuned using neuroscience prior knowledge.", "claim_type": "methodology", "paper_id": "vE1e1mLJ0U", "paper_title": "The Expressive Leaky Memory Neuron: an Efficient and Expressive Phenomenological Neuron Model Can Solve Long-Horizon Tasks.", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "2c423ciTlo", "reviewer": "Reviewer_27bc", "review_text": "Summary: This paper proposes artificial neural network models that incorporates important inductive bias (i.e. leaky memory dynamics, nonlinear synaptic integration) inspired from biological cortical neurons. The model is aimed to achieve two types of goals, one is to match the spike-level dynamics of pyramidal neuron, and the second is evaluated on bio-inspired tasks to evaluate temporal integration. For evaluation, they compare the model with others SOTA baselines (SNN, LSTM, Transformer) are evaluated on multiple biological inspired datasets and long sequence modeling task. It shows the benefits of efficiency in parameterization, and comparable or better performance compared to SOTA models. Meanwhile, the hyper-parameter tuning studies show overlap with previous literatures.\n\nStrengths: 1. The paper is well-motivated, and take the reductionist view to minimize the parameters from a more detailed modeling for cortical neurons, and aim to address the computational efficiency needs of standard models.\n2. Solid evaluations on multiple biological inspired datasets, and compared with multiple SOTA models, and follow by multiple hyperparameter tuning studies. \n3. The biological realism side shows interesting overlaps with previous neuroscience literatures.\n4. The results show the model is capable of achieving better accuracy with efficiency and fewer parameters than traditional deep model LSTM. The finding about simplification does not sacrifice the predictive performance is valuable.\n5. The paper is well-written, and organized in a good structure.\n\nWeaknesses: 1. This paper is aimed to balance the trade-off between fidelity, efficiency and biological realism. It did a fair job while still failed to capture some important aspects. For example, using MLP sacrifices the interpretability and biological realism to compensate accuracy. On the other hand, the model still sacrifices the accuracy and has a big performance gap when compared to SOTA models (S4 and Mega). \n2. Scalability of the method: as scaling law plays a big role for improving transformer's predictivity, one concern is that transformer might perform better with increasing number of parameters. However, it might not be the same for ELM. As firstly shown in Fig 3, the accuracy quickly saturates with simply increasing $d_m$ and $d_{mlp}$, and not able to get further improved to minimize the performance gap between ELM and Mega.\n3. Only one ML task is evaluated, more evaluations and benchmarks needed to demonstrate contributions in addressing long-range sequence modeling.\n4. As shown in Table S1, large number of hyper-parameters still needed in advance or be tuned based on prior knowledge from neuroscience literatures.\n5. The efficiency side might be over-claimed, as Table 1 shows ELM still requires 100k-200k parameters?\n\nQuestions: 1. What other critical components might be helpful to improve ELM model accuracy?\n2. After applying sparse regularization or quantization to S4 and Mega to match the number of parameters to ELM, how much accuracy drop they will have?\n3. Is the model able to be trained with other biological plausible learning rules instead of BPTT? How they might end up with different parameters?", "labeling_timestamp": "2026-01-11T17:37:14.003807", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper acknowledges several architectural hyperparameters (e.g., number of memory units d_m, MLP size, branching parameters) that must be set, but many internal timescales and weights are described as learnable rather than requiring prior neuroscience tuning. The claim is therefore partially true (there are hyperparameters), but the paper does not state they must be specified or tuned using neuroscience prior knowledge.", "evidence": "1) \"In the ELM neuron, we achieve this by making the number of memory units d m a hyper-parameter and equipping each of them with a τ m (always learnable), setting it apart most other computational neuroscience models.\" 2) \"parameterize the input integration using a Multilayer Perceptron (MLP) ( w p always learnable, with l mlp = 1 and d mlp = 2 d m )\" 3) \"In the Branch-ELM variant ... d tree ∗ d brch = d s ... in this variant the w s need to be learnable, as they are responsible for weighting the sum and cannot be absorbed in the MLP later.\" 4) \"individual parameters of the ELM neuron are not designed for direct biophysical interpretability.\"", "section": "Section 2 (The memory unit dynamics; The integration mechanism dynamics) and Introduction"}
{"claim": "The paper's efficiency claims may be overstated because Table 1 shows ELM still requires roughly 100,000–200,000 parameters.", "claim_type": "quantitative", "paper_id": "vE1e1mLJ0U", "paper_title": "The Expressive Leaky Memory Neuron: an Efficient and Expressive Phenomenological Neuron Model Can Solve Long-Horizon Tasks.", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "2c423ciTlo", "reviewer": "Reviewer_27bc", "review_text": "Summary: This paper proposes artificial neural network models that incorporates important inductive bias (i.e. leaky memory dynamics, nonlinear synaptic integration) inspired from biological cortical neurons. The model is aimed to achieve two types of goals, one is to match the spike-level dynamics of pyramidal neuron, and the second is evaluated on bio-inspired tasks to evaluate temporal integration. For evaluation, they compare the model with others SOTA baselines (SNN, LSTM, Transformer) are evaluated on multiple biological inspired datasets and long sequence modeling task. It shows the benefits of efficiency in parameterization, and comparable or better performance compared to SOTA models. Meanwhile, the hyper-parameter tuning studies show overlap with previous literatures.\n\nStrengths: 1. The paper is well-motivated, and take the reductionist view to minimize the parameters from a more detailed modeling for cortical neurons, and aim to address the computational efficiency needs of standard models.\n2. Solid evaluations on multiple biological inspired datasets, and compared with multiple SOTA models, and follow by multiple hyperparameter tuning studies. \n3. The biological realism side shows interesting overlaps with previous neuroscience literatures.\n4. The results show the model is capable of achieving better accuracy with efficiency and fewer parameters than traditional deep model LSTM. The finding about simplification does not sacrifice the predictive performance is valuable.\n5. The paper is well-written, and organized in a good structure.\n\nWeaknesses: 1. This paper is aimed to balance the trade-off between fidelity, efficiency and biological realism. It did a fair job while still failed to capture some important aspects. For example, using MLP sacrifices the interpretability and biological realism to compensate accuracy. On the other hand, the model still sacrifices the accuracy and has a big performance gap when compared to SOTA models (S4 and Mega). \n2. Scalability of the method: as scaling law plays a big role for improving transformer's predictivity, one concern is that transformer might perform better with increasing number of parameters. However, it might not be the same for ELM. As firstly shown in Fig 3, the accuracy quickly saturates with simply increasing $d_m$ and $d_{mlp}$, and not able to get further improved to minimize the performance gap between ELM and Mega.\n3. Only one ML task is evaluated, more evaluations and benchmarks needed to demonstrate contributions in addressing long-range sequence modeling.\n4. As shown in Table S1, large number of hyper-parameters still needed in advance or be tuned based on prior knowledge from neuroscience literatures.\n5. The efficiency side might be over-claimed, as Table 1 shows ELM still requires 100k-200k parameters?\n\nQuestions: 1. What other critical components might be helpful to improve ELM model accuracy?\n2. After applying sparse regularization or quantization to S4 and Mega to match the number of parameters to ELM, how much accuracy drop they will have?\n3. Is the model able to be trained with other biological plausible learning rules instead of BPTT? How they might end up with different parameters?", "labeling_timestamp": "2026-01-11T17:37:01.713060", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly claims the ELM neuron matches the biophysical I/O relationship with under ten thousand (and in places 'a few thousand') trainable parameters, so the reviewer's assertion that Table 1 shows ELM requires ~100k–200k parameters contradicts the paper's stated parameter counts. The provided content does not include a Table 1 that reports 100k–200k parameters.", "evidence": "Abstract: \"our ELM neuron can accurately match the aforementioned input-output relationship with under ten thousand trainable parameters.\" Introduction: \"Our experiments show that a variant of the ELM neuron is expressive enough to accurately match the spike level I/O of a detailed biophysical model ... with a few thousand parameters, in stark contrast to the millions of parameters required by temporal convolutional networks.\"", "section": "Abstract; 1 Introduction"}
{"claim": "It is unclear which additional critical components would improve ELM model accuracy, as the paper does not identify potential architectural additions.", "claim_type": "methodology", "paper_id": "vE1e1mLJ0U", "paper_title": "The Expressive Leaky Memory Neuron: an Efficient and Expressive Phenomenological Neuron Model Can Solve Long-Horizon Tasks.", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "2c423ciTlo", "reviewer": "Reviewer_27bc", "review_text": "Summary: This paper proposes artificial neural network models that incorporates important inductive bias (i.e. leaky memory dynamics, nonlinear synaptic integration) inspired from biological cortical neurons. The model is aimed to achieve two types of goals, one is to match the spike-level dynamics of pyramidal neuron, and the second is evaluated on bio-inspired tasks to evaluate temporal integration. For evaluation, they compare the model with others SOTA baselines (SNN, LSTM, Transformer) are evaluated on multiple biological inspired datasets and long sequence modeling task. It shows the benefits of efficiency in parameterization, and comparable or better performance compared to SOTA models. Meanwhile, the hyper-parameter tuning studies show overlap with previous literatures.\n\nStrengths: 1. The paper is well-motivated, and take the reductionist view to minimize the parameters from a more detailed modeling for cortical neurons, and aim to address the computational efficiency needs of standard models.\n2. Solid evaluations on multiple biological inspired datasets, and compared with multiple SOTA models, and follow by multiple hyperparameter tuning studies. \n3. The biological realism side shows interesting overlaps with previous neuroscience literatures.\n4. The results show the model is capable of achieving better accuracy with efficiency and fewer parameters than traditional deep model LSTM. The finding about simplification does not sacrifice the predictive performance is valuable.\n5. The paper is well-written, and organized in a good structure.\n\nWeaknesses: 1. This paper is aimed to balance the trade-off between fidelity, efficiency and biological realism. It did a fair job while still failed to capture some important aspects. For example, using MLP sacrifices the interpretability and biological realism to compensate accuracy. On the other hand, the model still sacrifices the accuracy and has a big performance gap when compared to SOTA models (S4 and Mega). \n2. Scalability of the method: as scaling law plays a big role for improving transformer's predictivity, one concern is that transformer might perform better with increasing number of parameters. However, it might not be the same for ELM. As firstly shown in Fig 3, the accuracy quickly saturates with simply increasing $d_m$ and $d_{mlp}$, and not able to get further improved to minimize the performance gap between ELM and Mega.\n3. Only one ML task is evaluated, more evaluations and benchmarks needed to demonstrate contributions in addressing long-range sequence modeling.\n4. As shown in Table S1, large number of hyper-parameters still needed in advance or be tuned based on prior knowledge from neuroscience literatures.\n5. The efficiency side might be over-claimed, as Table 1 shows ELM still requires 100k-200k parameters?\n\nQuestions: 1. What other critical components might be helpful to improve ELM model accuracy?\n2. After applying sparse regularization or quantization to S4 and Mega to match the number of parameters to ELM, how much accuracy drop they will have?\n3. Is the model able to be trained with other biological plausible learning rules instead of BPTT? How they might end up with different parameters?", "labeling_timestamp": "2026-01-11T17:37:06.190816", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly identifies architectural components and variants that improve ELM accuracy (e.g., multiple memory units with long timescales, highly nonlinear synaptic integration, and the Branch-ELM variant), and describes using ablations to probe these additions (Abstract, Section 2). Thus the claim that the paper does not identify potential architectural additions is contradicted.", "evidence": "Abstract: \"Conceptually, we find accurate surrogate models to require multiple memory-like hidden states with longer timescales and highly nonlinear synaptic integration.\" Section 2: \"In the ELM neuron, we achieve this by making the number of memory units d_m a hyper-parameter and equipping each of them with a τ_m (always learnable)...\" Section 2 (integration mechanism): \"we choose to parameterize the input integration using a Multilayer Perceptron (MLP) ... which can be used to explore the full range of hypothesized integration complexities, while offering a straightforward way to quantify and ablate the ELM neuron integration complexity. In the Branch-ELM variant ... we extend the integration mechanism dynamics; before the MLP is applied... In this variant the w_s need to be learnable...\"", "section": "Abstract; Section 2 (The Expressive Leaky Memory Neuron)"}
{"claim": "The authors do not evaluate how S4 or Mega perform after sparsification or quantization to match ELM's parameter count.", "claim_type": "baseline", "paper_id": "vE1e1mLJ0U", "paper_title": "The Expressive Leaky Memory Neuron: an Efficient and Expressive Phenomenological Neuron Model Can Solve Long-Horizon Tasks.", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "2c423ciTlo", "reviewer": "Reviewer_27bc", "review_text": "Summary: This paper proposes artificial neural network models that incorporates important inductive bias (i.e. leaky memory dynamics, nonlinear synaptic integration) inspired from biological cortical neurons. The model is aimed to achieve two types of goals, one is to match the spike-level dynamics of pyramidal neuron, and the second is evaluated on bio-inspired tasks to evaluate temporal integration. For evaluation, they compare the model with others SOTA baselines (SNN, LSTM, Transformer) are evaluated on multiple biological inspired datasets and long sequence modeling task. It shows the benefits of efficiency in parameterization, and comparable or better performance compared to SOTA models. Meanwhile, the hyper-parameter tuning studies show overlap with previous literatures.\n\nStrengths: 1. The paper is well-motivated, and take the reductionist view to minimize the parameters from a more detailed modeling for cortical neurons, and aim to address the computational efficiency needs of standard models.\n2. Solid evaluations on multiple biological inspired datasets, and compared with multiple SOTA models, and follow by multiple hyperparameter tuning studies. \n3. The biological realism side shows interesting overlaps with previous neuroscience literatures.\n4. The results show the model is capable of achieving better accuracy with efficiency and fewer parameters than traditional deep model LSTM. The finding about simplification does not sacrifice the predictive performance is valuable.\n5. The paper is well-written, and organized in a good structure.\n\nWeaknesses: 1. This paper is aimed to balance the trade-off between fidelity, efficiency and biological realism. It did a fair job while still failed to capture some important aspects. For example, using MLP sacrifices the interpretability and biological realism to compensate accuracy. On the other hand, the model still sacrifices the accuracy and has a big performance gap when compared to SOTA models (S4 and Mega). \n2. Scalability of the method: as scaling law plays a big role for improving transformer's predictivity, one concern is that transformer might perform better with increasing number of parameters. However, it might not be the same for ELM. As firstly shown in Fig 3, the accuracy quickly saturates with simply increasing $d_m$ and $d_{mlp}$, and not able to get further improved to minimize the performance gap between ELM and Mega.\n3. Only one ML task is evaluated, more evaluations and benchmarks needed to demonstrate contributions in addressing long-range sequence modeling.\n4. As shown in Table S1, large number of hyper-parameters still needed in advance or be tuned based on prior knowledge from neuroscience literatures.\n5. The efficiency side might be over-claimed, as Table 1 shows ELM still requires 100k-200k parameters?\n\nQuestions: 1. What other critical components might be helpful to improve ELM model accuracy?\n2. After applying sparse regularization or quantization to S4 and Mega to match the number of parameters to ELM, how much accuracy drop they will have?\n3. Is the model able to be trained with other biological plausible learning rules instead of BPTT? How they might end up with different parameters?", "labeling_timestamp": "2026-01-11T17:37:08.190961", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper compares ELM to Transformers and Chrono-LSTM and contrasts its parameter count with TCNs, but contains no evaluation or discussion of S4 or Mega, nor any mention of sparsification or quantization experiments to match ELM's parameter count.", "evidence": "Abstract: \"reliably outperforming the classic Transformer or Chrono-LSTM architectures on LRA\"; Introduction: \"It was found that a highly expressive temporal convolutional network with millions of parameters was essential to reproduce the aforementioned I/O relationship.\"", "section": "Abstract; 1 INTRODUCTION"}
{"claim": "The paper does not evaluate training ELM with biologically plausible learning rules instead of BPTT, leaving biological learning compatibility untested.", "claim_type": "experimental", "paper_id": "vE1e1mLJ0U", "paper_title": "The Expressive Leaky Memory Neuron: an Efficient and Expressive Phenomenological Neuron Model Can Solve Long-Horizon Tasks.", "paper_venue": "iclr2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "2c423ciTlo", "reviewer": "Reviewer_27bc", "review_text": "Summary: This paper proposes artificial neural network models that incorporates important inductive bias (i.e. leaky memory dynamics, nonlinear synaptic integration) inspired from biological cortical neurons. The model is aimed to achieve two types of goals, one is to match the spike-level dynamics of pyramidal neuron, and the second is evaluated on bio-inspired tasks to evaluate temporal integration. For evaluation, they compare the model with others SOTA baselines (SNN, LSTM, Transformer) are evaluated on multiple biological inspired datasets and long sequence modeling task. It shows the benefits of efficiency in parameterization, and comparable or better performance compared to SOTA models. Meanwhile, the hyper-parameter tuning studies show overlap with previous literatures.\n\nStrengths: 1. The paper is well-motivated, and take the reductionist view to minimize the parameters from a more detailed modeling for cortical neurons, and aim to address the computational efficiency needs of standard models.\n2. Solid evaluations on multiple biological inspired datasets, and compared with multiple SOTA models, and follow by multiple hyperparameter tuning studies. \n3. The biological realism side shows interesting overlaps with previous neuroscience literatures.\n4. The results show the model is capable of achieving better accuracy with efficiency and fewer parameters than traditional deep model LSTM. The finding about simplification does not sacrifice the predictive performance is valuable.\n5. The paper is well-written, and organized in a good structure.\n\nWeaknesses: 1. This paper is aimed to balance the trade-off between fidelity, efficiency and biological realism. It did a fair job while still failed to capture some important aspects. For example, using MLP sacrifices the interpretability and biological realism to compensate accuracy. On the other hand, the model still sacrifices the accuracy and has a big performance gap when compared to SOTA models (S4 and Mega). \n2. Scalability of the method: as scaling law plays a big role for improving transformer's predictivity, one concern is that transformer might perform better with increasing number of parameters. However, it might not be the same for ELM. As firstly shown in Fig 3, the accuracy quickly saturates with simply increasing $d_m$ and $d_{mlp}$, and not able to get further improved to minimize the performance gap between ELM and Mega.\n3. Only one ML task is evaluated, more evaluations and benchmarks needed to demonstrate contributions in addressing long-range sequence modeling.\n4. As shown in Table S1, large number of hyper-parameters still needed in advance or be tuned based on prior knowledge from neuroscience literatures.\n5. The efficiency side might be over-claimed, as Table 1 shows ELM still requires 100k-200k parameters?\n\nQuestions: 1. What other critical components might be helpful to improve ELM model accuracy?\n2. After applying sparse regularization or quantization to S4 and Mega to match the number of parameters to ELM, how much accuracy drop they will have?\n3. Is the model able to be trained with other biological plausible learning rules instead of BPTT? How they might end up with different parameters?", "labeling_timestamp": "2026-01-11T17:37:18.636226", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper does not describe or evaluate training the ELM with biologically plausible learning rules nor mention alternatives to standard gradient-based training (e.g., BPTT). It repeatedly states that parameters are ‘learnable’ and that low-level biological processes are abstracted away, implying no exploration of biologically plausible learning methods.", "evidence": "\"While biologically inspired, low-level biological processes are abstracted away for computational efficiency, and consequently, individual parameters of the ELM neuron are not designed for direct biophysical interpretability.\" \n\n\"w_p always learnable\" (describing the MLP integration parameters) and other parameters are described as learnable.", "section": "Section 2 (and Introduction)"}
{"claim": "The paper does not compare with node-degree preserving hypergraph projection methods such as [1].", "claim_type": "baseline", "paper_id": "5M2MjyNR2w", "paper_title": "Adaptive Expansion for Hypergraph Learning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "2eO0wOQPFu", "reviewer": "Reviewer_1ZMR", "review_text": "Summary: The authors propose a method to project a hypergraph to a corresponding graph where the edge weights are not straightforwardly copied from the hyperedges, but an adaptive approach calculates them. The proposed approach is comprised of two steps - using GS-Net to identify representative nodes, followed by a distance-aware kernel to compute the edge weights.\n\nStrengths: 1. The problem space is very interesting. There is very little attention paid to finding innovative ways to convert hypergraphs to graphs by preserving the desired properties. \n2. The proposed method outperforms the existing baselines. The ablation study provides an understanding of the contribution of different components.\n\nWeaknesses: 1. Existing works like node-degree preserving hypergraph projection [1] are not considered. Clique/star-based expansions are not the right representative of SOTA methods. \n2. The idea of using node features to compute edge weights assumes that there is an underlying homophily, which was not well captured by hyperedges (hence, the existence of a hyperedge does not mean the constituent nodes share the same strengthened bond), but the projected graph will capture it better. It requires more justification. (please provide additional justification if I got the idea wrong).\n3. The current write-up lacks motivation behind the proposed steps in the method. A short text explaining the intuition behind the reason for selecting representative nodes and follow-up steps would be helpful.\n\n[1] \"Hypergraph clustering by iteratively reweighted modularity maximization.\" Applied Network Science 5.1 (2020): 1-22.\n\nQuestions: 1. What is the rationale behind having a representative pair of nodes for each hyperedge? Also, provide an explanation on how this approach will cater to hyperedges of varying sizes (it can be 1 to n).\n2. How does this approach work on a hypergraph where nodes do not have any attributes? Several domains, such as computational biology, where hypergraphs have extensive applications, but getting node-level features is challenging. \n3. Provide more details on the hypergraph datasets, such as their size, average degree distribution, average hyperedge size, etc. This will help explain the result trends. \n4. Does the proposed method apply to tasks other than node classification? (such as clustering, hyperedge prediction, etc.)", "labeling_timestamp": "2026-01-11T17:37:30.647383", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper states it compares AdE to classic expansion models (CE, SE, LE and variants) but does not mention or cite node-degree preserving hypergraph projection methods; there is no reference to degree-preserving projection methods in the provided content or experimental comparisons.", "evidence": "1) \"Extensive theoretical justifications and empirical experiments over five benchmark hypergraph datasets demonstrate that AdE has excellent rationality, generalization, and effectiveness compared to classic expansion models.\" 2) \"Most hypergraph neural networks (HyGNNs) first convert hypergraphs to (weighted) graphs via classic expansion methods, i.e., clique expansion (CE) (Sun et al., 2008), star expansion (SE) (Agarwal et al., 2006), line expansion (LE) (Yang et al., 2022) and its variants (Yadati et al., 2019; Feng et al., 2019; Zhou et al., 2006) and further feed the converted graph to neural networks for representation learning.\"", "section": "ABSTRACT; RELATED WORKS"}
{"claim": "Using clique- or star-based expansions as baselines is inappropriate because they are not representative of state-of-the-art hypergraph projection methods.", "claim_type": "baseline", "paper_id": "5M2MjyNR2w", "paper_title": "Adaptive Expansion for Hypergraph Learning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "2eO0wOQPFu", "reviewer": "Reviewer_1ZMR", "review_text": "Summary: The authors propose a method to project a hypergraph to a corresponding graph where the edge weights are not straightforwardly copied from the hyperedges, but an adaptive approach calculates them. The proposed approach is comprised of two steps - using GS-Net to identify representative nodes, followed by a distance-aware kernel to compute the edge weights.\n\nStrengths: 1. The problem space is very interesting. There is very little attention paid to finding innovative ways to convert hypergraphs to graphs by preserving the desired properties. \n2. The proposed method outperforms the existing baselines. The ablation study provides an understanding of the contribution of different components.\n\nWeaknesses: 1. Existing works like node-degree preserving hypergraph projection [1] are not considered. Clique/star-based expansions are not the right representative of SOTA methods. \n2. The idea of using node features to compute edge weights assumes that there is an underlying homophily, which was not well captured by hyperedges (hence, the existence of a hyperedge does not mean the constituent nodes share the same strengthened bond), but the projected graph will capture it better. It requires more justification. (please provide additional justification if I got the idea wrong).\n3. The current write-up lacks motivation behind the proposed steps in the method. A short text explaining the intuition behind the reason for selecting representative nodes and follow-up steps would be helpful.\n\n[1] \"Hypergraph clustering by iteratively reweighted modularity maximization.\" Applied Network Science 5.1 (2020): 1-22.\n\nQuestions: 1. What is the rationale behind having a representative pair of nodes for each hyperedge? Also, provide an explanation on how this approach will cater to hyperedges of varying sizes (it can be 1 to n).\n2. How does this approach work on a hypergraph where nodes do not have any attributes? Several domains, such as computational biology, where hypergraphs have extensive applications, but getting node-level features is challenging. \n3. Provide more details on the hypergraph datasets, such as their size, average degree distribution, average hyperedge size, etc. This will help explain the result trends. \n4. Does the proposed method apply to tasks other than node classification? (such as clustering, hyperedge prediction, etc.)", "labeling_timestamp": "2026-01-11T17:37:33.057751", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper treats clique and star expansions as standard, widely-used methods in recent HyGNN work and cites state-of-the-art models that use them; it criticizes certain limitations but does not claim they are inappropriate or unrepresentative of SOTA projection methods.", "evidence": "1) \"most HyGNNs methods primarily leverage typical expansion methods, e.g., clique expansion (CE) (Sun et al., 2008), star expansion (SE) (Agarwal et al., 2006), and line expansion (LE) (Yang et al., 2022), to convert hypergraphs into graphs\". 2) \"For instance, HyperGCN (Yadati et al., 2019) utilizes a CE-based method to transfer the hypergraph structure into a weighted graph structure.\" 3) \"Most hypergraph neural networks (HyGNNs) first convert hypergraphs to (weighted) graphs via classic expansion methods, i.e., clique expansion (CE) (Sun et al., 2008), star expansion (SE) (Agarwal et al., 2006), line expansion (LE) (Yang et al., 2022) and its variants (Yadati et al., 2019; Feng et al., 2019; Zhou et al., 2006)\".", "section": "Introduction / Related Works"}
{"claim": "The method assumes node features reflect homophily, but this assumption is not justified in the paper.", "claim_type": "methodology", "paper_id": "5M2MjyNR2w", "paper_title": "Adaptive Expansion for Hypergraph Learning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "2eO0wOQPFu", "reviewer": "Reviewer_1ZMR", "review_text": "Summary: The authors propose a method to project a hypergraph to a corresponding graph where the edge weights are not straightforwardly copied from the hyperedges, but an adaptive approach calculates them. The proposed approach is comprised of two steps - using GS-Net to identify representative nodes, followed by a distance-aware kernel to compute the edge weights.\n\nStrengths: 1. The problem space is very interesting. There is very little attention paid to finding innovative ways to convert hypergraphs to graphs by preserving the desired properties. \n2. The proposed method outperforms the existing baselines. The ablation study provides an understanding of the contribution of different components.\n\nWeaknesses: 1. Existing works like node-degree preserving hypergraph projection [1] are not considered. Clique/star-based expansions are not the right representative of SOTA methods. \n2. The idea of using node features to compute edge weights assumes that there is an underlying homophily, which was not well captured by hyperedges (hence, the existence of a hyperedge does not mean the constituent nodes share the same strengthened bond), but the projected graph will capture it better. It requires more justification. (please provide additional justification if I got the idea wrong).\n3. The current write-up lacks motivation behind the proposed steps in the method. A short text explaining the intuition behind the reason for selecting representative nodes and follow-up steps would be helpful.\n\n[1] \"Hypergraph clustering by iteratively reweighted modularity maximization.\" Applied Network Science 5.1 (2020): 1-22.\n\nQuestions: 1. What is the rationale behind having a representative pair of nodes for each hyperedge? Also, provide an explanation on how this approach will cater to hyperedges of varying sizes (it can be 1 to n).\n2. How does this approach work on a hypergraph where nodes do not have any attributes? Several domains, such as computational biology, where hypergraphs have extensive applications, but getting node-level features is challenging. \n3. Provide more details on the hypergraph datasets, such as their size, average degree distribution, average hyperedge size, etc. This will help explain the result trends. \n4. Does the proposed method apply to tasks other than node classification? (such as clustering, hyperedge prediction, etc.)", "labeling_timestamp": "2026-01-11T17:37:31.357108", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly assumes that nodes with similar attribute features within the same hyperedge are more likely to be strongly connected and designs a distance-aware kernel to exploit that (Abstract, Introduction). However, in the provided content the assumption is asserted as motivation and used to design the method but no explicit empirical or theoretical justification for the homophily assumption itself is given (the text offers method-level justification and claims 'theoretic justification' for AdE in general, but not a direct justification of the feature-homophily assumption).", "evidence": "“Most expansion methods often employ fixed edge weights while ignoring the fact that nodes having similar attribute features within the same hyperedge are more likely to be connected compared with nodes with dissimilar features.”", "section": "Abstract; 1 INTRODUCTION"}
{"claim": "A hyperedge's existence does not necessarily imply that its constituent nodes share a strengthened bond.", "claim_type": "subjective", "paper_id": "5M2MjyNR2w", "paper_title": "Adaptive Expansion for Hypergraph Learning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "2eO0wOQPFu", "reviewer": "Reviewer_1ZMR", "review_text": "Summary: The authors propose a method to project a hypergraph to a corresponding graph where the edge weights are not straightforwardly copied from the hyperedges, but an adaptive approach calculates them. The proposed approach is comprised of two steps - using GS-Net to identify representative nodes, followed by a distance-aware kernel to compute the edge weights.\n\nStrengths: 1. The problem space is very interesting. There is very little attention paid to finding innovative ways to convert hypergraphs to graphs by preserving the desired properties. \n2. The proposed method outperforms the existing baselines. The ablation study provides an understanding of the contribution of different components.\n\nWeaknesses: 1. Existing works like node-degree preserving hypergraph projection [1] are not considered. Clique/star-based expansions are not the right representative of SOTA methods. \n2. The idea of using node features to compute edge weights assumes that there is an underlying homophily, which was not well captured by hyperedges (hence, the existence of a hyperedge does not mean the constituent nodes share the same strengthened bond), but the projected graph will capture it better. It requires more justification. (please provide additional justification if I got the idea wrong).\n3. The current write-up lacks motivation behind the proposed steps in the method. A short text explaining the intuition behind the reason for selecting representative nodes and follow-up steps would be helpful.\n\n[1] \"Hypergraph clustering by iteratively reweighted modularity maximization.\" Applied Network Science 5.1 (2020): 1-22.\n\nQuestions: 1. What is the rationale behind having a representative pair of nodes for each hyperedge? Also, provide an explanation on how this approach will cater to hyperedges of varying sizes (it can be 1 to n).\n2. How does this approach work on a hypergraph where nodes do not have any attributes? Several domains, such as computational biology, where hypergraphs have extensive applications, but getting node-level features is challenging. \n3. Provide more details on the hypergraph datasets, such as their size, average degree distribution, average hyperedge size, etc. This will help explain the result trends. \n4. Does the proposed method apply to tasks other than node classification? (such as clustering, hyperedge prediction, etc.)", "labeling_timestamp": "2026-01-11T17:38:02.473317", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper explicitly states that nodes within the same hyperedge need not all have equally strong connections and criticizes clique expansion for uniformly connecting all node pairs; it also argues that nodes with similar attributes within a hyperedge are more likely to be strongly connected, implying hyperedge existence alone does not guarantee a strengthened bond among all constituents.", "evidence": "Abstract: \"Most expansion methods often employ fixed edge weights while ignoring the fact that nodes having similar attribute features within the same hyperedge are more likely to be connected compared with nodes with dissimilar features.\" Introduction: \"For instance, the classic CE method connects all node pairs within the same hyperedge and makes it a fully connected subgraph, which will bring the redundant information in the converted graph (Sun et al., 2008).\"", "section": "Abstract / 1 INTRODUCTION"}
{"claim": "The authors do not justify why the projected graph will better capture homophily than the original hypergraph.", "claim_type": "methodology", "paper_id": "5M2MjyNR2w", "paper_title": "Adaptive Expansion for Hypergraph Learning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "2eO0wOQPFu", "reviewer": "Reviewer_1ZMR", "review_text": "Summary: The authors propose a method to project a hypergraph to a corresponding graph where the edge weights are not straightforwardly copied from the hyperedges, but an adaptive approach calculates them. The proposed approach is comprised of two steps - using GS-Net to identify representative nodes, followed by a distance-aware kernel to compute the edge weights.\n\nStrengths: 1. The problem space is very interesting. There is very little attention paid to finding innovative ways to convert hypergraphs to graphs by preserving the desired properties. \n2. The proposed method outperforms the existing baselines. The ablation study provides an understanding of the contribution of different components.\n\nWeaknesses: 1. Existing works like node-degree preserving hypergraph projection [1] are not considered. Clique/star-based expansions are not the right representative of SOTA methods. \n2. The idea of using node features to compute edge weights assumes that there is an underlying homophily, which was not well captured by hyperedges (hence, the existence of a hyperedge does not mean the constituent nodes share the same strengthened bond), but the projected graph will capture it better. It requires more justification. (please provide additional justification if I got the idea wrong).\n3. The current write-up lacks motivation behind the proposed steps in the method. A short text explaining the intuition behind the reason for selecting representative nodes and follow-up steps would be helpful.\n\n[1] \"Hypergraph clustering by iteratively reweighted modularity maximization.\" Applied Network Science 5.1 (2020): 1-22.\n\nQuestions: 1. What is the rationale behind having a representative pair of nodes for each hyperedge? Also, provide an explanation on how this approach will cater to hyperedges of varying sizes (it can be 1 to n).\n2. How does this approach work on a hypergraph where nodes do not have any attributes? Several domains, such as computational biology, where hypergraphs have extensive applications, but getting node-level features is challenging. \n3. Provide more details on the hypergraph datasets, such as their size, average degree distribution, average hyperedge size, etc. This will help explain the result trends. \n4. Does the proposed method apply to tasks other than node classification? (such as clustering, hyperedge prediction, etc.)", "labeling_timestamp": "2026-01-11T17:38:04.007069", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly justifies that their projected (expanded) graph will better capture homophily by (1) noting that prior expansions use fixed/uniform weights and thus ignore attribute similarity, and (2) proposing a distance-aware kernel that increases edge weights between nodes with similar attributes—i.e., an explicit rationale for why the projected graph better reflects homophily.", "evidence": "\"we further design a distance-aware kernel function to dynamically adjust the edge weights to make sure that node pairs having similar attribute features within the corresponding hyperedge are more likely to be connected with large weights.\"", "section": "Abstract / 1 INTRODUCTION"}
{"claim": "The write-up lacks motivation and intuition for selecting representative nodes and the subsequent method steps.", "claim_type": "presentation", "paper_id": "5M2MjyNR2w", "paper_title": "Adaptive Expansion for Hypergraph Learning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "2eO0wOQPFu", "reviewer": "Reviewer_1ZMR", "review_text": "Summary: The authors propose a method to project a hypergraph to a corresponding graph where the edge weights are not straightforwardly copied from the hyperedges, but an adaptive approach calculates them. The proposed approach is comprised of two steps - using GS-Net to identify representative nodes, followed by a distance-aware kernel to compute the edge weights.\n\nStrengths: 1. The problem space is very interesting. There is very little attention paid to finding innovative ways to convert hypergraphs to graphs by preserving the desired properties. \n2. The proposed method outperforms the existing baselines. The ablation study provides an understanding of the contribution of different components.\n\nWeaknesses: 1. Existing works like node-degree preserving hypergraph projection [1] are not considered. Clique/star-based expansions are not the right representative of SOTA methods. \n2. The idea of using node features to compute edge weights assumes that there is an underlying homophily, which was not well captured by hyperedges (hence, the existence of a hyperedge does not mean the constituent nodes share the same strengthened bond), but the projected graph will capture it better. It requires more justification. (please provide additional justification if I got the idea wrong).\n3. The current write-up lacks motivation behind the proposed steps in the method. A short text explaining the intuition behind the reason for selecting representative nodes and follow-up steps would be helpful.\n\n[1] \"Hypergraph clustering by iteratively reweighted modularity maximization.\" Applied Network Science 5.1 (2020): 1-22.\n\nQuestions: 1. What is the rationale behind having a representative pair of nodes for each hyperedge? Also, provide an explanation on how this approach will cater to hyperedges of varying sizes (it can be 1 to n).\n2. How does this approach work on a hypergraph where nodes do not have any attributes? Several domains, such as computational biology, where hypergraphs have extensive applications, but getting node-level features is challenging. \n3. Provide more details on the hypergraph datasets, such as their size, average degree distribution, average hyperedge size, etc. This will help explain the result trends. \n4. Does the proposed method apply to tasks other than node classification? (such as clustering, hyperedge prediction, etc.)", "labeling_timestamp": "2026-01-11T17:38:04.058782", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly motivates the need to select representative nodes (citing limitations of clique expansion and HyperGCN) and describes intuition and concrete steps for adaptive representative-node selection via the Global Simulation Network and pooling/simulation-weighting mechanism.", "evidence": "1) \"However, the selected nodes are not representative enough of the corresponding hyperedge, which causes certain information loss.\" 2) \"To handle the first challenge, instead of connecting all nodes within the same hyperedge indiscriminately, we design a novel Global Simulation Network (GSi-Net) to select two nodes for symbolizing each hyperedge adaptively. In particular, we first employ a pooling layer to obtain the global representations of the attribute features, followed by a simulation network to learn the importance of each feature dimension, and further obtain the adaptive weight matrix for the attribute features. After obtaining the scaled attribute feature with the adaptive weight matrix, we select two representative nodes for each hyperedge dynamically.\"", "section": "Introduction / Section 4.1 (GSi-Net for adaptive representative node selection)"}
{"claim": "The paper does not justify using a representative pair of nodes per hyperedge; the rationale is missing.", "claim_type": "methodology", "paper_id": "5M2MjyNR2w", "paper_title": "Adaptive Expansion for Hypergraph Learning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "2eO0wOQPFu", "reviewer": "Reviewer_1ZMR", "review_text": "Summary: The authors propose a method to project a hypergraph to a corresponding graph where the edge weights are not straightforwardly copied from the hyperedges, but an adaptive approach calculates them. The proposed approach is comprised of two steps - using GS-Net to identify representative nodes, followed by a distance-aware kernel to compute the edge weights.\n\nStrengths: 1. The problem space is very interesting. There is very little attention paid to finding innovative ways to convert hypergraphs to graphs by preserving the desired properties. \n2. The proposed method outperforms the existing baselines. The ablation study provides an understanding of the contribution of different components.\n\nWeaknesses: 1. Existing works like node-degree preserving hypergraph projection [1] are not considered. Clique/star-based expansions are not the right representative of SOTA methods. \n2. The idea of using node features to compute edge weights assumes that there is an underlying homophily, which was not well captured by hyperedges (hence, the existence of a hyperedge does not mean the constituent nodes share the same strengthened bond), but the projected graph will capture it better. It requires more justification. (please provide additional justification if I got the idea wrong).\n3. The current write-up lacks motivation behind the proposed steps in the method. A short text explaining the intuition behind the reason for selecting representative nodes and follow-up steps would be helpful.\n\n[1] \"Hypergraph clustering by iteratively reweighted modularity maximization.\" Applied Network Science 5.1 (2020): 1-22.\n\nQuestions: 1. What is the rationale behind having a representative pair of nodes for each hyperedge? Also, provide an explanation on how this approach will cater to hyperedges of varying sizes (it can be 1 to n).\n2. How does this approach work on a hypergraph where nodes do not have any attributes? Several domains, such as computational biology, where hypergraphs have extensive applications, but getting node-level features is challenging. \n3. Provide more details on the hypergraph datasets, such as their size, average degree distribution, average hyperedge size, etc. This will help explain the result trends. \n4. Does the proposed method apply to tasks other than node classification? (such as clustering, hyperedge prediction, etc.)", "labeling_timestamp": "2026-01-11T17:38:15.314717", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly motivates selecting a representative node pair per hyperedge as a way to avoid the redundancy of clique expansion and to improve over prior work (HyperGCN) whose chosen pair is claimed to be insufficiently representative. It then proposes the Global Simulation Network (GSi-Net) to adaptively select two representative nodes per hyperedge.", "evidence": "1) \"the classic CE method connects all node pairs within the same hyperedge and makes it a fully connected subgraph, which will bring the redundant information in the converted graph (Sun et al., 2008). HyperGCN, another classic CE-based method, proposes to pick two nodes to symbolize the corresponding hyperedge and further connect the rest of the nodes with the selected nodes (Yadati et al., 2019). However, the selected nodes are not representative enough of the corresponding hyperedge, which causes certain information loss.\" 2) \"To handle the first challenge, instead of connecting all nodes within the same hyperedge indiscriminately, we design a novel Global Simulation Network (GSi-Net) to select two nodes for symbolizing each hyperedge adaptively.\" 3) \"We first introduce a Global Simulation Network to pick two representative nodes for symbolizing each hyperedge in an adaptive manner.\"", "section": "Introduction; Abstract; 4.1 GSI-NET FOR ADAPTIVE REPRESENTATIVE NODE SELECTION"}
{"claim": "It is unclear how the approach handles hyperedges of varying sizes, including singleton or very large hyperedges.", "claim_type": "methodology", "paper_id": "5M2MjyNR2w", "paper_title": "Adaptive Expansion for Hypergraph Learning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "2eO0wOQPFu", "reviewer": "Reviewer_1ZMR", "review_text": "Summary: The authors propose a method to project a hypergraph to a corresponding graph where the edge weights are not straightforwardly copied from the hyperedges, but an adaptive approach calculates them. The proposed approach is comprised of two steps - using GS-Net to identify representative nodes, followed by a distance-aware kernel to compute the edge weights.\n\nStrengths: 1. The problem space is very interesting. There is very little attention paid to finding innovative ways to convert hypergraphs to graphs by preserving the desired properties. \n2. The proposed method outperforms the existing baselines. The ablation study provides an understanding of the contribution of different components.\n\nWeaknesses: 1. Existing works like node-degree preserving hypergraph projection [1] are not considered. Clique/star-based expansions are not the right representative of SOTA methods. \n2. The idea of using node features to compute edge weights assumes that there is an underlying homophily, which was not well captured by hyperedges (hence, the existence of a hyperedge does not mean the constituent nodes share the same strengthened bond), but the projected graph will capture it better. It requires more justification. (please provide additional justification if I got the idea wrong).\n3. The current write-up lacks motivation behind the proposed steps in the method. A short text explaining the intuition behind the reason for selecting representative nodes and follow-up steps would be helpful.\n\n[1] \"Hypergraph clustering by iteratively reweighted modularity maximization.\" Applied Network Science 5.1 (2020): 1-22.\n\nQuestions: 1. What is the rationale behind having a representative pair of nodes for each hyperedge? Also, provide an explanation on how this approach will cater to hyperedges of varying sizes (it can be 1 to n).\n2. How does this approach work on a hypergraph where nodes do not have any attributes? Several domains, such as computational biology, where hypergraphs have extensive applications, but getting node-level features is challenging. \n3. Provide more details on the hypergraph datasets, such as their size, average degree distribution, average hyperedge size, etc. This will help explain the result trends. \n4. Does the proposed method apply to tasks other than node classification? (such as clustering, hyperedge prediction, etc.)", "labeling_timestamp": "2026-01-11T17:38:02.135768", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper repeatedly states the method selects two representative nodes per hyperedge (e.g., Abstract, Section 4.1, Figure 2 description) but does not describe special handling for hyperedges of size one or discuss behavior/scaling for very large hyperedges. Thus it is unclear how varying hyperedge sizes, including singletons or extremely large hyperedges, are handled.", "evidence": "Abstract: \"we first introduce a Global Simulation Network to pick two representative nodes for symbolizing each hyperedge in an adaptive manner.\"; Figure 2 caption: \"AdE scales the attribute feature X a with the learnable weight matrix W g and identifies a representative node pair ( v e -, v e + ) for each hyperedge e .\"; Section 4.1: \"we design a novel Global Simulation Network (GSi-Net) to select two nodes for symbolizing each hyperedge adaptively.\"", "section": "Abstract; Figure 2 description; Section 4.1 (GSi-Net)"}
{"claim": "The method's applicability when nodes lack attributes is not addressed, leaving performance on attribute-less hypergraphs unclear.", "claim_type": "methodology", "paper_id": "5M2MjyNR2w", "paper_title": "Adaptive Expansion for Hypergraph Learning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "2eO0wOQPFu", "reviewer": "Reviewer_1ZMR", "review_text": "Summary: The authors propose a method to project a hypergraph to a corresponding graph where the edge weights are not straightforwardly copied from the hyperedges, but an adaptive approach calculates them. The proposed approach is comprised of two steps - using GS-Net to identify representative nodes, followed by a distance-aware kernel to compute the edge weights.\n\nStrengths: 1. The problem space is very interesting. There is very little attention paid to finding innovative ways to convert hypergraphs to graphs by preserving the desired properties. \n2. The proposed method outperforms the existing baselines. The ablation study provides an understanding of the contribution of different components.\n\nWeaknesses: 1. Existing works like node-degree preserving hypergraph projection [1] are not considered. Clique/star-based expansions are not the right representative of SOTA methods. \n2. The idea of using node features to compute edge weights assumes that there is an underlying homophily, which was not well captured by hyperedges (hence, the existence of a hyperedge does not mean the constituent nodes share the same strengthened bond), but the projected graph will capture it better. It requires more justification. (please provide additional justification if I got the idea wrong).\n3. The current write-up lacks motivation behind the proposed steps in the method. A short text explaining the intuition behind the reason for selecting representative nodes and follow-up steps would be helpful.\n\n[1] \"Hypergraph clustering by iteratively reweighted modularity maximization.\" Applied Network Science 5.1 (2020): 1-22.\n\nQuestions: 1. What is the rationale behind having a representative pair of nodes for each hyperedge? Also, provide an explanation on how this approach will cater to hyperedges of varying sizes (it can be 1 to n).\n2. How does this approach work on a hypergraph where nodes do not have any attributes? Several domains, such as computational biology, where hypergraphs have extensive applications, but getting node-level features is challenging. \n3. Provide more details on the hypergraph datasets, such as their size, average degree distribution, average hyperedge size, etc. This will help explain the result trends. \n4. Does the proposed method apply to tasks other than node classification? (such as clustering, hyperedge prediction, etc.)", "labeling_timestamp": "2026-01-11T17:38:32.295921", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper consistently assumes node attribute features X and describes all AdE components (GSi-Net, distance-aware kernel) as operating on those attributes, but it does not discuss or present mechanisms or experiments for the case where nodes lack attributes. Thus the claim that applicability on attribute-less hypergraphs is not addressed is accurate.", "evidence": "1) \"we design a distance-aware kernel function to dynamically adjust the edge weights to make sure that node pairs having similar attribute features within the corresponding hyperedge are more likely to be connected with large weights.\" 2) \"given a hypergraph H = ( V , E , X ) with attribute feature X , (i) AdE first feeds the attribute feature X into a global pooling layer, and leverages simulation network (Si-Net) to learn the importance of each feature dimension... Afterward, AdE scales the attribute feature X_a with the learnable weight matrix W_g and identifies a representative node pair (v_e-, v_e+) for each hyperedge e.\" 3) \"Definition 3.1. Hypergraph. ... and X is the attribute feature set.\"", "section": "Abstract; Figure 2 caption / Methodology overview; Section 3 (Preliminary)"}
{"claim": "The paper omits dataset details such as size, average degree distribution, and average hyperedge size.", "claim_type": "experimental", "paper_id": "5M2MjyNR2w", "paper_title": "Adaptive Expansion for Hypergraph Learning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "2eO0wOQPFu", "reviewer": "Reviewer_1ZMR", "review_text": "Summary: The authors propose a method to project a hypergraph to a corresponding graph where the edge weights are not straightforwardly copied from the hyperedges, but an adaptive approach calculates them. The proposed approach is comprised of two steps - using GS-Net to identify representative nodes, followed by a distance-aware kernel to compute the edge weights.\n\nStrengths: 1. The problem space is very interesting. There is very little attention paid to finding innovative ways to convert hypergraphs to graphs by preserving the desired properties. \n2. The proposed method outperforms the existing baselines. The ablation study provides an understanding of the contribution of different components.\n\nWeaknesses: 1. Existing works like node-degree preserving hypergraph projection [1] are not considered. Clique/star-based expansions are not the right representative of SOTA methods. \n2. The idea of using node features to compute edge weights assumes that there is an underlying homophily, which was not well captured by hyperedges (hence, the existence of a hyperedge does not mean the constituent nodes share the same strengthened bond), but the projected graph will capture it better. It requires more justification. (please provide additional justification if I got the idea wrong).\n3. The current write-up lacks motivation behind the proposed steps in the method. A short text explaining the intuition behind the reason for selecting representative nodes and follow-up steps would be helpful.\n\n[1] \"Hypergraph clustering by iteratively reweighted modularity maximization.\" Applied Network Science 5.1 (2020): 1-22.\n\nQuestions: 1. What is the rationale behind having a representative pair of nodes for each hyperedge? Also, provide an explanation on how this approach will cater to hyperedges of varying sizes (it can be 1 to n).\n2. How does this approach work on a hypergraph where nodes do not have any attributes? Several domains, such as computational biology, where hypergraphs have extensive applications, but getting node-level features is challenging. \n3. Provide more details on the hypergraph datasets, such as their size, average degree distribution, average hyperedge size, etc. This will help explain the result trends. \n4. Does the proposed method apply to tasks other than node classification? (such as clustering, hyperedge prediction, etc.)", "labeling_timestamp": "2026-01-11T17:38:23.091691", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper mentions experiments on \"five benchmark hypergraph datasets\" but does not provide any dataset statistics or descriptions (e.g., dataset sizes, degree distributions, or average hyperedge sizes) in the provided content (Abstract, Introduction, Preliminaries, Methodology, Related Works).", "evidence": "“Extensive theoretical justifications and empirical experiments over five benchmark hypergraph datasets demonstrate that AdE has excellent rationality, generalization, and effectiveness compared to classic expansion models.”", "section": "Abstract"}
{"claim": "The evaluation does not demonstrate whether the proposed method generalizes to other tasks like clustering or hyperedge prediction.", "claim_type": "experimental", "paper_id": "5M2MjyNR2w", "paper_title": "Adaptive Expansion for Hypergraph Learning", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "2eO0wOQPFu", "reviewer": "Reviewer_1ZMR", "review_text": "Summary: The authors propose a method to project a hypergraph to a corresponding graph where the edge weights are not straightforwardly copied from the hyperedges, but an adaptive approach calculates them. The proposed approach is comprised of two steps - using GS-Net to identify representative nodes, followed by a distance-aware kernel to compute the edge weights.\n\nStrengths: 1. The problem space is very interesting. There is very little attention paid to finding innovative ways to convert hypergraphs to graphs by preserving the desired properties. \n2. The proposed method outperforms the existing baselines. The ablation study provides an understanding of the contribution of different components.\n\nWeaknesses: 1. Existing works like node-degree preserving hypergraph projection [1] are not considered. Clique/star-based expansions are not the right representative of SOTA methods. \n2. The idea of using node features to compute edge weights assumes that there is an underlying homophily, which was not well captured by hyperedges (hence, the existence of a hyperedge does not mean the constituent nodes share the same strengthened bond), but the projected graph will capture it better. It requires more justification. (please provide additional justification if I got the idea wrong).\n3. The current write-up lacks motivation behind the proposed steps in the method. A short text explaining the intuition behind the reason for selecting representative nodes and follow-up steps would be helpful.\n\n[1] \"Hypergraph clustering by iteratively reweighted modularity maximization.\" Applied Network Science 5.1 (2020): 1-22.\n\nQuestions: 1. What is the rationale behind having a representative pair of nodes for each hyperedge? Also, provide an explanation on how this approach will cater to hyperedges of varying sizes (it can be 1 to n).\n2. How does this approach work on a hypergraph where nodes do not have any attributes? Several domains, such as computational biology, where hypergraphs have extensive applications, but getting node-level features is challenging. \n3. Provide more details on the hypergraph datasets, such as their size, average degree distribution, average hyperedge size, etc. This will help explain the result trends. \n4. Does the proposed method apply to tasks other than node classification? (such as clustering, hyperedge prediction, etc.)", "labeling_timestamp": "2026-01-11T17:38:42.786445", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper frames the objective and experiments around node-level tasks with ground-truth labels (e.g., node classification) and states empirical evaluation over datasets, but the provided content contains no experimental results or discussion evaluating other tasks such as clustering or hyperedge prediction. Therefore the reviewer's claim that the evaluation does not demonstrate generalization to those other tasks is correct.", "evidence": ["Problem 1. Given a hypergraph H = ( V , E , X ) with ground-truth labels Y , the objective is to design a hypergraph expansion method to convert hypergraph to a weighted graph G a and further employ GNNs with a mapping function f ϕ : V → R d (with parameter ϕ ) to project each node v i ∈ V to a d -dimensional embedding for downstream tasks with Y .", "convert hypergraphs into weighted graphs and further learn the hypergraph representations for node classification tasks through graph neural networks, e.g, GCN (Kipf & Welling, 2017) and GAT (Veliˇ ckovi´ c et al., 2018).", "Extensive theoretical justifications and empirical experiments over five benchmark hypergraph datasets demonstrate that AdE has excellent rationality, generalization, and effectiveness compared to classic expansion models."], "section": ["PRELIMINARY (Problem 1)", "RELATED WORKS", "ABSTRACT"]}
{"claim": "Restricting to automorphisms of cycles and simple subgraphs may make computing the decomposition into irreducible representations computationally feasible.", "claim_type": "methodology", "paper_id": "HRnSVflpgt", "paper_title": "Schur Nets: exploiting local structure for equivariance in higher order graph neural networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vD5C5zNY0Z", "reviewer": "Reviewer_EFuW", "review_text": "Comment: I highly appreciate your detailed rebuttal.\n\n1. What I'm missing is that if you only restrict to automorphisms of cycles and other simple subgraphs, isn't it computationally feasible to know the decomposition into irreducible representations? Isn't it the decomposition of the representation of the cyclic group acting on the vectors on the subgraph (`neuron')?\n\n2. Thank you for the clarification. Yes, upon looking it over again the number of linear equivariant mappings you have is always larger as the automorphism group of the subgraph is a subgroup of $S_m$, where $m$ is the subgraph size.\n\n3. \"cache a variety of control data structures on the GPU\": By this you mean the eigendecompositions of the subgraphs?\n\n4. I agree with the comment re [Feng et al.].\n\n\nI'm still debating whether this merits a score increase. is it possible to put together all the experimental results in a more readable format like a pdf? Or just a reply that is succinct with highlights from the best experimental claims you think from both the paper and rebuttal.\n\nThank you", "labeling_timestamp": "2026-01-11T17:38:39.914027", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper states that the Autobahn approach explicitly handles automorphism groups for cycles and paths and implements bespoke equivariant operations for them (implying decomposition was carried out for these simple subgraphs). However, the paper also emphasizes that explicitly determining automorphism groups and crafting representations for many subgraph types quickly becomes infeasible, so it does not claim this is generally a scalable solution.", "evidence": "\"The Autobahn architecture described in [37] explicitly accounts for the automorpism group of two specific types of subgraphs, cycles and path. Defining bespoke convolution operations on these two types of subgraphs was shown to improve performance on molecular datasets like ZINC [26]. The drawback of the Autobahn approach is that it requires explicitly identifying the automorphism group of each type of subgraph, and crafting specialized equivariant operations based on its representation theory. For more flexible architectures leveraging not just cycles and paths but many other types of subgraphs this quickly becomes infeasible.\"", "section": "Introduction (and related sentences in Abstract)"}
{"claim": "The decomposition might simply be the decomposition of the cyclic group's representation acting on the vectors on the subgraph ('neuron').", "claim_type": "methodology", "paper_id": "HRnSVflpgt", "paper_title": "Schur Nets: exploiting local structure for equivariance in higher order graph neural networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vD5C5zNY0Z", "reviewer": "Reviewer_EFuW", "review_text": "Comment: I highly appreciate your detailed rebuttal.\n\n1. What I'm missing is that if you only restrict to automorphisms of cycles and other simple subgraphs, isn't it computationally feasible to know the decomposition into irreducible representations? Isn't it the decomposition of the representation of the cyclic group acting on the vectors on the subgraph (`neuron')?\n\n2. Thank you for the clarification. Yes, upon looking it over again the number of linear equivariant mappings you have is always larger as the automorphism group of the subgraph is a subgroup of $S_m$, where $m$ is the subgraph size.\n\n3. \"cache a variety of control data structures on the GPU\": By this you mean the eigendecompositions of the subgraphs?\n\n4. I agree with the comment re [Feng et al.].\n\n\nI'm still debating whether this merits a score increase. is it possible to put together all the experimental results in a more readable format like a pdf? Or just a reply that is succinct with highlights from the best experimental claims you think from both the paper and rebuttal.\n\nThank you", "labeling_timestamp": "2026-01-11T17:38:44.065572", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper states that one should decompose the representation of the automorphism group Aut(S) acting on the subgraph (Section 3). For cycle subgraphs the reviewer’s suggestion (decomposing a cyclic group's representation) captures the rotational part, but the paper requires equivariance to the full automorphism group (which for an undirected cycle is the dihedral group including reflections), so the claim is only partially correct.", "evidence": "“However, A cannot convey any information about permutations that leave it invariant, so ϕ_A must still be equivariant to the group of all such permutations, called the automorphism group of G, denoted Aut(G) or just Aut(A).”\n\n“Crucially, as σ ranges over the automorphisms of S, these product matrices P^k(σ) form a unitary representation of the automorphism group. According to representation theory, P^k must then be decomposable into a direct sum of irreps ρ_1, . . . , ρ_p of Aut(S) ...”", "section": "Introduction (cycles mention) and Section 3 (group-theoretic decomposition)"}
{"claim": "The number of linear equivariant mappings reported is always larger because the subgraph automorphism group is a subgroup of S_m, where m is subgraph size.", "claim_type": "methodology", "paper_id": "HRnSVflpgt", "paper_title": "Schur Nets: exploiting local structure for equivariance in higher order graph neural networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vD5C5zNY0Z", "reviewer": "Reviewer_EFuW", "review_text": "Comment: I highly appreciate your detailed rebuttal.\n\n1. What I'm missing is that if you only restrict to automorphisms of cycles and other simple subgraphs, isn't it computationally feasible to know the decomposition into irreducible representations? Isn't it the decomposition of the representation of the cyclic group acting on the vectors on the subgraph (`neuron')?\n\n2. Thank you for the clarification. Yes, upon looking it over again the number of linear equivariant mappings you have is always larger as the automorphism group of the subgraph is a subgroup of $S_m$, where $m$ is the subgraph size.\n\n3. \"cache a variety of control data structures on the GPU\": By this you mean the eigendecompositions of the subgraphs?\n\n4. I agree with the comment re [Feng et al.].\n\n\nI'm still debating whether this merits a score increase. is it possible to put together all the experimental results in a more readable format like a pdf? Or just a reply that is succinct with highlights from the best experimental claims you think from both the paper and rebuttal.\n\nThank you", "labeling_timestamp": "2026-01-11T17:39:05.885204", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly states that operations should be equivariant to the automorphism group Aut(S) rather than the full symmetric group S_m and calls full S_m-equivariance \"overly restrictive,\" which implies the space of maps equivariant to Aut(S) is less constrained (hence larger) than that equivariant to S_m. However, the paper does not explicitly state or report numeric counts showing that the number of linear equivariant mappings is always larger, so the claim's quantitative phrasing is not directly confirmed.", "evidence": "1) \"When considering specific, structurally important subgraphs such as paths or cycles, the operations local to such a subgraph S should really only be equivariant to the automorphism group of S , rather than all | S | ! permutations.\" 2) \"As we discussed above, this is an overly restrictive condition that limits the extent to which a higher order subgraph neural network can exploit the underlying topology. In the following sections we focus on just the type of messages that are sent from a given subgraph S to itself (called linmaps in the P -tensors nomenclature), and derive a way of making these messages equivariant to just Aut( S ) rather than the full symmetric group S m .\"", "section": "1 Introduction; 2.1 Higher order GNNs"}
{"claim": "It is unclear whether 'cache a variety of control data structures on the GPU' refers to caching eigendecompositions of the subgraphs.", "claim_type": "methodology", "paper_id": "HRnSVflpgt", "paper_title": "Schur Nets: exploiting local structure for equivariance in higher order graph neural networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vD5C5zNY0Z", "reviewer": "Reviewer_EFuW", "review_text": "Comment: I highly appreciate your detailed rebuttal.\n\n1. What I'm missing is that if you only restrict to automorphisms of cycles and other simple subgraphs, isn't it computationally feasible to know the decomposition into irreducible representations? Isn't it the decomposition of the representation of the cyclic group acting on the vectors on the subgraph (`neuron')?\n\n2. Thank you for the clarification. Yes, upon looking it over again the number of linear equivariant mappings you have is always larger as the automorphism group of the subgraph is a subgroup of $S_m$, where $m$ is the subgraph size.\n\n3. \"cache a variety of control data structures on the GPU\": By this you mean the eigendecompositions of the subgraphs?\n\n4. I agree with the comment re [Feng et al.].\n\n\nI'm still debating whether this merits a score increase. is it possible to put together all the experimental results in a more readable format like a pdf? Or just a reply that is succinct with highlights from the best experimental claims you think from both the paper and rebuttal.\n\nThank you", "labeling_timestamp": "2026-01-11T17:38:53.138977", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper text describes a spectral approach using the graph Laplacian to construct equivariant bases but contains no mention of 'caching', 'GPU', 'control data structures', or explicitly caching eigendecompositions of subgraphs; therefore the paper does not address whether that phrase refers to caching eigendecompositions.", "evidence": "Abstract: \"...constructs a basis for equivariant operations directly from the graph Laplacian.\" Main contributions: \"The main contribution of this paper is a simple algorithm based on spectral graph theory for constructing a basis of automorphism equivariant operations on any possible subgraph without explicitly having to determine its automorphism group... Section 4 describes the algorithm itself and the resulting neural network operations, which we call Schur layers.\"", "section": "Abstract / Main contributions"}
{"claim": "The paper and rebuttal do not present all experimental results in a consolidated, readable format such as a single PDF.", "claim_type": "presentation", "paper_id": "HRnSVflpgt", "paper_title": "Schur Nets: exploiting local structure for equivariance in higher order graph neural networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vD5C5zNY0Z", "reviewer": "Reviewer_EFuW", "review_text": "Comment: I highly appreciate your detailed rebuttal.\n\n1. What I'm missing is that if you only restrict to automorphisms of cycles and other simple subgraphs, isn't it computationally feasible to know the decomposition into irreducible representations? Isn't it the decomposition of the representation of the cyclic group acting on the vectors on the subgraph (`neuron')?\n\n2. Thank you for the clarification. Yes, upon looking it over again the number of linear equivariant mappings you have is always larger as the automorphism group of the subgraph is a subgroup of $S_m$, where $m$ is the subgraph size.\n\n3. \"cache a variety of control data structures on the GPU\": By this you mean the eigendecompositions of the subgraphs?\n\n4. I agree with the comment re [Feng et al.].\n\n\nI'm still debating whether this merits a score increase. is it possible to put together all the experimental results in a more readable format like a pdf? Or just a reply that is succinct with highlights from the best experimental claims you think from both the paper and rebuttal.\n\nThank you", "labeling_timestamp": "2026-01-11T17:38:58.281831", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt refers to experimental results in Section 5 but the actual experimental results and any rebuttal text are not included in the content given, so there is insufficient information to verify whether all experimental results are presented in a consolidated, readable format (e.g., a single PDF).", "evidence": "“In Section 5 we show that the introduction of Schur layers does indeed improve the performance of higher order graph neural networks on standard molecular benchmarks.”", "section": "Introduction (Main contributions / end of Introduction)"}
{"claim": "The authors did not provide a succinct reply highlighting the strongest experimental claims from both the paper and rebuttal.", "claim_type": "presentation", "paper_id": "HRnSVflpgt", "paper_title": "Schur Nets: exploiting local structure for equivariance in higher order graph neural networks", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "vD5C5zNY0Z", "reviewer": "Reviewer_EFuW", "review_text": "Comment: I highly appreciate your detailed rebuttal.\n\n1. What I'm missing is that if you only restrict to automorphisms of cycles and other simple subgraphs, isn't it computationally feasible to know the decomposition into irreducible representations? Isn't it the decomposition of the representation of the cyclic group acting on the vectors on the subgraph (`neuron')?\n\n2. Thank you for the clarification. Yes, upon looking it over again the number of linear equivariant mappings you have is always larger as the automorphism group of the subgraph is a subgroup of $S_m$, where $m$ is the subgraph size.\n\n3. \"cache a variety of control data structures on the GPU\": By this you mean the eigendecompositions of the subgraphs?\n\n4. I agree with the comment re [Feng et al.].\n\n\nI'm still debating whether this merits a score increase. is it possible to put together all the experimental results in a more readable format like a pdf? Or just a reply that is succinct with highlights from the best experimental claims you think from both the paper and rebuttal.\n\nThank you", "labeling_timestamp": "2026-01-11T17:39:25.597319", "model": "gpt-5-mini", "label": "Supported", "justification": "The submitted paper contains the regular manuscript (abstract, introduction, methods, etc.) and does not include any separate author 'reply' or rebuttal highlighting experimental claims. Therefore the reviewer's statement that the authors did not provide a succinct reply is accurate with respect to the paper content provided.", "evidence": "\"Main contributions. The main contribution of this paper is a simple algorithm based on spectral graph theory for constructing a basis of automorphism equivariant operations on any possible subgraph without explicitly having to determine its automorphism group, let alone derive its irreducible representations.\"", "section": "1 Introduction (Main contributions)"}
{"claim": "The specific parameterization of the PID model (Eqn. 7) is not well-justified and appears undermotivated.", "claim_type": "methodology", "paper_id": "a24gfxA7jD", "paper_title": "Physics Informed Distillation for Diffusion Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "GvTxqBwc1Y", "reviewer": "Reviewer_rfnU", "review_text": "Summary: The authors propose Physics Informed Distillation (PID), a method for distillation of a teacher diffusion model up to single-step image generation. Inspired by models from the Physics Informed Neural Network (PINN) architecture, the method trains a student model to approximately satisfy the probability flow ODE induced by the teacher diffusion model. To speed up the training process, the residual loss is approximated using a first-order Euler discretization step, instead of having to apply backpropagation. A theoretical analysis of the discretization error is also provided.\n\nStrengths: - The authors make an interesting connection between distillation of diffusion models and PINNs via enforcement of the probability flow ODE.\n- The authors propose PID, a relatively simple method for distillation, which shows results comparable to state-of-the-art single-step image generation for CIFAR10 and ImageNet64.\n- The paper is generally well-written and clear.\n- The PID distillation method achieves results comparable to current state-of-the-art single-step image generation methods (1) using an arguably simpler method involving fewer hyperparameters/training tricks.\n\n1. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models.\n\nWeaknesses: - The specific parameterization of the PID model (Eqn. 7) seems somewhat undermotivated to me. The authors mention they take inspiration from the two common approaches to enforcing boundary conditions with PINNs, soft and strict conditions. However, beyond this high-level explanation, the parameterization is not justified and no ablations are performed.\n- Similarly, a first-order numerical approximation of the residual loss is proposed for the sake of efficient training, but no ablations are performed as to how much this discretization affects the performance.\n- A bit more background about PINNs, especially the soft enforcement of boundary conditions, could be helpful to better motivate the authors' choice of model parameterization.\n\nQuestions: - Is there a reason why a combination of soft and hard enforcement of boundary conditions is necessary?\n- Once a first-order discretization scheme for the residual loss is chosen, the student model training looks very similar in form to existing distillation techniques (1, 2). How is PID related to these techniques, e.g. can they be described as a special case of PID given a specific choice of time discretization of the probability flow ODE?\n- How does the first-order approximation of the residual error loss compare to training directly using the ODE (backpropagation), or with using a higher-order approximation?\n- What is the benefit of enforcing the probability flow ODE in a PINN-inspired way, as opposed to an operator learning method such as (3)?\n\n1. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models.\n2. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models.\n3. Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast sampling of diffusion models via operator learning.", "labeling_timestamp": "2026-01-11T17:39:02.722074", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper provides explicit motivation for the parameterization in Eqn. 7: it enforces the boundary condition via a hard-conditioning / skip-connection approach (motivated by PINNs and common diffusion training practices) and points to Appendix A.2 for further insight. It also describes a variable shift to stabilize training near t=0. Thus the claim that the parameterization is 'not well-justified and appears undermotivated' is contradicted by the paper's stated motivations.", "evidence": "“Since the use of skip connections is also common in diffusion training (Karras et al. (2022)), we take inspiration from both fields and parametrize our model as: … Here, X_θ denotes the neural network to be trained (insight into the choice of skip connection functions are provided in Appendix A.2.)”\n\n“To alleviate this, we shift the variables as: … Through this, we can obtain a stable training loss that does not explode when time values near the origin are sampled.”", "section": "4.1 TRAJECTORY FUNCTIONS"}
{"claim": "The authors do not perform ablation studies to justify the chosen PID model parameterization.", "claim_type": "experimental", "paper_id": "a24gfxA7jD", "paper_title": "Physics Informed Distillation for Diffusion Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "GvTxqBwc1Y", "reviewer": "Reviewer_rfnU", "review_text": "Summary: The authors propose Physics Informed Distillation (PID), a method for distillation of a teacher diffusion model up to single-step image generation. Inspired by models from the Physics Informed Neural Network (PINN) architecture, the method trains a student model to approximately satisfy the probability flow ODE induced by the teacher diffusion model. To speed up the training process, the residual loss is approximated using a first-order Euler discretization step, instead of having to apply backpropagation. A theoretical analysis of the discretization error is also provided.\n\nStrengths: - The authors make an interesting connection between distillation of diffusion models and PINNs via enforcement of the probability flow ODE.\n- The authors propose PID, a relatively simple method for distillation, which shows results comparable to state-of-the-art single-step image generation for CIFAR10 and ImageNet64.\n- The paper is generally well-written and clear.\n- The PID distillation method achieves results comparable to current state-of-the-art single-step image generation methods (1) using an arguably simpler method involving fewer hyperparameters/training tricks.\n\n1. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models.\n\nWeaknesses: - The specific parameterization of the PID model (Eqn. 7) seems somewhat undermotivated to me. The authors mention they take inspiration from the two common approaches to enforcing boundary conditions with PINNs, soft and strict conditions. However, beyond this high-level explanation, the parameterization is not justified and no ablations are performed.\n- Similarly, a first-order numerical approximation of the residual loss is proposed for the sake of efficient training, but no ablations are performed as to how much this discretization affects the performance.\n- A bit more background about PINNs, especially the soft enforcement of boundary conditions, could be helpful to better motivate the authors' choice of model parameterization.\n\nQuestions: - Is there a reason why a combination of soft and hard enforcement of boundary conditions is necessary?\n- Once a first-order discretization scheme for the residual loss is chosen, the student model training looks very similar in form to existing distillation techniques (1, 2). How is PID related to these techniques, e.g. can they be described as a special case of PID given a specific choice of time discretization of the probability flow ODE?\n- How does the first-order approximation of the residual error loss compare to training directly using the ODE (backpropagation), or with using a higher-order approximation?\n- What is the benefit of enforcing the probability flow ODE in a PINN-inspired way, as opposed to an operator learning method such as (3)?\n\n1. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models.\n2. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models.\n3. Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast sampling of diffusion models via operator learning.", "labeling_timestamp": "2026-01-11T17:39:26.054324", "model": "gpt-5-mini", "label": "Supported", "justification": "The provided paper content contains no ablation studies or any section describing ablation experiments to justify design or parameterization choices. The only related material is a pointer to Appendix A.2 for insights on skip connections, but no reported ablation results appear in the main text (Abstract, Introduction, Contributions, or Methods sections).", "evidence": "1) \"insight into the choice of skip connection functions are provided in Appendix A.2.)\" (Section 4.1)\n2) Contribution statements and method descriptions discuss results and stability but do not present ablation experiments: \"We demonstrate that similar to PINNs where the performance improvements saturate at a sufficiently large number of collocation points, our approach with a high enough discretization number performs best...\" (Introduction / Contributions)", "section": "4.1 TRAJECTORY FUNCTIONS; Introduction / Contributions"}
{"claim": "The paper proposes a first-order numerical approximation of the residual loss without ablation studies evaluating how discretization affects performance.", "claim_type": "methodology", "paper_id": "a24gfxA7jD", "paper_title": "Physics Informed Distillation for Diffusion Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "GvTxqBwc1Y", "reviewer": "Reviewer_rfnU", "review_text": "Summary: The authors propose Physics Informed Distillation (PID), a method for distillation of a teacher diffusion model up to single-step image generation. Inspired by models from the Physics Informed Neural Network (PINN) architecture, the method trains a student model to approximately satisfy the probability flow ODE induced by the teacher diffusion model. To speed up the training process, the residual loss is approximated using a first-order Euler discretization step, instead of having to apply backpropagation. A theoretical analysis of the discretization error is also provided.\n\nStrengths: - The authors make an interesting connection between distillation of diffusion models and PINNs via enforcement of the probability flow ODE.\n- The authors propose PID, a relatively simple method for distillation, which shows results comparable to state-of-the-art single-step image generation for CIFAR10 and ImageNet64.\n- The paper is generally well-written and clear.\n- The PID distillation method achieves results comparable to current state-of-the-art single-step image generation methods (1) using an arguably simpler method involving fewer hyperparameters/training tricks.\n\n1. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models.\n\nWeaknesses: - The specific parameterization of the PID model (Eqn. 7) seems somewhat undermotivated to me. The authors mention they take inspiration from the two common approaches to enforcing boundary conditions with PINNs, soft and strict conditions. However, beyond this high-level explanation, the parameterization is not justified and no ablations are performed.\n- Similarly, a first-order numerical approximation of the residual loss is proposed for the sake of efficient training, but no ablations are performed as to how much this discretization affects the performance.\n- A bit more background about PINNs, especially the soft enforcement of boundary conditions, could be helpful to better motivate the authors' choice of model parameterization.\n\nQuestions: - Is there a reason why a combination of soft and hard enforcement of boundary conditions is necessary?\n- Once a first-order discretization scheme for the residual loss is chosen, the student model training looks very similar in form to existing distillation techniques (1, 2). How is PID related to these techniques, e.g. can they be described as a special case of PID given a specific choice of time discretization of the probability flow ODE?\n- How does the first-order approximation of the residual error loss compare to training directly using the ODE (backpropagation), or with using a higher-order approximation?\n- What is the benefit of enforcing the probability flow ODE in a PINN-inspired way, as opposed to an operator learning method such as (3)?\n\n1. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models.\n2. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models.\n3. Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast sampling of diffusion models via operator learning.", "labeling_timestamp": "2026-01-11T17:39:27.423415", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper clearly discusses using numerical differentiation to compute the residual loss (Section 4.2) and repeatedly refers to discretization/collocation number and its effect on stability and performance (Abstract and Introduction), so the parts of the claim about numerical approximation and discretization being important are supported. However, the paper excerpts do not explicitly state that the authors use a 'first-order' numerical approximation, and in fact the paper claims to demonstrate the effect of discretization (contradicting the reviewer's statement that no ablation was done). Thus the reviewer's claim is only partially accurate.", "evidence": "Abstract: \"we establish the stability of our method under conditions involving a sufficiently high discretization number, paralleling observations found in the PINN literature...\"; Introduction contributions: \"We demonstrate that similar to PINNs where the performance improvements saturate at a sufficiently large number of collocation points, our approach with a high enough discretization number performs best...\"; Section 4.2: \"NUMERICAL DIFFERENTIATION Regrettably, the residual loss in PINNs requires computing gradients of the trajectory function with respect to its inputs, which can be computationally expensive...\"", "section": "Abstract; 1 Introduction (Contributions); 4.2 Numerical Differentiation"}
{"claim": "There is no empirical comparison between training with the first-order discretization and training using the exact ODE with backpropagation or higher-order schemes.", "claim_type": "baseline", "paper_id": "a24gfxA7jD", "paper_title": "Physics Informed Distillation for Diffusion Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "GvTxqBwc1Y", "reviewer": "Reviewer_rfnU", "review_text": "Summary: The authors propose Physics Informed Distillation (PID), a method for distillation of a teacher diffusion model up to single-step image generation. Inspired by models from the Physics Informed Neural Network (PINN) architecture, the method trains a student model to approximately satisfy the probability flow ODE induced by the teacher diffusion model. To speed up the training process, the residual loss is approximated using a first-order Euler discretization step, instead of having to apply backpropagation. A theoretical analysis of the discretization error is also provided.\n\nStrengths: - The authors make an interesting connection between distillation of diffusion models and PINNs via enforcement of the probability flow ODE.\n- The authors propose PID, a relatively simple method for distillation, which shows results comparable to state-of-the-art single-step image generation for CIFAR10 and ImageNet64.\n- The paper is generally well-written and clear.\n- The PID distillation method achieves results comparable to current state-of-the-art single-step image generation methods (1) using an arguably simpler method involving fewer hyperparameters/training tricks.\n\n1. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models.\n\nWeaknesses: - The specific parameterization of the PID model (Eqn. 7) seems somewhat undermotivated to me. The authors mention they take inspiration from the two common approaches to enforcing boundary conditions with PINNs, soft and strict conditions. However, beyond this high-level explanation, the parameterization is not justified and no ablations are performed.\n- Similarly, a first-order numerical approximation of the residual loss is proposed for the sake of efficient training, but no ablations are performed as to how much this discretization affects the performance.\n- A bit more background about PINNs, especially the soft enforcement of boundary conditions, could be helpful to better motivate the authors' choice of model parameterization.\n\nQuestions: - Is there a reason why a combination of soft and hard enforcement of boundary conditions is necessary?\n- Once a first-order discretization scheme for the residual loss is chosen, the student model training looks very similar in form to existing distillation techniques (1, 2). How is PID related to these techniques, e.g. can they be described as a special case of PID given a specific choice of time discretization of the probability flow ODE?\n- How does the first-order approximation of the residual error loss compare to training directly using the ODE (backpropagation), or with using a higher-order approximation?\n- What is the benefit of enforcing the probability flow ODE in a PINN-inspired way, as opposed to an operator learning method such as (3)?\n\n1. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models.\n2. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models.\n3. Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast sampling of diffusion models via operator learning.", "labeling_timestamp": "2026-01-11T17:39:46.908139", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper discusses discretization and numerical-differentiation issues and cites high-order solvers in related work, but nowhere in the provided content does it present an empirical comparison between training with a first-order discretization and training using the exact ODE with backpropagation or higher-order schemes.", "evidence": [{"quote": "We demonstrate that similar to PINNs where the performance improvements saturate at a sufficiently large number of collocation points, our approach with a high enough discretization number performs best, showcasing its potential as a knowledge distillation approach that does not need additional tuning of method specific hyperparameters.", "section": "ABSTRACT / 1 INTRODUCTION (contributions)"}, {"quote": "From this point onwards, we adopt the configuration introduced in EDM (Karras et al. (2022)). The configuration utilizes the empirical probability flow ODE on the interval t ∈ [ϵ, T] ... As such, we can conceptualize each noise as having its uniquely associated trajectory function, as illustrated in Figure 1. We denote this trajectory function as x(z, t) ... To achieve this, we adopt a PINN-like approach to learn the trajectory functions x(z, t). Specifically, we can minimize the residual loss, similar to the methodology employed in PINNs.", "section": "4.1 TRAJECTORY FUNCTIONS"}, {"quote": "Regrettably, the residual loss in PINNs requires computing gradients of the trajectory function with respect to its inputs, which can be computationally expensive using forward-mode backpropagation and may not be readily available in certain packages. Furthermore, studies such as (Chiu et al. (2022)) have demonstrated that training PINNs using automatic differentiation can lead to convergence to un", "section": "4.2 NUMERICAL DIFFERENTIATION"}, {"quote": "Notably, (Lu et al. (2022)) introduced DPM-Solver, a high-order solver for diffusion ODEs. ... These methods demonstrate the surprising ability to significantly reduce the number of model evaluations while remaining training-free.", "section": "2 RELATED WORKS (Training-free methods)"}], "section": "Overall (Abstract, Sections 2, 4.1, 4.2)"}
{"claim": "The manuscript lacks sufficient background on PINNs, especially soft enforcement of boundary conditions, to motivate the model parameterization.", "claim_type": "presentation", "paper_id": "a24gfxA7jD", "paper_title": "Physics Informed Distillation for Diffusion Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "GvTxqBwc1Y", "reviewer": "Reviewer_rfnU", "review_text": "Summary: The authors propose Physics Informed Distillation (PID), a method for distillation of a teacher diffusion model up to single-step image generation. Inspired by models from the Physics Informed Neural Network (PINN) architecture, the method trains a student model to approximately satisfy the probability flow ODE induced by the teacher diffusion model. To speed up the training process, the residual loss is approximated using a first-order Euler discretization step, instead of having to apply backpropagation. A theoretical analysis of the discretization error is also provided.\n\nStrengths: - The authors make an interesting connection between distillation of diffusion models and PINNs via enforcement of the probability flow ODE.\n- The authors propose PID, a relatively simple method for distillation, which shows results comparable to state-of-the-art single-step image generation for CIFAR10 and ImageNet64.\n- The paper is generally well-written and clear.\n- The PID distillation method achieves results comparable to current state-of-the-art single-step image generation methods (1) using an arguably simpler method involving fewer hyperparameters/training tricks.\n\n1. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models.\n\nWeaknesses: - The specific parameterization of the PID model (Eqn. 7) seems somewhat undermotivated to me. The authors mention they take inspiration from the two common approaches to enforcing boundary conditions with PINNs, soft and strict conditions. However, beyond this high-level explanation, the parameterization is not justified and no ablations are performed.\n- Similarly, a first-order numerical approximation of the residual loss is proposed for the sake of efficient training, but no ablations are performed as to how much this discretization affects the performance.\n- A bit more background about PINNs, especially the soft enforcement of boundary conditions, could be helpful to better motivate the authors' choice of model parameterization.\n\nQuestions: - Is there a reason why a combination of soft and hard enforcement of boundary conditions is necessary?\n- Once a first-order discretization scheme for the residual loss is chosen, the student model training looks very similar in form to existing distillation techniques (1, 2). How is PID related to these techniques, e.g. can they be described as a special case of PID given a specific choice of time discretization of the probability flow ODE?\n- How does the first-order approximation of the residual error loss compare to training directly using the ODE (backpropagation), or with using a higher-order approximation?\n- What is the benefit of enforcing the probability flow ODE in a PINN-inspired way, as opposed to an operator learning method such as (3)?\n\n1. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models.\n2. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models.\n3. Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast sampling of diffusion models via operator learning.", "labeling_timestamp": "2026-01-11T17:39:43.614310", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper includes a dedicated PINNs background section that explicitly describes both soft and hard (skip-connection) boundary-conditioning approaches and states it focuses on the hard approach, and Section 4.1 reiterates this and cites Appendix A.2 for skip-connection details—therefore the manuscript does provide background on PINNs and mentions soft enforcement.", "evidence": "3.2 PHYSICS INFORMED NEURAL NETWORKS (PINN): \"To solve this ODE system, Physics Informed Neural Networks (PINNs) can be divided into two distinct approaches: a soft conditioning approach (Cuomo et al. (2022)), in which boundary conditions are sampled and trained, and a hard conditioning approach, (Lagaris et al. (1998); Cuomo et al. (2022)) where such conditions are automatically satisfied through the utilization of skip connections. For the sake of simplicity, we exclusively focus on the latter easier approach, where the PINNs output is represented as follows:\"; 4.1 TRAJECTORY FUNCTIONS: \"In PINN literature, the two common approaches to achieve this is either by using a soft condition (Cuomo et al. (2022)), in which boundary conditions are sampled and trained or through a strict condition (Lagaris et al. (1998)) where these conditions are arbitrarily satisfied using skip connections. Since the use of skip connections is also common in diffusion training (Karras et al. (2022)), we take inspiration from both fields and parametrize our model as: ... (insight into the choice of skip connection functions are provided in Appendix A.2.)\"", "section": "3.2 PHYSICS INFORMED NEURAL NETWORKS (PINN) and 4.1 TRAJECTORY FUNCTIONS"}
{"claim": "It is unclear why a combination of soft and hard enforcement of boundary conditions is necessary for PID.", "claim_type": "subjective", "paper_id": "a24gfxA7jD", "paper_title": "Physics Informed Distillation for Diffusion Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "GvTxqBwc1Y", "reviewer": "Reviewer_rfnU", "review_text": "Summary: The authors propose Physics Informed Distillation (PID), a method for distillation of a teacher diffusion model up to single-step image generation. Inspired by models from the Physics Informed Neural Network (PINN) architecture, the method trains a student model to approximately satisfy the probability flow ODE induced by the teacher diffusion model. To speed up the training process, the residual loss is approximated using a first-order Euler discretization step, instead of having to apply backpropagation. A theoretical analysis of the discretization error is also provided.\n\nStrengths: - The authors make an interesting connection between distillation of diffusion models and PINNs via enforcement of the probability flow ODE.\n- The authors propose PID, a relatively simple method for distillation, which shows results comparable to state-of-the-art single-step image generation for CIFAR10 and ImageNet64.\n- The paper is generally well-written and clear.\n- The PID distillation method achieves results comparable to current state-of-the-art single-step image generation methods (1) using an arguably simpler method involving fewer hyperparameters/training tricks.\n\n1. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models.\n\nWeaknesses: - The specific parameterization of the PID model (Eqn. 7) seems somewhat undermotivated to me. The authors mention they take inspiration from the two common approaches to enforcing boundary conditions with PINNs, soft and strict conditions. However, beyond this high-level explanation, the parameterization is not justified and no ablations are performed.\n- Similarly, a first-order numerical approximation of the residual loss is proposed for the sake of efficient training, but no ablations are performed as to how much this discretization affects the performance.\n- A bit more background about PINNs, especially the soft enforcement of boundary conditions, could be helpful to better motivate the authors' choice of model parameterization.\n\nQuestions: - Is there a reason why a combination of soft and hard enforcement of boundary conditions is necessary?\n- Once a first-order discretization scheme for the residual loss is chosen, the student model training looks very similar in form to existing distillation techniques (1, 2). How is PID related to these techniques, e.g. can they be described as a special case of PID given a specific choice of time discretization of the probability flow ODE?\n- How does the first-order approximation of the residual error loss compare to training directly using the ODE (backpropagation), or with using a higher-order approximation?\n- What is the benefit of enforcing the probability flow ODE in a PINN-inspired way, as opposed to an operator learning method such as (3)?\n\n1. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models.\n2. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models.\n3. Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast sampling of diffusion models via operator learning.", "labeling_timestamp": "2026-01-11T17:39:48.230704", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper mentions both soft and hard (skip-connection) boundary-condition approaches and states it \"takes inspiration from both fields\" when parametrizing the model, but it does not provide a clear explanation of why a combination of soft and hard enforcement is necessary. The text points readers to Appendix A.2 for skip-connection details but offers no explicit rationale for combining the two enforcement styles in the main text.", "evidence": "1) \"PINNs ... can be divided into two distinct approaches: a soft conditioning approach ..., in which boundary conditions are sampled and trained, and a hard conditioning approach ... For the sake of simplicity, we exclusively focus on the latter easier approach...\" 2) \"Since the use of skip connections is also common in diffusion training (Karras et al. (2022)), we take inspiration from both fields and parametrize our model as: ... (insight into the choice of skip connection functions are provided in Appendix A.2.)\"", "section": "3.2 PHYSICS INFORMED NEURAL NETWORKS (PINN); 4.1 TRAJECTORY FUNCTIONS"}
{"claim": "The authors do not analyze how PID relates to existing distillation techniques, nor whether those methods are special cases under particular time discretizations.", "claim_type": "methodology", "paper_id": "a24gfxA7jD", "paper_title": "Physics Informed Distillation for Diffusion Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "GvTxqBwc1Y", "reviewer": "Reviewer_rfnU", "review_text": "Summary: The authors propose Physics Informed Distillation (PID), a method for distillation of a teacher diffusion model up to single-step image generation. Inspired by models from the Physics Informed Neural Network (PINN) architecture, the method trains a student model to approximately satisfy the probability flow ODE induced by the teacher diffusion model. To speed up the training process, the residual loss is approximated using a first-order Euler discretization step, instead of having to apply backpropagation. A theoretical analysis of the discretization error is also provided.\n\nStrengths: - The authors make an interesting connection between distillation of diffusion models and PINNs via enforcement of the probability flow ODE.\n- The authors propose PID, a relatively simple method for distillation, which shows results comparable to state-of-the-art single-step image generation for CIFAR10 and ImageNet64.\n- The paper is generally well-written and clear.\n- The PID distillation method achieves results comparable to current state-of-the-art single-step image generation methods (1) using an arguably simpler method involving fewer hyperparameters/training tricks.\n\n1. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models.\n\nWeaknesses: - The specific parameterization of the PID model (Eqn. 7) seems somewhat undermotivated to me. The authors mention they take inspiration from the two common approaches to enforcing boundary conditions with PINNs, soft and strict conditions. However, beyond this high-level explanation, the parameterization is not justified and no ablations are performed.\n- Similarly, a first-order numerical approximation of the residual loss is proposed for the sake of efficient training, but no ablations are performed as to how much this discretization affects the performance.\n- A bit more background about PINNs, especially the soft enforcement of boundary conditions, could be helpful to better motivate the authors' choice of model parameterization.\n\nQuestions: - Is there a reason why a combination of soft and hard enforcement of boundary conditions is necessary?\n- Once a first-order discretization scheme for the residual loss is chosen, the student model training looks very similar in form to existing distillation techniques (1, 2). How is PID related to these techniques, e.g. can they be described as a special case of PID given a specific choice of time discretization of the probability flow ODE?\n- How does the first-order approximation of the residual error loss compare to training directly using the ODE (backpropagation), or with using a higher-order approximation?\n- What is the benefit of enforcing the probability flow ODE in a PINN-inspired way, as opposed to an operator learning method such as (3)?\n\n1. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models.\n2. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models.\n3. Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast sampling of diffusion models via operator learning.", "labeling_timestamp": "2026-01-11T17:39:56.786245", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper lists prior distillation methods in Related Works and presents PID as a new PINN-inspired approach, but contains no analysis or discussion that connects PID to existing distillation techniques or shows those methods are special cases under particular time discretizations.", "evidence": "\"In the domain of diffusion model distillation, training-based strategies can be broadly classified into two primary groups: those relying on synthetic data and those trained with original data. Among the former, noteworthy methods such as Knowledge Distillation (Luhman & Luhman (2021)), DSNO (Zheng et al. (2022)), and Rectified Flow (Liu et al. (2022)). have recently exhibited their efficacy in distilling single-step student models.\" \n\n\"We propose Physics Informed Distillation (PID), a knowledge distillation technique heavily inspired by PINNs that enables single-step image generation.\"", "section": "Related Works; Introduction/Contributions"}
{"claim": "The paper does not justify the benefit of enforcing the probability flow ODE in a PINN-inspired way instead of using operator learning methods such as Zheng et al.", "claim_type": "baseline", "paper_id": "a24gfxA7jD", "paper_title": "Physics Informed Distillation for Diffusion Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "GvTxqBwc1Y", "reviewer": "Reviewer_rfnU", "review_text": "Summary: The authors propose Physics Informed Distillation (PID), a method for distillation of a teacher diffusion model up to single-step image generation. Inspired by models from the Physics Informed Neural Network (PINN) architecture, the method trains a student model to approximately satisfy the probability flow ODE induced by the teacher diffusion model. To speed up the training process, the residual loss is approximated using a first-order Euler discretization step, instead of having to apply backpropagation. A theoretical analysis of the discretization error is also provided.\n\nStrengths: - The authors make an interesting connection between distillation of diffusion models and PINNs via enforcement of the probability flow ODE.\n- The authors propose PID, a relatively simple method for distillation, which shows results comparable to state-of-the-art single-step image generation for CIFAR10 and ImageNet64.\n- The paper is generally well-written and clear.\n- The PID distillation method achieves results comparable to current state-of-the-art single-step image generation methods (1) using an arguably simpler method involving fewer hyperparameters/training tricks.\n\n1. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models.\n\nWeaknesses: - The specific parameterization of the PID model (Eqn. 7) seems somewhat undermotivated to me. The authors mention they take inspiration from the two common approaches to enforcing boundary conditions with PINNs, soft and strict conditions. However, beyond this high-level explanation, the parameterization is not justified and no ablations are performed.\n- Similarly, a first-order numerical approximation of the residual loss is proposed for the sake of efficient training, but no ablations are performed as to how much this discretization affects the performance.\n- A bit more background about PINNs, especially the soft enforcement of boundary conditions, could be helpful to better motivate the authors' choice of model parameterization.\n\nQuestions: - Is there a reason why a combination of soft and hard enforcement of boundary conditions is necessary?\n- Once a first-order discretization scheme for the residual loss is chosen, the student model training looks very similar in form to existing distillation techniques (1, 2). How is PID related to these techniques, e.g. can they be described as a special case of PID given a specific choice of time discretization of the probability flow ODE?\n- How does the first-order approximation of the residual error loss compare to training directly using the ODE (backpropagation), or with using a higher-order approximation?\n- What is the benefit of enforcing the probability flow ODE in a PINN-inspired way, as opposed to an operator learning method such as (3)?\n\n1. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models.\n2. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models.\n3. Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast sampling of diffusion models via operator learning.", "labeling_timestamp": "2026-01-11T17:40:09.188186", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper introduces a PINN-inspired distillation and mentions Zheng et al. (DSNO) among synthetic-data-based distillation methods, noting their costly synthetic data generation. However, it does not provide an explicit theoretical or empirical justification comparing the benefit of enforcing the probability flow ODE in a PINN-like way versus operator-learning methods such as Zheng et al.; there is no direct comparison, analysis, or ablation that argues why the PINN-inspired enforcement is preferable to operator learning.", "evidence": "“Among the former, noteworthy methods such as Knowledge Distillation (Luhman & Luhman (2021)), DSNO (Zheng et al. (2022)), and Rectified Flow (Liu et al. (2022)). have recently exhibited their efficacy in distilling single-step student models. However, a notable drawback inherent in these approaches is the computationally costly nature of generating such synthetic data, which presents scalability challenges, particularly for larger models.”\n\n“Building upon these developments, we propose a novel distillation method for diffusion models called Physics Informed Distillation (PID), a method that takes a PINN-like approach to distill a single-step diffusion model.”", "section": "Related Works; Introduction (and 4.1 Trajectory Functions)"}
{"claim": "Training the student model using the proposed first-order discretization appears very similar to existing distillation techniques, but distinctions are not clarified.", "claim_type": "presentation", "paper_id": "a24gfxA7jD", "paper_title": "Physics Informed Distillation for Diffusion Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "GvTxqBwc1Y", "reviewer": "Reviewer_rfnU", "review_text": "Summary: The authors propose Physics Informed Distillation (PID), a method for distillation of a teacher diffusion model up to single-step image generation. Inspired by models from the Physics Informed Neural Network (PINN) architecture, the method trains a student model to approximately satisfy the probability flow ODE induced by the teacher diffusion model. To speed up the training process, the residual loss is approximated using a first-order Euler discretization step, instead of having to apply backpropagation. A theoretical analysis of the discretization error is also provided.\n\nStrengths: - The authors make an interesting connection between distillation of diffusion models and PINNs via enforcement of the probability flow ODE.\n- The authors propose PID, a relatively simple method for distillation, which shows results comparable to state-of-the-art single-step image generation for CIFAR10 and ImageNet64.\n- The paper is generally well-written and clear.\n- The PID distillation method achieves results comparable to current state-of-the-art single-step image generation methods (1) using an arguably simpler method involving fewer hyperparameters/training tricks.\n\n1. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models.\n\nWeaknesses: - The specific parameterization of the PID model (Eqn. 7) seems somewhat undermotivated to me. The authors mention they take inspiration from the two common approaches to enforcing boundary conditions with PINNs, soft and strict conditions. However, beyond this high-level explanation, the parameterization is not justified and no ablations are performed.\n- Similarly, a first-order numerical approximation of the residual loss is proposed for the sake of efficient training, but no ablations are performed as to how much this discretization affects the performance.\n- A bit more background about PINNs, especially the soft enforcement of boundary conditions, could be helpful to better motivate the authors' choice of model parameterization.\n\nQuestions: - Is there a reason why a combination of soft and hard enforcement of boundary conditions is necessary?\n- Once a first-order discretization scheme for the residual loss is chosen, the student model training looks very similar in form to existing distillation techniques (1, 2). How is PID related to these techniques, e.g. can they be described as a special case of PID given a specific choice of time discretization of the probability flow ODE?\n- How does the first-order approximation of the residual error loss compare to training directly using the ODE (backpropagation), or with using a higher-order approximation?\n- What is the benefit of enforcing the probability flow ODE in a PINN-inspired way, as opposed to an operator learning method such as (3)?\n\n1. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models.\n2. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models.\n3. Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast sampling of diffusion models via operator learning.", "labeling_timestamp": "2026-01-11T17:40:13.606701", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper describes a PINN-like distillation (residual loss, variable shift, stopping teacher gradients) and begins a discussion of numerical differentiation, but the provided text does not mention a \"first-order discretization\" explicitly nor does it present a direct comparison showing whether the proposed training is similar to existing distillation techniques or whether distinctions are clarified. Therefore there is insufficient information to verify the reviewer's specific claim about a proposed first-order discretization and the lack of clarified distinctions.", "evidence": "\"We propose Physics Informed Distillation (PID), a knowledge distillation technique heavily inspired by PINNs that enables single-step image generation.\" (ABSTRACT)\n\n\"Specifically, we can minimize the residual loss, similar to the methodology employed in PINNs.\" (4.1 TRAJECTORY FUNCTIONS)\n\n\"By performing this straightforward variable manipulation, we can interpret the residual loss as the process of learning to match the composite student model, parametrized by θ, on the left portion of the distance metric with the output of the teacher diffusion model on the right portion of the distance metric. ... In line with this, we also stop the gradients flowing from the teacher diffusion model during training.\" (4.1 TRAJECTORY FUNCTIONS)\n\n\"Regrettably, the residual loss in PINNs requires computing gradients of the trajectory function with respect to its inputs, which can be computationally expensive using forward-mode backpropagation and may not be readily available in certain packages.\" (4.2 NUMERICAL DIFFERENTIATION)", "section": "ABSTRACT; 4.1 TRAJECTORY FUNCTIONS; 4.2 NUMERICAL DIFFERENTIATION"}
{"claim": "The paper fails to include empirical comparisons or discussion against operator learning baselines for fast sampling of diffusion models.", "claim_type": "baseline", "paper_id": "a24gfxA7jD", "paper_title": "Physics Informed Distillation for Diffusion Models", "paper_venue": "iclr2024", "paper_decision": "Reject", "decision": "Reject", "review_id": "GvTxqBwc1Y", "reviewer": "Reviewer_rfnU", "review_text": "Summary: The authors propose Physics Informed Distillation (PID), a method for distillation of a teacher diffusion model up to single-step image generation. Inspired by models from the Physics Informed Neural Network (PINN) architecture, the method trains a student model to approximately satisfy the probability flow ODE induced by the teacher diffusion model. To speed up the training process, the residual loss is approximated using a first-order Euler discretization step, instead of having to apply backpropagation. A theoretical analysis of the discretization error is also provided.\n\nStrengths: - The authors make an interesting connection between distillation of diffusion models and PINNs via enforcement of the probability flow ODE.\n- The authors propose PID, a relatively simple method for distillation, which shows results comparable to state-of-the-art single-step image generation for CIFAR10 and ImageNet64.\n- The paper is generally well-written and clear.\n- The PID distillation method achieves results comparable to current state-of-the-art single-step image generation methods (1) using an arguably simpler method involving fewer hyperparameters/training tricks.\n\n1. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models.\n\nWeaknesses: - The specific parameterization of the PID model (Eqn. 7) seems somewhat undermotivated to me. The authors mention they take inspiration from the two common approaches to enforcing boundary conditions with PINNs, soft and strict conditions. However, beyond this high-level explanation, the parameterization is not justified and no ablations are performed.\n- Similarly, a first-order numerical approximation of the residual loss is proposed for the sake of efficient training, but no ablations are performed as to how much this discretization affects the performance.\n- A bit more background about PINNs, especially the soft enforcement of boundary conditions, could be helpful to better motivate the authors' choice of model parameterization.\n\nQuestions: - Is there a reason why a combination of soft and hard enforcement of boundary conditions is necessary?\n- Once a first-order discretization scheme for the residual loss is chosen, the student model training looks very similar in form to existing distillation techniques (1, 2). How is PID related to these techniques, e.g. can they be described as a special case of PID given a specific choice of time discretization of the probability flow ODE?\n- How does the first-order approximation of the residual error loss compare to training directly using the ODE (backpropagation), or with using a higher-order approximation?\n- What is the benefit of enforcing the probability flow ODE in a PINN-inspired way, as opposed to an operator learning method such as (3)?\n\n1. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models.\n2. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models.\n3. Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast sampling of diffusion models via operator learning.", "labeling_timestamp": "2026-01-11T17:40:19.197753", "model": "gpt-5-mini", "label": "Supported", "justification": "The Related Works section surveys training-free (DPM-Solver, DEIS) and training-based distillation methods (Knowledge Distillation, DSNO, Rectified Flow, Progressive Distillation, Consistency Models, etc.) but contains no mention or discussion of operator-learning baselines nor are any empirical comparisons to operator learning approaches presented in the provided content.", "evidence": "Training-based methods . In the domain of diffusion model distillation, training-based strategies can be broadly classified into two primary groups: those relying on synthetic data and those trained with original data. Among the former, noteworthy methods such as Knowledge Distillation (Luhman & Luhman (2021)), DSNO (Zheng et al. (2022)), and Rectified Flow (Liu et al. (2022)). have recently exhibited their efficacy in distilling single-step student models. However, a notable drawback inherent in these approaches is the computationally costly nature of generating such synthetic data, which presents scalability challenges, particularly for larger models. In response, methods using only the original dataset such as (Salimans & Ho (2022)) and (Song et al. (2023)) have emerged as solutions to this issue. Progressive Distillation adopts a multi-stage distillation approach, while Consistency Models employ a self-teacher model, reminiscent of self-supervised learning techniques.", "section": "2 RELATED WORKS (Training-based methods)"}
{"claim": "The main contribution appears to be applying the well-known CoreSet method on representations learned by an existing contrastive loss, calling the paper's novelty into question.", "claim_type": "novelty", "paper_id": "uyqjpycMbU", "paper_title": "Integrating Deep Metric Learning with Coreset for Active Learning in 3D Segmentation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "H0XOuH3aDu", "reviewer": "Reviewer_gm1Z", "review_text": "Comment: Thank you for the clear and detailed answers. I understand that [3] uses its group-wise contrastive loss for self-supervised pretraining, and not specifically for active learning. However, one can also argue that your main contribution is applying the well-known CoreSet method on the representation learned by this existing contrastive loss.\n\nMoreover, I still do not fully understand why the contrastive learning-based encoder is not used in the downstream segmentation task. Since active learning methods only differ in how they select the samples to annotate, the comparison would be fair if all methods use the same training strategy with their own selected annotations. For example, the random selection strategy could simply fine-tune the contrastive learning-based encoder + the decoder using annotations of randomly selected images.\n\nLast, I still have some doubts regarding the alignment of structures in the scribble-weighted distance. In ACDC, even though images are globally registered, the position and size of cardiac structures rarely match across different images due to the movement of the heart and respiration. The local contrastive loss in [3] compares local regions in the *same* image. From my understanding, the images in your eq (5) can be different ?", "labeling_timestamp": "2026-01-11T17:40:49.635363", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper does apply Coreset on representations learned with a contrastive-style loss (so the reviewer’s observation is partly true). However, the paper proposes a novel Group-based Contrastive Learning (GCL) modification of NT-Xent that leverages medical data groupings (patient, volume, adjacent slices) and explicitly integrates this metric learning with Coreset for slice-based and weakly supervised 3D segmentation, which supports novel elements beyond simply applying an existing contrastive loss.", "evidence": "Abstract: \"This paper introduces a novel metric learning method for Coreset to perform slice-based active learning in 3D medical segmentation. By merging contrastive learning with inherent data groupings in medical imaging, we learn a metric that emphasizes the relevant differences in samples for training 3D medical segmentation models.\"  \n\nSection 3.1: \"This motivates us to consider deep metric learning to utilize more task-relevant information, where d_ϕ is some parameterized metric.\"  \n\nSection 3.2.1: \"we propose a general group contrastive loss based on NT-Xent loss... In our group-based loss, we promote similar embeddings for slices from the same group and dissimilar embeddings for slices from different groups, enhancing group-level representation.\"  \n\nSection 1 (Contributions): \"1. The first work to integrate deep metric learning with Coreset during active learning for 3D medical segmentation.\"", "section": "Abstract; 3.1 Problem Formulation; 3.2.1 Group-based Contrastive Learning; 1 Introduction"}
{"claim": "The paper does not explain why the contrastive learning-based encoder is not used during downstream segmentation training.", "claim_type": "methodology", "paper_id": "uyqjpycMbU", "paper_title": "Integrating Deep Metric Learning with Coreset for Active Learning in 3D Segmentation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "H0XOuH3aDu", "reviewer": "Reviewer_gm1Z", "review_text": "Comment: Thank you for the clear and detailed answers. I understand that [3] uses its group-wise contrastive loss for self-supervised pretraining, and not specifically for active learning. However, one can also argue that your main contribution is applying the well-known CoreSet method on the representation learned by this existing contrastive loss.\n\nMoreover, I still do not fully understand why the contrastive learning-based encoder is not used in the downstream segmentation task. Since active learning methods only differ in how they select the samples to annotate, the comparison would be fair if all methods use the same training strategy with their own selected annotations. For example, the random selection strategy could simply fine-tune the contrastive learning-based encoder + the decoder using annotations of randomly selected images.\n\nLast, I still have some doubts regarding the alignment of structures in the scribble-weighted distance. In ACDC, even though images are globally registered, the position and size of cardiac structures rarely match across different images due to the movement of the heart and respiration. The local contrastive loss in [3] compares local regions in the *same* image. From my understanding, the images in your eq (5) can be different ?", "labeling_timestamp": "2026-01-11T17:40:37.752949", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes training a contrastive learning-based encoder to produce d_ϕ for Coreset selection (Section 3.2 / 3.2.1) and then states that the collected annotations are used to train a segmentation model, but it does not provide any explanation for why the contrastive encoder would not be reused during downstream segmentation training.", "evidence": "“We do this by training a contrastive learning-based encoder with a unique Group-based Contrastive Learning (GCL) ... d_ϕ is then used by the Coreset algorithm to select the optimal set of slices. ... The annotations for the collected slices are then used to train a segmentation model.”", "section": "3.2 Metric learning (and 3.2.1 Group-based Contrastive Learning)"}
{"claim": "Experimental comparisons may be unfair because different active learning methods do not use the same training strategy with their selected annotations.", "claim_type": "baseline", "paper_id": "uyqjpycMbU", "paper_title": "Integrating Deep Metric Learning with Coreset for Active Learning in 3D Segmentation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "H0XOuH3aDu", "reviewer": "Reviewer_gm1Z", "review_text": "Comment: Thank you for the clear and detailed answers. I understand that [3] uses its group-wise contrastive loss for self-supervised pretraining, and not specifically for active learning. However, one can also argue that your main contribution is applying the well-known CoreSet method on the representation learned by this existing contrastive loss.\n\nMoreover, I still do not fully understand why the contrastive learning-based encoder is not used in the downstream segmentation task. Since active learning methods only differ in how they select the samples to annotate, the comparison would be fair if all methods use the same training strategy with their own selected annotations. For example, the random selection strategy could simply fine-tune the contrastive learning-based encoder + the decoder using annotations of randomly selected images.\n\nLast, I still have some doubts regarding the alignment of structures in the scribble-weighted distance. In ACDC, even though images are globally registered, the position and size of cardiac structures rarely match across different images due to the movement of the heart and respiration. The local contrastive loss in [3] compares local regions in the *same* image. From my understanding, the images in your eq (5) can be different ?", "labeling_timestamp": "2026-01-11T17:40:54.085598", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper text does not describe the experimental training protocol or state whether all compared active learning methods used the same training strategy for their selected annotations. The paper mentions evaluations and that annotations are used to train a segmentation model, but it does not specify whether training strategies were kept identical across methods.", "evidence": "Abstract: \"We perform comprehensive evaluations using both weak and full annotations across four datasets (medical and non-medical).\"\nMethodology (3.2): \"The annotations for the collected slices are then used to train a segmentation model.\"", "section": "Abstract; 3.2 Methodology"}
{"claim": "The authors did not fine-tune the contrastive learning-based encoder plus decoder for the random selection baseline, omitting a fair baseline experiment.", "claim_type": "baseline", "paper_id": "uyqjpycMbU", "paper_title": "Integrating Deep Metric Learning with Coreset for Active Learning in 3D Segmentation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "H0XOuH3aDu", "reviewer": "Reviewer_gm1Z", "review_text": "Comment: Thank you for the clear and detailed answers. I understand that [3] uses its group-wise contrastive loss for self-supervised pretraining, and not specifically for active learning. However, one can also argue that your main contribution is applying the well-known CoreSet method on the representation learned by this existing contrastive loss.\n\nMoreover, I still do not fully understand why the contrastive learning-based encoder is not used in the downstream segmentation task. Since active learning methods only differ in how they select the samples to annotate, the comparison would be fair if all methods use the same training strategy with their own selected annotations. For example, the random selection strategy could simply fine-tune the contrastive learning-based encoder + the decoder using annotations of randomly selected images.\n\nLast, I still have some doubts regarding the alignment of structures in the scribble-weighted distance. In ACDC, even though images are globally registered, the position and size of cardiac structures rarely match across different images due to the movement of the heart and respiration. The local contrastive loss in [3] compares local regions in the *same* image. From my understanding, the images in your eq (5) can be different ?", "labeling_timestamp": "2026-01-11T17:41:11.384951", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper text describes training a contrastive encoder (GCL) and discusses related work on retraining encoders, but the excerpt contains no experimental details or training protocol that indicate how baselines (including the random selection baseline) were trained or whether the contrastive encoder+decoder were fine-tuned for that baseline. Therefore the claim cannot be verified from the provided content.", "evidence": "1) \"We do this by training a contrastive learning-based encoder with a unique Group-based Contrastive Learning (GCL) which utilizes inherent data groupings specific to medical imaging to generate the feature representation.\" 2) \"Recent work in active learning can be computationally expensive because they retrain the feature encoder after each active learning round (85).\"", "section": "3.2 Metric learning; 2 Related work"}
{"claim": "The scribble-weighted distance may be unreliable because cardiac structures are misaligned across images, undermining the distance's validity.", "claim_type": "methodology", "paper_id": "uyqjpycMbU", "paper_title": "Integrating Deep Metric Learning with Coreset for Active Learning in 3D Segmentation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "H0XOuH3aDu", "reviewer": "Reviewer_gm1Z", "review_text": "Comment: Thank you for the clear and detailed answers. I understand that [3] uses its group-wise contrastive loss for self-supervised pretraining, and not specifically for active learning. However, one can also argue that your main contribution is applying the well-known CoreSet method on the representation learned by this existing contrastive loss.\n\nMoreover, I still do not fully understand why the contrastive learning-based encoder is not used in the downstream segmentation task. Since active learning methods only differ in how they select the samples to annotate, the comparison would be fair if all methods use the same training strategy with their own selected annotations. For example, the random selection strategy could simply fine-tune the contrastive learning-based encoder + the decoder using annotations of randomly selected images.\n\nLast, I still have some doubts regarding the alignment of structures in the scribble-weighted distance. In ACDC, even though images are globally registered, the position and size of cardiac structures rarely match across different images due to the movement of the heart and respiration. The local contrastive loss in [3] compares local regions in the *same* image. From my understanding, the images in your eq (5) can be different ?", "labeling_timestamp": "2026-01-11T17:40:51.526076", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does not discuss or define a \"scribble-weighted distance\" nor analyze misalignment of cardiac structures across images as a failure mode for any distance metric. It mentions scribbles only as a form of weak annotation and focuses on learning a deep feature metric via group-based contrastive learning; there is insufficient information in the paper to evaluate the reviewer's specific claim about a scribble-weighted distance being unreliable due to cardiac misalignment.", "evidence": "\"Alternatively, weakly supervised methods, which require simpler annotations like scribbles (34; 29; 11), bounding boxes (13; 83), points (82), or semi-automated techniques (5; 44; 62; 56), have been shown to perform comparably to fully supervised methods ...\" (1 Introduction). \"In the original Coreset paper (64), s ∪ Δs is the set cover of D with radius δ. However, the Euclidean metric used to calculate the radius does not utilize task-relevant information and may be sub-optimal. This motivates us to consider deep metric learning to utilize more task-relevant information, where d_ϕ(x1, x2) is some parameterized metric.\" (3.1 Problem Formulation). \"We define 'group' as a set of 2D slices associated with one patient.\" (3.2.1 Group-based Contrastive Learning).", "section": "1 Introduction"}
{"claim": "In the ACDC dataset, cardiac structure positions and sizes frequently differ across images despite global registration due to heart movement and respiration.", "claim_type": "experimental", "paper_id": "uyqjpycMbU", "paper_title": "Integrating Deep Metric Learning with Coreset for Active Learning in 3D Segmentation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "H0XOuH3aDu", "reviewer": "Reviewer_gm1Z", "review_text": "Comment: Thank you for the clear and detailed answers. I understand that [3] uses its group-wise contrastive loss for self-supervised pretraining, and not specifically for active learning. However, one can also argue that your main contribution is applying the well-known CoreSet method on the representation learned by this existing contrastive loss.\n\nMoreover, I still do not fully understand why the contrastive learning-based encoder is not used in the downstream segmentation task. Since active learning methods only differ in how they select the samples to annotate, the comparison would be fair if all methods use the same training strategy with their own selected annotations. For example, the random selection strategy could simply fine-tune the contrastive learning-based encoder + the decoder using annotations of randomly selected images.\n\nLast, I still have some doubts regarding the alignment of structures in the scribble-weighted distance. In ACDC, even though images are globally registered, the position and size of cardiac structures rarely match across different images due to the movement of the heart and respiration. The local contrastive loss in [3] compares local regions in the *same* image. From my understanding, the images in your eq (5) can be different ?", "labeling_timestamp": "2026-01-11T17:41:02.427710", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper reports pixel-value variability across groups in the ACDC dataset (Section 3.2.1) but does not mention global registration, nor attribute differences in cardiac structure positions or sizes to heart movement or respiration. Therefore the specific claim about positions/sizes differing despite global registration due to movement/respiration is not addressed.", "evidence": "For the ACDC dataset, we note the mean pairwise absolute deviation of the normalized training slice pixel values within different groups averaged over the dataset: 0.217 over the entire dataset, 0.159 within patient groups, 0.166 within volume groups, and 0.115 within adjacent slice groups. Note that the volume groups and the adjacent slice groups are the most and least diverse respectively.", "section": "3.2.1 Group-based Contrastive Learning for feature representation in metric learning"}
{"claim": "The local contrastive loss in [3] compares local regions within the same image rather than across different images.", "claim_type": "methodology", "paper_id": "uyqjpycMbU", "paper_title": "Integrating Deep Metric Learning with Coreset for Active Learning in 3D Segmentation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "H0XOuH3aDu", "reviewer": "Reviewer_gm1Z", "review_text": "Comment: Thank you for the clear and detailed answers. I understand that [3] uses its group-wise contrastive loss for self-supervised pretraining, and not specifically for active learning. However, one can also argue that your main contribution is applying the well-known CoreSet method on the representation learned by this existing contrastive loss.\n\nMoreover, I still do not fully understand why the contrastive learning-based encoder is not used in the downstream segmentation task. Since active learning methods only differ in how they select the samples to annotate, the comparison would be fair if all methods use the same training strategy with their own selected annotations. For example, the random selection strategy could simply fine-tune the contrastive learning-based encoder + the decoder using annotations of randomly selected images.\n\nLast, I still have some doubts regarding the alignment of structures in the scribble-weighted distance. In ACDC, even though images are globally registered, the position and size of cardiac structures rarely match across different images due to the movement of the heart and respiration. The local contrastive loss in [3] compares local regions in the *same* image. From my understanding, the images in your eq (5) can be different ?", "labeling_timestamp": "2026-01-11T17:41:15.302763", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The paper does not discuss a “local contrastive loss in [3]” or describe that prior method; instead it presents a group-based contrastive loss that promotes similarity among slices from the same group (e.g., patient) and dissimilarity across groups. There is no information in the paper that verifies whether [3]'s local contrastive loss compares local regions within the same image.", "evidence": "\"In the same vein, in our group-based loss, we promote similar embeddings for slices from the same group and dissimilar embeddings for slices from different groups, enhancing group-level representation.\" \"We define 'group' as a set of 2D slices associated with one patient.\"", "section": "3.2.1 Group-based Contrastive Learning for feature representation in metric learning"}
{"claim": "Equation (5) appears to permit comparisons between different images, which would be problematic given inter-image anatomical misalignment.", "claim_type": "methodology", "paper_id": "uyqjpycMbU", "paper_title": "Integrating Deep Metric Learning with Coreset for Active Learning in 3D Segmentation", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "H0XOuH3aDu", "reviewer": "Reviewer_gm1Z", "review_text": "Comment: Thank you for the clear and detailed answers. I understand that [3] uses its group-wise contrastive loss for self-supervised pretraining, and not specifically for active learning. However, one can also argue that your main contribution is applying the well-known CoreSet method on the representation learned by this existing contrastive loss.\n\nMoreover, I still do not fully understand why the contrastive learning-based encoder is not used in the downstream segmentation task. Since active learning methods only differ in how they select the samples to annotate, the comparison would be fair if all methods use the same training strategy with their own selected annotations. For example, the random selection strategy could simply fine-tune the contrastive learning-based encoder + the decoder using annotations of randomly selected images.\n\nLast, I still have some doubts regarding the alignment of structures in the scribble-weighted distance. In ACDC, even though images are globally registered, the position and size of cardiac structures rarely match across different images due to the movement of the heart and respiration. The local contrastive loss in [3] compares local regions in the *same* image. From my understanding, the images in your eq (5) can be different ?", "labeling_timestamp": "2026-01-11T17:41:20.496444", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper’s contrastive formulation explicitly compares embeddings of different slices/images (supporting the reviewer’s observation that Equation (5) permits inter-image comparisons). However, the paper does not claim such comparisons are problematic due to anatomical misalignment and does not discuss inter-image anatomical misalignment as an issue; instead it mitigates unwanted comparisons via group definitions and exclusions, so the reviewer’s concern about misalignment is not supported by the text.", "evidence": "“The NT-Xent loss focuses on generating and comparing embeddings for image pairs and their augmentations.”; “In the same vein, in our group-based loss, we promote similar embeddings for slices from the same group and dissimilar embeddings for slices from different groups, enhancing group-level representation.”; “The denominator excludes patient slices for a particular data point that are not part of the same group. Excluding non-group patient slices ensures that the model does not promote dissimilarity between non-group slices from the same patient.”", "section": "3.2 and 3.2.1 (Metric learning; Group-based Contrastive Learning)"}
{"claim": "The ablation study lacks a ViT variant using super-pixel plus convolutional embedding instead of patch extraction, preventing isolation of the super-pixel contribution.", "claim_type": "experimental", "paper_id": "IRcv4yFX6z", "paper_title": "Learning Hierarchical Image Segmentation For Recognition and By Recognition", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "GtHJBOtrXQ", "reviewer": "Reviewer_5FXt", "review_text": "Summary: This paper presents CAST - a variant of ViTs that does hierarchical segmentation of inputs and tokens as part of its pipeline. The method is simple - input pixels are partitioned into super-pixels (in contrast to simple patches for ViTs) a convolutional network is applied over the input image and the resulting features are pooled across the superpixels (average pooling) to create the initial set of tokens. \nFrom here the method works by passing the set of tokens through ViT blocks and between them graph pooling layers which group tokens into a coarser set of tokens by sampling centroids and measuring similarity between tokens and centroids. \n\nThe method is trained with a self supervised objective (MoCo, read out from the last layer) and demonstrated to work well in a variety of contexts.\n\nStrengths: I think this is an interesting paper with good motivation and a simple and effective implementation of core ideas.\n\n* While simple, the use of super pixels and feature pooling to replace the awkward patch based embeddings of ViTs is a nice and original idea.\n* The method is quite elegant and naturally lends itself to several use cases which are demonstrated in the paper in a convincing manner (mostly)\n* Paper is well presented and well executed and was quite easy to follow.\n* A highly applicable method which should be easy to use in several contexts so significant to the community.\n\nWeaknesses: I have some (relatively minor) concerns about parts of this work:\n\n* The major thing which I find missing is more in-depth discussion of the role of super-pixels here - I feel the ablation table is missing one critical row which is a ViT with a super-pixel (+conv net) embedding layer instead of patch extraction. This is also a possible explanation to the results in Table 5. There is a chance that super-pixels are the source of most of the improvements in the paper (quantitatively, at least, clearly there would be no clear hierarchical structure in this case) and this is not discussed enough.\n\n* The positional embedding pooling is a bit odd - super pixels can be with quite arbitrary shapes and there is a chance the resulting average PE over the super pixels would bare little connection to the actual \"position\" of the super pixel. Can the authors comment on that?\n\n* While the authors claim there's a top-down element here (and arguably there is through training) I would argue it is not truly top-down at inference. The super pixels and segmentations do not depend and will not change based on what the top layers infer - the network has top-down pathway to inform them (other than gradients in training). This is of course fine, but it should be discussed.\n\nI hope to see a discussion about the points above and I would be happy to raise my score if these are discussed and addressed to.\n\nQuestions: More minor questions:\n\n* How do segmentations are extracted from ViTs in the paper? it's not clear to me how the results in, say, Figure 4, were generated for ViT-S.\n\n* Is the PE added to the CLS token as well?", "labeling_timestamp": "2026-01-11T17:41:31.595416", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided paper excerpt describes using superpixels instead of regular patches and introduces CAST, but does not include the experiments or ablation study details needed to verify whether a ViT variant using super-pixel plus convolutional embedding (instead of patch extraction) was evaluated. Therefore we cannot determine from the given content whether the claimed ablation is absent.", "evidence": "“We implement our concept by innovating Vision Transformer (ViT) (Dosovitskiy et al., 2020) on two aspects. 1) We use arbitrarily-shaped superpixels instead of square patches on a regular grid as the visual units for ViT tokens. 2) We use graph-pooling to group these segment tokens successively towards recognition, forming a fine-to-coarse segmentation hierarchy that reflects partto-whole relationships.”", "section": "Introduction"}
{"claim": "The paper does not sufficiently consider that super-pixels alone might explain most quantitative improvements, potentially confounding declared hierarchical architecture benefits.", "claim_type": "methodology", "paper_id": "IRcv4yFX6z", "paper_title": "Learning Hierarchical Image Segmentation For Recognition and By Recognition", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "GtHJBOtrXQ", "reviewer": "Reviewer_5FXt", "review_text": "Summary: This paper presents CAST - a variant of ViTs that does hierarchical segmentation of inputs and tokens as part of its pipeline. The method is simple - input pixels are partitioned into super-pixels (in contrast to simple patches for ViTs) a convolutional network is applied over the input image and the resulting features are pooled across the superpixels (average pooling) to create the initial set of tokens. \nFrom here the method works by passing the set of tokens through ViT blocks and between them graph pooling layers which group tokens into a coarser set of tokens by sampling centroids and measuring similarity between tokens and centroids. \n\nThe method is trained with a self supervised objective (MoCo, read out from the last layer) and demonstrated to work well in a variety of contexts.\n\nStrengths: I think this is an interesting paper with good motivation and a simple and effective implementation of core ideas.\n\n* While simple, the use of super pixels and feature pooling to replace the awkward patch based embeddings of ViTs is a nice and original idea.\n* The method is quite elegant and naturally lends itself to several use cases which are demonstrated in the paper in a convincing manner (mostly)\n* Paper is well presented and well executed and was quite easy to follow.\n* A highly applicable method which should be easy to use in several contexts so significant to the community.\n\nWeaknesses: I have some (relatively minor) concerns about parts of this work:\n\n* The major thing which I find missing is more in-depth discussion of the role of super-pixels here - I feel the ablation table is missing one critical row which is a ViT with a super-pixel (+conv net) embedding layer instead of patch extraction. This is also a possible explanation to the results in Table 5. There is a chance that super-pixels are the source of most of the improvements in the paper (quantitatively, at least, clearly there would be no clear hierarchical structure in this case) and this is not discussed enough.\n\n* The positional embedding pooling is a bit odd - super pixels can be with quite arbitrary shapes and there is a chance the resulting average PE over the super pixels would bare little connection to the actual \"position\" of the super pixel. Can the authors comment on that?\n\n* While the authors claim there's a top-down element here (and arguably there is through training) I would argue it is not truly top-down at inference. The super pixels and segmentations do not depend and will not change based on what the top layers infer - the network has top-down pathway to inform them (other than gradients in training). This is of course fine, but it should be discussed.\n\nI hope to see a discussion about the points above and I would be happy to raise my score if these are discussed and addressed to.\n\nQuestions: More minor questions:\n\n* How do segmentations are extracted from ViTs in the paper? it's not clear to me how the results in, say, Figure 4, were generated for ViT-S.\n\n* Is the PE added to the CLS token as well?", "labeling_timestamp": "2026-01-11T17:41:47.278832", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper introduces two simultaneous design changes—using arbitrarily-shaped superpixels as tokens and graph-pooling to form a fine-to-coarse hierarchy—and attributes improved segmentation and recognition to the combined CAST design, but the provided content contains no controlled ablation or analysis that isolates the effect of using superpixels alone versus the hierarchical graph-pooling. Thus the reviewer’s concern that super-pixels alone might explain much of the quantitative gain and is not sufficiently considered is supported by the paper text.", "evidence": "\"We implement our concept by innovating Vision Transformer (ViT) (Dosovitskiy et al., 2020) on two aspects. 1) We use arbitrarily-shaped superpixels instead of square patches on a regular grid as the visual units for ViT tokens. 2) We use graph-pooling to group these segment tokens successively towards recognition, forming a fine-to-coarse segmentation hierarchy that reflects partto-whole relationships. The entire model is learned solely from image recognition objectives, either unsupervised (Wu et al., 2018; He et al., 2020) or supervised (Touvron et al., 2021).\"", "section": "Introduction / Implementation description (Section 1 and start of Section 3)"}
{"claim": "Averaging positional embeddings over arbitrarily shaped super-pixels may not reflect their true spatial positions, and this positional embedding pooling issue is not analyzed.", "claim_type": "methodology", "paper_id": "IRcv4yFX6z", "paper_title": "Learning Hierarchical Image Segmentation For Recognition and By Recognition", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "GtHJBOtrXQ", "reviewer": "Reviewer_5FXt", "review_text": "Summary: This paper presents CAST - a variant of ViTs that does hierarchical segmentation of inputs and tokens as part of its pipeline. The method is simple - input pixels are partitioned into super-pixels (in contrast to simple patches for ViTs) a convolutional network is applied over the input image and the resulting features are pooled across the superpixels (average pooling) to create the initial set of tokens. \nFrom here the method works by passing the set of tokens through ViT blocks and between them graph pooling layers which group tokens into a coarser set of tokens by sampling centroids and measuring similarity between tokens and centroids. \n\nThe method is trained with a self supervised objective (MoCo, read out from the last layer) and demonstrated to work well in a variety of contexts.\n\nStrengths: I think this is an interesting paper with good motivation and a simple and effective implementation of core ideas.\n\n* While simple, the use of super pixels and feature pooling to replace the awkward patch based embeddings of ViTs is a nice and original idea.\n* The method is quite elegant and naturally lends itself to several use cases which are demonstrated in the paper in a convincing manner (mostly)\n* Paper is well presented and well executed and was quite easy to follow.\n* A highly applicable method which should be easy to use in several contexts so significant to the community.\n\nWeaknesses: I have some (relatively minor) concerns about parts of this work:\n\n* The major thing which I find missing is more in-depth discussion of the role of super-pixels here - I feel the ablation table is missing one critical row which is a ViT with a super-pixel (+conv net) embedding layer instead of patch extraction. This is also a possible explanation to the results in Table 5. There is a chance that super-pixels are the source of most of the improvements in the paper (quantitatively, at least, clearly there would be no clear hierarchical structure in this case) and this is not discussed enough.\n\n* The positional embedding pooling is a bit odd - super pixels can be with quite arbitrary shapes and there is a chance the resulting average PE over the super pixels would bare little connection to the actual \"position\" of the super pixel. Can the authors comment on that?\n\n* While the authors claim there's a top-down element here (and arguably there is through training) I would argue it is not truly top-down at inference. The super pixels and segmentations do not depend and will not change based on what the top layers infer - the network has top-down pathway to inform them (other than gradients in training). This is of course fine, but it should be discussed.\n\nI hope to see a discussion about the points above and I would be happy to raise my score if these are discussed and addressed to.\n\nQuestions: More minor questions:\n\n* How do segmentations are extracted from ViTs in the paper? it's not clear to me how the results in, say, Figure 4, were generated for ViT-S.\n\n* Is the PE added to the CLS token as well?", "labeling_timestamp": "2026-01-11T17:41:54.133230", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly states it uses arbitrarily-shaped superpixels as ViT tokens (supporting the reviewer’s premise about superpixels), but the provided content contains no discussion or analysis of positional embeddings or of how positional encodings are pooled/averaged over superpixels. Thus the paper does not confirm or analyze the specific positional-embedding pooling concern; that part of the reviewer’s claim is not supported by the paper text.", "evidence": "“We use arbitrarily-shaped superpixels instead of square patches on a regular grid as the visual units for ViT tokens.”", "section": "Introduction"}
{"claim": "Although the authors claim a top-down element, at inference the super-pixel segmentations do not change based on higher-layer outputs, so it is not truly top-down and this is not discussed.", "claim_type": "methodology", "paper_id": "IRcv4yFX6z", "paper_title": "Learning Hierarchical Image Segmentation For Recognition and By Recognition", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "GtHJBOtrXQ", "reviewer": "Reviewer_5FXt", "review_text": "Summary: This paper presents CAST - a variant of ViTs that does hierarchical segmentation of inputs and tokens as part of its pipeline. The method is simple - input pixels are partitioned into super-pixels (in contrast to simple patches for ViTs) a convolutional network is applied over the input image and the resulting features are pooled across the superpixels (average pooling) to create the initial set of tokens. \nFrom here the method works by passing the set of tokens through ViT blocks and between them graph pooling layers which group tokens into a coarser set of tokens by sampling centroids and measuring similarity between tokens and centroids. \n\nThe method is trained with a self supervised objective (MoCo, read out from the last layer) and demonstrated to work well in a variety of contexts.\n\nStrengths: I think this is an interesting paper with good motivation and a simple and effective implementation of core ideas.\n\n* While simple, the use of super pixels and feature pooling to replace the awkward patch based embeddings of ViTs is a nice and original idea.\n* The method is quite elegant and naturally lends itself to several use cases which are demonstrated in the paper in a convincing manner (mostly)\n* Paper is well presented and well executed and was quite easy to follow.\n* A highly applicable method which should be easy to use in several contexts so significant to the community.\n\nWeaknesses: I have some (relatively minor) concerns about parts of this work:\n\n* The major thing which I find missing is more in-depth discussion of the role of super-pixels here - I feel the ablation table is missing one critical row which is a ViT with a super-pixel (+conv net) embedding layer instead of patch extraction. This is also a possible explanation to the results in Table 5. There is a chance that super-pixels are the source of most of the improvements in the paper (quantitatively, at least, clearly there would be no clear hierarchical structure in this case) and this is not discussed enough.\n\n* The positional embedding pooling is a bit odd - super pixels can be with quite arbitrary shapes and there is a chance the resulting average PE over the super pixels would bare little connection to the actual \"position\" of the super pixel. Can the authors comment on that?\n\n* While the authors claim there's a top-down element here (and arguably there is through training) I would argue it is not truly top-down at inference. The super pixels and segmentations do not depend and will not change based on what the top layers infer - the network has top-down pathway to inform them (other than gradients in training). This is of course fine, but it should be discussed.\n\nI hope to see a discussion about the points above and I would be happy to raise my score if these are discussed and addressed to.\n\nQuestions: More minor questions:\n\n* How do segmentations are extracted from ViTs in the paper? it's not clear to me how the results in, say, Figure 4, were generated for ViT-S.\n\n* Is the PE added to the CLS token as well?", "labeling_timestamp": "2026-01-11T17:41:51.523483", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly describes test-time adaptation and a reverse (top-down) hierarchy where backpropagation refines internal segmentation based on higher-layer recognition outputs, so the claim that super-pixel segmentations do not change at inference and that this is not discussed is contradicted.", "evidence": "“For uncertain recognition, our model can continue test-time adaptation (TTA) to solidify recognition. With targeted feedback backpropagating in a reverse hierarchy, it refines internal part-to-whole segmentation alongside improvements in final recognition, capturing vision with scrutiny (Fig. 3 and Appx. C).”\n\n“After backpropagating to increase dog activation, the model undergoes test-time adaptation in a reverse hierarchy. This adjustment allows the next feed-forward process to uncover the whole dog and boost dog activation to 97% ! Our segmentation and recognition thus mutually influence and enhance each other.”", "section": "Figure 3 caption / Introduction and Section 3 (OUR HIERARCHICAL SEGMENTER AND RECOGNIZER ON A CONTINUUM)"}
{"claim": "The paper does not explain how segmentations were extracted from ViTs, making the ViT-S results shown in Figure 4 unclear.", "claim_type": "methodology", "paper_id": "IRcv4yFX6z", "paper_title": "Learning Hierarchical Image Segmentation For Recognition and By Recognition", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "GtHJBOtrXQ", "reviewer": "Reviewer_5FXt", "review_text": "Summary: This paper presents CAST - a variant of ViTs that does hierarchical segmentation of inputs and tokens as part of its pipeline. The method is simple - input pixels are partitioned into super-pixels (in contrast to simple patches for ViTs) a convolutional network is applied over the input image and the resulting features are pooled across the superpixels (average pooling) to create the initial set of tokens. \nFrom here the method works by passing the set of tokens through ViT blocks and between them graph pooling layers which group tokens into a coarser set of tokens by sampling centroids and measuring similarity between tokens and centroids. \n\nThe method is trained with a self supervised objective (MoCo, read out from the last layer) and demonstrated to work well in a variety of contexts.\n\nStrengths: I think this is an interesting paper with good motivation and a simple and effective implementation of core ideas.\n\n* While simple, the use of super pixels and feature pooling to replace the awkward patch based embeddings of ViTs is a nice and original idea.\n* The method is quite elegant and naturally lends itself to several use cases which are demonstrated in the paper in a convincing manner (mostly)\n* Paper is well presented and well executed and was quite easy to follow.\n* A highly applicable method which should be easy to use in several contexts so significant to the community.\n\nWeaknesses: I have some (relatively minor) concerns about parts of this work:\n\n* The major thing which I find missing is more in-depth discussion of the role of super-pixels here - I feel the ablation table is missing one critical row which is a ViT with a super-pixel (+conv net) embedding layer instead of patch extraction. This is also a possible explanation to the results in Table 5. There is a chance that super-pixels are the source of most of the improvements in the paper (quantitatively, at least, clearly there would be no clear hierarchical structure in this case) and this is not discussed enough.\n\n* The positional embedding pooling is a bit odd - super pixels can be with quite arbitrary shapes and there is a chance the resulting average PE over the super pixels would bare little connection to the actual \"position\" of the super pixel. Can the authors comment on that?\n\n* While the authors claim there's a top-down element here (and arguably there is through training) I would argue it is not truly top-down at inference. The super pixels and segmentations do not depend and will not change based on what the top layers infer - the network has top-down pathway to inform them (other than gradients in training). This is of course fine, but it should be discussed.\n\nI hope to see a discussion about the points above and I would be happy to raise my score if these are discussed and addressed to.\n\nQuestions: More minor questions:\n\n* How do segmentations are extracted from ViTs in the paper? it's not clear to me how the results in, say, Figure 4, were generated for ViT-S.\n\n* Is the PE added to the CLS token as well?", "labeling_timestamp": "2026-01-11T17:41:54.169894", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper describes how its own model (CAST) produces hierarchical segmentation (using superpixels and graph-pooling) but does not provide a description of how segmentations were extracted from baseline ViT models (e.g., ViT-S) for the comparisons in Figure 4. The Introduction and related discussion contrast ViT patch-based recognition with CAST's superpixel-based segmentation, but no method for deriving segmentation masks from ViT is given in the provided content.", "evidence": "1) \"We use arbitrarily-shaped superpixels instead of square patches on a regular grid as the visual units for ViT tokens.\" 2) \"Due to their distinct architectural designs, recognition models cannot be directly used for segmentation tasks. Instead, architectural modifications and fine-tuning with segmentation labels are often required (Ahn & Kwak, 2018).\" 3) \"By looping hierarchical segmentation into recognition, CAST delivers four major results. 1) CAST derives a hierarchical segmentation...\"", "section": "1 INTRODUCTION"}
{"claim": "It is unclear whether positional embeddings are added to the CLS token, because the paper does not state this implementation detail.", "claim_type": "methodology", "paper_id": "IRcv4yFX6z", "paper_title": "Learning Hierarchical Image Segmentation For Recognition and By Recognition", "paper_venue": "iclr2024", "paper_decision": "Accept (spotlight)", "decision": "Spotlight", "review_id": "GtHJBOtrXQ", "reviewer": "Reviewer_5FXt", "review_text": "Summary: This paper presents CAST - a variant of ViTs that does hierarchical segmentation of inputs and tokens as part of its pipeline. The method is simple - input pixels are partitioned into super-pixels (in contrast to simple patches for ViTs) a convolutional network is applied over the input image and the resulting features are pooled across the superpixels (average pooling) to create the initial set of tokens. \nFrom here the method works by passing the set of tokens through ViT blocks and between them graph pooling layers which group tokens into a coarser set of tokens by sampling centroids and measuring similarity between tokens and centroids. \n\nThe method is trained with a self supervised objective (MoCo, read out from the last layer) and demonstrated to work well in a variety of contexts.\n\nStrengths: I think this is an interesting paper with good motivation and a simple and effective implementation of core ideas.\n\n* While simple, the use of super pixels and feature pooling to replace the awkward patch based embeddings of ViTs is a nice and original idea.\n* The method is quite elegant and naturally lends itself to several use cases which are demonstrated in the paper in a convincing manner (mostly)\n* Paper is well presented and well executed and was quite easy to follow.\n* A highly applicable method which should be easy to use in several contexts so significant to the community.\n\nWeaknesses: I have some (relatively minor) concerns about parts of this work:\n\n* The major thing which I find missing is more in-depth discussion of the role of super-pixels here - I feel the ablation table is missing one critical row which is a ViT with a super-pixel (+conv net) embedding layer instead of patch extraction. This is also a possible explanation to the results in Table 5. There is a chance that super-pixels are the source of most of the improvements in the paper (quantitatively, at least, clearly there would be no clear hierarchical structure in this case) and this is not discussed enough.\n\n* The positional embedding pooling is a bit odd - super pixels can be with quite arbitrary shapes and there is a chance the resulting average PE over the super pixels would bare little connection to the actual \"position\" of the super pixel. Can the authors comment on that?\n\n* While the authors claim there's a top-down element here (and arguably there is through training) I would argue it is not truly top-down at inference. The super pixels and segmentations do not depend and will not change based on what the top layers infer - the network has top-down pathway to inform them (other than gradients in training). This is of course fine, but it should be discussed.\n\nI hope to see a discussion about the points above and I would be happy to raise my score if these are discussed and addressed to.\n\nQuestions: More minor questions:\n\n* How do segmentations are extracted from ViTs in the paper? it's not clear to me how the results in, say, Figure 4, were generated for ViT-S.\n\n* Is the PE added to the CLS token as well?", "labeling_timestamp": "2026-01-11T17:41:51.935599", "model": "gpt-5-mini", "label": "Supported", "justification": "Nowhere in the provided paper text (Introduction, Related Works, or start of Section 3) does the paper specify implementation details about adding positional embeddings to a CLS token; the method description discusses using superpixels and graph-pooling but omits any mention of CLS tokens or positional embeddings.", "evidence": "We implement our concept by innovating Vision Transformer (ViT) on two aspects. 1) We use arbitrarily-shaped superpixels instead of square patches on a regular grid as the visual units for ViT tokens. 2) We use graph-pooling to group these segment tokens successively towards recognition, forming a fine-to-coarse segmentation hierarchy that reflects partto-whole relationships. The entire model is learned solely from image recognition objectives, either unsupervised (Wu et al., 2018; He et al., 2020) or supervised (Touvron et al., 2021).", "section": "1 INTRODUCTION"}
{"claim": "The paper does not compare the proposed technique with related work beyond CSGM [18], lacking sufficient comparison across other relevant methods.", "claim_type": "baseline", "paper_id": "CgGjT8EG8A", "paper_title": "Universal Exact Compression of Differentially Private Mechanisms", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "i340zWiLwc", "reviewer": "Reviewer_U6Tv", "review_text": "Comment: Thank you for your rebuttal and your efforts for clarifying concerns.  \n\nI appreciate the clarifications on the use of shared randomness and the advantages with respect to CSGM [18]. You have addressed these points successfully.\n\nHowever, I still feel that the proposed technique should be further compared with related work. It is not clear to me why [18] is the only work for which you should compare concrete trade-offs. Even if you have been clear about the conceptual advantages of exact simulation (i.e., unbiased distributions, \"summable\" noise), it does not seem clear why approximate simulation would imply biased estimates. Therefore, I don't fully understand why other techniques mentioned in \"Generic compression of local DP mechanisms\" and \"DME under DP\" paragraphs of Section 2 are excluded from more concrete comparisons made in Section 7. \n\nIn particular (and as already raised in the discussion), the proposed protocol inflates the privacy budget. Therefore if we want to obtain a compressed mechanisms with the exact same $(\\epsilon, \\delta)$ parameters as the original, your protocol should simulate a distribution that provides privacy with lower privacy budget, modifying (even if probably slightly) the distribution parameters. I don't see why this is not a distortion of the original distribution as claimed for other techniques. Therefore, my impression is that the work should either compare with a more extensive number of techniques, putting more illustrations of concrete privacy-accuracy-communication trade-offs on the table or better clarify why these comparisons are not relevant.", "labeling_timestamp": "2026-01-11T17:42:08.063627", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper's abstract and contributions only report experimental comparison against the coordinate subsampled Gaussian mechanism (CSGM) and do not present empirical comparisons to other related methods (e.g., rejection/importance-sampling compressors or dithered-quantization approaches) in the provided content.", "evidence": "\"Experiment results on distributed mean estimation show that PPR consistently gives a better trade-off between communication, accuracy and central differential privacy compared to the coordinate subsampled Gaussian mechanism, while also providing local differential privacy.\"", "section": "Abstract / Our contribution"}
{"claim": "The authors only provide concrete trade-off comparisons against [18] without justifying why other relevant techniques are excluded from these comparisons.", "claim_type": "baseline", "paper_id": "CgGjT8EG8A", "paper_title": "Universal Exact Compression of Differentially Private Mechanisms", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "i340zWiLwc", "reviewer": "Reviewer_U6Tv", "review_text": "Comment: Thank you for your rebuttal and your efforts for clarifying concerns.  \n\nI appreciate the clarifications on the use of shared randomness and the advantages with respect to CSGM [18]. You have addressed these points successfully.\n\nHowever, I still feel that the proposed technique should be further compared with related work. It is not clear to me why [18] is the only work for which you should compare concrete trade-offs. Even if you have been clear about the conceptual advantages of exact simulation (i.e., unbiased distributions, \"summable\" noise), it does not seem clear why approximate simulation would imply biased estimates. Therefore, I don't fully understand why other techniques mentioned in \"Generic compression of local DP mechanisms\" and \"DME under DP\" paragraphs of Section 2 are excluded from more concrete comparisons made in Section 7. \n\nIn particular (and as already raised in the discussion), the proposed protocol inflates the privacy budget. Therefore if we want to obtain a compressed mechanisms with the exact same $(\\epsilon, \\delta)$ parameters as the original, your protocol should simulate a distribution that provides privacy with lower privacy budget, modifying (even if probably slightly) the distribution parameters. I don't see why this is not a distortion of the original distribution as claimed for other techniques. Therefore, my impression is that the work should either compare with a more extensive number of techniques, putting more illustrations of concrete privacy-accuracy-communication trade-offs on the table or better clarify why these comparisons are not relevant.", "labeling_timestamp": "2026-01-11T17:42:24.911455", "model": "gpt-5-mini", "label": "Supported", "justification": "The paper's abstract and contributions explicitly state experimental comparisons only against the coordinate subsampled Gaussian mechanism, while the Related Work section discusses other relevant techniques (e.g., Feldman & Talwar [31], Shah et al. [71], privUnit variants) but the provided text contains no justification for excluding those methods from the concrete trade-off experiments.", "evidence": "Abstract: \"Experiment results on distributed mean estimation show that PPR consistently gives a better trade-off between communication, accuracy and central differential privacy compared to the coordinate subsampled Gaussian mechanism, while also providing local differential privacy.\"; Related Work: \"Subsequently, communication-efficient mechanisms introduced by Feldman and Talwar [31], Shah et al. [71], Isik et al. [51] aimed to construct communication-efficient versions of privUnit, either through distributed simulation or discretization. However, these approaches only approximate the privUnit distribution, while our proposed method ensures exact distribution recovery.\"", "section": "Abstract; 2 Related Work"}
{"claim": "Techniques discussed in the 'Generic compression of local DP mechanisms' and 'DME under DP' paragraphs of Section 2 are excluded from the concrete comparisons presented in Section 7.", "claim_type": "baseline", "paper_id": "CgGjT8EG8A", "paper_title": "Universal Exact Compression of Differentially Private Mechanisms", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "i340zWiLwc", "reviewer": "Reviewer_U6Tv", "review_text": "Comment: Thank you for your rebuttal and your efforts for clarifying concerns.  \n\nI appreciate the clarifications on the use of shared randomness and the advantages with respect to CSGM [18]. You have addressed these points successfully.\n\nHowever, I still feel that the proposed technique should be further compared with related work. It is not clear to me why [18] is the only work for which you should compare concrete trade-offs. Even if you have been clear about the conceptual advantages of exact simulation (i.e., unbiased distributions, \"summable\" noise), it does not seem clear why approximate simulation would imply biased estimates. Therefore, I don't fully understand why other techniques mentioned in \"Generic compression of local DP mechanisms\" and \"DME under DP\" paragraphs of Section 2 are excluded from more concrete comparisons made in Section 7. \n\nIn particular (and as already raised in the discussion), the proposed protocol inflates the privacy budget. Therefore if we want to obtain a compressed mechanisms with the exact same $(\\epsilon, \\delta)$ parameters as the original, your protocol should simulate a distribution that provides privacy with lower privacy budget, modifying (even if probably slightly) the distribution parameters. I don't see why this is not a distortion of the original distribution as claimed for other techniques. Therefore, my impression is that the work should either compare with a more extensive number of techniques, putting more illustrations of concrete privacy-accuracy-communication trade-offs on the table or better clarify why these comparisons are not relevant.", "labeling_timestamp": "2026-01-11T17:42:20.704708", "model": "gpt-5-mini", "label": "Not Determined", "justification": "The provided excerpt contains Section 2 (Related Work) and the Abstract but does not include Section 7 or its comparisons. The paper mentions prior compression techniques in Section 2 and reports experiments in the Abstract that compare PPR to a coordinate-subsampled Gaussian mechanism, but there is no available text showing the contents of Section 7 to confirm whether the techniques from the two paragraphs were excluded from concrete comparisons.", "evidence": "Section 2: \"...Feldman and Talwar [31] proposed a distributed simulation approach using rejection sampling, while Shah et al. [71], Triastcyn et al. [78] utilized importance sampling (or more specifically, minimum random coding [20, 74, 47]). However, all these methods only approximate the original local DP mechanism, unlike our scheme, which achieves an exact distribution recovery.\"; Abstract: \"Experiment results on distributed mean estimation show that PPR consistently gives a better trade-off between communication, accuracy and central differential privacy compared to the coordinate subsampled Gaussian mechanism, while also providing local differential privacy.\"", "section": "Section 2 (Related Work) and Abstract"}
{"claim": "The manuscript does not explain why approximate simulation would necessarily lead to biased estimates despite claiming conceptual advantages for exact simulation.", "claim_type": "methodology", "paper_id": "CgGjT8EG8A", "paper_title": "Universal Exact Compression of Differentially Private Mechanisms", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "i340zWiLwc", "reviewer": "Reviewer_U6Tv", "review_text": "Comment: Thank you for your rebuttal and your efforts for clarifying concerns.  \n\nI appreciate the clarifications on the use of shared randomness and the advantages with respect to CSGM [18]. You have addressed these points successfully.\n\nHowever, I still feel that the proposed technique should be further compared with related work. It is not clear to me why [18] is the only work for which you should compare concrete trade-offs. Even if you have been clear about the conceptual advantages of exact simulation (i.e., unbiased distributions, \"summable\" noise), it does not seem clear why approximate simulation would imply biased estimates. Therefore, I don't fully understand why other techniques mentioned in \"Generic compression of local DP mechanisms\" and \"DME under DP\" paragraphs of Section 2 are excluded from more concrete comparisons made in Section 7. \n\nIn particular (and as already raised in the discussion), the proposed protocol inflates the privacy budget. Therefore if we want to obtain a compressed mechanisms with the exact same $(\\epsilon, \\delta)$ parameters as the original, your protocol should simulate a distribution that provides privacy with lower privacy budget, modifying (even if probably slightly) the distribution parameters. I don't see why this is not a distortion of the original distribution as claimed for other techniques. Therefore, my impression is that the work should either compare with a more extensive number of techniques, putting more illustrations of concrete privacy-accuracy-communication trade-offs on the table or better clarify why these comparisons are not relevant.", "labeling_timestamp": "2026-01-11T17:42:29.918588", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states that approximate simulation changes the output distribution and therefore can fail to preserve statistical properties such as unbiasedness, and it expressly notes prior simulation-based methods are often biased unless debiasing is applied.", "evidence": "“Since the induced mechanism is different from the original mechanism, these statistical properties are not preserved.” / “In contrast, reconstruction distributions in prior simulation-based compression methods [31, 71] are often biased unless specific debiasing steps are performed (only possible for certain DP mechanisms [71]).”", "section": "Introduction (discussion of prior methods) and Our contribution (Exactness)"}
{"claim": "The proposed protocol appears to inflate the privacy budget, which raises concerns about the validity of the privacy guarantees and comparisons.", "claim_type": "methodology", "paper_id": "CgGjT8EG8A", "paper_title": "Universal Exact Compression of Differentially Private Mechanisms", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "i340zWiLwc", "reviewer": "Reviewer_U6Tv", "review_text": "Comment: Thank you for your rebuttal and your efforts for clarifying concerns.  \n\nI appreciate the clarifications on the use of shared randomness and the advantages with respect to CSGM [18]. You have addressed these points successfully.\n\nHowever, I still feel that the proposed technique should be further compared with related work. It is not clear to me why [18] is the only work for which you should compare concrete trade-offs. Even if you have been clear about the conceptual advantages of exact simulation (i.e., unbiased distributions, \"summable\" noise), it does not seem clear why approximate simulation would imply biased estimates. Therefore, I don't fully understand why other techniques mentioned in \"Generic compression of local DP mechanisms\" and \"DME under DP\" paragraphs of Section 2 are excluded from more concrete comparisons made in Section 7. \n\nIn particular (and as already raised in the discussion), the proposed protocol inflates the privacy budget. Therefore if we want to obtain a compressed mechanisms with the exact same $(\\epsilon, \\delta)$ parameters as the original, your protocol should simulate a distribution that provides privacy with lower privacy budget, modifying (even if probably slightly) the distribution parameters. I don't see why this is not a distortion of the original distribution as claimed for other techniques. Therefore, my impression is that the work should either compare with a more extensive number of techniques, putting more illustrations of concrete privacy-accuracy-communication trade-offs on the table or better clarify why these comparisons are not relevant.", "labeling_timestamp": "2026-01-11T17:42:27.033321", "model": "gpt-5-mini", "label": "Partially Supported", "justification": "The paper explicitly states that PPR can result in a weaker local DP (larger ε_local) while achieving stronger central DP, so the reviewer’s observation about inflating the local privacy budget is correct. However, the paper also provides constructions (a randomized encoder) and arguments that preserve exact output distributions and enable precise privacy accounting, arguing that privacy guarantees and comparisons remain valid. Thus the reviewer’s concern is only partially reflected in the paper.", "evidence": "“PPR can compress a central DP mechanism (e.g., the Gaussian mechanism [27]) and simultaneously achieve weaker local DP (i.e., with a larger ε_local) and stronger central DP (i.e., with a smaller ε_central), while maintaining exactly the same privacy-utility trade-offs as the uncompressed Gaussian mechanism.”; “Bun et al. [12] proposed another generic compression technique based on rejection sampling, which compresses a ε -DP mechanism into a 10 ε -DP mechanism.”; “To ensure local DP, we introduce a randomized encoder based on the Poisson functional representation, which stochastically maps a private local message to its representation. Hence, PPR achieves order-wise trade-offs between privacy, communication, and accuracy, while preserving the original distribution of local randomizers.”", "section": "Our contribution; Related Work; Introduction"}
{"claim": "To obtain a compressed mechanism with the same (ε, δ) parameters as the original, the protocol must simulate a distribution providing privacy under a lower budget, thus modifying distribution parameters.", "claim_type": "methodology", "paper_id": "CgGjT8EG8A", "paper_title": "Universal Exact Compression of Differentially Private Mechanisms", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "i340zWiLwc", "reviewer": "Reviewer_U6Tv", "review_text": "Comment: Thank you for your rebuttal and your efforts for clarifying concerns.  \n\nI appreciate the clarifications on the use of shared randomness and the advantages with respect to CSGM [18]. You have addressed these points successfully.\n\nHowever, I still feel that the proposed technique should be further compared with related work. It is not clear to me why [18] is the only work for which you should compare concrete trade-offs. Even if you have been clear about the conceptual advantages of exact simulation (i.e., unbiased distributions, \"summable\" noise), it does not seem clear why approximate simulation would imply biased estimates. Therefore, I don't fully understand why other techniques mentioned in \"Generic compression of local DP mechanisms\" and \"DME under DP\" paragraphs of Section 2 are excluded from more concrete comparisons made in Section 7. \n\nIn particular (and as already raised in the discussion), the proposed protocol inflates the privacy budget. Therefore if we want to obtain a compressed mechanisms with the exact same $(\\epsilon, \\delta)$ parameters as the original, your protocol should simulate a distribution that provides privacy with lower privacy budget, modifying (even if probably slightly) the distribution parameters. I don't see why this is not a distortion of the original distribution as claimed for other techniques. Therefore, my impression is that the work should either compare with a more extensive number of techniques, putting more illustrations of concrete privacy-accuracy-communication trade-offs on the table or better clarify why these comparisons are not relevant.", "labeling_timestamp": "2026-01-11T17:42:29.547785", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states that PPR performs exact simulation and preserves the original joint distribution of data and output, so it does not require modifying the distribution (or 'lowering the privacy budget') to obtain a compressed mechanism with the same distributional properties. While the paper notes prior methods approximate mechanisms and can alter privacy budgets, PPR is presented as avoiding that by exact distribution recovery.", "evidence": "“PPR exactly preserves the joint distribution of the data and the output of the original local randomizer.”; “Unlike previous DP mechanism compressors ... PPR enables exact simulation, ensuring that the reproduced distribution perfectly matches the original one.”; “PPR can compress a central DP mechanism (e.g., the Gaussian mechanism [27]) and simultaneously achieve weaker local DP (i.e., with a larger ε_local) and stronger central DP (i.e., with a smaller ε_central), while maintaining exactly the same privacy-utility trade-offs as the uncompressed Gaussian mechanism.”", "section": "Our contribution / Exactness (Introduction)"}
{"claim": "If the protocol modifies distribution parameters to match privacy budgets, that modification should be considered a distortion similar to distortions claimed for other techniques.", "claim_type": "subjective", "paper_id": "CgGjT8EG8A", "paper_title": "Universal Exact Compression of Differentially Private Mechanisms", "paper_venue": "neurips2024", "paper_decision": "Accept (poster)", "decision": "Poster", "review_id": "i340zWiLwc", "reviewer": "Reviewer_U6Tv", "review_text": "Comment: Thank you for your rebuttal and your efforts for clarifying concerns.  \n\nI appreciate the clarifications on the use of shared randomness and the advantages with respect to CSGM [18]. You have addressed these points successfully.\n\nHowever, I still feel that the proposed technique should be further compared with related work. It is not clear to me why [18] is the only work for which you should compare concrete trade-offs. Even if you have been clear about the conceptual advantages of exact simulation (i.e., unbiased distributions, \"summable\" noise), it does not seem clear why approximate simulation would imply biased estimates. Therefore, I don't fully understand why other techniques mentioned in \"Generic compression of local DP mechanisms\" and \"DME under DP\" paragraphs of Section 2 are excluded from more concrete comparisons made in Section 7. \n\nIn particular (and as already raised in the discussion), the proposed protocol inflates the privacy budget. Therefore if we want to obtain a compressed mechanisms with the exact same $(\\epsilon, \\delta)$ parameters as the original, your protocol should simulate a distribution that provides privacy with lower privacy budget, modifying (even if probably slightly) the distribution parameters. I don't see why this is not a distortion of the original distribution as claimed for other techniques. Therefore, my impression is that the work should either compare with a more extensive number of techniques, putting more illustrations of concrete privacy-accuracy-communication trade-offs on the table or better clarify why these comparisons are not relevant.", "labeling_timestamp": "2026-01-11T17:42:49.612458", "model": "gpt-5-mini", "label": "Contradicted", "justification": "The paper explicitly states that PPR achieves exact simulation and 'exactly preserves the joint distribution of the data and the output of the original local randomizer', so it does not modify distribution parameters in the way the reviewer claims; it presents prior methods' parameter changes/approximations as distortions but positions PPR as avoiding such distortions.", "evidence": "\"PPR exactly preserves the joint distribution of the data and the output of the original local randomizer.\" \"Unlike previous DP mechanism compressors such as Feldman and Talwar [31], Shah et al. [71], Triastcyn et al. [78], PPR enables exact simulation, ensuring that the reproduced distribution perfectly matches the original one.\"", "section": "Abstract; Our contribution (Exactness)"}
